{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries;\n",
    "import numpy as np\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# use seaborn plotting defaults\n",
    "import seaborn as sns; sns.set_style('white')\n",
    "\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from numpy import genfromtxt\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import load_digits, make_blobs\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, plot_confusion_matrix\n",
    "from sklearn.model_selection import KFold, train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, StandardScaler \n",
    "from sklearn.preprocessing import RobustScaler, Normalizer, QuantileTransformer, PowerTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import seaborn as sns\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65532,)\n",
      "(65532, 10)\n",
      "(65532, 11)\n",
      "[0 0 0 ... 2 2 2]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source Port</th>\n",
       "      <th>Destination Port</th>\n",
       "      <th>NAT Source Port</th>\n",
       "      <th>NAT Destination Port</th>\n",
       "      <th>Bytes</th>\n",
       "      <th>Bytes Sent</th>\n",
       "      <th>Bytes Received</th>\n",
       "      <th>Packets</th>\n",
       "      <th>Elapsed Time (sec)</th>\n",
       "      <th>pkts_sent</th>\n",
       "      <th>Action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57222</td>\n",
       "      <td>53</td>\n",
       "      <td>54587</td>\n",
       "      <td>53</td>\n",
       "      <td>177</td>\n",
       "      <td>94</td>\n",
       "      <td>83</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>allow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56258</td>\n",
       "      <td>3389</td>\n",
       "      <td>56258</td>\n",
       "      <td>3389</td>\n",
       "      <td>4768</td>\n",
       "      <td>1600</td>\n",
       "      <td>3168</td>\n",
       "      <td>19</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>allow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6881</td>\n",
       "      <td>50321</td>\n",
       "      <td>43265</td>\n",
       "      <td>50321</td>\n",
       "      <td>238</td>\n",
       "      <td>118</td>\n",
       "      <td>120</td>\n",
       "      <td>2</td>\n",
       "      <td>1199</td>\n",
       "      <td>1</td>\n",
       "      <td>allow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50553</td>\n",
       "      <td>3389</td>\n",
       "      <td>50553</td>\n",
       "      <td>3389</td>\n",
       "      <td>3327</td>\n",
       "      <td>1438</td>\n",
       "      <td>1889</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>allow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50002</td>\n",
       "      <td>443</td>\n",
       "      <td>45848</td>\n",
       "      <td>443</td>\n",
       "      <td>25358</td>\n",
       "      <td>6778</td>\n",
       "      <td>18580</td>\n",
       "      <td>31</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>allow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Source Port  Destination Port  NAT Source Port  NAT Destination Port  \\\n",
       "0        57222                53            54587                    53   \n",
       "1        56258              3389            56258                  3389   \n",
       "2         6881             50321            43265                 50321   \n",
       "3        50553              3389            50553                  3389   \n",
       "4        50002               443            45848                   443   \n",
       "\n",
       "   Bytes  Bytes Sent  Bytes Received  Packets  Elapsed Time (sec)  pkts_sent  \\\n",
       "0    177          94              83        2                  30          1   \n",
       "1   4768        1600            3168       19                  17         10   \n",
       "2    238         118             120        2                1199          1   \n",
       "3   3327        1438            1889       15                  17          8   \n",
       "4  25358        6778           18580       31                  16         13   \n",
       "\n",
       "  Action  \n",
       "0  allow  \n",
       "1  allow  \n",
       "2  allow  \n",
       "3  allow  \n",
       "4  allow  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fireWall = pd.read_csv('log2.csv')\n",
    "\n",
    "fireWallXData = fireWall[['Source Port', 'Destination Port','NAT Source Port',\n",
    "                         'NAT Destination Port','Bytes','Bytes Sent','Bytes Received','Packets',\n",
    "                        'Elapsed Time (sec)','pkts_sent','pkts_received']]\n",
    "fireWallyData = fireWall[['Action']]\n",
    "\n",
    "X = fireWallXData.iloc[:,0:10]\n",
    "y = fireWallyData.iloc[:,0]\n",
    "\n",
    "\n",
    "print(y.shape)\n",
    "print(X.shape)\n",
    "\n",
    "#print(fireWall.shape)\n",
    "\n",
    "FireWALLData = pd.concat([X, y], axis=1)\n",
    "print(FireWALLData.shape)\n",
    "\n",
    "newX = FireWALLData.iloc[:,0:9]\n",
    "newy = FireWALLData.iloc[:,10]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encodeLabel = LabelEncoder()\n",
    "newy = encodeLabel.fit_transform(newy)\n",
    "print(newy)\n",
    "\n",
    "\n",
    "FireWALLData.loc[FireWALLData['Action'] == 0, 'Action'] = 1\n",
    "FireWALLData.loc[FireWALLData['Action'] == 1, 'Action'] = 0\n",
    "FireWALLData.loc[FireWALLData['Action'] == 3, 'Action'] = 0\n",
    "FireWALLData.loc[FireWALLData['Action'] == 4, 'Action'] = 0\n",
    "\n",
    "FireWALLData.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( newX, newy, test_size=60000, random_state=1738)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTI LAYERED PERCEPTRON TRIAL ONE ON FIRE WALL DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 288 candidates, totalling 576 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    6.6s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:    8.5s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    9.5s\n",
      "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed:   10.7s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   11.9s\n",
      "[Parallel(n_jobs=-1)]: Done 189 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=-1)]: Done 210 tasks      | elapsed:   15.9s\n",
      "[Parallel(n_jobs=-1)]: Done 233 tasks      | elapsed:   18.4s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:   21.6s\n",
      "[Parallel(n_jobs=-1)]: Done 281 tasks      | elapsed:   24.7s\n",
      "[Parallel(n_jobs=-1)]: Done 306 tasks      | elapsed:   27.9s\n",
      "[Parallel(n_jobs=-1)]: Done 333 tasks      | elapsed:   31.4s\n",
      "[Parallel(n_jobs=-1)]: Done 360 tasks      | elapsed:   35.0s\n",
      "[Parallel(n_jobs=-1)]: Done 389 tasks      | elapsed:   39.0s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:   42.9s\n",
      "[Parallel(n_jobs=-1)]: Done 449 tasks      | elapsed:   47.3s\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:   51.5s\n",
      "[Parallel(n_jobs=-1)]: Done 513 tasks      | elapsed:   56.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL ONE RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 576 out of 576 | elapsed:  1.1min finished\n"
     ]
    }
   ],
   "source": [
    "ITERATIONS = 1\n",
    "for i in range(1):\n",
    "    #For reach \"trial\", randomly select 5000 data points\n",
    "    random5000DataPoints = FireWALLData.sample(n = 5000, replace = True)\n",
    "    #From that 5000, create x set and y set\n",
    "    xSet = random5000DataPoints.iloc[:,0:9].values\n",
    "    ySet = random5000DataPoints.iloc[:,10].values\n",
    "    ySet = encodeLabel.fit_transform(ySet)\n",
    "    \n",
    "    pipe = Pipeline([('std', StandardScaler()),('classifier', MLPClassifier(max_iter = 250))])\n",
    "\n",
    "    search_space = [\n",
    "                {\n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                {\n",
    "                 \n",
    "                  \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['sgd'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                \n",
    "                {\n",
    "                \n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['adam'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                ]\n",
    "    \n",
    "    #'precision_micro','roc_auc_ovo','jaccard','neg_log_loss'] JACCARD DOES NOT WORK FOR NON BINARY?\n",
    "    runHyperParamSearch = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=2), \n",
    "                          scoring=['recall_micro','f1_micro','roc_auc_ovo','neg_log_loss'], \n",
    "                          refit=False, verbose=10, n_jobs = -1)\n",
    "    TRIAL_1_MLP = runHyperParamSearch.fit(xSet, ySet)\n",
    "    print(\"---------------------------------TRIAL ONE RESULTS ---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.4984293 , 0.71811748, 0.53846347, 0.71061039, 0.65581536,\n",
       "        0.70735872, 0.84147406, 1.03163695, 1.01362216, 1.12872076,\n",
       "        1.09969616, 1.0719223 , 0.68633974, 0.53245974, 0.68433917,\n",
       "        0.85298252, 0.83947313, 0.83071518, 0.98459625, 0.98259461,\n",
       "        1.07142174, 1.15349197, 1.08067918, 1.0351392 , 0.66982651,\n",
       "        0.47816181, 0.67683256, 0.80344045, 0.80444431, 0.76440704,\n",
       "        0.71636546, 0.92529583, 0.9140377 , 0.99360526, 0.9983598 ,\n",
       "        1.02212989, 0.37807977, 0.30276036, 0.37582445, 0.53120685,\n",
       "        0.52795649, 0.44212985, 0.67007673, 0.67382932, 0.54446828,\n",
       "        0.60577238, 0.68158662, 0.68959379, 0.64505458, 0.61277759,\n",
       "        0.74238801, 0.93605614, 0.88776374, 0.91478705, 1.11746156,\n",
       "        1.1292218 , 1.15023971, 1.29786742, 1.23806453, 1.26859093,\n",
       "        0.56823874, 0.75289583, 0.72988009, 0.89376807, 0.90502691,\n",
       "        0.91203558, 1.10444951, 1.11495972, 1.14523423, 1.25532913,\n",
       "        1.21854627, 1.25708163, 0.68558943, 0.73288059, 0.7048558 ,\n",
       "        0.89201832, 0.8950206 , 0.91728926, 1.13747883, 1.13747752,\n",
       "        1.09894419, 1.21554577, 1.23080873, 1.2395668 , 0.71186125,\n",
       "        0.67157543, 0.54847014, 0.90327656, 0.89476943, 0.88075686,\n",
       "        1.09444201, 1.13597703, 1.07892835, 1.3071245 , 1.18051445,\n",
       "        1.20478594, 1.96368861, 1.02137673, 1.90563977, 1.99271357,\n",
       "        1.20028174, 1.95843363, 2.1233263 , 1.36192179, 2.14434409,\n",
       "        2.16686368, 1.74174869, 2.14909852, 1.81906426, 1.03413916,\n",
       "        1.82506883, 1.91889906, 1.28585625, 1.91764939, 2.03199756,\n",
       "        1.07292271, 2.08779621, 2.13458586, 1.74900389, 2.13308465,\n",
       "        1.77953053, 0.91778958, 1.81731367, 2.01448262, 0.70110321,\n",
       "        1.95768428, 2.17612207, 2.02023757, 2.20689845, 2.3567785 ,\n",
       "        2.52342033, 2.43409324, 2.10706198, 1.62039328, 2.02849472,\n",
       "        2.11181593, 1.50904667, 1.99546611, 2.31524146, 2.08554375,\n",
       "        2.25168598, 2.1928854 , 2.33625901, 2.21715796, 1.89062655,\n",
       "        1.82106638, 1.87085915, 2.01998758, 1.92015183, 1.98095369,\n",
       "        2.15835571, 2.24142861, 2.17211711, 2.28121173, 2.13733768,\n",
       "        2.25443888, 1.86685634, 1.31788373, 1.823318  , 2.00822711,\n",
       "        2.00847733, 2.05101407, 2.19313574, 2.19538796, 2.25293744,\n",
       "        2.31373978, 2.24593115, 2.24442983, 1.88487077, 1.6238966 ,\n",
       "        1.89162695, 2.0194869 , 1.98595798, 2.06327581, 2.24392974,\n",
       "        2.22266185, 2.20614767, 2.27445602, 2.22091031, 2.3382616 ,\n",
       "        1.94392157, 1.96744227, 1.97870231, 2.15610528, 1.91414666,\n",
       "        2.11832201, 2.32274711, 2.23792422, 2.28921902, 2.3898052 ,\n",
       "        2.31323969, 2.3587786 , 2.06552672, 2.11857188, 2.02223933,\n",
       "        2.09004736, 2.16360998, 2.154603  , 2.29097104, 2.27920985,\n",
       "        2.28096163, 2.33150494, 2.35577619, 2.28171265, 1.94567335,\n",
       "        1.96318901, 1.97670043, 2.10781252, 2.1070621 , 2.0742842 ,\n",
       "        2.25544012, 2.26569796, 2.26144516, 2.33976257, 2.28871799,\n",
       "        2.34351563, 1.98095405, 1.95217919, 1.97720134, 2.07128048,\n",
       "        2.10405958, 2.09204984, 2.3822993 , 2.2749573 , 2.28071129,\n",
       "        2.36728597, 2.36478317, 2.44134986, 2.09204888, 2.08379233,\n",
       "        1.97519994, 2.08354151, 2.05776989, 2.10981441, 2.24092734,\n",
       "        2.2596935 , 2.23442173, 2.33700955, 2.3640337 , 2.31273937,\n",
       "        1.97144544, 2.00097108, 2.00647521, 2.16561282, 2.09905541,\n",
       "        2.18913364, 2.32424903, 2.35327351, 2.30172944, 2.43409216,\n",
       "        2.40581906, 2.47813034, 2.00797665, 2.07978845, 2.12232506,\n",
       "        2.33200514, 2.31273901, 2.53017569, 2.53668082, 2.65703595,\n",
       "        2.5897274 , 2.66028893, 2.52467072, 2.51316082, 2.107813  ,\n",
       "        2.18713045, 2.25368786, 2.36603463, 2.43234181, 2.36453342,\n",
       "        2.55094397, 2.54605496, 2.41032362, 2.83118546, 2.54668975,\n",
       "        2.59848452, 2.2261641 , 2.1258297 , 2.13695085, 2.23116827,\n",
       "        2.27520609, 2.13458526, 2.18613398, 2.13558555, 2.10330939,\n",
       "        2.04000366, 1.90688992, 1.71872735]),\n",
       " 'std_fit_time': array([2.01675177e-01, 3.75323296e-02, 6.50557280e-02, 8.15699100e-02,\n",
       "        2.25444794e-01, 1.77902579e-01, 2.15936422e-01, 2.32696533e-02,\n",
       "        1.12596750e-02, 2.82735825e-02, 7.50541687e-04, 2.60219574e-02,\n",
       "        4.62898016e-02, 6.65593147e-02, 6.43049479e-02, 2.42691040e-02,\n",
       "        6.75690174e-03, 9.50884819e-03, 1.27611160e-02, 4.37875986e-02,\n",
       "        2.65227556e-02, 3.55310440e-02, 1.92666054e-02, 4.25255299e-03,\n",
       "        1.17604733e-02, 1.83908224e-01, 7.75754452e-03, 4.27869558e-02,\n",
       "        7.75814056e-03, 2.72728205e-02, 2.12432504e-01, 9.99689102e-04,\n",
       "        3.25274467e-03, 8.25726986e-03, 6.50632381e-03, 4.75323200e-03,\n",
       "        1.22565031e-02, 1.90155506e-02, 1.80156231e-02, 1.10845327e-01,\n",
       "        8.15713406e-02, 1.00337267e-01, 1.30611777e-01, 6.08017445e-02,\n",
       "        8.95770788e-02, 5.42949438e-02, 2.20185518e-02, 5.10442257e-02,\n",
       "        4.15349007e-02, 1.19852901e-01, 2.87744999e-02, 1.12578869e-02,\n",
       "        2.70230770e-02, 4.05344963e-02, 2.75217295e-02, 5.25391102e-03,\n",
       "        2.42712498e-02, 5.12944460e-02, 2.60233879e-02, 3.10275555e-02,\n",
       "        2.27692127e-02, 3.82840633e-02, 3.42814922e-02, 5.50556183e-03,\n",
       "        1.97668076e-02, 1.22605562e-02, 9.00888443e-03, 1.50036812e-03,\n",
       "        6.25550747e-03, 1.75261497e-03, 7.50839710e-03, 2.25194693e-02,\n",
       "        5.00432253e-02, 1.02584362e-02, 2.62717009e-02, 2.37689018e-02,\n",
       "        3.52796316e-02, 1.55134201e-02, 1.95171833e-02, 2.05181837e-02,\n",
       "        3.05275917e-02, 2.30196714e-02, 1.32615566e-02, 4.50301170e-03,\n",
       "        4.75466251e-03, 2.70227194e-02, 5.00429869e-02, 2.15190649e-02,\n",
       "        5.30452728e-02, 2.00152397e-03, 4.00346518e-02, 1.95167065e-02,\n",
       "        2.50256062e-03, 3.30277681e-02, 3.50288153e-02, 5.07942438e-02,\n",
       "        8.00609589e-03, 3.44795108e-01, 2.15193033e-02, 6.60573244e-02,\n",
       "        7.92430997e-01, 3.02770138e-02, 7.50684738e-03, 7.68911362e-01,\n",
       "        3.00240517e-03, 2.90250778e-02, 1.23856425e-01, 1.67638063e-02,\n",
       "        1.75150633e-02, 1.09344244e-01, 5.00476360e-03, 7.25626945e-03,\n",
       "        6.20283604e-01, 2.35203505e-02, 3.52801085e-02, 8.73251677e-01,\n",
       "        2.20189095e-02, 1.47621632e-02, 4.43382025e-01, 3.82817984e-02,\n",
       "        8.50796700e-03, 5.08437037e-01, 2.57723331e-02, 1.31363034e-01,\n",
       "        3.33286285e-01, 2.10182667e-02, 1.06341720e-01, 4.42380548e-01,\n",
       "        3.60317230e-02, 1.81905627e-01, 1.92661285e-02, 1.62139177e-01,\n",
       "        1.24857783e-01, 3.20277214e-02, 1.23356700e-01, 7.05609322e-02,\n",
       "        6.61818624e-01, 1.97666883e-02, 1.20854020e-01, 2.47963548e-01,\n",
       "        6.23041391e-02, 1.45120621e-02, 3.97840738e-02, 1.12606287e-02,\n",
       "        4.65401411e-02, 8.65738392e-02, 4.67901230e-02, 4.75430489e-03,\n",
       "        5.35458326e-02, 1.67648792e-02, 4.20365334e-02, 3.30290794e-02,\n",
       "        1.77656412e-02, 3.82832289e-02, 9.90848541e-02, 3.60298157e-02,\n",
       "        5.75470924e-03, 6.01267815e-01, 1.22601986e-02, 1.40123367e-02,\n",
       "        3.32783461e-02, 7.23122358e-02, 3.32784653e-02, 2.75238752e-02,\n",
       "        8.50737095e-03, 3.97846699e-02, 6.45558834e-02, 5.05430698e-02,\n",
       "        2.47716904e-02, 2.83244133e-01, 4.00412083e-03, 2.32703686e-02,\n",
       "        1.17601156e-02, 1.60143375e-02, 5.00380993e-03, 4.07854319e-02,\n",
       "        8.23211670e-02, 7.50613213e-03, 3.55305672e-02, 4.75335121e-03,\n",
       "        4.72905636e-02, 8.25667381e-03, 6.45558834e-02, 4.87918854e-02,\n",
       "        1.78153992e-01, 1.95170641e-02, 1.27607584e-02, 9.05784369e-02,\n",
       "        1.48378253e-01, 3.52803469e-02, 3.32778692e-02, 1.02590322e-02,\n",
       "        1.42621994e-02, 4.37878370e-02, 9.50825214e-03, 2.42705345e-02,\n",
       "        1.42619610e-02, 5.07931709e-02, 3.85334492e-02, 7.50064850e-04,\n",
       "        3.95337343e-02, 1.50126219e-02, 6.83084726e-02, 2.67730951e-02,\n",
       "        8.50713253e-03, 1.20106936e-02, 3.75319719e-02, 5.00082970e-04,\n",
       "        1.47628784e-02, 3.45295668e-02, 2.85242796e-02, 7.50780106e-04,\n",
       "        1.40117407e-02, 1.24990940e-03, 1.67644024e-02, 5.50484657e-03,\n",
       "        3.37785482e-02, 1.60138607e-02, 3.20276022e-02, 7.50637054e-03,\n",
       "        2.87743807e-02, 5.25450706e-03, 7.98181295e-02, 3.50296497e-03,\n",
       "        7.75730610e-03, 2.02670097e-02, 2.49266624e-04, 8.98274183e-02,\n",
       "        8.38227272e-02, 1.03589654e-01, 2.40194798e-02, 2.87748575e-02,\n",
       "        2.20187902e-02, 2.50215530e-02, 4.10352945e-02, 2.07675695e-02,\n",
       "        3.15270424e-02, 1.00084543e-02, 7.00664520e-03, 7.08115101e-02,\n",
       "        1.67642832e-02, 5.25403023e-03, 4.42879200e-02, 9.25743580e-03,\n",
       "        7.75659084e-03, 1.77648067e-02, 3.37789059e-02, 2.82746553e-02,\n",
       "        2.22690105e-02, 2.50184536e-03, 1.52627230e-02, 1.90169811e-02,\n",
       "        1.27600431e-02, 2.55217552e-02, 7.00598955e-02, 3.10267210e-02,\n",
       "        1.09844327e-01, 8.30717087e-02, 1.21606231e-01, 2.62476563e-01,\n",
       "        1.34616137e-01, 1.67144656e-01, 2.35205889e-02, 3.00253630e-02,\n",
       "        7.00628757e-03, 1.44875050e-01, 8.73243809e-02, 9.00775194e-02,\n",
       "        3.25274467e-03, 4.50391769e-02, 6.93103075e-02, 1.04725480e-01,\n",
       "        5.27944565e-02, 2.82751322e-02, 4.95426655e-02, 6.48057461e-02,\n",
       "        1.75237656e-03, 1.00586891e-01, 5.04306555e-02, 2.52709389e-02,\n",
       "        1.67646408e-02, 2.25079060e-03, 1.67680979e-02, 1.02591515e-02,\n",
       "        1.35116577e-02, 2.49981880e-04, 2.27692127e-02, 4.07853127e-02]),\n",
       " 'mean_score_time': array([0.02051675, 0.02502227, 0.02101684, 0.02226913, 0.02351928,\n",
       "        0.02226877, 0.02402091, 0.02677393, 0.02426994, 0.02552223,\n",
       "        0.02251923, 0.02377117, 0.02151895, 0.02076602, 0.02226996,\n",
       "        0.02302003, 0.02351952, 0.02251828, 0.02302074, 0.02502155,\n",
       "        0.02802384, 0.02427137, 0.02477109, 0.02302051, 0.02452147,\n",
       "        0.02251923, 0.02377152, 0.02952635, 0.02526939, 0.0242722 ,\n",
       "        0.02352107, 0.02377021, 0.02276909, 0.0247705 , 0.02427042,\n",
       "        0.02577126, 0.02076292, 0.02477157, 0.02076709, 0.02277052,\n",
       "        0.0222677 , 0.02352047, 0.02251995, 0.02401984, 0.02402055,\n",
       "        0.02276778, 0.02351999, 0.02326941, 0.02101743, 0.02176929,\n",
       "        0.02201903, 0.02226806, 0.02527165, 0.02352107, 0.02352083,\n",
       "        0.02251863, 0.02251947, 0.02477121, 0.02402031, 0.02677357,\n",
       "        0.02201831, 0.02101898, 0.02276838, 0.02502203, 0.02302003,\n",
       "        0.02502048, 0.02427161, 0.02477062, 0.02326989, 0.02552152,\n",
       "        0.0252732 , 0.02902591, 0.02176857, 0.02276945, 0.02176964,\n",
       "        0.02552104, 0.02902424, 0.02402186, 0.02452123, 0.02402139,\n",
       "        0.02577245, 0.02302015, 0.02452159, 0.02927423, 0.02852476,\n",
       "        0.02201974, 0.02076674, 0.03027666, 0.02702272, 0.02802515,\n",
       "        0.03177714, 0.02452183, 0.02702415, 0.02477002, 0.02377129,\n",
       "        0.02527165, 0.02126849, 0.02251923, 0.02101696, 0.02201879,\n",
       "        0.02126849, 0.02176869, 0.02427101, 0.02176893, 0.02201903,\n",
       "        0.02327025, 0.02101815, 0.02155769, 0.02101803, 0.02151895,\n",
       "        0.02126908, 0.02201879, 0.02176845, 0.02602196, 0.02126801,\n",
       "        0.02252007, 0.02226853, 0.02402079, 0.02302003, 0.02201867,\n",
       "        0.02377045, 0.02151942, 0.02427077, 0.02327037, 0.02101779,\n",
       "        0.03227758, 0.02552211, 0.02251947, 0.03327811, 0.02602124,\n",
       "        0.02176809, 0.03377962, 0.02327001, 0.02377021, 0.02251887,\n",
       "        0.02101803, 0.02852499, 0.0337851 , 0.03327835, 0.02151847,\n",
       "        0.02326989, 0.02352023, 0.02126861, 0.02627194, 0.02076769,\n",
       "        0.02051783, 0.02226889, 0.02201915, 0.02076721, 0.02126825,\n",
       "        0.02251935, 0.02251816, 0.02702379, 0.03027594, 0.02277029,\n",
       "        0.0255214 , 0.02001786, 0.02151823, 0.02802515, 0.02226961,\n",
       "        0.02301991, 0.02201915, 0.02452111, 0.02176869, 0.02226889,\n",
       "        0.02502191, 0.02452123, 0.02251935, 0.02251923, 0.02051711,\n",
       "        0.02026701, 0.02151823, 0.02477217, 0.02477026, 0.02327096,\n",
       "        0.02552235, 0.02201879, 0.02452111, 0.02552187, 0.02276957,\n",
       "        0.03277814, 0.02126801, 0.03452945, 0.02226901, 0.02301931,\n",
       "        0.03553069, 0.0332793 , 0.0215193 , 0.02377033, 0.02527201,\n",
       "        0.02301896, 0.02276957, 0.02076745, 0.02051783, 0.0250212 ,\n",
       "        0.02151847, 0.02101839, 0.02201867, 0.02176893, 0.02251935,\n",
       "        0.02327013, 0.02201867, 0.02276981, 0.02151847, 0.02076805,\n",
       "        0.02026689, 0.02201855, 0.02051771, 0.02201927, 0.02126849,\n",
       "        0.02326941, 0.02076805, 0.02226865, 0.0225184 , 0.02226973,\n",
       "        0.02427089, 0.02051759, 0.02101767, 0.02076721, 0.02151895,\n",
       "        0.02176869, 0.02101827, 0.02176881, 0.02126789, 0.02176845,\n",
       "        0.02201879, 0.02327037, 0.02201903, 0.02026737, 0.02001703,\n",
       "        0.02126718, 0.02402103, 0.02126884, 0.02076769, 0.02201855,\n",
       "        0.02051747, 0.02176869, 0.02226889, 0.02301991, 0.02226889,\n",
       "        0.02001727, 0.02151895, 0.02251971, 0.02301943, 0.02277005,\n",
       "        0.0257715 , 0.02301955, 0.02226925, 0.02201867, 0.02402103,\n",
       "        0.02226901, 0.02226937, 0.02176845, 0.02101839, 0.01976621,\n",
       "        0.02502191, 0.02126837, 0.02852488, 0.02402055, 0.02652252,\n",
       "        0.02552271, 0.0297749 , 0.02302027, 0.02427125, 0.02602255,\n",
       "        0.02026761, 0.02101803, 0.0307765 , 0.02702391, 0.02852476,\n",
       "        0.02301979, 0.02465653, 0.02627265, 0.02777302, 0.02226901,\n",
       "        0.02226901, 0.01951623, 0.02151787, 0.02065492, 0.02076793,\n",
       "        0.02151918, 0.02051806, 0.01525939, 0.01476324, 0.01351178,\n",
       "        0.0135119 , 0.01326096, 0.01301146]),\n",
       " 'std_score_time': array([2.02655792e-06, 2.00176239e-03, 2.38418579e-07, 1.25110149e-03,\n",
       "        1.50227547e-03, 1.75261497e-03, 1.00064278e-03, 4.25243378e-03,\n",
       "        1.25133991e-03, 1.50203705e-03, 5.00559807e-04, 2.50220299e-04,\n",
       "        5.00440598e-04, 2.47955322e-04, 7.51256943e-04, 1.00040436e-03,\n",
       "        7.15255737e-07, 1.00123882e-03, 2.38418579e-07, 2.38418579e-07,\n",
       "        5.50520420e-03, 1.25110149e-03, 1.75178051e-03, 1.00040436e-03,\n",
       "        4.50265408e-03, 1.00100040e-03, 2.25138664e-03, 6.50870800e-03,\n",
       "        1.24847889e-03, 1.75142288e-03, 1.50120258e-03, 2.50697136e-04,\n",
       "        2.49266624e-04, 2.25186348e-03, 7.51614571e-04, 1.75273418e-03,\n",
       "        7.54833221e-04, 1.25086308e-03, 2.49743462e-04, 2.50458717e-04,\n",
       "        2.50935555e-04, 5.01155853e-04, 1.00076199e-03, 2.50279903e-03,\n",
       "        5.96046448e-07, 7.48634338e-04, 2.00104713e-03, 2.25138664e-03,\n",
       "        5.00798225e-04, 2.49624252e-04, 1.19209290e-07, 7.51972198e-04,\n",
       "        1.75166130e-03, 1.50191784e-03, 3.21865082e-06, 4.99248505e-04,\n",
       "        4.99367714e-04, 2.25043297e-03, 1.50191784e-03, 2.50697136e-04,\n",
       "        1.00052357e-03, 1.66893005e-06, 2.49028206e-04, 5.00917435e-04,\n",
       "        5.00440598e-04, 2.50208378e-03, 7.51614571e-04, 1.25205517e-03,\n",
       "        1.25145912e-03, 5.01155853e-04, 1.75273418e-03, 5.00380993e-03,\n",
       "        1.25098228e-03, 2.50339508e-04, 2.50458717e-04, 1.66893005e-06,\n",
       "        5.00333309e-03, 5.02347946e-04, 5.96046448e-07, 2.38418579e-07,\n",
       "        1.25086308e-03, 1.07288361e-06, 5.00917435e-04, 4.25434113e-03,\n",
       "        6.00540638e-03, 5.01990318e-04, 2.51531601e-04, 8.25798512e-03,\n",
       "        2.00235844e-03, 3.50117683e-03, 8.75735283e-03, 1.00183487e-03,\n",
       "        5.00321388e-04, 2.48908997e-04, 7.51256943e-04, 2.50101089e-04,\n",
       "        2.50935555e-04, 1.50096416e-03, 1.00195408e-03, 1.50072575e-03,\n",
       "        2.50697136e-04, 7.50422478e-04, 2.25186348e-03, 2.49505043e-04,\n",
       "        1.19209290e-07, 1.25181675e-03, 1.19209290e-07, 3.94582748e-05,\n",
       "        4.76837158e-07, 2.38418579e-07, 1.75118446e-03, 1.00100040e-03,\n",
       "        2.49981880e-04, 4.50372696e-03, 2.49981880e-04, 1.00040436e-03,\n",
       "        7.50303268e-04, 5.00321388e-04, 1.50060654e-03, 7.15255737e-07,\n",
       "        3.75354290e-03, 2.00295448e-03, 3.25298309e-03, 2.50577927e-04,\n",
       "        5.00679016e-04, 1.07598305e-02, 4.50456142e-03, 1.50120258e-03,\n",
       "        1.22596025e-02, 3.50189209e-03, 2.49862671e-04, 1.17602348e-02,\n",
       "        2.75290012e-03, 2.50220299e-04, 1.50179863e-03, 0.00000000e+00,\n",
       "        8.00597668e-03, 1.12659931e-02, 8.25750828e-03, 5.00202179e-04,\n",
       "        1.75189972e-03, 5.00202179e-04, 2.50101089e-04, 2.25317478e-03,\n",
       "        2.50101089e-04, 5.00440598e-04, 1.25157833e-03, 1.50179863e-03,\n",
       "        2.50339508e-04, 2.49981880e-04, 1.50132179e-03, 5.00679016e-04,\n",
       "        1.50084496e-03, 2.75218487e-03, 7.50422478e-04, 4.99844551e-04,\n",
       "        5.00917435e-04, 1.50108337e-03, 5.50580025e-03, 2.50697136e-04,\n",
       "        3.57627869e-07, 0.00000000e+00, 2.50196457e-03, 2.50220299e-04,\n",
       "        2.49743462e-04, 3.00252438e-03, 2.50208378e-03, 5.00440598e-04,\n",
       "        2.00235844e-03, 5.00202179e-04, 7.50303268e-04, 0.00000000e+00,\n",
       "        7.50541687e-04, 1.25026703e-03, 1.75035000e-03, 4.00340557e-03,\n",
       "        5.00559807e-04, 1.50203705e-03, 1.00076199e-03, 7.51137733e-04,\n",
       "        1.17601156e-02, 2.50697136e-04, 1.40116215e-02, 7.50780106e-04,\n",
       "        1.00016594e-03, 1.00091696e-02, 1.12601519e-02, 1.00171566e-03,\n",
       "        1.25193596e-03, 2.25245953e-03, 1.00076199e-03, 7.49945641e-04,\n",
       "        1.25050545e-03, 0.00000000e+00, 5.00380993e-03, 5.00917435e-04,\n",
       "        5.00798225e-04, 1.50132179e-03, 2.51173973e-04, 5.00917435e-04,\n",
       "        2.75206566e-03, 4.76837158e-07, 7.51376152e-04, 2.38418579e-07,\n",
       "        2.50220299e-04, 2.50220299e-04, 2.00092793e-03, 5.00321388e-04,\n",
       "        5.01036644e-04, 7.50899315e-04, 2.50101089e-04, 2.49981880e-04,\n",
       "        1.75178051e-03, 5.01394272e-04, 2.50101089e-04, 2.25198269e-03,\n",
       "        5.01155853e-04, 1.00028515e-03, 2.50339508e-04, 4.76837158e-07,\n",
       "        1.25157833e-03, 2.38418579e-07, 7.49826431e-04, 2.50101089e-04,\n",
       "        2.50458717e-04, 1.19209290e-07, 7.50780106e-04, 1.19209290e-07,\n",
       "        7.51137733e-04, 3.57627869e-07, 7.49588013e-04, 2.50232220e-03,\n",
       "        1.75166130e-03, 2.50577927e-04, 1.00052357e-03, 1.19209290e-07,\n",
       "        2.50458717e-04, 1.25110149e-03, 5.00082970e-04, 7.50184059e-04,\n",
       "        3.57627869e-07, 1.50156021e-03, 1.19209290e-07, 5.00559807e-04,\n",
       "        7.50899315e-04, 2.75194645e-03, 1.50156021e-03, 2.50101089e-04,\n",
       "        1.00088120e-03, 5.00559807e-04, 7.50780106e-04, 2.50458717e-04,\n",
       "        1.75130367e-03, 1.00100040e-03, 7.50184059e-04, 2.00188160e-03,\n",
       "        2.50101089e-04, 6.50572777e-03, 2.00092793e-03, 4.99367714e-04,\n",
       "        2.00152397e-03, 2.25162506e-03, 1.00040436e-03, 1.25145912e-03,\n",
       "        5.50377369e-03, 2.49981880e-04, 5.01394272e-04, 2.25234032e-03,\n",
       "        2.50208378e-03, 2.00092793e-03, 5.00440598e-04, 2.36654282e-03,\n",
       "        2.75337696e-03, 4.25422192e-03, 2.50339508e-04, 7.51256943e-04,\n",
       "        0.00000000e+00, 1.50167942e-03, 6.37769699e-04, 2.49862671e-04,\n",
       "        5.00440598e-04, 9.99689102e-04, 1.24824047e-03, 7.50422478e-04,\n",
       "        3.57627869e-07, 0.00000000e+00, 2.50458717e-04, 2.38418579e-07]),\n",
       " 'param_classifier': masked_array(data=[MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__activation': masked_array(data=['logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__alpha': masked_array(data=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__hidden_layer_sizes': masked_array(data=[(5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__learning_rate': masked_array(data=['constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'}],\n",
       " 'split0_test_recall_micro': array([0.9952, 0.9952, 0.9948, 0.9956, 0.9948, 0.994 , 0.9952, 0.9952,\n",
       "        0.9948, 0.9924, 0.992 , 0.9948, 0.996 , 0.996 , 0.9916, 0.9952,\n",
       "        0.9948, 0.9964, 0.992 , 0.9944, 0.9956, 0.9924, 0.9948, 0.9956,\n",
       "        0.994 , 0.9932, 0.994 , 0.9912, 0.9908, 0.9936, 0.9932, 0.9916,\n",
       "        0.9924, 0.9908, 0.9908, 0.9924, 0.9924, 0.992 , 0.9928, 0.9928,\n",
       "        0.9932, 0.992 , 0.9924, 0.9924, 0.9936, 0.9932, 0.9928, 0.9928,\n",
       "        0.9948, 0.9964, 0.9948, 0.9952, 0.9952, 0.994 , 0.9952, 0.9936,\n",
       "        0.9944, 0.994 , 0.9928, 0.9948, 0.9948, 0.9952, 0.9944, 0.9936,\n",
       "        0.9936, 0.9952, 0.9944, 0.994 , 0.9932, 0.9948, 0.9924, 0.9948,\n",
       "        0.9948, 0.994 , 0.994 , 0.9904, 0.9928, 0.9952, 0.9952, 0.9904,\n",
       "        0.9904, 0.9908, 0.9892, 0.9916, 0.9932, 0.9924, 0.9916, 0.9932,\n",
       "        0.9928, 0.9928, 0.992 , 0.992 , 0.9916, 0.992 , 0.992 , 0.9912,\n",
       "        0.9476, 0.528 , 0.9672, 0.962 , 0.2264, 0.9552, 0.9424, 0.574 ,\n",
       "        0.936 , 0.9392, 0.574 , 0.9408, 0.9584, 0.198 , 0.9444, 0.94  ,\n",
       "        0.574 , 0.9432, 0.944 , 0.574 , 0.9456, 0.9488, 0.574 , 0.936 ,\n",
       "        0.964 , 0.0232, 0.772 , 0.9472, 0.2272, 0.9476, 0.9468, 0.602 ,\n",
       "        0.9384, 0.9432, 0.5688, 0.9396, 0.9508, 0.198 , 0.9448, 0.9488,\n",
       "        0.198 , 0.9576, 0.9444, 0.2268, 0.9384, 0.9516, 0.23  , 0.944 ,\n",
       "        0.9476, 0.4488, 0.9452, 0.9496, 0.4888, 0.95  , 0.9504, 0.6132,\n",
       "        0.9512, 0.9476, 0.5604, 0.9504, 0.9456, 0.5056, 0.9472, 0.9488,\n",
       "        0.2608, 0.9468, 0.9496, 0.3668, 0.9472, 0.9488, 0.5236, 0.9472,\n",
       "        0.9472, 0.0664, 0.95  , 0.9508, 0.2948, 0.9484, 0.9476, 0.46  ,\n",
       "        0.9464, 0.9476, 0.2284, 0.948 , 0.9484, 0.3036, 0.9504, 0.9504,\n",
       "        0.1164, 0.9464, 0.9464, 0.4788, 0.95  , 0.9468, 0.5856, 0.9476,\n",
       "        0.9552, 0.9584, 0.9564, 0.9608, 0.9608, 0.9604, 0.9624, 0.964 ,\n",
       "        0.9616, 0.9624, 0.9644, 0.9664, 0.956 , 0.9536, 0.9564, 0.9616,\n",
       "        0.9604, 0.96  , 0.962 , 0.9624, 0.9624, 0.9668, 0.966 , 0.9656,\n",
       "        0.9524, 0.9552, 0.9576, 0.9616, 0.96  , 0.9616, 0.966 , 0.96  ,\n",
       "        0.9608, 0.9652, 0.9668, 0.9668, 0.95  , 0.9524, 0.952 , 0.958 ,\n",
       "        0.9536, 0.9588, 0.9592, 0.9576, 0.9592, 0.9588, 0.9588, 0.9588,\n",
       "        0.9676, 0.9816, 0.9848, 0.9864, 0.986 , 0.9856, 0.9888, 0.9892,\n",
       "        0.9884, 0.9888, 0.988 , 0.9896, 0.9808, 0.9708, 0.9832, 0.9848,\n",
       "        0.988 , 0.986 , 0.9888, 0.9896, 0.9876, 0.9888, 0.99  , 0.9888,\n",
       "        0.9704, 0.9788, 0.968 , 0.986 , 0.9868, 0.9836, 0.9868, 0.9884,\n",
       "        0.9884, 0.9888, 0.9896, 0.9884, 0.9676, 0.9648, 0.9824, 0.9844,\n",
       "        0.9832, 0.9848, 0.9852, 0.9852, 0.9852, 0.9868, 0.9868, 0.9864]),\n",
       " 'split1_test_recall_micro': array([0.9928, 0.9916, 0.9928, 0.9932, 0.9916, 0.9936, 0.9928, 0.9932,\n",
       "        0.9932, 0.9924, 0.9932, 0.9924, 0.9936, 0.9912, 0.9948, 0.994 ,\n",
       "        0.992 , 0.9932, 0.9912, 0.9928, 0.9932, 0.9912, 0.9916, 0.9916,\n",
       "        0.9904, 0.99  , 0.9904, 0.99  , 0.9908, 0.9912, 0.9904, 0.9904,\n",
       "        0.9908, 0.9904, 0.9904, 0.99  , 0.9896, 0.9888, 0.9888, 0.9888,\n",
       "        0.9888, 0.9888, 0.9888, 0.9888, 0.9888, 0.9888, 0.9888, 0.9888,\n",
       "        0.9936, 0.9916, 0.9948, 0.9924, 0.9924, 0.9928, 0.994 , 0.9912,\n",
       "        0.9944, 0.9928, 0.9936, 0.9932, 0.9908, 0.9916, 0.9896, 0.9896,\n",
       "        0.9928, 0.9912, 0.9888, 0.9908, 0.99  , 0.992 , 0.992 , 0.99  ,\n",
       "        0.9896, 0.9904, 0.9916, 0.9904, 0.99  , 0.99  , 0.9896, 0.9908,\n",
       "        0.9912, 0.9896, 0.9888, 0.99  , 0.9896, 0.9896, 0.99  , 0.9896,\n",
       "        0.9896, 0.9896, 0.9896, 0.9896, 0.9896, 0.9896, 0.9896, 0.9896,\n",
       "        0.7676, 0.0436, 0.77  , 0.944 , 0.5656, 0.96  , 0.944 , 0.2268,\n",
       "        0.9436, 0.936 , 0.574 , 0.9368, 0.9672, 0.0708, 0.948 , 0.96  ,\n",
       "        0.24  , 0.9396, 0.9552, 0.5676, 0.9468, 0.9432, 0.574 , 0.9344,\n",
       "        0.954 , 0.5204, 0.9596, 0.9524, 0.5732, 0.9424, 0.9424, 0.5496,\n",
       "        0.9432, 0.9364, 0.3476, 0.9384, 0.964 , 0.258 , 0.96  , 0.9532,\n",
       "        0.574 , 0.9592, 0.954 , 0.5648, 0.9344, 0.9376, 0.204 , 0.942 ,\n",
       "        0.9472, 0.2108, 0.9448, 0.9404, 0.3516, 0.9436, 0.9432, 0.4924,\n",
       "        0.9424, 0.9428, 0.6872, 0.938 , 0.9444, 0.2632, 0.944 , 0.9448,\n",
       "        0.2252, 0.9424, 0.9412, 0.4784, 0.9408, 0.942 , 0.5648, 0.9432,\n",
       "        0.9388, 0.376 , 0.938 , 0.9408, 0.5976, 0.9448, 0.94  , 0.5644,\n",
       "        0.938 , 0.9404, 0.3988, 0.9416, 0.944 , 0.236 , 0.9424, 0.9424,\n",
       "        0.6352, 0.9404, 0.942 , 0.3164, 0.9428, 0.9416, 0.4608, 0.9412,\n",
       "        0.9504, 0.9468, 0.9492, 0.952 , 0.9532, 0.9516, 0.9548, 0.954 ,\n",
       "        0.9552, 0.956 , 0.958 , 0.956 , 0.9496, 0.9464, 0.95  , 0.952 ,\n",
       "        0.9508, 0.9524, 0.9552, 0.954 , 0.9548, 0.9568, 0.9576, 0.9564,\n",
       "        0.9496, 0.9504, 0.9496, 0.9528, 0.9508, 0.9528, 0.9548, 0.954 ,\n",
       "        0.9544, 0.958 , 0.9552, 0.9556, 0.9436, 0.9484, 0.948 , 0.95  ,\n",
       "        0.9492, 0.9496, 0.9516, 0.9504, 0.95  , 0.952 , 0.9516, 0.9532,\n",
       "        0.9624, 0.9756, 0.96  , 0.9812, 0.9776, 0.9624, 0.9812, 0.9824,\n",
       "        0.9808, 0.9816, 0.982 , 0.9848, 0.9592, 0.9628, 0.9728, 0.9784,\n",
       "        0.9796, 0.9784, 0.9776, 0.982 , 0.9828, 0.984 , 0.9824, 0.9804,\n",
       "        0.9604, 0.9604, 0.962 , 0.9812, 0.9832, 0.9628, 0.9796, 0.9792,\n",
       "        0.9812, 0.98  , 0.9872, 0.9844, 0.9728, 0.9592, 0.9596, 0.976 ,\n",
       "        0.9772, 0.9624, 0.978 , 0.9772, 0.98  , 0.9796, 0.9784, 0.9804]),\n",
       " 'mean_test_recall_micro': array([0.994 , 0.9934, 0.9938, 0.9944, 0.9932, 0.9938, 0.994 , 0.9942,\n",
       "        0.994 , 0.9924, 0.9926, 0.9936, 0.9948, 0.9936, 0.9932, 0.9946,\n",
       "        0.9934, 0.9948, 0.9916, 0.9936, 0.9944, 0.9918, 0.9932, 0.9936,\n",
       "        0.9922, 0.9916, 0.9922, 0.9906, 0.9908, 0.9924, 0.9918, 0.991 ,\n",
       "        0.9916, 0.9906, 0.9906, 0.9912, 0.991 , 0.9904, 0.9908, 0.9908,\n",
       "        0.991 , 0.9904, 0.9906, 0.9906, 0.9912, 0.991 , 0.9908, 0.9908,\n",
       "        0.9942, 0.994 , 0.9948, 0.9938, 0.9938, 0.9934, 0.9946, 0.9924,\n",
       "        0.9944, 0.9934, 0.9932, 0.994 , 0.9928, 0.9934, 0.992 , 0.9916,\n",
       "        0.9932, 0.9932, 0.9916, 0.9924, 0.9916, 0.9934, 0.9922, 0.9924,\n",
       "        0.9922, 0.9922, 0.9928, 0.9904, 0.9914, 0.9926, 0.9924, 0.9906,\n",
       "        0.9908, 0.9902, 0.989 , 0.9908, 0.9914, 0.991 , 0.9908, 0.9914,\n",
       "        0.9912, 0.9912, 0.9908, 0.9908, 0.9906, 0.9908, 0.9908, 0.9904,\n",
       "        0.8576, 0.2858, 0.8686, 0.953 , 0.396 , 0.9576, 0.9432, 0.4004,\n",
       "        0.9398, 0.9376, 0.574 , 0.9388, 0.9628, 0.1344, 0.9462, 0.95  ,\n",
       "        0.407 , 0.9414, 0.9496, 0.5708, 0.9462, 0.946 , 0.574 , 0.9352,\n",
       "        0.959 , 0.2718, 0.8658, 0.9498, 0.4002, 0.945 , 0.9446, 0.5758,\n",
       "        0.9408, 0.9398, 0.4582, 0.939 , 0.9574, 0.228 , 0.9524, 0.951 ,\n",
       "        0.386 , 0.9584, 0.9492, 0.3958, 0.9364, 0.9446, 0.217 , 0.943 ,\n",
       "        0.9474, 0.3298, 0.945 , 0.945 , 0.4202, 0.9468, 0.9468, 0.5528,\n",
       "        0.9468, 0.9452, 0.6238, 0.9442, 0.945 , 0.3844, 0.9456, 0.9468,\n",
       "        0.243 , 0.9446, 0.9454, 0.4226, 0.944 , 0.9454, 0.5442, 0.9452,\n",
       "        0.943 , 0.2212, 0.944 , 0.9458, 0.4462, 0.9466, 0.9438, 0.5122,\n",
       "        0.9422, 0.944 , 0.3136, 0.9448, 0.9462, 0.2698, 0.9464, 0.9464,\n",
       "        0.3758, 0.9434, 0.9442, 0.3976, 0.9464, 0.9442, 0.5232, 0.9444,\n",
       "        0.9528, 0.9526, 0.9528, 0.9564, 0.957 , 0.956 , 0.9586, 0.959 ,\n",
       "        0.9584, 0.9592, 0.9612, 0.9612, 0.9528, 0.95  , 0.9532, 0.9568,\n",
       "        0.9556, 0.9562, 0.9586, 0.9582, 0.9586, 0.9618, 0.9618, 0.961 ,\n",
       "        0.951 , 0.9528, 0.9536, 0.9572, 0.9554, 0.9572, 0.9604, 0.957 ,\n",
       "        0.9576, 0.9616, 0.961 , 0.9612, 0.9468, 0.9504, 0.95  , 0.954 ,\n",
       "        0.9514, 0.9542, 0.9554, 0.954 , 0.9546, 0.9554, 0.9552, 0.956 ,\n",
       "        0.965 , 0.9786, 0.9724, 0.9838, 0.9818, 0.974 , 0.985 , 0.9858,\n",
       "        0.9846, 0.9852, 0.985 , 0.9872, 0.97  , 0.9668, 0.978 , 0.9816,\n",
       "        0.9838, 0.9822, 0.9832, 0.9858, 0.9852, 0.9864, 0.9862, 0.9846,\n",
       "        0.9654, 0.9696, 0.965 , 0.9836, 0.985 , 0.9732, 0.9832, 0.9838,\n",
       "        0.9848, 0.9844, 0.9884, 0.9864, 0.9702, 0.962 , 0.971 , 0.9802,\n",
       "        0.9802, 0.9736, 0.9816, 0.9812, 0.9826, 0.9832, 0.9826, 0.9834]),\n",
       " 'std_test_recall_micro': array([1.200e-03, 1.800e-03, 1.000e-03, 1.200e-03, 1.600e-03, 2.000e-04,\n",
       "        1.200e-03, 1.000e-03, 8.000e-04, 0.000e+00, 6.000e-04, 1.200e-03,\n",
       "        1.200e-03, 2.400e-03, 1.600e-03, 6.000e-04, 1.400e-03, 1.600e-03,\n",
       "        4.000e-04, 8.000e-04, 1.200e-03, 6.000e-04, 1.600e-03, 2.000e-03,\n",
       "        1.800e-03, 1.600e-03, 1.800e-03, 6.000e-04, 0.000e+00, 1.200e-03,\n",
       "        1.400e-03, 6.000e-04, 8.000e-04, 2.000e-04, 2.000e-04, 1.200e-03,\n",
       "        1.400e-03, 1.600e-03, 2.000e-03, 2.000e-03, 2.200e-03, 1.600e-03,\n",
       "        1.800e-03, 1.800e-03, 2.400e-03, 2.200e-03, 2.000e-03, 2.000e-03,\n",
       "        6.000e-04, 2.400e-03, 0.000e+00, 1.400e-03, 1.400e-03, 6.000e-04,\n",
       "        6.000e-04, 1.200e-03, 0.000e+00, 6.000e-04, 4.000e-04, 8.000e-04,\n",
       "        2.000e-03, 1.800e-03, 2.400e-03, 2.000e-03, 4.000e-04, 2.000e-03,\n",
       "        2.800e-03, 1.600e-03, 1.600e-03, 1.400e-03, 2.000e-04, 2.400e-03,\n",
       "        2.600e-03, 1.800e-03, 1.200e-03, 0.000e+00, 1.400e-03, 2.600e-03,\n",
       "        2.800e-03, 2.000e-04, 4.000e-04, 6.000e-04, 2.000e-04, 8.000e-04,\n",
       "        1.800e-03, 1.400e-03, 8.000e-04, 1.800e-03, 1.600e-03, 1.600e-03,\n",
       "        1.200e-03, 1.200e-03, 1.000e-03, 1.200e-03, 1.200e-03, 8.000e-04,\n",
       "        9.000e-02, 2.422e-01, 9.860e-02, 9.000e-03, 1.696e-01, 2.400e-03,\n",
       "        8.000e-04, 1.736e-01, 3.800e-03, 1.600e-03, 0.000e+00, 2.000e-03,\n",
       "        4.400e-03, 6.360e-02, 1.800e-03, 1.000e-02, 1.670e-01, 1.800e-03,\n",
       "        5.600e-03, 3.200e-03, 6.000e-04, 2.800e-03, 0.000e+00, 8.000e-04,\n",
       "        5.000e-03, 2.486e-01, 9.380e-02, 2.600e-03, 1.730e-01, 2.600e-03,\n",
       "        2.200e-03, 2.620e-02, 2.400e-03, 3.400e-03, 1.106e-01, 6.000e-04,\n",
       "        6.600e-03, 3.000e-02, 7.600e-03, 2.200e-03, 1.880e-01, 8.000e-04,\n",
       "        4.800e-03, 1.690e-01, 2.000e-03, 7.000e-03, 1.300e-02, 1.000e-03,\n",
       "        2.000e-04, 1.190e-01, 2.000e-04, 4.600e-03, 6.860e-02, 3.200e-03,\n",
       "        3.600e-03, 6.040e-02, 4.400e-03, 2.400e-03, 6.340e-02, 6.200e-03,\n",
       "        6.000e-04, 1.212e-01, 1.600e-03, 2.000e-03, 1.780e-02, 2.200e-03,\n",
       "        4.200e-03, 5.580e-02, 3.200e-03, 3.400e-03, 2.060e-02, 2.000e-03,\n",
       "        4.200e-03, 1.548e-01, 6.000e-03, 5.000e-03, 1.514e-01, 1.800e-03,\n",
       "        3.800e-03, 5.220e-02, 4.200e-03, 3.600e-03, 8.520e-02, 3.200e-03,\n",
       "        2.200e-03, 3.380e-02, 4.000e-03, 4.000e-03, 2.594e-01, 3.000e-03,\n",
       "        2.200e-03, 8.120e-02, 3.600e-03, 2.600e-03, 6.240e-02, 3.200e-03,\n",
       "        2.400e-03, 5.800e-03, 3.600e-03, 4.400e-03, 3.800e-03, 4.400e-03,\n",
       "        3.800e-03, 5.000e-03, 3.200e-03, 3.200e-03, 3.200e-03, 5.200e-03,\n",
       "        3.200e-03, 3.600e-03, 3.200e-03, 4.800e-03, 4.800e-03, 3.800e-03,\n",
       "        3.400e-03, 4.200e-03, 3.800e-03, 5.000e-03, 4.200e-03, 4.600e-03,\n",
       "        1.400e-03, 2.400e-03, 4.000e-03, 4.400e-03, 4.600e-03, 4.400e-03,\n",
       "        5.600e-03, 3.000e-03, 3.200e-03, 3.600e-03, 5.800e-03, 5.600e-03,\n",
       "        3.200e-03, 2.000e-03, 2.000e-03, 4.000e-03, 2.200e-03, 4.600e-03,\n",
       "        3.800e-03, 3.600e-03, 4.600e-03, 3.400e-03, 3.600e-03, 2.800e-03,\n",
       "        2.600e-03, 3.000e-03, 1.240e-02, 2.600e-03, 4.200e-03, 1.160e-02,\n",
       "        3.800e-03, 3.400e-03, 3.800e-03, 3.600e-03, 3.000e-03, 2.400e-03,\n",
       "        1.080e-02, 4.000e-03, 5.200e-03, 3.200e-03, 4.200e-03, 3.800e-03,\n",
       "        5.600e-03, 3.800e-03, 2.400e-03, 2.400e-03, 3.800e-03, 4.200e-03,\n",
       "        5.000e-03, 9.200e-03, 3.000e-03, 2.400e-03, 1.800e-03, 1.040e-02,\n",
       "        3.600e-03, 4.600e-03, 3.600e-03, 4.400e-03, 1.200e-03, 2.000e-03,\n",
       "        2.600e-03, 2.800e-03, 1.140e-02, 4.200e-03, 3.000e-03, 1.120e-02,\n",
       "        3.600e-03, 4.000e-03, 2.600e-03, 3.600e-03, 4.200e-03, 3.000e-03]),\n",
       " 'rank_test_recall_micro': array([ 11,  24,  16,   6,  30,  16,  11,   9,  11,  40,  38,  20,   1,\n",
       "         20,  30,   4,  24,   3,  54,  20,   6,  52,  30,  20,  46,  54,\n",
       "         46,  85,  72,  40,  52,  67,  54,  85,  85,  66,  67,  91,  72,\n",
       "         72,  67,  91,  85,  85,  63,  67,  72,  72,   9,  11,   1,  16,\n",
       "         16,  24,   4,  40,   6,  24,  30,  11,  36,  24,  51,  54,  30,\n",
       "         35,  54,  40,  54,  24,  46,  40,  46,  46,  36,  91,  60,  38,\n",
       "         40,  85,  83,  95,  96,  72,  60,  67,  72,  60,  63,  63,  72,\n",
       "         72,  84,  72,  72,  91, 256, 281, 254, 187, 274, 164, 241, 271,\n",
       "        247, 251, 259, 250, 144, 288, 214, 198, 270, 245, 202, 261, 214,\n",
       "        217, 259, 253, 156, 282, 255, 201, 272, 224, 229, 258, 246, 247,\n",
       "        266, 249, 166, 285, 193, 195, 276, 161, 203, 275, 252, 229, 287,\n",
       "        243, 204, 279, 224, 224, 269, 207, 205, 262, 205, 222, 257, 233,\n",
       "        224, 277, 219, 207, 284, 229, 220, 268, 236, 220, 263, 222, 242,\n",
       "        286, 236, 218, 267, 210, 239, 265, 244, 236, 280, 228, 214, 283,\n",
       "        211, 211, 278, 240, 233, 273, 213, 233, 264, 232, 188, 192, 188,\n",
       "        172, 169, 174, 158, 156, 161, 155, 149, 149, 191, 198, 186, 171,\n",
       "        176, 173, 158, 163, 158, 146, 146, 152, 195, 188, 185, 167, 177,\n",
       "        167, 154, 170, 164, 148, 152, 149, 207, 197, 198, 183, 194, 182,\n",
       "        177, 183, 181, 177, 180, 174, 142, 130, 135, 113, 124, 132, 106,\n",
       "        102, 111, 104, 106,  98, 138, 140, 131, 125, 113, 123, 118, 102,\n",
       "        104,  99, 101, 110, 141, 139, 143, 116, 106, 134, 118, 113, 109,\n",
       "        112,  97,  99, 137, 145, 136, 128, 128, 133, 125, 127, 122, 118,\n",
       "        121, 117]),\n",
       " 'split0_test_f1_micro': array([0.9952, 0.9952, 0.9948, 0.9956, 0.9948, 0.994 , 0.9952, 0.9952,\n",
       "        0.9948, 0.9924, 0.992 , 0.9948, 0.996 , 0.996 , 0.9916, 0.9952,\n",
       "        0.9948, 0.9964, 0.992 , 0.9944, 0.9956, 0.9924, 0.9948, 0.9956,\n",
       "        0.994 , 0.9932, 0.994 , 0.9912, 0.9908, 0.9936, 0.9932, 0.9916,\n",
       "        0.9924, 0.9908, 0.9908, 0.9924, 0.9924, 0.992 , 0.9928, 0.9928,\n",
       "        0.9932, 0.992 , 0.9924, 0.9924, 0.9936, 0.9932, 0.9928, 0.9928,\n",
       "        0.9948, 0.9964, 0.9948, 0.9952, 0.9952, 0.994 , 0.9952, 0.9936,\n",
       "        0.9944, 0.994 , 0.9928, 0.9948, 0.9948, 0.9952, 0.9944, 0.9936,\n",
       "        0.9936, 0.9952, 0.9944, 0.994 , 0.9932, 0.9948, 0.9924, 0.9948,\n",
       "        0.9948, 0.994 , 0.994 , 0.9904, 0.9928, 0.9952, 0.9952, 0.9904,\n",
       "        0.9904, 0.9908, 0.9892, 0.9916, 0.9932, 0.9924, 0.9916, 0.9932,\n",
       "        0.9928, 0.9928, 0.992 , 0.992 , 0.9916, 0.992 , 0.992 , 0.9912,\n",
       "        0.9476, 0.528 , 0.9672, 0.962 , 0.2264, 0.9552, 0.9424, 0.574 ,\n",
       "        0.936 , 0.9392, 0.574 , 0.9408, 0.9584, 0.198 , 0.9444, 0.94  ,\n",
       "        0.574 , 0.9432, 0.944 , 0.574 , 0.9456, 0.9488, 0.574 , 0.936 ,\n",
       "        0.964 , 0.0232, 0.772 , 0.9472, 0.2272, 0.9476, 0.9468, 0.602 ,\n",
       "        0.9384, 0.9432, 0.5688, 0.9396, 0.9508, 0.198 , 0.9448, 0.9488,\n",
       "        0.198 , 0.9576, 0.9444, 0.2268, 0.9384, 0.9516, 0.23  , 0.944 ,\n",
       "        0.9476, 0.4488, 0.9452, 0.9496, 0.4888, 0.95  , 0.9504, 0.6132,\n",
       "        0.9512, 0.9476, 0.5604, 0.9504, 0.9456, 0.5056, 0.9472, 0.9488,\n",
       "        0.2608, 0.9468, 0.9496, 0.3668, 0.9472, 0.9488, 0.5236, 0.9472,\n",
       "        0.9472, 0.0664, 0.95  , 0.9508, 0.2948, 0.9484, 0.9476, 0.46  ,\n",
       "        0.9464, 0.9476, 0.2284, 0.948 , 0.9484, 0.3036, 0.9504, 0.9504,\n",
       "        0.1164, 0.9464, 0.9464, 0.4788, 0.95  , 0.9468, 0.5856, 0.9476,\n",
       "        0.9552, 0.9584, 0.9564, 0.9608, 0.9608, 0.9604, 0.9624, 0.964 ,\n",
       "        0.9616, 0.9624, 0.9644, 0.9664, 0.956 , 0.9536, 0.9564, 0.9616,\n",
       "        0.9604, 0.96  , 0.962 , 0.9624, 0.9624, 0.9668, 0.966 , 0.9656,\n",
       "        0.9524, 0.9552, 0.9576, 0.9616, 0.96  , 0.9616, 0.966 , 0.96  ,\n",
       "        0.9608, 0.9652, 0.9668, 0.9668, 0.95  , 0.9524, 0.952 , 0.958 ,\n",
       "        0.9536, 0.9588, 0.9592, 0.9576, 0.9592, 0.9588, 0.9588, 0.9588,\n",
       "        0.9676, 0.9816, 0.9848, 0.9864, 0.986 , 0.9856, 0.9888, 0.9892,\n",
       "        0.9884, 0.9888, 0.988 , 0.9896, 0.9808, 0.9708, 0.9832, 0.9848,\n",
       "        0.988 , 0.986 , 0.9888, 0.9896, 0.9876, 0.9888, 0.99  , 0.9888,\n",
       "        0.9704, 0.9788, 0.968 , 0.986 , 0.9868, 0.9836, 0.9868, 0.9884,\n",
       "        0.9884, 0.9888, 0.9896, 0.9884, 0.9676, 0.9648, 0.9824, 0.9844,\n",
       "        0.9832, 0.9848, 0.9852, 0.9852, 0.9852, 0.9868, 0.9868, 0.9864]),\n",
       " 'split1_test_f1_micro': array([0.9928, 0.9916, 0.9928, 0.9932, 0.9916, 0.9936, 0.9928, 0.9932,\n",
       "        0.9932, 0.9924, 0.9932, 0.9924, 0.9936, 0.9912, 0.9948, 0.994 ,\n",
       "        0.992 , 0.9932, 0.9912, 0.9928, 0.9932, 0.9912, 0.9916, 0.9916,\n",
       "        0.9904, 0.99  , 0.9904, 0.99  , 0.9908, 0.9912, 0.9904, 0.9904,\n",
       "        0.9908, 0.9904, 0.9904, 0.99  , 0.9896, 0.9888, 0.9888, 0.9888,\n",
       "        0.9888, 0.9888, 0.9888, 0.9888, 0.9888, 0.9888, 0.9888, 0.9888,\n",
       "        0.9936, 0.9916, 0.9948, 0.9924, 0.9924, 0.9928, 0.994 , 0.9912,\n",
       "        0.9944, 0.9928, 0.9936, 0.9932, 0.9908, 0.9916, 0.9896, 0.9896,\n",
       "        0.9928, 0.9912, 0.9888, 0.9908, 0.99  , 0.992 , 0.992 , 0.99  ,\n",
       "        0.9896, 0.9904, 0.9916, 0.9904, 0.99  , 0.99  , 0.9896, 0.9908,\n",
       "        0.9912, 0.9896, 0.9888, 0.99  , 0.9896, 0.9896, 0.99  , 0.9896,\n",
       "        0.9896, 0.9896, 0.9896, 0.9896, 0.9896, 0.9896, 0.9896, 0.9896,\n",
       "        0.7676, 0.0436, 0.77  , 0.944 , 0.5656, 0.96  , 0.944 , 0.2268,\n",
       "        0.9436, 0.936 , 0.574 , 0.9368, 0.9672, 0.0708, 0.948 , 0.96  ,\n",
       "        0.24  , 0.9396, 0.9552, 0.5676, 0.9468, 0.9432, 0.574 , 0.9344,\n",
       "        0.954 , 0.5204, 0.9596, 0.9524, 0.5732, 0.9424, 0.9424, 0.5496,\n",
       "        0.9432, 0.9364, 0.3476, 0.9384, 0.964 , 0.258 , 0.96  , 0.9532,\n",
       "        0.574 , 0.9592, 0.954 , 0.5648, 0.9344, 0.9376, 0.204 , 0.942 ,\n",
       "        0.9472, 0.2108, 0.9448, 0.9404, 0.3516, 0.9436, 0.9432, 0.4924,\n",
       "        0.9424, 0.9428, 0.6872, 0.938 , 0.9444, 0.2632, 0.944 , 0.9448,\n",
       "        0.2252, 0.9424, 0.9412, 0.4784, 0.9408, 0.942 , 0.5648, 0.9432,\n",
       "        0.9388, 0.376 , 0.938 , 0.9408, 0.5976, 0.9448, 0.94  , 0.5644,\n",
       "        0.938 , 0.9404, 0.3988, 0.9416, 0.944 , 0.236 , 0.9424, 0.9424,\n",
       "        0.6352, 0.9404, 0.942 , 0.3164, 0.9428, 0.9416, 0.4608, 0.9412,\n",
       "        0.9504, 0.9468, 0.9492, 0.952 , 0.9532, 0.9516, 0.9548, 0.954 ,\n",
       "        0.9552, 0.956 , 0.958 , 0.956 , 0.9496, 0.9464, 0.95  , 0.952 ,\n",
       "        0.9508, 0.9524, 0.9552, 0.954 , 0.9548, 0.9568, 0.9576, 0.9564,\n",
       "        0.9496, 0.9504, 0.9496, 0.9528, 0.9508, 0.9528, 0.9548, 0.954 ,\n",
       "        0.9544, 0.958 , 0.9552, 0.9556, 0.9436, 0.9484, 0.948 , 0.95  ,\n",
       "        0.9492, 0.9496, 0.9516, 0.9504, 0.95  , 0.952 , 0.9516, 0.9532,\n",
       "        0.9624, 0.9756, 0.96  , 0.9812, 0.9776, 0.9624, 0.9812, 0.9824,\n",
       "        0.9808, 0.9816, 0.982 , 0.9848, 0.9592, 0.9628, 0.9728, 0.9784,\n",
       "        0.9796, 0.9784, 0.9776, 0.982 , 0.9828, 0.984 , 0.9824, 0.9804,\n",
       "        0.9604, 0.9604, 0.962 , 0.9812, 0.9832, 0.9628, 0.9796, 0.9792,\n",
       "        0.9812, 0.98  , 0.9872, 0.9844, 0.9728, 0.9592, 0.9596, 0.976 ,\n",
       "        0.9772, 0.9624, 0.978 , 0.9772, 0.98  , 0.9796, 0.9784, 0.9804]),\n",
       " 'mean_test_f1_micro': array([0.994 , 0.9934, 0.9938, 0.9944, 0.9932, 0.9938, 0.994 , 0.9942,\n",
       "        0.994 , 0.9924, 0.9926, 0.9936, 0.9948, 0.9936, 0.9932, 0.9946,\n",
       "        0.9934, 0.9948, 0.9916, 0.9936, 0.9944, 0.9918, 0.9932, 0.9936,\n",
       "        0.9922, 0.9916, 0.9922, 0.9906, 0.9908, 0.9924, 0.9918, 0.991 ,\n",
       "        0.9916, 0.9906, 0.9906, 0.9912, 0.991 , 0.9904, 0.9908, 0.9908,\n",
       "        0.991 , 0.9904, 0.9906, 0.9906, 0.9912, 0.991 , 0.9908, 0.9908,\n",
       "        0.9942, 0.994 , 0.9948, 0.9938, 0.9938, 0.9934, 0.9946, 0.9924,\n",
       "        0.9944, 0.9934, 0.9932, 0.994 , 0.9928, 0.9934, 0.992 , 0.9916,\n",
       "        0.9932, 0.9932, 0.9916, 0.9924, 0.9916, 0.9934, 0.9922, 0.9924,\n",
       "        0.9922, 0.9922, 0.9928, 0.9904, 0.9914, 0.9926, 0.9924, 0.9906,\n",
       "        0.9908, 0.9902, 0.989 , 0.9908, 0.9914, 0.991 , 0.9908, 0.9914,\n",
       "        0.9912, 0.9912, 0.9908, 0.9908, 0.9906, 0.9908, 0.9908, 0.9904,\n",
       "        0.8576, 0.2858, 0.8686, 0.953 , 0.396 , 0.9576, 0.9432, 0.4004,\n",
       "        0.9398, 0.9376, 0.574 , 0.9388, 0.9628, 0.1344, 0.9462, 0.95  ,\n",
       "        0.407 , 0.9414, 0.9496, 0.5708, 0.9462, 0.946 , 0.574 , 0.9352,\n",
       "        0.959 , 0.2718, 0.8658, 0.9498, 0.4002, 0.945 , 0.9446, 0.5758,\n",
       "        0.9408, 0.9398, 0.4582, 0.939 , 0.9574, 0.228 , 0.9524, 0.951 ,\n",
       "        0.386 , 0.9584, 0.9492, 0.3958, 0.9364, 0.9446, 0.217 , 0.943 ,\n",
       "        0.9474, 0.3298, 0.945 , 0.945 , 0.4202, 0.9468, 0.9468, 0.5528,\n",
       "        0.9468, 0.9452, 0.6238, 0.9442, 0.945 , 0.3844, 0.9456, 0.9468,\n",
       "        0.243 , 0.9446, 0.9454, 0.4226, 0.944 , 0.9454, 0.5442, 0.9452,\n",
       "        0.943 , 0.2212, 0.944 , 0.9458, 0.4462, 0.9466, 0.9438, 0.5122,\n",
       "        0.9422, 0.944 , 0.3136, 0.9448, 0.9462, 0.2698, 0.9464, 0.9464,\n",
       "        0.3758, 0.9434, 0.9442, 0.3976, 0.9464, 0.9442, 0.5232, 0.9444,\n",
       "        0.9528, 0.9526, 0.9528, 0.9564, 0.957 , 0.956 , 0.9586, 0.959 ,\n",
       "        0.9584, 0.9592, 0.9612, 0.9612, 0.9528, 0.95  , 0.9532, 0.9568,\n",
       "        0.9556, 0.9562, 0.9586, 0.9582, 0.9586, 0.9618, 0.9618, 0.961 ,\n",
       "        0.951 , 0.9528, 0.9536, 0.9572, 0.9554, 0.9572, 0.9604, 0.957 ,\n",
       "        0.9576, 0.9616, 0.961 , 0.9612, 0.9468, 0.9504, 0.95  , 0.954 ,\n",
       "        0.9514, 0.9542, 0.9554, 0.954 , 0.9546, 0.9554, 0.9552, 0.956 ,\n",
       "        0.965 , 0.9786, 0.9724, 0.9838, 0.9818, 0.974 , 0.985 , 0.9858,\n",
       "        0.9846, 0.9852, 0.985 , 0.9872, 0.97  , 0.9668, 0.978 , 0.9816,\n",
       "        0.9838, 0.9822, 0.9832, 0.9858, 0.9852, 0.9864, 0.9862, 0.9846,\n",
       "        0.9654, 0.9696, 0.965 , 0.9836, 0.985 , 0.9732, 0.9832, 0.9838,\n",
       "        0.9848, 0.9844, 0.9884, 0.9864, 0.9702, 0.962 , 0.971 , 0.9802,\n",
       "        0.9802, 0.9736, 0.9816, 0.9812, 0.9826, 0.9832, 0.9826, 0.9834]),\n",
       " 'std_test_f1_micro': array([1.200e-03, 1.800e-03, 1.000e-03, 1.200e-03, 1.600e-03, 2.000e-04,\n",
       "        1.200e-03, 1.000e-03, 8.000e-04, 0.000e+00, 6.000e-04, 1.200e-03,\n",
       "        1.200e-03, 2.400e-03, 1.600e-03, 6.000e-04, 1.400e-03, 1.600e-03,\n",
       "        4.000e-04, 8.000e-04, 1.200e-03, 6.000e-04, 1.600e-03, 2.000e-03,\n",
       "        1.800e-03, 1.600e-03, 1.800e-03, 6.000e-04, 0.000e+00, 1.200e-03,\n",
       "        1.400e-03, 6.000e-04, 8.000e-04, 2.000e-04, 2.000e-04, 1.200e-03,\n",
       "        1.400e-03, 1.600e-03, 2.000e-03, 2.000e-03, 2.200e-03, 1.600e-03,\n",
       "        1.800e-03, 1.800e-03, 2.400e-03, 2.200e-03, 2.000e-03, 2.000e-03,\n",
       "        6.000e-04, 2.400e-03, 0.000e+00, 1.400e-03, 1.400e-03, 6.000e-04,\n",
       "        6.000e-04, 1.200e-03, 0.000e+00, 6.000e-04, 4.000e-04, 8.000e-04,\n",
       "        2.000e-03, 1.800e-03, 2.400e-03, 2.000e-03, 4.000e-04, 2.000e-03,\n",
       "        2.800e-03, 1.600e-03, 1.600e-03, 1.400e-03, 2.000e-04, 2.400e-03,\n",
       "        2.600e-03, 1.800e-03, 1.200e-03, 0.000e+00, 1.400e-03, 2.600e-03,\n",
       "        2.800e-03, 2.000e-04, 4.000e-04, 6.000e-04, 2.000e-04, 8.000e-04,\n",
       "        1.800e-03, 1.400e-03, 8.000e-04, 1.800e-03, 1.600e-03, 1.600e-03,\n",
       "        1.200e-03, 1.200e-03, 1.000e-03, 1.200e-03, 1.200e-03, 8.000e-04,\n",
       "        9.000e-02, 2.422e-01, 9.860e-02, 9.000e-03, 1.696e-01, 2.400e-03,\n",
       "        8.000e-04, 1.736e-01, 3.800e-03, 1.600e-03, 0.000e+00, 2.000e-03,\n",
       "        4.400e-03, 6.360e-02, 1.800e-03, 1.000e-02, 1.670e-01, 1.800e-03,\n",
       "        5.600e-03, 3.200e-03, 6.000e-04, 2.800e-03, 0.000e+00, 8.000e-04,\n",
       "        5.000e-03, 2.486e-01, 9.380e-02, 2.600e-03, 1.730e-01, 2.600e-03,\n",
       "        2.200e-03, 2.620e-02, 2.400e-03, 3.400e-03, 1.106e-01, 6.000e-04,\n",
       "        6.600e-03, 3.000e-02, 7.600e-03, 2.200e-03, 1.880e-01, 8.000e-04,\n",
       "        4.800e-03, 1.690e-01, 2.000e-03, 7.000e-03, 1.300e-02, 1.000e-03,\n",
       "        2.000e-04, 1.190e-01, 2.000e-04, 4.600e-03, 6.860e-02, 3.200e-03,\n",
       "        3.600e-03, 6.040e-02, 4.400e-03, 2.400e-03, 6.340e-02, 6.200e-03,\n",
       "        6.000e-04, 1.212e-01, 1.600e-03, 2.000e-03, 1.780e-02, 2.200e-03,\n",
       "        4.200e-03, 5.580e-02, 3.200e-03, 3.400e-03, 2.060e-02, 2.000e-03,\n",
       "        4.200e-03, 1.548e-01, 6.000e-03, 5.000e-03, 1.514e-01, 1.800e-03,\n",
       "        3.800e-03, 5.220e-02, 4.200e-03, 3.600e-03, 8.520e-02, 3.200e-03,\n",
       "        2.200e-03, 3.380e-02, 4.000e-03, 4.000e-03, 2.594e-01, 3.000e-03,\n",
       "        2.200e-03, 8.120e-02, 3.600e-03, 2.600e-03, 6.240e-02, 3.200e-03,\n",
       "        2.400e-03, 5.800e-03, 3.600e-03, 4.400e-03, 3.800e-03, 4.400e-03,\n",
       "        3.800e-03, 5.000e-03, 3.200e-03, 3.200e-03, 3.200e-03, 5.200e-03,\n",
       "        3.200e-03, 3.600e-03, 3.200e-03, 4.800e-03, 4.800e-03, 3.800e-03,\n",
       "        3.400e-03, 4.200e-03, 3.800e-03, 5.000e-03, 4.200e-03, 4.600e-03,\n",
       "        1.400e-03, 2.400e-03, 4.000e-03, 4.400e-03, 4.600e-03, 4.400e-03,\n",
       "        5.600e-03, 3.000e-03, 3.200e-03, 3.600e-03, 5.800e-03, 5.600e-03,\n",
       "        3.200e-03, 2.000e-03, 2.000e-03, 4.000e-03, 2.200e-03, 4.600e-03,\n",
       "        3.800e-03, 3.600e-03, 4.600e-03, 3.400e-03, 3.600e-03, 2.800e-03,\n",
       "        2.600e-03, 3.000e-03, 1.240e-02, 2.600e-03, 4.200e-03, 1.160e-02,\n",
       "        3.800e-03, 3.400e-03, 3.800e-03, 3.600e-03, 3.000e-03, 2.400e-03,\n",
       "        1.080e-02, 4.000e-03, 5.200e-03, 3.200e-03, 4.200e-03, 3.800e-03,\n",
       "        5.600e-03, 3.800e-03, 2.400e-03, 2.400e-03, 3.800e-03, 4.200e-03,\n",
       "        5.000e-03, 9.200e-03, 3.000e-03, 2.400e-03, 1.800e-03, 1.040e-02,\n",
       "        3.600e-03, 4.600e-03, 3.600e-03, 4.400e-03, 1.200e-03, 2.000e-03,\n",
       "        2.600e-03, 2.800e-03, 1.140e-02, 4.200e-03, 3.000e-03, 1.120e-02,\n",
       "        3.600e-03, 4.000e-03, 2.600e-03, 3.600e-03, 4.200e-03, 3.000e-03]),\n",
       " 'rank_test_f1_micro': array([ 11,  24,  16,   6,  30,  16,  11,   9,  11,  40,  38,  20,   1,\n",
       "         20,  30,   4,  24,   3,  54,  20,   6,  52,  30,  20,  46,  54,\n",
       "         46,  85,  72,  40,  52,  67,  54,  85,  85,  66,  67,  91,  72,\n",
       "         72,  67,  91,  85,  85,  63,  67,  72,  72,   9,  11,   1,  16,\n",
       "         16,  24,   4,  40,   6,  24,  30,  11,  36,  24,  51,  54,  30,\n",
       "         35,  54,  40,  54,  24,  46,  40,  46,  46,  36,  91,  60,  38,\n",
       "         40,  85,  83,  95,  96,  72,  60,  67,  72,  60,  63,  63,  72,\n",
       "         72,  84,  72,  72,  91, 256, 281, 254, 187, 274, 164, 241, 271,\n",
       "        247, 251, 259, 250, 144, 288, 214, 198, 270, 245, 202, 261, 214,\n",
       "        217, 259, 253, 156, 282, 255, 201, 272, 224, 229, 258, 246, 247,\n",
       "        266, 249, 166, 285, 193, 195, 276, 161, 203, 275, 252, 229, 287,\n",
       "        243, 204, 279, 224, 224, 269, 205, 205, 262, 205, 222, 257, 233,\n",
       "        224, 277, 219, 209, 284, 229, 220, 268, 236, 220, 263, 222, 242,\n",
       "        286, 237, 218, 267, 210, 239, 265, 244, 237, 280, 228, 214, 283,\n",
       "        211, 211, 278, 240, 233, 273, 211, 233, 264, 232, 188, 192, 188,\n",
       "        172, 169, 174, 158, 156, 161, 155, 149, 149, 191, 198, 186, 171,\n",
       "        176, 173, 158, 163, 158, 146, 146, 152, 195, 188, 185, 167, 177,\n",
       "        167, 154, 170, 164, 148, 152, 149, 205, 197, 198, 183, 194, 182,\n",
       "        177, 183, 181, 177, 180, 174, 142, 130, 135, 113, 124, 132, 106,\n",
       "        102, 111, 104, 106,  98, 138, 140, 131, 125, 113, 123, 118, 102,\n",
       "        104,  99, 101, 110, 141, 139, 143, 116, 106, 134, 118, 113, 109,\n",
       "        112,  97,  99, 137, 145, 136, 128, 128, 133, 125, 127, 122, 118,\n",
       "        121, 117]),\n",
       " 'split0_test_roc_auc_ovo': array([0.90683191, 0.91895042, 0.9180574 , 0.91205022, 0.88544344,\n",
       "        0.92028641, 0.91734247, 0.93925212, 0.9107661 , 0.92502193,\n",
       "        0.93226241, 0.92977083, 0.90079466, 0.89813071, 0.91521672,\n",
       "        0.89408347, 0.92687399, 0.8989405 , 0.91945407, 0.91245633,\n",
       "        0.91449196, 0.94563591, 0.93181394, 0.91201064, 0.9483819 ,\n",
       "        0.92359858, 0.87461787, 0.92200665, 0.93470676, 0.91596797,\n",
       "        0.89439033, 0.92922905, 0.92839285, 0.96032678, 0.8873671 ,\n",
       "        0.92398673, 0.94031804, 0.94409646, 0.93791036, 0.94034956,\n",
       "        0.94767334, 0.93221784, 0.93649653, 0.9355275 , 0.93481786,\n",
       "        0.9289347 , 0.93485372, 0.94379791, 0.91542683, 0.88196567,\n",
       "        0.89145303, 0.90064553, 0.9160353 , 0.92446769, 0.93078494,\n",
       "        0.92964684, 0.93843057, 0.94925085, 0.91813753, 0.90827631,\n",
       "        0.92424252, 0.89125309, 0.90183227, 0.91722489, 0.89702593,\n",
       "        0.91667713, 0.91079994, 0.90614326, 0.90248157, 0.93636658,\n",
       "        0.93164703, 0.91089914, 0.91976033, 0.92971478, 0.95973262,\n",
       "        0.93188694, 0.91106767, 0.9228619 , 0.89999682, 0.92895453,\n",
       "        0.89326138, 0.91917363, 0.96360614, 0.94551882, 0.94394341,\n",
       "        0.90400662, 0.92336028, 0.93834182, 0.92920451, 0.95068926,\n",
       "        0.95892597, 0.94313188, 0.94547737, 0.96066701, 0.96048943,\n",
       "        0.91571889, 0.90903538, 0.52371285, 0.84612013, 0.80904365,\n",
       "        0.40770268, 0.80695206, 0.86328755, 0.29694393, 0.83177831,\n",
       "        0.83470233, 0.54619592, 0.84050697, 0.91077698, 0.4725427 ,\n",
       "        0.86375697, 0.83883462, 0.28758466, 0.8067092 , 0.83351385,\n",
       "        0.76308939, 0.80511292, 0.83905659, 0.66428454, 0.82574452,\n",
       "        0.8124639 , 0.79794428, 0.80302405, 0.85833743, 0.51374903,\n",
       "        0.80616685, 0.83955422, 0.49162906, 0.83329515, 0.80882637,\n",
       "        0.4841145 , 0.84681295, 0.91247665, 0.47945536, 0.83687989,\n",
       "        0.81275517, 0.42615689, 0.81923451, 0.81676402, 0.47962366,\n",
       "        0.8634431 , 0.8239274 , 0.41950825, 0.80415396, 0.84492663,\n",
       "        0.66684372, 0.84574676, 0.83678295, 0.60055843, 0.84460322,\n",
       "        0.81282777, 0.6651776 , 0.82528569, 0.84412636, 0.80281741,\n",
       "        0.83719285, 0.84051897, 0.51157686, 0.8260035 , 0.83504375,\n",
       "        0.78748723, 0.84648621, 0.84284778, 0.49478416, 0.84769999,\n",
       "        0.84826344, 0.51319108, 0.84592073, 0.80848953, 0.61567245,\n",
       "        0.84251579, 0.83805476, 0.43350429, 0.83755414, 0.80415797,\n",
       "        0.76317342, 0.82932045, 0.83901103, 0.56411958, 0.84131216,\n",
       "        0.83722143, 0.40328239, 0.85033243, 0.83559871, 0.52906439,\n",
       "        0.84150087, 0.85387599, 0.72555635, 0.84658201, 0.83794503,\n",
       "        0.74450945, 0.83738267, 0.84324963, 0.83563493, 0.84064155,\n",
       "        0.82762387, 0.84003989, 0.83322896, 0.83914133, 0.83541445,\n",
       "        0.83794016, 0.83794904, 0.84989837, 0.84692032, 0.84961842,\n",
       "        0.84295783, 0.83508358, 0.83321798, 0.83814376, 0.84204791,\n",
       "        0.85169991, 0.84490219, 0.84384856, 0.84328125, 0.85351483,\n",
       "        0.84228939, 0.8492642 , 0.84048593, 0.84220024, 0.84570172,\n",
       "        0.84099657, 0.83752018, 0.84983625, 0.84387472, 0.83167937,\n",
       "        0.85704851, 0.84765536, 0.84840363, 0.78842164, 0.84766671,\n",
       "        0.84246197, 0.8367188 , 0.84111306, 0.85510273, 0.84465933,\n",
       "        0.82676682, 0.84390622, 0.8468297 , 0.84595416, 0.84643247,\n",
       "        0.83152395, 0.89322255, 0.84699774, 0.92473363, 0.90317389,\n",
       "        0.93165201, 0.92920965, 0.93636819, 0.90167611, 0.92112728,\n",
       "        0.92001077, 0.93479952, 0.85108505, 0.85044174, 0.86405571,\n",
       "        0.92635174, 0.90303717, 0.90105422, 0.92371024, 0.92640606,\n",
       "        0.92146669, 0.93648542, 0.93760653, 0.92473788, 0.84783832,\n",
       "        0.86929867, 0.85559774, 0.92153988, 0.92845359, 0.89147171,\n",
       "        0.92898959, 0.92302774, 0.93046078, 0.92825223, 0.93060056,\n",
       "        0.91290341, 0.89955147, 0.8985619 , 0.85137793, 0.89022724,\n",
       "        0.89548473, 0.88405566, 0.90419184, 0.92238545, 0.91234595,\n",
       "        0.92684791, 0.92607258, 0.91226538]),\n",
       " 'split1_test_roc_auc_ovo': array([0.91549251, 0.94433266, 0.94814661, 0.96791667, 0.93839248,\n",
       "        0.93435795, 0.9611465 , 0.91500151, 0.94348452, 0.92171267,\n",
       "        0.9338165 , 0.93325217, 0.93982655, 0.96669741, 0.94368551,\n",
       "        0.96460835, 0.96967386, 0.9473897 , 0.97172954, 0.95086315,\n",
       "        0.92928794, 0.97146477, 0.94702499, 0.95391226, 0.95904081,\n",
       "        0.96166488, 0.95245337, 0.97033536, 0.93480688, 0.95929559,\n",
       "        0.95876715, 0.96343537, 0.96543419, 0.95637616, 0.92884794,\n",
       "        0.96673874, 0.94369394, 0.94481785, 0.95467748, 0.91955744,\n",
       "        0.92877591, 0.9458781 , 0.91832732, 0.92781402, 0.9201218 ,\n",
       "        0.92998683, 0.92349797, 0.92971119, 0.95001767, 0.96525265,\n",
       "        0.9270248 , 0.91207988, 0.9391634 , 0.9270906 , 0.91862821,\n",
       "        0.92332408, 0.90268305, 0.94150511, 0.92202613, 0.93459509,\n",
       "        0.9533924 , 0.89371144, 0.89240374, 0.906007  , 0.94848596,\n",
       "        0.94232475, 0.95904123, 0.92293893, 0.91905501, 0.94680378,\n",
       "        0.93964517, 0.94212294, 0.93779499, 0.95420781, 0.88548416,\n",
       "        0.94863849, 0.95357234, 0.95118064, 0.94545239, 0.93041348,\n",
       "        0.92885815, 0.93298012, 0.83144792, 0.94888354, 0.92857487,\n",
       "        0.95157322, 0.9201712 , 0.93532315, 0.96508818, 0.91509189,\n",
       "        0.93692281, 0.95525251, 0.93969974, 0.94257016, 0.95509883,\n",
       "        0.95247237, 0.89508277, 0.72910251, 0.80972514, 0.81067828,\n",
       "        0.44245827, 0.81844743, 0.82459775, 0.51656488, 0.82604023,\n",
       "        0.87473505, 0.54618556, 0.81669529, 0.81538291, 0.38242895,\n",
       "        0.91510495, 0.82376971, 0.66808437, 0.82777276, 0.82563289,\n",
       "        0.58406367, 0.82682828, 0.81766935, 0.63463468, 0.86656905,\n",
       "        0.88670509, 0.5158731 , 0.92405428, 0.83715318, 0.4038559 ,\n",
       "        0.8276549 , 0.82316909, 0.77735535, 0.8270512 , 0.87056903,\n",
       "        0.5386876 , 0.81344485, 0.93793716, 0.6108223 , 0.8245674 ,\n",
       "        0.81063479, 0.7064917 , 0.92391247, 0.82194634, 0.82204457,\n",
       "        0.82158613, 0.81958633, 0.48660519, 0.89354859, 0.82022859,\n",
       "        0.40215144, 0.80172712, 0.82891564, 0.66693759, 0.82104618,\n",
       "        0.82184416, 0.78676668, 0.82877226, 0.81380088, 0.749298  ,\n",
       "        0.82192553, 0.82566567, 0.42220166, 0.8529322 , 0.82382523,\n",
       "        0.66444092, 0.81843722, 0.82532542, 0.37448663, 0.83462223,\n",
       "        0.818696  , 0.64840414, 0.82774913, 0.83574638, 0.66908701,\n",
       "        0.81423963, 0.81376719, 0.5242875 , 0.82787196, 0.8250295 ,\n",
       "        0.80811925, 0.81959541, 0.82035759, 0.44981453, 0.80759199,\n",
       "        0.87605757, 0.37844701, 0.82627237, 0.82574219, 0.700678  ,\n",
       "        0.82702372, 0.8217636 , 0.45638044, 0.82010602, 0.82819956,\n",
       "        0.67700084, 0.82900395, 0.81337224, 0.81939581, 0.82802686,\n",
       "        0.81827151, 0.82764284, 0.82265704, 0.83117848, 0.83204883,\n",
       "        0.82903209, 0.83030325, 0.8374698 , 0.83621664, 0.82853712,\n",
       "        0.8291962 , 0.87773391, 0.82775308, 0.82282348, 0.83261298,\n",
       "        0.83941159, 0.82864182, 0.83130188, 0.83069861, 0.83882957,\n",
       "        0.83553989, 0.78638985, 0.81281558, 0.8040599 , 0.82352058,\n",
       "        0.82655833, 0.83259544, 0.83419849, 0.8180357 , 0.82358014,\n",
       "        0.83670227, 0.85470377, 0.83214572, 0.82160849, 0.82217206,\n",
       "        0.82762311, 0.83082754, 0.82435563, 0.82365028, 0.82703542,\n",
       "        0.83011472, 0.83180117, 0.83630749, 0.8373898 , 0.8373249 ,\n",
       "        0.76861444, 0.85003477, 0.83747185, 0.87129114, 0.82517796,\n",
       "        0.88017684, 0.8732481 , 0.8778353 , 0.85586938, 0.90621701,\n",
       "        0.90459486, 0.88652985, 0.8381489 , 0.84570092, 0.78283685,\n",
       "        0.89805985, 0.92173969, 0.8525113 , 0.91017739, 0.87997037,\n",
       "        0.88617118, 0.91334963, 0.89696762, 0.9118219 , 0.83697586,\n",
       "        0.83660713, 0.82700904, 0.89737133, 0.92270524, 0.91233282,\n",
       "        0.89999748, 0.88884307, 0.90375335, 0.88089754, 0.90982443,\n",
       "        0.90929929, 0.86354171, 0.84671923, 0.84563099, 0.88866495,\n",
       "        0.86881436, 0.84682341, 0.91049167, 0.91546531, 0.86424818,\n",
       "        0.89187231, 0.89208754, 0.90723262]),\n",
       " 'mean_test_roc_auc_ovo': array([0.91116221, 0.93164154, 0.933102  , 0.93998345, 0.91191796,\n",
       "        0.92732218, 0.93924449, 0.92712682, 0.92712531, 0.9233673 ,\n",
       "        0.93303945, 0.9315115 , 0.9203106 , 0.93241406, 0.92945112,\n",
       "        0.92934591, 0.94827392, 0.9231651 , 0.94559181, 0.93165974,\n",
       "        0.92188995, 0.95855034, 0.93941947, 0.93296145, 0.95371135,\n",
       "        0.94263173, 0.91353562, 0.946171  , 0.93475682, 0.93763178,\n",
       "        0.92657874, 0.94633221, 0.94691352, 0.95835147, 0.90810752,\n",
       "        0.94536274, 0.94200599, 0.94445716, 0.94629392, 0.9299535 ,\n",
       "        0.93822462, 0.93904797, 0.92741193, 0.93167076, 0.92746983,\n",
       "        0.92946076, 0.92917584, 0.93675455, 0.93272225, 0.92360916,\n",
       "        0.90923892, 0.9063627 , 0.92759935, 0.92577914, 0.92470658,\n",
       "        0.92648546, 0.92055681, 0.94537798, 0.92008183, 0.9214357 ,\n",
       "        0.93881746, 0.89248226, 0.897118  , 0.91161595, 0.92275594,\n",
       "        0.92950094, 0.93492058, 0.9145411 , 0.91076829, 0.94158518,\n",
       "        0.9356461 , 0.92651104, 0.92877766, 0.94196129, 0.92260839,\n",
       "        0.94026272, 0.93232001, 0.93702127, 0.92272461, 0.929684  ,\n",
       "        0.91105976, 0.92607688, 0.89752703, 0.94720118, 0.93625914,\n",
       "        0.92778992, 0.92176574, 0.93683249, 0.94714635, 0.93289058,\n",
       "        0.94792439, 0.9491922 , 0.94258856, 0.95161858, 0.95779413,\n",
       "        0.93409563, 0.90205907, 0.62640768, 0.82792263, 0.80986097,\n",
       "        0.42508047, 0.81269975, 0.84394265, 0.40675441, 0.82890927,\n",
       "        0.85471869, 0.54619074, 0.82860113, 0.86307994, 0.42748583,\n",
       "        0.88943096, 0.83130216, 0.47783452, 0.81724098, 0.82957337,\n",
       "        0.67357653, 0.8159706 , 0.82836297, 0.64945961, 0.84615678,\n",
       "        0.84958449, 0.65690869, 0.86353916, 0.84774531, 0.45880246,\n",
       "        0.81691087, 0.83136166, 0.6344922 , 0.83017317, 0.8396977 ,\n",
       "        0.51140105, 0.8301289 , 0.92520691, 0.54513883, 0.83072365,\n",
       "        0.81169498, 0.5663243 , 0.87157349, 0.81935518, 0.65083411,\n",
       "        0.84251462, 0.82175686, 0.45305672, 0.84885127, 0.83257761,\n",
       "        0.53449758, 0.82373694, 0.8328493 , 0.63374801, 0.8328247 ,\n",
       "        0.81733597, 0.72597214, 0.82702898, 0.82896362, 0.77605771,\n",
       "        0.82955919, 0.83309232, 0.46688926, 0.83946785, 0.82943449,\n",
       "        0.72596408, 0.83246171, 0.8340866 , 0.43463539, 0.84116111,\n",
       "        0.83347972, 0.58079761, 0.83683493, 0.82211796, 0.64237973,\n",
       "        0.82837771, 0.82591097, 0.4788959 , 0.83271305, 0.81459374,\n",
       "        0.78564633, 0.82445793, 0.82968431, 0.50696706, 0.82445208,\n",
       "        0.8566395 , 0.3908647 , 0.8383024 , 0.83067045, 0.6148712 ,\n",
       "        0.83426229, 0.83781979, 0.5909684 , 0.83334402, 0.8330723 ,\n",
       "        0.71075515, 0.83319331, 0.82831093, 0.82751537, 0.8343342 ,\n",
       "        0.82294769, 0.83384136, 0.827943  , 0.83515991, 0.83373164,\n",
       "        0.83348613, 0.83412615, 0.84368408, 0.84156848, 0.83907777,\n",
       "        0.83607702, 0.85640875, 0.83048553, 0.83048362, 0.83733044,\n",
       "        0.84555575, 0.836772  , 0.83757522, 0.83698993, 0.8461722 ,\n",
       "        0.83891464, 0.81782703, 0.82665075, 0.82313007, 0.83461115,\n",
       "        0.83377745, 0.83505781, 0.84201737, 0.83095521, 0.82762975,\n",
       "        0.84687539, 0.85117956, 0.84027468, 0.80501507, 0.83491938,\n",
       "        0.83504254, 0.83377317, 0.83273434, 0.8393765 , 0.83584737,\n",
       "        0.82844077, 0.8378537 , 0.8415686 , 0.84167198, 0.84187868,\n",
       "        0.8000692 , 0.87162866, 0.84223479, 0.89801238, 0.86417593,\n",
       "        0.90591442, 0.90122887, 0.90710174, 0.87877275, 0.91367215,\n",
       "        0.91230282, 0.91066469, 0.84461698, 0.84807133, 0.82344628,\n",
       "        0.91220579, 0.91238843, 0.87678276, 0.91694382, 0.90318821,\n",
       "        0.90381893, 0.92491752, 0.91728707, 0.91827989, 0.84240709,\n",
       "        0.8529529 , 0.84130339, 0.9094556 , 0.92557941, 0.90190227,\n",
       "        0.91449354, 0.9059354 , 0.91710706, 0.90457488, 0.9202125 ,\n",
       "        0.91110135, 0.88154659, 0.87264057, 0.84850446, 0.88944609,\n",
       "        0.88214955, 0.86543954, 0.90734176, 0.91892538, 0.88829707,\n",
       "        0.90936011, 0.90908006, 0.909749  ]),\n",
       " 'std_test_roc_auc_ovo': array([4.33030133e-03, 1.26911186e-02, 1.50446012e-02, 2.79332248e-02,\n",
       "        2.64745200e-02, 7.03577264e-03, 2.19020157e-02, 1.21253042e-02,\n",
       "        1.63592121e-02, 1.65463291e-03, 7.77043161e-04, 1.74066698e-03,\n",
       "        1.95159452e-02, 3.42833477e-02, 1.42343978e-02, 3.52624420e-02,\n",
       "        2.13999358e-02, 2.42246003e-02, 2.61377315e-02, 1.92034133e-02,\n",
       "        7.39798888e-03, 1.29144293e-02, 7.60552564e-03, 2.09508083e-02,\n",
       "        5.32945547e-03, 1.90331486e-02, 3.89177493e-02, 2.41643528e-02,\n",
       "        5.00609362e-05, 2.16638093e-02, 3.21884125e-02, 1.71031612e-02,\n",
       "        1.85206727e-02, 1.97530602e-03, 2.07404161e-02, 2.13760050e-02,\n",
       "        1.68794984e-03, 3.60691180e-04, 8.38355582e-03, 1.03960608e-02,\n",
       "        9.44871152e-03, 6.83012897e-03, 9.08460781e-03, 3.85673847e-03,\n",
       "        7.34802949e-03, 5.26063845e-04, 5.67787321e-03, 7.04336210e-03,\n",
       "        1.72954204e-02, 4.16434904e-02, 1.77858869e-02, 5.71717605e-03,\n",
       "        1.15640472e-02, 1.31145462e-03, 6.07836209e-03, 3.16137666e-03,\n",
       "        1.78737604e-02, 3.87287012e-03, 1.94429955e-03, 1.31593906e-02,\n",
       "        1.45749398e-02, 1.22917503e-03, 4.71426650e-03, 5.60894371e-03,\n",
       "        2.57300123e-02, 1.28238116e-02, 2.41206469e-02, 8.39783277e-03,\n",
       "        8.28671951e-03, 5.21859679e-03, 3.99906817e-03, 1.56119002e-02,\n",
       "        9.01733078e-03, 1.22465156e-02, 3.71242284e-02, 8.37577668e-03,\n",
       "        2.12523355e-02, 1.41593692e-02, 2.27277803e-02, 7.29475192e-04,\n",
       "        1.77983836e-02, 6.90324236e-03, 6.60791101e-02, 1.68235992e-03,\n",
       "        7.68427160e-03, 2.37832964e-02, 1.59453843e-03, 1.50933545e-03,\n",
       "        1.79418348e-02, 1.77986821e-02, 1.10015817e-02, 6.06031733e-03,\n",
       "        2.88881515e-03, 9.04842530e-03, 2.69529999e-03, 1.83767393e-02,\n",
       "        6.97630537e-03, 1.02694834e-01, 1.81974958e-02, 8.17316439e-04,\n",
       "        1.73777928e-02, 5.74768490e-03, 1.93448990e-02, 1.09810474e-01,\n",
       "        2.86903835e-03, 2.00163579e-02, 5.17996802e-06, 1.19058366e-02,\n",
       "        4.76970348e-02, 4.50568769e-02, 2.56739888e-02, 7.53245231e-03,\n",
       "        1.90249853e-01, 1.05317782e-02, 3.94047853e-03, 8.95128614e-02,\n",
       "        1.08576770e-02, 1.06936231e-02, 1.48249275e-02, 2.04122645e-02,\n",
       "        3.71205976e-02, 1.41035592e-01, 6.05151153e-02, 1.05921270e-02,\n",
       "        5.49465661e-02, 1.07440232e-02, 8.19256553e-03, 1.42863147e-01,\n",
       "        3.12197344e-03, 3.08713320e-02, 2.72865495e-02, 1.66840475e-02,\n",
       "        1.27302565e-02, 6.56834696e-02, 6.15624484e-03, 1.06019170e-03,\n",
       "        1.40167404e-01, 5.23389768e-02, 2.59115576e-03, 1.71210453e-01,\n",
       "        2.09284859e-02, 2.17053359e-03, 3.35484701e-02, 4.46973133e-02,\n",
       "        1.23490218e-02, 1.32346142e-01, 2.20098222e-02, 3.93365234e-03,\n",
       "        3.31895774e-02, 1.17785168e-02, 4.50819427e-03, 6.07945376e-02,\n",
       "        1.74328873e-03, 1.51627434e-02, 2.67597071e-02, 7.63365800e-03,\n",
       "        7.42665270e-03, 4.46875974e-02, 1.34643523e-02, 5.60926300e-03,\n",
       "        6.15231542e-02, 1.40244935e-02, 8.76117659e-03, 6.01487648e-02,\n",
       "        6.53887858e-03, 1.47837205e-02, 6.76065294e-02, 9.08579971e-03,\n",
       "        1.36284263e-02, 2.67072781e-02, 1.41380785e-02, 1.21437809e-02,\n",
       "        4.53916046e-02, 4.84108754e-03, 1.04357649e-02, 2.24729186e-02,\n",
       "        4.86251957e-03, 9.32671696e-03, 5.71525238e-02, 1.68600854e-02,\n",
       "        1.94180682e-02, 1.24176926e-02, 1.20300268e-02, 4.92825997e-03,\n",
       "        8.58068007e-02, 7.23857369e-03, 1.60561953e-02, 1.34587957e-01,\n",
       "        1.32379974e-02, 4.87273345e-03, 3.37543040e-02, 4.18935625e-03,\n",
       "        1.49386967e-02, 8.11955882e-03, 6.30734875e-03, 4.67617862e-03,\n",
       "        6.19852292e-03, 5.28596080e-03, 3.98142600e-03, 1.68280758e-03,\n",
       "        4.45403050e-03, 3.82289184e-03, 6.21428424e-03, 5.35183893e-03,\n",
       "        1.05406497e-02, 6.88081525e-03, 2.13251651e-02, 2.73245099e-03,\n",
       "        7.66013672e-03, 4.71746177e-03, 6.14416144e-03, 8.13018887e-03,\n",
       "        6.27334210e-03, 6.29132062e-03, 7.34262829e-03, 3.37474676e-03,\n",
       "        3.14371721e-02, 1.38351749e-02, 1.90701721e-02, 1.10905658e-02,\n",
       "        7.21912282e-03, 2.46237179e-03, 7.81888425e-03, 1.29195110e-02,\n",
       "        4.04961900e-03, 1.01731199e-02, 3.52420860e-03, 8.12895448e-03,\n",
       "        1.65934225e-02, 1.27473254e-02, 7.41943079e-03, 2.94562878e-03,\n",
       "        8.37871422e-03, 1.57262245e-02, 8.81195919e-03, 1.67394999e-03,\n",
       "        6.05252212e-03, 5.26110677e-03, 4.28217895e-03, 4.55378441e-03,\n",
       "        3.14547557e-02, 2.15938899e-02, 4.76294397e-03, 2.67212420e-02,\n",
       "        3.89979654e-02, 2.57375854e-02, 2.79807755e-02, 2.92664432e-02,\n",
       "        2.29033656e-02, 7.45513243e-03, 7.70795219e-03, 2.41348330e-02,\n",
       "        6.46807071e-03, 2.37041258e-03, 4.06094319e-02, 1.41459471e-02,\n",
       "        9.35125718e-03, 2.42714620e-02, 6.76642583e-03, 2.32178463e-02,\n",
       "        1.76477559e-02, 1.15678933e-02, 2.03194563e-02, 6.45798933e-03,\n",
       "        5.43122914e-03, 1.63457718e-02, 1.42943487e-02, 1.20842746e-02,\n",
       "        2.87417736e-03, 1.04305579e-02, 1.44960507e-02, 1.70923350e-02,\n",
       "        1.33537139e-02, 2.36773489e-02, 1.03880635e-02, 1.80205782e-03,\n",
       "        1.80048795e-02, 2.59213349e-02, 2.87347178e-03, 7.81143807e-04,\n",
       "        1.33351845e-02, 1.86161236e-02, 3.14991817e-03, 3.46006914e-03,\n",
       "        2.40488886e-02, 1.74877962e-02, 1.69925198e-02, 2.51637914e-03]),\n",
       " 'rank_test_roc_auc_ovo': array([102,  49,  40,  25, 100,  63,  27,  64,  65,  76,  41,  50,  85,\n",
       "         45,  55,  56,   7,  77,  15,  48,  81,   1,  26,  42,   4,  19,\n",
       "         96,  14,  38,  31,  66,  12,  11,   2, 112,  17,  21,  18,  13,\n",
       "         51,  30,  28,  62,  47,  61,  54,  57,  34,  44,  75, 110, 115,\n",
       "         60,  70,  74,  68,  84,  16,  87,  83,  29, 127, 126, 101,  78,\n",
       "         53,  37,  93, 105,  23,  36,  67,  58,  22,  80,  24,  46,  32,\n",
       "         79,  52, 104,  69, 125,   9,  35,  59,  82,  33,  10,  43,   8,\n",
       "          6,  20,   5,   3,  39, 121, 269, 231, 254, 286, 252, 157, 287,\n",
       "        224, 144, 274, 225, 141, 285, 129, 211, 280, 248, 220, 262, 250,\n",
       "        228, 265, 154, 147, 263, 140, 151, 282, 249, 210, 267, 217, 170,\n",
       "        277, 218,  72, 275, 213, 253, 273, 137, 245, 264, 159, 244, 283,\n",
       "        148, 208, 276, 239, 204, 268, 205, 247, 259, 234, 223, 258, 221,\n",
       "        202, 281, 171, 222, 260, 209, 193, 284, 168, 199, 272, 181, 243,\n",
       "        266, 227, 236, 279, 207, 251, 257, 237, 219, 278, 238, 142, 288,\n",
       "        175, 214, 270, 191, 177, 271, 200, 203, 261, 201, 229, 233, 190,\n",
       "        242, 194, 230, 185, 197, 198, 192, 158, 166, 173, 183, 143, 215,\n",
       "        216, 179, 155, 182, 178, 180, 153, 174, 246, 235, 241, 189, 195,\n",
       "        186, 162, 212, 232, 152, 146, 169, 255, 188, 187, 196, 206, 172,\n",
       "        184, 226, 176, 165, 164, 163, 256, 136, 161, 124, 139, 117, 123,\n",
       "        114, 133,  95,  98, 106, 156, 150, 240,  99,  97, 134,  92, 120,\n",
       "        119,  73,  90,  89, 160, 145, 167, 108,  71, 122,  94, 116,  91,\n",
       "        118,  86, 103, 132, 135, 149, 128, 131, 138, 113,  88, 130, 109,\n",
       "        111, 107]),\n",
       " 'split0_test_neg_log_loss': array([-0.03812185, -0.06245776, -0.06254231, -0.0470794 , -0.05758646,\n",
       "        -0.04795842, -0.07139588, -0.07651195, -0.07661669, -0.09051201,\n",
       "        -0.09319948, -0.08367193, -0.04226112, -0.02992563, -0.07066143,\n",
       "        -0.0363126 , -0.04320379, -0.04256811, -0.05951129, -0.09194285,\n",
       "        -0.03905478, -0.081447  , -0.0787588 , -0.04804062, -0.02859099,\n",
       "        -0.03306328, -0.03521775, -0.05897172, -0.04304461, -0.03348063,\n",
       "        -0.03557995, -0.04795086, -0.0492231 , -0.05561834, -0.05741564,\n",
       "        -0.05721111, -0.03932309, -0.03960699, -0.03872411, -0.03937527,\n",
       "        -0.03872194, -0.04087778, -0.03959691, -0.04005676, -0.03909601,\n",
       "        -0.03941409, -0.03948057, -0.03987896, -0.04045878, -0.04200886,\n",
       "        -0.03799073, -0.06114397, -0.06432802, -0.08656809, -0.09592702,\n",
       "        -0.10792229, -0.07979677, -0.07378864, -0.16634131, -0.06037284,\n",
       "        -0.03011136, -0.04975559, -0.06774588, -0.05609404, -0.09083802,\n",
       "        -0.06309561, -0.07540076, -0.06672491, -0.14696143, -0.05040036,\n",
       "        -0.08681343, -0.07157707, -0.03507571, -0.03713887, -0.05157494,\n",
       "        -0.07610054, -0.06063556, -0.05262073, -0.06438082, -0.07777481,\n",
       "        -0.0663101 , -0.08397132, -0.08277127, -0.0870247 , -0.03478396,\n",
       "        -0.04284891, -0.03962421, -0.0376515 , -0.03998279, -0.0366249 ,\n",
       "        -0.03788371, -0.03960162, -0.04352777, -0.03866109, -0.03943134,\n",
       "        -0.04259947, -0.40909854, -1.33630778, -0.43816393, -0.39952421,\n",
       "        -1.48947753, -0.38042651, -0.35237124, -1.2290489 , -0.33980697,\n",
       "        -0.32980193, -1.23449784, -0.33971921, -0.50803633, -1.42707132,\n",
       "        -0.45404437, -0.3745409 , -1.2843712 , -0.36176812, -0.35684262,\n",
       "        -1.07162331, -0.35178748, -0.35931259, -1.24340106, -0.3413929 ,\n",
       "        -0.43195447, -1.39451257, -0.46959367, -0.37490447, -1.2622469 ,\n",
       "        -0.37803894, -0.36156254, -1.2537664 , -0.34552225, -0.34961733,\n",
       "        -1.27133652, -0.34813952, -0.42284514, -1.47419563, -0.37526355,\n",
       "        -0.37916252, -1.58059767, -0.42669617, -0.34435157, -1.31193265,\n",
       "        -0.35434701, -0.3737703 , -1.41622581, -0.33871658, -0.18202518,\n",
       "        -1.21987763, -0.18623651, -0.17481253, -1.22768273, -0.17338232,\n",
       "        -0.16761153, -1.00945374, -0.16668467, -0.17195538, -1.1641761 ,\n",
       "        -0.16867194, -0.1911174 , -1.05641437, -0.18812502, -0.174399  ,\n",
       "        -1.27166855, -0.17770876, -0.16819432, -1.37305596, -0.17024447,\n",
       "        -0.1697069 , -1.16122587, -0.17358229, -0.18251326, -1.68006267,\n",
       "        -0.17811685, -0.16981919, -1.50610058, -0.17191469, -0.17563396,\n",
       "        -1.30514437, -0.1724318 , -0.17454102, -1.42082723, -0.16938319,\n",
       "        -0.18150355, -1.54725538, -0.17522254, -0.17199919, -1.66061701,\n",
       "        -0.17676786, -0.17581232, -1.33206023, -0.17411246, -0.17549956,\n",
       "        -1.202754  , -0.17317867, -0.17049857, -0.15455078, -0.15989071,\n",
       "        -0.13501316, -0.14031825, -0.13497025, -0.12974742, -0.12200197,\n",
       "        -0.12528917, -0.12158703, -0.11645872, -0.11835579, -0.16087193,\n",
       "        -0.16885468, -0.17037925, -0.13389696, -0.13600566, -0.14037155,\n",
       "        -0.12383459, -0.12721273, -0.12773529, -0.11461672, -0.11527216,\n",
       "        -0.11702849, -0.17057824, -0.17359035, -0.16003577, -0.13223751,\n",
       "        -0.14203555, -0.1332727 , -0.11574571, -0.13677619, -0.13117259,\n",
       "        -0.1149046 , -0.11242209, -0.1138096 , -0.18572906, -0.17722341,\n",
       "        -0.17583183, -0.14883211, -0.15808178, -0.14353634, -0.13702276,\n",
       "        -0.14576355, -0.13666519, -0.12976897, -0.1359709 , -0.1348031 ,\n",
       "        -0.11817381, -0.09871586, -0.09501359, -0.0813135 , -0.08486333,\n",
       "        -0.07947024, -0.07160144, -0.07175307, -0.07629587, -0.06666949,\n",
       "        -0.07540453, -0.06101763, -0.10135044, -0.10247618, -0.0977492 ,\n",
       "        -0.08078422, -0.07681776, -0.08035105, -0.06923794, -0.06695985,\n",
       "        -0.07441755, -0.06867276, -0.06185012, -0.07470082, -0.11411012,\n",
       "        -0.10468283, -0.10962101, -0.08049679, -0.07370618, -0.09503475,\n",
       "        -0.08049343, -0.0713121 , -0.07522419, -0.0764312 , -0.06365352,\n",
       "        -0.07375329, -0.11661286, -0.12102688, -0.09734608, -0.08619269,\n",
       "        -0.09044365, -0.08841625, -0.0851652 , -0.08589443, -0.08554552,\n",
       "        -0.07925476, -0.07595995, -0.08097225]),\n",
       " 'split1_test_neg_log_loss': array([-0.04238437, -0.04106298, -0.03473545, -0.04060001, -0.03847696,\n",
       "        -0.03915634, -0.07975431, -0.05228737, -0.04062886, -0.08380022,\n",
       "        -0.0546405 , -0.06938875, -0.02424179, -0.04305199, -0.02437005,\n",
       "        -0.0330406 , -0.03990112, -0.03750915, -0.04628311, -0.03751004,\n",
       "        -0.03068582, -0.04579566, -0.0316634 , -0.05966008, -0.03548268,\n",
       "        -0.04489167, -0.03703937, -0.03959513, -0.04327033, -0.03301279,\n",
       "        -0.04265349, -0.04052848, -0.04229929, -0.04130414, -0.04568932,\n",
       "        -0.04341795, -0.04738438, -0.04727543, -0.04754044, -0.04829643,\n",
       "        -0.04813323, -0.04789905, -0.04860411, -0.04805834, -0.0483489 ,\n",
       "        -0.04786908, -0.04810464, -0.04767216, -0.03823947, -0.04423048,\n",
       "        -0.04367427, -0.08045749, -0.03905531, -0.04102875, -0.06356869,\n",
       "        -0.09217663, -0.0568993 , -0.09242267, -0.09885183, -0.07771586,\n",
       "        -0.05140977, -0.04807539, -0.05299172, -0.06379891, -0.02305862,\n",
       "        -0.03729698, -0.05413191, -0.08389259, -0.09055338, -0.08613537,\n",
       "        -0.0553043 , -0.05335701, -0.04391806, -0.04301273, -0.03783333,\n",
       "        -0.04266016, -0.04673952, -0.0426392 , -0.04990278, -0.04543454,\n",
       "        -0.04526695, -0.05161588, -0.07482449, -0.04763906, -0.04551901,\n",
       "        -0.04515492, -0.04445706, -0.04539669, -0.04411646, -0.04520736,\n",
       "        -0.04591924, -0.04415195, -0.04536799, -0.04443875, -0.04469522,\n",
       "        -0.04429414, -0.47058541, -1.41091652, -0.49160883, -0.37077459,\n",
       "        -1.20622963, -0.39933115, -0.36149503, -1.28816398, -0.35207455,\n",
       "        -0.32643974, -1.23594198, -0.33136961, -0.44628626, -1.39901042,\n",
       "        -0.45097739, -0.41826279, -1.37493822, -0.35622585, -0.37400414,\n",
       "        -1.31131782, -0.35383908, -0.34997408, -1.19215142, -0.34101556,\n",
       "        -0.42072582, -1.27758453, -0.42960618, -0.38617125, -1.21088354,\n",
       "        -0.36067658, -0.35273339, -1.30644185, -0.35495053, -0.32920027,\n",
       "        -1.70941868, -0.33980821, -0.43339125, -1.41237742, -0.43025126,\n",
       "        -0.38486585, -1.2458512 , -0.40244857, -0.38139643, -1.24118345,\n",
       "        -0.34550479, -0.33883996, -1.30225424, -0.34121174, -0.17732432,\n",
       "        -1.57622564, -0.18217997, -0.18097157, -1.4482425 , -0.17363541,\n",
       "        -0.17532056, -1.17457237, -0.1744883 , -0.17333655, -0.90540305,\n",
       "        -0.1815649 , -0.18812368, -2.02120618, -0.18279716, -0.17552185,\n",
       "        -1.48590448, -0.17988113, -0.17937216, -1.26846223, -0.17638864,\n",
       "        -0.17522837, -1.1171606 , -0.17016005, -0.18915264, -1.10756541,\n",
       "        -0.1943195 , -0.18386111, -1.26117985, -0.17282605, -0.17899624,\n",
       "        -1.09359185, -0.18309515, -0.17720398, -1.31742744, -0.17549411,\n",
       "        -0.1822446 , -1.39765129, -0.18430294, -0.18209176, -1.0367885 ,\n",
       "        -0.1818274 , -0.17284947, -1.39010827, -0.17413608, -0.17305771,\n",
       "        -1.19380721, -0.17360275, -0.16491608, -0.18071183, -0.16457756,\n",
       "        -0.14205123, -0.13936358, -0.14535507, -0.13105399, -0.13103084,\n",
       "        -0.12686816, -0.12473012, -0.11811798, -0.1201841 , -0.16854932,\n",
       "        -0.18840834, -0.17722556, -0.13945636, -0.15106704, -0.14042993,\n",
       "        -0.12494524, -0.13359686, -0.12865671, -0.12208047, -0.11783786,\n",
       "        -0.12317408, -0.16644316, -0.16400219, -0.17721308, -0.14210201,\n",
       "        -0.14580319, -0.13762431, -0.12877214, -0.13727949, -0.13433658,\n",
       "        -0.12122449, -0.12580073, -0.12793619, -0.20408794, -0.17539967,\n",
       "        -0.17516906, -0.15749063, -0.16152353, -0.1588006 , -0.14527175,\n",
       "        -0.14398099, -0.14756696, -0.13773501, -0.14093433, -0.13585434,\n",
       "        -0.10053083, -0.10045345, -0.11113424, -0.08338007, -0.09435275,\n",
       "        -0.09498458, -0.07660275, -0.07676436, -0.07821032, -0.0736452 ,\n",
       "        -0.07225029, -0.06630571, -0.10492035, -0.10091179, -0.10673673,\n",
       "        -0.08304476, -0.08139054, -0.08794631, -0.08584574, -0.07746771,\n",
       "        -0.07589009, -0.06973842, -0.07128346, -0.08031849, -0.10391338,\n",
       "        -0.10196039, -0.1038722 , -0.07889692, -0.0787731 , -0.09368646,\n",
       "        -0.07945362, -0.08229216, -0.07166778, -0.07953541, -0.06228271,\n",
       "        -0.0668191 , -0.10874307, -0.11127315, -0.10707664, -0.09665315,\n",
       "        -0.08786475, -0.09491121, -0.0818224 , -0.08669718, -0.08095643,\n",
       "        -0.07934704, -0.08155772, -0.07745504]),\n",
       " 'mean_test_neg_log_loss': array([-0.04025311, -0.05176037, -0.04863888, -0.04383971, -0.04803171,\n",
       "        -0.04355738, -0.07557509, -0.06439966, -0.05862278, -0.08715611,\n",
       "        -0.07391999, -0.07653034, -0.03325145, -0.03648881, -0.04751574,\n",
       "        -0.0346766 , -0.04155245, -0.04003863, -0.0528972 , -0.06472644,\n",
       "        -0.0348703 , -0.06362133, -0.0552111 , -0.05385035, -0.03203683,\n",
       "        -0.03897748, -0.03612856, -0.04928342, -0.04315747, -0.03324671,\n",
       "        -0.03911672, -0.04423967, -0.04576119, -0.04846124, -0.05155248,\n",
       "        -0.05031453, -0.04335374, -0.04344121, -0.04313227, -0.04383585,\n",
       "        -0.04342759, -0.04438841, -0.04410051, -0.04405755, -0.04372246,\n",
       "        -0.04364158, -0.0437926 , -0.04377556, -0.03934912, -0.04311967,\n",
       "        -0.0408325 , -0.07080073, -0.05169166, -0.06379842, -0.07974785,\n",
       "        -0.10004946, -0.06834804, -0.08310566, -0.13259657, -0.06904435,\n",
       "        -0.04076057, -0.04891549, -0.0603688 , -0.05994647, -0.05694832,\n",
       "        -0.05019629, -0.06476634, -0.07530875, -0.1187574 , -0.06826787,\n",
       "        -0.07105886, -0.06246704, -0.03949689, -0.0400758 , -0.04470413,\n",
       "        -0.05938035, -0.05368754, -0.04762996, -0.0571418 , -0.06160467,\n",
       "        -0.05578852, -0.0677936 , -0.07879788, -0.06733188, -0.04015148,\n",
       "        -0.04400191, -0.04204064, -0.0415241 , -0.04204962, -0.04091613,\n",
       "        -0.04190148, -0.04187678, -0.04444788, -0.04154992, -0.04206328,\n",
       "        -0.04344681, -0.43984198, -1.37361215, -0.46488638, -0.3851494 ,\n",
       "        -1.34785358, -0.38987883, -0.35693314, -1.25860644, -0.34594076,\n",
       "        -0.32812084, -1.23521991, -0.33554441, -0.4771613 , -1.41304087,\n",
       "        -0.45251088, -0.39640185, -1.32965471, -0.35899699, -0.36542338,\n",
       "        -1.19147056, -0.35281328, -0.35464334, -1.21777624, -0.34120423,\n",
       "        -0.42634014, -1.33604855, -0.44959992, -0.38053786, -1.23656522,\n",
       "        -0.36935776, -0.35714796, -1.28010412, -0.35023639, -0.3394088 ,\n",
       "        -1.4903776 , -0.34397387, -0.4281182 , -1.44328653, -0.40275741,\n",
       "        -0.38201418, -1.41322443, -0.41457237, -0.362874  , -1.27655805,\n",
       "        -0.3499259 , -0.35630513, -1.35924003, -0.33996416, -0.17967475,\n",
       "        -1.39805164, -0.18420824, -0.17789205, -1.33796261, -0.17350887,\n",
       "        -0.17146605, -1.09201305, -0.17058648, -0.17264597, -1.03478957,\n",
       "        -0.17511842, -0.18962054, -1.53881027, -0.18546109, -0.17496042,\n",
       "        -1.37878651, -0.17879494, -0.17378324, -1.3207591 , -0.17331656,\n",
       "        -0.17246764, -1.13919324, -0.17187117, -0.18583295, -1.39381404,\n",
       "        -0.18621818, -0.17684015, -1.38364021, -0.17237037, -0.1773151 ,\n",
       "        -1.19936811, -0.17776347, -0.1758725 , -1.36912734, -0.17243865,\n",
       "        -0.18187408, -1.47245334, -0.17976274, -0.17704548, -1.34870275,\n",
       "        -0.17929763, -0.17433089, -1.36108425, -0.17412427, -0.17427863,\n",
       "        -1.1982806 , -0.17339071, -0.16770733, -0.16763131, -0.16223414,\n",
       "        -0.13853219, -0.13984091, -0.14016266, -0.1304007 , -0.12651641,\n",
       "        -0.12607866, -0.12315858, -0.11728835, -0.11926995, -0.16471062,\n",
       "        -0.17863151, -0.1738024 , -0.13667666, -0.14353635, -0.14040074,\n",
       "        -0.12438992, -0.1304048 , -0.128196  , -0.1183486 , -0.11655501,\n",
       "        -0.12010128, -0.1685107 , -0.16879627, -0.16862443, -0.13716976,\n",
       "        -0.14391937, -0.1354485 , -0.12225892, -0.13702784, -0.13275458,\n",
       "        -0.11806454, -0.11911141, -0.12087289, -0.1949085 , -0.17631154,\n",
       "        -0.17550045, -0.15316137, -0.15980266, -0.15116847, -0.14114725,\n",
       "        -0.14487227, -0.14211607, -0.13375199, -0.13845262, -0.13532872,\n",
       "        -0.10935232, -0.09958465, -0.10307392, -0.08234679, -0.08960804,\n",
       "        -0.08722741, -0.07410209, -0.07425872, -0.0772531 , -0.07015734,\n",
       "        -0.07382741, -0.06366167, -0.10313539, -0.10169399, -0.10224297,\n",
       "        -0.08191449, -0.07910415, -0.08414868, -0.07754184, -0.07221378,\n",
       "        -0.07515382, -0.06920559, -0.06656679, -0.07750966, -0.10901175,\n",
       "        -0.10332161, -0.10674661, -0.07969685, -0.07623964, -0.0943606 ,\n",
       "        -0.07997353, -0.07680213, -0.07344599, -0.07798331, -0.06296812,\n",
       "        -0.07028619, -0.11267797, -0.11615001, -0.10221136, -0.09142292,\n",
       "        -0.0891542 , -0.09166373, -0.0834938 , -0.0862958 , -0.08325097,\n",
       "        -0.0793009 , -0.07875884, -0.07921365]),\n",
       " 'std_test_neg_log_loss': array([2.13126117e-03, 1.06973882e-02, 1.39034301e-02, 3.23969224e-03,\n",
       "        9.55475303e-03, 4.40104098e-03, 4.17921307e-03, 1.21122871e-02,\n",
       "        1.79939133e-02, 3.35589721e-03, 1.92794900e-02, 7.14159143e-03,\n",
       "        9.00966218e-03, 6.56318140e-03, 2.31456888e-02, 1.63600242e-03,\n",
       "        1.65133135e-03, 2.52947987e-03, 6.61408676e-03, 2.72164029e-02,\n",
       "        4.18448022e-03, 1.78256667e-02, 2.35476993e-02, 5.80972707e-03,\n",
       "        3.44584628e-03, 5.91419297e-03, 9.10814776e-04, 9.68829439e-03,\n",
       "        1.12860768e-04, 2.33922651e-04, 3.53677070e-03, 3.71119111e-03,\n",
       "        3.46190698e-03, 7.15709903e-03, 5.86315666e-03, 6.89658045e-03,\n",
       "        4.03064568e-03, 3.83421804e-03, 4.40816892e-03, 4.46057918e-03,\n",
       "        4.70564519e-03, 3.51063323e-03, 4.50360141e-03, 4.00078894e-03,\n",
       "        4.62644532e-03, 4.22749663e-03, 4.31203683e-03, 3.89659784e-03,\n",
       "        1.10965368e-03, 1.11081052e-03, 2.84177115e-03, 9.65675762e-03,\n",
       "        1.26363565e-02, 2.27696719e-02, 1.61791662e-02, 7.87282930e-03,\n",
       "        1.14487316e-02, 9.31701664e-03, 3.37447407e-02, 8.67151074e-03,\n",
       "        1.06492021e-02, 8.40100441e-04, 7.37708181e-03, 3.85243729e-03,\n",
       "        3.38896996e-02, 1.28993147e-02, 1.06344246e-02, 8.58384078e-03,\n",
       "        2.82040207e-02, 1.78675030e-02, 1.57545673e-02, 9.11002941e-03,\n",
       "        4.42117594e-03, 2.93692609e-03, 6.87080701e-03, 1.67201917e-02,\n",
       "        6.94801757e-03, 4.99076513e-03, 7.23902205e-03, 1.61701359e-02,\n",
       "        1.05215777e-02, 1.61777228e-02, 3.97338669e-03, 1.96928203e-02,\n",
       "        5.36752574e-03, 1.15300590e-03, 2.41642308e-03, 3.87259350e-03,\n",
       "        2.06683712e-03, 4.29123093e-03, 4.01776855e-03, 2.27516297e-03,\n",
       "        9.20109058e-04, 2.88882831e-03, 2.63193745e-03, 8.47334218e-04,\n",
       "        3.07434366e-02, 3.73043740e-02, 2.67224460e-02, 1.43748081e-02,\n",
       "        1.41623953e-01, 9.45231894e-03, 4.56189908e-03, 2.95575398e-02,\n",
       "        6.13379234e-03, 1.68109041e-03, 7.22068793e-04, 4.17479782e-03,\n",
       "        3.08750379e-02, 1.40304499e-02, 1.53348889e-03, 2.18609477e-02,\n",
       "        4.52835107e-02, 2.77113556e-03, 8.58076199e-03, 1.19847255e-01,\n",
       "        1.02579902e-03, 4.66925462e-03, 2.56248189e-02, 1.88671713e-04,\n",
       "        5.61432744e-03, 5.84640171e-02, 1.99937437e-02, 5.63339037e-03,\n",
       "        2.56816803e-02, 8.68118002e-03, 4.41457326e-03, 2.63377215e-02,\n",
       "        4.71413791e-03, 1.02085293e-02, 2.19041078e-01, 4.16565148e-03,\n",
       "        5.27305622e-03, 3.09091028e-02, 2.74938553e-02, 2.85166341e-03,\n",
       "        1.67373236e-01, 1.21238013e-02, 1.85224303e-02, 3.53745970e-02,\n",
       "        4.42110959e-03, 1.74651689e-02, 5.69857847e-02, 1.24757969e-03,\n",
       "        2.35043022e-03, 1.78174008e-01, 2.02827027e-03, 3.07951871e-03,\n",
       "        1.10279888e-01, 1.26548130e-04, 3.85451616e-03, 8.25593188e-02,\n",
       "        3.90181931e-03, 6.90583369e-04, 1.29386527e-01, 6.44647830e-03,\n",
       "        1.49685822e-03, 4.82395905e-01, 2.66393139e-03, 5.61427020e-04,\n",
       "        1.07117965e-01, 1.08618813e-03, 5.58892066e-03, 5.22968669e-02,\n",
       "        3.07208648e-03, 2.76073692e-03, 2.20326348e-02, 1.71112254e-03,\n",
       "        3.31968978e-03, 2.86248633e-01, 8.10132745e-03, 7.02096279e-03,\n",
       "        1.22460363e-01, 4.55680245e-04, 1.68114186e-03, 1.05776260e-01,\n",
       "        5.33167644e-03, 1.33148001e-03, 5.16998958e-02, 3.05545669e-03,\n",
       "        3.70524717e-04, 7.48020482e-02, 4.54019573e-03, 5.04628411e-03,\n",
       "        3.11914256e-01, 2.52977059e-03, 1.48142547e-03, 2.90240216e-02,\n",
       "        1.18109371e-05, 1.22092392e-03, 4.47339613e-03, 2.12043188e-04,\n",
       "        2.79124307e-03, 1.30805254e-02, 2.34342548e-03, 3.51903317e-03,\n",
       "        4.77339398e-04, 5.19241219e-03, 6.53285253e-04, 4.51443728e-03,\n",
       "        7.89496527e-04, 1.57154486e-03, 8.29630361e-04, 9.14155053e-04,\n",
       "        3.83869714e-03, 9.77683214e-03, 3.42315485e-03, 2.77970186e-03,\n",
       "        7.53068529e-03, 2.91926643e-05, 5.55322130e-04, 3.19206512e-03,\n",
       "        4.60711392e-04, 3.73187369e-03, 1.28285258e-03, 3.07279317e-03,\n",
       "        2.06753893e-03, 4.79407788e-03, 8.58865786e-03, 4.93225165e-03,\n",
       "        1.88381895e-03, 2.17580762e-03, 6.51321432e-03, 2.51647424e-04,\n",
       "        1.58199663e-03, 3.15994112e-03, 6.68932208e-03, 7.06329635e-03,\n",
       "        9.17943984e-03, 9.11870660e-04, 3.31381286e-04, 4.32926291e-03,\n",
       "        1.72087478e-03, 7.63212999e-03, 4.12449343e-03, 8.91279584e-04,\n",
       "        5.45088197e-03, 3.98301909e-03, 2.48171814e-03, 5.25618856e-04,\n",
       "        8.82149063e-03, 8.68792773e-04, 8.06032286e-03, 1.03328436e-03,\n",
       "        4.74470967e-03, 7.75717036e-03, 2.50065337e-03, 2.50564586e-03,\n",
       "        9.57229044e-04, 3.48785263e-03, 1.57712064e-03, 2.64404046e-03,\n",
       "        1.78495314e-03, 7.82195740e-04, 4.49376493e-03, 1.13027040e-03,\n",
       "        2.28639136e-03, 3.79763337e-03, 8.30389926e-03, 5.25393247e-03,\n",
       "        7.36267423e-04, 5.32833317e-04, 4.71666650e-03, 2.80883694e-03,\n",
       "        5.09836963e-03, 1.36122202e-03, 2.87440190e-03, 7.99937524e-04,\n",
       "        2.53345901e-03, 6.74145762e-04, 5.19900643e-04, 5.49002851e-03,\n",
       "        1.77820737e-03, 1.55210826e-03, 6.85407824e-04, 3.46709705e-03,\n",
       "        3.93489609e-03, 4.87686525e-03, 4.86528160e-03, 5.23023163e-03,\n",
       "        1.28945262e-03, 3.24748275e-03, 1.67139566e-03, 4.01378731e-04,\n",
       "        2.29454813e-03, 4.61386057e-05, 2.79888046e-03, 1.75860288e-03]),\n",
       " 'rank_test_neg_log_loss': array([ 15,  60,  53,  40,  51,  34, 100,  78,  68, 123,  95, 102,   3,\n",
       "          7,  49,   4,  21,  12,  61,  79,   5,  75,  64,  63,   1,   8,\n",
       "          6,  55,  29,   2,   9,  44,  48,  52,  58,  57,  30,  32,  28,\n",
       "         39,  31,  45,  43,  42,  36,  35,  38,  37,  10,  27,  17,  90,\n",
       "         59,  77, 114, 131,  85, 118, 160,  86,  16,  54,  71,  70,  66,\n",
       "         56,  80,  99, 147,  84,  91,  73,  11,  13,  47,  69,  62,  50,\n",
       "         67,  72,  65,  83, 109,  82,  14,  41,  24,  19,  25,  18,  23,\n",
       "         22,  46,  20,  26,  33, 252, 278, 255, 245, 273, 246, 237, 266,\n",
       "        231, 225, 264, 226, 256, 283, 254, 247, 270, 239, 241, 260, 234,\n",
       "        235, 263, 229, 250, 271, 253, 243, 265, 242, 238, 268, 233, 227,\n",
       "        287, 230, 251, 285, 248, 244, 284, 249, 240, 267, 232, 236, 275,\n",
       "        228, 216, 282, 219, 212, 272, 197, 189, 258, 188, 194, 257, 204,\n",
       "        223, 288, 220, 203, 279, 214, 198, 269, 195, 193, 259, 190, 221,\n",
       "        281, 222, 208, 280, 191, 210, 262, 211, 206, 277, 192, 218, 286,\n",
       "        217, 209, 274, 215, 202, 276, 200, 201, 261, 196, 184, 183, 181,\n",
       "        169, 170, 171, 158, 156, 155, 153, 144, 149, 182, 213, 199, 165,\n",
       "        175, 172, 154, 159, 157, 146, 143, 150, 185, 187, 186, 167, 176,\n",
       "        164, 152, 166, 161, 145, 148, 151, 224, 207, 205, 179, 180, 178,\n",
       "        173, 177, 174, 162, 168, 163, 140, 130, 135, 117, 126, 124,  96,\n",
       "         97, 104,  88,  94,  76, 136, 132, 134, 116, 110, 121, 106,  92,\n",
       "         98,  87,  81, 105, 139, 137, 138, 113, 101, 129, 115, 103,  93,\n",
       "        107,  74,  89, 141, 142, 133, 127, 125, 128, 120, 122, 119, 112,\n",
       "        108, 111])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRIAL_1_MLP.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL ONE NEG LOG LOSS RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.01,\n",
       " 'classifier__hidden_layer_sizes': (5,),\n",
       " 'classifier__learning_rate': 'constant',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL ONE NEG LOG LOSS RESULTS ---------------------------------------------\")\n",
    "TRIAL_1_MLP.cv_results_['rank_test_neg_log_loss']\n",
    "TRIAL_1_MLP.cv_results_['params'][ np.argmin(TRIAL_1_MLP.cv_results_['rank_test_neg_log_loss']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL ONE F1 RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.001,\n",
       " 'classifier__hidden_layer_sizes': (5,),\n",
       " 'classifier__learning_rate': 'constant',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL ONE F1 RESULTS ---------------------------------------------\")\n",
    "TRIAL_1_MLP.cv_results_['rank_test_f1_micro']\n",
    "TRIAL_1_MLP.cv_results_['params'][ np.argmin(TRIAL_1_MLP.cv_results_['rank_test_f1_micro']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL ONE ROC AUC RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.001,\n",
       " 'classifier__hidden_layer_sizes': (20,),\n",
       " 'classifier__learning_rate': 'constant',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL ONE ROC AUC RESULTS ---------------------------------------------\")\n",
    "TRIAL_1_MLP.cv_results_['rank_test_roc_auc_ovo']\n",
    "TRIAL_1_MLP.cv_results_['params'][ np.argmin(TRIAL_1_MLP.cv_results_['rank_test_roc_auc_ovo']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 1 MLP using best NEG LOG LOSS hyperparameters :0.9923\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 1 MLP using best F1 hyperparameters :0.9958166666666667\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 1 MLP using best ROC_AUC hyperparameters :0.99545\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best NEG LOG LOSS results to train on test set, \n",
    "p = TRIAL_1_MLP.cv_results_['params'][ np.argmin(TRIAL_1_MLP.cv_results_['rank_test_neg_log_loss']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train__neg_log_loss = pipe.predict(X_train)\n",
    "y_pred_test_neg_log_loss = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 1 MLP using best NEG LOG LOSS hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_neg_log_loss)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best F1 results to train on test set, LR_Trial 1\n",
    "p = TRIAL_1_MLP.cv_results_['params'][ np.argmin(TRIAL_1_MLP.cv_results_['rank_test_f1_micro']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train_roc_auc = pipe.predict(X_train)\n",
    "y_pred_test_f1 = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 1 MLP using best F1 hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_f1)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best ROC_AUC results to train on test set, LR_Trial 1\n",
    "p  = TRIAL_1_MLP.cv_results_['params'][ np.argmin(TRIAL_1_MLP.cv_results_['rank_test_roc_auc_ovo']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train_roc_auc = pipe.predict(X_train)\n",
    "y_pred_test_roc_auc = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 1 MLP using best ROC_AUC hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_roc_auc)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTI LAYERED PERCEPTRON TRIAL TWO ON FIRE WALLDATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 288 candidates, totalling 576 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed:    7.7s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    9.1s\n",
      "[Parallel(n_jobs=-1)]: Done 189 tasks      | elapsed:   10.6s\n",
      "[Parallel(n_jobs=-1)]: Done 210 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=-1)]: Done 233 tasks      | elapsed:   16.4s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:   19.3s\n",
      "[Parallel(n_jobs=-1)]: Done 281 tasks      | elapsed:   22.4s\n",
      "[Parallel(n_jobs=-1)]: Done 306 tasks      | elapsed:   25.8s\n",
      "[Parallel(n_jobs=-1)]: Done 333 tasks      | elapsed:   29.5s\n",
      "[Parallel(n_jobs=-1)]: Done 360 tasks      | elapsed:   33.0s\n",
      "[Parallel(n_jobs=-1)]: Done 389 tasks      | elapsed:   36.9s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:   41.4s\n",
      "[Parallel(n_jobs=-1)]: Done 449 tasks      | elapsed:   45.8s\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:   50.1s\n",
      "[Parallel(n_jobs=-1)]: Done 513 tasks      | elapsed:   54.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL TWO RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 576 out of 576 | elapsed:  1.1min finished\n"
     ]
    }
   ],
   "source": [
    "ITERATIONS = 1\n",
    "for i in range(1):\n",
    "    #For reach \"trial\", randomly select 5000 data points\n",
    "    random5000DataPoints = FireWALLData.sample(n = 5000, replace = True)\n",
    "    #From that 5000, create x set and y set\n",
    "    xSet = random5000DataPoints.iloc[:,0:9].values\n",
    "    ySet = random5000DataPoints.iloc[:,10].values\n",
    "    ySet = encodeLabel.fit_transform(ySet)\n",
    "    pipe = Pipeline([('std', StandardScaler()),('classifier', MLPClassifier(max_iter = 250))])\n",
    "\n",
    "    search_space = [\n",
    "                {\n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                {\n",
    "                 \n",
    "                  \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['sgd'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                \n",
    "                {\n",
    "                \n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['adam'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                ]\n",
    "    \n",
    "    #'precision_micro','roc_auc_ovo','jaccard','neg_log_loss'] JACCARD DOES NOT WORK FOR NON BINARY?\n",
    "    runHyperParamSearch = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=2), \n",
    "                          scoring=['recall_micro','f1_micro','roc_auc_ovo','neg_log_loss'], \n",
    "                          refit=False, verbose=10, n_jobs = -1)\n",
    "    TRIAL_2_MLP = runHyperParamSearch.fit(xSet, ySet)\n",
    "    print(\"---------------------------------TRIAL TWO RESULTS ---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.29675651, 0.48391616, 0.63629746, 0.74939549, 0.51994848,\n",
       "        0.68709195, 0.83797133, 0.98284626, 0.7956841 , 0.9375571 ,\n",
       "        1.01437306, 0.97083473, 0.37307   , 0.50668895, 0.54046547,\n",
       "        0.71061182, 0.79743564, 0.60026622, 0.80043852, 0.77491665,\n",
       "        0.97634041, 1.09118807, 0.89276862, 1.05615783, 0.49167085,\n",
       "        0.37457275, 0.52695382, 0.5677402 , 0.65706563, 0.62678957,\n",
       "        0.69184661, 0.84022248, 0.73488224, 0.75014544, 0.84697843,\n",
       "        0.82596028, 0.52770483, 0.40709889, 0.37081909, 0.62754083,\n",
       "        0.58050156, 0.51369226, 0.57299316, 0.56323552, 0.60351908,\n",
       "        0.71611726, 0.6235348 , 0.64980912, 0.54521656, 0.75314665,\n",
       "        0.55272424, 0.57299304, 0.87099957, 0.95157027, 1.1134578 ,\n",
       "        1.08143151, 0.88225937, 1.05115187, 1.17125595, 1.00986743,\n",
       "        0.54296482, 0.73613381, 0.59476268, 0.91303635, 0.69109356,\n",
       "        0.69810081, 1.0201267 , 1.14498472, 1.18802214, 1.33164489,\n",
       "        1.3341465 , 1.38143826, 0.72962558, 0.8279624 , 0.63629842,\n",
       "        1.06441629, 0.88701284, 0.95607305, 1.27084255, 1.22255123,\n",
       "        1.36917734, 1.32513857, 1.48052406, 1.2958647 , 0.65981603,\n",
       "        0.69835126, 0.73913503, 0.78692222, 0.86549401, 0.85398602,\n",
       "        0.98359573, 0.95331979, 0.98985195, 1.17826319, 0.99960959,\n",
       "        1.11646056, 1.92640626, 1.17976463, 1.91389632, 2.09204948,\n",
       "        1.20878947, 2.10305858, 2.48438656, 1.33414698, 2.39430904,\n",
       "        2.3772943 , 2.50615525, 2.33550894, 1.96769238, 0.80569291,\n",
       "        1.90563929, 2.43709588, 1.50104117, 2.29797661, 2.1788727 ,\n",
       "        2.25619006, 2.13683808, 2.67655253, 2.33575916, 2.39931321,\n",
       "        1.92265403, 1.42897904, 1.86785865, 2.03725183, 1.9229039 ,\n",
       "        1.91940129, 2.21765792, 1.43848646, 2.16085827, 2.40531814,\n",
       "        1.8193152 , 2.21890783, 1.86285162, 1.2177974 , 1.84658825,\n",
       "        2.08679521, 1.26558852, 2.10430992, 2.17161775, 2.1095643 ,\n",
       "        2.22191083, 2.45561123, 1.20053208, 2.36628485, 1.8836199 ,\n",
       "        1.71897852, 2.02999592, 2.07803738, 1.97619927, 2.01348257,\n",
       "        2.30097878, 2.3797971 , 2.4198308 , 2.35202301, 2.3465184 ,\n",
       "        2.24793375, 1.97494864, 1.58636427, 1.94592357, 2.05401647,\n",
       "        1.80755424, 2.0530163 , 2.17311835, 2.47312701, 2.21640539,\n",
       "        2.37279129, 2.39605999, 2.31148779, 2.02073789, 2.07503533,\n",
       "        1.94742453, 2.05426669, 2.0007199 , 2.16586268, 2.30898535,\n",
       "        2.25093651, 2.19538832, 2.36428297, 2.22191107, 2.30198002,\n",
       "        1.84959054, 1.22255206, 1.8520931 , 2.07578552, 2.07803786,\n",
       "        2.16636312, 2.14984953, 2.23442173, 2.18863213, 2.34076333,\n",
       "        2.21315348, 2.32174683, 2.05977118, 2.11381745, 2.00497425,\n",
       "        2.26719952, 2.13333547, 2.23267019, 2.29822648, 2.32950366,\n",
       "        2.42508578, 2.38980484, 2.3915571 , 2.41357553, 2.15135002,\n",
       "        2.04325795, 2.05026245, 2.20714808, 2.19238615, 2.24242914,\n",
       "        2.34902024, 2.31624186, 2.33650935, 2.36603546, 2.36453414,\n",
       "        2.32600033, 2.01298094, 2.02724409, 1.99446559, 2.09054708,\n",
       "        2.12057352, 2.13933992, 2.31974542, 2.32650137, 2.30848587,\n",
       "        2.35402453, 2.41007304, 2.43959844, 2.14509439, 2.15084958,\n",
       "        2.11682105, 2.17687273, 2.16636372, 2.29197097, 2.39881265,\n",
       "        2.40631938, 2.32875264, 2.40481758, 2.42858851, 2.3310045 ,\n",
       "        2.01448286, 2.04025447, 2.03775263, 2.20939982, 2.18714738,\n",
       "        2.21315277, 2.46286809, 2.37454259, 2.39831257, 2.47982562,\n",
       "        2.48238552, 2.48163426, 2.03249907, 2.03149748, 2.0049752 ,\n",
       "        2.16686392, 2.12032294, 2.15210104, 2.35602641, 2.34051335,\n",
       "        2.33726001, 2.43134177, 2.44960678, 2.44034874, 2.02323961,\n",
       "        2.05326617, 2.17487049, 2.20589757, 2.43159115, 2.37028944,\n",
       "        2.71458459, 2.56545627, 2.60474026, 2.65803528, 2.70032299,\n",
       "        2.63927031, 2.10280776, 2.27070367, 2.15885627, 2.23467219,\n",
       "        2.25143671, 2.34877002, 2.29647505, 2.19939315, 2.22916687,\n",
       "        2.20589626, 1.97519815, 1.73123765]),\n",
       " 'std_fit_time': array([5.75501919e-02, 1.69145226e-01, 2.67732143e-02, 1.59386754e-01,\n",
       "        1.66142702e-01, 2.24191785e-01, 5.07932901e-02, 1.45123005e-02,\n",
       "        1.79655552e-01, 2.78990626e-01, 1.50129080e-01, 9.05776024e-02,\n",
       "        1.00837946e-01, 1.67393327e-01, 1.40121341e-01, 1.42122269e-01,\n",
       "        5.72999716e-02, 2.76987553e-01, 2.70482421e-01, 2.02925444e-01,\n",
       "        3.57627869e-07, 4.02842760e-02, 1.45123720e-01, 1.25122070e-03,\n",
       "        2.00922728e-01, 2.57720947e-02, 1.51130080e-01, 9.28325653e-02,\n",
       "        1.35116816e-01, 1.98420525e-01, 2.38954067e-01, 9.45817232e-02,\n",
       "        1.92917943e-01, 2.46211529e-01, 2.34450817e-01, 2.20438838e-01,\n",
       "        3.25381756e-03, 4.02852297e-02, 4.60398197e-02, 3.20274830e-02,\n",
       "        6.35552406e-02, 2.07675695e-02, 6.50465488e-03, 4.32860851e-02,\n",
       "        7.95680285e-02, 3.05250883e-02, 9.50821638e-02, 9.08281803e-02,\n",
       "        2.11933374e-01, 1.95174217e-02, 1.15348458e-01, 3.24277878e-01,\n",
       "        4.27863598e-02, 1.07595921e-02, 3.57627869e-07, 1.01086259e-01,\n",
       "        2.75232792e-02, 8.23227167e-02, 2.44960666e-01, 2.72732973e-01,\n",
       "        1.99674010e-01, 1.80151463e-02, 1.15349412e-01, 6.08022213e-02,\n",
       "        2.82242656e-01, 3.03260088e-01, 1.15348220e-01, 3.70318890e-02,\n",
       "        1.20103359e-02, 2.35208273e-02, 1.85163021e-02, 6.43060207e-02,\n",
       "        1.00091696e-02, 4.77912426e-02, 1.78905010e-01, 4.50217724e-03,\n",
       "        9.28298235e-02, 4.27880287e-02, 1.09843969e-01, 6.50569201e-02,\n",
       "        1.75139904e-02, 1.75154209e-02, 7.23106861e-02, 3.17765474e-02,\n",
       "        1.39871359e-01, 1.27359986e-01, 1.70159340e-02, 2.75983095e-01,\n",
       "        1.27358675e-01, 6.68070316e-02, 1.85409427e-01, 1.73646688e-01,\n",
       "        1.06593370e-01, 6.98087215e-02, 2.28947282e-01, 1.46626234e-01,\n",
       "        9.33309793e-02, 3.82578492e-01, 3.02755833e-02, 2.42708921e-02,\n",
       "        4.01095033e-01, 9.28297043e-02, 5.12942076e-02, 7.68661499e-01,\n",
       "        3.37791443e-02, 1.67641640e-02, 2.94253349e-01, 1.37116313e-01,\n",
       "        1.50156021e-03, 3.30283403e-01, 7.25628138e-02, 1.79153919e-01,\n",
       "        6.37296915e-01, 3.18773627e-01, 8.30713511e-02, 1.45874977e-01,\n",
       "        2.10179090e-02, 2.88497686e-01, 1.46876574e-01, 2.22942352e-01,\n",
       "        7.55645037e-02, 6.66823506e-01, 2.32675076e-02, 1.55135393e-02,\n",
       "        1.27609968e-02, 1.87667608e-02, 9.63326693e-02, 1.05515826e+00,\n",
       "        5.45464754e-02, 1.59887910e-01, 3.15020800e-01, 6.90593719e-02,\n",
       "        2.87749767e-02, 9.45563197e-01, 5.15443087e-02, 1.29611373e-01,\n",
       "        2.65221596e-02, 9.95856524e-02, 7.93185234e-02, 2.75278091e-03,\n",
       "        1.17601037e-01, 5.50460815e-03, 9.57823873e-01, 7.03110695e-02,\n",
       "        3.00252438e-03, 1.98670745e-01, 1.90914154e-01, 3.02757025e-02,\n",
       "        1.00100040e-03, 5.47966957e-02, 4.80417013e-02, 1.32619143e-02,\n",
       "        5.12940884e-02, 5.95513582e-02, 8.55730772e-02, 4.00340557e-03,\n",
       "        9.38303471e-02, 3.17272186e-01, 4.37874794e-02, 5.67990541e-02,\n",
       "        2.84744978e-01, 6.38049841e-02, 1.67646408e-02, 9.10781622e-02,\n",
       "        2.05168724e-02, 1.39869213e-01, 1.50128126e-01, 1.05091333e-02,\n",
       "        1.04089737e-01, 7.28118420e-02, 1.12847209e-01, 1.80151463e-02,\n",
       "        1.00084543e-02, 1.17601514e-01, 1.33114934e-01, 1.34614944e-01,\n",
       "        5.45468330e-02, 2.97759771e-02, 8.00669193e-03, 1.00085974e-01,\n",
       "        1.00064278e-03, 6.35046005e-01, 2.85240412e-02, 1.32112384e-01,\n",
       "        1.31363511e-01, 5.20449877e-02, 1.15095377e-02, 7.75671005e-02,\n",
       "        1.12594366e-02, 1.07593536e-02, 7.25591183e-03, 6.93092346e-02,\n",
       "        5.80495596e-02, 3.50296497e-02, 2.92760134e-02, 9.93354321e-02,\n",
       "        1.65148973e-02, 4.42887545e-02, 1.62636042e-02, 7.85673857e-02,\n",
       "        1.37117863e-01, 9.63332653e-02, 5.60480356e-02, 3.00228596e-03,\n",
       "        1.95167065e-02, 4.50387001e-02, 5.40466309e-02, 4.52895164e-02,\n",
       "        2.40211487e-02, 1.70141459e-02, 4.60398197e-02, 1.07592344e-02,\n",
       "        7.00604916e-03, 4.50396538e-03, 9.50765610e-03, 2.85245180e-02,\n",
       "        2.77743340e-02, 2.45211124e-02, 1.67641640e-02, 2.75254250e-03,\n",
       "        1.57637596e-02, 2.40211487e-02, 3.12770605e-02, 2.35195160e-02,\n",
       "        3.40293646e-02, 6.50513172e-03, 4.15368080e-02, 2.50219107e-02,\n",
       "        9.13286209e-02, 1.77152157e-01, 2.05168724e-02, 7.10618496e-02,\n",
       "        3.80334854e-02, 3.35292816e-02, 1.21353984e-01, 6.08024597e-02,\n",
       "        9.25791264e-03, 7.28130341e-02, 3.90332937e-02, 2.00140476e-03,\n",
       "        2.49505043e-04, 1.95170641e-02, 2.65221596e-02, 1.80159807e-02,\n",
       "        2.47550011e-02, 1.92660093e-02, 3.07766199e-02, 1.15092993e-02,\n",
       "        1.75142288e-03, 1.17028952e-02, 6.75547123e-03, 6.00421429e-03,\n",
       "        6.43050671e-02, 4.32872772e-02, 2.67723799e-02, 3.95336151e-02,\n",
       "        1.10096931e-02, 1.02593899e-02, 1.80153847e-02, 1.45121813e-02,\n",
       "        7.25626945e-03, 1.27609968e-02, 3.00252438e-03, 1.97671652e-02,\n",
       "        1.90174580e-02, 3.50272655e-03, 9.25803185e-02, 1.75149441e-02,\n",
       "        8.15707445e-02, 7.33121634e-02, 1.71897888e-01, 1.61388993e-01,\n",
       "        1.25108957e-02, 1.13848448e-01, 1.38619065e-01, 5.10439873e-02,\n",
       "        1.00088120e-03, 2.27693319e-02, 1.06091619e-01, 6.38049841e-02,\n",
       "        2.05178261e-02, 6.63064718e-02, 1.25111341e-02, 9.00924206e-03,\n",
       "        6.53055906e-02, 5.00434637e-02, 5.50448895e-03, 3.97851467e-02]),\n",
       " 'mean_score_time': array([0.02026582, 0.0225203 , 0.02126837, 0.02201939, 0.02126777,\n",
       "        0.03127766, 0.02326965, 0.02502155, 0.02352071, 0.02351975,\n",
       "        0.02326989, 0.02277005, 0.02101755, 0.02076578, 0.02201855,\n",
       "        0.02352107, 0.02201879, 0.03628302, 0.02377129, 0.02452075,\n",
       "        0.02327013, 0.02977586, 0.02351964, 0.02326977, 0.02252114,\n",
       "        0.02076674, 0.02226865, 0.02501869, 0.02402008, 0.02226949,\n",
       "        0.02852261, 0.02276981, 0.02452123, 0.02552104, 0.02377093,\n",
       "        0.03102648, 0.0250206 , 0.02276933, 0.02101791, 0.02201855,\n",
       "        0.0227685 , 0.02352214, 0.02427244, 0.02301884, 0.02427101,\n",
       "        0.02452004, 0.02252018, 0.02226889, 0.02026713, 0.02301991,\n",
       "        0.02051795, 0.02402115, 0.02502024, 0.02376866, 0.0250212 ,\n",
       "        0.02176654, 0.02251959, 0.02477276, 0.02301991, 0.02402198,\n",
       "        0.02176929, 0.02326918, 0.02101779, 0.02326894, 0.02427101,\n",
       "        0.02502167, 0.02477157, 0.02527189, 0.02602291, 0.0322777 ,\n",
       "        0.02777493, 0.02452075, 0.02201879, 0.03152895, 0.02076805,\n",
       "        0.02301872, 0.02401984, 0.02527106, 0.03678203, 0.04478931,\n",
       "        0.03603137, 0.03252828, 0.03502917, 0.0332787 , 0.02402103,\n",
       "        0.03152716, 0.02226925, 0.02527201, 0.02577066, 0.03227663,\n",
       "        0.03603232, 0.02452028, 0.0265224 , 0.02352083, 0.02352083,\n",
       "        0.02226961, 0.02977526, 0.02276933, 0.01976621, 0.03327882,\n",
       "        0.02226925, 0.02727401, 0.02326953, 0.02151811, 0.02151871,\n",
       "        0.02302027, 0.02602232, 0.02226877, 0.02402008, 0.02076793,\n",
       "        0.01976693, 0.02752411, 0.02226853, 0.02351952, 0.02251995,\n",
       "        0.0335294 , 0.02251863, 0.03502965, 0.02376986, 0.02326989,\n",
       "        0.02001727, 0.02577233, 0.02601981, 0.0305264 , 0.02101827,\n",
       "        0.02151859, 0.02827406, 0.02201891, 0.02026784, 0.02251971,\n",
       "        0.02076817, 0.02752411, 0.0215193 , 0.02051759, 0.02051795,\n",
       "        0.03152645, 0.02377009, 0.02627265, 0.02251852, 0.02151859,\n",
       "        0.02327037, 0.02427065, 0.02677298, 0.03277791, 0.02101803,\n",
       "        0.02126849, 0.02126837, 0.02852464, 0.02402043, 0.02276838,\n",
       "        0.02201855, 0.02201903, 0.02276993, 0.02251971, 0.02877426,\n",
       "        0.02377033, 0.02151823, 0.0215193 , 0.02051783, 0.02326989,\n",
       "        0.02477157, 0.02176857, 0.02777398, 0.0250212 , 0.02201879,\n",
       "        0.02477086, 0.02402055, 0.02226913, 0.02176905, 0.02301884,\n",
       "        0.02452099, 0.02276921, 0.02126873, 0.0252713 , 0.02402079,\n",
       "        0.02327001, 0.02226877, 0.02251935, 0.02151823, 0.02151787,\n",
       "        0.0200181 , 0.02201867, 0.02251899, 0.02101684, 0.0252713 ,\n",
       "        0.02101779, 0.02276886, 0.02276921, 0.0312767 , 0.03027594,\n",
       "        0.02176881, 0.02777386, 0.02001762, 0.02852488, 0.02326965,\n",
       "        0.02352035, 0.02301943, 0.0270232 , 0.02126837, 0.02226877,\n",
       "        0.02226973, 0.02327085, 0.02827382, 0.03327954, 0.02302051,\n",
       "        0.02101779, 0.02051795, 0.02176893, 0.02126825, 0.02126789,\n",
       "        0.02201903, 0.02251959, 0.02277017, 0.02276933, 0.02176845,\n",
       "        0.02352047, 0.02026761, 0.02076721, 0.02026749, 0.02151918,\n",
       "        0.02026737, 0.02502155, 0.02276945, 0.02251828, 0.02276886,\n",
       "        0.02552199, 0.02827477, 0.02702296, 0.02151883, 0.02552223,\n",
       "        0.02952564, 0.02051735, 0.02402043, 0.02201927, 0.02151954,\n",
       "        0.02176929, 0.02226973, 0.02251983, 0.02477133, 0.02276957,\n",
       "        0.02176774, 0.02026796, 0.02327001, 0.02176893, 0.02075124,\n",
       "        0.02176857, 0.02251935, 0.0225184 , 0.02201951, 0.02332664,\n",
       "        0.02301967, 0.02351999, 0.02026701, 0.02126849, 0.02076674,\n",
       "        0.02276957, 0.02226889, 0.02151823, 0.02327025, 0.02276945,\n",
       "        0.02427065, 0.02301931, 0.02302003, 0.02327037, 0.02226901,\n",
       "        0.02126801, 0.02251935, 0.02151859, 0.02702343, 0.02401996,\n",
       "        0.03477991, 0.02276981, 0.02402127, 0.03703249, 0.02301967,\n",
       "        0.02276933, 0.02126861, 0.02652168, 0.02076817, 0.02852428,\n",
       "        0.02176821, 0.02001655, 0.02051771, 0.01526141, 0.01451325,\n",
       "        0.01351154, 0.01301146, 0.01326203]),\n",
       " 'std_score_time': array([7.49588013e-04, 2.50124931e-03, 2.50101089e-04, 1.00135803e-03,\n",
       "        7.51137733e-04, 1.25098228e-03, 1.75166130e-03, 4.99963760e-04,\n",
       "        1.50299072e-03, 1.50156021e-03, 1.25241280e-03, 2.51173973e-04,\n",
       "        5.00202179e-04, 2.49624252e-04, 2.00188160e-03, 5.00082970e-04,\n",
       "        5.96046448e-07, 1.52604580e-02, 2.48908997e-04, 5.01036644e-04,\n",
       "        2.48908997e-04, 3.25274467e-03, 5.00082970e-04, 2.50697136e-04,\n",
       "        1.50144100e-03, 2.50101089e-04, 1.25014782e-03, 2.00438499e-03,\n",
       "        4.98652458e-04, 7.51733780e-04, 5.00619411e-03, 7.51376152e-04,\n",
       "        3.00395489e-03, 4.99963760e-04, 1.25062466e-03, 3.00180912e-03,\n",
       "        5.01394272e-04, 2.50458717e-04, 5.01275063e-04, 5.96046448e-07,\n",
       "        7.50303268e-04, 1.00016594e-03, 2.52008438e-04, 5.00679016e-04,\n",
       "        2.75182724e-03, 1.00100040e-03, 5.00559807e-04, 2.50220299e-04,\n",
       "        2.49505043e-04, 3.57627869e-07, 1.00028515e-03, 1.00040436e-03,\n",
       "        2.00188160e-03, 2.25281715e-03, 5.00798225e-04, 2.49505043e-04,\n",
       "        1.00088120e-03, 1.24967098e-03, 1.07288361e-06, 5.00559807e-04,\n",
       "        2.25234032e-03, 2.50577927e-04, 1.50227547e-03, 1.25145912e-03,\n",
       "        1.75166130e-03, 1.00123882e-03, 7.50422478e-04, 2.49862671e-04,\n",
       "        1.50203705e-03, 7.51376152e-04, 3.75330448e-03, 1.00028515e-03,\n",
       "        1.00123882e-03, 6.00481033e-03, 2.49028206e-04, 9.99093056e-04,\n",
       "        5.00559807e-04, 2.25102901e-03, 1.75237656e-03, 1.07586384e-02,\n",
       "        8.50689411e-03, 6.00540638e-03, 8.50701332e-03, 3.25131416e-03,\n",
       "        2.00259686e-03, 9.00757313e-03, 1.75261497e-03, 1.25133991e-03,\n",
       "        2.25138664e-03, 8.75711441e-03, 1.00171566e-03, 1.00314617e-03,\n",
       "        4.00447845e-03, 1.00076199e-03, 1.50120258e-03, 2.50935555e-04,\n",
       "        7.25615025e-03, 3.25286388e-03, 2.50220299e-04, 1.32611990e-02,\n",
       "        1.75118446e-03, 6.75547123e-03, 2.50220299e-04, 5.00082970e-04,\n",
       "        5.00440598e-04, 1.50179863e-03, 3.00204754e-03, 1.24979019e-03,\n",
       "        4.00316715e-03, 7.50780106e-04, 2.50220299e-04, 3.00276279e-03,\n",
       "        2.50816345e-04, 3.00335884e-03, 5.01036644e-04, 1.30110979e-02,\n",
       "        4.99963760e-04, 5.50460815e-03, 2.75182724e-03, 2.25257874e-03,\n",
       "        1.00123882e-03, 5.75470924e-03, 6.50811195e-03, 7.00545311e-03,\n",
       "        5.00440598e-04, 4.99606133e-04, 7.25698471e-03, 1.50179863e-03,\n",
       "        2.49981880e-04, 5.00559807e-04, 7.50541687e-04, 3.50320339e-03,\n",
       "        1.00100040e-03, 5.00440598e-04, 4.99606133e-04, 1.10088587e-02,\n",
       "        7.50780106e-04, 4.25374508e-03, 1.50144100e-03, 1.19209290e-07,\n",
       "        2.25234032e-03, 1.25110149e-03, 3.75318527e-03, 1.22607946e-02,\n",
       "        2.38418579e-07, 1.75178051e-03, 1.25098228e-03, 7.50613213e-03,\n",
       "        2.50172615e-03, 2.50697136e-04, 5.01036644e-04, 5.96046448e-07,\n",
       "        2.50339508e-04, 5.00559807e-04, 6.75630569e-03, 1.75189972e-03,\n",
       "        2.00200081e-03, 4.99844551e-04, 5.00202179e-04, 7.50780106e-04,\n",
       "        3.25310230e-03, 2.50816345e-04, 5.25438786e-03, 1.50120258e-03,\n",
       "        5.00559807e-04, 2.25245953e-03, 5.00798225e-04, 1.25086308e-03,\n",
       "        1.25074387e-03, 2.50124931e-03, 2.00140476e-03, 7.50303268e-04,\n",
       "        2.50458717e-04, 4.75442410e-03, 2.00116634e-03, 7.51376152e-04,\n",
       "        2.49624252e-04, 2.38418579e-07, 5.00440598e-04, 5.00082970e-04,\n",
       "        2.38418579e-07, 5.00202179e-04, 1.00028515e-03, 5.00440598e-04,\n",
       "        3.75378132e-03, 2.38418579e-07, 2.50935555e-04, 2.50577927e-04,\n",
       "        4.75382805e-03, 5.75554371e-03, 2.50577927e-04, 4.25386429e-03,\n",
       "        1.00040436e-03, 2.50196457e-03, 2.25210190e-03, 2.50256062e-03,\n",
       "        2.50279903e-03, 6.50584698e-03, 2.49862671e-04, 7.51018524e-04,\n",
       "        7.50541687e-04, 1.75118446e-03, 6.75606728e-03, 1.07597113e-02,\n",
       "        1.50108337e-03, 1.43051147e-06, 5.00798225e-04, 2.49981880e-04,\n",
       "        7.51137733e-04, 7.51495361e-04, 5.00559807e-04, 5.00202179e-04,\n",
       "        1.75166130e-03, 1.25133991e-03, 2.49981880e-04, 1.00088120e-03,\n",
       "        2.50220299e-04, 7.50303268e-04, 2.50339508e-04, 1.00040436e-03,\n",
       "        2.50220299e-04, 2.00176239e-03, 1.75094604e-03, 3.57627869e-07,\n",
       "        7.50660896e-04, 1.50156021e-03, 4.25434113e-03, 3.00228596e-03,\n",
       "        1.00147724e-03, 2.00176239e-03, 5.00321388e-04, 9.53674316e-07,\n",
       "        2.00104713e-03, 1.00100040e-03, 5.00559807e-04, 2.49862671e-04,\n",
       "        1.25098228e-03, 4.76837158e-07, 2.25198269e-03, 1.25133991e-03,\n",
       "        1.25157833e-03, 2.50339508e-04, 2.49028206e-04, 7.50899315e-04,\n",
       "        2.66551971e-04, 1.25122070e-03, 4.99963760e-04, 9.99927521e-04,\n",
       "        5.00559807e-04, 8.07523727e-04, 1.19209290e-07, 1.00135803e-03,\n",
       "        2.50816345e-04, 2.50220299e-04, 2.50339508e-04, 2.50458717e-04,\n",
       "        2.49981880e-04, 4.99963760e-04, 1.25110149e-03, 2.50816345e-04,\n",
       "        7.50660896e-04, 1.00064278e-03, 1.00088120e-03, 7.50780106e-04,\n",
       "        2.75230408e-03, 2.50220299e-04, 5.00202179e-04, 5.00082970e-04,\n",
       "        4.00340557e-03, 7.15255737e-07, 2.75242329e-03, 7.50660896e-04,\n",
       "        2.50208378e-03, 1.40119791e-02, 3.57627869e-07, 7.50899315e-04,\n",
       "        2.50816345e-04, 2.38418579e-07, 7.51018524e-04, 3.00228596e-03,\n",
       "        2.50220299e-04, 5.00559807e-04, 1.19209290e-07, 7.52210617e-04,\n",
       "        1.00064278e-03, 5.00559807e-04, 0.00000000e+00, 2.50816345e-04]),\n",
       " 'param_classifier': masked_array(data=[MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__activation': masked_array(data=['logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__alpha': masked_array(data=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__hidden_layer_sizes': masked_array(data=[(5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__learning_rate': masked_array(data=['constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'}],\n",
       " 'split0_test_recall_micro': array([9.908e-01, 9.916e-01, 9.936e-01, 9.936e-01, 9.936e-01, 9.928e-01,\n",
       "        9.928e-01, 9.936e-01, 9.928e-01, 9.920e-01, 9.920e-01, 9.912e-01,\n",
       "        9.932e-01, 9.932e-01, 9.924e-01, 9.936e-01, 9.920e-01, 9.932e-01,\n",
       "        9.912e-01, 9.916e-01, 9.920e-01, 9.932e-01, 9.928e-01, 9.924e-01,\n",
       "        9.924e-01, 9.924e-01, 9.928e-01, 9.924e-01, 9.940e-01, 9.936e-01,\n",
       "        9.920e-01, 9.932e-01, 9.936e-01, 9.932e-01, 9.924e-01, 9.932e-01,\n",
       "        9.928e-01, 9.928e-01, 9.928e-01, 9.924e-01, 9.920e-01, 9.928e-01,\n",
       "        9.920e-01, 9.920e-01, 9.928e-01, 9.928e-01, 9.924e-01, 9.920e-01,\n",
       "        9.940e-01, 9.932e-01, 9.944e-01, 9.908e-01, 9.960e-01, 9.952e-01,\n",
       "        9.940e-01, 9.944e-01, 9.928e-01, 9.920e-01, 9.916e-01, 9.928e-01,\n",
       "        9.916e-01, 9.928e-01, 9.972e-01, 9.944e-01, 9.932e-01, 9.912e-01,\n",
       "        9.916e-01, 9.932e-01, 9.936e-01, 9.928e-01, 9.924e-01, 9.932e-01,\n",
       "        9.928e-01, 9.928e-01, 9.936e-01, 9.928e-01, 9.908e-01, 9.920e-01,\n",
       "        9.920e-01, 9.912e-01, 9.912e-01, 9.920e-01, 9.924e-01, 9.936e-01,\n",
       "        9.936e-01, 9.928e-01, 9.932e-01, 9.928e-01, 9.928e-01, 9.920e-01,\n",
       "        9.924e-01, 9.928e-01, 9.920e-01, 9.932e-01, 9.920e-01, 9.924e-01,\n",
       "        7.848e-01, 2.328e-01, 7.884e-01, 9.572e-01, 5.816e-01, 9.636e-01,\n",
       "        9.556e-01, 5.640e-01, 9.548e-01, 9.560e-01, 5.400e-01, 9.560e-01,\n",
       "        7.828e-01, 2.356e-01, 8.032e-01, 9.532e-01, 1.848e-01, 9.600e-01,\n",
       "        9.540e-01, 1.848e-01, 9.596e-01, 9.576e-01, 2.036e-01, 9.532e-01,\n",
       "        8.024e-01, 2.840e-02, 8.016e-01, 9.676e-01, 8.000e-04, 9.612e-01,\n",
       "        9.560e-01, 5.816e-01, 9.592e-01, 9.516e-01, 5.720e-01, 9.520e-01,\n",
       "        7.852e-01, 2.096e-01, 7.848e-01, 9.620e-01, 5.816e-01, 9.304e-01,\n",
       "        9.556e-01, 1.508e-01, 9.576e-01, 9.516e-01, 2.588e-01, 9.524e-01,\n",
       "        9.560e-01, 5.188e-01, 9.540e-01, 9.536e-01, 1.772e-01, 9.540e-01,\n",
       "        9.532e-01, 4.720e-02, 9.532e-01, 9.540e-01, 1.832e-01, 9.520e-01,\n",
       "        9.504e-01, 1.848e-01, 9.496e-01, 9.524e-01, 5.488e-01, 9.544e-01,\n",
       "        9.524e-01, 4.752e-01, 9.520e-01, 9.516e-01, 4.736e-01, 9.536e-01,\n",
       "        9.476e-01, 1.560e-02, 9.512e-01, 9.520e-01, 6.360e-02, 9.520e-01,\n",
       "        9.524e-01, 2.512e-01, 9.536e-01, 9.532e-01, 4.924e-01, 9.540e-01,\n",
       "        9.512e-01, 9.840e-02, 9.516e-01, 9.488e-01, 6.412e-01, 9.488e-01,\n",
       "        9.500e-01, 1.192e-01, 9.524e-01, 9.520e-01, 9.240e-02, 9.520e-01,\n",
       "        9.580e-01, 9.592e-01, 9.592e-01, 9.616e-01, 9.608e-01, 9.608e-01,\n",
       "        9.628e-01, 9.624e-01, 9.628e-01, 9.636e-01, 9.632e-01, 9.636e-01,\n",
       "        9.596e-01, 9.572e-01, 9.588e-01, 9.604e-01, 9.608e-01, 9.604e-01,\n",
       "        9.636e-01, 9.632e-01, 9.628e-01, 9.640e-01, 9.648e-01, 9.640e-01,\n",
       "        9.600e-01, 9.560e-01, 9.584e-01, 9.604e-01, 9.600e-01, 9.604e-01,\n",
       "        9.628e-01, 9.632e-01, 9.624e-01, 9.632e-01, 9.640e-01, 9.628e-01,\n",
       "        9.576e-01, 9.588e-01, 9.572e-01, 9.596e-01, 9.596e-01, 9.596e-01,\n",
       "        9.596e-01, 9.608e-01, 9.596e-01, 9.608e-01, 9.620e-01, 9.608e-01,\n",
       "        9.844e-01, 9.796e-01, 9.684e-01, 9.856e-01, 9.860e-01, 9.840e-01,\n",
       "        9.868e-01, 9.868e-01, 9.860e-01, 9.884e-01, 9.868e-01, 9.868e-01,\n",
       "        9.792e-01, 9.812e-01, 9.640e-01, 9.868e-01, 9.844e-01, 9.848e-01,\n",
       "        9.856e-01, 9.872e-01, 9.868e-01, 9.876e-01, 9.880e-01, 9.892e-01,\n",
       "        9.800e-01, 9.660e-01, 9.788e-01, 9.848e-01, 9.832e-01, 9.836e-01,\n",
       "        9.872e-01, 9.864e-01, 9.872e-01, 9.872e-01, 9.880e-01, 9.872e-01,\n",
       "        9.780e-01, 9.796e-01, 9.800e-01, 9.836e-01, 9.848e-01, 9.840e-01,\n",
       "        9.864e-01, 9.836e-01, 9.832e-01, 9.868e-01, 9.864e-01, 9.868e-01]),\n",
       " 'split1_test_recall_micro': array([0.9924, 0.9948, 0.9964, 0.9968, 0.996 , 0.9948, 0.994 , 0.996 ,\n",
       "        0.9956, 0.9932, 0.9944, 0.9944, 0.9932, 0.9952, 0.9932, 0.994 ,\n",
       "        0.9944, 0.9952, 0.9952, 0.9948, 0.9956, 0.9932, 0.9936, 0.9936,\n",
       "        0.9932, 0.9932, 0.992 , 0.9928, 0.9936, 0.9932, 0.9932, 0.9932,\n",
       "        0.9916, 0.9932, 0.994 , 0.9928, 0.9944, 0.9956, 0.9944, 0.9948,\n",
       "        0.9932, 0.9944, 0.9932, 0.9932, 0.9932, 0.9932, 0.9932, 0.9932,\n",
       "        0.9948, 0.996 , 0.9972, 0.9944, 0.9972, 0.994 , 0.996 , 0.996 ,\n",
       "        0.9968, 0.9964, 0.9944, 0.9952, 0.9964, 0.996 , 0.9964, 0.9944,\n",
       "        0.994 , 0.9964, 0.9964, 0.996 , 0.996 , 0.9952, 0.9956, 0.9964,\n",
       "        0.9952, 0.9936, 0.9932, 0.9948, 0.9952, 0.996 , 0.9952, 0.994 ,\n",
       "        0.994 , 0.9944, 0.996 , 0.9936, 0.9936, 0.9924, 0.9932, 0.9932,\n",
       "        0.9936, 0.9932, 0.994 , 0.9932, 0.9936, 0.9932, 0.9932, 0.9932,\n",
       "        0.934 , 0.3408, 0.786 , 0.9632, 0.6092, 0.9584, 0.9508, 0.5812,\n",
       "        0.9632, 0.9564, 0.4356, 0.9524, 0.7804, 0.51  , 0.7876, 0.9568,\n",
       "        0.5812, 0.956 , 0.9568, 0.532 , 0.956 , 0.9488, 0.2328, 0.9552,\n",
       "        0.782 , 0.5812, 0.7836, 0.9584, 0.1852, 0.9616, 0.9492, 0.5812,\n",
       "        0.9624, 0.9528, 0.5772, 0.9528, 0.7672, 0.5716, 0.8076, 0.9632,\n",
       "        0.2092, 0.962 , 0.954 , 0.466 , 0.9504, 0.9496, 0.5812, 0.9532,\n",
       "        0.9492, 0.4736, 0.9448, 0.9428, 0.0524, 0.9524, 0.9452, 0.2656,\n",
       "        0.952 , 0.9504, 0.2624, 0.9508, 0.9524, 0.448 , 0.9464, 0.9488,\n",
       "        0.2052, 0.9492, 0.9484, 0.23  , 0.9504, 0.9476, 0.3076, 0.9468,\n",
       "        0.9532, 0.1292, 0.9492, 0.9488, 0.5756, 0.9468, 0.9468, 0.534 ,\n",
       "        0.9524, 0.948 , 0.5044, 0.9472, 0.9464, 0.6052, 0.9516, 0.9496,\n",
       "        0.486 , 0.9464, 0.9488, 0.262 , 0.952 , 0.9492, 0.772 , 0.9496,\n",
       "        0.9564, 0.9568, 0.958 , 0.9624, 0.958 , 0.9608, 0.964 , 0.9612,\n",
       "        0.9632, 0.9652, 0.9656, 0.9648, 0.956 , 0.9564, 0.952 , 0.9568,\n",
       "        0.9608, 0.96  , 0.9648, 0.9628, 0.9644, 0.9652, 0.966 , 0.9644,\n",
       "        0.9564, 0.956 , 0.9568, 0.9612, 0.9596, 0.9612, 0.9612, 0.964 ,\n",
       "        0.9644, 0.9652, 0.9636, 0.9648, 0.9528, 0.954 , 0.952 , 0.9572,\n",
       "        0.9564, 0.9568, 0.9608, 0.958 , 0.96  , 0.9624, 0.9604, 0.958 ,\n",
       "        0.9876, 0.9832, 0.9868, 0.9884, 0.9892, 0.9888, 0.9888, 0.9912,\n",
       "        0.9892, 0.9904, 0.9912, 0.9912, 0.9828, 0.982 , 0.9644, 0.9872,\n",
       "        0.9884, 0.9868, 0.9904, 0.9868, 0.9884, 0.9912, 0.9908, 0.9912,\n",
       "        0.9688, 0.9796, 0.9824, 0.988 , 0.988 , 0.988 , 0.9896, 0.99  ,\n",
       "        0.9864, 0.9912, 0.9908, 0.9908, 0.9864, 0.9836, 0.9832, 0.9872,\n",
       "        0.9836, 0.9868, 0.988 , 0.988 , 0.9884, 0.9892, 0.9896, 0.9892]),\n",
       " 'mean_test_recall_micro': array([0.9916, 0.9932, 0.995 , 0.9952, 0.9948, 0.9938, 0.9934, 0.9948,\n",
       "        0.9942, 0.9926, 0.9932, 0.9928, 0.9932, 0.9942, 0.9928, 0.9938,\n",
       "        0.9932, 0.9942, 0.9932, 0.9932, 0.9938, 0.9932, 0.9932, 0.993 ,\n",
       "        0.9928, 0.9928, 0.9924, 0.9926, 0.9938, 0.9934, 0.9926, 0.9932,\n",
       "        0.9926, 0.9932, 0.9932, 0.993 , 0.9936, 0.9942, 0.9936, 0.9936,\n",
       "        0.9926, 0.9936, 0.9926, 0.9926, 0.993 , 0.993 , 0.9928, 0.9926,\n",
       "        0.9944, 0.9946, 0.9958, 0.9926, 0.9966, 0.9946, 0.995 , 0.9952,\n",
       "        0.9948, 0.9942, 0.993 , 0.994 , 0.994 , 0.9944, 0.9968, 0.9944,\n",
       "        0.9936, 0.9938, 0.994 , 0.9946, 0.9948, 0.994 , 0.994 , 0.9948,\n",
       "        0.994 , 0.9932, 0.9934, 0.9938, 0.993 , 0.994 , 0.9936, 0.9926,\n",
       "        0.9926, 0.9932, 0.9942, 0.9936, 0.9936, 0.9926, 0.9932, 0.993 ,\n",
       "        0.9932, 0.9926, 0.9932, 0.993 , 0.9928, 0.9932, 0.9926, 0.9928,\n",
       "        0.8594, 0.2868, 0.7872, 0.9602, 0.5954, 0.961 , 0.9532, 0.5726,\n",
       "        0.959 , 0.9562, 0.4878, 0.9542, 0.7816, 0.3728, 0.7954, 0.955 ,\n",
       "        0.383 , 0.958 , 0.9554, 0.3584, 0.9578, 0.9532, 0.2182, 0.9542,\n",
       "        0.7922, 0.3048, 0.7926, 0.963 , 0.093 , 0.9614, 0.9526, 0.5814,\n",
       "        0.9608, 0.9522, 0.5746, 0.9524, 0.7762, 0.3906, 0.7962, 0.9626,\n",
       "        0.3954, 0.9462, 0.9548, 0.3084, 0.954 , 0.9506, 0.42  , 0.9528,\n",
       "        0.9526, 0.4962, 0.9494, 0.9482, 0.1148, 0.9532, 0.9492, 0.1564,\n",
       "        0.9526, 0.9522, 0.2228, 0.9514, 0.9514, 0.3164, 0.948 , 0.9506,\n",
       "        0.377 , 0.9518, 0.9504, 0.3526, 0.9512, 0.9496, 0.3906, 0.9502,\n",
       "        0.9504, 0.0724, 0.9502, 0.9504, 0.3196, 0.9494, 0.9496, 0.3926,\n",
       "        0.953 , 0.9506, 0.4984, 0.9506, 0.9488, 0.3518, 0.9516, 0.9492,\n",
       "        0.5636, 0.9476, 0.9494, 0.1906, 0.9522, 0.9506, 0.4322, 0.9508,\n",
       "        0.9572, 0.958 , 0.9586, 0.962 , 0.9594, 0.9608, 0.9634, 0.9618,\n",
       "        0.963 , 0.9644, 0.9644, 0.9642, 0.9578, 0.9568, 0.9554, 0.9586,\n",
       "        0.9608, 0.9602, 0.9642, 0.963 , 0.9636, 0.9646, 0.9654, 0.9642,\n",
       "        0.9582, 0.956 , 0.9576, 0.9608, 0.9598, 0.9608, 0.962 , 0.9636,\n",
       "        0.9634, 0.9642, 0.9638, 0.9638, 0.9552, 0.9564, 0.9546, 0.9584,\n",
       "        0.958 , 0.9582, 0.9602, 0.9594, 0.9598, 0.9616, 0.9612, 0.9594,\n",
       "        0.986 , 0.9814, 0.9776, 0.987 , 0.9876, 0.9864, 0.9878, 0.989 ,\n",
       "        0.9876, 0.9894, 0.989 , 0.989 , 0.981 , 0.9816, 0.9642, 0.987 ,\n",
       "        0.9864, 0.9858, 0.988 , 0.987 , 0.9876, 0.9894, 0.9894, 0.9902,\n",
       "        0.9744, 0.9728, 0.9806, 0.9864, 0.9856, 0.9858, 0.9884, 0.9882,\n",
       "        0.9868, 0.9892, 0.9894, 0.989 , 0.9822, 0.9816, 0.9816, 0.9854,\n",
       "        0.9842, 0.9854, 0.9872, 0.9858, 0.9858, 0.988 , 0.988 , 0.988 ]),\n",
       " 'std_test_recall_micro': array([8.000e-04, 1.600e-03, 1.400e-03, 1.600e-03, 1.200e-03, 1.000e-03,\n",
       "        6.000e-04, 1.200e-03, 1.400e-03, 6.000e-04, 1.200e-03, 1.600e-03,\n",
       "        0.000e+00, 1.000e-03, 4.000e-04, 2.000e-04, 1.200e-03, 1.000e-03,\n",
       "        2.000e-03, 1.600e-03, 1.800e-03, 0.000e+00, 4.000e-04, 6.000e-04,\n",
       "        4.000e-04, 4.000e-04, 4.000e-04, 2.000e-04, 2.000e-04, 2.000e-04,\n",
       "        6.000e-04, 0.000e+00, 1.000e-03, 0.000e+00, 8.000e-04, 2.000e-04,\n",
       "        8.000e-04, 1.400e-03, 8.000e-04, 1.200e-03, 6.000e-04, 8.000e-04,\n",
       "        6.000e-04, 6.000e-04, 2.000e-04, 2.000e-04, 4.000e-04, 6.000e-04,\n",
       "        4.000e-04, 1.400e-03, 1.400e-03, 1.800e-03, 6.000e-04, 6.000e-04,\n",
       "        1.000e-03, 8.000e-04, 2.000e-03, 2.200e-03, 1.400e-03, 1.200e-03,\n",
       "        2.400e-03, 1.600e-03, 4.000e-04, 0.000e+00, 4.000e-04, 2.600e-03,\n",
       "        2.400e-03, 1.400e-03, 1.200e-03, 1.200e-03, 1.600e-03, 1.600e-03,\n",
       "        1.200e-03, 4.000e-04, 2.000e-04, 1.000e-03, 2.200e-03, 2.000e-03,\n",
       "        1.600e-03, 1.400e-03, 1.400e-03, 1.200e-03, 1.800e-03, 0.000e+00,\n",
       "        0.000e+00, 2.000e-04, 0.000e+00, 2.000e-04, 4.000e-04, 6.000e-04,\n",
       "        8.000e-04, 2.000e-04, 8.000e-04, 0.000e+00, 6.000e-04, 4.000e-04,\n",
       "        7.460e-02, 5.400e-02, 1.200e-03, 3.000e-03, 1.380e-02, 2.600e-03,\n",
       "        2.400e-03, 8.600e-03, 4.200e-03, 2.000e-04, 5.220e-02, 1.800e-03,\n",
       "        1.200e-03, 1.372e-01, 7.800e-03, 1.800e-03, 1.982e-01, 2.000e-03,\n",
       "        1.400e-03, 1.736e-01, 1.800e-03, 4.400e-03, 1.460e-02, 1.000e-03,\n",
       "        1.020e-02, 2.764e-01, 9.000e-03, 4.600e-03, 9.220e-02, 2.000e-04,\n",
       "        3.400e-03, 2.000e-04, 1.600e-03, 6.000e-04, 2.600e-03, 4.000e-04,\n",
       "        9.000e-03, 1.810e-01, 1.140e-02, 6.000e-04, 1.862e-01, 1.580e-02,\n",
       "        8.000e-04, 1.576e-01, 3.600e-03, 1.000e-03, 1.612e-01, 4.000e-04,\n",
       "        3.400e-03, 2.260e-02, 4.600e-03, 5.400e-03, 6.240e-02, 8.000e-04,\n",
       "        4.000e-03, 1.092e-01, 6.000e-04, 1.800e-03, 3.960e-02, 6.000e-04,\n",
       "        1.000e-03, 1.316e-01, 1.600e-03, 1.800e-03, 1.718e-01, 2.600e-03,\n",
       "        2.000e-03, 1.226e-01, 8.000e-04, 2.000e-03, 8.300e-02, 3.400e-03,\n",
       "        2.800e-03, 5.680e-02, 1.000e-03, 1.600e-03, 2.560e-01, 2.600e-03,\n",
       "        2.800e-03, 1.414e-01, 6.000e-04, 2.600e-03, 6.000e-03, 3.400e-03,\n",
       "        2.400e-03, 2.534e-01, 0.000e+00, 4.000e-04, 7.760e-02, 1.200e-03,\n",
       "        6.000e-04, 7.140e-02, 2.000e-04, 1.400e-03, 3.398e-01, 1.200e-03,\n",
       "        8.000e-04, 1.200e-03, 6.000e-04, 4.000e-04, 1.400e-03, 0.000e+00,\n",
       "        6.000e-04, 6.000e-04, 2.000e-04, 8.000e-04, 1.200e-03, 6.000e-04,\n",
       "        1.800e-03, 4.000e-04, 3.400e-03, 1.800e-03, 0.000e+00, 2.000e-04,\n",
       "        6.000e-04, 2.000e-04, 8.000e-04, 6.000e-04, 6.000e-04, 2.000e-04,\n",
       "        1.800e-03, 0.000e+00, 8.000e-04, 4.000e-04, 2.000e-04, 4.000e-04,\n",
       "        8.000e-04, 4.000e-04, 1.000e-03, 1.000e-03, 2.000e-04, 1.000e-03,\n",
       "        2.400e-03, 2.400e-03, 2.600e-03, 1.200e-03, 1.600e-03, 1.400e-03,\n",
       "        6.000e-04, 1.400e-03, 2.000e-04, 8.000e-04, 8.000e-04, 1.400e-03,\n",
       "        1.600e-03, 1.800e-03, 9.200e-03, 1.400e-03, 1.600e-03, 2.400e-03,\n",
       "        1.000e-03, 2.200e-03, 1.600e-03, 1.000e-03, 2.200e-03, 2.200e-03,\n",
       "        1.800e-03, 4.000e-04, 2.000e-04, 2.000e-04, 2.000e-03, 1.000e-03,\n",
       "        2.400e-03, 2.000e-04, 8.000e-04, 1.800e-03, 1.400e-03, 1.000e-03,\n",
       "        5.600e-03, 6.800e-03, 1.800e-03, 1.600e-03, 2.400e-03, 2.200e-03,\n",
       "        1.200e-03, 1.800e-03, 4.000e-04, 2.000e-03, 1.400e-03, 1.800e-03,\n",
       "        4.200e-03, 2.000e-03, 1.600e-03, 1.800e-03, 6.000e-04, 1.400e-03,\n",
       "        8.000e-04, 2.200e-03, 2.600e-03, 1.200e-03, 1.600e-03, 1.200e-03]),\n",
       " 'rank_test_recall_micro': array([ 96,  49,   6,   4,   8,  32,  46,   8,  19,  82,  54,  75,  54,\n",
       "         19,  75,  32,  54,  19,  54,  49,  32,  54,  49,  66,  75,  75,\n",
       "         95,  82,  32,  46,  82,  54,  81,  54,  54,  66,  38,  19,  38,\n",
       "         38,  82,  38,  82,  82,  66,  66,  75,  82,  16,  13,   3,  82,\n",
       "          2,  13,   6,   5,   8,  19,  66,  25,  25,  16,   1,  16,  38,\n",
       "         32,  25,  13,   8,  25,  25,  12,  25,  49,  46,  32,  66,  25,\n",
       "         38,  82,  82,  54,  19,  38,  38,  82,  54,  66,  49,  82,  54,\n",
       "         66,  74,  54,  82,  75, 249, 281, 254, 175, 257, 169, 209, 260,\n",
       "        183, 198, 264, 207, 255, 273, 251, 203, 271, 189, 200, 274, 192,\n",
       "        209, 283, 206, 253, 280, 252, 159, 287, 167, 214, 258, 170, 218,\n",
       "        259, 217, 256, 269, 250, 162, 267, 248, 204, 279, 208, 227, 266,\n",
       "        213, 214, 263, 239, 245, 286, 209, 242, 285, 214, 218, 282, 223,\n",
       "        223, 278, 246, 227, 272, 221, 232, 275, 225, 237, 269, 236, 232,\n",
       "        288, 235, 234, 277, 239, 237, 268, 212, 227, 262, 227, 244, 276,\n",
       "        222, 242, 261, 247, 239, 284, 218, 227, 265, 226, 195, 189, 184,\n",
       "        163, 180, 173, 157, 165, 160, 146, 146, 148, 192, 196, 200, 184,\n",
       "        173, 175, 148, 160, 155, 145, 144, 148, 187, 199, 194, 170, 178,\n",
       "        170, 163, 155, 157, 148, 153, 153, 202, 197, 205, 186, 189, 187,\n",
       "        175, 180, 178, 166, 168, 180, 125, 138, 141, 118, 114, 122, 113,\n",
       "        103, 114, 101, 103, 103, 139, 135, 148, 118, 122, 126, 109, 118,\n",
       "        114,  98,  98,  97, 142, 143, 140, 122, 130, 126, 107, 108, 121,\n",
       "        102,  98, 103, 134, 135, 135, 131, 133, 131, 117, 126, 126, 109,\n",
       "        109, 109]),\n",
       " 'split0_test_f1_micro': array([9.908e-01, 9.916e-01, 9.936e-01, 9.936e-01, 9.936e-01, 9.928e-01,\n",
       "        9.928e-01, 9.936e-01, 9.928e-01, 9.920e-01, 9.920e-01, 9.912e-01,\n",
       "        9.932e-01, 9.932e-01, 9.924e-01, 9.936e-01, 9.920e-01, 9.932e-01,\n",
       "        9.912e-01, 9.916e-01, 9.920e-01, 9.932e-01, 9.928e-01, 9.924e-01,\n",
       "        9.924e-01, 9.924e-01, 9.928e-01, 9.924e-01, 9.940e-01, 9.936e-01,\n",
       "        9.920e-01, 9.932e-01, 9.936e-01, 9.932e-01, 9.924e-01, 9.932e-01,\n",
       "        9.928e-01, 9.928e-01, 9.928e-01, 9.924e-01, 9.920e-01, 9.928e-01,\n",
       "        9.920e-01, 9.920e-01, 9.928e-01, 9.928e-01, 9.924e-01, 9.920e-01,\n",
       "        9.940e-01, 9.932e-01, 9.944e-01, 9.908e-01, 9.960e-01, 9.952e-01,\n",
       "        9.940e-01, 9.944e-01, 9.928e-01, 9.920e-01, 9.916e-01, 9.928e-01,\n",
       "        9.916e-01, 9.928e-01, 9.972e-01, 9.944e-01, 9.932e-01, 9.912e-01,\n",
       "        9.916e-01, 9.932e-01, 9.936e-01, 9.928e-01, 9.924e-01, 9.932e-01,\n",
       "        9.928e-01, 9.928e-01, 9.936e-01, 9.928e-01, 9.908e-01, 9.920e-01,\n",
       "        9.920e-01, 9.912e-01, 9.912e-01, 9.920e-01, 9.924e-01, 9.936e-01,\n",
       "        9.936e-01, 9.928e-01, 9.932e-01, 9.928e-01, 9.928e-01, 9.920e-01,\n",
       "        9.924e-01, 9.928e-01, 9.920e-01, 9.932e-01, 9.920e-01, 9.924e-01,\n",
       "        7.848e-01, 2.328e-01, 7.884e-01, 9.572e-01, 5.816e-01, 9.636e-01,\n",
       "        9.556e-01, 5.640e-01, 9.548e-01, 9.560e-01, 5.400e-01, 9.560e-01,\n",
       "        7.828e-01, 2.356e-01, 8.032e-01, 9.532e-01, 1.848e-01, 9.600e-01,\n",
       "        9.540e-01, 1.848e-01, 9.596e-01, 9.576e-01, 2.036e-01, 9.532e-01,\n",
       "        8.024e-01, 2.840e-02, 8.016e-01, 9.676e-01, 8.000e-04, 9.612e-01,\n",
       "        9.560e-01, 5.816e-01, 9.592e-01, 9.516e-01, 5.720e-01, 9.520e-01,\n",
       "        7.852e-01, 2.096e-01, 7.848e-01, 9.620e-01, 5.816e-01, 9.304e-01,\n",
       "        9.556e-01, 1.508e-01, 9.576e-01, 9.516e-01, 2.588e-01, 9.524e-01,\n",
       "        9.560e-01, 5.188e-01, 9.540e-01, 9.536e-01, 1.772e-01, 9.540e-01,\n",
       "        9.532e-01, 4.720e-02, 9.532e-01, 9.540e-01, 1.832e-01, 9.520e-01,\n",
       "        9.504e-01, 1.848e-01, 9.496e-01, 9.524e-01, 5.488e-01, 9.544e-01,\n",
       "        9.524e-01, 4.752e-01, 9.520e-01, 9.516e-01, 4.736e-01, 9.536e-01,\n",
       "        9.476e-01, 1.560e-02, 9.512e-01, 9.520e-01, 6.360e-02, 9.520e-01,\n",
       "        9.524e-01, 2.512e-01, 9.536e-01, 9.532e-01, 4.924e-01, 9.540e-01,\n",
       "        9.512e-01, 9.840e-02, 9.516e-01, 9.488e-01, 6.412e-01, 9.488e-01,\n",
       "        9.500e-01, 1.192e-01, 9.524e-01, 9.520e-01, 9.240e-02, 9.520e-01,\n",
       "        9.580e-01, 9.592e-01, 9.592e-01, 9.616e-01, 9.608e-01, 9.608e-01,\n",
       "        9.628e-01, 9.624e-01, 9.628e-01, 9.636e-01, 9.632e-01, 9.636e-01,\n",
       "        9.596e-01, 9.572e-01, 9.588e-01, 9.604e-01, 9.608e-01, 9.604e-01,\n",
       "        9.636e-01, 9.632e-01, 9.628e-01, 9.640e-01, 9.648e-01, 9.640e-01,\n",
       "        9.600e-01, 9.560e-01, 9.584e-01, 9.604e-01, 9.600e-01, 9.604e-01,\n",
       "        9.628e-01, 9.632e-01, 9.624e-01, 9.632e-01, 9.640e-01, 9.628e-01,\n",
       "        9.576e-01, 9.588e-01, 9.572e-01, 9.596e-01, 9.596e-01, 9.596e-01,\n",
       "        9.596e-01, 9.608e-01, 9.596e-01, 9.608e-01, 9.620e-01, 9.608e-01,\n",
       "        9.844e-01, 9.796e-01, 9.684e-01, 9.856e-01, 9.860e-01, 9.840e-01,\n",
       "        9.868e-01, 9.868e-01, 9.860e-01, 9.884e-01, 9.868e-01, 9.868e-01,\n",
       "        9.792e-01, 9.812e-01, 9.640e-01, 9.868e-01, 9.844e-01, 9.848e-01,\n",
       "        9.856e-01, 9.872e-01, 9.868e-01, 9.876e-01, 9.880e-01, 9.892e-01,\n",
       "        9.800e-01, 9.660e-01, 9.788e-01, 9.848e-01, 9.832e-01, 9.836e-01,\n",
       "        9.872e-01, 9.864e-01, 9.872e-01, 9.872e-01, 9.880e-01, 9.872e-01,\n",
       "        9.780e-01, 9.796e-01, 9.800e-01, 9.836e-01, 9.848e-01, 9.840e-01,\n",
       "        9.864e-01, 9.836e-01, 9.832e-01, 9.868e-01, 9.864e-01, 9.868e-01]),\n",
       " 'split1_test_f1_micro': array([0.9924, 0.9948, 0.9964, 0.9968, 0.996 , 0.9948, 0.994 , 0.996 ,\n",
       "        0.9956, 0.9932, 0.9944, 0.9944, 0.9932, 0.9952, 0.9932, 0.994 ,\n",
       "        0.9944, 0.9952, 0.9952, 0.9948, 0.9956, 0.9932, 0.9936, 0.9936,\n",
       "        0.9932, 0.9932, 0.992 , 0.9928, 0.9936, 0.9932, 0.9932, 0.9932,\n",
       "        0.9916, 0.9932, 0.994 , 0.9928, 0.9944, 0.9956, 0.9944, 0.9948,\n",
       "        0.9932, 0.9944, 0.9932, 0.9932, 0.9932, 0.9932, 0.9932, 0.9932,\n",
       "        0.9948, 0.996 , 0.9972, 0.9944, 0.9972, 0.994 , 0.996 , 0.996 ,\n",
       "        0.9968, 0.9964, 0.9944, 0.9952, 0.9964, 0.996 , 0.9964, 0.9944,\n",
       "        0.994 , 0.9964, 0.9964, 0.996 , 0.996 , 0.9952, 0.9956, 0.9964,\n",
       "        0.9952, 0.9936, 0.9932, 0.9948, 0.9952, 0.996 , 0.9952, 0.994 ,\n",
       "        0.994 , 0.9944, 0.996 , 0.9936, 0.9936, 0.9924, 0.9932, 0.9932,\n",
       "        0.9936, 0.9932, 0.994 , 0.9932, 0.9936, 0.9932, 0.9932, 0.9932,\n",
       "        0.934 , 0.3408, 0.786 , 0.9632, 0.6092, 0.9584, 0.9508, 0.5812,\n",
       "        0.9632, 0.9564, 0.4356, 0.9524, 0.7804, 0.51  , 0.7876, 0.9568,\n",
       "        0.5812, 0.956 , 0.9568, 0.532 , 0.956 , 0.9488, 0.2328, 0.9552,\n",
       "        0.782 , 0.5812, 0.7836, 0.9584, 0.1852, 0.9616, 0.9492, 0.5812,\n",
       "        0.9624, 0.9528, 0.5772, 0.9528, 0.7672, 0.5716, 0.8076, 0.9632,\n",
       "        0.2092, 0.962 , 0.954 , 0.466 , 0.9504, 0.9496, 0.5812, 0.9532,\n",
       "        0.9492, 0.4736, 0.9448, 0.9428, 0.0524, 0.9524, 0.9452, 0.2656,\n",
       "        0.952 , 0.9504, 0.2624, 0.9508, 0.9524, 0.448 , 0.9464, 0.9488,\n",
       "        0.2052, 0.9492, 0.9484, 0.23  , 0.9504, 0.9476, 0.3076, 0.9468,\n",
       "        0.9532, 0.1292, 0.9492, 0.9488, 0.5756, 0.9468, 0.9468, 0.534 ,\n",
       "        0.9524, 0.948 , 0.5044, 0.9472, 0.9464, 0.6052, 0.9516, 0.9496,\n",
       "        0.486 , 0.9464, 0.9488, 0.262 , 0.952 , 0.9492, 0.772 , 0.9496,\n",
       "        0.9564, 0.9568, 0.958 , 0.9624, 0.958 , 0.9608, 0.964 , 0.9612,\n",
       "        0.9632, 0.9652, 0.9656, 0.9648, 0.956 , 0.9564, 0.952 , 0.9568,\n",
       "        0.9608, 0.96  , 0.9648, 0.9628, 0.9644, 0.9652, 0.966 , 0.9644,\n",
       "        0.9564, 0.956 , 0.9568, 0.9612, 0.9596, 0.9612, 0.9612, 0.964 ,\n",
       "        0.9644, 0.9652, 0.9636, 0.9648, 0.9528, 0.954 , 0.952 , 0.9572,\n",
       "        0.9564, 0.9568, 0.9608, 0.958 , 0.96  , 0.9624, 0.9604, 0.958 ,\n",
       "        0.9876, 0.9832, 0.9868, 0.9884, 0.9892, 0.9888, 0.9888, 0.9912,\n",
       "        0.9892, 0.9904, 0.9912, 0.9912, 0.9828, 0.982 , 0.9644, 0.9872,\n",
       "        0.9884, 0.9868, 0.9904, 0.9868, 0.9884, 0.9912, 0.9908, 0.9912,\n",
       "        0.9688, 0.9796, 0.9824, 0.988 , 0.988 , 0.988 , 0.9896, 0.99  ,\n",
       "        0.9864, 0.9912, 0.9908, 0.9908, 0.9864, 0.9836, 0.9832, 0.9872,\n",
       "        0.9836, 0.9868, 0.988 , 0.988 , 0.9884, 0.9892, 0.9896, 0.9892]),\n",
       " 'mean_test_f1_micro': array([0.9916, 0.9932, 0.995 , 0.9952, 0.9948, 0.9938, 0.9934, 0.9948,\n",
       "        0.9942, 0.9926, 0.9932, 0.9928, 0.9932, 0.9942, 0.9928, 0.9938,\n",
       "        0.9932, 0.9942, 0.9932, 0.9932, 0.9938, 0.9932, 0.9932, 0.993 ,\n",
       "        0.9928, 0.9928, 0.9924, 0.9926, 0.9938, 0.9934, 0.9926, 0.9932,\n",
       "        0.9926, 0.9932, 0.9932, 0.993 , 0.9936, 0.9942, 0.9936, 0.9936,\n",
       "        0.9926, 0.9936, 0.9926, 0.9926, 0.993 , 0.993 , 0.9928, 0.9926,\n",
       "        0.9944, 0.9946, 0.9958, 0.9926, 0.9966, 0.9946, 0.995 , 0.9952,\n",
       "        0.9948, 0.9942, 0.993 , 0.994 , 0.994 , 0.9944, 0.9968, 0.9944,\n",
       "        0.9936, 0.9938, 0.994 , 0.9946, 0.9948, 0.994 , 0.994 , 0.9948,\n",
       "        0.994 , 0.9932, 0.9934, 0.9938, 0.993 , 0.994 , 0.9936, 0.9926,\n",
       "        0.9926, 0.9932, 0.9942, 0.9936, 0.9936, 0.9926, 0.9932, 0.993 ,\n",
       "        0.9932, 0.9926, 0.9932, 0.993 , 0.9928, 0.9932, 0.9926, 0.9928,\n",
       "        0.8594, 0.2868, 0.7872, 0.9602, 0.5954, 0.961 , 0.9532, 0.5726,\n",
       "        0.959 , 0.9562, 0.4878, 0.9542, 0.7816, 0.3728, 0.7954, 0.955 ,\n",
       "        0.383 , 0.958 , 0.9554, 0.3584, 0.9578, 0.9532, 0.2182, 0.9542,\n",
       "        0.7922, 0.3048, 0.7926, 0.963 , 0.093 , 0.9614, 0.9526, 0.5814,\n",
       "        0.9608, 0.9522, 0.5746, 0.9524, 0.7762, 0.3906, 0.7962, 0.9626,\n",
       "        0.3954, 0.9462, 0.9548, 0.3084, 0.954 , 0.9506, 0.42  , 0.9528,\n",
       "        0.9526, 0.4962, 0.9494, 0.9482, 0.1148, 0.9532, 0.9492, 0.1564,\n",
       "        0.9526, 0.9522, 0.2228, 0.9514, 0.9514, 0.3164, 0.948 , 0.9506,\n",
       "        0.377 , 0.9518, 0.9504, 0.3526, 0.9512, 0.9496, 0.3906, 0.9502,\n",
       "        0.9504, 0.0724, 0.9502, 0.9504, 0.3196, 0.9494, 0.9496, 0.3926,\n",
       "        0.953 , 0.9506, 0.4984, 0.9506, 0.9488, 0.3518, 0.9516, 0.9492,\n",
       "        0.5636, 0.9476, 0.9494, 0.1906, 0.9522, 0.9506, 0.4322, 0.9508,\n",
       "        0.9572, 0.958 , 0.9586, 0.962 , 0.9594, 0.9608, 0.9634, 0.9618,\n",
       "        0.963 , 0.9644, 0.9644, 0.9642, 0.9578, 0.9568, 0.9554, 0.9586,\n",
       "        0.9608, 0.9602, 0.9642, 0.963 , 0.9636, 0.9646, 0.9654, 0.9642,\n",
       "        0.9582, 0.956 , 0.9576, 0.9608, 0.9598, 0.9608, 0.962 , 0.9636,\n",
       "        0.9634, 0.9642, 0.9638, 0.9638, 0.9552, 0.9564, 0.9546, 0.9584,\n",
       "        0.958 , 0.9582, 0.9602, 0.9594, 0.9598, 0.9616, 0.9612, 0.9594,\n",
       "        0.986 , 0.9814, 0.9776, 0.987 , 0.9876, 0.9864, 0.9878, 0.989 ,\n",
       "        0.9876, 0.9894, 0.989 , 0.989 , 0.981 , 0.9816, 0.9642, 0.987 ,\n",
       "        0.9864, 0.9858, 0.988 , 0.987 , 0.9876, 0.9894, 0.9894, 0.9902,\n",
       "        0.9744, 0.9728, 0.9806, 0.9864, 0.9856, 0.9858, 0.9884, 0.9882,\n",
       "        0.9868, 0.9892, 0.9894, 0.989 , 0.9822, 0.9816, 0.9816, 0.9854,\n",
       "        0.9842, 0.9854, 0.9872, 0.9858, 0.9858, 0.988 , 0.988 , 0.988 ]),\n",
       " 'std_test_f1_micro': array([8.000e-04, 1.600e-03, 1.400e-03, 1.600e-03, 1.200e-03, 1.000e-03,\n",
       "        6.000e-04, 1.200e-03, 1.400e-03, 6.000e-04, 1.200e-03, 1.600e-03,\n",
       "        0.000e+00, 1.000e-03, 4.000e-04, 2.000e-04, 1.200e-03, 1.000e-03,\n",
       "        2.000e-03, 1.600e-03, 1.800e-03, 0.000e+00, 4.000e-04, 6.000e-04,\n",
       "        4.000e-04, 4.000e-04, 4.000e-04, 2.000e-04, 2.000e-04, 2.000e-04,\n",
       "        6.000e-04, 0.000e+00, 1.000e-03, 0.000e+00, 8.000e-04, 2.000e-04,\n",
       "        8.000e-04, 1.400e-03, 8.000e-04, 1.200e-03, 6.000e-04, 8.000e-04,\n",
       "        6.000e-04, 6.000e-04, 2.000e-04, 2.000e-04, 4.000e-04, 6.000e-04,\n",
       "        4.000e-04, 1.400e-03, 1.400e-03, 1.800e-03, 6.000e-04, 6.000e-04,\n",
       "        1.000e-03, 8.000e-04, 2.000e-03, 2.200e-03, 1.400e-03, 1.200e-03,\n",
       "        2.400e-03, 1.600e-03, 4.000e-04, 0.000e+00, 4.000e-04, 2.600e-03,\n",
       "        2.400e-03, 1.400e-03, 1.200e-03, 1.200e-03, 1.600e-03, 1.600e-03,\n",
       "        1.200e-03, 4.000e-04, 2.000e-04, 1.000e-03, 2.200e-03, 2.000e-03,\n",
       "        1.600e-03, 1.400e-03, 1.400e-03, 1.200e-03, 1.800e-03, 0.000e+00,\n",
       "        0.000e+00, 2.000e-04, 0.000e+00, 2.000e-04, 4.000e-04, 6.000e-04,\n",
       "        8.000e-04, 2.000e-04, 8.000e-04, 0.000e+00, 6.000e-04, 4.000e-04,\n",
       "        7.460e-02, 5.400e-02, 1.200e-03, 3.000e-03, 1.380e-02, 2.600e-03,\n",
       "        2.400e-03, 8.600e-03, 4.200e-03, 2.000e-04, 5.220e-02, 1.800e-03,\n",
       "        1.200e-03, 1.372e-01, 7.800e-03, 1.800e-03, 1.982e-01, 2.000e-03,\n",
       "        1.400e-03, 1.736e-01, 1.800e-03, 4.400e-03, 1.460e-02, 1.000e-03,\n",
       "        1.020e-02, 2.764e-01, 9.000e-03, 4.600e-03, 9.220e-02, 2.000e-04,\n",
       "        3.400e-03, 2.000e-04, 1.600e-03, 6.000e-04, 2.600e-03, 4.000e-04,\n",
       "        9.000e-03, 1.810e-01, 1.140e-02, 6.000e-04, 1.862e-01, 1.580e-02,\n",
       "        8.000e-04, 1.576e-01, 3.600e-03, 1.000e-03, 1.612e-01, 4.000e-04,\n",
       "        3.400e-03, 2.260e-02, 4.600e-03, 5.400e-03, 6.240e-02, 8.000e-04,\n",
       "        4.000e-03, 1.092e-01, 6.000e-04, 1.800e-03, 3.960e-02, 6.000e-04,\n",
       "        1.000e-03, 1.316e-01, 1.600e-03, 1.800e-03, 1.718e-01, 2.600e-03,\n",
       "        2.000e-03, 1.226e-01, 8.000e-04, 2.000e-03, 8.300e-02, 3.400e-03,\n",
       "        2.800e-03, 5.680e-02, 1.000e-03, 1.600e-03, 2.560e-01, 2.600e-03,\n",
       "        2.800e-03, 1.414e-01, 6.000e-04, 2.600e-03, 6.000e-03, 3.400e-03,\n",
       "        2.400e-03, 2.534e-01, 0.000e+00, 4.000e-04, 7.760e-02, 1.200e-03,\n",
       "        6.000e-04, 7.140e-02, 2.000e-04, 1.400e-03, 3.398e-01, 1.200e-03,\n",
       "        8.000e-04, 1.200e-03, 6.000e-04, 4.000e-04, 1.400e-03, 0.000e+00,\n",
       "        6.000e-04, 6.000e-04, 2.000e-04, 8.000e-04, 1.200e-03, 6.000e-04,\n",
       "        1.800e-03, 4.000e-04, 3.400e-03, 1.800e-03, 0.000e+00, 2.000e-04,\n",
       "        6.000e-04, 2.000e-04, 8.000e-04, 6.000e-04, 6.000e-04, 2.000e-04,\n",
       "        1.800e-03, 0.000e+00, 8.000e-04, 4.000e-04, 2.000e-04, 4.000e-04,\n",
       "        8.000e-04, 4.000e-04, 1.000e-03, 1.000e-03, 2.000e-04, 1.000e-03,\n",
       "        2.400e-03, 2.400e-03, 2.600e-03, 1.200e-03, 1.600e-03, 1.400e-03,\n",
       "        6.000e-04, 1.400e-03, 2.000e-04, 8.000e-04, 8.000e-04, 1.400e-03,\n",
       "        1.600e-03, 1.800e-03, 9.200e-03, 1.400e-03, 1.600e-03, 2.400e-03,\n",
       "        1.000e-03, 2.200e-03, 1.600e-03, 1.000e-03, 2.200e-03, 2.200e-03,\n",
       "        1.800e-03, 4.000e-04, 2.000e-04, 2.000e-04, 2.000e-03, 1.000e-03,\n",
       "        2.400e-03, 2.000e-04, 8.000e-04, 1.800e-03, 1.400e-03, 1.000e-03,\n",
       "        5.600e-03, 6.800e-03, 1.800e-03, 1.600e-03, 2.400e-03, 2.200e-03,\n",
       "        1.200e-03, 1.800e-03, 4.000e-04, 2.000e-03, 1.400e-03, 1.800e-03,\n",
       "        4.200e-03, 2.000e-03, 1.600e-03, 1.800e-03, 6.000e-04, 1.400e-03,\n",
       "        8.000e-04, 2.200e-03, 2.600e-03, 1.200e-03, 1.600e-03, 1.200e-03]),\n",
       " 'rank_test_f1_micro': array([ 96,  49,   6,   4,   8,  32,  46,   8,  19,  82,  54,  75,  54,\n",
       "         19,  75,  32,  54,  19,  54,  49,  32,  54,  49,  66,  75,  75,\n",
       "         95,  82,  32,  46,  82,  54,  81,  54,  54,  66,  38,  19,  38,\n",
       "         38,  82,  38,  82,  82,  66,  66,  75,  82,  16,  13,   3,  82,\n",
       "          2,  13,   6,   5,   8,  19,  66,  25,  25,  16,   1,  16,  38,\n",
       "         32,  25,  13,   8,  25,  25,  12,  25,  49,  46,  32,  66,  25,\n",
       "         38,  82,  82,  54,  19,  38,  38,  82,  54,  66,  49,  82,  54,\n",
       "         66,  74,  54,  82,  75, 249, 281, 254, 175, 257, 169, 209, 260,\n",
       "        183, 198, 264, 207, 255, 273, 251, 203, 271, 189, 200, 274, 192,\n",
       "        209, 283, 206, 253, 280, 252, 159, 287, 167, 214, 258, 170, 218,\n",
       "        259, 217, 256, 269, 250, 162, 267, 248, 204, 279, 208, 227, 266,\n",
       "        213, 214, 263, 239, 245, 286, 209, 242, 285, 214, 218, 282, 223,\n",
       "        223, 278, 246, 227, 272, 221, 232, 275, 225, 237, 269, 236, 232,\n",
       "        288, 235, 234, 277, 239, 237, 268, 212, 227, 262, 227, 244, 276,\n",
       "        222, 242, 261, 247, 239, 284, 218, 227, 265, 226, 195, 189, 184,\n",
       "        163, 180, 173, 157, 165, 160, 146, 146, 148, 192, 196, 200, 184,\n",
       "        173, 175, 148, 160, 155, 145, 144, 148, 187, 199, 194, 170, 178,\n",
       "        170, 163, 155, 157, 148, 153, 153, 202, 197, 205, 186, 189, 187,\n",
       "        175, 180, 178, 166, 168, 180, 125, 138, 141, 118, 114, 122, 113,\n",
       "        103, 114, 101, 103, 103, 139, 135, 148, 118, 122, 126, 109, 118,\n",
       "        114,  98,  98,  97, 142, 143, 140, 122, 130, 126, 107, 108, 121,\n",
       "        102,  98, 103, 134, 135, 135, 131, 133, 131, 117, 126, 126, 109,\n",
       "        109, 109]),\n",
       " 'split0_test_roc_auc_ovo': array([0.87287341, 0.94122195, 0.98950615, 0.85250316, 0.94944256,\n",
       "        0.88540983, 0.95079489, 0.9244353 , 0.89672693, 0.88811697,\n",
       "        0.9212427 , 0.94284304, 0.80365395, 0.89445949, 0.88302143,\n",
       "        0.90960333, 0.84955639, 0.86041299, 0.97712271, 0.94717568,\n",
       "        0.91416815, 0.98719544, 0.86068943, 0.97064578, 0.91319872,\n",
       "        0.94446173, 0.89777964, 0.95155808, 0.9080624 , 0.94835632,\n",
       "        0.90348296, 0.9350654 , 0.86928168, 0.94631713, 0.96539199,\n",
       "        0.92461697, 0.96061428, 0.95039914, 0.95197907, 0.96044278,\n",
       "        0.95870105, 0.96137029, 0.95764669, 0.95937029, 0.95801525,\n",
       "        0.95422711, 0.94541316, 0.95646912, 0.97755412, 0.93339947,\n",
       "        0.88110667, 0.91248402, 0.88838714, 0.89940851, 0.87647118,\n",
       "        0.97128191, 0.92231848, 0.99544634, 0.94394314, 0.99794598,\n",
       "        0.93239851, 0.85334139, 0.89324016, 0.92967703, 0.89347129,\n",
       "        0.91384264, 0.96538055, 0.93086237, 0.98524117, 0.93101857,\n",
       "        0.89816055, 0.94650755, 0.92317386, 0.99029892, 0.9322576 ,\n",
       "        0.92479321, 0.97175231, 0.94878258, 0.88754843, 0.94889628,\n",
       "        0.94333538, 0.94557808, 0.98867219, 0.92517756, 0.91101378,\n",
       "        0.86082833, 0.92645309, 0.8759639 , 0.89404185, 0.89482116,\n",
       "        0.95666792, 0.93865633, 0.96926045, 0.95351549, 0.94227144,\n",
       "        0.96877778, 0.77982484, 0.35732229, 0.7827631 , 0.79726174,\n",
       "        0.53178423, 0.78381105, 0.850476  , 0.68710572, 0.79809379,\n",
       "        0.84835101, 0.63545473, 0.79477508, 0.8531619 , 0.63509012,\n",
       "        0.85115707, 0.82762621, 0.43053909, 0.79008095, 0.80266234,\n",
       "        0.58850297, 0.79287736, 0.79097179, 0.52166427, 0.79583793,\n",
       "        0.75105908, 0.51877291, 0.79813338, 0.79450597, 0.63495754,\n",
       "        0.85516308, 0.84264202, 0.37905801, 0.80478937, 0.79710695,\n",
       "        0.65676906, 0.83074436, 0.80163477, 0.71803076, 0.795739  ,\n",
       "        0.78467885, 0.70845052, 0.79320366, 0.79828916, 0.51296865,\n",
       "        0.8516993 , 0.79883479, 0.56343121, 0.82059805, 0.83391459,\n",
       "        0.48108046, 0.85117029, 0.85493545, 0.50089857, 0.7880619 ,\n",
       "        0.79619883, 0.52344962, 0.80543461, 0.79652352, 0.5605581 ,\n",
       "        0.80223516, 0.85291397, 0.4633007 , 0.84442459, 0.81498548,\n",
       "        0.64148094, 0.80121991, 0.80077839, 0.70503082, 0.84531866,\n",
       "        0.80496641, 0.43169365, 0.80484012, 0.80060432, 0.37231375,\n",
       "        0.79865578, 0.79007161, 0.7155951 , 0.79769115, 0.83595629,\n",
       "        0.51802867, 0.80291843, 0.83111758, 0.42870946, 0.85399415,\n",
       "        0.80421696, 0.39834565, 0.79436635, 0.79736596, 0.64262203,\n",
       "        0.80080566, 0.82996584, 0.60468395, 0.77431669, 0.8218844 ,\n",
       "        0.5643324 , 0.78782704, 0.81057983, 0.81162813, 0.81164496,\n",
       "        0.8086302 , 0.82067711, 0.80714045, 0.82307228, 0.83420176,\n",
       "        0.81115062, 0.8331991 , 0.83499664, 0.82576992, 0.80814589,\n",
       "        0.80405965, 0.81884595, 0.82294146, 0.82904284, 0.80971469,\n",
       "        0.81341142, 0.81308191, 0.8094362 , 0.81926527, 0.81432585,\n",
       "        0.83400579, 0.8083586 , 0.80167512, 0.80084382, 0.82833906,\n",
       "        0.80906668, 0.80818363, 0.82564901, 0.82516285, 0.82959366,\n",
       "        0.81364518, 0.81455343, 0.83173667, 0.83447562, 0.80637209,\n",
       "        0.83325897, 0.80958784, 0.83201867, 0.80530453, 0.82203425,\n",
       "        0.81532055, 0.8081606 , 0.81155972, 0.81968059, 0.81310982,\n",
       "        0.81186397, 0.81007638, 0.84697008, 0.87528305, 0.84855037,\n",
       "        0.84577105, 0.87106767, 0.85891884, 0.85196504, 0.8790209 ,\n",
       "        0.87891919, 0.86800864, 0.8385629 , 0.82409671, 0.81646088,\n",
       "        0.80651947, 0.81363695, 0.84793118, 0.83639526, 0.84129261,\n",
       "        0.82115373, 0.85442516, 0.85337809, 0.86694088, 0.80759512,\n",
       "        0.83663243, 0.83327632, 0.84618182, 0.87364959, 0.77185622,\n",
       "        0.8457571 , 0.89050216, 0.87626276, 0.85694   , 0.89625523,\n",
       "        0.83524532, 0.80992173, 0.84365719, 0.82213979, 0.83706476,\n",
       "        0.82203482, 0.82679961, 0.83151116, 0.8320823 , 0.81352313,\n",
       "        0.85729802, 0.85368565, 0.87884195]),\n",
       " 'split1_test_roc_auc_ovo': array([0.92583293, 0.95943661, 0.90906755, 0.94116688, 0.90927819,\n",
       "        0.97148723, 0.96462859, 0.88511343, 0.9466037 , 0.96478213,\n",
       "        0.87713268, 0.95499275, 0.82848883, 0.90799439, 0.95470538,\n",
       "        0.96477464, 0.97515319, 0.89814945, 0.98155357, 0.97531624,\n",
       "        0.85474207, 0.96122821, 0.96368779, 0.96209678, 0.93522552,\n",
       "        0.91918361, 0.89102539, 0.93287744, 0.93570014, 0.90423249,\n",
       "        0.90840167, 0.94578102, 0.92520271, 0.90107118, 0.87395061,\n",
       "        0.91221664, 0.87834711, 0.8703809 , 0.91078687, 0.90415299,\n",
       "        0.95461206, 0.91095653, 0.9317028 , 0.96482719, 0.90478274,\n",
       "        0.92755353, 0.93086657, 0.89986236, 0.95887629, 0.944103  ,\n",
       "        0.92715604, 0.94728231, 0.97096476, 0.96017392, 0.91095997,\n",
       "        0.90366626, 0.92552277, 0.93950844, 0.94424647, 0.92800677,\n",
       "        0.8731802 , 0.86083244, 0.908401  , 0.95240368, 0.93330765,\n",
       "        0.92314703, 0.95601233, 0.96888672, 0.95205515, 0.97975577,\n",
       "        0.93169553, 0.95193388, 0.86370136, 0.94853538, 0.95266588,\n",
       "        0.9648435 , 0.96897729, 0.87496116, 0.97280332, 0.93989798,\n",
       "        0.93276068, 0.91731444, 0.91220761, 0.87086416, 0.94099558,\n",
       "        0.94006896, 0.90764756, 0.95861715, 0.93951581, 0.86788191,\n",
       "        0.97586331, 0.9292911 , 0.94001488, 0.8910386 , 0.90999838,\n",
       "        0.90517825, 0.74853166, 0.59569333, 0.73982168, 0.72970189,\n",
       "        0.50905599, 0.72983606, 0.73102661, 0.47534284, 0.7377829 ,\n",
       "        0.73160222, 0.26783244, 0.72714943, 0.72315242, 0.5766471 ,\n",
       "        0.73301179, 0.73299068, 0.41995451, 0.74208477, 0.73824456,\n",
       "        0.40252028, 0.72843418, 0.74378998, 0.75470811, 0.74030258,\n",
       "        0.73636848, 0.39319447, 0.76299304, 0.74108485, 0.27032505,\n",
       "        0.72980086, 0.73916153, 0.55324878, 0.73848867, 0.72890216,\n",
       "        0.59441768, 0.73200072, 0.75457865, 0.50655402, 0.72689021,\n",
       "        0.73460421, 0.5472568 , 0.72889472, 0.74254637, 0.54311789,\n",
       "        0.73426883, 0.73944374, 0.53019813, 0.73592445, 0.77635841,\n",
       "        0.64545795, 0.74346967, 0.75953923, 0.57616957, 0.74056324,\n",
       "        0.74109006, 0.7738538 , 0.73514273, 0.72874226, 0.53877398,\n",
       "        0.7106279 , 0.71930761, 0.6133103 , 0.73599978, 0.77454234,\n",
       "        0.77602515, 0.7310421 , 0.75194192, 0.79709754, 0.73325468,\n",
       "        0.72516294, 0.38825295, 0.74385214, 0.72814227, 0.47404586,\n",
       "        0.72620858, 0.77055013, 0.51429625, 0.73957275, 0.71859494,\n",
       "        0.58431149, 0.75327574, 0.72859634, 0.45689315, 0.73187194,\n",
       "        0.72779436, 0.41198487, 0.77552287, 0.72374712, 0.62937891,\n",
       "        0.73816331, 0.75907821, 0.55516519, 0.73387125, 0.75173137,\n",
       "        0.87338009, 0.74546865, 0.7529321 , 0.75811828, 0.75248023,\n",
       "        0.74248126, 0.74676991, 0.74685841, 0.74189496, 0.75086429,\n",
       "        0.76117713, 0.76676249, 0.74781124, 0.75430023, 0.76300379,\n",
       "        0.75014779, 0.74695991, 0.75006753, 0.75554002, 0.74844084,\n",
       "        0.74463682, 0.74865931, 0.74558023, 0.76243185, 0.74825299,\n",
       "        0.76037475, 0.74362461, 0.76503091, 0.73526203, 0.75736978,\n",
       "        0.73963828, 0.7489576 , 0.74462335, 0.75735692, 0.75514916,\n",
       "        0.74971527, 0.74759864, 0.75755204, 0.74365302, 0.74710985,\n",
       "        0.74287028, 0.74608857, 0.74329303, 0.75018608, 0.74680609,\n",
       "        0.75581599, 0.75411439, 0.74503924, 0.75500935, 0.74447058,\n",
       "        0.81308393, 0.75895554, 0.75705301, 0.77155159, 0.85166273,\n",
       "        0.83135883, 0.83176427, 0.84806807, 0.80126959, 0.88674131,\n",
       "        0.83260522, 0.78493966, 0.75784706, 0.75366924, 0.78688469,\n",
       "        0.74205609, 0.83417365, 0.7641029 , 0.84547684, 0.7708285 ,\n",
       "        0.77947338, 0.81271641, 0.84733387, 0.80079345, 0.75874597,\n",
       "        0.78661012, 0.82140808, 0.83146686, 0.80342512, 0.75842796,\n",
       "        0.83117539, 0.84120314, 0.85194925, 0.80460077, 0.84061924,\n",
       "        0.83490197, 0.74163803, 0.7783108 , 0.74707859, 0.83783475,\n",
       "        0.81615388, 0.81482866, 0.79722971, 0.7868963 , 0.81090434,\n",
       "        0.796912  , 0.79842219, 0.75734719]),\n",
       " 'mean_test_roc_auc_ovo': array([0.89935317, 0.95032928, 0.94928685, 0.89683502, 0.92936037,\n",
       "        0.92844853, 0.95771174, 0.90477436, 0.92166532, 0.92644955,\n",
       "        0.89918769, 0.9489179 , 0.81607139, 0.90122694, 0.91886341,\n",
       "        0.93718898, 0.91235479, 0.87928122, 0.97933814, 0.96124596,\n",
       "        0.88445511, 0.97421182, 0.91218861, 0.96637128, 0.92421212,\n",
       "        0.93182267, 0.89440252, 0.94221776, 0.92188127, 0.9262944 ,\n",
       "        0.90594231, 0.94042321, 0.8972422 , 0.92369416, 0.9196713 ,\n",
       "        0.91841681, 0.9194807 , 0.91039002, 0.93138297, 0.93229789,\n",
       "        0.95665656, 0.93616341, 0.94467474, 0.96209874, 0.93139899,\n",
       "        0.94089032, 0.93813986, 0.92816574, 0.9682152 , 0.93875124,\n",
       "        0.90413135, 0.92988317, 0.92967595, 0.92979121, 0.89371558,\n",
       "        0.93747409, 0.92392063, 0.96747739, 0.94409481, 0.96297638,\n",
       "        0.90278935, 0.85708692, 0.90082058, 0.94104035, 0.91338947,\n",
       "        0.91849483, 0.96069644, 0.94987454, 0.96864816, 0.95538717,\n",
       "        0.91492804, 0.94922072, 0.89343761, 0.96941715, 0.94246174,\n",
       "        0.94481835, 0.9703648 , 0.91187187, 0.93017587, 0.94439713,\n",
       "        0.93804803, 0.93144626, 0.9504399 , 0.89802086, 0.92600468,\n",
       "        0.90044865, 0.91705032, 0.91729053, 0.91677883, 0.88135153,\n",
       "        0.96626562, 0.93397371, 0.95463767, 0.92227705, 0.92613491,\n",
       "        0.93697801, 0.76417825, 0.47650781, 0.76129239, 0.76348181,\n",
       "        0.52042011, 0.75682355, 0.7907513 , 0.58122428, 0.76793835,\n",
       "        0.78997662, 0.45164358, 0.76096226, 0.78815716, 0.60586861,\n",
       "        0.79208443, 0.78030845, 0.4252468 , 0.76608286, 0.77045345,\n",
       "        0.49551163, 0.76065577, 0.76738089, 0.63818619, 0.76807025,\n",
       "        0.74371378, 0.45598369, 0.78056321, 0.76779541, 0.45264129,\n",
       "        0.79248197, 0.79090178, 0.4661534 , 0.77163902, 0.76300455,\n",
       "        0.62559337, 0.78137254, 0.77810671, 0.61229239, 0.7613146 ,\n",
       "        0.75964153, 0.62785366, 0.76104919, 0.77041776, 0.52804327,\n",
       "        0.79298407, 0.76913926, 0.54681467, 0.77826125, 0.8051365 ,\n",
       "        0.5632692 , 0.79731998, 0.80723734, 0.53853407, 0.76431257,\n",
       "        0.76864444, 0.64865171, 0.77028867, 0.76263289, 0.54966604,\n",
       "        0.75643153, 0.78611079, 0.5383055 , 0.79021218, 0.79476391,\n",
       "        0.70875304, 0.766131  , 0.77636016, 0.75106418, 0.78928667,\n",
       "        0.76506468, 0.4099733 , 0.77434613, 0.76437329, 0.42317981,\n",
       "        0.76243218, 0.78031087, 0.61494568, 0.76863195, 0.77727562,\n",
       "        0.55117008, 0.77809708, 0.77985696, 0.4428013 , 0.79293305,\n",
       "        0.76600566, 0.40516526, 0.78494461, 0.76055654, 0.63600047,\n",
       "        0.76948449, 0.79452203, 0.57992457, 0.75409397, 0.78680789,\n",
       "        0.71885624, 0.76664784, 0.78175597, 0.7848732 , 0.7820626 ,\n",
       "        0.77555573, 0.78372351, 0.77699943, 0.78248362, 0.79253303,\n",
       "        0.78616388, 0.79998079, 0.79140394, 0.79003507, 0.78557484,\n",
       "        0.77710372, 0.78290293, 0.78650449, 0.79229143, 0.77907777,\n",
       "        0.77902412, 0.78087061, 0.77750822, 0.79084856, 0.78128942,\n",
       "        0.79719027, 0.77599161, 0.78335301, 0.76805293, 0.79285442,\n",
       "        0.77435248, 0.77857061, 0.78513618, 0.79125988, 0.79237141,\n",
       "        0.78168022, 0.78107603, 0.79464435, 0.78906432, 0.77674097,\n",
       "        0.78806463, 0.77783821, 0.78765585, 0.77774531, 0.78442017,\n",
       "        0.78556827, 0.78113749, 0.77829948, 0.78734497, 0.7787902 ,\n",
       "        0.81247395, 0.78451596, 0.80201155, 0.82341732, 0.85010655,\n",
       "        0.83856494, 0.85141597, 0.85349346, 0.82661732, 0.8828811 ,\n",
       "        0.85576221, 0.82647415, 0.79820498, 0.78888297, 0.80167279,\n",
       "        0.77428778, 0.8239053 , 0.80601704, 0.84093605, 0.80606055,\n",
       "        0.80031356, 0.83357079, 0.85035598, 0.83386716, 0.78317055,\n",
       "        0.81162128, 0.8273422 , 0.83882434, 0.83853736, 0.76514209,\n",
       "        0.83846625, 0.86585265, 0.86410601, 0.83077038, 0.86843724,\n",
       "        0.83507364, 0.77577988, 0.81098399, 0.78460919, 0.83744975,\n",
       "        0.81909435, 0.82081414, 0.81437043, 0.8094893 , 0.81221373,\n",
       "        0.82710501, 0.82605392, 0.81809457]),\n",
       " 'std_test_roc_auc_ovo': array([2.64797599e-02, 9.10732757e-03, 4.02192964e-02, 4.43318582e-02,\n",
       "        2.00821889e-02, 4.30386979e-02, 6.91685124e-03, 1.96609327e-02,\n",
       "        2.49383886e-02, 3.83325824e-02, 2.20550098e-02, 6.07485490e-03,\n",
       "        1.24174399e-02, 6.76744954e-03, 3.58419756e-02, 2.75856525e-02,\n",
       "        6.27984020e-02, 1.88682284e-02, 2.21543104e-03, 1.40702826e-02,\n",
       "        2.97130423e-02, 1.29836131e-02, 5.14991825e-02, 4.27449725e-03,\n",
       "        1.10133999e-02, 1.26390609e-02, 3.37712527e-03, 9.34031687e-03,\n",
       "        1.38188701e-02, 2.20619144e-02, 2.45935390e-03, 5.35781457e-03,\n",
       "        2.79605179e-02, 2.26229755e-02, 4.57206906e-02, 6.20016776e-03,\n",
       "        4.11335843e-02, 4.00091164e-02, 2.05961012e-02, 2.81448974e-02,\n",
       "        2.04449227e-03, 2.52068801e-02, 1.29719450e-02, 2.72845315e-03,\n",
       "        2.66162557e-02, 1.33367870e-02, 7.27329710e-03, 2.83033825e-02,\n",
       "        9.33891367e-03, 5.35176434e-03, 2.30246827e-02, 1.73991422e-02,\n",
       "        4.12888127e-02, 3.03827024e-02, 1.72443929e-02, 3.38078270e-02,\n",
       "        1.60214478e-03, 2.79689538e-02, 1.51667463e-04, 3.49696046e-02,\n",
       "        2.96091577e-02, 3.74552849e-03, 7.58041827e-03, 1.13633236e-02,\n",
       "        1.99181792e-02, 4.65219502e-03, 4.68410728e-03, 1.90121728e-02,\n",
       "        1.65930114e-02, 2.43685993e-02, 1.67674933e-02, 2.71316615e-03,\n",
       "        2.97362508e-02, 2.08817659e-02, 1.02041373e-02, 2.00251440e-02,\n",
       "        1.38751301e-03, 3.69107104e-02, 4.26274479e-02, 4.49915298e-03,\n",
       "        5.28734792e-03, 1.41318198e-02, 3.82322899e-02, 2.71567000e-02,\n",
       "        1.49908987e-02, 3.96203167e-02, 9.40276639e-03, 4.13266227e-02,\n",
       "        2.27369796e-02, 1.34696268e-02, 9.59769112e-03, 4.68261325e-03,\n",
       "        1.46227859e-02, 3.12384467e-02, 1.61365297e-02, 3.17997653e-02,\n",
       "        1.56465881e-02, 1.19185517e-01, 2.14707091e-02, 3.37799262e-02,\n",
       "        1.13641196e-02, 2.69874938e-02, 5.97246972e-02, 1.05881442e-01,\n",
       "        3.01554477e-02, 5.83743948e-02, 1.83811146e-01, 3.38128242e-02,\n",
       "        6.50047414e-02, 2.92215112e-02, 5.90726408e-02, 4.73177636e-02,\n",
       "        5.29228928e-03, 2.39980906e-02, 3.22088916e-02, 9.29913487e-02,\n",
       "        3.22215923e-02, 2.35909077e-02, 1.16521920e-01, 2.77676746e-02,\n",
       "        7.34530391e-03, 6.27892209e-02, 1.75701668e-02, 2.67105607e-02,\n",
       "        1.82316243e-01, 6.26811058e-02, 5.17402455e-02, 8.70953876e-02,\n",
       "        3.31503544e-02, 3.41023955e-02, 3.11756899e-02, 4.93718161e-02,\n",
       "        2.35280570e-02, 1.05738371e-01, 3.44243955e-02, 2.50373191e-02,\n",
       "        8.05968620e-02, 3.21544713e-02, 2.78713951e-02, 1.50746215e-02,\n",
       "        5.87152319e-02, 2.96955249e-02, 1.66165377e-02, 4.23368047e-02,\n",
       "        2.87780922e-02, 8.21887481e-02, 5.38503085e-02, 4.76981085e-02,\n",
       "        3.76355009e-02, 2.37493306e-02, 2.75543841e-02, 1.25202091e-01,\n",
       "        3.51459399e-02, 3.38906295e-02, 1.08920585e-02, 4.58036317e-02,\n",
       "        6.68031815e-02, 7.50048022e-02, 5.42124071e-02, 2.02215690e-02,\n",
       "        6.72721052e-02, 3.50889035e-02, 2.44182317e-02, 4.60333626e-02,\n",
       "        5.60319913e-02, 3.99017390e-02, 2.17203502e-02, 3.04939904e-02,\n",
       "        3.62310272e-02, 5.08660551e-02, 3.62235982e-02, 9.76074156e-03,\n",
       "        1.00649423e-01, 2.90591984e-02, 5.86806780e-02, 3.31414092e-02,\n",
       "        2.48213458e-02, 5.12606215e-02, 1.40918440e-02, 6.10611067e-02,\n",
       "        3.82112984e-02, 6.81960920e-03, 9.42173601e-03, 3.68094184e-02,\n",
       "        6.62156347e-03, 3.13211796e-02, 3.54438163e-02, 2.47593815e-02,\n",
       "        2.02227227e-02, 3.50765149e-02, 1.54523845e-01, 2.11791917e-02,\n",
       "        2.88238655e-02, 2.67549247e-02, 2.95823623e-02, 3.30744673e-02,\n",
       "        3.69535959e-02, 3.01410223e-02, 4.05886600e-02, 4.16687359e-02,\n",
       "        2.49867442e-02, 3.32183047e-02, 4.35927048e-02, 3.57348466e-02,\n",
       "        2.25710476e-02, 2.69559313e-02, 3.59430214e-02, 3.64369690e-02,\n",
       "        3.67514107e-02, 3.06369253e-02, 3.43872970e-02, 3.22112972e-02,\n",
       "        3.19279831e-02, 2.84167130e-02, 3.30364286e-02, 3.68155184e-02,\n",
       "        3.23669929e-02, 1.83221087e-02, 3.27908984e-02, 3.54846423e-02,\n",
       "        3.47142027e-02, 2.96130133e-02, 4.05128293e-02, 3.39029651e-02,\n",
       "        3.72222514e-02, 3.19649561e-02, 3.34773946e-02, 3.70923124e-02,\n",
       "        4.54112958e-02, 2.96311158e-02, 4.51943447e-02, 3.17496339e-02,\n",
       "        4.43628220e-02, 2.75592248e-02, 3.76140774e-02, 2.97522816e-02,\n",
       "        2.70231040e-02, 3.32602408e-02, 3.23356194e-02, 3.43196225e-02,\n",
       "        6.09979019e-04, 2.55604187e-02, 4.49585327e-02, 5.18657286e-02,\n",
       "        1.55618313e-03, 7.20611022e-03, 1.96516991e-02, 5.42538363e-03,\n",
       "        2.53477248e-02, 3.86020475e-03, 2.31569836e-02, 4.15344886e-02,\n",
       "        4.03579185e-02, 3.52137328e-02, 1.47880939e-02, 3.22316876e-02,\n",
       "        1.02683488e-02, 4.19141419e-02, 4.54079103e-03, 3.52320564e-02,\n",
       "        2.08401721e-02, 2.08543750e-02, 3.02210994e-03, 3.30737168e-02,\n",
       "        2.44245763e-02, 2.50111556e-02, 5.93412114e-03, 7.35748060e-03,\n",
       "        3.51122373e-02, 6.71412841e-03, 7.29085400e-03, 2.46495067e-02,\n",
       "        1.21567553e-02, 2.61696161e-02, 2.78179928e-02, 1.71674342e-04,\n",
       "        3.41418491e-02, 3.26731915e-02, 3.75306010e-02, 3.84991398e-04,\n",
       "        2.94046977e-03, 5.98547466e-03, 1.71407252e-02, 2.25929999e-02,\n",
       "        1.30939183e-03, 3.01930126e-02, 2.76317281e-02, 6.07473836e-02]),\n",
       " 'rank_test_roc_auc_ovo': array([ 84,  19,  21,  88,  50,  51,  14,  78,  62,  53,  85,  23, 125,\n",
       "         81,  65,  37,  73,  95,   1,  12,  92,   2,  74,   8,  57,  42,\n",
       "         89,  29,  61,  54,  77,  32,  87,  59,  63,  67,  64,  76,  45,\n",
       "         41,  15,  39,  25,  11,  44,  31,  34,  52,   6,  33,  79,  47,\n",
       "         49,  48,  90,  36,  58,   7,  27,  10,  80,  99,  82,  30,  72,\n",
       "         66,  13,  20,   5,  16,  71,  22,  91,   4,  28,  24,   3,  75,\n",
       "         46,  26,  35,  43,  18,  86,  56,  83,  69,  68,  70,  94,   9,\n",
       "         40,  17,  60,  55,  38, 241, 279, 247, 242, 277, 253, 158, 268,\n",
       "        230, 161, 283, 249, 165, 267, 153, 196, 285, 235, 221, 278, 250,\n",
       "        232, 261, 228, 257, 281, 194, 231, 282, 150, 156, 280, 220, 243,\n",
       "        264, 189, 204, 266, 246, 252, 263, 248, 222, 276, 146, 225, 273,\n",
       "        203, 135, 270, 141, 132, 274, 240, 226, 260, 223, 244, 272, 254,\n",
       "        172, 275, 159, 143, 259, 234, 213, 256, 162, 238, 287, 218, 239,\n",
       "        286, 245, 195, 265, 227, 209, 271, 205, 197, 284, 147, 236, 288,\n",
       "        176, 251, 262, 224, 145, 269, 255, 169, 258, 233, 187, 177, 186,\n",
       "        216, 181, 211, 185, 149, 171, 139, 154, 160, 173, 210, 184, 170,\n",
       "        152, 198, 199, 193, 208, 157, 190, 142, 214, 182, 229, 148, 217,\n",
       "        201, 175, 155, 151, 188, 192, 144, 163, 212, 166, 206, 167, 207,\n",
       "        180, 174, 191, 202, 168, 200, 127, 179, 136, 121, 104, 107, 102,\n",
       "        101, 117,  93, 100, 118, 140, 164, 137, 219, 120, 134, 105, 133,\n",
       "        138, 113, 103, 112, 183, 129, 115, 106, 108, 237, 109,  97,  98,\n",
       "        114,  96, 111, 215, 130, 178, 110, 123, 122, 126, 131, 128, 116,\n",
       "        119, 124]),\n",
       " 'split0_test_neg_log_loss': array([-0.08847774, -0.05931156, -0.08406947, -0.11911621, -0.08294442,\n",
       "        -0.07270731, -0.16836732, -0.05895892, -0.08100622, -0.19900668,\n",
       "        -0.21574352, -0.10538664, -0.06560894, -0.06477235, -0.04954119,\n",
       "        -0.10778852, -0.10107761, -0.05995425, -0.07145139, -0.06392281,\n",
       "        -0.07212775, -0.08451354, -0.07306591, -0.10746264, -0.0419523 ,\n",
       "        -0.05384212, -0.04593638, -0.04860007, -0.04307015, -0.04849302,\n",
       "        -0.05014099, -0.03839873, -0.04819649, -0.05258865, -0.04290298,\n",
       "        -0.03992541, -0.03706754, -0.0408062 , -0.04368647, -0.0383643 ,\n",
       "        -0.03878829, -0.03993849, -0.04047512, -0.04230968, -0.04340822,\n",
       "        -0.04155029, -0.03941729, -0.04195267, -0.09809334, -0.09864   ,\n",
       "        -0.17135666, -0.18103153, -0.12531823, -0.10311171, -0.13461935,\n",
       "        -0.17816332, -0.19157853, -0.18704943, -0.15689717, -0.18638136,\n",
       "        -0.07044509, -0.07205431, -0.03764776, -0.0987238 , -0.13164127,\n",
       "        -0.15919047, -0.13899191, -0.12469915, -0.09944823, -0.1447177 ,\n",
       "        -0.13385182, -0.14433393, -0.03948986, -0.05761288, -0.04495281,\n",
       "        -0.04908759, -0.04770306, -0.04325089, -0.07852066, -0.08183764,\n",
       "        -0.07870402, -0.06089293, -0.04801521, -0.05676549, -0.03627813,\n",
       "        -0.04240317, -0.03500059, -0.04161977, -0.04211333, -0.04431553,\n",
       "        -0.04310298, -0.04470291, -0.03892356, -0.03703467, -0.03802801,\n",
       "        -0.03889194, -0.44715637, -1.45717446, -0.42777405, -0.36130024,\n",
       "        -1.23296527, -0.39708265, -0.35933846, -1.26737409, -0.35584755,\n",
       "        -0.34982945, -1.34465505, -0.35913795, -0.41925786, -1.31873955,\n",
       "        -0.44920854, -0.34602862, -1.60620562, -0.38853765, -0.3458275 ,\n",
       "        -1.43193389, -0.37940312, -0.35280711, -1.40002517, -0.34284944,\n",
       "        -0.49050403, -1.49345414, -0.44056053, -0.38781839, -1.51111248,\n",
       "        -0.39140932, -0.36605518, -1.26481447, -0.37465561, -0.32833324,\n",
       "        -1.30198458, -0.3399327 , -0.43464427, -1.68050862, -0.46207646,\n",
       "        -0.38550929, -1.22127398, -0.38130763, -0.36362555, -1.37547708,\n",
       "        -0.38194802, -0.34746924, -1.3530303 , -0.34011653, -0.17590723,\n",
       "        -1.19270515, -0.1775583 , -0.16589443, -1.56013709, -0.16961933,\n",
       "        -0.17014526, -1.56643474, -0.16962149, -0.16731061, -1.35835585,\n",
       "        -0.16932008, -0.18535145, -1.73344253, -0.18377804, -0.17400713,\n",
       "        -1.20828067, -0.16841061, -0.17005545, -1.34309902, -0.17320802,\n",
       "        -0.16469772, -1.27557648, -0.16576799, -0.18785144, -1.98488307,\n",
       "        -0.18549711, -0.17425209, -1.83773434, -0.17058257, -0.1654734 ,\n",
       "        -1.2562305 , -0.16350618, -0.16575032, -1.2625328 , -0.16581654,\n",
       "        -0.18136428, -1.93040174, -0.18545846, -0.1782784 , -1.17131491,\n",
       "        -0.17374811, -0.16993508, -1.56488134, -0.16437431, -0.17438191,\n",
       "        -1.34508419, -0.16975823, -0.16149125, -0.15444386, -0.16087674,\n",
       "        -0.12676676, -0.12669945, -0.13389977, -0.12419068, -0.12105204,\n",
       "        -0.11855099, -0.11084463, -0.11358729, -0.11483331, -0.16297618,\n",
       "        -0.17258104, -0.15970424, -0.13067025, -0.12737891, -0.12753814,\n",
       "        -0.11222221, -0.11797636, -0.11712527, -0.11030559, -0.10924593,\n",
       "        -0.1064933 , -0.15295544, -0.1792955 , -0.16533546, -0.12647239,\n",
       "        -0.13999809, -0.13098245, -0.11581412, -0.11215554, -0.12377342,\n",
       "        -0.10787505, -0.10860606, -0.11564456, -0.16467941, -0.16003778,\n",
       "        -0.16749643, -0.14291923, -0.14235594, -0.1411181 , -0.13813512,\n",
       "        -0.1283657 , -0.1385821 , -0.12389284, -0.12471578, -0.12868266,\n",
       "        -0.07726059, -0.09212029, -0.0894652 , -0.06812877, -0.07636348,\n",
       "        -0.07256684, -0.06517066, -0.06642307, -0.07652106, -0.06138951,\n",
       "        -0.0630736 , -0.06740259, -0.09504576, -0.08712181, -0.10318084,\n",
       "        -0.06909924, -0.08665217, -0.07694774, -0.07489486, -0.06325462,\n",
       "        -0.07311306, -0.06345016, -0.06447694, -0.06049209, -0.08902774,\n",
       "        -0.10073199, -0.09508278, -0.06675809, -0.08552534, -0.07613356,\n",
       "        -0.06421075, -0.07078091, -0.06361348, -0.05965183, -0.05971306,\n",
       "        -0.06176216, -0.0993763 , -0.09430673, -0.09719573, -0.07965265,\n",
       "        -0.08120574, -0.08203167, -0.07910867, -0.07436571, -0.07942918,\n",
       "        -0.06833762, -0.06647149, -0.06970946]),\n",
       " 'split1_test_neg_log_loss': array([-0.04926715, -0.0205416 , -0.02136032, -0.02266321, -0.04900408,\n",
       "        -0.02804417, -0.06153772, -0.04733099, -0.02803244, -0.03940146,\n",
       "        -0.07378727, -0.04302061, -0.02666899, -0.01884805, -0.02405416,\n",
       "        -0.02017776, -0.03597757, -0.0321215 , -0.03061969, -0.01960665,\n",
       "        -0.02297072, -0.04735004, -0.02259992, -0.04288974, -0.0223918 ,\n",
       "        -0.02532113, -0.05789479, -0.02461753, -0.02327788, -0.02355533,\n",
       "        -0.02628663, -0.02086567, -0.02691699, -0.02557107, -0.02300878,\n",
       "        -0.0263164 , -0.02832883, -0.02821133, -0.02838111, -0.02748447,\n",
       "        -0.0257363 , -0.02684399, -0.02712779, -0.02590353, -0.02636574,\n",
       "        -0.02634686, -0.02691806, -0.02657643, -0.0249564 , -0.06660336,\n",
       "        -0.01804961, -0.04079911, -0.02803275, -0.05439696, -0.04047353,\n",
       "        -0.05974648, -0.04664241, -0.03033686, -0.08855023, -0.07044016,\n",
       "        -0.01500632, -0.03872446, -0.01790876, -0.05844429, -0.06564784,\n",
       "        -0.03075119, -0.03010911, -0.05528217, -0.02936209, -0.02922272,\n",
       "        -0.03248754, -0.04549913, -0.01887256, -0.02397912, -0.02872584,\n",
       "        -0.01782685, -0.01498743, -0.02572808, -0.01711331, -0.02717117,\n",
       "        -0.03493542, -0.02160974, -0.02287817, -0.02873117, -0.02237283,\n",
       "        -0.02369478, -0.02225568, -0.02294563, -0.02335878, -0.02435776,\n",
       "        -0.02206231, -0.02262721, -0.02293053, -0.02386342, -0.02403098,\n",
       "        -0.02368826, -0.44004747, -1.323538  , -0.44923469, -0.3717114 ,\n",
       "        -1.28964053, -0.3937937 , -0.33774298, -1.17277063, -0.37650944,\n",
       "        -0.34760156, -1.31177759, -0.34426696, -0.47635585, -1.19150114,\n",
       "        -0.44700961, -0.36873776, -1.23988602, -0.35957283, -0.35001999,\n",
       "        -1.35376983, -0.34695415, -0.32572717, -1.40410118, -0.34174369,\n",
       "        -0.49976474, -1.3680336 , -0.43190194, -0.3633354 , -1.56085209,\n",
       "        -0.38121351, -0.34549215, -1.1035209 , -0.36783487, -0.34305581,\n",
       "        -1.17913798, -0.34429973, -0.58873428, -1.17096045, -0.44660205,\n",
       "        -0.37369434, -1.19774208, -0.38173834, -0.35967782, -1.36833523,\n",
       "        -0.34657114, -0.34117088, -1.0834327 , -0.32647978, -0.17622222,\n",
       "        -1.1514107 , -0.17799599, -0.16515274, -1.58464713, -0.15824216,\n",
       "        -0.16061614, -1.19599097, -0.15684718, -0.15643714, -1.51753584,\n",
       "        -0.16022968, -0.16562225, -1.21543709, -0.18204082, -0.17191336,\n",
       "        -1.36361066, -0.16678178, -0.15968782, -1.20939504, -0.1613961 ,\n",
       "        -0.16433368, -1.36380443, -0.16163334, -0.16326812, -1.6374485 ,\n",
       "        -0.18429727, -0.16441789, -1.12988543, -0.16629357, -0.16441183,\n",
       "        -1.22621111, -0.1578699 , -0.16597236, -1.31208981, -0.16081777,\n",
       "        -0.19285663, -1.1224677 , -0.16861517, -0.1668816 , -1.31781068,\n",
       "        -0.17287733, -0.16423088, -1.33092138, -0.15991627, -0.161244  ,\n",
       "        -0.92639936, -0.16169392, -0.17655457, -0.15397404, -0.1474118 ,\n",
       "        -0.12161364, -0.13531466, -0.12622193, -0.11046619, -0.11814493,\n",
       "        -0.11469783, -0.10381384, -0.10259971, -0.10307187, -0.16122631,\n",
       "        -0.15666643, -0.18146071, -0.14447777, -0.12053317, -0.12495719,\n",
       "        -0.10870253, -0.11947932, -0.10815731, -0.1061997 , -0.10094051,\n",
       "        -0.11002339, -0.15767351, -0.16042966, -0.15622416, -0.1221361 ,\n",
       "        -0.12717803, -0.12290305, -0.12064315, -0.11260751, -0.11272064,\n",
       "        -0.10614093, -0.11054639, -0.10754092, -0.16231535, -0.17329005,\n",
       "        -0.17993525, -0.13322908, -0.13715211, -0.14235073, -0.1261136 ,\n",
       "        -0.13046951, -0.12642321, -0.12068004, -0.12153572, -0.1270407 ,\n",
       "        -0.07441679, -0.08360287, -0.07852181, -0.06183457, -0.05949388,\n",
       "        -0.06115298, -0.05593295, -0.04999932, -0.05896716, -0.05297772,\n",
       "        -0.04679312, -0.04622695, -0.08622852, -0.08783292, -0.10408911,\n",
       "        -0.07666512, -0.06719667, -0.07144373, -0.05388483, -0.0665497 ,\n",
       "        -0.05576403, -0.04901937, -0.05451025, -0.05064232, -0.09443363,\n",
       "        -0.09652783, -0.08755453, -0.0703017 , -0.06614334, -0.06420564,\n",
       "        -0.05241935, -0.05392765, -0.0622201 , -0.04837213, -0.05197167,\n",
       "        -0.04956607, -0.08860462, -0.0822237 , -0.08559609, -0.0708451 ,\n",
       "        -0.07843512, -0.07507989, -0.06501731, -0.06797992, -0.06309968,\n",
       "        -0.0582388 , -0.05805631, -0.0588076 ]),\n",
       " 'mean_test_neg_log_loss': array([-0.06887244, -0.03992658, -0.05271489, -0.07088971, -0.06597425,\n",
       "        -0.05037574, -0.11495252, -0.05314495, -0.05451933, -0.11920407,\n",
       "        -0.1447654 , -0.07420362, -0.04613897, -0.0418102 , -0.03679768,\n",
       "        -0.06398314, -0.06852759, -0.04603787, -0.05103554, -0.04176473,\n",
       "        -0.04754923, -0.06593179, -0.04783291, -0.07517619, -0.03217205,\n",
       "        -0.03958163, -0.05191558, -0.0366088 , -0.03317402, -0.03602418,\n",
       "        -0.03821381, -0.0296322 , -0.03755674, -0.03907986, -0.03295588,\n",
       "        -0.03312091, -0.03269819, -0.03450877, -0.03603379, -0.03292439,\n",
       "        -0.03226229, -0.03339124, -0.03380145, -0.03410661, -0.03488698,\n",
       "        -0.03394857, -0.03316767, -0.03426455, -0.06152487, -0.08262168,\n",
       "        -0.09470314, -0.11091532, -0.07667549, -0.07875433, -0.08754644,\n",
       "        -0.1189549 , -0.11911047, -0.10869314, -0.1227237 , -0.12841076,\n",
       "        -0.04272571, -0.05538939, -0.02777826, -0.07858405, -0.09864456,\n",
       "        -0.09497083, -0.08455051, -0.08999066, -0.06440516, -0.08697021,\n",
       "        -0.08316968, -0.09491653, -0.02918121, -0.040796  , -0.03683932,\n",
       "        -0.03345722, -0.03134524, -0.03448948, -0.04781699, -0.0545044 ,\n",
       "        -0.05681972, -0.04125134, -0.03544669, -0.04274833, -0.02932548,\n",
       "        -0.03304897, -0.02862813, -0.0322827 , -0.03273605, -0.03433664,\n",
       "        -0.03258265, -0.03366506, -0.03092705, -0.03044904, -0.03102949,\n",
       "        -0.0312901 , -0.44360192, -1.39035623, -0.43850437, -0.36650582,\n",
       "        -1.2613029 , -0.39543817, -0.34854072, -1.22007236, -0.36617849,\n",
       "        -0.3487155 , -1.32821632, -0.35170246, -0.44780686, -1.25512035,\n",
       "        -0.44810908, -0.35738319, -1.42304582, -0.37405524, -0.34792375,\n",
       "        -1.39285186, -0.36317864, -0.33926714, -1.40206318, -0.34229656,\n",
       "        -0.49513439, -1.43074387, -0.43623124, -0.37557689, -1.53598228,\n",
       "        -0.38631142, -0.35577367, -1.18416769, -0.37124524, -0.33569452,\n",
       "        -1.24056128, -0.34211622, -0.51168928, -1.42573454, -0.45433926,\n",
       "        -0.37960181, -1.20950803, -0.38152299, -0.36165169, -1.37190615,\n",
       "        -0.36425958, -0.34432006, -1.2182315 , -0.33329816, -0.17606472,\n",
       "        -1.17205792, -0.17777715, -0.16552358, -1.57239211, -0.16393075,\n",
       "        -0.1653807 , -1.38121285, -0.16323434, -0.16187387, -1.43794584,\n",
       "        -0.16477488, -0.17548685, -1.47443981, -0.18290943, -0.17296025,\n",
       "        -1.28594566, -0.1675962 , -0.16487164, -1.27624703, -0.16730206,\n",
       "        -0.1645157 , -1.31969045, -0.16370066, -0.17555978, -1.81116578,\n",
       "        -0.18489719, -0.16933499, -1.48380989, -0.16843807, -0.16494262,\n",
       "        -1.24122081, -0.16068804, -0.16586134, -1.28731131, -0.16331715,\n",
       "        -0.18711045, -1.52643472, -0.17703681, -0.17258   , -1.24456279,\n",
       "        -0.17331272, -0.16708298, -1.44790136, -0.16214529, -0.16781295,\n",
       "        -1.13574178, -0.16572608, -0.16902291, -0.15420895, -0.15414427,\n",
       "        -0.1241902 , -0.13100706, -0.13006085, -0.11732844, -0.11959849,\n",
       "        -0.11662441, -0.10732924, -0.1080935 , -0.10895259, -0.16210124,\n",
       "        -0.16462374, -0.17058247, -0.13757401, -0.12395604, -0.12624767,\n",
       "        -0.11046237, -0.11872784, -0.11264129, -0.10825265, -0.10509322,\n",
       "        -0.10825835, -0.15531448, -0.16986258, -0.16077981, -0.12430425,\n",
       "        -0.13358806, -0.12694275, -0.11822863, -0.11238152, -0.11824703,\n",
       "        -0.10700799, -0.10957623, -0.11159274, -0.16349738, -0.16666391,\n",
       "        -0.17371584, -0.13807416, -0.13975402, -0.14173442, -0.13212436,\n",
       "        -0.12941761, -0.13250265, -0.12228644, -0.12312575, -0.12786168,\n",
       "        -0.07583869, -0.08786158, -0.0839935 , -0.06498167, -0.06792868,\n",
       "        -0.06685991, -0.06055181, -0.05821119, -0.06774411, -0.05718362,\n",
       "        -0.05493336, -0.05681477, -0.09063714, -0.08747736, -0.10363497,\n",
       "        -0.07288218, -0.07692442, -0.07419573, -0.06438984, -0.06490216,\n",
       "        -0.06443854, -0.05623477, -0.05949359, -0.0555672 , -0.09173068,\n",
       "        -0.09862991, -0.09131865, -0.0685299 , -0.07583434, -0.0701696 ,\n",
       "        -0.05831505, -0.06235428, -0.06291679, -0.05401198, -0.05584237,\n",
       "        -0.05566411, -0.09399046, -0.08826522, -0.09139591, -0.07524888,\n",
       "        -0.07982043, -0.07855578, -0.07206299, -0.07117281, -0.07126443,\n",
       "        -0.06328821, -0.0622639 , -0.06425853]),\n",
       " 'std_test_neg_log_loss': array([1.96052970e-02, 1.93849780e-02, 3.13545760e-02, 4.82265000e-02,\n",
       "        1.69701701e-02, 2.23315689e-02, 5.34147995e-02, 5.81396168e-03,\n",
       "        2.64868887e-02, 7.98026119e-02, 7.09781235e-02, 3.11830159e-02,\n",
       "        1.94699769e-02, 2.29621520e-02, 1.27435145e-02, 4.38053801e-02,\n",
       "        3.25500191e-02, 1.39163736e-02, 2.04158498e-02, 2.21580804e-02,\n",
       "        2.45785195e-02, 1.85817524e-02, 2.52329945e-02, 3.22864498e-02,\n",
       "        9.78025053e-03, 1.42604953e-02, 5.97920931e-03, 1.19912697e-02,\n",
       "        9.89613693e-03, 1.24688449e-02, 1.19271796e-02, 8.76653161e-03,\n",
       "        1.06397517e-02, 1.35087898e-02, 9.94709873e-03, 6.80450480e-03,\n",
       "        4.36935875e-03, 6.29743396e-03, 7.65267737e-03, 5.43991234e-03,\n",
       "        6.52599408e-03, 6.54724712e-03, 6.67366523e-03, 8.20307195e-03,\n",
       "        8.52124148e-03, 7.60171564e-03, 6.24961422e-03, 7.68812209e-03,\n",
       "        3.65684696e-02, 1.60183198e-02, 7.66535226e-02, 7.01162068e-02,\n",
       "        4.86427365e-02, 2.43573752e-02, 4.70729073e-02, 5.92084212e-02,\n",
       "        7.24680583e-02, 7.83562842e-02, 3.41734689e-02, 5.79705998e-02,\n",
       "        2.77193810e-02, 1.66649274e-02, 9.86950393e-03, 2.01397525e-02,\n",
       "        3.29967145e-02, 6.42196399e-02, 5.44414010e-02, 3.47084889e-02,\n",
       "        3.50430739e-02, 5.77474861e-02, 5.06821393e-02, 4.94174032e-02,\n",
       "        1.03086504e-02, 1.68168767e-02, 8.11348822e-03, 1.56303693e-02,\n",
       "        1.63578141e-02, 8.76140531e-03, 3.07036746e-02, 2.73332362e-02,\n",
       "        2.18842996e-02, 1.96415977e-02, 1.25685234e-02, 1.40171589e-02,\n",
       "        6.95264919e-03, 9.35419876e-03, 6.37245634e-03, 9.33706942e-03,\n",
       "        9.37727418e-03, 9.97888370e-03, 1.05203324e-02, 1.10378482e-02,\n",
       "        7.99651257e-03, 6.58562713e-03, 6.99851368e-03, 7.60184122e-03,\n",
       "        3.55445005e-03, 6.68182302e-02, 1.07303180e-02, 5.20558274e-03,\n",
       "        2.83376306e-02, 1.64447315e-03, 1.07977392e-02, 4.73017301e-02,\n",
       "        1.03309445e-02, 1.11394855e-03, 1.64387292e-02, 7.43549681e-03,\n",
       "        2.85489914e-02, 6.36192064e-02, 1.09946696e-03, 1.13545731e-02,\n",
       "        1.83159801e-01, 1.44824142e-02, 2.09624597e-03, 3.90820302e-02,\n",
       "        1.62244876e-02, 1.35399687e-02, 2.03800524e-03, 5.52874134e-04,\n",
       "        4.63035311e-03, 6.27102749e-02, 4.32929563e-03, 1.22414962e-02,\n",
       "        2.48698085e-02, 5.09790474e-03, 1.02815170e-02, 8.06467863e-02,\n",
       "        3.41037039e-03, 7.36128375e-03, 6.14232979e-02, 2.18351556e-03,\n",
       "        7.70450047e-02, 2.54774085e-01, 7.73720750e-03, 5.90747483e-03,\n",
       "        1.17659509e-02, 2.15355215e-04, 1.97386313e-03, 3.57092370e-03,\n",
       "        1.76884388e-02, 3.14917901e-03, 1.34798799e-01, 6.81837629e-03,\n",
       "        1.57495553e-04, 2.06472258e-02, 2.18848321e-04, 3.70845397e-04,\n",
       "        1.22550181e-02, 5.68858675e-03, 4.76455971e-03, 1.85221883e-01,\n",
       "        6.38715491e-03, 5.43673569e-03, 7.95899976e-02, 4.54519662e-03,\n",
       "        9.86459995e-03, 2.59002721e-01, 8.68611933e-04, 1.04688350e-03,\n",
       "        7.76649948e-02, 8.14416388e-04, 5.18381853e-03, 6.68519898e-02,\n",
       "        5.90595711e-03, 1.82022909e-04, 4.41139740e-02, 2.06732852e-03,\n",
       "        1.22916605e-02, 1.73717283e-01, 5.99922136e-04, 4.91710006e-03,\n",
       "        3.53924452e-01, 2.14450000e-03, 5.30787977e-04, 1.50096950e-02,\n",
       "        2.81814034e-03, 1.11021202e-04, 2.47785026e-02, 2.49938421e-03,\n",
       "        5.74617585e-03, 4.03967024e-01, 8.42164445e-03, 5.69840019e-03,\n",
       "        7.32478871e-02, 4.35387584e-04, 2.85209796e-03, 1.16979983e-01,\n",
       "        2.22901980e-03, 6.56895819e-03, 2.09342413e-01, 4.03215378e-03,\n",
       "        7.53165847e-03, 2.34910702e-04, 6.73246695e-03, 2.57655749e-03,\n",
       "        4.30760637e-03, 3.83891794e-03, 6.86224817e-03, 1.45355585e-03,\n",
       "        1.92657898e-03, 3.51539645e-03, 5.49379088e-03, 5.88072179e-03,\n",
       "        8.74937624e-04, 7.95730496e-03, 1.08782320e-02, 6.90375934e-03,\n",
       "        3.42287144e-03, 1.29047689e-03, 1.75983837e-03, 7.51480955e-04,\n",
       "        4.48397775e-03, 2.05294354e-03, 4.15270829e-03, 1.76504759e-03,\n",
       "        2.35903379e-03, 9.43292141e-03, 4.55565120e-03, 2.16814662e-03,\n",
       "        6.41003135e-03, 4.03969762e-03, 2.41451494e-03, 2.25983422e-04,\n",
       "        5.52639065e-03, 8.67060341e-04, 9.70165217e-04, 4.05182044e-03,\n",
       "        1.18202657e-03, 6.62613329e-03, 6.21941087e-03, 4.84507321e-03,\n",
       "        2.60191477e-03, 6.16315421e-04, 6.01075779e-03, 1.05190505e-03,\n",
       "        6.07944760e-03, 1.60640026e-03, 1.59003273e-03, 8.20979678e-04,\n",
       "        1.42190323e-03, 4.25871305e-03, 5.47169938e-03, 3.14710102e-03,\n",
       "        8.43480132e-03, 5.70693419e-03, 4.61885584e-03, 8.21187660e-03,\n",
       "        8.77695154e-03, 4.20589356e-03, 8.14023986e-03, 1.05878180e-02,\n",
       "        4.40861734e-03, 3.55551733e-04, 4.54131581e-04, 3.78294225e-03,\n",
       "        9.72775099e-03, 2.75200384e-03, 1.05050166e-02, 1.64754265e-03,\n",
       "        8.67451391e-03, 7.21539636e-03, 4.98334208e-03, 4.92488779e-03,\n",
       "        2.70294754e-03, 2.10207635e-03, 3.76412632e-03, 1.77180469e-03,\n",
       "        9.69100012e-03, 5.96396355e-03, 5.89570018e-03, 8.42663112e-03,\n",
       "        6.96689444e-04, 5.63985292e-03, 3.87069228e-03, 6.09804565e-03,\n",
       "        5.38584153e-03, 6.04151564e-03, 5.79982156e-03, 4.40377454e-03,\n",
       "        1.38531370e-03, 3.47588870e-03, 7.04568231e-03, 3.19289896e-03,\n",
       "        8.16474852e-03, 5.04940916e-03, 4.20758794e-03, 5.45093138e-03]),\n",
       " 'rank_test_neg_log_loss': array([ 96,  44,  59,  98,  90,  56, 150,  60,  63, 158, 180, 104,  52,\n",
       "         48,  38,  82,  94,  51,  57,  47,  53,  89,  55, 105,  11,  43,\n",
       "         58,  37,  22,  35,  41,   5,  40,  42,  18,  20,  15,  32,  36,\n",
       "         17,  12,  23,  26,  28,  33,  27,  21,  29,  77, 115, 130, 146,\n",
       "        109, 113, 121, 156, 157, 142, 161, 169,  49,  65,   1, 112, 134,\n",
       "        132, 118, 124,  85, 119, 116, 131,   3,  45,  39,  24,  10,  31,\n",
       "         54,  62,  71,  46,  34,  50,   4,  19,   2,  13,  16,  30,  14,\n",
       "         25,   7,   6,   8,   9, 251, 275, 250, 241, 267, 248, 232, 262,\n",
       "        240, 233, 272, 234, 252, 266, 253, 236, 278, 243, 231, 276, 238,\n",
       "        227, 277, 229, 255, 280, 249, 244, 286, 247, 235, 259, 242, 226,\n",
       "        263, 228, 256, 279, 254, 245, 260, 246, 237, 273, 239, 230, 261,\n",
       "        225, 219, 258, 221, 200, 287, 193, 199, 274, 189, 186, 281, 196,\n",
       "        217, 283, 222, 214, 269, 206, 197, 268, 205, 194, 271, 192, 218,\n",
       "        288, 223, 210, 284, 208, 198, 264, 184, 202, 270, 190, 224, 285,\n",
       "        220, 213, 265, 215, 204, 282, 188, 207, 257, 201, 209, 182, 181,\n",
       "        164, 172, 171, 152, 159, 151, 138, 139, 143, 187, 195, 212, 176,\n",
       "        163, 166, 145, 155, 149, 140, 136, 141, 183, 211, 185, 165, 175,\n",
       "        167, 153, 148, 154, 137, 144, 147, 191, 203, 216, 177, 178, 179,\n",
       "        173, 170, 174, 160, 162, 168, 108, 122, 117,  88,  93,  91,  76,\n",
       "         73,  92,  72,  64,  70, 125, 120, 135, 102, 110, 103,  84,  87,\n",
       "         86,  69,  75,  66, 128, 133, 126,  95, 107,  97,  74,  79,  80,\n",
       "         61,  68,  67, 129, 123, 127, 106, 114, 111, 101,  99, 100,  81,\n",
       "         78,  83])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRIAL_2_MLP.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL TWO NEG LOG LOSS RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'tanh',\n",
       " 'classifier__alpha': 0.001,\n",
       " 'classifier__hidden_layer_sizes': (5,),\n",
       " 'classifier__learning_rate': 'adaptive',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL TWO NEG LOG LOSS RESULTS ---------------------------------------------\")\n",
    "TRIAL_2_MLP.cv_results_['rank_test_neg_log_loss']\n",
    "TRIAL_2_MLP.cv_results_['params'][ np.argmin(TRIAL_2_MLP.cv_results_['rank_test_neg_log_loss']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL TWO F1 RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'tanh',\n",
       " 'classifier__alpha': 0.001,\n",
       " 'classifier__hidden_layer_sizes': (5,),\n",
       " 'classifier__learning_rate': 'adaptive',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL TWO F1 RESULTS ---------------------------------------------\")\n",
    "TRIAL_2_MLP.cv_results_['rank_test_f1_micro']\n",
    "TRIAL_2_MLP.cv_results_['params'][ np.argmin(TRIAL_2_MLP.cv_results_['rank_test_f1_micro']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL TWO ROC AUC RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.001,\n",
       " 'classifier__hidden_layer_sizes': (15,),\n",
       " 'classifier__learning_rate': 'constant',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL TWO ROC AUC RESULTS ---------------------------------------------\")\n",
    "TRIAL_2_MLP.cv_results_['rank_test_roc_auc_ovo']\n",
    "TRIAL_2_MLP.cv_results_['params'][ np.argmin(TRIAL_2_MLP.cv_results_['rank_test_roc_auc_ovo']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 2 MLP using best NEG LOG LOSS hyperparameters :0.9928666666666667\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 2 MLP using best F1 hyperparameters :0.9962\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 2 MLP using best ROC_AUC hyperparameters :0.9965\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best NEG LOG LOSS results to train on test set, \n",
    "p = TRIAL_2_MLP.cv_results_['params'][ np.argmin(TRIAL_2_MLP.cv_results_['rank_test_neg_log_loss']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train__neg_log_loss = pipe.predict(X_train)\n",
    "y_pred_test_neg_log_loss = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 2 MLP using best NEG LOG LOSS hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_neg_log_loss)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best F1 results to train on test set, LR_Trial 1\n",
    "p = TRIAL_2_MLP.cv_results_['params'][ np.argmin(TRIAL_2_MLP.cv_results_['rank_test_f1_micro']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train_roc_auc = pipe.predict(X_train)\n",
    "y_pred_test_f1 = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 2 MLP using best F1 hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_f1)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best ROC_AUC results to train on test set, LR_Trial 1\n",
    "p  = TRIAL_2_MLP.cv_results_['params'][ np.argmin(TRIAL_2_MLP.cv_results_['rank_test_roc_auc_ovo']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train_roc_auc = pipe.predict(X_train)\n",
    "y_pred_test_roc_auc = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 2 MLP using best ROC_AUC hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_roc_auc)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTI LAYERED PERCEPTRON TRIAL THREE ON FIRE WALL DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 288 candidates, totalling 576 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed:    9.5s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   10.9s\n",
      "[Parallel(n_jobs=-1)]: Done 189 tasks      | elapsed:   12.3s\n",
      "[Parallel(n_jobs=-1)]: Done 210 tasks      | elapsed:   15.4s\n",
      "[Parallel(n_jobs=-1)]: Done 233 tasks      | elapsed:   18.4s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:   21.2s\n",
      "[Parallel(n_jobs=-1)]: Done 281 tasks      | elapsed:   25.0s\n",
      "[Parallel(n_jobs=-1)]: Done 306 tasks      | elapsed:   28.8s\n",
      "[Parallel(n_jobs=-1)]: Done 333 tasks      | elapsed:   33.1s\n",
      "[Parallel(n_jobs=-1)]: Done 360 tasks      | elapsed:   37.3s\n",
      "[Parallel(n_jobs=-1)]: Done 389 tasks      | elapsed:   41.7s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:   46.4s\n",
      "[Parallel(n_jobs=-1)]: Done 449 tasks      | elapsed:   51.7s\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:   57.1s\n",
      "[Parallel(n_jobs=-1)]: Done 513 tasks      | elapsed:  1.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL THREE RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 576 out of 576 | elapsed:  1.2min finished\n"
     ]
    }
   ],
   "source": [
    "ITERATIONS = 1\n",
    "for i in range(1):\n",
    "    #For reach \"trial\", randomly select 5000 data points\n",
    "    random5000DataPoints = FireWALLData.sample(n = 5000, replace = True)\n",
    "    #From that 5000, create x set and y set\n",
    "    xSet = random5000DataPoints.iloc[:,0:9].values\n",
    "    ySet = random5000DataPoints.iloc[:,10].values\n",
    "    ySet = encodeLabel.fit_transform(ySet)\n",
    "    # Run GridSearch to find best hyperparameters per 5000 data points \n",
    "    \n",
    "    pipe = Pipeline([('std', StandardScaler()),('classifier', MLPClassifier(max_iter = 250))])\n",
    "\n",
    "    search_space = [\n",
    "                {\n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                {\n",
    "                 \n",
    "                  \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['sgd'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                \n",
    "                {\n",
    "                \n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['adam'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                ]\n",
    "    \n",
    "    #'precision_micro','roc_auc_ovo','jaccard','neg_log_loss'] JACCARD DOES NOT WORK FOR NON BINARY?\n",
    "    runHyperParamSearch = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=2), \n",
    "                          scoring=['recall_micro','f1_micro','roc_auc_ovo','neg_log_loss'], \n",
    "                          refit=False, verbose=10, n_jobs = -1)\n",
    "    TRIAL_3_MLP = runHyperParamSearch.fit(xSet, ySet)\n",
    "    print(\"---------------------------------TRIAL THREE RESULTS ---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.36231291, 0.52620339, 0.5124402 , 0.78767884, 0.49817944,\n",
       "        0.78893197, 0.91203606, 0.97333837, 1.02513182, 1.11445808,\n",
       "        1.09394145, 1.07117164, 0.71261287, 0.63279343, 0.66507304,\n",
       "        0.89527202, 0.67132819, 0.94256079, 0.98885071, 1.06241417,\n",
       "        1.08943701, 1.22580612, 1.19102275, 1.23431242, 0.79643512,\n",
       "        0.65856683, 0.72412169, 1.00085998, 0.89226758, 0.8604908 ,\n",
       "        1.0541563 , 1.09419012, 1.05265462, 1.27134395, 1.15449309,\n",
       "        1.01762474, 0.46339929, 0.60577142, 0.66131818, 0.53621197,\n",
       "        0.58775651, 0.68508899, 0.97158563, 0.81670177, 0.93180227,\n",
       "        0.89251828, 0.76040363, 0.91978896, 0.87200034, 0.85423398,\n",
       "        0.67908418, 1.00336242, 0.98834991, 1.0138731 , 1.24882412,\n",
       "        1.22080028, 1.17926431, 1.41446686, 1.28910732, 1.31312883,\n",
       "        0.75064898, 0.70560622, 0.72412276, 1.00060976, 1.05265594,\n",
       "        0.9518187 , 1.33865106, 1.16675436, 1.24982727, 1.44549394,\n",
       "        1.50254261, 1.47977173, 0.89602005, 0.92204261, 0.82295775,\n",
       "        0.9638288 , 0.97283554, 1.05140483, 1.20178235, 1.1880213 ,\n",
       "        1.17200828, 1.43698382, 1.31713319, 1.27684879, 0.79042983,\n",
       "        0.74363887, 0.71061063, 0.88776481, 0.91803968, 0.8927685 ,\n",
       "        1.07892799, 1.06466591, 1.05765903, 1.16074872, 1.18251705,\n",
       "        1.15974653, 1.98070037, 1.46776175, 2.15585315, 2.1340853 ,\n",
       "        2.15060055, 2.0655266 , 2.25318837, 0.95281899, 2.15560436,\n",
       "        2.31899476, 2.33475816, 2.22916698, 1.85934925, 1.73549294,\n",
       "        1.83958209, 1.99946964, 0.78467488, 1.9859575 , 2.23817492,\n",
       "        2.34151447, 2.38379979, 2.40982199, 1.71147275, 2.35677636,\n",
       "        1.95793378, 1.69745982, 2.36603463, 2.5269233 , 1.20253444,\n",
       "        2.41958094, 2.50415361, 2.37529254, 2.62850928, 2.52692342,\n",
       "        1.69721019, 2.45310926, 2.13633835, 1.34365571, 2.2154057 ,\n",
       "        2.27670825, 2.34351552, 2.65278125, 2.81567109, 2.43159199,\n",
       "        2.76262605, 2.39105666, 2.09480095, 2.53468013, 2.29447377,\n",
       "        1.86335266, 2.17487037, 2.22891688, 2.46637142, 2.2596935 ,\n",
       "        2.68080544, 2.63876855, 2.51841569, 2.70057321, 2.63201332,\n",
       "        2.62250578, 2.22191155, 2.27195394, 2.02624261, 2.24443042,\n",
       "        2.32224774, 2.26544809, 2.56495655, 2.59998584, 2.4368459 ,\n",
       "        2.80641365, 2.53918409, 2.76237547, 2.1345855 , 1.93316257,\n",
       "        2.23016763, 2.3505224 , 2.25168693, 2.27270532, 2.42683768,\n",
       "        2.5081569 , 2.42908967, 2.76988149, 2.62475598, 2.778139  ,\n",
       "        2.36878729, 2.29372227, 2.15185082, 2.33525825, 1.99146271,\n",
       "        2.23116994, 2.45986521, 2.67204797, 2.46461904, 2.38505089,\n",
       "        2.45836473, 2.51015866, 2.34126413, 2.39380836, 2.17061651,\n",
       "        2.43484378, 2.54819119, 2.53242791, 2.62700927, 2.70232463,\n",
       "        2.59423089, 2.53718305, 2.77663815, 2.68956256, 2.35502541,\n",
       "        2.23817444, 2.31223834, 2.49514556, 2.58997762, 2.60148704,\n",
       "        2.74135792, 2.68781221, 2.63326514, 2.83744037, 3.03636122,\n",
       "        2.93852675, 2.29647553, 2.11381817, 2.2141546 , 2.19613802,\n",
       "        2.5156641 , 2.46386862, 2.77438557, 2.81041658, 2.65102971,\n",
       "        2.93377304, 3.07339323, 3.14945877, 2.74310923, 2.39731205,\n",
       "        2.7361033 , 2.54193616, 2.53293002, 2.78464472, 2.72584331,\n",
       "        3.13494503, 2.81692231, 3.23478222, 2.82793236, 2.93902767,\n",
       "        2.53392899, 2.49914908, 2.49964964, 2.8471992 , 2.75712132,\n",
       "        2.48864043, 3.11567938, 3.13694656, 3.26180482, 3.60134733,\n",
       "        3.09190893, 2.95629263, 2.67705274, 2.44635367, 2.7160852 ,\n",
       "        2.40481746, 2.53593147, 2.60073709, 2.58447242, 2.44635403,\n",
       "        2.45110774, 2.57721734, 2.62525749, 2.65878654, 2.1988914 ,\n",
       "        2.14234221, 2.46211743, 2.63376546, 2.85495532, 2.59197927,\n",
       "        3.04411888, 2.80641377, 3.03260887, 2.94578338, 2.95704269,\n",
       "        2.82618093, 2.27395535, 2.31874442, 2.27620745, 2.75837159,\n",
       "        2.5531956 , 2.70983028, 2.85245371, 2.81116724, 2.83744085,\n",
       "        2.7403568 , 2.50090003, 2.19413686]),\n",
       " 'std_fit_time': array([2.08178639e-01, 8.03189278e-02, 1.25607967e-01, 4.00292873e-03,\n",
       "        9.13281441e-02, 4.52891588e-02, 9.13277864e-02, 8.00728798e-03,\n",
       "        1.72663927e-02, 6.30540848e-02, 8.15708637e-02, 2.12677717e-02,\n",
       "        1.95164680e-02, 6.28046989e-02, 4.05343771e-02, 3.05261612e-02,\n",
       "        3.15521359e-01, 1.72659159e-02, 2.40198374e-02, 7.00569153e-03,\n",
       "        4.00347710e-02, 9.75954533e-03, 9.25806761e-02, 2.07670927e-02,\n",
       "        3.02760601e-02, 4.80421782e-02, 3.90335321e-02, 2.90248394e-02,\n",
       "        6.55561686e-02, 4.27863598e-02, 5.98013401e-02, 5.75590134e-03,\n",
       "        1.02585554e-02, 4.92928028e-02, 5.00917435e-04, 6.75714016e-03,\n",
       "        1.50628448e-01, 7.23139048e-02, 5.02933264e-02, 1.32863522e-01,\n",
       "        1.87660456e-02, 3.85333300e-02, 2.92745829e-02, 1.81155801e-01,\n",
       "        7.20615387e-02, 2.82993317e-01, 9.25898552e-03, 1.36617064e-01,\n",
       "        4.37875986e-02, 1.05104446e-02, 8.50733519e-02, 8.05697441e-02,\n",
       "        6.30543232e-02, 9.15777683e-02, 6.78086281e-02, 6.88079596e-02,\n",
       "        9.75918770e-03, 6.75535202e-03, 2.05163956e-02, 3.50297689e-02,\n",
       "        2.65195370e-02, 1.25095844e-02, 2.50205994e-02, 3.82820368e-02,\n",
       "        2.07686424e-02, 1.45123005e-02, 5.15435934e-02, 3.27779055e-02,\n",
       "        2.17694044e-02, 1.32608414e-02, 4.27862406e-02, 9.50777531e-03,\n",
       "        1.27607584e-02, 7.23116398e-02, 3.75294685e-03, 5.25457859e-02,\n",
       "        4.45387363e-02, 6.90590143e-02, 2.75218487e-03, 4.00348902e-02,\n",
       "        3.50332260e-03, 1.82654858e-02, 3.20272446e-02, 2.42702961e-02,\n",
       "        5.07955551e-02, 3.10257673e-02, 8.50796700e-03, 1.60154104e-02,\n",
       "        8.75747204e-03, 2.75243521e-02, 1.00088120e-02, 6.75547123e-03,\n",
       "        1.07594728e-02, 2.42702961e-02, 7.00616837e-03, 2.25269794e-03,\n",
       "        1.10098124e-02, 4.46885109e-01, 7.20633268e-02, 6.48058653e-02,\n",
       "        8.28219652e-02, 5.77996969e-02, 5.57980537e-02, 3.10767293e-01,\n",
       "        1.17599964e-02, 7.25615025e-02, 9.73337889e-02, 7.75694847e-03,\n",
       "        4.75358963e-03, 1.34615183e-01, 1.10095739e-02, 2.62717009e-02,\n",
       "        3.38290453e-01, 1.75154209e-03, 6.48055077e-02, 1.06090903e-01,\n",
       "        7.48147964e-02, 1.22604370e-02, 8.45227242e-01, 6.93095922e-02,\n",
       "        7.93181658e-02, 1.34115338e-01, 4.75412607e-02, 8.23198557e-02,\n",
       "        8.28212023e-01, 2.38418579e-07, 1.40121341e-01, 1.25857592e-01,\n",
       "        1.01837873e-01, 1.25859022e-01, 8.58988643e-01, 4.50372696e-03,\n",
       "        1.39120221e-01, 4.32371736e-01, 5.15441895e-02, 1.39869571e-01,\n",
       "        1.52631879e-01, 1.62139416e-01, 1.62633657e-02, 2.35702276e-01,\n",
       "        1.92662477e-02, 1.37118220e-01, 8.22207451e-01, 2.00171947e-01,\n",
       "        2.57722020e-01, 4.46633697e-01, 9.10786390e-02, 1.10082626e-02,\n",
       "        5.47975302e-02, 1.07843041e-01, 6.70573711e-02, 2.85242796e-02,\n",
       "        6.73084259e-02, 2.32706070e-02, 8.03189278e-02, 1.00837111e-01,\n",
       "        1.42622590e-01, 1.47126794e-01, 8.70748758e-02, 8.00669193e-03,\n",
       "        1.42619610e-02, 1.08593464e-01, 4.67897654e-02, 3.32787037e-02,\n",
       "        4.07847166e-02, 3.70318890e-02, 1.93165779e-01, 1.28610492e-01,\n",
       "        9.58324671e-02, 4.17358994e-01, 4.75358963e-03, 1.05093718e-02,\n",
       "        1.18352294e-01, 3.47795486e-02, 1.47877216e-01, 1.20106936e-02,\n",
       "        4.25361395e-02, 8.25712681e-02, 2.50204802e-02, 1.19852185e-01,\n",
       "        8.68260860e-02, 3.92833948e-02, 8.55735540e-02, 4.82921600e-02,\n",
       "        3.90084982e-01, 9.98351574e-02, 6.48061037e-02, 4.97928858e-02,\n",
       "        4.20364141e-02, 7.70672560e-02, 7.23116398e-02, 7.50672817e-03,\n",
       "        1.00837111e-01, 1.81906223e-01, 1.02587938e-02, 3.47800255e-02,\n",
       "        1.49629235e-01, 3.52803469e-02, 2.25198269e-03, 4.40368652e-02,\n",
       "        1.52130842e-01, 4.20362949e-02, 7.63150454e-02, 3.77824306e-02,\n",
       "        4.05350924e-02, 7.13114738e-02, 1.35366559e-01, 5.25453091e-02,\n",
       "        2.27693319e-02, 1.46876216e-01, 2.18188167e-01, 2.61224389e-01,\n",
       "        2.28196025e-01, 1.45124197e-02, 3.52797508e-02, 7.30625391e-02,\n",
       "        7.40643740e-02, 1.00088120e-02, 2.50697136e-04, 2.82746553e-02,\n",
       "        1.45125389e-02, 2.13934183e-01, 4.50383425e-02, 1.97669864e-01,\n",
       "        1.94917798e-01, 2.09930420e-01, 5.62993288e-02, 4.22865152e-02,\n",
       "        9.73334312e-02, 6.38049841e-02, 2.06927776e-01, 6.58073425e-02,\n",
       "        1.94917560e-01, 1.45874858e-01, 1.39619946e-01, 5.13692021e-01,\n",
       "        6.55561686e-02, 3.64813805e-01, 1.57635450e-01, 2.45209932e-02,\n",
       "        1.05841517e-01, 2.89749384e-01, 6.25537634e-02, 1.09844446e-01,\n",
       "        1.14849091e-01, 3.00256014e-02, 9.45810080e-02, 2.07685232e-02,\n",
       "        9.85848904e-02, 4.00594592e-01, 1.30862713e-01, 2.19939709e-01,\n",
       "        2.67732143e-02, 1.18851423e-01, 1.66393042e-01, 9.23293829e-02,\n",
       "        1.24356389e-01, 1.97168827e-01, 5.98013401e-02, 2.57726908e-02,\n",
       "        6.50596619e-03, 7.85675049e-02, 5.35461903e-02, 1.05092525e-02,\n",
       "        1.70147419e-02, 1.25101805e-02, 8.00728798e-03, 3.51802111e-01,\n",
       "        8.80752802e-02, 3.20525169e-01, 1.25110149e-02, 2.24192739e-01,\n",
       "        4.25359011e-02, 2.83994436e-01, 4.60400581e-02, 1.28861427e-01,\n",
       "        9.50813293e-02, 4.42886353e-02, 3.67815495e-02, 1.05591297e-01,\n",
       "        1.33114219e-01, 1.14098310e-01, 3.15275192e-02, 2.75158882e-03,\n",
       "        1.50167942e-03, 1.95165873e-02, 1.02589130e-02, 4.92913723e-02]),\n",
       " 'mean_score_time': array([0.02051747, 0.02151859, 0.02251935, 0.02302086, 0.02151775,\n",
       "        0.02451909, 0.0235194 , 0.02552128, 0.02251863, 0.02452219,\n",
       "        0.02552176, 0.02552116, 0.02251887, 0.02777398, 0.02376866,\n",
       "        0.03928387, 0.02326989, 0.02927494, 0.02502179, 0.03052628,\n",
       "        0.02627361, 0.02827251, 0.02502084, 0.02927637, 0.04128432,\n",
       "        0.02101815, 0.03252792, 0.02627289, 0.02527225, 0.02326882,\n",
       "        0.02552128, 0.02376997, 0.02377152, 0.02852321, 0.02527249,\n",
       "        0.0270232 , 0.02502203, 0.02376938, 0.02802467, 0.02306366,\n",
       "        0.02677155, 0.02477133, 0.02627313, 0.03477991, 0.02777267,\n",
       "        0.02252018, 0.02402043, 0.0245229 , 0.02452087, 0.0250212 ,\n",
       "        0.02527106, 0.02552199, 0.02977443, 0.02977538, 0.02452159,\n",
       "        0.02351975, 0.02352107, 0.02427006, 0.0280242 , 0.02477169,\n",
       "        0.02476764, 0.02151918, 0.02201819, 0.03678095, 0.02977777,\n",
       "        0.02927542, 0.0242703 , 0.03703177, 0.02251744, 0.04453778,\n",
       "        0.02652228, 0.02827382, 0.04003429, 0.02101743, 0.02577269,\n",
       "        0.02477121, 0.03678179, 0.02477288, 0.02377081, 0.02577209,\n",
       "        0.02427137, 0.02652311, 0.03628147, 0.02477288, 0.02502108,\n",
       "        0.02151799, 0.02251923, 0.0247705 , 0.02377164, 0.03227723,\n",
       "        0.02527201, 0.02502179, 0.02352083, 0.02452064, 0.02352095,\n",
       "        0.02527213, 0.02151859, 0.02076769, 0.02151847, 0.02477169,\n",
       "        0.02502084, 0.02051747, 0.02351964, 0.02276993, 0.02076793,\n",
       "        0.02276909, 0.02176881, 0.02226937, 0.02076805, 0.02051747,\n",
       "        0.02276945, 0.02176845, 0.02702332, 0.02777386, 0.0300256 ,\n",
       "        0.03427899, 0.02702427, 0.02427113, 0.03052592, 0.02302074,\n",
       "        0.03002584, 0.02201915, 0.02076769, 0.02702367, 0.02552211,\n",
       "        0.02151835, 0.02427077, 0.02151859, 0.03352904, 0.03027582,\n",
       "        0.02777386, 0.02902579, 0.02251863, 0.02126861, 0.04203582,\n",
       "        0.02602184, 0.02427053, 0.02827418, 0.02402079, 0.02827346,\n",
       "        0.02502143, 0.02877486, 0.02427065, 0.03202736, 0.02401996,\n",
       "        0.02176821, 0.02377009, 0.03127706, 0.02402008, 0.02777398,\n",
       "        0.02402115, 0.02477181, 0.02602255, 0.02402067, 0.02827454,\n",
       "        0.02452087, 0.02477026, 0.03377914, 0.0277735 , 0.02301943,\n",
       "        0.02952504, 0.02652359, 0.02301931, 0.02352071, 0.03452921,\n",
       "        0.02977562, 0.0242703 , 0.02577221, 0.0277741 , 0.02452099,\n",
       "        0.02427137, 0.02301884, 0.02627254, 0.04328704, 0.03127694,\n",
       "        0.02727318, 0.02702284, 0.03828263, 0.02527189, 0.02477145,\n",
       "        0.02351952, 0.0275234 , 0.02326977, 0.02402031, 0.02176845,\n",
       "        0.02477062, 0.02752352, 0.02977502, 0.02402151, 0.02552187,\n",
       "        0.02427053, 0.02352011, 0.02276886, 0.02402115, 0.02126861,\n",
       "        0.02352095, 0.03427947, 0.02251911, 0.02377069, 0.02802312,\n",
       "        0.03052604, 0.02852392, 0.02627254, 0.03002596, 0.02302039,\n",
       "        0.02352047, 0.02126825, 0.0307765 , 0.04028428, 0.03127706,\n",
       "        0.02402115, 0.03077614, 0.03728163, 0.03978372, 0.02802432,\n",
       "        0.02151918, 0.02101803, 0.02026713, 0.02802372, 0.02752423,\n",
       "        0.02752399, 0.02852476, 0.02602279, 0.02677286, 0.06130254,\n",
       "        0.03177762, 0.03227746, 0.02392614, 0.03052616, 0.02577162,\n",
       "        0.02627254, 0.0275234 , 0.02276838, 0.02602232, 0.02752447,\n",
       "        0.02402174, 0.03978467, 0.02702284, 0.03878272, 0.02377045,\n",
       "        0.02602255, 0.02902532, 0.02852488, 0.03227746, 0.04053438,\n",
       "        0.02827406, 0.03027558, 0.03878379, 0.0282743 , 0.03402925,\n",
       "        0.03127706, 0.0312767 , 0.02301931, 0.02251887, 0.02126861,\n",
       "        0.02151871, 0.02402008, 0.02427018, 0.02176905, 0.02452183,\n",
       "        0.02276897, 0.02326977, 0.03452981, 0.02652264, 0.03803313,\n",
       "        0.03928375, 0.02577174, 0.02752507, 0.02452147, 0.0290252 ,\n",
       "        0.02552092, 0.03828239, 0.02727282, 0.0405345 , 0.02427137,\n",
       "        0.02452052, 0.02752376, 0.02502096, 0.03277814, 0.02852499,\n",
       "        0.02727461, 0.02452135, 0.02502048, 0.02201915, 0.01951623,\n",
       "        0.01751471, 0.01401293, 0.01401126]),\n",
       " 'std_score_time': array([4.99844551e-04, 1.50120258e-03, 1.50156021e-03, 1.00100040e-03,\n",
       "        5.00679016e-04, 1.00076199e-03, 1.00171566e-03, 1.50203705e-03,\n",
       "        4.99725342e-04, 1.00076199e-03, 2.00271606e-03, 2.00164318e-03,\n",
       "        1.00088120e-03, 5.25486469e-03, 2.25305557e-03, 4.25469875e-03,\n",
       "        7.51972198e-04, 6.25562668e-03, 1.50203705e-03, 6.50632381e-03,\n",
       "        2.25198269e-03, 4.75537777e-03, 3.00335884e-03, 1.24907494e-03,\n",
       "        1.67653561e-02, 9.99569893e-04, 8.50772858e-03, 1.25133991e-03,\n",
       "        7.49945641e-04, 2.50458717e-04, 2.00128555e-03, 2.50935555e-04,\n",
       "        7.50780106e-04, 1.00111961e-03, 1.25181675e-03, 2.50160694e-03,\n",
       "        3.50379944e-03, 2.75087357e-03, 9.99927521e-04, 1.46079063e-03,\n",
       "        1.25098228e-03, 2.75290012e-03, 2.25317478e-03, 1.75225735e-03,\n",
       "        5.25498390e-03, 1.00100040e-03, 2.00271606e-03, 1.00266933e-03,\n",
       "        0.00000000e+00, 4.50479984e-03, 4.25374508e-03, 1.00135803e-03,\n",
       "        5.25569916e-03, 4.75335121e-03, 2.38418579e-07, 7.15255737e-07,\n",
       "        1.00123882e-03, 7.51018524e-04, 4.00328636e-03, 2.50816345e-04,\n",
       "        1.24764442e-03, 1.00135803e-03, 5.00917435e-04, 1.75201893e-03,\n",
       "        2.53200531e-04, 2.25257874e-03, 7.50541687e-04, 1.20109320e-02,\n",
       "        4.99248505e-04, 8.00752640e-03, 2.50256062e-03, 7.50303268e-04,\n",
       "        1.50108337e-03, 5.00798225e-04, 2.50220299e-04, 2.25257874e-03,\n",
       "        1.25074387e-03, 1.25265121e-03, 1.25169754e-03, 2.75158882e-03,\n",
       "        7.50422478e-04, 1.19209290e-06, 4.25398350e-03, 7.51972198e-04,\n",
       "        9.53674316e-07, 1.00111961e-03, 1.50048733e-03, 1.74856186e-03,\n",
       "        2.49505043e-04, 7.25567341e-03, 1.25181675e-03, 4.99486923e-04,\n",
       "        1.00076199e-03, 1.00183487e-03, 2.00080872e-03, 7.50780106e-04,\n",
       "        5.00321388e-04, 2.50101089e-04, 2.50220299e-03, 1.75118446e-03,\n",
       "        1.50179863e-03, 5.00321388e-04, 5.00082970e-04, 1.25169754e-03,\n",
       "        7.50780106e-04, 7.51137733e-04, 7.49826431e-04, 2.50458717e-04,\n",
       "        7.50422478e-04, 5.00321388e-04, 7.50541687e-04, 7.50899315e-04,\n",
       "        7.00616837e-03, 4.75430489e-03, 1.00052357e-03, 1.22598410e-02,\n",
       "        2.50244141e-03, 7.50899315e-04, 9.50765610e-03, 2.38418579e-07,\n",
       "        9.50825214e-03, 5.00679016e-04, 2.49624252e-04, 1.50215626e-03,\n",
       "        4.00269032e-03, 1.00123882e-03, 3.25179100e-03, 3.57627869e-07,\n",
       "        2.38418579e-07, 3.25345993e-03, 4.25386429e-03, 2.50101089e-03,\n",
       "        1.50084496e-03, 1.25098228e-03, 1.45118237e-02, 1.50215626e-03,\n",
       "        1.25145912e-03, 7.75635242e-03, 2.00188160e-03, 6.25574589e-03,\n",
       "        1.00076199e-03, 6.75642490e-03, 7.50899315e-04, 8.00669193e-03,\n",
       "        5.00917435e-04, 7.50660896e-04, 2.50577927e-04, 2.49981880e-04,\n",
       "        1.00028515e-03, 2.75290012e-03, 1.50036812e-03, 7.51137733e-04,\n",
       "        3.50296497e-03, 5.00440598e-04, 2.25210190e-03, 1.00111961e-03,\n",
       "        1.75189972e-03, 1.32615566e-02, 3.75330448e-03, 1.00052357e-03,\n",
       "        3.00240517e-03, 1.00088120e-03, 1.00159645e-03, 1.00064278e-03,\n",
       "        3.50308418e-03, 5.75518608e-03, 1.75118446e-03, 1.25133991e-03,\n",
       "        5.75518608e-03, 5.00798225e-04, 7.51137733e-04, 5.00679016e-04,\n",
       "        7.51256943e-04, 1.75213814e-03, 9.25779343e-03, 2.50101089e-04,\n",
       "        1.00064278e-03, 1.17604733e-02, 1.25122070e-03, 2.49862671e-04,\n",
       "        4.99486923e-04, 3.00252438e-03, 2.25174427e-03, 1.00100040e-03,\n",
       "        2.50697136e-04, 1.25157833e-03, 3.50308418e-03, 7.50422478e-04,\n",
       "        2.00212002e-03, 3.57627869e-07, 7.50541687e-04, 1.00076199e-03,\n",
       "        1.25110149e-03, 5.00202179e-04, 1.75166130e-03, 5.00679016e-04,\n",
       "        5.75530529e-03, 5.00202179e-04, 2.75313854e-03, 4.50503826e-03,\n",
       "        5.00452518e-03, 5.00392914e-03, 2.50339508e-04, 3.50284576e-03,\n",
       "        1.00100040e-03, 1.00135803e-03, 2.49981880e-04, 2.50339508e-04,\n",
       "        1.12599134e-02, 7.50899315e-04, 5.01394272e-04, 6.75499439e-03,\n",
       "        1.37621164e-02, 4.75430489e-03, 7.00557232e-03, 0.00000000e+00,\n",
       "        1.00135803e-03, 2.49981880e-04, 4.50372696e-03, 1.00088120e-03,\n",
       "        2.50315666e-03, 6.00469112e-03, 2.00212002e-03, 1.75154209e-03,\n",
       "        3.62817049e-02, 4.75358963e-03, 6.75642490e-03, 5.94973564e-04,\n",
       "        8.00704956e-03, 2.50577927e-04, 1.75213814e-03, 3.50320339e-03,\n",
       "        1.25110149e-03, 5.00321388e-04, 2.00176239e-03, 1.50096416e-03,\n",
       "        4.25314903e-03, 3.50308418e-03, 4.25398350e-03, 2.49981880e-04,\n",
       "        3.00300121e-03, 7.15255737e-07, 2.50220299e-03, 7.25638866e-03,\n",
       "        1.55133009e-02, 7.51256943e-04, 2.25210190e-03, 1.37615204e-02,\n",
       "        2.25186348e-03, 1.00092888e-02, 6.25574589e-03, 5.25474548e-03,\n",
       "        1.50084496e-03, 1.00111961e-03, 1.25122070e-03, 4.76837158e-07,\n",
       "        2.50160694e-03, 2.25150585e-03, 2.50339508e-04, 4.99486923e-04,\n",
       "        2.50101089e-04, 2.49505043e-04, 1.00084543e-02, 2.50244141e-03,\n",
       "        1.50167942e-03, 5.75518608e-03, 7.51614571e-04, 6.00445271e-03,\n",
       "        2.50279903e-03, 7.00628757e-03, 5.00798225e-04, 1.12600327e-02,\n",
       "        4.75370884e-03, 1.85163021e-02, 1.25110149e-03, 4.99844551e-04,\n",
       "        4.00257111e-03, 4.00364399e-03, 3.25286388e-03, 2.50208378e-03,\n",
       "        3.25202942e-03, 3.00312042e-03, 2.50256062e-03, 1.00040436e-03,\n",
       "        1.50132179e-03, 2.00176239e-03, 4.99129295e-04, 1.07288361e-06]),\n",
       " 'param_classifier': masked_array(data=[MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__activation': masked_array(data=['logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__alpha': masked_array(data=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__hidden_layer_sizes': masked_array(data=[(5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__learning_rate': masked_array(data=['constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'}],\n",
       " 'split0_test_recall_micro': array([9.912e-01, 9.960e-01, 9.956e-01, 9.924e-01, 9.956e-01, 9.956e-01,\n",
       "        9.928e-01, 9.944e-01, 9.956e-01, 9.952e-01, 9.928e-01, 9.956e-01,\n",
       "        9.956e-01, 9.964e-01, 9.960e-01, 9.952e-01, 9.952e-01, 9.960e-01,\n",
       "        9.920e-01, 9.960e-01, 9.960e-01, 9.944e-01, 9.928e-01, 9.916e-01,\n",
       "        9.956e-01, 9.948e-01, 9.960e-01, 9.964e-01, 9.932e-01, 9.920e-01,\n",
       "        9.936e-01, 9.916e-01, 9.920e-01, 9.928e-01, 9.920e-01, 9.920e-01,\n",
       "        9.916e-01, 9.924e-01, 9.924e-01, 9.920e-01, 9.916e-01, 9.920e-01,\n",
       "        9.928e-01, 9.928e-01, 9.920e-01, 9.916e-01, 9.920e-01, 9.916e-01,\n",
       "        9.932e-01, 9.956e-01, 9.960e-01, 9.956e-01, 9.932e-01, 9.956e-01,\n",
       "        9.940e-01, 9.960e-01, 9.936e-01, 9.952e-01, 9.916e-01, 9.956e-01,\n",
       "        9.968e-01, 9.952e-01, 9.964e-01, 9.940e-01, 9.968e-01, 9.956e-01,\n",
       "        9.956e-01, 9.956e-01, 9.932e-01, 9.948e-01, 9.956e-01, 9.944e-01,\n",
       "        9.972e-01, 9.944e-01, 9.924e-01, 9.964e-01, 9.924e-01, 9.952e-01,\n",
       "        9.960e-01, 9.964e-01, 9.960e-01, 9.932e-01, 9.952e-01, 9.924e-01,\n",
       "        9.916e-01, 9.916e-01, 9.920e-01, 9.916e-01, 9.908e-01, 9.928e-01,\n",
       "        9.916e-01, 9.924e-01, 9.916e-01, 9.912e-01, 9.920e-01, 9.916e-01,\n",
       "        7.872e-01, 2.444e-01, 7.872e-01, 9.668e-01, 1.848e-01, 9.580e-01,\n",
       "        9.444e-01, 5.844e-01, 9.496e-01, 9.456e-01, 5.684e-01, 9.420e-01,\n",
       "        9.664e-01, 1.840e-01, 9.260e-01, 9.508e-01, 5.980e-01, 9.616e-01,\n",
       "        9.448e-01, 2.312e-01, 9.544e-01, 9.424e-01, 5.844e-01, 9.396e-01,\n",
       "        7.752e-01, 2.312e-01, 9.408e-01, 7.800e-01, 5.816e-01, 9.532e-01,\n",
       "        9.524e-01, 1.844e-01, 9.444e-01, 9.396e-01, 4.116e-01, 9.432e-01,\n",
       "        7.816e-01, 2.356e-01, 9.596e-01, 9.580e-01, 4.000e-04, 9.536e-01,\n",
       "        9.396e-01, 5.600e-01, 9.472e-01, 9.452e-01, 5.764e-01, 9.536e-01,\n",
       "        9.388e-01, 3.012e-01, 9.432e-01, 9.424e-01, 4.928e-01, 9.448e-01,\n",
       "        9.408e-01, 5.912e-01, 9.400e-01, 9.428e-01, 5.152e-01, 9.384e-01,\n",
       "        9.432e-01, 9.320e-02, 9.400e-01, 9.408e-01, 1.292e-01, 9.384e-01,\n",
       "        9.356e-01, 1.560e-01, 9.404e-01, 9.416e-01, 4.244e-01, 9.404e-01,\n",
       "        9.416e-01, 9.400e-02, 9.412e-01, 9.408e-01, 9.440e-02, 9.388e-01,\n",
       "        9.404e-01, 3.996e-01, 9.392e-01, 9.400e-01, 5.604e-01, 9.448e-01,\n",
       "        9.388e-01, 1.304e-01, 9.380e-01, 9.428e-01, 5.452e-01, 9.436e-01,\n",
       "        9.428e-01, 4.728e-01, 9.408e-01, 9.440e-01, 5.716e-01, 9.396e-01,\n",
       "        9.460e-01, 9.456e-01, 9.468e-01, 9.496e-01, 9.524e-01, 9.516e-01,\n",
       "        9.540e-01, 9.544e-01, 9.528e-01, 9.544e-01, 9.544e-01, 9.584e-01,\n",
       "        9.484e-01, 9.460e-01, 9.444e-01, 9.528e-01, 9.488e-01, 9.524e-01,\n",
       "        9.548e-01, 9.544e-01, 9.544e-01, 9.548e-01, 9.556e-01, 9.556e-01,\n",
       "        9.460e-01, 9.476e-01, 9.480e-01, 9.496e-01, 9.488e-01, 9.504e-01,\n",
       "        9.544e-01, 9.524e-01, 9.532e-01, 9.544e-01, 9.548e-01, 9.548e-01,\n",
       "        9.456e-01, 9.456e-01, 9.452e-01, 9.468e-01, 9.484e-01, 9.472e-01,\n",
       "        9.488e-01, 9.488e-01, 9.488e-01, 9.492e-01, 9.528e-01, 9.500e-01,\n",
       "        9.620e-01, 9.764e-01, 9.728e-01, 9.784e-01, 9.780e-01, 9.768e-01,\n",
       "        9.848e-01, 9.800e-01, 9.788e-01, 9.844e-01, 9.856e-01, 9.868e-01,\n",
       "        9.724e-01, 9.588e-01, 9.612e-01, 9.804e-01, 9.808e-01, 9.768e-01,\n",
       "        9.828e-01, 9.812e-01, 9.824e-01, 9.844e-01, 9.856e-01, 9.840e-01,\n",
       "        9.744e-01, 9.616e-01, 9.748e-01, 9.780e-01, 9.784e-01, 9.796e-01,\n",
       "        9.836e-01, 9.840e-01, 9.788e-01, 9.840e-01, 9.864e-01, 9.840e-01,\n",
       "        9.616e-01, 9.716e-01, 9.724e-01, 9.620e-01, 9.744e-01, 9.748e-01,\n",
       "        9.784e-01, 9.776e-01, 9.772e-01, 9.820e-01, 9.776e-01, 9.772e-01]),\n",
       " 'split1_test_recall_micro': array([9.948e-01, 9.944e-01, 9.944e-01, 9.956e-01, 9.940e-01, 9.936e-01,\n",
       "        9.968e-01, 9.956e-01, 9.940e-01, 9.952e-01, 9.952e-01, 9.936e-01,\n",
       "        9.948e-01, 9.952e-01, 9.948e-01, 9.920e-01, 9.884e-01, 9.908e-01,\n",
       "        9.956e-01, 9.940e-01, 9.896e-01, 9.896e-01, 9.956e-01, 9.916e-01,\n",
       "        9.940e-01, 9.884e-01, 9.888e-01, 9.892e-01, 9.892e-01, 9.896e-01,\n",
       "        9.908e-01, 9.872e-01, 9.900e-01, 9.892e-01, 9.888e-01, 9.884e-01,\n",
       "        9.864e-01, 9.872e-01, 9.864e-01, 9.864e-01, 9.880e-01, 9.872e-01,\n",
       "        9.872e-01, 9.864e-01, 9.872e-01, 9.872e-01, 9.872e-01, 9.872e-01,\n",
       "        9.964e-01, 9.952e-01, 9.904e-01, 9.940e-01, 9.892e-01, 9.928e-01,\n",
       "        9.908e-01, 9.900e-01, 9.908e-01, 9.936e-01, 9.908e-01, 9.920e-01,\n",
       "        9.952e-01, 9.952e-01, 9.952e-01, 9.912e-01, 9.904e-01, 9.948e-01,\n",
       "        9.956e-01, 9.912e-01, 9.924e-01, 9.880e-01, 9.900e-01, 9.940e-01,\n",
       "        9.904e-01, 9.964e-01, 9.896e-01, 9.952e-01, 9.912e-01, 9.896e-01,\n",
       "        9.892e-01, 9.912e-01, 9.892e-01, 9.900e-01, 9.904e-01, 9.900e-01,\n",
       "        9.884e-01, 9.888e-01, 9.876e-01, 9.888e-01, 9.884e-01, 9.876e-01,\n",
       "        9.888e-01, 9.884e-01, 9.880e-01, 9.876e-01, 9.880e-01, 9.888e-01,\n",
       "        9.608e-01, 4.000e-04, 9.608e-01, 9.516e-01, 3.176e-01, 9.452e-01,\n",
       "        9.452e-01, 5.844e-01, 9.488e-01, 9.504e-01, 2.628e-01, 9.472e-01,\n",
       "        7.776e-01, 1.844e-01, 7.768e-01, 9.520e-01, 5.788e-01, 9.516e-01,\n",
       "        9.476e-01, 1.844e-01, 9.460e-01, 9.512e-01, 1.720e-01, 9.488e-01,\n",
       "        9.636e-01, 4.600e-02, 7.800e-01, 9.520e-01, 5.840e-01, 9.536e-01,\n",
       "        9.580e-01, 5.772e-01, 9.472e-01, 9.492e-01, 5.844e-01, 9.528e-01,\n",
       "        7.776e-01, 5.520e-02, 7.784e-01, 9.568e-01, 4.880e-02, 9.576e-01,\n",
       "        9.504e-01, 5.632e-01, 9.548e-01, 9.496e-01, 1.844e-01, 9.448e-01,\n",
       "        9.488e-01, 5.200e-01, 9.476e-01, 9.476e-01, 3.520e-01, 9.476e-01,\n",
       "        9.480e-01, 5.016e-01, 9.456e-01, 9.484e-01, 1.792e-01, 9.488e-01,\n",
       "        9.480e-01, 1.916e-01, 9.480e-01, 9.476e-01, 2.888e-01, 9.488e-01,\n",
       "        9.492e-01, 1.252e-01, 9.508e-01, 9.480e-01, 5.500e-01, 9.464e-01,\n",
       "        9.484e-01, 5.680e-01, 9.492e-01, 9.484e-01, 3.088e-01, 9.488e-01,\n",
       "        9.480e-01, 2.956e-01, 9.484e-01, 9.484e-01, 5.232e-01, 9.488e-01,\n",
       "        9.472e-01, 1.784e-01, 9.484e-01, 9.500e-01, 3.488e-01, 9.492e-01,\n",
       "        9.484e-01, 5.148e-01, 9.472e-01, 9.480e-01, 2.632e-01, 9.480e-01,\n",
       "        9.544e-01, 9.524e-01, 9.548e-01, 9.556e-01, 9.580e-01, 9.584e-01,\n",
       "        9.596e-01, 9.624e-01, 9.616e-01, 9.620e-01, 9.616e-01, 9.620e-01,\n",
       "        9.532e-01, 9.524e-01, 9.532e-01, 9.560e-01, 9.592e-01, 9.588e-01,\n",
       "        9.608e-01, 9.604e-01, 9.600e-01, 9.620e-01, 9.624e-01, 9.612e-01,\n",
       "        9.516e-01, 9.556e-01, 9.540e-01, 9.592e-01, 9.580e-01, 9.588e-01,\n",
       "        9.608e-01, 9.616e-01, 9.612e-01, 9.608e-01, 9.616e-01, 9.612e-01,\n",
       "        9.532e-01, 9.516e-01, 9.524e-01, 9.560e-01, 9.536e-01, 9.532e-01,\n",
       "        9.564e-01, 9.572e-01, 9.564e-01, 9.580e-01, 9.572e-01, 9.592e-01,\n",
       "        9.760e-01, 9.748e-01, 9.768e-01, 9.780e-01, 9.768e-01, 9.780e-01,\n",
       "        9.828e-01, 9.816e-01, 9.816e-01, 9.808e-01, 9.828e-01, 9.824e-01,\n",
       "        9.748e-01, 9.760e-01, 9.640e-01, 9.820e-01, 9.824e-01, 9.800e-01,\n",
       "        9.816e-01, 9.820e-01, 9.812e-01, 9.824e-01, 9.832e-01, 9.816e-01,\n",
       "        9.768e-01, 9.760e-01, 9.616e-01, 9.812e-01, 9.780e-01, 9.808e-01,\n",
       "        9.820e-01, 9.816e-01, 9.812e-01, 9.820e-01, 9.836e-01, 9.836e-01,\n",
       "        9.756e-01, 9.612e-01, 9.752e-01, 9.776e-01, 9.788e-01, 9.756e-01,\n",
       "        9.800e-01, 9.800e-01, 9.800e-01, 9.804e-01, 9.812e-01, 9.812e-01]),\n",
       " 'mean_test_recall_micro': array([0.993 , 0.9952, 0.995 , 0.994 , 0.9948, 0.9946, 0.9948, 0.995 ,\n",
       "        0.9948, 0.9952, 0.994 , 0.9946, 0.9952, 0.9958, 0.9954, 0.9936,\n",
       "        0.9918, 0.9934, 0.9938, 0.995 , 0.9928, 0.992 , 0.9942, 0.9916,\n",
       "        0.9948, 0.9916, 0.9924, 0.9928, 0.9912, 0.9908, 0.9922, 0.9894,\n",
       "        0.991 , 0.991 , 0.9904, 0.9902, 0.989 , 0.9898, 0.9894, 0.9892,\n",
       "        0.9898, 0.9896, 0.99  , 0.9896, 0.9896, 0.9894, 0.9896, 0.9894,\n",
       "        0.9948, 0.9954, 0.9932, 0.9948, 0.9912, 0.9942, 0.9924, 0.993 ,\n",
       "        0.9922, 0.9944, 0.9912, 0.9938, 0.996 , 0.9952, 0.9958, 0.9926,\n",
       "        0.9936, 0.9952, 0.9956, 0.9934, 0.9928, 0.9914, 0.9928, 0.9942,\n",
       "        0.9938, 0.9954, 0.991 , 0.9958, 0.9918, 0.9924, 0.9926, 0.9938,\n",
       "        0.9926, 0.9916, 0.9928, 0.9912, 0.99  , 0.9902, 0.9898, 0.9902,\n",
       "        0.9896, 0.9902, 0.9902, 0.9904, 0.9898, 0.9894, 0.99  , 0.9902,\n",
       "        0.874 , 0.1224, 0.874 , 0.9592, 0.2512, 0.9516, 0.9448, 0.5844,\n",
       "        0.9492, 0.948 , 0.4156, 0.9446, 0.872 , 0.1842, 0.8514, 0.9514,\n",
       "        0.5884, 0.9566, 0.9462, 0.2078, 0.9502, 0.9468, 0.3782, 0.9442,\n",
       "        0.8694, 0.1386, 0.8604, 0.866 , 0.5828, 0.9534, 0.9552, 0.3808,\n",
       "        0.9458, 0.9444, 0.498 , 0.948 , 0.7796, 0.1454, 0.869 , 0.9574,\n",
       "        0.0246, 0.9556, 0.945 , 0.5616, 0.951 , 0.9474, 0.3804, 0.9492,\n",
       "        0.9438, 0.4106, 0.9454, 0.945 , 0.4224, 0.9462, 0.9444, 0.5464,\n",
       "        0.9428, 0.9456, 0.3472, 0.9436, 0.9456, 0.1424, 0.944 , 0.9442,\n",
       "        0.209 , 0.9436, 0.9424, 0.1406, 0.9456, 0.9448, 0.4872, 0.9434,\n",
       "        0.945 , 0.331 , 0.9452, 0.9446, 0.2016, 0.9438, 0.9442, 0.3476,\n",
       "        0.9438, 0.9442, 0.5418, 0.9468, 0.943 , 0.1544, 0.9432, 0.9464,\n",
       "        0.447 , 0.9464, 0.9456, 0.4938, 0.944 , 0.946 , 0.4174, 0.9438,\n",
       "        0.9502, 0.949 , 0.9508, 0.9526, 0.9552, 0.955 , 0.9568, 0.9584,\n",
       "        0.9572, 0.9582, 0.958 , 0.9602, 0.9508, 0.9492, 0.9488, 0.9544,\n",
       "        0.954 , 0.9556, 0.9578, 0.9574, 0.9572, 0.9584, 0.959 , 0.9584,\n",
       "        0.9488, 0.9516, 0.951 , 0.9544, 0.9534, 0.9546, 0.9576, 0.957 ,\n",
       "        0.9572, 0.9576, 0.9582, 0.958 , 0.9494, 0.9486, 0.9488, 0.9514,\n",
       "        0.951 , 0.9502, 0.9526, 0.953 , 0.9526, 0.9536, 0.955 , 0.9546,\n",
       "        0.969 , 0.9756, 0.9748, 0.9782, 0.9774, 0.9774, 0.9838, 0.9808,\n",
       "        0.9802, 0.9826, 0.9842, 0.9846, 0.9736, 0.9674, 0.9626, 0.9812,\n",
       "        0.9816, 0.9784, 0.9822, 0.9816, 0.9818, 0.9834, 0.9844, 0.9828,\n",
       "        0.9756, 0.9688, 0.9682, 0.9796, 0.9782, 0.9802, 0.9828, 0.9828,\n",
       "        0.98  , 0.983 , 0.985 , 0.9838, 0.9686, 0.9664, 0.9738, 0.9698,\n",
       "        0.9766, 0.9752, 0.9792, 0.9788, 0.9786, 0.9812, 0.9794, 0.9792]),\n",
       " 'std_test_recall_micro': array([1.800e-03, 8.000e-04, 6.000e-04, 1.600e-03, 8.000e-04, 1.000e-03,\n",
       "        2.000e-03, 6.000e-04, 8.000e-04, 0.000e+00, 1.200e-03, 1.000e-03,\n",
       "        4.000e-04, 6.000e-04, 6.000e-04, 1.600e-03, 3.400e-03, 2.600e-03,\n",
       "        1.800e-03, 1.000e-03, 3.200e-03, 2.400e-03, 1.400e-03, 0.000e+00,\n",
       "        8.000e-04, 3.200e-03, 3.600e-03, 3.600e-03, 2.000e-03, 1.200e-03,\n",
       "        1.400e-03, 2.200e-03, 1.000e-03, 1.800e-03, 1.600e-03, 1.800e-03,\n",
       "        2.600e-03, 2.600e-03, 3.000e-03, 2.800e-03, 1.800e-03, 2.400e-03,\n",
       "        2.800e-03, 3.200e-03, 2.400e-03, 2.200e-03, 2.400e-03, 2.200e-03,\n",
       "        1.600e-03, 2.000e-04, 2.800e-03, 8.000e-04, 2.000e-03, 1.400e-03,\n",
       "        1.600e-03, 3.000e-03, 1.400e-03, 8.000e-04, 4.000e-04, 1.800e-03,\n",
       "        8.000e-04, 0.000e+00, 6.000e-04, 1.400e-03, 3.200e-03, 4.000e-04,\n",
       "        0.000e+00, 2.200e-03, 4.000e-04, 3.400e-03, 2.800e-03, 2.000e-04,\n",
       "        3.400e-03, 1.000e-03, 1.400e-03, 6.000e-04, 6.000e-04, 2.800e-03,\n",
       "        3.400e-03, 2.600e-03, 3.400e-03, 1.600e-03, 2.400e-03, 1.200e-03,\n",
       "        1.600e-03, 1.400e-03, 2.200e-03, 1.400e-03, 1.200e-03, 2.600e-03,\n",
       "        1.400e-03, 2.000e-03, 1.800e-03, 1.800e-03, 2.000e-03, 1.400e-03,\n",
       "        8.680e-02, 1.220e-01, 8.680e-02, 7.600e-03, 6.640e-02, 6.400e-03,\n",
       "        4.000e-04, 0.000e+00, 4.000e-04, 2.400e-03, 1.528e-01, 2.600e-03,\n",
       "        9.440e-02, 2.000e-04, 7.460e-02, 6.000e-04, 9.600e-03, 5.000e-03,\n",
       "        1.400e-03, 2.340e-02, 4.200e-03, 4.400e-03, 2.062e-01, 4.600e-03,\n",
       "        9.420e-02, 9.260e-02, 8.040e-02, 8.600e-02, 1.200e-03, 2.000e-04,\n",
       "        2.800e-03, 1.964e-01, 1.400e-03, 4.800e-03, 8.640e-02, 4.800e-03,\n",
       "        2.000e-03, 9.020e-02, 9.060e-02, 6.000e-04, 2.420e-02, 2.000e-03,\n",
       "        5.400e-03, 1.600e-03, 3.800e-03, 2.200e-03, 1.960e-01, 4.400e-03,\n",
       "        5.000e-03, 1.094e-01, 2.200e-03, 2.600e-03, 7.040e-02, 1.400e-03,\n",
       "        3.600e-03, 4.480e-02, 2.800e-03, 2.800e-03, 1.680e-01, 5.200e-03,\n",
       "        2.400e-03, 4.920e-02, 4.000e-03, 3.400e-03, 7.980e-02, 5.200e-03,\n",
       "        6.800e-03, 1.540e-02, 5.200e-03, 3.200e-03, 6.280e-02, 3.000e-03,\n",
       "        3.400e-03, 2.370e-01, 4.000e-03, 3.800e-03, 1.072e-01, 5.000e-03,\n",
       "        3.800e-03, 5.200e-02, 4.600e-03, 4.200e-03, 1.860e-02, 2.000e-03,\n",
       "        4.200e-03, 2.400e-02, 5.200e-03, 3.600e-03, 9.820e-02, 2.800e-03,\n",
       "        2.800e-03, 2.100e-02, 3.200e-03, 2.000e-03, 1.542e-01, 4.200e-03,\n",
       "        4.200e-03, 3.400e-03, 4.000e-03, 3.000e-03, 2.800e-03, 3.400e-03,\n",
       "        2.800e-03, 4.000e-03, 4.400e-03, 3.800e-03, 3.600e-03, 1.800e-03,\n",
       "        2.400e-03, 3.200e-03, 4.400e-03, 1.600e-03, 5.200e-03, 3.200e-03,\n",
       "        3.000e-03, 3.000e-03, 2.800e-03, 3.600e-03, 3.400e-03, 2.800e-03,\n",
       "        2.800e-03, 4.000e-03, 3.000e-03, 4.800e-03, 4.600e-03, 4.200e-03,\n",
       "        3.200e-03, 4.600e-03, 4.000e-03, 3.200e-03, 3.400e-03, 3.200e-03,\n",
       "        3.800e-03, 3.000e-03, 3.600e-03, 4.600e-03, 2.600e-03, 3.000e-03,\n",
       "        3.800e-03, 4.200e-03, 3.800e-03, 4.400e-03, 2.200e-03, 4.600e-03,\n",
       "        7.000e-03, 8.000e-04, 2.000e-03, 2.000e-04, 6.000e-04, 6.000e-04,\n",
       "        1.000e-03, 8.000e-04, 1.400e-03, 1.800e-03, 1.400e-03, 2.200e-03,\n",
       "        1.200e-03, 8.600e-03, 1.400e-03, 8.000e-04, 8.000e-04, 1.600e-03,\n",
       "        6.000e-04, 4.000e-04, 6.000e-04, 1.000e-03, 1.200e-03, 1.200e-03,\n",
       "        1.200e-03, 7.200e-03, 6.600e-03, 1.600e-03, 2.000e-04, 6.000e-04,\n",
       "        8.000e-04, 1.200e-03, 1.200e-03, 1.000e-03, 1.400e-03, 2.000e-04,\n",
       "        7.000e-03, 5.200e-03, 1.400e-03, 7.800e-03, 2.200e-03, 4.000e-04,\n",
       "        8.000e-04, 1.200e-03, 1.400e-03, 8.000e-04, 1.800e-03, 2.000e-03]),\n",
       " 'rank_test_recall_micro': array([ 40,  11,  14,  29,  17,  23,  17,  14,  17,  11,  29,  23,   9,\n",
       "          2,   6,  35,  56,  37,  31,  14,  42,  55,  26,  58,  17,  58,\n",
       "         50,  44,  63,  69,  53,  90,  66,  66,  70,  72,  96,  81,  90,\n",
       "         95,  81,  85,  78,  85,  85,  90,  85,  90,  22,   6,  39,  17,\n",
       "         63,  26,  50,  40,  53,  25,  62,  31,   1,  11,   2,  47,  35,\n",
       "          9,   5,  37,  44,  61,  42,  26,  31,   8,  66,   2,  56,  50,\n",
       "         47,  31,  47,  58,  44,  63,  78,  72,  81,  72,  85,  72,  72,\n",
       "         70,  81,  90,  78,  72, 248, 287, 248, 146, 277, 184, 225, 258,\n",
       "        197, 205, 269, 227, 250, 281, 255, 186, 257, 165, 212, 279, 194,\n",
       "        208, 273, 231, 251, 286, 254, 253, 259, 178, 168, 271, 215, 229,\n",
       "        263, 205, 256, 283, 252, 158, 288, 166, 222, 260, 188, 207, 272,\n",
       "        197, 237, 270, 220, 222, 267, 212, 230, 261, 246, 216, 275, 241,\n",
       "        216, 284, 235, 231, 278, 241, 247, 285, 216, 226, 265, 243, 222,\n",
       "        276, 221, 227, 280, 237, 231, 274, 237, 231, 262, 209, 245, 282,\n",
       "        244, 211, 266, 210, 216, 264, 235, 214, 268, 237, 194, 200, 192,\n",
       "        181, 168, 170, 164, 148, 160, 151, 153, 145, 191, 197, 201, 175,\n",
       "        176, 166, 155, 158, 160, 150, 147, 148, 203, 184, 190, 174, 178,\n",
       "        172, 156, 163, 160, 156, 151, 153, 196, 204, 201, 186, 188, 193,\n",
       "        181, 180, 181, 177, 170, 172, 138, 131, 134, 126, 128, 128, 101,\n",
       "        115, 116, 108, 100,  98, 136, 142, 144, 113, 111, 125, 109, 111,\n",
       "        110, 103,  99, 105, 131, 139, 141, 119, 126, 116, 105, 105, 118,\n",
       "        104,  97, 101, 140, 143, 135, 137, 130, 133, 121, 123, 124, 113,\n",
       "        120, 122]),\n",
       " 'split0_test_f1_micro': array([9.912e-01, 9.960e-01, 9.956e-01, 9.924e-01, 9.956e-01, 9.956e-01,\n",
       "        9.928e-01, 9.944e-01, 9.956e-01, 9.952e-01, 9.928e-01, 9.956e-01,\n",
       "        9.956e-01, 9.964e-01, 9.960e-01, 9.952e-01, 9.952e-01, 9.960e-01,\n",
       "        9.920e-01, 9.960e-01, 9.960e-01, 9.944e-01, 9.928e-01, 9.916e-01,\n",
       "        9.956e-01, 9.948e-01, 9.960e-01, 9.964e-01, 9.932e-01, 9.920e-01,\n",
       "        9.936e-01, 9.916e-01, 9.920e-01, 9.928e-01, 9.920e-01, 9.920e-01,\n",
       "        9.916e-01, 9.924e-01, 9.924e-01, 9.920e-01, 9.916e-01, 9.920e-01,\n",
       "        9.928e-01, 9.928e-01, 9.920e-01, 9.916e-01, 9.920e-01, 9.916e-01,\n",
       "        9.932e-01, 9.956e-01, 9.960e-01, 9.956e-01, 9.932e-01, 9.956e-01,\n",
       "        9.940e-01, 9.960e-01, 9.936e-01, 9.952e-01, 9.916e-01, 9.956e-01,\n",
       "        9.968e-01, 9.952e-01, 9.964e-01, 9.940e-01, 9.968e-01, 9.956e-01,\n",
       "        9.956e-01, 9.956e-01, 9.932e-01, 9.948e-01, 9.956e-01, 9.944e-01,\n",
       "        9.972e-01, 9.944e-01, 9.924e-01, 9.964e-01, 9.924e-01, 9.952e-01,\n",
       "        9.960e-01, 9.964e-01, 9.960e-01, 9.932e-01, 9.952e-01, 9.924e-01,\n",
       "        9.916e-01, 9.916e-01, 9.920e-01, 9.916e-01, 9.908e-01, 9.928e-01,\n",
       "        9.916e-01, 9.924e-01, 9.916e-01, 9.912e-01, 9.920e-01, 9.916e-01,\n",
       "        7.872e-01, 2.444e-01, 7.872e-01, 9.668e-01, 1.848e-01, 9.580e-01,\n",
       "        9.444e-01, 5.844e-01, 9.496e-01, 9.456e-01, 5.684e-01, 9.420e-01,\n",
       "        9.664e-01, 1.840e-01, 9.260e-01, 9.508e-01, 5.980e-01, 9.616e-01,\n",
       "        9.448e-01, 2.312e-01, 9.544e-01, 9.424e-01, 5.844e-01, 9.396e-01,\n",
       "        7.752e-01, 2.312e-01, 9.408e-01, 7.800e-01, 5.816e-01, 9.532e-01,\n",
       "        9.524e-01, 1.844e-01, 9.444e-01, 9.396e-01, 4.116e-01, 9.432e-01,\n",
       "        7.816e-01, 2.356e-01, 9.596e-01, 9.580e-01, 4.000e-04, 9.536e-01,\n",
       "        9.396e-01, 5.600e-01, 9.472e-01, 9.452e-01, 5.764e-01, 9.536e-01,\n",
       "        9.388e-01, 3.012e-01, 9.432e-01, 9.424e-01, 4.928e-01, 9.448e-01,\n",
       "        9.408e-01, 5.912e-01, 9.400e-01, 9.428e-01, 5.152e-01, 9.384e-01,\n",
       "        9.432e-01, 9.320e-02, 9.400e-01, 9.408e-01, 1.292e-01, 9.384e-01,\n",
       "        9.356e-01, 1.560e-01, 9.404e-01, 9.416e-01, 4.244e-01, 9.404e-01,\n",
       "        9.416e-01, 9.400e-02, 9.412e-01, 9.408e-01, 9.440e-02, 9.388e-01,\n",
       "        9.404e-01, 3.996e-01, 9.392e-01, 9.400e-01, 5.604e-01, 9.448e-01,\n",
       "        9.388e-01, 1.304e-01, 9.380e-01, 9.428e-01, 5.452e-01, 9.436e-01,\n",
       "        9.428e-01, 4.728e-01, 9.408e-01, 9.440e-01, 5.716e-01, 9.396e-01,\n",
       "        9.460e-01, 9.456e-01, 9.468e-01, 9.496e-01, 9.524e-01, 9.516e-01,\n",
       "        9.540e-01, 9.544e-01, 9.528e-01, 9.544e-01, 9.544e-01, 9.584e-01,\n",
       "        9.484e-01, 9.460e-01, 9.444e-01, 9.528e-01, 9.488e-01, 9.524e-01,\n",
       "        9.548e-01, 9.544e-01, 9.544e-01, 9.548e-01, 9.556e-01, 9.556e-01,\n",
       "        9.460e-01, 9.476e-01, 9.480e-01, 9.496e-01, 9.488e-01, 9.504e-01,\n",
       "        9.544e-01, 9.524e-01, 9.532e-01, 9.544e-01, 9.548e-01, 9.548e-01,\n",
       "        9.456e-01, 9.456e-01, 9.452e-01, 9.468e-01, 9.484e-01, 9.472e-01,\n",
       "        9.488e-01, 9.488e-01, 9.488e-01, 9.492e-01, 9.528e-01, 9.500e-01,\n",
       "        9.620e-01, 9.764e-01, 9.728e-01, 9.784e-01, 9.780e-01, 9.768e-01,\n",
       "        9.848e-01, 9.800e-01, 9.788e-01, 9.844e-01, 9.856e-01, 9.868e-01,\n",
       "        9.724e-01, 9.588e-01, 9.612e-01, 9.804e-01, 9.808e-01, 9.768e-01,\n",
       "        9.828e-01, 9.812e-01, 9.824e-01, 9.844e-01, 9.856e-01, 9.840e-01,\n",
       "        9.744e-01, 9.616e-01, 9.748e-01, 9.780e-01, 9.784e-01, 9.796e-01,\n",
       "        9.836e-01, 9.840e-01, 9.788e-01, 9.840e-01, 9.864e-01, 9.840e-01,\n",
       "        9.616e-01, 9.716e-01, 9.724e-01, 9.620e-01, 9.744e-01, 9.748e-01,\n",
       "        9.784e-01, 9.776e-01, 9.772e-01, 9.820e-01, 9.776e-01, 9.772e-01]),\n",
       " 'split1_test_f1_micro': array([9.948e-01, 9.944e-01, 9.944e-01, 9.956e-01, 9.940e-01, 9.936e-01,\n",
       "        9.968e-01, 9.956e-01, 9.940e-01, 9.952e-01, 9.952e-01, 9.936e-01,\n",
       "        9.948e-01, 9.952e-01, 9.948e-01, 9.920e-01, 9.884e-01, 9.908e-01,\n",
       "        9.956e-01, 9.940e-01, 9.896e-01, 9.896e-01, 9.956e-01, 9.916e-01,\n",
       "        9.940e-01, 9.884e-01, 9.888e-01, 9.892e-01, 9.892e-01, 9.896e-01,\n",
       "        9.908e-01, 9.872e-01, 9.900e-01, 9.892e-01, 9.888e-01, 9.884e-01,\n",
       "        9.864e-01, 9.872e-01, 9.864e-01, 9.864e-01, 9.880e-01, 9.872e-01,\n",
       "        9.872e-01, 9.864e-01, 9.872e-01, 9.872e-01, 9.872e-01, 9.872e-01,\n",
       "        9.964e-01, 9.952e-01, 9.904e-01, 9.940e-01, 9.892e-01, 9.928e-01,\n",
       "        9.908e-01, 9.900e-01, 9.908e-01, 9.936e-01, 9.908e-01, 9.920e-01,\n",
       "        9.952e-01, 9.952e-01, 9.952e-01, 9.912e-01, 9.904e-01, 9.948e-01,\n",
       "        9.956e-01, 9.912e-01, 9.924e-01, 9.880e-01, 9.900e-01, 9.940e-01,\n",
       "        9.904e-01, 9.964e-01, 9.896e-01, 9.952e-01, 9.912e-01, 9.896e-01,\n",
       "        9.892e-01, 9.912e-01, 9.892e-01, 9.900e-01, 9.904e-01, 9.900e-01,\n",
       "        9.884e-01, 9.888e-01, 9.876e-01, 9.888e-01, 9.884e-01, 9.876e-01,\n",
       "        9.888e-01, 9.884e-01, 9.880e-01, 9.876e-01, 9.880e-01, 9.888e-01,\n",
       "        9.608e-01, 4.000e-04, 9.608e-01, 9.516e-01, 3.176e-01, 9.452e-01,\n",
       "        9.452e-01, 5.844e-01, 9.488e-01, 9.504e-01, 2.628e-01, 9.472e-01,\n",
       "        7.776e-01, 1.844e-01, 7.768e-01, 9.520e-01, 5.788e-01, 9.516e-01,\n",
       "        9.476e-01, 1.844e-01, 9.460e-01, 9.512e-01, 1.720e-01, 9.488e-01,\n",
       "        9.636e-01, 4.600e-02, 7.800e-01, 9.520e-01, 5.840e-01, 9.536e-01,\n",
       "        9.580e-01, 5.772e-01, 9.472e-01, 9.492e-01, 5.844e-01, 9.528e-01,\n",
       "        7.776e-01, 5.520e-02, 7.784e-01, 9.568e-01, 4.880e-02, 9.576e-01,\n",
       "        9.504e-01, 5.632e-01, 9.548e-01, 9.496e-01, 1.844e-01, 9.448e-01,\n",
       "        9.488e-01, 5.200e-01, 9.476e-01, 9.476e-01, 3.520e-01, 9.476e-01,\n",
       "        9.480e-01, 5.016e-01, 9.456e-01, 9.484e-01, 1.792e-01, 9.488e-01,\n",
       "        9.480e-01, 1.916e-01, 9.480e-01, 9.476e-01, 2.888e-01, 9.488e-01,\n",
       "        9.492e-01, 1.252e-01, 9.508e-01, 9.480e-01, 5.500e-01, 9.464e-01,\n",
       "        9.484e-01, 5.680e-01, 9.492e-01, 9.484e-01, 3.088e-01, 9.488e-01,\n",
       "        9.480e-01, 2.956e-01, 9.484e-01, 9.484e-01, 5.232e-01, 9.488e-01,\n",
       "        9.472e-01, 1.784e-01, 9.484e-01, 9.500e-01, 3.488e-01, 9.492e-01,\n",
       "        9.484e-01, 5.148e-01, 9.472e-01, 9.480e-01, 2.632e-01, 9.480e-01,\n",
       "        9.544e-01, 9.524e-01, 9.548e-01, 9.556e-01, 9.580e-01, 9.584e-01,\n",
       "        9.596e-01, 9.624e-01, 9.616e-01, 9.620e-01, 9.616e-01, 9.620e-01,\n",
       "        9.532e-01, 9.524e-01, 9.532e-01, 9.560e-01, 9.592e-01, 9.588e-01,\n",
       "        9.608e-01, 9.604e-01, 9.600e-01, 9.620e-01, 9.624e-01, 9.612e-01,\n",
       "        9.516e-01, 9.556e-01, 9.540e-01, 9.592e-01, 9.580e-01, 9.588e-01,\n",
       "        9.608e-01, 9.616e-01, 9.612e-01, 9.608e-01, 9.616e-01, 9.612e-01,\n",
       "        9.532e-01, 9.516e-01, 9.524e-01, 9.560e-01, 9.536e-01, 9.532e-01,\n",
       "        9.564e-01, 9.572e-01, 9.564e-01, 9.580e-01, 9.572e-01, 9.592e-01,\n",
       "        9.760e-01, 9.748e-01, 9.768e-01, 9.780e-01, 9.768e-01, 9.780e-01,\n",
       "        9.828e-01, 9.816e-01, 9.816e-01, 9.808e-01, 9.828e-01, 9.824e-01,\n",
       "        9.748e-01, 9.760e-01, 9.640e-01, 9.820e-01, 9.824e-01, 9.800e-01,\n",
       "        9.816e-01, 9.820e-01, 9.812e-01, 9.824e-01, 9.832e-01, 9.816e-01,\n",
       "        9.768e-01, 9.760e-01, 9.616e-01, 9.812e-01, 9.780e-01, 9.808e-01,\n",
       "        9.820e-01, 9.816e-01, 9.812e-01, 9.820e-01, 9.836e-01, 9.836e-01,\n",
       "        9.756e-01, 9.612e-01, 9.752e-01, 9.776e-01, 9.788e-01, 9.756e-01,\n",
       "        9.800e-01, 9.800e-01, 9.800e-01, 9.804e-01, 9.812e-01, 9.812e-01]),\n",
       " 'mean_test_f1_micro': array([0.993 , 0.9952, 0.995 , 0.994 , 0.9948, 0.9946, 0.9948, 0.995 ,\n",
       "        0.9948, 0.9952, 0.994 , 0.9946, 0.9952, 0.9958, 0.9954, 0.9936,\n",
       "        0.9918, 0.9934, 0.9938, 0.995 , 0.9928, 0.992 , 0.9942, 0.9916,\n",
       "        0.9948, 0.9916, 0.9924, 0.9928, 0.9912, 0.9908, 0.9922, 0.9894,\n",
       "        0.991 , 0.991 , 0.9904, 0.9902, 0.989 , 0.9898, 0.9894, 0.9892,\n",
       "        0.9898, 0.9896, 0.99  , 0.9896, 0.9896, 0.9894, 0.9896, 0.9894,\n",
       "        0.9948, 0.9954, 0.9932, 0.9948, 0.9912, 0.9942, 0.9924, 0.993 ,\n",
       "        0.9922, 0.9944, 0.9912, 0.9938, 0.996 , 0.9952, 0.9958, 0.9926,\n",
       "        0.9936, 0.9952, 0.9956, 0.9934, 0.9928, 0.9914, 0.9928, 0.9942,\n",
       "        0.9938, 0.9954, 0.991 , 0.9958, 0.9918, 0.9924, 0.9926, 0.9938,\n",
       "        0.9926, 0.9916, 0.9928, 0.9912, 0.99  , 0.9902, 0.9898, 0.9902,\n",
       "        0.9896, 0.9902, 0.9902, 0.9904, 0.9898, 0.9894, 0.99  , 0.9902,\n",
       "        0.874 , 0.1224, 0.874 , 0.9592, 0.2512, 0.9516, 0.9448, 0.5844,\n",
       "        0.9492, 0.948 , 0.4156, 0.9446, 0.872 , 0.1842, 0.8514, 0.9514,\n",
       "        0.5884, 0.9566, 0.9462, 0.2078, 0.9502, 0.9468, 0.3782, 0.9442,\n",
       "        0.8694, 0.1386, 0.8604, 0.866 , 0.5828, 0.9534, 0.9552, 0.3808,\n",
       "        0.9458, 0.9444, 0.498 , 0.948 , 0.7796, 0.1454, 0.869 , 0.9574,\n",
       "        0.0246, 0.9556, 0.945 , 0.5616, 0.951 , 0.9474, 0.3804, 0.9492,\n",
       "        0.9438, 0.4106, 0.9454, 0.945 , 0.4224, 0.9462, 0.9444, 0.5464,\n",
       "        0.9428, 0.9456, 0.3472, 0.9436, 0.9456, 0.1424, 0.944 , 0.9442,\n",
       "        0.209 , 0.9436, 0.9424, 0.1406, 0.9456, 0.9448, 0.4872, 0.9434,\n",
       "        0.945 , 0.331 , 0.9452, 0.9446, 0.2016, 0.9438, 0.9442, 0.3476,\n",
       "        0.9438, 0.9442, 0.5418, 0.9468, 0.943 , 0.1544, 0.9432, 0.9464,\n",
       "        0.447 , 0.9464, 0.9456, 0.4938, 0.944 , 0.946 , 0.4174, 0.9438,\n",
       "        0.9502, 0.949 , 0.9508, 0.9526, 0.9552, 0.955 , 0.9568, 0.9584,\n",
       "        0.9572, 0.9582, 0.958 , 0.9602, 0.9508, 0.9492, 0.9488, 0.9544,\n",
       "        0.954 , 0.9556, 0.9578, 0.9574, 0.9572, 0.9584, 0.959 , 0.9584,\n",
       "        0.9488, 0.9516, 0.951 , 0.9544, 0.9534, 0.9546, 0.9576, 0.957 ,\n",
       "        0.9572, 0.9576, 0.9582, 0.958 , 0.9494, 0.9486, 0.9488, 0.9514,\n",
       "        0.951 , 0.9502, 0.9526, 0.953 , 0.9526, 0.9536, 0.955 , 0.9546,\n",
       "        0.969 , 0.9756, 0.9748, 0.9782, 0.9774, 0.9774, 0.9838, 0.9808,\n",
       "        0.9802, 0.9826, 0.9842, 0.9846, 0.9736, 0.9674, 0.9626, 0.9812,\n",
       "        0.9816, 0.9784, 0.9822, 0.9816, 0.9818, 0.9834, 0.9844, 0.9828,\n",
       "        0.9756, 0.9688, 0.9682, 0.9796, 0.9782, 0.9802, 0.9828, 0.9828,\n",
       "        0.98  , 0.983 , 0.985 , 0.9838, 0.9686, 0.9664, 0.9738, 0.9698,\n",
       "        0.9766, 0.9752, 0.9792, 0.9788, 0.9786, 0.9812, 0.9794, 0.9792]),\n",
       " 'std_test_f1_micro': array([1.800e-03, 8.000e-04, 6.000e-04, 1.600e-03, 8.000e-04, 1.000e-03,\n",
       "        2.000e-03, 6.000e-04, 8.000e-04, 0.000e+00, 1.200e-03, 1.000e-03,\n",
       "        4.000e-04, 6.000e-04, 6.000e-04, 1.600e-03, 3.400e-03, 2.600e-03,\n",
       "        1.800e-03, 1.000e-03, 3.200e-03, 2.400e-03, 1.400e-03, 0.000e+00,\n",
       "        8.000e-04, 3.200e-03, 3.600e-03, 3.600e-03, 2.000e-03, 1.200e-03,\n",
       "        1.400e-03, 2.200e-03, 1.000e-03, 1.800e-03, 1.600e-03, 1.800e-03,\n",
       "        2.600e-03, 2.600e-03, 3.000e-03, 2.800e-03, 1.800e-03, 2.400e-03,\n",
       "        2.800e-03, 3.200e-03, 2.400e-03, 2.200e-03, 2.400e-03, 2.200e-03,\n",
       "        1.600e-03, 2.000e-04, 2.800e-03, 8.000e-04, 2.000e-03, 1.400e-03,\n",
       "        1.600e-03, 3.000e-03, 1.400e-03, 8.000e-04, 4.000e-04, 1.800e-03,\n",
       "        8.000e-04, 0.000e+00, 6.000e-04, 1.400e-03, 3.200e-03, 4.000e-04,\n",
       "        0.000e+00, 2.200e-03, 4.000e-04, 3.400e-03, 2.800e-03, 2.000e-04,\n",
       "        3.400e-03, 1.000e-03, 1.400e-03, 6.000e-04, 6.000e-04, 2.800e-03,\n",
       "        3.400e-03, 2.600e-03, 3.400e-03, 1.600e-03, 2.400e-03, 1.200e-03,\n",
       "        1.600e-03, 1.400e-03, 2.200e-03, 1.400e-03, 1.200e-03, 2.600e-03,\n",
       "        1.400e-03, 2.000e-03, 1.800e-03, 1.800e-03, 2.000e-03, 1.400e-03,\n",
       "        8.680e-02, 1.220e-01, 8.680e-02, 7.600e-03, 6.640e-02, 6.400e-03,\n",
       "        4.000e-04, 0.000e+00, 4.000e-04, 2.400e-03, 1.528e-01, 2.600e-03,\n",
       "        9.440e-02, 2.000e-04, 7.460e-02, 6.000e-04, 9.600e-03, 5.000e-03,\n",
       "        1.400e-03, 2.340e-02, 4.200e-03, 4.400e-03, 2.062e-01, 4.600e-03,\n",
       "        9.420e-02, 9.260e-02, 8.040e-02, 8.600e-02, 1.200e-03, 2.000e-04,\n",
       "        2.800e-03, 1.964e-01, 1.400e-03, 4.800e-03, 8.640e-02, 4.800e-03,\n",
       "        2.000e-03, 9.020e-02, 9.060e-02, 6.000e-04, 2.420e-02, 2.000e-03,\n",
       "        5.400e-03, 1.600e-03, 3.800e-03, 2.200e-03, 1.960e-01, 4.400e-03,\n",
       "        5.000e-03, 1.094e-01, 2.200e-03, 2.600e-03, 7.040e-02, 1.400e-03,\n",
       "        3.600e-03, 4.480e-02, 2.800e-03, 2.800e-03, 1.680e-01, 5.200e-03,\n",
       "        2.400e-03, 4.920e-02, 4.000e-03, 3.400e-03, 7.980e-02, 5.200e-03,\n",
       "        6.800e-03, 1.540e-02, 5.200e-03, 3.200e-03, 6.280e-02, 3.000e-03,\n",
       "        3.400e-03, 2.370e-01, 4.000e-03, 3.800e-03, 1.072e-01, 5.000e-03,\n",
       "        3.800e-03, 5.200e-02, 4.600e-03, 4.200e-03, 1.860e-02, 2.000e-03,\n",
       "        4.200e-03, 2.400e-02, 5.200e-03, 3.600e-03, 9.820e-02, 2.800e-03,\n",
       "        2.800e-03, 2.100e-02, 3.200e-03, 2.000e-03, 1.542e-01, 4.200e-03,\n",
       "        4.200e-03, 3.400e-03, 4.000e-03, 3.000e-03, 2.800e-03, 3.400e-03,\n",
       "        2.800e-03, 4.000e-03, 4.400e-03, 3.800e-03, 3.600e-03, 1.800e-03,\n",
       "        2.400e-03, 3.200e-03, 4.400e-03, 1.600e-03, 5.200e-03, 3.200e-03,\n",
       "        3.000e-03, 3.000e-03, 2.800e-03, 3.600e-03, 3.400e-03, 2.800e-03,\n",
       "        2.800e-03, 4.000e-03, 3.000e-03, 4.800e-03, 4.600e-03, 4.200e-03,\n",
       "        3.200e-03, 4.600e-03, 4.000e-03, 3.200e-03, 3.400e-03, 3.200e-03,\n",
       "        3.800e-03, 3.000e-03, 3.600e-03, 4.600e-03, 2.600e-03, 3.000e-03,\n",
       "        3.800e-03, 4.200e-03, 3.800e-03, 4.400e-03, 2.200e-03, 4.600e-03,\n",
       "        7.000e-03, 8.000e-04, 2.000e-03, 2.000e-04, 6.000e-04, 6.000e-04,\n",
       "        1.000e-03, 8.000e-04, 1.400e-03, 1.800e-03, 1.400e-03, 2.200e-03,\n",
       "        1.200e-03, 8.600e-03, 1.400e-03, 8.000e-04, 8.000e-04, 1.600e-03,\n",
       "        6.000e-04, 4.000e-04, 6.000e-04, 1.000e-03, 1.200e-03, 1.200e-03,\n",
       "        1.200e-03, 7.200e-03, 6.600e-03, 1.600e-03, 2.000e-04, 6.000e-04,\n",
       "        8.000e-04, 1.200e-03, 1.200e-03, 1.000e-03, 1.400e-03, 2.000e-04,\n",
       "        7.000e-03, 5.200e-03, 1.400e-03, 7.800e-03, 2.200e-03, 4.000e-04,\n",
       "        8.000e-04, 1.200e-03, 1.400e-03, 8.000e-04, 1.800e-03, 2.000e-03]),\n",
       " 'rank_test_f1_micro': array([ 40,  11,  14,  29,  17,  23,  17,  14,  17,  11,  29,  23,   9,\n",
       "          2,   6,  35,  56,  37,  31,  14,  42,  55,  26,  58,  17,  58,\n",
       "         50,  44,  63,  69,  53,  90,  66,  66,  70,  72,  96,  81,  90,\n",
       "         95,  81,  85,  78,  85,  85,  90,  85,  90,  22,   6,  39,  17,\n",
       "         63,  26,  50,  40,  53,  25,  62,  31,   1,  11,   2,  47,  35,\n",
       "          9,   5,  37,  44,  61,  42,  26,  31,   8,  66,   2,  56,  50,\n",
       "         47,  31,  47,  58,  44,  63,  78,  72,  81,  72,  85,  72,  72,\n",
       "         70,  81,  90,  78,  72, 248, 287, 248, 146, 277, 184, 225, 258,\n",
       "        197, 205, 269, 228, 250, 281, 255, 186, 257, 165, 212, 279, 194,\n",
       "        208, 273, 232, 251, 286, 254, 253, 259, 178, 168, 271, 215, 229,\n",
       "        263, 205, 256, 283, 252, 158, 288, 166, 222, 260, 188, 207, 272,\n",
       "        197, 237, 270, 220, 222, 267, 212, 229, 261, 246, 216, 275, 241,\n",
       "        216, 284, 236, 231, 278, 241, 247, 285, 216, 226, 265, 243, 222,\n",
       "        276, 221, 227, 280, 237, 232, 274, 237, 232, 262, 209, 245, 282,\n",
       "        244, 210, 266, 210, 216, 264, 235, 214, 268, 237, 194, 200, 192,\n",
       "        181, 168, 170, 164, 148, 160, 151, 153, 145, 191, 197, 201, 175,\n",
       "        176, 166, 155, 158, 160, 150, 147, 148, 203, 184, 190, 174, 178,\n",
       "        173, 156, 163, 160, 156, 151, 153, 196, 204, 201, 186, 188, 193,\n",
       "        181, 180, 181, 177, 170, 172, 138, 131, 134, 126, 128, 128, 101,\n",
       "        115, 116, 108, 100,  98, 136, 142, 144, 113, 111, 125, 109, 111,\n",
       "        110, 103,  99, 105, 131, 139, 141, 119, 126, 116, 105, 105, 118,\n",
       "        104,  97, 101, 140, 143, 135, 137, 130, 133, 121, 123, 124, 113,\n",
       "        120, 122]),\n",
       " 'split0_test_roc_auc_ovo': array([0.90209264, 0.92828109, 0.92037459, 0.99404579, 0.92253194,\n",
       "        0.90321245, 0.95072244, 0.90798139, 0.97639675, 0.99738228,\n",
       "        0.99620848, 0.99054442, 0.91653314, 0.92601125, 0.90478626,\n",
       "        0.98444929, 0.92026462, 0.98560667, 0.94595183, 0.98991522,\n",
       "        0.95918812, 0.91429535, 0.98367586, 0.99445073, 0.94400318,\n",
       "        0.98932046, 0.99044721, 0.96603549, 0.99682682, 0.99385613,\n",
       "        0.99388388, 0.99484217, 0.99664096, 0.99545397, 0.99546837,\n",
       "        0.99538631, 0.92731111, 0.96725291, 0.93486914, 0.93180968,\n",
       "        0.98232091, 0.98163835, 0.99080327, 0.97475443, 0.98464075,\n",
       "        0.99095344, 0.98091673, 0.98610658, 0.8943783 , 0.93820985,\n",
       "        0.92513275, 0.99323684, 0.94213036, 0.94179299, 0.96429589,\n",
       "        0.99721043, 0.98497787, 0.98781004, 0.98988417, 0.98892226,\n",
       "        0.96983945, 0.90701186, 0.9604108 , 0.98726264, 0.95650203,\n",
       "        0.91825628, 0.95414192, 0.94162406, 0.9969957 , 0.95413868,\n",
       "        0.99882024, 0.987683  , 0.91493195, 0.94203158, 0.9401966 ,\n",
       "        0.9648304 , 0.98126524, 0.96826214, 0.98530983, 0.98129786,\n",
       "        0.99656715, 0.99303789, 0.99742144, 0.99814298, 0.94056822,\n",
       "        0.9893552 , 0.96844173, 0.99179929, 0.99448988, 0.99311985,\n",
       "        0.99459679, 0.99449952, 0.99503574, 0.99496454, 0.9935256 ,\n",
       "        0.99441143, 0.79902616, 0.53170317, 0.78420816, 0.79837696,\n",
       "        0.5493995 , 0.80092689, 0.79773056, 0.6175421 , 0.79477497,\n",
       "        0.79345244, 0.45513614, 0.80959376, 0.91110025, 0.6014927 ,\n",
       "        0.90383003, 0.80342049, 0.55572123, 0.80519116, 0.78858533,\n",
       "        0.5388953 , 0.79621509, 0.78332176, 0.61984873, 0.80809165,\n",
       "        0.85648158, 0.48762216, 0.90911064, 0.90061833, 0.47773249,\n",
       "        0.7903343 , 0.78987361, 0.51012165, 0.79640574, 0.79603639,\n",
       "        0.53497036, 0.79943818, 0.89786294, 0.61398949, 0.90451652,\n",
       "        0.79145311, 0.78772767, 0.80676617, 0.79075317, 0.4904127 ,\n",
       "        0.80601731, 0.78875118, 0.36132124, 0.81131735, 0.81207578,\n",
       "        0.76172492, 0.80786654, 0.80325538, 0.87938058, 0.79588122,\n",
       "        0.789715  , 0.71368276, 0.81376751, 0.79501835, 0.78516787,\n",
       "        0.79752865, 0.79380809, 0.4663631 , 0.80047694, 0.800553  ,\n",
       "        0.4745432 , 0.7821959 , 0.78446639, 0.32866954, 0.79799629,\n",
       "        0.78361967, 0.7492227 , 0.79541388, 0.79507682, 0.38889945,\n",
       "        0.88505849, 0.80064696, 0.33126482, 0.7913068 , 0.78169635,\n",
       "        0.60101694, 0.78534394, 0.8017713 , 0.49886524, 0.79298297,\n",
       "        0.79338003, 0.56476719, 0.78977847, 0.79511274, 0.47266221,\n",
       "        0.77867609, 0.79468412, 0.49434091, 0.76658821, 0.79889626,\n",
       "        0.58297437, 0.79873646, 0.84501396, 0.79904559, 0.80026622,\n",
       "        0.7777401 , 0.79186033, 0.8077239 , 0.79118579, 0.79704327,\n",
       "        0.80054779, 0.80300966, 0.77917376, 0.80570129, 0.80559237,\n",
       "        0.80671801, 0.80117356, 0.79259558, 0.80252161, 0.74657412,\n",
       "        0.80795678, 0.81355895, 0.80110449, 0.79466196, 0.80397135,\n",
       "        0.80979943, 0.79751076, 0.79537541, 0.76940916, 0.74172133,\n",
       "        0.78467278, 0.80798372, 0.81460965, 0.79190448, 0.80082202,\n",
       "        0.81361927, 0.78669337, 0.80028865, 0.8020542 , 0.79106338,\n",
       "        0.74131866, 0.78248245, 0.80342143, 0.80757347, 0.80302688,\n",
       "        0.79976583, 0.79317151, 0.79219369, 0.81326211, 0.8112121 ,\n",
       "        0.81627686, 0.81426025, 0.81500597, 0.81834008, 0.83187466,\n",
       "        0.80919365, 0.74724641, 0.74759808, 0.90340954, 0.88388677,\n",
       "        0.76793913, 0.84455871, 0.78641076, 0.7889656 , 0.79719081,\n",
       "        0.75924931, 0.81760206, 0.80289291, 0.7764059 , 0.79020564,\n",
       "        0.82378171, 0.78964209, 0.740868  , 0.77918727, 0.80672286,\n",
       "        0.81177509, 0.80720518, 0.80491662, 0.81435291, 0.74482484,\n",
       "        0.73737843, 0.74000712, 0.81345867, 0.82418246, 0.78723097,\n",
       "        0.77620066, 0.81824026, 0.79080996, 0.80344064, 0.84141818,\n",
       "        0.81092578, 0.73866719, 0.79039553, 0.8134481 , 0.7557038 ,\n",
       "        0.7641375 , 0.88153061, 0.7432479 ]),\n",
       " 'split1_test_roc_auc_ovo': array([0.97371158, 0.95297589, 0.90270309, 0.96222937, 0.96027015,\n",
       "        0.98967333, 0.94661922, 0.96519677, 0.93010906, 0.97452876,\n",
       "        0.99790909, 0.99705523, 0.91578738, 0.96180169, 0.96873884,\n",
       "        0.98407933, 0.92845848, 0.93754378, 0.96899559, 0.96677266,\n",
       "        0.99674532, 0.9988049 , 0.99850533, 0.995424  , 0.96997076,\n",
       "        0.89722819, 0.94411392, 0.99327196, 0.99421501, 0.99373274,\n",
       "        0.99515202, 0.99401137, 0.99489098, 0.99546789, 0.99517924,\n",
       "        0.99373971, 0.92146056, 0.92445689, 0.94713995, 0.95984818,\n",
       "        0.98051505, 0.92639855, 0.95498625, 0.94891257, 0.97842808,\n",
       "        0.96367644, 0.95245315, 0.94258233, 0.95880109, 0.9650679 ,\n",
       "        0.91442312, 0.9669278 , 0.98311457, 0.91748582, 0.99498202,\n",
       "        0.99547692, 0.96756583, 0.93781676, 0.99487273, 0.99774561,\n",
       "        0.90321031, 0.95682723, 0.93983396, 0.96603241, 0.97278389,\n",
       "        0.98251526, 0.99671588, 0.99821609, 0.99661813, 0.99766226,\n",
       "        0.99539891, 0.9519907 , 0.99242574, 0.98208916, 0.95949756,\n",
       "        0.99221298, 0.9951787 , 0.99347031, 0.99519708, 0.99639354,\n",
       "        0.99507713, 0.99722182, 0.99486204, 0.99602719, 0.98412434,\n",
       "        0.94912207, 0.92671676, 0.99063086, 0.96511921, 0.98803639,\n",
       "        0.9926849 , 0.99106713, 0.99325971, 0.99197741, 0.99296228,\n",
       "        0.99212019, 0.8093506 , 0.51126813, 0.8118368 , 0.81842101,\n",
       "        0.52238157, 0.7987321 , 0.81700403, 0.73912414, 0.92308251,\n",
       "        0.83264774, 0.85179138, 0.8073087 , 0.81351661, 0.27309074,\n",
       "        0.9085577 , 0.82357835, 0.54748906, 0.81721711, 0.81687105,\n",
       "        0.74877183, 0.80757783, 0.80611705, 0.5132253 , 0.81336742,\n",
       "        0.93364464, 0.72388977, 0.81271818, 0.80253088, 0.60252468,\n",
       "        0.80363738, 0.81520717, 0.37853302, 0.80894741, 0.80814156,\n",
       "        0.51525369, 0.81370166, 0.86748177, 0.54503643, 0.81584166,\n",
       "        0.85094035, 0.49508403, 0.91922166, 0.82843175, 0.63887878,\n",
       "        0.81341468, 0.817032  , 0.30808672, 0.80729548, 0.78911493,\n",
       "        0.51128246, 0.80550741, 0.8115551 , 0.29833029, 0.80719818,\n",
       "        0.79914696, 0.50186148, 0.80825744, 0.81837553, 0.40309508,\n",
       "        0.80339198, 0.81817441, 0.40493247, 0.80577574, 0.75028744,\n",
       "        0.61898873, 0.79242445, 0.80134144, 0.42074442, 0.79054506,\n",
       "        0.80028914, 0.63551811, 0.8051953 , 0.81541594, 0.66042993,\n",
       "        0.80535413, 0.81315311, 0.7651478 , 0.78781219, 0.8119094 ,\n",
       "        0.45553393, 0.80723358, 0.80918796, 0.711305  , 0.80739944,\n",
       "        0.80510564, 0.40544922, 0.81227473, 0.79293615, 0.44746489,\n",
       "        0.80681608, 0.80939117, 0.47120351, 0.81194266, 0.80826796,\n",
       "        0.31743578, 0.80942908, 0.74450053, 0.75120625, 0.80572227,\n",
       "        0.75209095, 0.81324267, 0.81311796, 0.80166486, 0.81770956,\n",
       "        0.81647174, 0.80720486, 0.8158533 , 0.81757296, 0.79735031,\n",
       "        0.74965933, 0.81372681, 0.81075338, 0.81809275, 0.81046725,\n",
       "        0.75816043, 0.81589536, 0.8094024 , 0.81609903, 0.82248811,\n",
       "        0.81774941, 0.81431158, 0.84733069, 0.74478499, 0.77615695,\n",
       "        0.80959759, 0.7978139 , 0.81341801, 0.80647818, 0.79222825,\n",
       "        0.76839135, 0.81416087, 0.79110827, 0.78078403, 0.81309479,\n",
       "        0.80524119, 0.81075446, 0.81644305, 0.80889152, 0.80528047,\n",
       "        0.81107728, 0.81157068, 0.81627512, 0.78805286, 0.81743172,\n",
       "        0.82431575, 0.81005047, 0.82567554, 0.82071543, 0.82361832,\n",
       "        0.76765788, 0.80969019, 0.77635893, 0.7854671 , 0.78600343,\n",
       "        0.86320758, 0.82583845, 0.80623557, 0.75657143, 0.76937596,\n",
       "        0.80007985, 0.82600442, 0.83336831, 0.83358883, 0.82809989,\n",
       "        0.82039766, 0.82216405, 0.80568993, 0.84176421, 0.82953285,\n",
       "        0.82159777, 0.79566283, 0.81340573, 0.820314  , 0.82201007,\n",
       "        0.81711162, 0.82097294, 0.81314788, 0.8137019 , 0.81693869,\n",
       "        0.8203385 , 0.82683624, 0.78931338, 0.77670003, 0.7969504 ,\n",
       "        0.81582367, 0.78389252, 0.81531652, 0.79339514, 0.77718014,\n",
       "        0.82449378, 0.80242059, 0.81612874]),\n",
       " 'mean_test_roc_auc_ovo': array([0.93790211, 0.94062849, 0.91153884, 0.97813758, 0.94140104,\n",
       "        0.94644289, 0.94867083, 0.93658908, 0.95325291, 0.98595552,\n",
       "        0.99705878, 0.99379982, 0.91616026, 0.94390647, 0.93676255,\n",
       "        0.98426431, 0.92436155, 0.96157522, 0.95747371, 0.97834394,\n",
       "        0.97796672, 0.95655012, 0.99109059, 0.99493736, 0.95698697,\n",
       "        0.94327433, 0.96728056, 0.97965372, 0.99552092, 0.99379443,\n",
       "        0.99451795, 0.99442677, 0.99576597, 0.99546093, 0.9953238 ,\n",
       "        0.99456301, 0.92438583, 0.9458549 , 0.94100455, 0.94582893,\n",
       "        0.98141798, 0.95401845, 0.97289476, 0.9618335 , 0.98153441,\n",
       "        0.97731494, 0.96668494, 0.96434445, 0.92658969, 0.95163888,\n",
       "        0.91977794, 0.98008232, 0.96262246, 0.9296394 , 0.97963895,\n",
       "        0.99634368, 0.97627185, 0.9628134 , 0.99237845, 0.99333394,\n",
       "        0.93652488, 0.93191955, 0.95012238, 0.97664752, 0.96464296,\n",
       "        0.95038577, 0.9754289 , 0.96992008, 0.99680692, 0.97590047,\n",
       "        0.99710958, 0.96983685, 0.95367885, 0.96206037, 0.94984708,\n",
       "        0.97852169, 0.98822197, 0.98086622, 0.99025346, 0.9888457 ,\n",
       "        0.99582214, 0.99512986, 0.99614174, 0.99708509, 0.96234628,\n",
       "        0.96923863, 0.94757925, 0.99121508, 0.97980454, 0.99057812,\n",
       "        0.99364084, 0.99278333, 0.99414772, 0.99347097, 0.99324394,\n",
       "        0.99326581, 0.80418838, 0.52148565, 0.79802248, 0.80839899,\n",
       "        0.53589053, 0.7998295 , 0.8073673 , 0.67833312, 0.85892874,\n",
       "        0.81305009, 0.65346376, 0.80845123, 0.86230843, 0.43729172,\n",
       "        0.90619386, 0.81349942, 0.55160515, 0.81120413, 0.80272819,\n",
       "        0.64383356, 0.80189646, 0.7947194 , 0.56653702, 0.81072954,\n",
       "        0.89506311, 0.60575597, 0.86091441, 0.8515746 , 0.54012859,\n",
       "        0.79698584, 0.80254039, 0.44432733, 0.80267657, 0.80208897,\n",
       "        0.52511202, 0.80656992, 0.88267236, 0.57951296, 0.86017909,\n",
       "        0.82119673, 0.64140585, 0.86299391, 0.80959246, 0.56464574,\n",
       "        0.809716  , 0.80289159, 0.33470398, 0.80930641, 0.80059536,\n",
       "        0.63650369, 0.80668698, 0.80740524, 0.58885543, 0.8015397 ,\n",
       "        0.79443098, 0.60777212, 0.81101248, 0.80669694, 0.59413148,\n",
       "        0.80046031, 0.80599125, 0.43564779, 0.80312634, 0.77542022,\n",
       "        0.54676596, 0.78731017, 0.79290392, 0.37470698, 0.79427067,\n",
       "        0.79195441, 0.69237041, 0.80030459, 0.80524638, 0.52466469,\n",
       "        0.84520631, 0.80690003, 0.54820631, 0.7895595 , 0.79680287,\n",
       "        0.52827544, 0.79628876, 0.80547963, 0.60508512, 0.8001912 ,\n",
       "        0.79924283, 0.4851082 , 0.8010266 , 0.79402445, 0.46006355,\n",
       "        0.79274608, 0.80203765, 0.48277221, 0.78926544, 0.80358211,\n",
       "        0.45020508, 0.80408277, 0.79475725, 0.77512592, 0.80299425,\n",
       "        0.76491553, 0.8025515 , 0.81042093, 0.79642533, 0.80737641,\n",
       "        0.80850976, 0.80510726, 0.79751353, 0.81163712, 0.80147134,\n",
       "        0.77818867, 0.80745019, 0.80167448, 0.81030718, 0.77852068,\n",
       "        0.7830586 , 0.81472716, 0.80525345, 0.8053805 , 0.81322973,\n",
       "        0.81377442, 0.80591117, 0.82135305, 0.75709708, 0.75893914,\n",
       "        0.79713519, 0.80289881, 0.81401383, 0.79919133, 0.79652513,\n",
       "        0.79100531, 0.80042712, 0.79569846, 0.79141912, 0.80207909,\n",
       "        0.77327992, 0.79661846, 0.80993224, 0.8082325 , 0.80415368,\n",
       "        0.80542156, 0.8023711 , 0.8042344 , 0.80065748, 0.81432191,\n",
       "        0.82029631, 0.81215536, 0.82034075, 0.81952775, 0.82774649,\n",
       "        0.78842576, 0.7784683 , 0.7619785 , 0.84443832, 0.8349451 ,\n",
       "        0.81557335, 0.83519858, 0.79632316, 0.77276852, 0.78328339,\n",
       "        0.77966458, 0.82180324, 0.81813061, 0.80499736, 0.80915277,\n",
       "        0.82208969, 0.80590307, 0.77327897, 0.81047574, 0.81812785,\n",
       "        0.81668643, 0.801434  , 0.80916118, 0.81733346, 0.78341745,\n",
       "        0.77724502, 0.78049003, 0.81330327, 0.81894218, 0.80208483,\n",
       "        0.79826958, 0.82253825, 0.79006167, 0.79007033, 0.81918429,\n",
       "        0.81337472, 0.76127986, 0.80285603, 0.80342162, 0.76644197,\n",
       "        0.79431564, 0.8419756 , 0.77968832]),\n",
       " 'std_test_roc_auc_ovo': array([3.58094675e-02, 1.23474007e-02, 8.83575171e-03, 1.59082115e-02,\n",
       "        1.88691061e-02, 4.32304420e-02, 2.05161261e-03, 2.86076915e-02,\n",
       "        2.31438447e-02, 1.14267584e-02, 8.50303889e-04, 3.25540195e-03,\n",
       "        3.72881782e-04, 1.78952225e-02, 3.19762917e-02, 1.84981100e-04,\n",
       "        4.09693121e-03, 2.40314444e-02, 1.15218810e-02, 1.15712778e-02,\n",
       "        1.87786011e-02, 4.22547788e-02, 7.41473880e-03, 4.86636370e-04,\n",
       "        1.29837871e-02, 4.60461317e-02, 2.31666463e-02, 1.36182357e-02,\n",
       "        1.30590287e-03, 6.16946957e-05, 6.34072064e-04, 4.15397529e-04,\n",
       "        8.74990975e-04, 6.95899831e-06, 1.44567735e-04, 8.23299457e-04,\n",
       "        2.92527274e-03, 2.13980111e-02, 6.13540178e-03, 1.40192521e-02,\n",
       "        9.02927863e-04, 2.76198992e-02, 1.79085125e-02, 1.29209322e-02,\n",
       "        3.10633147e-03, 1.36385014e-02, 1.42317912e-02, 2.17621273e-02,\n",
       "        3.22113931e-02, 1.34290207e-02, 5.35481610e-03, 1.31545192e-02,\n",
       "        2.04921037e-02, 1.21535839e-02, 1.53430630e-02, 8.66753649e-04,\n",
       "        8.70602121e-03, 2.49966440e-02, 2.49428064e-03, 4.41167480e-03,\n",
       "        3.33145738e-02, 2.49076841e-02, 1.02884206e-02, 1.06151155e-02,\n",
       "        8.14093105e-03, 3.21294884e-02, 2.12869797e-02, 2.82960117e-02,\n",
       "        1.88786296e-04, 2.17617939e-02, 1.71066762e-03, 1.78461494e-02,\n",
       "        3.87468928e-02, 2.00287903e-02, 9.65047839e-03, 1.36912901e-02,\n",
       "        6.95672771e-03, 1.26040855e-02, 4.94362265e-03, 7.54784084e-03,\n",
       "        7.45009313e-04, 2.09196460e-03, 1.27970105e-03, 1.05789286e-03,\n",
       "        2.17780565e-02, 2.01165690e-02, 2.08624859e-02, 5.84215266e-04,\n",
       "        1.46853346e-02, 2.54172955e-03, 9.55945675e-04, 1.71619615e-03,\n",
       "        8.88015030e-04, 1.49356131e-03, 2.81659407e-04, 1.14561826e-03,\n",
       "        5.16221867e-03, 1.02175236e-02, 1.38143176e-02, 1.00220257e-02,\n",
       "        1.35089648e-02, 1.09739537e-03, 9.63673072e-03, 6.07910223e-02,\n",
       "        6.41537694e-02, 1.95976498e-02, 1.98327621e-01, 1.14252783e-03,\n",
       "        4.87918204e-02, 1.64200980e-01, 2.36383221e-03, 1.00789314e-02,\n",
       "        4.11608385e-03, 6.01297616e-03, 1.41428596e-02, 1.04938262e-01,\n",
       "        5.68137181e-03, 1.13976431e-02, 5.33117143e-02, 2.63788357e-03,\n",
       "        3.85815345e-02, 1.18133801e-01, 4.81962289e-02, 4.90437272e-02,\n",
       "        6.23960939e-02, 6.65154289e-03, 1.26667785e-02, 6.57943124e-02,\n",
       "        6.27083474e-03, 6.05258589e-03, 9.85833549e-03, 7.13173939e-03,\n",
       "        1.51905837e-02, 3.44765319e-02, 4.43374315e-02, 2.97436242e-02,\n",
       "        1.46321822e-01, 5.62277473e-02, 1.88392900e-02, 7.42330374e-02,\n",
       "        3.69868544e-03, 1.41404102e-02, 2.66172578e-02, 2.01093944e-03,\n",
       "        1.14804266e-02, 1.25221230e-01, 1.17956226e-03, 4.14986342e-03,\n",
       "        2.90525143e-01, 5.65847795e-03, 4.71597928e-03, 1.05910644e-01,\n",
       "        2.75503135e-03, 1.16785893e-02, 1.91036391e-01, 2.93166540e-03,\n",
       "        1.21831615e-02, 3.07153172e-02, 2.64939601e-03, 2.51327799e-02,\n",
       "        7.22227652e-02, 5.11427303e-03, 8.43752451e-03, 4.60374435e-02,\n",
       "        3.72561269e-03, 8.33473052e-03, 5.68522959e-02, 4.89070678e-03,\n",
       "        1.01695595e-02, 1.35765238e-01, 3.98521816e-02, 6.25307424e-03,\n",
       "        2.16941491e-01, 1.74730215e-03, 1.51065278e-02, 7.27415027e-02,\n",
       "        1.09448196e-02, 3.70832964e-03, 1.06219878e-01, 7.20823842e-03,\n",
       "        5.86280637e-03, 7.96589832e-02, 1.12481320e-02, 1.08829030e-03,\n",
       "        1.25986616e-02, 1.40699948e-02, 7.35352514e-03, 1.15687008e-02,\n",
       "        2.26772246e-02, 4.68584929e-03, 1.32769295e-01, 5.34630875e-03,\n",
       "        5.02567164e-02, 2.39196728e-02, 2.72802350e-03, 1.28245786e-02,\n",
       "        1.06911703e-02, 2.69702714e-03, 5.23953879e-03, 1.03331445e-02,\n",
       "        7.96197519e-03, 2.09759940e-03, 1.83397707e-02, 5.93583392e-03,\n",
       "        4.12103239e-03, 2.85293396e-02, 6.27662380e-03, 9.07889913e-03,\n",
       "        7.78556636e-03, 3.19465639e-02, 2.48981736e-02, 1.16820595e-03,\n",
       "        4.14895101e-03, 1.07185362e-02, 9.25838172e-03, 3.97499319e-03,\n",
       "        8.40041298e-03, 2.59776402e-02, 1.23120860e-02, 1.72178120e-02,\n",
       "        1.24624064e-02, 5.08490593e-03, 5.95823259e-04, 7.28684790e-03,\n",
       "        4.29688702e-03, 2.26139593e-02, 1.37337503e-02, 4.59018988e-03,\n",
       "        1.06350872e-02, 1.10157074e-02, 3.19612635e-02, 1.41360060e-02,\n",
       "        6.51080596e-03, 6.59024367e-04, 1.12679237e-03, 5.65572213e-03,\n",
       "        9.19958343e-03, 1.20407160e-02, 1.26046253e-02, 3.10980782e-03,\n",
       "        4.01944541e-03, 2.10489025e-03, 5.33478268e-03, 1.18767602e-03,\n",
       "        4.12816801e-03, 2.07678811e-02, 3.12218908e-02, 1.43804243e-02,\n",
       "        5.89712227e-02, 4.89416699e-02, 4.76342253e-02, 9.36013112e-03,\n",
       "        9.91240500e-03, 1.61970831e-02, 1.39074232e-02, 2.04152685e-02,\n",
       "        4.20118042e-03, 1.52376981e-02, 2.85914671e-02, 1.89471216e-02,\n",
       "        1.69202302e-03, 1.62609847e-02, 3.24109641e-02, 3.12884738e-02,\n",
       "        1.14049923e-02, 4.91133901e-03, 5.77117330e-03, 4.24455798e-03,\n",
       "        2.98054782e-03, 3.85926179e-02, 3.98665948e-02, 4.04829116e-02,\n",
       "        1.55393248e-04, 5.24027777e-03, 1.48538628e-02, 2.20689184e-02,\n",
       "        4.29798730e-03, 7.48291084e-04, 1.33703043e-02, 2.22338922e-02,\n",
       "        2.44894577e-03, 2.26126649e-02, 1.24604943e-02, 1.00264778e-02,\n",
       "        1.07381703e-02, 3.01781374e-02, 3.95550089e-02, 3.64404225e-02]),\n",
       " 'rank_test_roc_auc_ovo': array([ 85,  84,  96,  44,  82,  77,  75,  87,  70,  33,   3,  18,  95,\n",
       "         80,  86,  34,  93,  64,  65,  43,  45,  67,  28,  13,  66,  81,\n",
       "         55,  40,   9,  19,  15,  16,   8,  10,  11,  14,  92,  78,  83,\n",
       "         79,  36,  68,  51,  63,  35,  46,  56,  58,  91,  71,  94,  38,\n",
       "         60,  90,  41,   5,  48,  59,  26,  22,  88,  89,  73,  47,  57,\n",
       "         72,  50,  52,   4,  49,   1,  53,  69,  62,  74,  42,  32,  37,\n",
       "         30,  31,   7,  12,   6,   2,  61,  54,  76,  27,  39,  29,  20,\n",
       "         25,  17,  21,  24,  23, 173, 279, 208, 152, 275, 204, 157, 258,\n",
       "        104, 135, 259, 151, 101, 285,  97, 131, 271, 138, 183, 260, 192,\n",
       "        220, 269, 140,  98, 264, 102, 105, 274, 211, 186, 284, 184, 188,\n",
       "        277, 161,  99, 268, 103, 116, 261, 100, 146, 270, 145, 181, 288,\n",
       "        147, 199, 262, 160, 155, 267, 194, 221, 263, 139, 159, 266, 200,\n",
       "        162, 286, 178, 246, 273, 235, 225, 287, 223, 227, 257, 202, 169,\n",
       "        278, 106, 158, 272, 232, 212, 276, 217, 165, 265, 203, 205, 280,\n",
       "        197, 224, 282, 226, 191, 281, 233, 176, 283, 175, 219, 247, 179,\n",
       "        252, 185, 142, 215, 156, 150, 170, 209, 137, 195, 244, 154, 193,\n",
       "        143, 242, 238, 127, 168, 167, 134, 130, 163, 115, 256, 255, 210,\n",
       "        180, 129, 206, 214, 229, 201, 218, 228, 190, 248, 213, 144, 153,\n",
       "        174, 166, 187, 172, 198, 128, 118, 136, 117, 119, 111, 234, 243,\n",
       "        253, 107, 110, 126, 109, 216, 250, 237, 241, 114, 122, 171, 149,\n",
       "        113, 164, 249, 141, 123, 125, 196, 148, 124, 236, 245, 239, 133,\n",
       "        121, 189, 207, 112, 231, 230, 120, 132, 254, 182, 177, 251, 222,\n",
       "        108, 240]),\n",
       " 'split0_test_neg_log_loss': array([-0.03970947, -0.02498456, -0.02455202, -0.05516648, -0.02466298,\n",
       "        -0.03174165, -0.04177027, -0.05838177, -0.03365875, -0.05723401,\n",
       "        -0.03143696, -0.05705178, -0.02460628, -0.02578241, -0.0231439 ,\n",
       "        -0.03048247, -0.02728085, -0.02521138, -0.02729597, -0.02281747,\n",
       "        -0.02208484, -0.04139781, -0.05430239, -0.03104063, -0.01854244,\n",
       "        -0.02431856, -0.02048312, -0.01814296, -0.02256241, -0.03032803,\n",
       "        -0.02216623, -0.03453407, -0.02721175, -0.02719817, -0.02864122,\n",
       "        -0.0281165 , -0.03980798, -0.03588469, -0.03800195, -0.03701068,\n",
       "        -0.03654609, -0.03621649, -0.03612068, -0.03637422, -0.03616168,\n",
       "        -0.03560618, -0.03651342, -0.03592713, -0.03126678, -0.04698652,\n",
       "        -0.02580049, -0.03537375, -0.06890833, -0.03925586, -0.0350157 ,\n",
       "        -0.04757452, -0.03857863, -0.03864338, -0.10492501, -0.04976862,\n",
       "        -0.0185299 , -0.02404513, -0.02445137, -0.06742416, -0.01950083,\n",
       "        -0.02345947, -0.04165575, -0.04547435, -0.03993359, -0.0364735 ,\n",
       "        -0.03379231, -0.03163095, -0.0203791 , -0.02730767, -0.0349768 ,\n",
       "        -0.02106178, -0.0615704 , -0.03271895, -0.0178009 , -0.01557087,\n",
       "        -0.02127078, -0.02873948, -0.01877028, -0.0383124 , -0.03930115,\n",
       "        -0.03413621, -0.03269398, -0.03582786, -0.03173027, -0.03024089,\n",
       "        -0.03041929, -0.03045761, -0.03045261, -0.03143505, -0.03156091,\n",
       "        -0.03124379, -0.44612846, -1.388027  , -0.44440808, -0.39925443,\n",
       "        -1.40585675, -0.39514017, -0.34992462, -1.17150779, -0.36845905,\n",
       "        -0.35312207, -1.3332408 , -0.35096199, -0.47826495, -1.44244342,\n",
       "        -0.45874548, -0.37316306, -1.25546607, -0.38865712, -0.3625331 ,\n",
       "        -1.28985467, -0.38300748, -0.33828502, -1.14802309, -0.3397859 ,\n",
       "        -0.47796225, -1.5058827 , -0.48543496, -0.44391684, -1.12750907,\n",
       "        -0.37694009, -0.37934557, -1.42773642, -0.36210072, -0.34055318,\n",
       "        -1.22464062, -0.34539745, -0.47890626, -1.34075977, -0.4022127 ,\n",
       "        -0.3954474 , -1.48919257, -0.3915076 , -0.35859497, -1.32006205,\n",
       "        -0.36749677, -0.35827742, -1.16143274, -0.36579969, -0.2079847 ,\n",
       "        -1.49085065, -0.18378361, -0.17914754, -1.2087061 , -0.17030543,\n",
       "        -0.1764096 , -1.02557714, -0.17273282, -0.17515311, -1.09658073,\n",
       "        -0.17259983, -0.19131589, -1.65201024, -0.18452597, -0.17925276,\n",
       "        -1.49589437, -0.180336  , -0.18025131, -1.73936964, -0.17004637,\n",
       "        -0.16888808, -1.25444232, -0.17052378, -0.18636338, -2.04142869,\n",
       "        -0.19913344, -0.18047841, -1.74381665, -0.17578975, -0.1704259 ,\n",
       "        -1.22633298, -0.17443323, -0.17408306, -1.0149317 , -0.16809439,\n",
       "        -0.18210964, -1.6160551 , -0.19605917, -0.18089253, -1.17141896,\n",
       "        -0.18034549, -0.17413107, -1.14903167, -0.17285074, -0.1727094 ,\n",
       "        -1.30779553, -0.17414202, -0.17183121, -0.16989123, -0.1670469 ,\n",
       "        -0.1429304 , -0.139989  , -0.13664012, -0.13141816, -0.12928559,\n",
       "        -0.13561886, -0.12505703, -0.12907675, -0.11996637, -0.16307221,\n",
       "        -0.17425516, -0.18983325, -0.13879809, -0.14887157, -0.14178217,\n",
       "        -0.12633501, -0.12604539, -0.1309396 , -0.12645632, -0.12304545,\n",
       "        -0.12595694, -0.16378278, -0.16990361, -0.16411574, -0.1473412 ,\n",
       "        -0.14831502, -0.14358466, -0.1296685 , -0.13576435, -0.13257993,\n",
       "        -0.12603932, -0.12339607, -0.12652625, -0.17559036, -0.17791685,\n",
       "        -0.1831441 , -0.15718076, -0.15635224, -0.15530808, -0.14435882,\n",
       "        -0.14464627, -0.14550651, -0.14584301, -0.13604964, -0.13916298,\n",
       "        -0.10435773, -0.09491416, -0.10587535, -0.08542307, -0.08771308,\n",
       "        -0.09196178, -0.06770666, -0.08173876, -0.08198144, -0.0718047 ,\n",
       "        -0.06879571, -0.0643076 , -0.10179994, -0.11301443, -0.10418014,\n",
       "        -0.08471488, -0.07904695, -0.09243942, -0.07587781, -0.07818071,\n",
       "        -0.08050468, -0.0710259 , -0.06913115, -0.06894873, -0.1004383 ,\n",
       "        -0.10806349, -0.09645287, -0.08897155, -0.08530214, -0.08579748,\n",
       "        -0.07549168, -0.0725617 , -0.08313774, -0.06981343, -0.06798147,\n",
       "        -0.06666197, -0.10581804, -0.1028454 , -0.10838461, -0.09867382,\n",
       "        -0.09709613, -0.09615614, -0.08600864, -0.08507911, -0.08917361,\n",
       "        -0.07783304, -0.08592923, -0.0875898 ]),\n",
       " 'split1_test_neg_log_loss': array([-0.03920425, -0.02998465, -0.0296215 , -0.03231908, -0.06186415,\n",
       "        -0.02070806, -0.03230892, -0.07055277, -0.06032818, -0.06624341,\n",
       "        -0.03349044, -0.0532261 , -0.02776451, -0.0311941 , -0.03304215,\n",
       "        -0.05287929, -0.04752872, -0.04058186, -0.0465727 , -0.04859962,\n",
       "        -0.04400552, -0.0460343 , -0.01607409, -0.05216227, -0.01966769,\n",
       "        -0.03774482, -0.04026506, -0.03974939, -0.03420045, -0.03565888,\n",
       "        -0.03530226, -0.05395472, -0.03571381, -0.0391738 , -0.03988909,\n",
       "        -0.04554823, -0.049822  , -0.04929911, -0.05003746, -0.05300144,\n",
       "        -0.04580572, -0.04687432, -0.0466894 , -0.04780157, -0.04647172,\n",
       "        -0.04722592, -0.04733713, -0.0476943 , -0.02192391, -0.03003953,\n",
       "        -0.06110932, -0.08207841, -0.07082118, -0.03612739, -0.15346581,\n",
       "        -0.05336671, -0.11308967, -0.09954557, -0.0592908 , -0.0743773 ,\n",
       "        -0.02727413, -0.02732909, -0.0441251 , -0.05936891, -0.05898321,\n",
       "        -0.06747336, -0.02060067, -0.07681092, -0.03220403, -0.07695109,\n",
       "        -0.03968666, -0.05020806, -0.04336096, -0.0188334 , -0.04385387,\n",
       "        -0.02022558, -0.03724421, -0.03269795, -0.03807755, -0.03151082,\n",
       "        -0.03361943, -0.03678092, -0.06071318, -0.03651489, -0.04691497,\n",
       "        -0.0392513 , -0.05096986, -0.04142223, -0.0408811 , -0.04932   ,\n",
       "        -0.03924976, -0.04054274, -0.04165263, -0.04635277, -0.04062494,\n",
       "        -0.03932553, -0.45010707, -1.51005277, -0.41880575, -0.38525033,\n",
       "        -1.49118343, -0.35425358, -0.35001178, -1.18448969, -0.36614458,\n",
       "        -0.35412713, -1.30948638, -0.34822385, -0.48271318, -1.76393215,\n",
       "        -0.47334655, -0.37572901, -1.19394613, -0.36558055, -0.3533429 ,\n",
       "        -1.42269024, -0.35402723, -0.35193238, -1.28760617, -0.34328811,\n",
       "        -0.42391931, -1.42833904, -0.44150533, -0.36322304, -1.34302795,\n",
       "        -0.3837821 , -0.37783299, -1.29492597, -0.35017925, -0.34696914,\n",
       "        -1.11479324, -0.35544258, -0.49332057, -1.42812213, -0.45251191,\n",
       "        -0.39273915, -1.43139903, -0.40897171, -0.35705077, -1.20009439,\n",
       "        -0.37113719, -0.35319112, -1.31664145, -0.34641284, -0.18914778,\n",
       "        -1.21523944, -0.19154249, -0.18790382, -1.3027876 , -0.18456954,\n",
       "        -0.18226427, -1.21968837, -0.18800205, -0.1775079 , -1.65051155,\n",
       "        -0.18205077, -0.1912264 , -1.45533559, -0.19104115, -0.18254473,\n",
       "        -1.39327474, -0.18548497, -0.17662069, -1.53996937, -0.17484759,\n",
       "        -0.17882999, -0.99722363, -0.18690661, -0.18562016, -1.15297918,\n",
       "        -0.18581529, -0.18059155, -1.29786847, -0.18039703, -0.17729607,\n",
       "        -1.52521824, -0.18155004, -0.17811222, -1.20045748, -0.18187365,\n",
       "        -0.19114772, -1.35537014, -0.19794991, -0.1809469 , -1.30514094,\n",
       "        -0.18006783, -0.18014217, -1.52987574, -0.18470127, -0.18168103,\n",
       "        -1.47284617, -0.17917908, -0.18890103, -0.17110338, -0.16756605,\n",
       "        -0.15253034, -0.14721733, -0.14529109, -0.13264903, -0.12844973,\n",
       "        -0.13080975, -0.13183473, -0.12903016, -0.1337288 , -0.18337455,\n",
       "        -0.17884997, -0.18091034, -0.15248934, -0.146604  , -0.14055854,\n",
       "        -0.13719043, -0.1357588 , -0.13361482, -0.13215504, -0.12426301,\n",
       "        -0.13344667, -0.17170699, -0.17977224, -0.17667948, -0.15288329,\n",
       "        -0.14759155, -0.14250526, -0.1396347 , -0.13400737, -0.13637469,\n",
       "        -0.13769292, -0.1331857 , -0.13544116, -0.17234046, -0.17641734,\n",
       "        -0.17175328, -0.15145793, -0.15773756, -0.16402491, -0.14883088,\n",
       "        -0.14943858, -0.15144413, -0.14552729, -0.14734032, -0.14365703,\n",
       "        -0.11751933, -0.11696275, -0.11386056, -0.10085583, -0.10125427,\n",
       "        -0.11382469, -0.09141087, -0.10224067, -0.10459912, -0.10760672,\n",
       "        -0.09311831, -0.10053333, -0.1222843 , -0.1225688 , -0.11230053,\n",
       "        -0.08749814, -0.09217751, -0.10218667, -0.092097  , -0.09468006,\n",
       "        -0.09657069, -0.09458178, -0.09500982, -0.09637827, -0.11341194,\n",
       "        -0.11884877, -0.12471723, -0.09829599, -0.11218304, -0.10408482,\n",
       "        -0.08806391, -0.10023678, -0.10495298, -0.11001995, -0.08434156,\n",
       "        -0.09078931, -0.12295981, -0.12527225, -0.11997293, -0.10964483,\n",
       "        -0.10516167, -0.11700266, -0.10534904, -0.1024602 , -0.10121848,\n",
       "        -0.09779515, -0.09599277, -0.10117187]),\n",
       " 'mean_test_neg_log_loss': array([-0.03945686, -0.02748461, -0.02708676, -0.04374278, -0.04326356,\n",
       "        -0.02622485, -0.0370396 , -0.06446727, -0.04699346, -0.06173871,\n",
       "        -0.0324637 , -0.05513894, -0.0261854 , -0.02848825, -0.02809303,\n",
       "        -0.04168088, -0.03740479, -0.03289662, -0.03693434, -0.03570854,\n",
       "        -0.03304518, -0.04371606, -0.03518824, -0.04160145, -0.01910507,\n",
       "        -0.03103169, -0.03037409, -0.02894618, -0.02838143, -0.03299346,\n",
       "        -0.02873425, -0.04424439, -0.03146278, -0.03318599, -0.03426515,\n",
       "        -0.03683237, -0.04481499, -0.0425919 , -0.0440197 , -0.04500606,\n",
       "        -0.0411759 , -0.04154541, -0.04140504, -0.0420879 , -0.0413167 ,\n",
       "        -0.04141605, -0.04192528, -0.04181071, -0.02659534, -0.03851302,\n",
       "        -0.0434549 , -0.05872608, -0.06986476, -0.03769163, -0.09424075,\n",
       "        -0.05047061, -0.07583415, -0.06909447, -0.0821079 , -0.06207296,\n",
       "        -0.02290201, -0.02568711, -0.03428824, -0.06339654, -0.03924202,\n",
       "        -0.04546641, -0.03112821, -0.06114263, -0.03606881, -0.0567123 ,\n",
       "        -0.03673949, -0.0409195 , -0.03187003, -0.02307054, -0.03941533,\n",
       "        -0.02064368, -0.0494073 , -0.03270845, -0.02793922, -0.02354085,\n",
       "        -0.0274451 , -0.0327602 , -0.03974173, -0.03741365, -0.04310806,\n",
       "        -0.03669376, -0.04183192, -0.03862505, -0.03630568, -0.03978044,\n",
       "        -0.03483452, -0.03550017, -0.03605262, -0.03889391, -0.03609292,\n",
       "        -0.03528466, -0.44811777, -1.44903989, -0.43160692, -0.39225238,\n",
       "        -1.44852009, -0.37469688, -0.3499682 , -1.17799874, -0.36730182,\n",
       "        -0.3536246 , -1.32136359, -0.34959292, -0.48048906, -1.60318779,\n",
       "        -0.46604602, -0.37444603, -1.2247061 , -0.37711884, -0.357938  ,\n",
       "        -1.35627245, -0.36851735, -0.3451087 , -1.21781463, -0.341537  ,\n",
       "        -0.45094078, -1.46711087, -0.46347015, -0.40356994, -1.23526851,\n",
       "        -0.38036109, -0.37858928, -1.3613312 , -0.35613998, -0.34376116,\n",
       "        -1.16971693, -0.35042002, -0.48611341, -1.38444095, -0.4273623 ,\n",
       "        -0.39409327, -1.4602958 , -0.40023966, -0.35782287, -1.26007822,\n",
       "        -0.36931698, -0.35573427, -1.2390371 , -0.35610627, -0.19856624,\n",
       "        -1.35304505, -0.18766305, -0.18352568, -1.25574685, -0.17743749,\n",
       "        -0.17933694, -1.12263276, -0.18036744, -0.17633051, -1.37354614,\n",
       "        -0.1773253 , -0.19127115, -1.55367291, -0.18778356, -0.18089874,\n",
       "        -1.44458456, -0.18291049, -0.178436  , -1.63966951, -0.17244698,\n",
       "        -0.17385904, -1.12583298, -0.17871519, -0.18599177, -1.59720394,\n",
       "        -0.19247436, -0.18053498, -1.52084256, -0.17809339, -0.17386099,\n",
       "        -1.37577561, -0.17799164, -0.17609764, -1.10769459, -0.17498402,\n",
       "        -0.18662868, -1.48571262, -0.19700454, -0.18091972, -1.23827995,\n",
       "        -0.18020666, -0.17713662, -1.33945371, -0.17877601, -0.17719521,\n",
       "        -1.39032085, -0.17666055, -0.18036612, -0.17049731, -0.16730648,\n",
       "        -0.14773037, -0.14360317, -0.14096561, -0.1320336 , -0.12886766,\n",
       "        -0.13321431, -0.12844588, -0.12905346, -0.12684759, -0.17322338,\n",
       "        -0.17655257, -0.18537179, -0.14564372, -0.14773779, -0.14117035,\n",
       "        -0.13176272, -0.13090209, -0.13227721, -0.12930568, -0.12365423,\n",
       "        -0.1297018 , -0.16774488, -0.17483793, -0.17039761, -0.15011224,\n",
       "        -0.14795328, -0.14304496, -0.1346516 , -0.13488586, -0.13447731,\n",
       "        -0.13186612, -0.12829089, -0.1309837 , -0.17396541, -0.1771671 ,\n",
       "        -0.17744869, -0.15431935, -0.1570449 , -0.15966649, -0.14659485,\n",
       "        -0.14704242, -0.14847532, -0.14568515, -0.14169498, -0.14141001,\n",
       "        -0.11093853, -0.10593846, -0.10986796, -0.09313945, -0.09448368,\n",
       "        -0.10289323, -0.07955877, -0.09198971, -0.09329028, -0.08970571,\n",
       "        -0.08095701, -0.08242046, -0.11204212, -0.11779161, -0.10824033,\n",
       "        -0.08610651, -0.08561223, -0.09731304, -0.0839874 , -0.08643038,\n",
       "        -0.08853769, -0.08280384, -0.08207049, -0.0826635 , -0.10692512,\n",
       "        -0.11345613, -0.11058505, -0.09363377, -0.09874259, -0.09494115,\n",
       "        -0.08177779, -0.08639924, -0.09404536, -0.08991669, -0.07616151,\n",
       "        -0.07872564, -0.11438892, -0.11405882, -0.11417877, -0.10415933,\n",
       "        -0.1011289 , -0.1065794 , -0.09567884, -0.09376965, -0.09519605,\n",
       "        -0.08781409, -0.090961  , -0.09438083]),\n",
       " 'std_test_neg_log_loss': array([2.52611959e-04, 2.50004453e-03, 2.53473886e-03, 1.14237029e-02,\n",
       "        1.86005842e-02, 5.51679753e-03, 4.73067608e-03, 6.08549887e-03,\n",
       "        1.33347179e-02, 4.50469828e-03, 1.02674016e-03, 1.91283941e-03,\n",
       "        1.57911320e-03, 2.70584566e-03, 4.94912430e-03, 1.11984095e-02,\n",
       "        1.01239342e-02, 7.68523533e-03, 9.63836544e-03, 1.28910760e-02,\n",
       "        1.09603432e-02, 2.31824563e-03, 1.91141507e-02, 1.05608206e-02,\n",
       "        5.62621686e-04, 6.71312938e-03, 9.89097151e-03, 1.08032140e-02,\n",
       "        5.81902009e-03, 2.66542654e-03, 6.56801217e-03, 9.71032522e-03,\n",
       "        4.25102800e-03, 5.98781104e-03, 5.62393322e-03, 8.71586910e-03,\n",
       "        5.00701285e-03, 6.70720634e-03, 6.01775380e-03, 7.99538108e-03,\n",
       "        4.62981298e-03, 5.32891325e-03, 5.28435854e-03, 5.71367114e-03,\n",
       "        5.15502084e-03, 5.80986964e-03, 5.41185882e-03, 5.88358385e-03,\n",
       "        4.67143513e-03, 8.47349446e-03, 1.76544171e-02, 2.33523290e-02,\n",
       "        9.56424243e-04, 1.56423538e-03, 5.92250542e-02, 2.89609763e-03,\n",
       "        3.72555195e-02, 3.04510961e-02, 2.28171043e-02, 1.23043405e-02,\n",
       "        4.37211132e-03, 1.64197668e-03, 9.83686295e-03, 4.02762583e-03,\n",
       "        1.97411878e-02, 2.20069469e-02, 1.05275403e-02, 1.56682852e-02,\n",
       "        3.86477936e-03, 2.02387941e-02, 2.94717370e-03, 9.28855543e-03,\n",
       "        1.14909282e-02, 4.23713626e-03, 4.43853576e-03, 4.18100537e-04,\n",
       "        1.21630928e-02, 1.05029950e-05, 1.01383224e-02, 7.96997423e-03,\n",
       "        6.17432892e-03, 4.02072217e-03, 2.09714470e-02, 8.98752387e-04,\n",
       "        3.80690837e-03, 2.55754338e-03, 9.13793958e-03, 2.79718260e-03,\n",
       "        4.57541502e-03, 9.53955329e-03, 4.41523859e-03, 5.04256938e-03,\n",
       "        5.60000825e-03, 7.45886047e-03, 4.53201350e-03, 4.04087045e-03,\n",
       "        1.98930272e-03, 6.10128882e-02, 1.28011685e-02, 7.00204965e-03,\n",
       "        4.26633408e-02, 2.04432953e-02, 4.35808110e-05, 6.49095105e-03,\n",
       "        1.15723038e-03, 5.02528571e-04, 1.18772096e-02, 1.36907266e-03,\n",
       "        2.22411423e-03, 1.60744364e-01, 7.30053291e-03, 1.28297369e-03,\n",
       "        3.07599699e-02, 1.15382814e-02, 4.59509956e-03, 6.64177850e-02,\n",
       "        1.44901208e-02, 6.82368205e-03, 6.97915425e-02, 1.75110713e-03,\n",
       "        2.70214718e-02, 3.87718302e-02, 2.19648122e-02, 4.03469005e-02,\n",
       "        1.07759437e-01, 3.42100387e-03, 7.56287046e-04, 6.64052256e-02,\n",
       "        5.96073162e-03, 3.20798048e-03, 5.49236923e-02, 5.02256599e-03,\n",
       "        7.20715127e-03, 4.36811803e-02, 2.51496050e-02, 1.35412474e-03,\n",
       "        2.88967719e-02, 8.73205464e-03, 7.72098407e-04, 5.99838276e-02,\n",
       "        1.82020980e-03, 2.54315321e-03, 7.76043525e-02, 9.69342405e-03,\n",
       "        9.41846150e-03, 1.37805605e-01, 3.87943815e-03, 4.37813927e-03,\n",
       "        4.70407523e-02, 7.13205224e-03, 2.92733549e-03, 9.70556192e-02,\n",
       "        7.63461859e-03, 1.17739650e-03, 2.76965410e-01, 4.72547322e-03,\n",
       "        4.47482412e-05, 9.83373245e-02, 3.25758807e-03, 1.64598519e-03,\n",
       "        5.13098171e-02, 2.57448560e-03, 1.81530975e-03, 9.97001353e-02,\n",
       "        2.40061167e-03, 4.97095138e-03, 1.28609346e-01, 8.19141928e-03,\n",
       "        3.71610621e-04, 4.44224753e-01, 6.65907647e-03, 5.65700680e-05,\n",
       "        2.22974089e-01, 2.30363814e-03, 3.43508484e-03, 1.49442634e-01,\n",
       "        3.55840379e-03, 2.01457915e-03, 9.27628935e-02, 6.88963250e-03,\n",
       "        4.51903789e-03, 1.30342480e-01, 9.45369509e-04, 2.71839503e-05,\n",
       "        6.68609911e-02, 1.38829028e-04, 3.00555086e-03, 1.90422033e-01,\n",
       "        5.92526316e-03, 4.48581620e-03, 8.25253187e-02, 2.51853084e-03,\n",
       "        8.53491224e-03, 6.06073907e-04, 2.59571653e-04, 4.79997089e-03,\n",
       "        3.61416301e-03, 4.32548400e-03, 6.15438658e-04, 4.17930176e-04,\n",
       "        2.40455667e-03, 3.38884845e-03, 2.32931827e-05, 6.88121313e-03,\n",
       "        1.01511689e-02, 2.29740361e-03, 4.46145586e-03, 6.84562747e-03,\n",
       "        1.13378738e-03, 6.11811925e-04, 5.42771052e-03, 4.85670660e-03,\n",
       "        1.33760939e-03, 2.84935853e-03, 6.08776601e-04, 3.74486539e-03,\n",
       "        3.96210617e-03, 4.93431978e-03, 6.28186982e-03, 2.77104658e-03,\n",
       "        3.61735623e-04, 5.39700805e-04, 4.98309977e-03, 8.78490158e-04,\n",
       "        1.89737943e-03, 5.82679723e-03, 4.89481767e-03, 4.45745331e-03,\n",
       "        1.62495163e-03, 7.49755617e-04, 5.69540944e-03, 2.86141719e-03,\n",
       "        6.92662917e-04, 4.35841666e-03, 2.23602745e-03, 2.39615388e-03,\n",
       "        2.96880947e-03, 1.57860275e-04, 5.64534093e-03, 2.24702512e-03,\n",
       "        6.58079987e-03, 1.10242956e-02, 3.99260564e-03, 7.71638032e-03,\n",
       "        6.77059369e-03, 1.09314509e-02, 1.18521067e-02, 1.02509535e-02,\n",
       "        1.13088413e-02, 1.79010095e-02, 1.21613017e-02, 1.81128634e-02,\n",
       "        1.02421828e-02, 4.77718524e-03, 4.06019498e-03, 1.39163029e-03,\n",
       "        6.56528161e-03, 4.87362858e-03, 8.10959612e-03, 8.24967733e-03,\n",
       "        8.03300391e-03, 1.17779389e-02, 1.29393347e-02, 1.37147735e-02,\n",
       "        6.48682061e-03, 5.39263926e-03, 1.41321812e-02, 4.66222005e-03,\n",
       "        1.34404528e-02, 9.14367391e-03, 6.28611631e-03, 1.38375397e-02,\n",
       "        1.09076188e-02, 2.01032593e-02, 8.18004805e-03, 1.20636707e-02,\n",
       "        8.57088701e-03, 1.12134216e-02, 5.79416101e-03, 5.48550523e-03,\n",
       "        4.03276593e-03, 1.04232554e-02, 9.67019822e-03, 8.69054478e-03,\n",
       "        6.02243548e-03, 9.98105477e-03, 5.03177159e-03, 6.79103465e-03]),\n",
       " 'rank_test_neg_log_loss': array([ 55,  12,  10,  75,  72,   8,  46,  91,  81,  88,  24,  84,   7,\n",
       "         16,  14,  65,  47,  27,  45,  37,  29,  74,  34,  64,   1,  20,\n",
       "         19,  18,  15,  28,  17,  77,  22,  30,  31,  44,  78,  70,  76,\n",
       "         79,  59,  63,  61,  69,  60,  62,  68,  66,   9,  50,  73,  86,\n",
       "         93,  49, 121,  83,  94,  92, 101,  89,   3,   6,  32,  90,  53,\n",
       "         80,  21,  87,  39,  85,  43,  58,  23,   4,  54,   2,  82,  25,\n",
       "         13,   5,  11,  26,  56,  48,  71,  42,  67,  51,  41,  57,  33,\n",
       "         36,  38,  52,  40,  35, 251, 280, 250, 245, 279, 241, 229, 261,\n",
       "        237, 231, 269, 228, 255, 287, 254, 240, 263, 242, 236, 272, 238,\n",
       "        227, 262, 225, 252, 282, 253, 248, 264, 244, 243, 273, 234, 226,\n",
       "        260, 230, 256, 276, 249, 246, 281, 247, 235, 268, 239, 232, 266,\n",
       "        233, 224, 271, 219, 215, 267, 200, 207, 258, 210, 193, 274, 199,\n",
       "        221, 285, 220, 212, 278, 214, 204, 288, 185, 187, 259, 205, 217,\n",
       "        286, 222, 211, 284, 203, 188, 275, 202, 192, 257, 191, 218, 283,\n",
       "        223, 213, 265, 208, 196, 270, 206, 198, 277, 195, 209, 184, 181,\n",
       "        173, 168, 163, 157, 149, 159, 148, 150, 146, 186, 194, 216, 169,\n",
       "        174, 164, 155, 153, 158, 151, 145, 152, 182, 190, 183, 177, 175,\n",
       "        167, 161, 162, 160, 156, 147, 154, 189, 197, 201, 178, 179, 180,\n",
       "        171, 172, 176, 170, 166, 165, 138, 132, 136, 116, 123, 130,  97,\n",
       "        115, 117, 112,  98, 102, 139, 144, 135, 107, 106, 127, 105, 109,\n",
       "        111, 104, 100, 103, 134, 140, 137, 118, 128, 124,  99, 108, 120,\n",
       "        113,  95,  96, 143, 141, 142, 131, 129, 133, 126, 119, 125, 110,\n",
       "        114, 122])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRIAL_3_MLP.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL THREE NEG LOG LOSS RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.01,\n",
       " 'classifier__hidden_layer_sizes': (5,),\n",
       " 'classifier__learning_rate': 'constant',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL THREE NEG LOG LOSS RESULTS ---------------------------------------------\")\n",
    "TRIAL_3_MLP.cv_results_['rank_test_neg_log_loss']\n",
    "TRIAL_3_MLP.cv_results_['params'][ np.argmin(TRIAL_3_MLP.cv_results_['rank_test_neg_log_loss']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL THREE F1 RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'tanh',\n",
       " 'classifier__alpha': 0.001,\n",
       " 'classifier__hidden_layer_sizes': (5,),\n",
       " 'classifier__learning_rate': 'constant',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL THREE F1 RESULTS ---------------------------------------------\")\n",
    "TRIAL_3_MLP.cv_results_['rank_test_f1_micro']\n",
    "TRIAL_3_MLP.cv_results_['params'][ np.argmin(TRIAL_3_MLP.cv_results_['rank_test_f1_micro']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL THREE ROC AUC RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'tanh',\n",
       " 'classifier__alpha': 0.001,\n",
       " 'classifier__hidden_layer_sizes': (20,),\n",
       " 'classifier__learning_rate': 'invscaling',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL THREE ROC AUC RESULTS ---------------------------------------------\")\n",
    "TRIAL_3_MLP.cv_results_['rank_test_roc_auc_ovo']\n",
    "TRIAL_3_MLP.cv_results_['params'][ np.argmin(TRIAL_3_MLP.cv_results_['rank_test_roc_auc_ovo']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 3 MLP using best NEG LOG LOSS hyperparameters :0.9965\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 3 MLP using best F1 hyperparameters :0.9964833333333334\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 3 MLP using best ROC_AUC hyperparameters :0.99625\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best NEG LOG LOSS results to train on test set, \n",
    "p = TRIAL_3_MLP.cv_results_['params'][ np.argmin(TRIAL_3_MLP.cv_results_['rank_test_neg_log_loss']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train__neg_log_loss = pipe.predict(X_train)\n",
    "y_pred_test_neg_log_loss = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 3 MLP using best NEG LOG LOSS hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_neg_log_loss)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best F1 results to train on test set, LR_Trial 1\n",
    "p = TRIAL_3_MLP.cv_results_['params'][ np.argmin(TRIAL_3_MLP.cv_results_['rank_test_f1_micro']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train_roc_auc = pipe.predict(X_train)\n",
    "y_pred_test_f1 = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 3 MLP using best F1 hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_f1)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best ROC_AUC results to train on test set, LR_Trial 1\n",
    "p  = TRIAL_3_MLP.cv_results_['params'][ np.argmin(TRIAL_3_MLP.cv_results_['rank_test_roc_auc_ovo']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train_roc_auc = pipe.predict(X_train)\n",
    "y_pred_test_roc_auc = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 3 MLP using best ROC_AUC hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_roc_auc)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTI LAYERED PERCEPTRON TRIAL FOUR ON FIRE WALL DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 288 candidates, totalling 576 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:    6.4s\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:    7.9s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    9.1s\n",
      "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed:   10.5s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   12.0s\n",
      "[Parallel(n_jobs=-1)]: Done 189 tasks      | elapsed:   13.9s\n",
      "[Parallel(n_jobs=-1)]: Done 210 tasks      | elapsed:   16.8s\n",
      "[Parallel(n_jobs=-1)]: Done 233 tasks      | elapsed:   20.4s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:   23.5s\n",
      "[Parallel(n_jobs=-1)]: Done 281 tasks      | elapsed:   27.0s\n",
      "[Parallel(n_jobs=-1)]: Done 306 tasks      | elapsed:   31.1s\n",
      "[Parallel(n_jobs=-1)]: Done 333 tasks      | elapsed:   35.0s\n",
      "[Parallel(n_jobs=-1)]: Done 360 tasks      | elapsed:   39.2s\n",
      "[Parallel(n_jobs=-1)]: Done 389 tasks      | elapsed:   43.9s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:   48.7s\n",
      "[Parallel(n_jobs=-1)]: Done 449 tasks      | elapsed:   53.5s\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:   58.4s\n",
      "[Parallel(n_jobs=-1)]: Done 513 tasks      | elapsed:  1.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FOUR RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 576 out of 576 | elapsed:  1.2min finished\n"
     ]
    }
   ],
   "source": [
    "ITERATIONS = 1\n",
    "for i in range(1):\n",
    "    #For reach \"trial\", randomly select 5000 data points\n",
    "    random5000DataPoints = FireWALLData.sample(n = 5000, replace = True)\n",
    "    #From that 5000, create x set and y set\n",
    "    xSet = random5000DataPoints.iloc[:,0:9].values\n",
    "    ySet = random5000DataPoints.iloc[:,10].values\n",
    "    ySet = encodeLabel.fit_transform(ySet)\n",
    "    \n",
    "    pipe = Pipeline([('std', StandardScaler()),('classifier', MLPClassifier(max_iter = 250))])\n",
    "\n",
    "    search_space = [\n",
    "                {\n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                {\n",
    "                 \n",
    "                  \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['sgd'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                \n",
    "                {\n",
    "                \n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['adam'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                ]\n",
    "    \n",
    "    #'precision_micro','roc_auc_ovo','jaccard','neg_log_loss'] JACCARD DOES NOT WORK FOR NON BINARY?\n",
    "    runHyperParamSearch = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=2), \n",
    "                          scoring=['recall_micro','f1_micro','roc_auc_ovo','neg_log_loss'], \n",
    "                          refit=False, verbose=10, n_jobs = -1)\n",
    "    TRIAL_4_MLP = runHyperParamSearch.fit(xSet, ySet)\n",
    "    print(\"---------------------------------TRIAL FOUR RESULTS ---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.87699342, 0.4613986 , 0.54346836, 0.68058562, 1.0263828 ,\n",
       "        1.03313887, 0.89001679, 0.81720245, 0.87925756, 1.33689845,\n",
       "        1.29086065, 1.42472565, 0.98209453, 0.63329422, 0.57099056,\n",
       "        1.02463102, 1.0151248 , 0.95382297, 0.79293191, 1.34615612,\n",
       "        1.13622642, 1.30962622, 1.31563139, 1.45625341, 0.75739992,\n",
       "        0.83621871, 0.73037803, 0.97133613, 0.88275945, 0.9825958 ,\n",
       "        1.10119665, 1.14348304, 1.14648545, 1.20553601, 1.23255885,\n",
       "        1.01262116, 0.52169979, 0.54722095, 0.57049   , 0.78817928,\n",
       "        0.76891136, 0.85034847, 0.88401163, 0.92629921, 0.90703022,\n",
       "        0.91303444, 0.80769408, 0.78767884, 0.8920182 , 0.85348368,\n",
       "        0.81194901, 1.25482857, 1.10294998, 1.00436497, 1.34498727,\n",
       "        1.39619994, 1.37968588, 1.5538367 , 1.52931488, 1.52806532,\n",
       "        0.84372556, 0.88651156, 0.93680573, 1.06916928, 1.07867646,\n",
       "        1.16625535, 1.53431976, 1.35341311, 1.34890974, 1.49603677,\n",
       "        1.4439919 , 1.52356207, 0.88701248, 0.97183669, 0.82671165,\n",
       "        1.11320758, 1.13047159, 1.09944427, 1.4635098 , 1.39169669,\n",
       "        1.41621864, 1.44524229, 1.39269733, 1.50554442, 0.83071446,\n",
       "        0.82871282, 0.62929261, 1.08699024, 0.95106733, 0.99785876,\n",
       "        1.2425704 , 1.16024804, 1.28360295, 1.39144695, 1.56935072,\n",
       "        1.33890116, 2.11781931, 2.05301619, 2.22716451, 2.46637189,\n",
       "        0.89276838, 2.53042603, 2.42508614, 2.48788953, 2.49739754,\n",
       "        2.67179751, 1.78503442, 2.64302242, 2.20839918, 0.65581441,\n",
       "        2.16210938, 2.28746784, 1.33539832, 2.42258286, 2.53868341,\n",
       "        1.91664791, 2.5984844 , 2.41932964, 2.48889017, 2.55369604,\n",
       "        2.27020228, 1.16099882, 2.07753742, 2.45210886, 2.28096163,\n",
       "        2.32274723, 2.78364444, 2.58122039, 2.32675087, 2.65528321,\n",
       "        1.72648549, 2.53793299, 2.42733836, 0.57074082, 2.26269531,\n",
       "        2.23241925, 1.25708139, 2.21740746, 2.76212573, 2.29822671,\n",
       "        2.51991677, 2.62700891, 1.61789131, 2.77563691, 2.19138372,\n",
       "        1.101197  , 2.15084946, 2.50415385, 2.17662191, 2.39180756,\n",
       "        2.62400615, 2.81567121, 2.90599954, 2.67104793, 2.77688742,\n",
       "        2.64302266, 2.2997278 , 2.0582701 , 2.22616446, 2.38555229,\n",
       "        1.98470664, 2.41357565, 2.69656932, 2.91951132, 2.75011563,\n",
       "        2.99457479, 2.6878109 , 3.41994131, 2.81492138, 1.72998834,\n",
       "        2.62200451, 2.23066854, 2.26494777, 2.39155674, 2.73059809,\n",
       "        2.65077984, 2.71783674, 2.60899341, 2.35052168, 2.45661294,\n",
       "        2.25193703, 1.22205079, 2.08729434, 2.48964119, 2.59323025,\n",
       "        2.27695811, 2.71808767, 2.33000362, 2.76287639, 2.88923538,\n",
       "        2.47387695, 2.6963191 , 2.59122849, 2.51991737, 2.13933933,\n",
       "        2.57221186, 2.44310129, 2.60724235, 2.77838981, 2.59473181,\n",
       "        2.53342819, 2.93777561, 2.64827728, 2.95553994, 2.11732066,\n",
       "        2.29547477, 2.20589757, 2.14884853, 2.57146204, 2.21565568,\n",
       "        2.37654412, 2.41182411, 2.6600374 , 2.89023578, 2.84544671,\n",
       "        2.49989998, 2.32149613, 2.25994313, 2.33450711, 2.38755393,\n",
       "        2.56420541, 2.55294609, 2.6662935 , 2.65353191, 2.37253976,\n",
       "        2.71508539, 2.81692243, 2.64527559, 2.36078048, 2.44410288,\n",
       "        2.16261005, 2.4536103 , 2.34551775, 2.38605213, 2.53493047,\n",
       "        2.62475646, 2.51591396, 2.64577496, 2.72859609, 2.58147049,\n",
       "        2.1648618 , 2.26669943, 2.30122924, 2.2511853 , 2.43084049,\n",
       "        2.50165141, 2.6322639 , 2.65353215, 2.76888108, 2.69907188,\n",
       "        2.64502537, 2.77889073, 2.36353219, 2.23241937, 2.1798749 ,\n",
       "        2.45661271, 2.42958939, 2.25894237, 2.57996881, 2.68105638,\n",
       "        2.64402378, 2.7966547 , 2.60048652, 2.722592  , 2.24993515,\n",
       "        2.26694953, 2.32374835, 2.3590287 , 2.39656103, 2.41232407,\n",
       "        2.65153098, 2.52992523, 2.57696617, 2.5569483 , 2.70907998,\n",
       "        2.67705154, 2.11406791, 2.20139265, 2.28046155, 2.35177255,\n",
       "        2.38304973, 2.38330007, 2.33700979, 2.39681125, 2.27420533,\n",
       "        2.21440458, 2.00897694, 1.88962424]),\n",
       " 'std_fit_time': array([8.18314552e-02, 2.55215168e-02, 2.80742049e-01, 4.09352779e-01,\n",
       "        4.10364866e-02, 3.47784758e-02, 2.09929228e-01, 9.50826406e-02,\n",
       "        3.06262136e-01, 1.01336598e-01, 1.28861666e-01, 1.86159968e-01,\n",
       "        6.68079853e-02, 1.67393088e-01, 2.95254230e-01, 2.92761326e-02,\n",
       "        4.02848721e-02, 2.70231962e-02, 4.12104487e-01, 1.50179863e-03,\n",
       "        2.12688446e-02, 5.45471907e-02, 1.75147057e-02, 9.25782919e-02,\n",
       "        3.52793932e-02, 7.20621347e-02, 1.52880549e-01, 2.35195160e-02,\n",
       "        4.00339365e-02, 3.32795382e-02, 6.33045435e-02, 3.20278406e-02,\n",
       "        6.50608540e-03, 2.35196352e-02, 3.65313292e-02, 2.40456820e-01,\n",
       "        4.42875624e-02, 6.73079491e-02, 1.05090022e-01, 9.05789137e-02,\n",
       "        7.58156776e-02, 1.30496740e-01, 3.32797766e-02, 2.19687104e-01,\n",
       "        5.67997694e-02, 1.44874573e-01, 2.95270681e-02, 9.65837240e-02,\n",
       "        8.03197622e-02, 2.17685699e-02, 1.13847017e-01, 2.37711668e-02,\n",
       "        1.10095382e-01, 2.90248394e-02, 1.13426566e-01, 4.50387001e-02,\n",
       "        4.10356522e-02, 1.35124922e-02, 9.40817595e-02, 1.42871499e-01,\n",
       "        4.70403433e-02, 7.98180103e-02, 8.05690289e-02, 5.83000183e-02,\n",
       "        9.83335972e-02, 4.12839651e-02, 8.00645351e-03, 5.98020554e-02,\n",
       "        5.07937670e-02, 1.06841803e-01, 7.43148327e-02, 5.57986498e-02,\n",
       "        2.37700939e-02, 1.02086902e-01, 1.10099316e-02, 2.22692490e-02,\n",
       "        5.60487509e-02, 2.50214338e-02, 1.57639980e-02, 2.25198269e-02,\n",
       "        4.85421419e-02, 8.70746374e-02, 1.40109062e-02, 5.47974110e-02,\n",
       "        1.07091904e-01, 1.85166597e-02, 1.48877263e-01, 2.54658461e-02,\n",
       "        2.37694979e-02, 1.20604992e-01, 1.11597300e-01, 1.32617950e-02,\n",
       "        3.50308418e-02, 1.24857783e-01, 8.40725899e-02, 8.33209753e-02,\n",
       "        8.45720768e-02, 9.13293362e-02, 8.75687599e-03, 1.11845613e-01,\n",
       "        2.97255993e-01, 3.06013107e-01, 1.13098025e-01, 1.33864880e-01,\n",
       "        9.23296213e-02, 1.09094143e-01, 7.75667191e-01, 1.38368249e-01,\n",
       "        1.13597512e-01, 3.16521883e-01, 2.32703686e-02, 1.10095739e-02,\n",
       "        6.91845298e-01, 1.08093262e-01, 1.86159611e-01, 8.65243912e-01,\n",
       "        1.13347888e-01, 4.92923260e-02, 2.21440554e-01, 6.25540018e-02,\n",
       "        2.33451009e-01, 8.88263464e-01, 4.72904444e-02, 2.71733761e-01,\n",
       "        3.16772342e-01, 5.67994118e-02, 1.40871048e-01, 1.81155443e-01,\n",
       "        9.68335867e-02, 8.00657272e-03, 9.17789221e-01, 1.67393804e-01,\n",
       "        1.88411713e-01, 3.50051522e-01, 3.02510977e-01, 2.08179474e-01,\n",
       "        1.61638856e-01, 1.12095833e-01, 2.10931540e-01, 5.32962084e-02,\n",
       "        9.68332291e-02, 1.04340076e-01, 4.71905470e-01, 1.03339195e-01,\n",
       "        3.60304117e-02, 6.25514984e-03, 1.47626996e-01, 1.90663695e-01,\n",
       "        1.56384230e-01, 1.57625675e-02, 2.53468156e-01, 3.96590948e-01,\n",
       "        2.22190738e-01, 1.90913677e-01, 1.57635450e-01, 1.10844493e-01,\n",
       "        1.01837754e-01, 1.53131843e-01, 2.25269794e-03, 6.65572882e-02,\n",
       "        3.31785679e-01, 2.24693537e-01, 3.24029088e-01, 1.33615017e-01,\n",
       "        1.15349293e-01, 2.83242941e-01, 1.18101597e-01, 1.00086093e-01,\n",
       "        7.90672302e-02, 1.62640095e-01, 5.73003292e-02, 1.77648067e-02,\n",
       "        9.35810804e-02, 2.82742500e-01, 7.83172846e-02, 9.35806036e-02,\n",
       "        8.50713253e-03, 8.43230486e-02, 8.60735178e-02, 1.41121507e-01,\n",
       "        1.00123882e-03, 3.61310363e-01, 3.15270424e-02, 1.70147419e-02,\n",
       "        1.48627996e-01, 1.30612731e-01, 8.25738907e-03, 4.55392599e-02,\n",
       "        7.30632544e-02, 1.05340123e-01, 1.29361868e-01, 5.00433445e-02,\n",
       "        8.50701332e-03, 1.49880052e-01, 5.90499640e-02, 1.20603919e-01,\n",
       "        2.06679225e-01, 6.30548000e-02, 1.18601203e-01, 6.00509644e-02,\n",
       "        4.82916832e-02, 3.26029301e-01, 2.45711088e-01, 3.08763742e-01,\n",
       "        1.10098124e-02, 2.80740738e-01, 6.00552559e-03, 3.65307331e-02,\n",
       "        3.41043591e-01, 3.42794657e-02, 4.10350561e-02, 5.72987795e-02,\n",
       "        1.81406379e-01, 3.79076838e-01, 3.14270377e-01, 2.92755365e-02,\n",
       "        1.55133009e-02, 5.20447493e-02, 2.45200396e-02, 7.55650997e-02,\n",
       "        8.90769958e-02, 1.55383348e-01, 3.05255651e-02, 7.78166056e-02,\n",
       "        3.70318890e-02, 1.92667246e-02, 1.16099358e-01, 1.94166899e-01,\n",
       "        1.67641640e-02, 1.49628758e-01, 1.04339838e-01, 1.15090609e-02,\n",
       "        5.45479059e-02, 5.05433083e-02, 8.18207264e-02, 2.95251608e-02,\n",
       "        8.73246193e-02, 9.60825682e-02, 1.41872525e-01, 3.12774181e-02,\n",
       "        6.05520010e-02, 3.75330448e-03, 6.03017807e-02, 4.25374508e-03,\n",
       "        1.27359390e-01, 2.23692536e-01, 4.00342941e-02, 6.88096285e-02,\n",
       "        1.40120506e-01, 1.92670822e-02, 7.50899315e-04, 5.70482016e-02,\n",
       "        1.30612254e-01, 3.55302095e-02, 1.03589058e-01, 5.50473928e-02,\n",
       "        1.95165873e-02, 4.40373421e-02, 2.42706537e-02, 3.77832651e-02,\n",
       "        1.93917394e-01, 6.78081512e-02, 1.12593174e-02, 1.42872691e-01,\n",
       "        1.80149078e-02, 6.00588322e-03, 2.92754173e-02, 1.60137534e-01,\n",
       "        6.25537634e-02, 6.08018637e-02, 1.28859997e-01, 2.67730951e-02,\n",
       "        2.62726545e-02, 1.25157833e-03, 1.07591152e-02, 1.31363869e-01,\n",
       "        5.77999353e-02, 3.20272446e-02, 9.65831280e-02, 5.57987690e-02,\n",
       "        6.00481033e-03, 3.75342369e-03, 2.30201483e-02, 1.42629147e-02,\n",
       "        1.92664862e-02, 6.05518818e-02, 6.58066273e-02, 1.20097399e-02]),\n",
       " 'mean_score_time': array([0.03177822, 0.02527094, 0.02502203, 0.0247705 , 0.02877414,\n",
       "        0.02677107, 0.03152633, 0.02577293, 0.03152537, 0.02452171,\n",
       "        0.02952611, 0.02877343, 0.03002548, 0.02377033, 0.03152764,\n",
       "        0.02752411, 0.02902353, 0.03202438, 0.02577281, 0.02602303,\n",
       "        0.03177762, 0.03452992, 0.02977586, 0.03627908, 0.02477098,\n",
       "        0.02377093, 0.02452135, 0.03102612, 0.02752316, 0.02301967,\n",
       "        0.03953302, 0.02652454, 0.02902472, 0.02902579, 0.03903317,\n",
       "        0.02777505, 0.03452885, 0.03377879, 0.02727377, 0.02451921,\n",
       "        0.03002679, 0.02977335, 0.0277735 , 0.03452742, 0.02702403,\n",
       "        0.02352095, 0.0267731 , 0.02577031, 0.02377033, 0.02452135,\n",
       "        0.03252649, 0.02852559, 0.02477098, 0.02977514, 0.04120517,\n",
       "        0.02927506, 0.02677381, 0.02452135, 0.02627218, 0.0245204 ,\n",
       "        0.02752376, 0.02702475, 0.03703189, 0.03352845, 0.02802396,\n",
       "        0.03853011, 0.03578079, 0.02852643, 0.03503215, 0.0287751 ,\n",
       "        0.03953445, 0.03427839, 0.0290246 , 0.02502072, 0.0252713 ,\n",
       "        0.02402115, 0.03553081, 0.03753221, 0.02802324, 0.02777421,\n",
       "        0.04353726, 0.03077734, 0.03202808, 0.03853261, 0.03102672,\n",
       "        0.03127682, 0.02977657, 0.02727556, 0.03102696, 0.02602243,\n",
       "        0.02977407, 0.02402031, 0.02927721, 0.02677298, 0.03077567,\n",
       "        0.0255214 , 0.03553128, 0.02226889, 0.02377129, 0.02677274,\n",
       "        0.02251935, 0.02452195, 0.02452064, 0.0275234 , 0.0302763 ,\n",
       "        0.03778291, 0.04128587, 0.02552211, 0.02727342, 0.0235194 ,\n",
       "        0.02026761, 0.02727318, 0.02176845, 0.0325284 , 0.02326965,\n",
       "        0.02226913, 0.02351999, 0.02477157, 0.02276981, 0.0280242 ,\n",
       "        0.03352904, 0.02251947, 0.02852428, 0.02502155, 0.02201962,\n",
       "        0.02352023, 0.03628087, 0.02226865, 0.02527189, 0.03803337,\n",
       "        0.02251863, 0.02802384, 0.02602184, 0.02427065, 0.02352035,\n",
       "        0.03653145, 0.02276921, 0.02402055, 0.03327823, 0.02802432,\n",
       "        0.03653145, 0.03978395, 0.02276945, 0.03402948, 0.02702332,\n",
       "        0.02101791, 0.0280242 , 0.02977514, 0.03227878, 0.02827406,\n",
       "        0.02602327, 0.02777386, 0.02502084, 0.02502131, 0.02677321,\n",
       "        0.02602267, 0.02352083, 0.04003501, 0.02076852, 0.0260216 ,\n",
       "        0.02552199, 0.02678072, 0.02477121, 0.02552235, 0.02427089,\n",
       "        0.03127682, 0.02477133, 0.03352857, 0.02427101, 0.03377938,\n",
       "        0.02427113, 0.02201939, 0.02577281, 0.02402055, 0.02877522,\n",
       "        0.0252713 , 0.02552187, 0.02502155, 0.02552176, 0.02377057,\n",
       "        0.02276957, 0.02477109, 0.02176917, 0.02677262, 0.03377903,\n",
       "        0.02352071, 0.03327811, 0.0335288 , 0.02452087, 0.02852416,\n",
       "        0.0262723 , 0.0287745 , 0.02477098, 0.02827442, 0.03703237,\n",
       "        0.02952576, 0.02151811, 0.02977574, 0.02577221, 0.02276957,\n",
       "        0.02502191, 0.0305264 , 0.02276981, 0.03202796, 0.02352059,\n",
       "        0.02276874, 0.02552128, 0.02051687, 0.02301967, 0.02977526,\n",
       "        0.03377879, 0.03002572, 0.02877533, 0.02652299, 0.03528047,\n",
       "        0.02627242, 0.02126873, 0.02327025, 0.02527153, 0.02226877,\n",
       "        0.03402925, 0.02902508, 0.02326918, 0.02652276, 0.02852535,\n",
       "        0.03302884, 0.02577257, 0.02276874, 0.02952564, 0.02627146,\n",
       "        0.02126789, 0.02301884, 0.0227685 , 0.02377033, 0.02602279,\n",
       "        0.03503072, 0.02176905, 0.02276993, 0.02652299, 0.03127635,\n",
       "        0.02201939, 0.02201891, 0.02652323, 0.02151906, 0.02502131,\n",
       "        0.02452123, 0.02527201, 0.02752399, 0.03152728, 0.0260222 ,\n",
       "        0.02326977, 0.02727246, 0.02026796, 0.02727425, 0.02226961,\n",
       "        0.02527142, 0.02677286, 0.02502179, 0.0272733 , 0.02477038,\n",
       "        0.02427077, 0.03553081, 0.02577198, 0.02577138, 0.02677274,\n",
       "        0.02927601, 0.02377057, 0.04303694, 0.02702367, 0.02251899,\n",
       "        0.02827382, 0.03327894, 0.02176905, 0.03152812, 0.02452147,\n",
       "        0.02351999, 0.02477229, 0.02101851, 0.02952504, 0.03202689,\n",
       "        0.02351987, 0.01876628, 0.02026689, 0.01776481, 0.01426256,\n",
       "        0.01651466, 0.01351178, 0.01301193]),\n",
       " 'std_score_time': array([6.25574589e-03, 1.75118446e-03, 4.00304794e-03, 3.75294685e-03,\n",
       "        1.75249577e-03, 1.74999237e-03, 3.00431252e-03, 1.75106525e-03,\n",
       "        2.50267982e-03, 1.00052357e-03, 2.50208378e-03, 3.25167179e-03,\n",
       "        5.00297546e-03, 2.50577927e-04, 2.50279903e-03, 2.50208378e-03,\n",
       "        4.00340557e-03, 4.99248505e-04, 3.25417519e-03, 1.00076199e-03,\n",
       "        6.25562668e-03, 8.50868225e-03, 2.25210190e-03, 8.25822353e-03,\n",
       "        7.49826431e-04, 1.75058842e-03, 1.00135803e-03, 2.00080872e-03,\n",
       "        1.00100040e-03, 1.00100040e-03, 1.20102167e-02, 1.99985504e-03,\n",
       "        3.00276279e-03, 2.38418579e-07, 1.00095272e-02, 1.25122070e-03,\n",
       "        2.50136852e-03, 1.25181675e-03, 1.75249577e-03, 3.00145149e-03,\n",
       "        3.50368023e-03, 4.25326824e-03, 5.25510311e-03, 8.50927830e-03,\n",
       "        5.00917435e-04, 1.50108337e-03, 7.48991966e-04, 2.75313854e-03,\n",
       "        1.25217438e-03, 4.50515747e-03, 1.00090504e-02, 5.01394272e-04,\n",
       "        1.75309181e-03, 2.50101089e-04, 1.23393536e-02, 1.75201893e-03,\n",
       "        7.51137733e-04, 1.00040436e-03, 7.50422478e-04, 1.50275230e-03,\n",
       "        3.00240517e-03, 3.00121307e-03, 5.00440598e-03, 9.00685787e-03,\n",
       "        5.00202179e-04, 1.30105019e-02, 3.75187397e-03, 3.00061703e-03,\n",
       "        8.00597668e-03, 2.75242329e-03, 7.50672817e-03, 4.25148010e-03,\n",
       "        7.50660896e-03, 2.50303745e-03, 3.75425816e-03, 4.99963760e-04,\n",
       "        2.00200081e-03, 9.50872898e-03, 4.00447845e-03, 2.25079060e-03,\n",
       "        1.15102530e-02, 4.75418568e-03, 6.50513172e-03, 1.35113001e-02,\n",
       "        1.05086565e-02, 5.75554371e-03, 8.75687599e-03, 3.25155258e-03,\n",
       "        1.50215626e-03, 3.50189209e-03, 7.48991966e-04, 5.00559807e-04,\n",
       "        2.25317478e-03, 2.25162506e-03, 2.25102901e-03, 4.99606133e-04,\n",
       "        1.15091801e-02, 1.75130367e-03, 7.49826431e-04, 3.25274467e-03,\n",
       "        1.00135803e-03, 4.99844551e-04, 2.50196457e-03, 5.00321388e-04,\n",
       "        8.25738907e-03, 1.17604733e-02, 2.75218487e-03, 2.00212002e-03,\n",
       "        2.75278091e-03, 1.19209290e-07, 1.25086308e-03, 5.25498390e-03,\n",
       "        1.75154209e-03, 1.00088120e-03, 2.50816345e-04, 2.50220299e-04,\n",
       "        5.00440598e-04, 7.50660896e-04, 2.49981880e-04, 6.00504875e-03,\n",
       "        1.10099316e-02, 1.50120258e-03, 3.50296497e-03, 2.00176239e-03,\n",
       "        1.50084496e-03, 2.00176239e-03, 2.75278091e-03, 2.49981880e-04,\n",
       "        2.25234032e-03, 5.00559807e-04, 1.00064278e-03, 2.00092793e-03,\n",
       "        2.50113010e-03, 7.50899315e-04, 1.50215626e-03, 7.50660896e-03,\n",
       "        7.50780106e-04, 5.00321388e-04, 6.25491142e-03, 4.50360775e-03,\n",
       "        1.40116215e-02, 4.75454330e-03, 2.49624252e-04, 3.50332260e-03,\n",
       "        3.50260735e-03, 5.00321388e-04, 5.01394272e-04, 1.25169754e-03,\n",
       "        2.25043297e-03, 3.25274467e-03, 1.50167942e-03, 3.25298309e-03,\n",
       "        7.15255737e-07, 2.00176239e-03, 2.75254250e-03, 1.50060654e-03,\n",
       "        4.99606133e-04, 1.95164680e-02, 7.50184059e-04, 1.50144100e-03,\n",
       "        2.00080872e-03, 7.58051872e-04, 1.75118446e-03, 2.50184536e-03,\n",
       "        2.75266171e-03, 3.25286388e-03, 1.75106525e-03, 2.00152397e-03,\n",
       "        1.75213814e-03, 2.25162506e-03, 2.25198269e-03, 1.00064278e-03,\n",
       "        3.75294685e-03, 2.00092793e-03, 6.75582886e-03, 1.75130367e-03,\n",
       "        5.00798225e-04, 5.01155853e-04, 2.50148773e-03, 7.50780106e-04,\n",
       "        2.49505043e-04, 4.75418568e-03, 2.50458717e-04, 1.25038624e-03,\n",
       "        4.25374508e-03, 1.00183487e-03, 2.49981880e-04, 3.50308418e-03,\n",
       "        1.00111961e-03, 5.00679016e-04, 4.75430489e-03, 2.75230408e-03,\n",
       "        2.50339508e-04, 2.75194645e-03, 7.00640678e-03, 7.50708580e-03,\n",
       "        9.99569893e-04, 6.75666332e-03, 1.25181675e-03, 7.51852989e-04,\n",
       "        1.50096416e-03, 9.00793076e-03, 2.49981880e-04, 0.00000000e+00,\n",
       "        1.00147724e-03, 7.50541687e-04, 5.00440598e-03, 2.38418579e-07,\n",
       "        1.50167942e-03, 5.75506687e-03, 7.25638866e-03, 5.00679016e-04,\n",
       "        5.75482845e-03, 4.50432301e-03, 1.32615566e-02, 1.75225735e-03,\n",
       "        2.49981880e-04, 1.75082684e-03, 7.50899315e-04, 2.25186348e-03,\n",
       "        5.50436974e-03, 2.50267982e-03, 2.49624252e-04, 2.50232220e-03,\n",
       "        6.50548935e-03, 1.00085735e-02, 7.51256943e-04, 1.25098228e-03,\n",
       "        6.00540638e-03, 2.50697136e-04, 2.50339508e-04, 1.50179863e-03,\n",
       "        2.75135040e-03, 1.25098228e-03, 4.99844551e-04, 1.05096102e-02,\n",
       "        1.25122070e-03, 2.50101089e-04, 3.50344181e-03, 2.75194645e-03,\n",
       "        5.00202179e-04, 4.99725342e-04, 2.50279903e-03, 1.19209290e-07,\n",
       "        5.00679016e-04, 2.00164318e-03, 1.75106525e-03, 3.50356102e-03,\n",
       "        5.50460815e-03, 0.00000000e+00, 2.50935555e-04, 4.25410271e-03,\n",
       "        2.50339508e-04, 3.75354290e-03, 2.75242329e-03, 1.25074387e-03,\n",
       "        2.49981880e-04, 3.00240517e-03, 5.25438786e-03, 1.25038624e-03,\n",
       "        1.25098228e-03, 2.00176239e-03, 7.50660896e-04, 7.50541687e-04,\n",
       "        1.75046921e-03, 7.75754452e-03, 1.75166130e-03, 4.00352478e-03,\n",
       "        3.00252438e-03, 5.01036644e-04, 2.25281715e-03, 2.25162506e-03,\n",
       "        2.50339508e-04, 6.50560856e-03, 1.00100040e-03, 1.00088120e-03,\n",
       "        2.50935555e-04, 1.00088120e-03, 8.00704956e-03, 6.00564480e-03,\n",
       "        2.00188160e-03, 1.24990940e-03, 1.25205517e-03, 1.25169754e-03,\n",
       "        2.50220299e-04, 2.50208378e-03, 4.99844551e-04, 2.38418579e-07]),\n",
       " 'param_classifier': masked_array(data=[MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__activation': masked_array(data=['logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__alpha': masked_array(data=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__hidden_layer_sizes': masked_array(data=[(5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__learning_rate': masked_array(data=['constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'}],\n",
       " 'split0_test_recall_micro': array([0.9956, 0.9948, 0.9956, 0.9928, 0.994 , 0.9948, 0.9952, 0.994 ,\n",
       "        0.9944, 0.994 , 0.9884, 0.9948, 0.9944, 0.9936, 0.9936, 0.9952,\n",
       "        0.9952, 0.9948, 0.992 , 0.994 , 0.9912, 0.9924, 0.9908, 0.9956,\n",
       "        0.9952, 0.9904, 0.9944, 0.9916, 0.992 , 0.9928, 0.9912, 0.9908,\n",
       "        0.9912, 0.9908, 0.9908, 0.9908, 0.992 , 0.99  , 0.9908, 0.9908,\n",
       "        0.9904, 0.99  , 0.99  , 0.99  , 0.9892, 0.9904, 0.99  , 0.9904,\n",
       "        0.9952, 0.996 , 0.9952, 0.9952, 0.994 , 0.9912, 0.9936, 0.9944,\n",
       "        0.994 , 0.9916, 0.992 , 0.994 , 0.9952, 0.9936, 0.9948, 0.99  ,\n",
       "        0.9952, 0.9948, 0.992 , 0.9944, 0.9936, 0.9896, 0.9936, 0.99  ,\n",
       "        0.9948, 0.9904, 0.994 , 0.992 , 0.99  , 0.99  , 0.9896, 0.9904,\n",
       "        0.9896, 0.99  , 0.99  , 0.9896, 0.9916, 0.9924, 0.9908, 0.9904,\n",
       "        0.9912, 0.9904, 0.9904, 0.9908, 0.9904, 0.9908, 0.9904, 0.9904,\n",
       "        0.9584, 0.1888, 0.9612, 0.9584, 0.5852, 0.9612, 0.9532, 0.2256,\n",
       "        0.956 , 0.958 , 0.5852, 0.9588, 0.7812, 0.532 , 0.7936, 0.9572,\n",
       "        0.0452, 0.96  , 0.9596, 0.2756, 0.9524, 0.9528, 0.2156, 0.9568,\n",
       "        0.9092, 0.5852, 0.9672, 0.9624, 0.5504, 0.9584, 0.9556, 0.5812,\n",
       "        0.9588, 0.9564, 0.5832, 0.9552, 0.7972, 0.5836, 0.778 , 0.9644,\n",
       "        0.5784, 0.96  , 0.9524, 0.6992, 0.9576, 0.9536, 0.5828, 0.95  ,\n",
       "        0.9508, 0.5624, 0.9524, 0.954 , 0.2752, 0.9548, 0.9556, 0.294 ,\n",
       "        0.9508, 0.9548, 0.5452, 0.956 , 0.952 , 0.5744, 0.9512, 0.9548,\n",
       "        0.4676, 0.9544, 0.9552, 0.4568, 0.956 , 0.9544, 0.4   , 0.9544,\n",
       "        0.9532, 0.4752, 0.9516, 0.954 , 0.3888, 0.9552, 0.9528, 0.5936,\n",
       "        0.9548, 0.9556, 0.1496, 0.9552, 0.9556, 0.2708, 0.9504, 0.9532,\n",
       "        0.5184, 0.9548, 0.9556, 0.5564, 0.952 , 0.95  , 0.3132, 0.956 ,\n",
       "        0.9592, 0.964 , 0.9608, 0.964 , 0.9644, 0.964 , 0.9672, 0.9672,\n",
       "        0.968 , 0.9688, 0.9668, 0.968 , 0.9576, 0.9612, 0.9608, 0.9652,\n",
       "        0.9644, 0.9648, 0.9668, 0.9672, 0.9672, 0.9676, 0.9664, 0.9692,\n",
       "        0.962 , 0.9604, 0.964 , 0.964 , 0.9652, 0.9648, 0.9672, 0.968 ,\n",
       "        0.9676, 0.9676, 0.9672, 0.968 , 0.9604, 0.96  , 0.96  , 0.964 ,\n",
       "        0.964 , 0.96  , 0.964 , 0.9648, 0.9644, 0.9656, 0.9644, 0.964 ,\n",
       "        0.9696, 0.9844, 0.972 , 0.9884, 0.9856, 0.986 , 0.9864, 0.988 ,\n",
       "        0.988 , 0.988 , 0.988 , 0.988 , 0.9828, 0.9828, 0.9828, 0.986 ,\n",
       "        0.9856, 0.9864, 0.988 , 0.9876, 0.9892, 0.9876, 0.9884, 0.9888,\n",
       "        0.9844, 0.984 , 0.9724, 0.986 , 0.986 , 0.9864, 0.988 , 0.988 ,\n",
       "        0.988 , 0.988 , 0.988 , 0.9888, 0.972 , 0.9848, 0.968 , 0.988 ,\n",
       "        0.9856, 0.9852, 0.988 , 0.9864, 0.988 , 0.9864, 0.988 , 0.988 ]),\n",
       " 'split1_test_recall_micro': array([9.976e-01, 9.948e-01, 9.892e-01, 9.880e-01, 9.968e-01, 9.964e-01,\n",
       "        9.948e-01, 9.904e-01, 9.916e-01, 9.944e-01, 9.888e-01, 9.904e-01,\n",
       "        9.964e-01, 9.892e-01, 9.892e-01, 9.964e-01, 9.900e-01, 9.940e-01,\n",
       "        9.956e-01, 9.956e-01, 9.908e-01, 9.968e-01, 9.900e-01, 9.908e-01,\n",
       "        9.900e-01, 9.904e-01, 9.896e-01, 9.900e-01, 9.904e-01, 9.904e-01,\n",
       "        9.900e-01, 9.900e-01, 9.900e-01, 9.900e-01, 9.896e-01, 9.896e-01,\n",
       "        9.892e-01, 9.888e-01, 9.888e-01, 9.888e-01, 9.884e-01, 9.884e-01,\n",
       "        9.888e-01, 9.888e-01, 9.888e-01, 9.888e-01, 9.888e-01, 9.888e-01,\n",
       "        9.980e-01, 9.972e-01, 9.956e-01, 9.936e-01, 9.972e-01, 9.980e-01,\n",
       "        9.968e-01, 9.964e-01, 9.948e-01, 9.968e-01, 9.960e-01, 9.916e-01,\n",
       "        9.928e-01, 9.920e-01, 9.928e-01, 9.916e-01, 9.968e-01, 9.980e-01,\n",
       "        9.952e-01, 9.912e-01, 9.904e-01, 9.960e-01, 9.960e-01, 9.956e-01,\n",
       "        9.964e-01, 9.916e-01, 9.912e-01, 9.900e-01, 9.964e-01, 9.900e-01,\n",
       "        9.908e-01, 9.964e-01, 9.900e-01, 9.904e-01, 9.904e-01, 9.900e-01,\n",
       "        9.900e-01, 9.896e-01, 9.884e-01, 9.900e-01, 9.900e-01, 9.900e-01,\n",
       "        9.892e-01, 9.900e-01, 9.900e-01, 9.900e-01, 9.900e-01, 9.900e-01,\n",
       "        7.928e-01, 1.888e-01, 9.568e-01, 9.572e-01, 6.732e-01, 9.412e-01,\n",
       "        9.520e-01, 5.832e-01, 9.488e-01, 9.420e-01, 5.848e-01, 9.436e-01,\n",
       "        9.616e-01, 5.744e-01, 8.600e-01, 9.532e-01, 5.028e-01, 9.520e-01,\n",
       "        9.464e-01, 5.848e-01, 9.384e-01, 9.400e-01, 5.832e-01, 9.440e-01,\n",
       "        7.916e-01, 2.280e-01, 9.504e-01, 9.492e-01, 5.508e-01, 9.204e-01,\n",
       "        9.460e-01, 5.840e-01, 9.384e-01, 9.464e-01, 5.848e-01, 9.424e-01,\n",
       "        7.940e-01, 5.848e-01, 7.796e-01, 9.440e-01, 1.892e-01, 9.592e-01,\n",
       "        9.532e-01, 8.000e-04, 9.476e-01, 9.408e-01, 5.848e-01, 9.388e-01,\n",
       "        9.456e-01, 5.460e-01, 9.380e-01, 9.412e-01, 6.460e-01, 9.420e-01,\n",
       "        9.412e-01, 6.648e-01, 9.444e-01, 9.384e-01, 6.348e-01, 9.416e-01,\n",
       "        9.404e-01, 2.236e-01, 9.352e-01, 9.372e-01, 6.940e-01, 9.428e-01,\n",
       "        9.388e-01, 5.208e-01, 9.400e-01, 9.400e-01, 4.876e-01, 9.400e-01,\n",
       "        9.408e-01, 5.160e-01, 9.428e-01, 9.428e-01, 7.960e-02, 9.396e-01,\n",
       "        9.408e-01, 7.108e-01, 9.428e-01, 9.432e-01, 5.088e-01, 9.420e-01,\n",
       "        9.416e-01, 5.472e-01, 9.396e-01, 9.428e-01, 2.760e-01, 9.416e-01,\n",
       "        9.420e-01, 2.536e-01, 9.396e-01, 9.388e-01, 3.000e-01, 9.416e-01,\n",
       "        9.520e-01, 9.508e-01, 9.484e-01, 9.556e-01, 9.556e-01, 9.556e-01,\n",
       "        9.560e-01, 9.568e-01, 9.580e-01, 9.596e-01, 9.556e-01, 9.580e-01,\n",
       "        9.492e-01, 9.492e-01, 9.512e-01, 9.556e-01, 9.548e-01, 9.544e-01,\n",
       "        9.564e-01, 9.568e-01, 9.568e-01, 9.572e-01, 9.600e-01, 9.584e-01,\n",
       "        9.488e-01, 9.488e-01, 9.504e-01, 9.540e-01, 9.520e-01, 9.536e-01,\n",
       "        9.560e-01, 9.560e-01, 9.552e-01, 9.580e-01, 9.572e-01, 9.596e-01,\n",
       "        9.468e-01, 9.444e-01, 9.476e-01, 9.532e-01, 9.524e-01, 9.508e-01,\n",
       "        9.524e-01, 9.540e-01, 9.520e-01, 9.528e-01, 9.540e-01, 9.536e-01,\n",
       "        9.724e-01, 9.692e-01, 9.616e-01, 9.768e-01, 9.792e-01, 9.772e-01,\n",
       "        9.812e-01, 9.804e-01, 9.800e-01, 9.800e-01, 9.800e-01, 9.816e-01,\n",
       "        9.616e-01, 9.760e-01, 9.720e-01, 9.792e-01, 9.788e-01, 9.772e-01,\n",
       "        9.780e-01, 9.800e-01, 9.792e-01, 9.808e-01, 9.796e-01, 9.792e-01,\n",
       "        9.708e-01, 9.596e-01, 9.732e-01, 9.780e-01, 9.744e-01, 9.788e-01,\n",
       "        9.808e-01, 9.788e-01, 9.804e-01, 9.816e-01, 9.804e-01, 9.800e-01,\n",
       "        9.588e-01, 9.580e-01, 9.728e-01, 9.744e-01, 9.776e-01, 9.768e-01,\n",
       "        9.772e-01, 9.776e-01, 9.776e-01, 9.800e-01, 9.788e-01, 9.776e-01]),\n",
       " 'mean_test_recall_micro': array([0.9966, 0.9948, 0.9924, 0.9904, 0.9954, 0.9956, 0.995 , 0.9922,\n",
       "        0.993 , 0.9942, 0.9886, 0.9926, 0.9954, 0.9914, 0.9914, 0.9958,\n",
       "        0.9926, 0.9944, 0.9938, 0.9948, 0.991 , 0.9946, 0.9904, 0.9932,\n",
       "        0.9926, 0.9904, 0.992 , 0.9908, 0.9912, 0.9916, 0.9906, 0.9904,\n",
       "        0.9906, 0.9904, 0.9902, 0.9902, 0.9906, 0.9894, 0.9898, 0.9898,\n",
       "        0.9894, 0.9892, 0.9894, 0.9894, 0.989 , 0.9896, 0.9894, 0.9896,\n",
       "        0.9966, 0.9966, 0.9954, 0.9944, 0.9956, 0.9946, 0.9952, 0.9954,\n",
       "        0.9944, 0.9942, 0.994 , 0.9928, 0.994 , 0.9928, 0.9938, 0.9908,\n",
       "        0.996 , 0.9964, 0.9936, 0.9928, 0.992 , 0.9928, 0.9948, 0.9928,\n",
       "        0.9956, 0.991 , 0.9926, 0.991 , 0.9932, 0.99  , 0.9902, 0.9934,\n",
       "        0.9898, 0.9902, 0.9902, 0.9898, 0.9908, 0.991 , 0.9896, 0.9902,\n",
       "        0.9906, 0.9902, 0.9898, 0.9904, 0.9902, 0.9904, 0.9902, 0.9902,\n",
       "        0.8756, 0.1888, 0.959 , 0.9578, 0.6292, 0.9512, 0.9526, 0.4044,\n",
       "        0.9524, 0.95  , 0.585 , 0.9512, 0.8714, 0.5532, 0.8268, 0.9552,\n",
       "        0.274 , 0.956 , 0.953 , 0.4302, 0.9454, 0.9464, 0.3994, 0.9504,\n",
       "        0.8504, 0.4066, 0.9588, 0.9558, 0.5506, 0.9394, 0.9508, 0.5826,\n",
       "        0.9486, 0.9514, 0.584 , 0.9488, 0.7956, 0.5842, 0.7788, 0.9542,\n",
       "        0.3838, 0.9596, 0.9528, 0.35  , 0.9526, 0.9472, 0.5838, 0.9444,\n",
       "        0.9482, 0.5542, 0.9452, 0.9476, 0.4606, 0.9484, 0.9484, 0.4794,\n",
       "        0.9476, 0.9466, 0.59  , 0.9488, 0.9462, 0.399 , 0.9432, 0.946 ,\n",
       "        0.5808, 0.9486, 0.947 , 0.4888, 0.948 , 0.9472, 0.4438, 0.9472,\n",
       "        0.947 , 0.4956, 0.9472, 0.9484, 0.2342, 0.9474, 0.9468, 0.6522,\n",
       "        0.9488, 0.9494, 0.3292, 0.9486, 0.9486, 0.409 , 0.945 , 0.948 ,\n",
       "        0.3972, 0.9482, 0.9488, 0.405 , 0.9458, 0.9444, 0.3066, 0.9488,\n",
       "        0.9556, 0.9574, 0.9546, 0.9598, 0.96  , 0.9598, 0.9616, 0.962 ,\n",
       "        0.963 , 0.9642, 0.9612, 0.963 , 0.9534, 0.9552, 0.956 , 0.9604,\n",
       "        0.9596, 0.9596, 0.9616, 0.962 , 0.962 , 0.9624, 0.9632, 0.9638,\n",
       "        0.9554, 0.9546, 0.9572, 0.959 , 0.9586, 0.9592, 0.9616, 0.962 ,\n",
       "        0.9614, 0.9628, 0.9622, 0.9638, 0.9536, 0.9522, 0.9538, 0.9586,\n",
       "        0.9582, 0.9554, 0.9582, 0.9594, 0.9582, 0.9592, 0.9592, 0.9588,\n",
       "        0.971 , 0.9768, 0.9668, 0.9826, 0.9824, 0.9816, 0.9838, 0.9842,\n",
       "        0.984 , 0.984 , 0.984 , 0.9848, 0.9722, 0.9794, 0.9774, 0.9826,\n",
       "        0.9822, 0.9818, 0.983 , 0.9838, 0.9842, 0.9842, 0.984 , 0.984 ,\n",
       "        0.9776, 0.9718, 0.9728, 0.982 , 0.9802, 0.9826, 0.9844, 0.9834,\n",
       "        0.9842, 0.9848, 0.9842, 0.9844, 0.9654, 0.9714, 0.9704, 0.9812,\n",
       "        0.9816, 0.981 , 0.9826, 0.982 , 0.9828, 0.9832, 0.9834, 0.9828]),\n",
       " 'std_test_recall_micro': array([1.000e-03, 0.000e+00, 3.200e-03, 2.400e-03, 1.400e-03, 8.000e-04,\n",
       "        2.000e-04, 1.800e-03, 1.400e-03, 2.000e-04, 2.000e-04, 2.200e-03,\n",
       "        1.000e-03, 2.200e-03, 2.200e-03, 6.000e-04, 2.600e-03, 4.000e-04,\n",
       "        1.800e-03, 8.000e-04, 2.000e-04, 2.200e-03, 4.000e-04, 2.400e-03,\n",
       "        2.600e-03, 0.000e+00, 2.400e-03, 8.000e-04, 8.000e-04, 1.200e-03,\n",
       "        6.000e-04, 4.000e-04, 6.000e-04, 4.000e-04, 6.000e-04, 6.000e-04,\n",
       "        1.400e-03, 6.000e-04, 1.000e-03, 1.000e-03, 1.000e-03, 8.000e-04,\n",
       "        6.000e-04, 6.000e-04, 2.000e-04, 8.000e-04, 6.000e-04, 8.000e-04,\n",
       "        1.400e-03, 6.000e-04, 2.000e-04, 8.000e-04, 1.600e-03, 3.400e-03,\n",
       "        1.600e-03, 1.000e-03, 4.000e-04, 2.600e-03, 2.000e-03, 1.200e-03,\n",
       "        1.200e-03, 8.000e-04, 1.000e-03, 8.000e-04, 8.000e-04, 1.600e-03,\n",
       "        1.600e-03, 1.600e-03, 1.600e-03, 3.200e-03, 1.200e-03, 2.800e-03,\n",
       "        8.000e-04, 6.000e-04, 1.400e-03, 1.000e-03, 3.200e-03, 0.000e+00,\n",
       "        6.000e-04, 3.000e-03, 2.000e-04, 2.000e-04, 2.000e-04, 2.000e-04,\n",
       "        8.000e-04, 1.400e-03, 1.200e-03, 2.000e-04, 6.000e-04, 2.000e-04,\n",
       "        6.000e-04, 4.000e-04, 2.000e-04, 4.000e-04, 2.000e-04, 2.000e-04,\n",
       "        8.280e-02, 0.000e+00, 2.200e-03, 6.000e-04, 4.400e-02, 1.000e-02,\n",
       "        6.000e-04, 1.788e-01, 3.600e-03, 8.000e-03, 2.000e-04, 7.600e-03,\n",
       "        9.020e-02, 2.120e-02, 3.320e-02, 2.000e-03, 2.288e-01, 4.000e-03,\n",
       "        6.600e-03, 1.546e-01, 7.000e-03, 6.400e-03, 1.838e-01, 6.400e-03,\n",
       "        5.880e-02, 1.786e-01, 8.400e-03, 6.600e-03, 2.000e-04, 1.900e-02,\n",
       "        4.800e-03, 1.400e-03, 1.020e-02, 5.000e-03, 8.000e-04, 6.400e-03,\n",
       "        1.600e-03, 6.000e-04, 8.000e-04, 1.020e-02, 1.946e-01, 4.000e-04,\n",
       "        4.000e-04, 3.492e-01, 5.000e-03, 6.400e-03, 1.000e-03, 5.600e-03,\n",
       "        2.600e-03, 8.200e-03, 7.200e-03, 6.400e-03, 1.854e-01, 6.400e-03,\n",
       "        7.200e-03, 1.854e-01, 3.200e-03, 8.200e-03, 4.480e-02, 7.200e-03,\n",
       "        5.800e-03, 1.754e-01, 8.000e-03, 8.800e-03, 1.132e-01, 5.800e-03,\n",
       "        8.200e-03, 3.200e-02, 8.000e-03, 7.200e-03, 4.380e-02, 7.200e-03,\n",
       "        6.200e-03, 2.040e-02, 4.400e-03, 5.600e-03, 1.546e-01, 7.800e-03,\n",
       "        6.000e-03, 5.860e-02, 6.000e-03, 6.200e-03, 1.796e-01, 6.600e-03,\n",
       "        7.000e-03, 1.382e-01, 5.400e-03, 5.200e-03, 1.212e-01, 6.600e-03,\n",
       "        6.800e-03, 1.514e-01, 6.200e-03, 5.600e-03, 6.600e-03, 7.200e-03,\n",
       "        3.600e-03, 6.600e-03, 6.200e-03, 4.200e-03, 4.400e-03, 4.200e-03,\n",
       "        5.600e-03, 5.200e-03, 5.000e-03, 4.600e-03, 5.600e-03, 5.000e-03,\n",
       "        4.200e-03, 6.000e-03, 4.800e-03, 4.800e-03, 4.800e-03, 5.200e-03,\n",
       "        5.200e-03, 5.200e-03, 5.200e-03, 5.200e-03, 3.200e-03, 5.400e-03,\n",
       "        6.600e-03, 5.800e-03, 6.800e-03, 5.000e-03, 6.600e-03, 5.600e-03,\n",
       "        5.600e-03, 6.000e-03, 6.200e-03, 4.800e-03, 5.000e-03, 4.200e-03,\n",
       "        6.800e-03, 7.800e-03, 6.200e-03, 5.400e-03, 5.800e-03, 4.600e-03,\n",
       "        5.800e-03, 5.400e-03, 6.200e-03, 6.400e-03, 5.200e-03, 5.200e-03,\n",
       "        1.400e-03, 7.600e-03, 5.200e-03, 5.800e-03, 3.200e-03, 4.400e-03,\n",
       "        2.600e-03, 3.800e-03, 4.000e-03, 4.000e-03, 4.000e-03, 3.200e-03,\n",
       "        1.060e-02, 3.400e-03, 5.400e-03, 3.400e-03, 3.400e-03, 4.600e-03,\n",
       "        5.000e-03, 3.800e-03, 5.000e-03, 3.400e-03, 4.400e-03, 4.800e-03,\n",
       "        6.800e-03, 1.220e-02, 4.000e-04, 4.000e-03, 5.800e-03, 3.800e-03,\n",
       "        3.600e-03, 4.600e-03, 3.800e-03, 3.200e-03, 3.800e-03, 4.400e-03,\n",
       "        6.600e-03, 1.340e-02, 2.400e-03, 6.800e-03, 4.000e-03, 4.200e-03,\n",
       "        5.400e-03, 4.400e-03, 5.200e-03, 3.200e-03, 4.600e-03, 5.200e-03]),\n",
       " 'rank_test_recall_micro': array([  1,  16,  44,  63,  10,   7,  15,  45,  34,  24,  96,  40,  12,\n",
       "         49,  49,   6,  40,  21,  28,  16,  52,  19,  63,  32,  40,  63,\n",
       "         46,  56,  51,  48,  59,  63,  59,  63,  70,  70,  59,  89,  81,\n",
       "         81,  93,  94,  89,  89,  95,  86,  89,  86,   2,   2,  10,  21,\n",
       "          7,  19,  14,  12,  21,  24,  26,  35,  26,  35,  28,  56,   5,\n",
       "          4,  30,  39,  46,  35,  16,  35,   7,  52,  40,  52,  33,  80,\n",
       "         70,  31,  81,  70,  70,  81,  56,  52,  86,  70,  59,  70,  81,\n",
       "         63,  70,  63,  70,  70, 251, 288, 174, 183, 258, 207, 202, 278,\n",
       "        204, 211, 260, 207, 252, 267, 254, 192, 286, 186, 200, 274, 244,\n",
       "        240, 279, 210, 253, 276, 176, 188, 268, 250, 209, 264, 218, 206,\n",
       "        262, 213, 255, 261, 256, 196, 282, 167, 201, 283, 202, 232, 263,\n",
       "        247, 225, 266, 245, 229, 272, 223, 222, 271, 229, 239, 259, 214,\n",
       "        241, 280, 249, 242, 265, 218, 236, 270, 227, 232, 273, 232, 236,\n",
       "        269, 232, 223, 287, 231, 238, 257, 214, 212, 284, 218, 218, 275,\n",
       "        246, 227, 281, 225, 214, 277, 243, 247, 285, 214, 189, 184, 194,\n",
       "        165, 164, 165, 158, 154, 149, 145, 162, 149, 199, 192, 186, 163,\n",
       "        167, 167, 158, 154, 154, 152, 148, 146, 190, 194, 185, 175, 179,\n",
       "        171, 158, 154, 161, 151, 153, 146, 198, 205, 197, 178, 180, 190,\n",
       "        180, 170, 180, 171, 171, 176, 141, 136, 143, 120, 123, 128, 111,\n",
       "        101, 106, 106, 106,  97, 138, 133, 135, 120, 124, 127, 116, 111,\n",
       "        101, 101, 106, 106, 134, 139, 137, 125, 132, 119,  99, 113, 101,\n",
       "         97, 101,  99, 144, 140, 142, 130, 128, 131, 120, 125, 117, 115,\n",
       "        113, 117]),\n",
       " 'split0_test_f1_micro': array([0.9956, 0.9948, 0.9956, 0.9928, 0.994 , 0.9948, 0.9952, 0.994 ,\n",
       "        0.9944, 0.994 , 0.9884, 0.9948, 0.9944, 0.9936, 0.9936, 0.9952,\n",
       "        0.9952, 0.9948, 0.992 , 0.994 , 0.9912, 0.9924, 0.9908, 0.9956,\n",
       "        0.9952, 0.9904, 0.9944, 0.9916, 0.992 , 0.9928, 0.9912, 0.9908,\n",
       "        0.9912, 0.9908, 0.9908, 0.9908, 0.992 , 0.99  , 0.9908, 0.9908,\n",
       "        0.9904, 0.99  , 0.99  , 0.99  , 0.9892, 0.9904, 0.99  , 0.9904,\n",
       "        0.9952, 0.996 , 0.9952, 0.9952, 0.994 , 0.9912, 0.9936, 0.9944,\n",
       "        0.994 , 0.9916, 0.992 , 0.994 , 0.9952, 0.9936, 0.9948, 0.99  ,\n",
       "        0.9952, 0.9948, 0.992 , 0.9944, 0.9936, 0.9896, 0.9936, 0.99  ,\n",
       "        0.9948, 0.9904, 0.994 , 0.992 , 0.99  , 0.99  , 0.9896, 0.9904,\n",
       "        0.9896, 0.99  , 0.99  , 0.9896, 0.9916, 0.9924, 0.9908, 0.9904,\n",
       "        0.9912, 0.9904, 0.9904, 0.9908, 0.9904, 0.9908, 0.9904, 0.9904,\n",
       "        0.9584, 0.1888, 0.9612, 0.9584, 0.5852, 0.9612, 0.9532, 0.2256,\n",
       "        0.956 , 0.958 , 0.5852, 0.9588, 0.7812, 0.532 , 0.7936, 0.9572,\n",
       "        0.0452, 0.96  , 0.9596, 0.2756, 0.9524, 0.9528, 0.2156, 0.9568,\n",
       "        0.9092, 0.5852, 0.9672, 0.9624, 0.5504, 0.9584, 0.9556, 0.5812,\n",
       "        0.9588, 0.9564, 0.5832, 0.9552, 0.7972, 0.5836, 0.778 , 0.9644,\n",
       "        0.5784, 0.96  , 0.9524, 0.6992, 0.9576, 0.9536, 0.5828, 0.95  ,\n",
       "        0.9508, 0.5624, 0.9524, 0.954 , 0.2752, 0.9548, 0.9556, 0.294 ,\n",
       "        0.9508, 0.9548, 0.5452, 0.956 , 0.952 , 0.5744, 0.9512, 0.9548,\n",
       "        0.4676, 0.9544, 0.9552, 0.4568, 0.956 , 0.9544, 0.4   , 0.9544,\n",
       "        0.9532, 0.4752, 0.9516, 0.954 , 0.3888, 0.9552, 0.9528, 0.5936,\n",
       "        0.9548, 0.9556, 0.1496, 0.9552, 0.9556, 0.2708, 0.9504, 0.9532,\n",
       "        0.5184, 0.9548, 0.9556, 0.5564, 0.952 , 0.95  , 0.3132, 0.956 ,\n",
       "        0.9592, 0.964 , 0.9608, 0.964 , 0.9644, 0.964 , 0.9672, 0.9672,\n",
       "        0.968 , 0.9688, 0.9668, 0.968 , 0.9576, 0.9612, 0.9608, 0.9652,\n",
       "        0.9644, 0.9648, 0.9668, 0.9672, 0.9672, 0.9676, 0.9664, 0.9692,\n",
       "        0.962 , 0.9604, 0.964 , 0.964 , 0.9652, 0.9648, 0.9672, 0.968 ,\n",
       "        0.9676, 0.9676, 0.9672, 0.968 , 0.9604, 0.96  , 0.96  , 0.964 ,\n",
       "        0.964 , 0.96  , 0.964 , 0.9648, 0.9644, 0.9656, 0.9644, 0.964 ,\n",
       "        0.9696, 0.9844, 0.972 , 0.9884, 0.9856, 0.986 , 0.9864, 0.988 ,\n",
       "        0.988 , 0.988 , 0.988 , 0.988 , 0.9828, 0.9828, 0.9828, 0.986 ,\n",
       "        0.9856, 0.9864, 0.988 , 0.9876, 0.9892, 0.9876, 0.9884, 0.9888,\n",
       "        0.9844, 0.984 , 0.9724, 0.986 , 0.986 , 0.9864, 0.988 , 0.988 ,\n",
       "        0.988 , 0.988 , 0.988 , 0.9888, 0.972 , 0.9848, 0.968 , 0.988 ,\n",
       "        0.9856, 0.9852, 0.988 , 0.9864, 0.988 , 0.9864, 0.988 , 0.988 ]),\n",
       " 'split1_test_f1_micro': array([9.976e-01, 9.948e-01, 9.892e-01, 9.880e-01, 9.968e-01, 9.964e-01,\n",
       "        9.948e-01, 9.904e-01, 9.916e-01, 9.944e-01, 9.888e-01, 9.904e-01,\n",
       "        9.964e-01, 9.892e-01, 9.892e-01, 9.964e-01, 9.900e-01, 9.940e-01,\n",
       "        9.956e-01, 9.956e-01, 9.908e-01, 9.968e-01, 9.900e-01, 9.908e-01,\n",
       "        9.900e-01, 9.904e-01, 9.896e-01, 9.900e-01, 9.904e-01, 9.904e-01,\n",
       "        9.900e-01, 9.900e-01, 9.900e-01, 9.900e-01, 9.896e-01, 9.896e-01,\n",
       "        9.892e-01, 9.888e-01, 9.888e-01, 9.888e-01, 9.884e-01, 9.884e-01,\n",
       "        9.888e-01, 9.888e-01, 9.888e-01, 9.888e-01, 9.888e-01, 9.888e-01,\n",
       "        9.980e-01, 9.972e-01, 9.956e-01, 9.936e-01, 9.972e-01, 9.980e-01,\n",
       "        9.968e-01, 9.964e-01, 9.948e-01, 9.968e-01, 9.960e-01, 9.916e-01,\n",
       "        9.928e-01, 9.920e-01, 9.928e-01, 9.916e-01, 9.968e-01, 9.980e-01,\n",
       "        9.952e-01, 9.912e-01, 9.904e-01, 9.960e-01, 9.960e-01, 9.956e-01,\n",
       "        9.964e-01, 9.916e-01, 9.912e-01, 9.900e-01, 9.964e-01, 9.900e-01,\n",
       "        9.908e-01, 9.964e-01, 9.900e-01, 9.904e-01, 9.904e-01, 9.900e-01,\n",
       "        9.900e-01, 9.896e-01, 9.884e-01, 9.900e-01, 9.900e-01, 9.900e-01,\n",
       "        9.892e-01, 9.900e-01, 9.900e-01, 9.900e-01, 9.900e-01, 9.900e-01,\n",
       "        7.928e-01, 1.888e-01, 9.568e-01, 9.572e-01, 6.732e-01, 9.412e-01,\n",
       "        9.520e-01, 5.832e-01, 9.488e-01, 9.420e-01, 5.848e-01, 9.436e-01,\n",
       "        9.616e-01, 5.744e-01, 8.600e-01, 9.532e-01, 5.028e-01, 9.520e-01,\n",
       "        9.464e-01, 5.848e-01, 9.384e-01, 9.400e-01, 5.832e-01, 9.440e-01,\n",
       "        7.916e-01, 2.280e-01, 9.504e-01, 9.492e-01, 5.508e-01, 9.204e-01,\n",
       "        9.460e-01, 5.840e-01, 9.384e-01, 9.464e-01, 5.848e-01, 9.424e-01,\n",
       "        7.940e-01, 5.848e-01, 7.796e-01, 9.440e-01, 1.892e-01, 9.592e-01,\n",
       "        9.532e-01, 8.000e-04, 9.476e-01, 9.408e-01, 5.848e-01, 9.388e-01,\n",
       "        9.456e-01, 5.460e-01, 9.380e-01, 9.412e-01, 6.460e-01, 9.420e-01,\n",
       "        9.412e-01, 6.648e-01, 9.444e-01, 9.384e-01, 6.348e-01, 9.416e-01,\n",
       "        9.404e-01, 2.236e-01, 9.352e-01, 9.372e-01, 6.940e-01, 9.428e-01,\n",
       "        9.388e-01, 5.208e-01, 9.400e-01, 9.400e-01, 4.876e-01, 9.400e-01,\n",
       "        9.408e-01, 5.160e-01, 9.428e-01, 9.428e-01, 7.960e-02, 9.396e-01,\n",
       "        9.408e-01, 7.108e-01, 9.428e-01, 9.432e-01, 5.088e-01, 9.420e-01,\n",
       "        9.416e-01, 5.472e-01, 9.396e-01, 9.428e-01, 2.760e-01, 9.416e-01,\n",
       "        9.420e-01, 2.536e-01, 9.396e-01, 9.388e-01, 3.000e-01, 9.416e-01,\n",
       "        9.520e-01, 9.508e-01, 9.484e-01, 9.556e-01, 9.556e-01, 9.556e-01,\n",
       "        9.560e-01, 9.568e-01, 9.580e-01, 9.596e-01, 9.556e-01, 9.580e-01,\n",
       "        9.492e-01, 9.492e-01, 9.512e-01, 9.556e-01, 9.548e-01, 9.544e-01,\n",
       "        9.564e-01, 9.568e-01, 9.568e-01, 9.572e-01, 9.600e-01, 9.584e-01,\n",
       "        9.488e-01, 9.488e-01, 9.504e-01, 9.540e-01, 9.520e-01, 9.536e-01,\n",
       "        9.560e-01, 9.560e-01, 9.552e-01, 9.580e-01, 9.572e-01, 9.596e-01,\n",
       "        9.468e-01, 9.444e-01, 9.476e-01, 9.532e-01, 9.524e-01, 9.508e-01,\n",
       "        9.524e-01, 9.540e-01, 9.520e-01, 9.528e-01, 9.540e-01, 9.536e-01,\n",
       "        9.724e-01, 9.692e-01, 9.616e-01, 9.768e-01, 9.792e-01, 9.772e-01,\n",
       "        9.812e-01, 9.804e-01, 9.800e-01, 9.800e-01, 9.800e-01, 9.816e-01,\n",
       "        9.616e-01, 9.760e-01, 9.720e-01, 9.792e-01, 9.788e-01, 9.772e-01,\n",
       "        9.780e-01, 9.800e-01, 9.792e-01, 9.808e-01, 9.796e-01, 9.792e-01,\n",
       "        9.708e-01, 9.596e-01, 9.732e-01, 9.780e-01, 9.744e-01, 9.788e-01,\n",
       "        9.808e-01, 9.788e-01, 9.804e-01, 9.816e-01, 9.804e-01, 9.800e-01,\n",
       "        9.588e-01, 9.580e-01, 9.728e-01, 9.744e-01, 9.776e-01, 9.768e-01,\n",
       "        9.772e-01, 9.776e-01, 9.776e-01, 9.800e-01, 9.788e-01, 9.776e-01]),\n",
       " 'mean_test_f1_micro': array([0.9966, 0.9948, 0.9924, 0.9904, 0.9954, 0.9956, 0.995 , 0.9922,\n",
       "        0.993 , 0.9942, 0.9886, 0.9926, 0.9954, 0.9914, 0.9914, 0.9958,\n",
       "        0.9926, 0.9944, 0.9938, 0.9948, 0.991 , 0.9946, 0.9904, 0.9932,\n",
       "        0.9926, 0.9904, 0.992 , 0.9908, 0.9912, 0.9916, 0.9906, 0.9904,\n",
       "        0.9906, 0.9904, 0.9902, 0.9902, 0.9906, 0.9894, 0.9898, 0.9898,\n",
       "        0.9894, 0.9892, 0.9894, 0.9894, 0.989 , 0.9896, 0.9894, 0.9896,\n",
       "        0.9966, 0.9966, 0.9954, 0.9944, 0.9956, 0.9946, 0.9952, 0.9954,\n",
       "        0.9944, 0.9942, 0.994 , 0.9928, 0.994 , 0.9928, 0.9938, 0.9908,\n",
       "        0.996 , 0.9964, 0.9936, 0.9928, 0.992 , 0.9928, 0.9948, 0.9928,\n",
       "        0.9956, 0.991 , 0.9926, 0.991 , 0.9932, 0.99  , 0.9902, 0.9934,\n",
       "        0.9898, 0.9902, 0.9902, 0.9898, 0.9908, 0.991 , 0.9896, 0.9902,\n",
       "        0.9906, 0.9902, 0.9898, 0.9904, 0.9902, 0.9904, 0.9902, 0.9902,\n",
       "        0.8756, 0.1888, 0.959 , 0.9578, 0.6292, 0.9512, 0.9526, 0.4044,\n",
       "        0.9524, 0.95  , 0.585 , 0.9512, 0.8714, 0.5532, 0.8268, 0.9552,\n",
       "        0.274 , 0.956 , 0.953 , 0.4302, 0.9454, 0.9464, 0.3994, 0.9504,\n",
       "        0.8504, 0.4066, 0.9588, 0.9558, 0.5506, 0.9394, 0.9508, 0.5826,\n",
       "        0.9486, 0.9514, 0.584 , 0.9488, 0.7956, 0.5842, 0.7788, 0.9542,\n",
       "        0.3838, 0.9596, 0.9528, 0.35  , 0.9526, 0.9472, 0.5838, 0.9444,\n",
       "        0.9482, 0.5542, 0.9452, 0.9476, 0.4606, 0.9484, 0.9484, 0.4794,\n",
       "        0.9476, 0.9466, 0.59  , 0.9488, 0.9462, 0.399 , 0.9432, 0.946 ,\n",
       "        0.5808, 0.9486, 0.947 , 0.4888, 0.948 , 0.9472, 0.4438, 0.9472,\n",
       "        0.947 , 0.4956, 0.9472, 0.9484, 0.2342, 0.9474, 0.9468, 0.6522,\n",
       "        0.9488, 0.9494, 0.3292, 0.9486, 0.9486, 0.409 , 0.945 , 0.948 ,\n",
       "        0.3972, 0.9482, 0.9488, 0.405 , 0.9458, 0.9444, 0.3066, 0.9488,\n",
       "        0.9556, 0.9574, 0.9546, 0.9598, 0.96  , 0.9598, 0.9616, 0.962 ,\n",
       "        0.963 , 0.9642, 0.9612, 0.963 , 0.9534, 0.9552, 0.956 , 0.9604,\n",
       "        0.9596, 0.9596, 0.9616, 0.962 , 0.962 , 0.9624, 0.9632, 0.9638,\n",
       "        0.9554, 0.9546, 0.9572, 0.959 , 0.9586, 0.9592, 0.9616, 0.962 ,\n",
       "        0.9614, 0.9628, 0.9622, 0.9638, 0.9536, 0.9522, 0.9538, 0.9586,\n",
       "        0.9582, 0.9554, 0.9582, 0.9594, 0.9582, 0.9592, 0.9592, 0.9588,\n",
       "        0.971 , 0.9768, 0.9668, 0.9826, 0.9824, 0.9816, 0.9838, 0.9842,\n",
       "        0.984 , 0.984 , 0.984 , 0.9848, 0.9722, 0.9794, 0.9774, 0.9826,\n",
       "        0.9822, 0.9818, 0.983 , 0.9838, 0.9842, 0.9842, 0.984 , 0.984 ,\n",
       "        0.9776, 0.9718, 0.9728, 0.982 , 0.9802, 0.9826, 0.9844, 0.9834,\n",
       "        0.9842, 0.9848, 0.9842, 0.9844, 0.9654, 0.9714, 0.9704, 0.9812,\n",
       "        0.9816, 0.981 , 0.9826, 0.982 , 0.9828, 0.9832, 0.9834, 0.9828]),\n",
       " 'std_test_f1_micro': array([1.000e-03, 0.000e+00, 3.200e-03, 2.400e-03, 1.400e-03, 8.000e-04,\n",
       "        2.000e-04, 1.800e-03, 1.400e-03, 2.000e-04, 2.000e-04, 2.200e-03,\n",
       "        1.000e-03, 2.200e-03, 2.200e-03, 6.000e-04, 2.600e-03, 4.000e-04,\n",
       "        1.800e-03, 8.000e-04, 2.000e-04, 2.200e-03, 4.000e-04, 2.400e-03,\n",
       "        2.600e-03, 0.000e+00, 2.400e-03, 8.000e-04, 8.000e-04, 1.200e-03,\n",
       "        6.000e-04, 4.000e-04, 6.000e-04, 4.000e-04, 6.000e-04, 6.000e-04,\n",
       "        1.400e-03, 6.000e-04, 1.000e-03, 1.000e-03, 1.000e-03, 8.000e-04,\n",
       "        6.000e-04, 6.000e-04, 2.000e-04, 8.000e-04, 6.000e-04, 8.000e-04,\n",
       "        1.400e-03, 6.000e-04, 2.000e-04, 8.000e-04, 1.600e-03, 3.400e-03,\n",
       "        1.600e-03, 1.000e-03, 4.000e-04, 2.600e-03, 2.000e-03, 1.200e-03,\n",
       "        1.200e-03, 8.000e-04, 1.000e-03, 8.000e-04, 8.000e-04, 1.600e-03,\n",
       "        1.600e-03, 1.600e-03, 1.600e-03, 3.200e-03, 1.200e-03, 2.800e-03,\n",
       "        8.000e-04, 6.000e-04, 1.400e-03, 1.000e-03, 3.200e-03, 0.000e+00,\n",
       "        6.000e-04, 3.000e-03, 2.000e-04, 2.000e-04, 2.000e-04, 2.000e-04,\n",
       "        8.000e-04, 1.400e-03, 1.200e-03, 2.000e-04, 6.000e-04, 2.000e-04,\n",
       "        6.000e-04, 4.000e-04, 2.000e-04, 4.000e-04, 2.000e-04, 2.000e-04,\n",
       "        8.280e-02, 0.000e+00, 2.200e-03, 6.000e-04, 4.400e-02, 1.000e-02,\n",
       "        6.000e-04, 1.788e-01, 3.600e-03, 8.000e-03, 2.000e-04, 7.600e-03,\n",
       "        9.020e-02, 2.120e-02, 3.320e-02, 2.000e-03, 2.288e-01, 4.000e-03,\n",
       "        6.600e-03, 1.546e-01, 7.000e-03, 6.400e-03, 1.838e-01, 6.400e-03,\n",
       "        5.880e-02, 1.786e-01, 8.400e-03, 6.600e-03, 2.000e-04, 1.900e-02,\n",
       "        4.800e-03, 1.400e-03, 1.020e-02, 5.000e-03, 8.000e-04, 6.400e-03,\n",
       "        1.600e-03, 6.000e-04, 8.000e-04, 1.020e-02, 1.946e-01, 4.000e-04,\n",
       "        4.000e-04, 3.492e-01, 5.000e-03, 6.400e-03, 1.000e-03, 5.600e-03,\n",
       "        2.600e-03, 8.200e-03, 7.200e-03, 6.400e-03, 1.854e-01, 6.400e-03,\n",
       "        7.200e-03, 1.854e-01, 3.200e-03, 8.200e-03, 4.480e-02, 7.200e-03,\n",
       "        5.800e-03, 1.754e-01, 8.000e-03, 8.800e-03, 1.132e-01, 5.800e-03,\n",
       "        8.200e-03, 3.200e-02, 8.000e-03, 7.200e-03, 4.380e-02, 7.200e-03,\n",
       "        6.200e-03, 2.040e-02, 4.400e-03, 5.600e-03, 1.546e-01, 7.800e-03,\n",
       "        6.000e-03, 5.860e-02, 6.000e-03, 6.200e-03, 1.796e-01, 6.600e-03,\n",
       "        7.000e-03, 1.382e-01, 5.400e-03, 5.200e-03, 1.212e-01, 6.600e-03,\n",
       "        6.800e-03, 1.514e-01, 6.200e-03, 5.600e-03, 6.600e-03, 7.200e-03,\n",
       "        3.600e-03, 6.600e-03, 6.200e-03, 4.200e-03, 4.400e-03, 4.200e-03,\n",
       "        5.600e-03, 5.200e-03, 5.000e-03, 4.600e-03, 5.600e-03, 5.000e-03,\n",
       "        4.200e-03, 6.000e-03, 4.800e-03, 4.800e-03, 4.800e-03, 5.200e-03,\n",
       "        5.200e-03, 5.200e-03, 5.200e-03, 5.200e-03, 3.200e-03, 5.400e-03,\n",
       "        6.600e-03, 5.800e-03, 6.800e-03, 5.000e-03, 6.600e-03, 5.600e-03,\n",
       "        5.600e-03, 6.000e-03, 6.200e-03, 4.800e-03, 5.000e-03, 4.200e-03,\n",
       "        6.800e-03, 7.800e-03, 6.200e-03, 5.400e-03, 5.800e-03, 4.600e-03,\n",
       "        5.800e-03, 5.400e-03, 6.200e-03, 6.400e-03, 5.200e-03, 5.200e-03,\n",
       "        1.400e-03, 7.600e-03, 5.200e-03, 5.800e-03, 3.200e-03, 4.400e-03,\n",
       "        2.600e-03, 3.800e-03, 4.000e-03, 4.000e-03, 4.000e-03, 3.200e-03,\n",
       "        1.060e-02, 3.400e-03, 5.400e-03, 3.400e-03, 3.400e-03, 4.600e-03,\n",
       "        5.000e-03, 3.800e-03, 5.000e-03, 3.400e-03, 4.400e-03, 4.800e-03,\n",
       "        6.800e-03, 1.220e-02, 4.000e-04, 4.000e-03, 5.800e-03, 3.800e-03,\n",
       "        3.600e-03, 4.600e-03, 3.800e-03, 3.200e-03, 3.800e-03, 4.400e-03,\n",
       "        6.600e-03, 1.340e-02, 2.400e-03, 6.800e-03, 4.000e-03, 4.200e-03,\n",
       "        5.400e-03, 4.400e-03, 5.200e-03, 3.200e-03, 4.600e-03, 5.200e-03]),\n",
       " 'rank_test_f1_micro': array([  1,  16,  44,  63,  10,   7,  15,  45,  34,  24,  96,  40,  12,\n",
       "         49,  49,   6,  40,  21,  28,  16,  52,  19,  63,  32,  40,  63,\n",
       "         46,  56,  51,  48,  59,  63,  59,  63,  70,  70,  59,  89,  81,\n",
       "         81,  93,  94,  89,  89,  95,  86,  89,  86,   2,   2,  10,  21,\n",
       "          7,  19,  14,  12,  21,  24,  26,  35,  26,  35,  28,  56,   5,\n",
       "          4,  30,  39,  46,  35,  16,  35,   7,  52,  40,  52,  33,  80,\n",
       "         70,  31,  81,  70,  70,  81,  56,  52,  86,  70,  59,  70,  81,\n",
       "         63,  70,  63,  70,  70, 251, 288, 174, 183, 258, 207, 202, 278,\n",
       "        204, 211, 260, 207, 252, 267, 254, 192, 286, 186, 200, 274, 244,\n",
       "        240, 279, 210, 253, 276, 176, 188, 268, 250, 209, 264, 218, 206,\n",
       "        262, 213, 255, 261, 256, 196, 282, 167, 201, 283, 202, 232, 263,\n",
       "        247, 225, 266, 245, 229, 272, 223, 222, 271, 229, 239, 259, 214,\n",
       "        241, 280, 249, 242, 265, 218, 236, 270, 227, 232, 273, 232, 236,\n",
       "        269, 232, 223, 287, 231, 238, 257, 214, 212, 284, 218, 218, 275,\n",
       "        246, 227, 281, 225, 214, 277, 243, 247, 285, 214, 189, 184, 194,\n",
       "        165, 164, 165, 158, 154, 149, 145, 162, 149, 199, 192, 186, 163,\n",
       "        167, 167, 158, 154, 154, 152, 148, 146, 190, 194, 185, 175, 179,\n",
       "        171, 158, 154, 161, 151, 153, 146, 198, 205, 197, 178, 180, 190,\n",
       "        180, 170, 180, 171, 171, 176, 141, 136, 143, 120, 123, 128, 111,\n",
       "        101, 106, 106, 106,  97, 138, 133, 135, 120, 124, 127, 116, 111,\n",
       "        101, 101, 106, 106, 134, 139, 137, 125, 132, 119,  99, 113, 101,\n",
       "         97, 101,  99, 144, 140, 142, 130, 128, 131, 120, 125, 117, 115,\n",
       "        113, 117]),\n",
       " 'split0_test_roc_auc_ovo': array([0.95096255, 0.93252006, 0.94135198, 0.91671404, 0.96444851,\n",
       "        0.86445462, 0.98745848, 0.93064113, 0.96492733, 0.90111755,\n",
       "        0.7477719 , 0.92293873, 0.84423694, 0.97309348, 0.96000719,\n",
       "        0.98244752, 0.99502044, 0.87489727, 0.94539781, 0.9154414 ,\n",
       "        0.91448065, 0.9320319 , 0.81051558, 0.87671875, 0.84649098,\n",
       "        0.90957128, 0.89043685, 0.87882563, 0.85943015, 0.8376471 ,\n",
       "        0.75819271, 0.83347965, 0.69980427, 0.76090897, 0.76883023,\n",
       "        0.7742074 , 0.86299819, 0.88668592, 0.84583526, 0.8521226 ,\n",
       "        0.8520147 , 0.82144387, 0.89630702, 0.79476755, 0.78755098,\n",
       "        0.81952531, 0.79057826, 0.80550264, 0.8590575 , 0.92711962,\n",
       "        0.87674558, 0.75292523, 0.89954962, 0.98663917, 0.87464757,\n",
       "        0.8067647 , 0.85443657, 0.79599482, 0.96354002, 0.90789782,\n",
       "        0.93949046, 0.91454799, 0.90593076, 0.93130577, 0.85954855,\n",
       "        0.91526312, 0.90872003, 0.88728349, 0.93255729, 0.85354425,\n",
       "        0.82516882, 0.80159958, 0.88731485, 0.77946991, 0.85466223,\n",
       "        0.93666852, 0.88041637, 0.89591039, 0.82599507, 0.77018642,\n",
       "        0.94021444, 0.78662339, 0.78230991, 0.80183932, 0.88650106,\n",
       "        0.89414127, 0.86298064, 0.73670205, 0.84085302, 0.75896924,\n",
       "        0.89667748, 0.77000551, 0.7485536 , 0.78921754, 0.71125623,\n",
       "        0.76897047, 0.86609343, 0.21200423, 0.73673761, 0.7261532 ,\n",
       "        0.58922181, 0.72561216, 0.72763075, 0.37320388, 0.73772674,\n",
       "        0.72914858, 0.6586441 , 0.7346775 , 0.72898272, 0.49351257,\n",
       "        0.71714681, 0.73392521, 0.6335321 , 0.71502625, 0.72828273,\n",
       "        0.57121267, 0.72893915, 0.70963464, 0.33314651, 0.7209602 ,\n",
       "        0.70172922, 0.54022097, 0.72599401, 0.72516117, 0.4214824 ,\n",
       "        0.73100433, 0.73470842, 0.21695464, 0.74096288, 0.73368721,\n",
       "        0.34772038, 0.70145666, 0.73806376, 0.557133  , 0.7413886 ,\n",
       "        0.73486833, 0.51690784, 0.73685526, 0.7371986 , 0.82542247,\n",
       "        0.73929053, 0.74238573, 0.47556898, 0.71276333, 0.74263672,\n",
       "        0.55065796, 0.75757097, 0.74575432, 0.47855928, 0.70863123,\n",
       "        0.76432959, 0.78070357, 0.72625185, 0.72701316, 0.53641056,\n",
       "        0.71085867, 0.73203002, 0.62512158, 0.71548605, 0.7233367 ,\n",
       "        0.36425549, 0.73357522, 0.75730119, 0.44302885, 0.73550849,\n",
       "        0.71753641, 0.60835   , 0.72109173, 0.74400447, 0.32425919,\n",
       "        0.76551395, 0.73591595, 0.41307018, 0.72525035, 0.71125334,\n",
       "        0.62034757, 0.77047858, 0.72557219, 0.3796798 , 0.68771996,\n",
       "        0.7200664 , 0.32703641, 0.75904599, 0.74396513, 0.85784546,\n",
       "        0.69595849, 0.72686003, 0.64433477, 0.70898976, 0.71035732,\n",
       "        0.66601518, 0.74604014, 0.70953199, 0.71694844, 0.72275756,\n",
       "        0.70074617, 0.73701169, 0.74566809, 0.72821384, 0.72388599,\n",
       "        0.73845805, 0.72741065, 0.67491474, 0.7326386 , 0.73375848,\n",
       "        0.68114872, 0.75060488, 0.68519602, 0.68218428, 0.71615498,\n",
       "        0.72786771, 0.74443435, 0.7331311 , 0.71371227, 0.68069329,\n",
       "        0.73311524, 0.72325599, 0.70652715, 0.71969886, 0.72661385,\n",
       "        0.7292465 , 0.74422207, 0.71295308, 0.7380026 , 0.72455396,\n",
       "        0.71766993, 0.70011245, 0.73495701, 0.66984444, 0.67185347,\n",
       "        0.74752267, 0.74144824, 0.70926088, 0.71997033, 0.7263068 ,\n",
       "        0.73095656, 0.74002716, 0.73074253, 0.70252547, 0.70322512,\n",
       "        0.7445242 , 0.87809225, 0.73511714, 0.88510565, 0.83201385,\n",
       "        0.75337737, 0.78483145, 0.90480673, 0.7733607 , 0.8774456 ,\n",
       "        0.82583289, 0.81672725, 0.77683757, 0.87587813, 0.7555724 ,\n",
       "        0.7940139 , 0.78755299, 0.83782812, 0.78282957, 0.74174311,\n",
       "        0.7536642 , 0.74677011, 0.78443535, 0.80400701, 0.7387322 ,\n",
       "        0.82652323, 0.77007332, 0.84492101, 0.74345012, 0.81932935,\n",
       "        0.82144677, 0.74357978, 0.76083375, 0.89662613, 0.82739243,\n",
       "        0.75257534, 0.75821912, 0.74969343, 0.73279837, 0.82482197,\n",
       "        0.80055634, 0.73702479, 0.74267576, 0.76682173, 0.87046592,\n",
       "        0.80116154, 0.82471367, 0.73813626]),\n",
       " 'split1_test_roc_auc_ovo': array([0.8809403 , 0.901867  , 0.94672168, 0.8641374 , 0.87262766,\n",
       "        0.83439948, 0.89298866, 0.90277088, 0.89933282, 0.94817032,\n",
       "        0.86248392, 0.8777816 , 0.9488502 , 0.90592894, 0.89738818,\n",
       "        0.89638138, 0.91397083, 0.88669431, 0.89635804, 0.96564287,\n",
       "        0.92217989, 0.93064232, 0.85264679, 0.864672  , 0.85307233,\n",
       "        0.87747013, 0.8445282 , 0.77097982, 0.92284441, 0.80975579,\n",
       "        0.80664138, 0.9135275 , 0.86209573, 0.83180163, 0.85821321,\n",
       "        0.87476241, 0.86275991, 0.84188523, 0.87491523, 0.79526551,\n",
       "        0.80071019, 0.86690819, 0.76445265, 0.86379821, 0.8257707 ,\n",
       "        0.83601922, 0.81093366, 0.82465742, 0.88862201, 0.90776903,\n",
       "        0.84890982, 0.78648835, 0.88867481, 0.92057924, 0.88788734,\n",
       "        0.90732334, 0.90982386, 0.87727968, 0.79482047, 0.83842986,\n",
       "        0.88895182, 0.84066376, 0.90264591, 0.8808676 , 0.88473345,\n",
       "        0.92104066, 0.82783781, 0.90995873, 0.87570509, 0.88190698,\n",
       "        0.89588629, 0.87038122, 0.92779745, 0.92048476, 0.86533239,\n",
       "        0.88074938, 0.9030636 , 0.86559692, 0.86160803, 0.80708067,\n",
       "        0.83549185, 0.90287241, 0.8551438 , 0.90633025, 0.87456563,\n",
       "        0.85868765, 0.90607067, 0.8126368 , 0.85062018, 0.86706867,\n",
       "        0.80285273, 0.76128001, 0.77226951, 0.85451123, 0.83929552,\n",
       "        0.8205525 , 0.84313869, 0.32925653, 0.84223874, 0.95006089,\n",
       "        0.63798689, 0.84184045, 0.87541473, 0.41027048, 0.83977177,\n",
       "        0.89134866, 0.68550295, 0.92881128, 0.82710892, 0.36075717,\n",
       "        0.83227437, 0.88119877, 0.57810148, 0.928545  , 0.93646692,\n",
       "        0.57335436, 0.84289929, 0.94320396, 0.74907147, 0.89319981,\n",
       "        0.82679828, 0.22068907, 0.90499566, 0.89661367, 0.45003506,\n",
       "        0.93591002, 0.88631618, 0.61362437, 0.88549135, 0.89859071,\n",
       "        0.41478028, 0.84424925, 0.88040427, 0.41410028, 0.8307188 ,\n",
       "        0.83264179, 0.42454988, 0.8966799 , 0.86558696, 0.61129978,\n",
       "        0.94570823, 0.89766697, 0.63453761, 0.8797464 , 0.82715415,\n",
       "        0.44220382, 0.81422765, 0.84516169, 0.65087967, 0.83608992,\n",
       "        0.84964833, 0.7190054 , 0.84491721, 0.8365707 , 0.56190293,\n",
       "        0.83345274, 0.8348459 , 0.40570668, 0.86025019, 0.83889396,\n",
       "        0.6954341 , 0.83867088, 0.84006225, 0.83221182, 0.82957818,\n",
       "        0.86946471, 0.45662066, 0.8375622 , 0.84551443, 0.55497321,\n",
       "        0.81471181, 0.84865871, 0.66821558, 0.82219628, 0.83904473,\n",
       "        0.8350281 , 0.84597573, 0.84382041, 0.43077038, 0.84348939,\n",
       "        0.86464584, 0.52151446, 0.84833597, 0.83692901, 0.55388203,\n",
       "        0.85125034, 0.84625621, 0.72121178, 0.84540277, 0.8432099 ,\n",
       "        0.42213333, 0.84135933, 0.83788073, 0.84527073, 0.83767397,\n",
       "        0.83916414, 0.82647388, 0.84570709, 0.83387489, 0.82993469,\n",
       "        0.83727925, 0.84164009, 0.83701483, 0.84297076, 0.79884988,\n",
       "        0.84239002, 0.82958444, 0.83878314, 0.84130139, 0.84279369,\n",
       "        0.83471572, 0.83025123, 0.83740508, 0.83931184, 0.8435181 ,\n",
       "        0.83248063, 0.82338565, 0.82631088, 0.84360757, 0.82919202,\n",
       "        0.83441608, 0.8208564 , 0.8364524 , 0.84286571, 0.84135059,\n",
       "        0.84201922, 0.83773265, 0.83962583, 0.83991102, 0.84438048,\n",
       "        0.84089722, 0.83670025, 0.82820555, 0.83923059, 0.83260646,\n",
       "        0.84015238, 0.83320625, 0.83869114, 0.8370357 , 0.8392341 ,\n",
       "        0.82645784, 0.78741053, 0.83742156, 0.82240355, 0.83333169,\n",
       "        0.84800376, 0.84669716, 0.77836799, 0.7892849 , 0.79118402,\n",
       "        0.86745962, 0.76888827, 0.87335346, 0.84559228, 0.84477944,\n",
       "        0.83129024, 0.77471137, 0.78769904, 0.82618101, 0.78690536,\n",
       "        0.84802216, 0.86219445, 0.83549409, 0.80102775, 0.84597746,\n",
       "        0.84095285, 0.84016102, 0.85075868, 0.81641625, 0.80200068,\n",
       "        0.84342813, 0.78378492, 0.83435522, 0.8879206 , 0.82472015,\n",
       "        0.79910659, 0.85445269, 0.84067661, 0.84669071, 0.84914059,\n",
       "        0.8515317 , 0.8510953 , 0.80666763, 0.84898059, 0.80866345,\n",
       "        0.8815843 , 0.81489129, 0.79834998]),\n",
       " 'mean_test_roc_auc_ovo': array([0.91595142, 0.91719353, 0.94403683, 0.89042572, 0.91853808,\n",
       "        0.84942705, 0.94022357, 0.916706  , 0.93213007, 0.92464394,\n",
       "        0.80512791, 0.90036016, 0.89654357, 0.93951121, 0.92869769,\n",
       "        0.93941445, 0.95449564, 0.88079579, 0.92087792, 0.94054214,\n",
       "        0.91833027, 0.93133711, 0.83158119, 0.87069538, 0.84978166,\n",
       "        0.8935207 , 0.86748253, 0.82490272, 0.89113728, 0.82370144,\n",
       "        0.78241704, 0.87350357, 0.78095   , 0.7963553 , 0.81352172,\n",
       "        0.8244849 , 0.86287905, 0.86428557, 0.86037525, 0.82369406,\n",
       "        0.82636244, 0.84417603, 0.83037984, 0.82928288, 0.80666084,\n",
       "        0.82777227, 0.80075596, 0.81508003, 0.87383976, 0.91744432,\n",
       "        0.8628277 , 0.76970679, 0.89411221, 0.95360921, 0.88126745,\n",
       "        0.85704402, 0.88213021, 0.83663725, 0.87918025, 0.87316384,\n",
       "        0.91422114, 0.87760587, 0.90428833, 0.90608668, 0.872141  ,\n",
       "        0.91815189, 0.86827892, 0.89862111, 0.90413119, 0.86772561,\n",
       "        0.86052755, 0.8359904 , 0.90755615, 0.84997734, 0.85999731,\n",
       "        0.90870895, 0.89173999, 0.88075366, 0.84380155, 0.78863355,\n",
       "        0.88785315, 0.8447479 , 0.81872685, 0.85408478, 0.88053335,\n",
       "        0.87641446, 0.88452566, 0.77466943, 0.8457366 , 0.81301895,\n",
       "        0.84976511, 0.76564276, 0.76041156, 0.82186438, 0.77527587,\n",
       "        0.79476149, 0.85461606, 0.27063038, 0.78948818, 0.83810705,\n",
       "        0.61360435, 0.78372631, 0.80152274, 0.39173718, 0.78874926,\n",
       "        0.81024862, 0.67207353, 0.83174439, 0.77804582, 0.42713487,\n",
       "        0.77471059, 0.80756199, 0.60581679, 0.82178562, 0.83237483,\n",
       "        0.57228352, 0.78591922, 0.8264193 , 0.54110899, 0.80708001,\n",
       "        0.76426375, 0.38045502, 0.81549484, 0.81088742, 0.43575873,\n",
       "        0.83345717, 0.8105123 , 0.4152895 , 0.81322712, 0.81613896,\n",
       "        0.38125033, 0.77285296, 0.80923401, 0.48561664, 0.7860537 ,\n",
       "        0.78375506, 0.47072886, 0.81676758, 0.80139278, 0.71836113,\n",
       "        0.84249938, 0.82002635, 0.5550533 , 0.79625486, 0.78489544,\n",
       "        0.49643089, 0.78589931, 0.795458  , 0.56471948, 0.77236058,\n",
       "        0.80698896, 0.74985448, 0.78558453, 0.78179193, 0.54915675,\n",
       "        0.77215571, 0.78343796, 0.51541413, 0.78786812, 0.78111533,\n",
       "        0.5298448 , 0.78612305, 0.79868172, 0.63762034, 0.78254334,\n",
       "        0.79350056, 0.53248533, 0.77932697, 0.79475945, 0.4396162 ,\n",
       "        0.79011288, 0.79228733, 0.54064288, 0.77372331, 0.77514903,\n",
       "        0.72768784, 0.80822715, 0.7846963 , 0.40522509, 0.76560468,\n",
       "        0.79235612, 0.42427544, 0.80369098, 0.79044707, 0.70586374,\n",
       "        0.77360442, 0.78655812, 0.68277327, 0.77719627, 0.77678361,\n",
       "        0.54407425, 0.79369973, 0.77370636, 0.78110958, 0.78021576,\n",
       "        0.76995516, 0.78174278, 0.79568759, 0.78104437, 0.77691034,\n",
       "        0.78786865, 0.78452537, 0.75596478, 0.78780468, 0.76630418,\n",
       "        0.76176937, 0.79009466, 0.76198958, 0.76174283, 0.77947434,\n",
       "        0.78129171, 0.78734279, 0.78526809, 0.77651205, 0.7621057 ,\n",
       "        0.78279794, 0.77332082, 0.76641901, 0.78165321, 0.77790294,\n",
       "        0.78183129, 0.78253924, 0.77470274, 0.79043416, 0.78295228,\n",
       "        0.77984457, 0.76892255, 0.78729142, 0.75487773, 0.75811698,\n",
       "        0.79420994, 0.78907425, 0.76873321, 0.77960046, 0.77945663,\n",
       "        0.78555447, 0.78661671, 0.78471683, 0.76978058, 0.77122961,\n",
       "        0.78549102, 0.83275139, 0.78626935, 0.8537546 , 0.83267277,\n",
       "        0.80069057, 0.81576431, 0.84158736, 0.7813228 , 0.83431481,\n",
       "        0.84664626, 0.79280776, 0.82509552, 0.8607352 , 0.80017592,\n",
       "        0.81265207, 0.78113218, 0.81276358, 0.80450529, 0.76432423,\n",
       "        0.80084318, 0.80448228, 0.80996472, 0.80251738, 0.79235483,\n",
       "        0.83373804, 0.80511717, 0.84783985, 0.77993319, 0.81066502,\n",
       "        0.83243745, 0.76368235, 0.79759449, 0.89227336, 0.82605629,\n",
       "        0.77584097, 0.80633591, 0.79518502, 0.78974454, 0.83698128,\n",
       "        0.82604402, 0.79406004, 0.7746717 , 0.80790116, 0.83956469,\n",
       "        0.84137292, 0.81980248, 0.76824312]),\n",
       " 'std_test_roc_auc_ovo': array([3.50111207e-02, 1.53265278e-02, 2.68485062e-03, 2.62883178e-02,\n",
       "        4.59104260e-02, 1.50275704e-02, 4.72349117e-02, 1.39351236e-02,\n",
       "        3.27972532e-02, 2.35263856e-02, 5.73560147e-02, 2.25785639e-02,\n",
       "        5.23066327e-02, 3.35822678e-02, 3.13095099e-02, 4.30330680e-02,\n",
       "        4.05248020e-02, 5.89852009e-03, 2.45198849e-02, 2.51007312e-02,\n",
       "        3.84961968e-03, 6.94790052e-04, 2.10656072e-02, 6.02337496e-03,\n",
       "        3.29067557e-03, 1.60505722e-02, 2.29543251e-02, 5.39229051e-02,\n",
       "        3.17071281e-02, 1.39456534e-02, 2.42243350e-02, 4.00239273e-02,\n",
       "        8.11457312e-02, 3.54463316e-02, 4.46914868e-02, 5.02775073e-02,\n",
       "        1.19139784e-04, 2.24003458e-02, 1.45399870e-02, 2.84285449e-02,\n",
       "        2.56522548e-02, 2.27321612e-02, 6.59271837e-02, 3.45153301e-02,\n",
       "        1.91098599e-02, 8.24695476e-03, 1.01777013e-02, 9.57738912e-03,\n",
       "        1.47822549e-02, 9.67529982e-03, 1.39178792e-02, 1.67815599e-02,\n",
       "        5.43740170e-03, 3.30299651e-02, 6.61988121e-03, 5.02793188e-02,\n",
       "        2.76936473e-02, 4.06424280e-02, 8.43597754e-02, 3.47339808e-02,\n",
       "        2.52693235e-02, 3.69421146e-02, 1.64242546e-03, 2.52190860e-02,\n",
       "        1.25924519e-02, 2.88877345e-03, 4.04411065e-02, 1.13376197e-02,\n",
       "        2.84261021e-02, 1.41813658e-02, 3.53587335e-02, 3.43908171e-02,\n",
       "        2.02412967e-02, 7.05074262e-02, 5.33507590e-03, 2.79595664e-02,\n",
       "        1.13236196e-02, 1.51567380e-02, 1.78064825e-02, 1.84471229e-02,\n",
       "        5.23612962e-02, 5.81245090e-02, 3.64169460e-02, 5.22454608e-02,\n",
       "        5.96771401e-03, 1.77268066e-02, 2.15450188e-02, 3.79673739e-02,\n",
       "        4.88358087e-03, 5.40497152e-02, 4.69123770e-02, 4.36274797e-03,\n",
       "        1.18579549e-02, 3.26468433e-02, 6.40196488e-02, 2.57910158e-02,\n",
       "        1.14773746e-02, 5.86261534e-02, 5.27505655e-02, 1.11953847e-01,\n",
       "        2.43825400e-02, 5.81141429e-02, 7.38919892e-02, 1.85333011e-02,\n",
       "        5.10225143e-02, 8.11000439e-02, 1.34294263e-02, 9.70668902e-02,\n",
       "        4.90630997e-02, 6.63777024e-02, 5.75637822e-02, 7.36367795e-02,\n",
       "        2.77153099e-02, 1.06759374e-01, 1.04092094e-01, 1.07084435e-03,\n",
       "        5.69800718e-02, 1.16784657e-01, 2.07962480e-01, 8.61198026e-02,\n",
       "        6.25345330e-02, 1.59765951e-01, 8.95008261e-02, 8.57262490e-02,\n",
       "        1.42763311e-02, 1.02452843e-01, 7.58038792e-02, 1.98334865e-01,\n",
       "        7.22642346e-02, 8.24517502e-02, 3.35299488e-02, 7.13962972e-02,\n",
       "        7.11702539e-02, 7.15163604e-02, 4.46651023e-02, 4.88867339e-02,\n",
       "        4.61789809e-02, 7.99123206e-02, 6.41941798e-02, 1.07061345e-01,\n",
       "        1.03208848e-01, 7.76406177e-02, 7.94843115e-02, 8.34915330e-02,\n",
       "        4.22587144e-02, 5.42270677e-02, 2.83283391e-02, 4.97036828e-02,\n",
       "        8.61601953e-02, 6.37293408e-02, 4.26593692e-02, 3.08490874e-02,\n",
       "        5.93326803e-02, 5.47787704e-02, 1.27461877e-02, 6.12970329e-02,\n",
       "        5.14079393e-02, 1.09707451e-01, 7.23820693e-02, 5.77786276e-02,\n",
       "        1.65589304e-01, 5.25478288e-02, 4.13805322e-02, 1.94591485e-01,\n",
       "        4.70348474e-02, 7.59641527e-02, 7.58646713e-02, 5.82352367e-02,\n",
       "        5.07549819e-02, 1.15357010e-01, 2.45989293e-02, 5.63713798e-02,\n",
       "        1.27572700e-01, 4.84729669e-02, 6.38956965e-02, 1.07340265e-01,\n",
       "        3.77485766e-02, 5.91241099e-02, 2.55452927e-02, 7.78847149e-02,\n",
       "        7.22897213e-02, 9.72390255e-02, 4.46449877e-02, 4.64819441e-02,\n",
       "        1.51981715e-01, 7.76459287e-02, 5.96980912e-02, 3.84385067e-02,\n",
       "        6.82065036e-02, 6.64262908e-02, 1.21940921e-01, 4.76595956e-02,\n",
       "        6.41743674e-02, 6.41611424e-02, 5.74582049e-02, 6.92089835e-02,\n",
       "        4.47310954e-02, 5.00195014e-02, 5.28305251e-02, 5.30243517e-02,\n",
       "        4.94106035e-02, 5.71147207e-02, 8.10500434e-02, 5.51660769e-02,\n",
       "        3.25456973e-02, 8.06206485e-02, 3.94897819e-02, 7.67935631e-02,\n",
       "        7.95585548e-02, 6.33193593e-02, 5.34240057e-02, 4.29084385e-02,\n",
       "        5.21369880e-02, 6.27997860e-02, 8.14124041e-02, 4.96826913e-02,\n",
       "        5.00648313e-02, 5.98918616e-02, 6.19543571e-02, 5.12890850e-02,\n",
       "        5.25847855e-02, 3.83171647e-02, 6.17496575e-02, 5.24315550e-02,\n",
       "        5.83983137e-02, 6.21746414e-02, 6.88100991e-02, 5.23344126e-02,\n",
       "        8.50332889e-02, 8.62635067e-02, 4.66872750e-02, 4.76260049e-02,\n",
       "        5.94723340e-02, 5.96301263e-02, 5.31498322e-02, 5.45979127e-02,\n",
       "        4.65895440e-02, 5.39743044e-02, 6.72551176e-02, 6.80044895e-02,\n",
       "        4.09668198e-02, 4.53408624e-02, 5.11522094e-02, 3.13510495e-02,\n",
       "        6.58920720e-04, 4.73131954e-02, 3.09328542e-02, 6.32193701e-02,\n",
       "        7.96209871e-03, 4.31307910e-02, 2.08133650e-02, 2.39194866e-02,\n",
       "        4.82579456e-02, 1.51429252e-02, 4.46035155e-02, 1.86381705e-02,\n",
       "        6.42081071e-03, 2.50645447e-02, 2.16757209e-02, 2.25811266e-02,\n",
       "        4.71789796e-02, 5.77121690e-02, 2.55293712e-02, 1.48962898e-03,\n",
       "        5.36226279e-02, 7.21480796e-03, 3.50438455e-02, 2.91883129e-03,\n",
       "        3.64830676e-02, 8.66433587e-03, 1.09906795e-02, 2.01025705e-02,\n",
       "        3.67607353e-02, 4.35276574e-03, 1.33613699e-03, 2.32656243e-02,\n",
       "        4.81167885e-02, 4.54915895e-02, 5.69461688e-02, 1.21593115e-02,\n",
       "        2.54876793e-02, 5.70352558e-02, 3.19959341e-02, 4.10794310e-02,\n",
       "        3.09012362e-02, 4.02113774e-02, 4.91119172e-03, 3.01068608e-02]),\n",
       " 'rank_test_roc_auc_ovo': array([ 19,  17,   3,  34,  13,  67,   5,  18,   8,  11, 131,  26,  28,\n",
       "          6,  10,   7,   1,  39,  12,   4,  14,   9,  90,  49,  65,  30,\n",
       "         52,  99,  33, 101, 196,  46, 207, 145, 113, 100,  54,  53,  58,\n",
       "        102,  95,  72,  91,  92, 129,  93, 140, 112,  45,  16,  55, 238,\n",
       "         29,   2,  38,  60,  37,  80,  42,  47,  20,  43,  24,  23,  48,\n",
       "         15,  50,  27,  25,  51,  57,  81,  22,  64,  59,  21,  32,  40,\n",
       "         73, 168,  35,  71, 107,  62,  41,  44,  36, 227,  70, 115,  66,\n",
       "        244, 253, 103, 222, 150,  61, 288, 165,  78, 264, 190, 137, 285,\n",
       "        167, 121, 262,  89, 215, 281, 224, 126, 265, 104,  88, 266, 179,\n",
       "         94, 271, 127, 247, 287, 111, 118, 280,  84, 120, 283, 114, 109,\n",
       "        286, 232, 123, 277, 178, 189, 278, 108, 138, 259,  74, 105, 268,\n",
       "        146, 185, 276, 180, 148, 267, 233, 128, 257, 181, 198, 269, 234,\n",
       "        191, 275, 170, 204, 274, 177, 143, 263, 194, 155, 273, 214, 151,\n",
       "        279, 162, 159, 272, 228, 223, 258, 124, 187, 284, 245, 157, 282,\n",
       "        135, 160, 260, 230, 175, 261, 217, 219, 270, 154, 229, 205, 208,\n",
       "        236, 199, 147, 206, 218, 169, 188, 255, 171, 243, 251, 163, 250,\n",
       "        252, 212, 202, 172, 184, 220, 249, 193, 231, 242, 200, 216, 197,\n",
       "        195, 225, 161, 192, 210, 239, 173, 256, 254, 152, 166, 240, 211,\n",
       "        213, 182, 174, 186, 237, 235, 183,  85, 176,  63,  86, 141, 110,\n",
       "         75, 201,  82,  69, 156,  98,  56, 142, 117, 203, 116, 133, 246,\n",
       "        139, 134, 122, 136, 158,  83, 132,  68, 209, 119,  87, 248, 144,\n",
       "         31,  96, 221, 130, 149, 164,  79,  97, 153, 226, 125,  77,  76,\n",
       "        106, 241]),\n",
       " 'split0_test_neg_log_loss': array([-0.02170421, -0.02287895, -0.03036513, -0.09338286, -0.05353557,\n",
       "        -0.03758335, -0.0198375 , -0.03501877, -0.04566294, -0.05813851,\n",
       "        -0.07054134, -0.03216179, -0.04921906, -0.02800668, -0.03257074,\n",
       "        -0.02434047, -0.0237561 , -0.03098713, -0.03228737, -0.02192827,\n",
       "        -0.05335904, -0.06940988, -0.06350528, -0.01581578, -0.02504031,\n",
       "        -0.03631584, -0.02006431, -0.03641072, -0.03234263, -0.02301241,\n",
       "        -0.03779093, -0.03577359, -0.03760567, -0.03758394, -0.03925045,\n",
       "        -0.04340417, -0.03887698, -0.04030639, -0.04011343, -0.03990165,\n",
       "        -0.04061524, -0.04174629, -0.0405349 , -0.0436714 , -0.04265584,\n",
       "        -0.04194914, -0.04400199, -0.04226602, -0.04271721, -0.01608903,\n",
       "        -0.04080119, -0.03086721, -0.05383346, -0.0474172 , -0.06134399,\n",
       "        -0.03342954, -0.05652776, -0.08250853, -0.10322832, -0.0452935 ,\n",
       "        -0.02101646, -0.02788998, -0.02210531, -0.05968229, -0.04560089,\n",
       "        -0.02368665, -0.04633068, -0.05109855, -0.05704464, -0.05049329,\n",
       "        -0.0357838 , -0.07601335, -0.02408837, -0.04517433, -0.04566061,\n",
       "        -0.02455949, -0.04576592, -0.0424009 , -0.0426399 , -0.04030115,\n",
       "        -0.03905504, -0.05862446, -0.04739085, -0.05998495, -0.03508989,\n",
       "        -0.03557387, -0.03797812, -0.04354242, -0.0397895 , -0.04220398,\n",
       "        -0.0400429 , -0.04103597, -0.0445381 , -0.04055409, -0.04457579,\n",
       "        -0.04112413, -0.39430644, -1.58604694, -0.4101807 , -0.36607493,\n",
       "        -1.20236732, -0.39735379, -0.36058848, -1.32527065, -0.36752587,\n",
       "        -0.3518523 , -1.13358133, -0.34974778, -0.42687546, -1.14530553,\n",
       "        -0.4897421 , -0.37604697, -1.53695926, -0.39210945, -0.37503097,\n",
       "        -1.34254395, -0.35418907, -0.34422426, -1.25491939, -0.35631341,\n",
       "        -0.43073275, -1.18761092, -0.41583828, -0.3947734 , -1.38478411,\n",
       "        -0.38092506, -0.34441858, -1.31528811, -0.3634027 , -0.34774679,\n",
       "        -1.31707236, -0.34502738, -0.49846798, -1.32898294, -0.48752248,\n",
       "        -0.41549167, -1.252073  , -0.40983081, -0.35799195, -1.29959709,\n",
       "        -0.37316826, -0.36100514, -1.1451863 , -0.33493743, -0.17276587,\n",
       "        -1.17281991, -0.17948898, -0.1628049 , -1.58136875, -0.16112651,\n",
       "        -0.15579105, -1.35908252, -0.16995632, -0.16311403, -1.15799932,\n",
       "        -0.16004616, -0.19128126, -1.18056228, -0.17999478, -0.16330398,\n",
       "        -1.30984575, -0.1698725 , -0.16451753, -1.27089497, -0.15766474,\n",
       "        -0.15857427, -1.20486271, -0.16414467, -0.17216636, -1.33869944,\n",
       "        -0.17523384, -0.16600263, -1.20024115, -0.1665036 , -0.16438961,\n",
       "        -1.08556511, -0.16065717, -0.16097474, -1.26565211, -0.16239988,\n",
       "        -0.17076137, -1.39646341, -0.18126826, -0.17478912, -1.15843268,\n",
       "        -0.16734703, -0.16325256, -1.09527503, -0.16942926, -0.17096065,\n",
       "        -1.41460614, -0.15766374, -0.15919864, -0.14719646, -0.16179805,\n",
       "        -0.13106754, -0.1240786 , -0.13302684, -0.11611439, -0.11558056,\n",
       "        -0.10974212, -0.10467788, -0.11395983, -0.10697743, -0.18159328,\n",
       "        -0.15542396, -0.1593032 , -0.12295798, -0.12979721, -0.12498313,\n",
       "        -0.11641973, -0.11469035, -0.11343245, -0.10961229, -0.11551663,\n",
       "        -0.10364601, -0.14903824, -0.15105616, -0.14280608, -0.12775377,\n",
       "        -0.1223585 , -0.12527043, -0.11497462, -0.10791112, -0.11248887,\n",
       "        -0.11123674, -0.11286214, -0.10321567, -0.16641741, -0.16662039,\n",
       "        -0.16197513, -0.12988866, -0.13616609, -0.14953583, -0.13253561,\n",
       "        -0.12727701, -0.13135784, -0.11596281, -0.12526613, -0.13031261,\n",
       "        -0.10276872, -0.08771873, -0.09131339, -0.06613299, -0.07053281,\n",
       "        -0.07125473, -0.06597888, -0.06750037, -0.06444625, -0.06261504,\n",
       "        -0.06072898, -0.05759324, -0.08934191, -0.08942668, -0.09009814,\n",
       "        -0.07427348, -0.07897868, -0.0692937 , -0.06006898, -0.06423942,\n",
       "        -0.05908246, -0.05908898, -0.05546182, -0.05695626, -0.08568058,\n",
       "        -0.08517992, -0.09302561, -0.07388288, -0.069287  , -0.07150849,\n",
       "        -0.06134101, -0.06426508, -0.06552006, -0.06390796, -0.06286677,\n",
       "        -0.06042996, -0.09172268, -0.08750191, -0.11192218, -0.07074074,\n",
       "        -0.08079921, -0.0801514 , -0.06669503, -0.07168614, -0.06980095,\n",
       "        -0.06493929, -0.06542439, -0.06622585]),\n",
       " 'split1_test_neg_log_loss': array([-0.0385234 , -0.03414815, -0.05407797, -0.08374629, -0.06833662,\n",
       "        -0.03189041, -0.07196292, -0.07613706, -0.05790145, -0.0440882 ,\n",
       "        -0.07780633, -0.0544584 , -0.02852118, -0.05368564, -0.07717584,\n",
       "        -0.05145863, -0.07205651, -0.05300944, -0.05501653, -0.05210569,\n",
       "        -0.05829385, -0.04137355, -0.07351432, -0.05604752, -0.04717367,\n",
       "        -0.04706703, -0.0548044 , -0.04488152, -0.04824598, -0.04585962,\n",
       "        -0.05446281, -0.0548437 , -0.05192805, -0.05736803, -0.05645256,\n",
       "        -0.0556957 , -0.05132765, -0.05020636, -0.04874397, -0.05384881,\n",
       "        -0.05405533, -0.05252561, -0.05511517, -0.05549682, -0.05369686,\n",
       "        -0.05689904, -0.05756805, -0.05550788, -0.03340061, -0.04471363,\n",
       "        -0.05294602, -0.06746042, -0.06705883, -0.03774206, -0.05149469,\n",
       "        -0.06893968, -0.08827166, -0.05271235, -0.06864019, -0.09097132,\n",
       "        -0.04934441, -0.07724802, -0.0493949 , -0.07091481, -0.04564979,\n",
       "        -0.031687  , -0.07579571, -0.05584268, -0.08696164, -0.04659843,\n",
       "        -0.05512564, -0.06521488, -0.03762615, -0.04301865, -0.05474607,\n",
       "        -0.05612456, -0.0397946 , -0.05358941, -0.05653911, -0.05174379,\n",
       "        -0.05880631, -0.05640514, -0.05497556, -0.05118119, -0.04604947,\n",
       "        -0.05148457, -0.05155864, -0.05102433, -0.04973167, -0.05365917,\n",
       "        -0.05665945, -0.05631696, -0.05607846, -0.05580239, -0.05568508,\n",
       "        -0.05350552, -0.47134856, -1.49224312, -0.41981135, -0.39710771,\n",
       "        -1.24196273, -0.36746716, -0.38064031, -1.29067151, -0.36319003,\n",
       "        -0.3552989 , -1.26282075, -0.36049645, -0.42356908, -1.38672722,\n",
       "        -0.46284788, -0.39696139, -1.14213   , -0.43281646, -0.3699497 ,\n",
       "        -1.16676945, -0.34231847, -0.35531245, -1.24624662, -0.35445707,\n",
       "        -0.49037913, -1.51806912, -0.43948863, -0.38017969, -1.35725774,\n",
       "        -0.4176236 , -0.36357276, -1.28827212, -0.34701274, -0.37504548,\n",
       "        -1.2118392 , -0.34636253, -0.48019667, -1.12555156, -0.4551554 ,\n",
       "        -0.38557371, -1.27563149, -0.40834929, -0.37192336, -1.47489517,\n",
       "        -0.38541397, -0.34431873, -1.23923119, -0.34983776, -0.1845625 ,\n",
       "        -1.32166199, -0.19865672, -0.18279708, -1.02724022, -0.18434137,\n",
       "        -0.19344547, -1.13053804, -0.17955172, -0.18667184, -1.0706454 ,\n",
       "        -0.18468699, -0.19885604, -1.66865071, -0.21049723, -0.19459201,\n",
       "        -1.08641546, -0.18301526, -0.19669312, -1.20385194, -0.18928277,\n",
       "        -0.18367381, -1.31973335, -0.18680783, -0.19762168, -1.2399071 ,\n",
       "        -0.20521056, -0.18591099, -1.57502151, -0.19321932, -0.18399502,\n",
       "        -0.93755386, -0.18366499, -0.18203181, -1.13653343, -0.18203165,\n",
       "        -0.2013306 , -1.23739593, -0.19858517, -0.18850764, -1.7718374 ,\n",
       "        -0.18804104, -0.18569292, -1.30596964, -0.18780347, -0.18811956,\n",
       "        -1.4259112 , -0.18294404, -0.17241858, -0.16864101, -0.17843139,\n",
       "        -0.14803754, -0.14085923, -0.14892764, -0.13791781, -0.14153639,\n",
       "        -0.13248653, -0.12300791, -0.13350708, -0.12743226, -0.17856782,\n",
       "        -0.1708566 , -0.17280878, -0.14685191, -0.14468732, -0.14690743,\n",
       "        -0.13662775, -0.1446298 , -0.13449397, -0.13194924, -0.12669349,\n",
       "        -0.12742845, -0.1835551 , -0.18290628, -0.17326445, -0.14919045,\n",
       "        -0.1535035 , -0.15166763, -0.14048211, -0.13477773, -0.13714952,\n",
       "        -0.1276984 , -0.13197711, -0.12653364, -0.18547308, -0.19083309,\n",
       "        -0.18849914, -0.15667819, -0.1583784 , -0.16304292, -0.15192949,\n",
       "        -0.14713213, -0.15628153, -0.1516074 , -0.14561751, -0.14539994,\n",
       "        -0.11275998, -0.12215685, -0.11438225, -0.09672589, -0.09532363,\n",
       "        -0.09762454, -0.07740103, -0.08320074, -0.08611076, -0.07934863,\n",
       "        -0.07815397, -0.0758086 , -0.10828585, -0.10773693, -0.11909658,\n",
       "        -0.08429114, -0.08715183, -0.09322832, -0.08331654, -0.08501227,\n",
       "        -0.08588781, -0.07832264, -0.08205018, -0.08642297, -0.11688369,\n",
       "        -0.11973235, -0.10835722, -0.08647144, -0.10561233, -0.09267375,\n",
       "        -0.0826284 , -0.08429865, -0.0848322 , -0.07381312, -0.07972189,\n",
       "        -0.08044912, -0.11949902, -0.12394147, -0.10776392, -0.10370618,\n",
       "        -0.09803041, -0.09596106, -0.09251385, -0.09660148, -0.09828696,\n",
       "        -0.08306398, -0.08477322, -0.09055497]),\n",
       " 'mean_test_neg_log_loss': array([-0.0301138 , -0.02851355, -0.04222155, -0.08856457, -0.0609361 ,\n",
       "        -0.03473688, -0.04590021, -0.05557792, -0.05178219, -0.05111336,\n",
       "        -0.07417384, -0.04331009, -0.03887012, -0.04084616, -0.05487329,\n",
       "        -0.03789955, -0.0479063 , -0.04199828, -0.04365195, -0.03701698,\n",
       "        -0.05582645, -0.05539172, -0.0685098 , -0.03593165, -0.03610699,\n",
       "        -0.04169144, -0.03743435, -0.04064612, -0.0402943 , -0.03443601,\n",
       "        -0.04612687, -0.04530865, -0.04476686, -0.04747598, -0.04785151,\n",
       "        -0.04954994, -0.04510232, -0.04525638, -0.0444287 , -0.04687523,\n",
       "        -0.04733528, -0.04713595, -0.04782504, -0.04958411, -0.04817635,\n",
       "        -0.04942409, -0.05078502, -0.04888695, -0.03805891, -0.03040133,\n",
       "        -0.0468736 , -0.04916381, -0.06044615, -0.04257963, -0.05641934,\n",
       "        -0.05118461, -0.07239971, -0.06761044, -0.08593425, -0.06813241,\n",
       "        -0.03518043, -0.052569  , -0.03575011, -0.06529855, -0.04562534,\n",
       "        -0.02768682, -0.0610632 , -0.05347062, -0.07200314, -0.04854586,\n",
       "        -0.04545472, -0.07061411, -0.03085726, -0.04409649, -0.05020334,\n",
       "        -0.04034202, -0.04278026, -0.04799516, -0.0495895 , -0.04602247,\n",
       "        -0.04893068, -0.0575148 , -0.0511832 , -0.05558307, -0.04056968,\n",
       "        -0.04352922, -0.04476838, -0.04728337, -0.04476059, -0.04793157,\n",
       "        -0.04835118, -0.04867647, -0.05030828, -0.04817824, -0.05013043,\n",
       "        -0.04731483, -0.4328275 , -1.53914503, -0.41499603, -0.38159132,\n",
       "        -1.22216503, -0.38241048, -0.3706144 , -1.30797108, -0.36535795,\n",
       "        -0.3535756 , -1.19820104, -0.35512212, -0.42522227, -1.26601637,\n",
       "        -0.47629499, -0.38650418, -1.33954463, -0.41246296, -0.37249034,\n",
       "        -1.2546567 , -0.34825377, -0.34976836, -1.250583  , -0.35538524,\n",
       "        -0.46055594, -1.35284002, -0.42766346, -0.38747655, -1.37102092,\n",
       "        -0.39927433, -0.35399567, -1.30178011, -0.35520772, -0.36139613,\n",
       "        -1.26445578, -0.34569496, -0.48933233, -1.22726725, -0.47133894,\n",
       "        -0.40053269, -1.26385225, -0.40909005, -0.36495766, -1.38724613,\n",
       "        -0.37929112, -0.35266194, -1.19220874, -0.34238759, -0.17866419,\n",
       "        -1.24724095, -0.18907285, -0.17280099, -1.30430448, -0.17273394,\n",
       "        -0.17461826, -1.24481028, -0.17475402, -0.17489294, -1.11432236,\n",
       "        -0.17236657, -0.19506865, -1.4246065 , -0.195246  , -0.17894799,\n",
       "        -1.1981306 , -0.17644388, -0.18060532, -1.23737346, -0.17347375,\n",
       "        -0.17112404, -1.26229803, -0.17547625, -0.18489402, -1.28930327,\n",
       "        -0.1902222 , -0.17595681, -1.38763133, -0.17986146, -0.17419231,\n",
       "        -1.01155949, -0.17216108, -0.17150328, -1.20109277, -0.17221577,\n",
       "        -0.18604598, -1.31692967, -0.18992672, -0.18164838, -1.46513504,\n",
       "        -0.17769404, -0.17447274, -1.20062233, -0.17861637, -0.17954011,\n",
       "        -1.42025867, -0.17030389, -0.16580861, -0.15791873, -0.17011472,\n",
       "        -0.13955254, -0.13246892, -0.14097724, -0.1270161 , -0.12855848,\n",
       "        -0.12111433, -0.11384289, -0.12373346, -0.11720485, -0.18008055,\n",
       "        -0.16314028, -0.16605599, -0.13490495, -0.13724226, -0.13594528,\n",
       "        -0.12652374, -0.12966007, -0.12396321, -0.12078076, -0.12110506,\n",
       "        -0.11553723, -0.16629667, -0.16698122, -0.15803526, -0.13847211,\n",
       "        -0.137931  , -0.13846903, -0.12772836, -0.12134442, -0.1248192 ,\n",
       "        -0.11946757, -0.12241962, -0.11487465, -0.17594524, -0.17872674,\n",
       "        -0.17523714, -0.14328343, -0.14727225, -0.15628938, -0.14223255,\n",
       "        -0.13720457, -0.14381969, -0.13378511, -0.13544182, -0.13785627,\n",
       "        -0.10776435, -0.10493779, -0.10284782, -0.08142944, -0.08292822,\n",
       "        -0.08443964, -0.07168995, -0.07535056, -0.07527851, -0.07098184,\n",
       "        -0.06944148, -0.06670092, -0.09881388, -0.09858181, -0.10459736,\n",
       "        -0.07928231, -0.08306526, -0.08126101, -0.07169276, -0.07462585,\n",
       "        -0.07248514, -0.06870581, -0.068756  , -0.07168962, -0.10128213,\n",
       "        -0.10245614, -0.10069142, -0.08017716, -0.08744967, -0.08209112,\n",
       "        -0.0719847 , -0.07428186, -0.07517613, -0.06886054, -0.07129433,\n",
       "        -0.07043954, -0.10561085, -0.10572169, -0.10984305, -0.08722346,\n",
       "        -0.08941481, -0.08805623, -0.07960444, -0.08414381, -0.08404395,\n",
       "        -0.07400164, -0.0750988 , -0.07839041]),\n",
       " 'std_test_neg_log_loss': array([8.40959309e-03, 5.63460423e-03, 1.18564240e-02, 4.81828156e-03,\n",
       "        7.40052338e-03, 2.84647322e-03, 2.60627073e-02, 2.05591468e-02,\n",
       "        6.11925248e-03, 7.02515416e-03, 3.63249337e-03, 1.11483029e-02,\n",
       "        1.03489419e-02, 1.28394821e-02, 2.23025486e-02, 1.35590773e-02,\n",
       "        2.41502031e-02, 1.10111565e-02, 1.13645785e-02, 1.50887092e-02,\n",
       "        2.46740207e-03, 1.40181660e-02, 5.00451820e-03, 2.01158689e-02,\n",
       "        1.10666830e-02, 5.37559870e-03, 1.73700461e-02, 4.23540009e-03,\n",
       "        7.95167725e-03, 1.14236034e-02, 8.33593715e-03, 9.53505117e-03,\n",
       "        7.16118702e-03, 9.89204434e-03, 8.60105234e-03, 6.14576100e-03,\n",
       "        6.22533425e-03, 4.94998427e-03, 4.31526917e-03, 6.97357750e-03,\n",
       "        6.72004577e-03, 5.38966101e-03, 7.29013515e-03, 5.91270890e-03,\n",
       "        5.52050845e-03, 7.47494884e-03, 6.78302694e-03, 6.62093048e-03,\n",
       "        4.65829766e-03, 1.43123017e-02, 6.07241807e-03, 1.82966026e-02,\n",
       "        6.61268519e-03, 4.83756877e-03, 4.92465170e-03, 1.77550705e-02,\n",
       "        1.58719501e-02, 1.48980943e-02, 1.72940648e-02, 2.28389113e-02,\n",
       "        1.41639791e-02, 2.46790234e-02, 1.36447935e-02, 5.61625821e-03,\n",
       "        2.44520413e-05, 4.00017059e-03, 1.47325152e-02, 2.37206608e-03,\n",
       "        1.49585003e-02, 1.94742765e-03, 9.67091599e-03, 5.39923755e-03,\n",
       "        6.76888900e-03, 1.07783695e-03, 4.54273049e-03, 1.57825357e-02,\n",
       "        2.98566289e-03, 5.59425735e-03, 6.94960219e-03, 5.72131831e-03,\n",
       "        9.87563346e-03, 1.10965682e-03, 3.79235632e-03, 4.40187980e-03,\n",
       "        5.47979303e-03, 7.95535140e-03, 6.79025646e-03, 3.74095309e-03,\n",
       "        4.97108272e-03, 5.72759319e-03, 8.30827442e-03, 7.64049810e-03,\n",
       "        5.77018351e-03, 7.62415215e-03, 5.55464213e-03, 6.19069427e-03,\n",
       "        3.85210625e-02, 4.69019109e-02, 4.81532856e-03, 1.55163926e-02,\n",
       "        1.97977069e-02, 1.49433111e-02, 1.00259110e-02, 1.72995705e-02,\n",
       "        2.16792304e-03, 1.72329968e-03, 6.46197094e-02, 5.37433670e-03,\n",
       "        1.65318940e-03, 1.20710841e-01, 1.34471099e-02, 1.04572085e-02,\n",
       "        1.97414627e-01, 2.03535049e-02, 2.54063360e-03, 8.78872504e-02,\n",
       "        5.93529783e-03, 5.54409340e-03, 4.33638735e-03, 9.28174229e-04,\n",
       "        2.98231919e-02, 1.65229102e-01, 1.18251764e-02, 7.29685878e-03,\n",
       "        1.37631875e-02, 1.83492713e-02, 9.57708814e-03, 1.35079938e-02,\n",
       "        8.19497691e-03, 1.36493435e-02, 5.26165841e-02, 6.67575201e-04,\n",
       "        9.13565596e-03, 1.01715688e-01, 1.61835406e-02, 1.49589840e-02,\n",
       "        1.17792454e-02, 7.40757303e-04, 6.96570219e-03, 8.76490414e-02,\n",
       "        6.12285279e-03, 8.34320789e-03, 4.70224451e-02, 7.45016422e-03,\n",
       "        5.89831323e-03, 7.44210377e-02, 9.58386574e-03, 9.99608997e-03,\n",
       "        2.77064267e-01, 1.16074277e-02, 1.88272120e-02, 1.14272238e-01,\n",
       "        4.79770454e-03, 1.17789084e-02, 4.36769580e-02, 1.23204170e-02,\n",
       "        3.78738935e-03, 2.44044213e-01, 1.52512231e-02, 1.56440146e-02,\n",
       "        1.11715146e-01, 6.57138221e-03, 1.60877932e-02, 3.35215117e-02,\n",
       "        1.58090148e-02, 1.25497727e-02, 5.74353203e-02, 1.13315809e-02,\n",
       "        1.27276576e-02, 4.93961684e-02, 1.49883616e-02, 9.95417975e-03,\n",
       "        1.87390180e-01, 1.33578592e-02, 9.80270383e-03, 7.40056256e-02,\n",
       "        1.15039103e-02, 1.05285308e-02, 6.45593359e-02, 9.81588507e-03,\n",
       "        1.52846157e-02, 7.95337397e-02, 8.65845060e-03, 6.85925977e-03,\n",
       "        3.06702363e-01, 1.03470063e-02, 1.12201771e-02, 1.05347304e-01,\n",
       "        9.18710515e-03, 8.57945470e-03, 5.65253168e-03, 1.26401499e-02,\n",
       "        6.60996690e-03, 1.07222735e-02, 8.31667009e-03, 8.48499868e-03,\n",
       "        8.39031479e-03, 7.95040194e-03, 1.09017127e-02, 1.29779167e-02,\n",
       "        1.13722088e-02, 9.16501771e-03, 9.77362981e-03, 1.02274172e-02,\n",
       "        1.51273213e-03, 7.71632412e-03, 6.75278816e-03, 1.19469629e-02,\n",
       "        7.44505334e-03, 1.09621468e-02, 1.01040112e-02, 1.49697269e-02,\n",
       "        1.05307607e-02, 1.11684724e-02, 5.58843041e-03, 1.18912173e-02,\n",
       "        1.72584327e-02, 1.59250603e-02, 1.52291851e-02, 1.07183399e-02,\n",
       "        1.55725017e-02, 1.31986030e-02, 1.27537415e-02, 1.34333053e-02,\n",
       "        1.23303270e-02, 8.23083185e-03, 9.55748554e-03, 1.16589835e-02,\n",
       "        9.52783307e-03, 1.21063523e-02, 1.32620053e-02, 1.33947640e-02,\n",
       "        1.11061593e-02, 6.75354639e-03, 9.69693987e-03, 9.92756034e-03,\n",
       "        1.24618468e-02, 1.78222949e-02, 1.01756879e-02, 7.54366304e-03,\n",
       "        4.99563179e-03, 1.72190563e-02, 1.15344266e-02, 1.52964526e-02,\n",
       "        1.23954084e-02, 1.31849035e-02, 5.71107420e-03, 7.85018254e-03,\n",
       "        1.08322558e-02, 8.36679634e-03, 8.71249358e-03, 9.10768128e-03,\n",
       "        9.47196923e-03, 9.15512374e-03, 1.44992192e-02, 5.00882962e-03,\n",
       "        4.08657499e-03, 1.19673100e-02, 1.16237790e-02, 1.03864251e-02,\n",
       "        1.34026741e-02, 9.61682656e-03, 1.32941763e-02, 1.47333553e-02,\n",
       "        1.56015579e-02, 1.72762142e-02, 7.66580572e-03, 6.29428421e-03,\n",
       "        1.81626635e-02, 1.05826330e-02, 1.06436933e-02, 1.00167845e-02,\n",
       "        9.65606975e-03, 4.95258248e-03, 8.42756006e-03, 1.00095786e-02,\n",
       "        1.38881707e-02, 1.82197824e-02, 2.07913280e-03, 1.64827225e-02,\n",
       "        8.61560048e-03, 7.90482617e-03, 1.29094076e-02, 1.24576669e-02,\n",
       "        1.42430037e-02, 9.06234788e-03, 9.67441336e-03, 1.21645582e-02]),\n",
       " 'rank_test_neg_log_loss': array([  3,   2,  24, 131,  85,   7,  40,  79,  74,  71, 108,  27,  16,\n",
       "         21,  77,  14,  52,  23,  29,  12,  81,  78,  91,  10,  11,  22,\n",
       "         13,  20,  17,   6,  42,  37,  33,  49,  51,  64,  35,  36,  31,\n",
       "         44,  48,  45,  50,  65,  55,  63,  70,  60,  15,   4,  43,  62,\n",
       "         84,  25,  82,  73, 105,  89, 127,  90,   8,  75,   9,  87,  39,\n",
       "          1,  86,  76, 104,  58,  38,  97,   5,  30,  68,  18,  26,  54,\n",
       "         66,  41,  61,  83,  72,  80,  19,  28,  34,  46,  32,  53,  57,\n",
       "         59,  69,  56,  67,  47, 252, 288, 249, 241, 264, 242, 238, 278,\n",
       "        237, 230, 261, 232, 250, 274, 255, 243, 280, 248, 239, 270, 227,\n",
       "        228, 269, 234, 253, 281, 251, 244, 282, 245, 231, 276, 233, 235,\n",
       "        273, 226, 256, 265, 254, 246, 272, 247, 236, 283, 240, 229, 259,\n",
       "        225, 210, 268, 220, 196, 277, 195, 200, 267, 201, 202, 258, 194,\n",
       "        223, 286, 224, 212, 260, 207, 216, 266, 197, 190, 271, 204, 218,\n",
       "        275, 222, 206, 284, 214, 198, 257, 192, 191, 263, 193, 219, 279,\n",
       "        221, 217, 287, 208, 199, 262, 209, 213, 285, 189, 184, 181, 188,\n",
       "        174, 163, 175, 159, 161, 152, 145, 155, 148, 215, 183, 185, 165,\n",
       "        169, 167, 158, 162, 156, 150, 151, 147, 186, 187, 182, 173, 171,\n",
       "        172, 160, 153, 157, 149, 154, 146, 205, 211, 203, 177, 179, 180,\n",
       "        176, 168, 178, 164, 166, 170, 143, 140, 138, 120, 122, 126, 101,\n",
       "        114, 113,  98,  95,  88, 134, 133, 139, 116, 123, 119, 102, 110,\n",
       "        106,  92,  93, 100, 136, 137, 135, 118, 129, 121, 103, 109, 112,\n",
       "         94,  99,  96, 141, 142, 144, 128, 132, 130, 117, 125, 124, 107,\n",
       "        111, 115])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRIAL_4_MLP.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FOUR NEG LOG LOSS RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'tanh',\n",
       " 'classifier__alpha': 0.001,\n",
       " 'classifier__hidden_layer_sizes': (10,),\n",
       " 'classifier__learning_rate': 'adaptive',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL FOUR NEG LOG LOSS RESULTS ---------------------------------------------\")\n",
    "TRIAL_4_MLP.cv_results_['rank_test_neg_log_loss']\n",
    "TRIAL_4_MLP.cv_results_['params'][ np.argmin(TRIAL_4_MLP.cv_results_['rank_test_neg_log_loss']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FOUR F1 RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.0001,\n",
       " 'classifier__hidden_layer_sizes': (5,),\n",
       " 'classifier__learning_rate': 'constant',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL FOUR F1 RESULTS ---------------------------------------------\")\n",
    "TRIAL_4_MLP.cv_results_['rank_test_f1_micro']\n",
    "TRIAL_4_MLP.cv_results_['params'][ np.argmin(TRIAL_4_MLP.cv_results_['rank_test_f1_micro']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FOUR ROC AUC RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.001,\n",
       " 'classifier__hidden_layer_sizes': (10,),\n",
       " 'classifier__learning_rate': 'invscaling',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL FOUR ROC AUC RESULTS ---------------------------------------------\")\n",
    "TRIAL_4_MLP.cv_results_['rank_test_roc_auc_ovo']\n",
    "TRIAL_4_MLP.cv_results_['params'][ np.argmin(TRIAL_4_MLP.cv_results_['rank_test_roc_auc_ovo']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 4 MPL using best NEG LOG LOSS hyperparameters :0.99265\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 4 MPL using best F1 hyperparameters :0.9958833333333333\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 4 MPL using best ROC_AUC hyperparameters :0.9958333333333333\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best NEG LOG LOSS results to train on test set, \n",
    "p = TRIAL_4_MLP.cv_results_['params'][ np.argmin(TRIAL_4_MLP.cv_results_['rank_test_neg_log_loss']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train__neg_log_loss = pipe.predict(X_train)\n",
    "y_pred_test_neg_log_loss = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 4 MPL using best NEG LOG LOSS hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_neg_log_loss)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best F1 results to train on test set, LR_Trial 1\n",
    "p = TRIAL_4_MLP.cv_results_['params'][ np.argmin(TRIAL_4_MLP.cv_results_['rank_test_f1_micro']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train_roc_auc = pipe.predict(X_train)\n",
    "y_pred_test_f1 = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 4 MPL using best F1 hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_f1)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best ROC_AUC results to train on test set, LR_Trial 1\n",
    "p  = TRIAL_4_MLP.cv_results_['params'][ np.argmin(TRIAL_4_MLP.cv_results_['rank_test_roc_auc_ovo']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train_roc_auc = pipe.predict(X_train)\n",
    "y_pred_test_roc_auc = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 4 MPL using best ROC_AUC hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_roc_auc)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTI LAYERED PERCEPTRON TRIAL FIVE ON FIRE WALL DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 288 candidates, totalling 576 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    7.9s\n",
      "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed:    9.1s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   10.6s\n",
      "[Parallel(n_jobs=-1)]: Done 189 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=-1)]: Done 210 tasks      | elapsed:   14.9s\n",
      "[Parallel(n_jobs=-1)]: Done 233 tasks      | elapsed:   18.2s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:   20.6s\n",
      "[Parallel(n_jobs=-1)]: Done 281 tasks      | elapsed:   23.8s\n",
      "[Parallel(n_jobs=-1)]: Done 306 tasks      | elapsed:   27.2s\n",
      "[Parallel(n_jobs=-1)]: Done 333 tasks      | elapsed:   31.2s\n",
      "[Parallel(n_jobs=-1)]: Done 360 tasks      | elapsed:   34.6s\n",
      "[Parallel(n_jobs=-1)]: Done 389 tasks      | elapsed:   38.5s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:   42.6s\n",
      "[Parallel(n_jobs=-1)]: Done 449 tasks      | elapsed:   47.2s\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:   51.9s\n",
      "[Parallel(n_jobs=-1)]: Done 513 tasks      | elapsed:   57.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FIVE RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 576 out of 576 | elapsed:  1.1min finished\n"
     ]
    }
   ],
   "source": [
    "ITERATIONS = 1\n",
    "for i in range(1):\n",
    "    #For reach \"trial\", randomly select 5000 data points\n",
    "    random5000DataPoints = FireWALLData.sample(n = 5000, replace = True)\n",
    "    #From that 5000, create x set and y set\n",
    "    xSet = random5000DataPoints.iloc[:,0:9].values\n",
    "    ySet = random5000DataPoints.iloc[:,10].values\n",
    "    ySet = encodeLabel.fit_transform(ySet)\n",
    "    # Run GridSearch to find best hyperparameters per 5000 data points \n",
    "    \n",
    "    pipe = Pipeline([('std', StandardScaler()),('classifier', MLPClassifier(max_iter = 250))])\n",
    "\n",
    "    search_space = [\n",
    "                {\n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                {\n",
    "                 \n",
    "                  \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['sgd'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                \n",
    "                {\n",
    "                \n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['adam'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                ]\n",
    "    \n",
    "    #'precision_micro','roc_auc_ovo','jaccard','neg_log_loss'] JACCARD DOES NOT WORK FOR NON BINARY?\n",
    "    runHyperParamSearch = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=2), \n",
    "                          scoring=['recall_micro','f1_micro','roc_auc_ovo','neg_log_loss'], \n",
    "                          refit=False, verbose=10, n_jobs = -1)\n",
    "    TRIAL_5_MLP = runHyperParamSearch.fit(xSet, ySet)\n",
    "    print(\"---------------------------------TRIAL FIVE RESULTS ---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.41335738, 0.67107761, 0.63029349, 0.66182053, 0.91453743,\n",
       "        0.88301027, 1.14248419, 1.02638459, 0.95281911, 0.91728866,\n",
       "        1.1777637 , 1.09769452, 0.73112917, 0.74363947, 0.78917682,\n",
       "        0.88726354, 0.76791096, 0.90627956, 1.00336158, 1.08368254,\n",
       "        0.57449436, 1.11696017, 1.10344875, 1.14473438, 0.72962832,\n",
       "        0.61678004, 0.68859267, 0.8389715 , 0.83596849, 0.8111974 ,\n",
       "        0.9605757 , 1.02738571, 1.02363038, 1.08893645, 1.09494233,\n",
       "        1.08618367, 0.55697799, 0.61627924, 0.47515798, 0.67783237,\n",
       "        0.80844545, 0.8129487 , 0.89276898, 0.93905818, 0.85173333,\n",
       "        0.7446413 , 0.86199129, 0.9300499 , 0.52895534, 0.53420961,\n",
       "        0.67082787, 0.97708964, 0.99885941, 0.88801336, 1.21229196,\n",
       "        1.13097262, 1.13722801, 1.3248893 , 1.33539724, 1.30462289,\n",
       "        0.78342533, 0.78442562, 0.65981805, 1.03188848, 0.80644608,\n",
       "        0.95056856, 1.1274699 , 1.17976618, 1.14848781, 1.30437326,\n",
       "        1.30962718, 1.31312859, 0.75564933, 0.77741897, 0.76991153,\n",
       "        0.93480432, 0.95181882, 0.95457423, 1.22955835, 1.16275096,\n",
       "        1.11220634, 1.38719332, 1.44249141, 1.49853814, 0.84722912,\n",
       "        0.87300086, 0.76290822, 0.97058606, 1.04865205, 1.03439021,\n",
       "        1.20578766, 1.17676115, 1.11896276, 1.2373122 , 1.23155725,\n",
       "        1.24457037, 2.06602633, 0.79092932, 2.06102216, 2.25568914,\n",
       "        1.85684741, 2.18362832, 2.38029706, 1.79529428, 2.21240294,\n",
       "        2.4348439 , 2.30072856, 2.34051299, 1.94592321, 1.90113425,\n",
       "        1.98445678, 2.06977975, 1.35566568, 2.20564771, 2.23867643,\n",
       "        2.1283313 , 2.18212724, 2.30848658, 2.26219499, 2.27645743,\n",
       "        1.8943789 , 0.3633126 , 1.83257616, 1.96018553, 1.33815074,\n",
       "        1.91739929, 2.10205829, 2.11406791, 2.14559579, 2.22966743,\n",
       "        0.74238884, 2.16761506, 1.82757187, 1.01687467, 1.89037585,\n",
       "        2.04150605, 0.90052426, 2.00897777, 2.25669062, 2.15885603,\n",
       "        2.3335067 , 2.30898523, 1.32013607, 2.45310962, 2.15385222,\n",
       "        1.43273258, 1.97219658, 2.11957276, 2.18913305, 2.18362713,\n",
       "        2.40506876, 2.33951163, 2.37604332, 2.38104808, 2.33325636,\n",
       "        2.559201  , 2.12958109, 1.93241167, 2.04175603, 2.15660501,\n",
       "        2.21740711, 2.22166038, 2.49814808, 2.3162415 , 2.44960654,\n",
       "        2.52191889, 2.32149625, 2.32900286, 1.91464663, 1.53557062,\n",
       "        1.99696743, 2.10856378, 2.00022078, 2.04025471, 2.21640611,\n",
       "        2.1193229 , 2.21840835, 2.27045286, 2.24167788, 2.23216975,\n",
       "        1.86084986, 1.26208413, 1.87436223, 2.03650093, 1.98370647,\n",
       "        1.98295557, 2.20189357, 2.18688154, 2.23442245, 2.38980508,\n",
       "        2.37979674, 2.32875335, 2.06002212, 2.02774346, 2.08729541,\n",
       "        2.21666002, 2.3487699 , 2.18688047, 2.44335151, 2.41407621,\n",
       "        2.43834758, 2.50265169, 2.43009007, 2.40882134, 2.07253206,\n",
       "        2.11156607, 2.06427562, 2.1968894 , 2.21790683, 2.2994771 ,\n",
       "        2.45010722, 2.38780332, 2.46086681, 2.5727123 , 2.4225843 ,\n",
       "        2.40456784, 2.04150629, 2.05501699, 2.07703614, 2.14184177,\n",
       "        2.13783932, 2.16185951, 2.42908883, 2.38029683, 2.38580191,\n",
       "        2.51216018, 2.56845856, 2.583722  , 2.32750094, 2.26795018,\n",
       "        2.12207496, 2.31023741, 2.25694084, 2.41983116, 2.6177516 ,\n",
       "        2.30623329, 2.37078905, 2.42008138, 2.57846749, 2.34201491,\n",
       "        2.30723429, 2.12582803, 2.1053108 , 2.21065104, 2.24968481,\n",
       "        2.25994325, 2.46787298, 2.48363602, 2.46637046, 2.62100434,\n",
       "        2.60073793, 2.62500739, 2.18062627, 2.26144552, 2.29747629,\n",
       "        2.43734634, 2.52266908, 2.39831316, 2.56320512, 2.45561171,\n",
       "        2.7260946 , 2.90149498, 2.79115045, 2.80166018, 2.26419818,\n",
       "        2.25293779, 2.13108289, 2.41582811, 2.4093219 , 2.33275652,\n",
       "        2.4526093 , 2.56720841, 2.4558624 , 2.60799313, 2.63601613,\n",
       "        2.72409284, 2.1788733 , 2.18162596, 2.17787266, 2.28971875,\n",
       "        2.25043595, 2.38855469, 2.44860685, 2.21790659, 2.19839048,\n",
       "        2.0575192 , 1.88862324, 1.82481825]),\n",
       " 'std_fit_time': array([1.10596061e-01, 1.96169019e-01, 2.12682486e-02, 1.37868047e-01,\n",
       "        5.25474548e-03, 1.16849542e-01, 1.50629282e-01, 1.42122507e-01,\n",
       "        1.20602608e-01, 2.03675151e-01, 1.25193596e-03, 6.08018637e-02,\n",
       "        2.30207443e-02, 5.30462265e-02, 3.05258036e-02, 2.55212784e-02,\n",
       "        1.44374371e-01, 4.00352478e-03, 3.00157070e-03, 5.88011742e-02,\n",
       "        4.24364090e-01, 7.00557232e-03, 1.30108595e-02, 2.12690830e-02,\n",
       "        9.50956345e-03, 7.38134384e-02, 3.20274830e-02, 4.92914915e-02,\n",
       "        9.75728035e-03, 1.50133371e-02, 1.87667608e-02, 1.00040436e-03,\n",
       "        2.17683315e-02, 1.40110254e-02, 2.95267105e-02, 6.07999563e-02,\n",
       "        4.95417118e-02, 5.77996969e-02, 1.07595921e-02, 1.43373013e-01,\n",
       "        1.37610435e-02, 2.50101089e-04, 6.50560856e-03, 1.20353341e-01,\n",
       "        7.30630159e-02, 1.75147057e-02, 1.75400615e-01, 1.62389040e-01,\n",
       "        3.50320339e-03, 1.93918347e-01, 1.18851900e-01, 3.12771797e-02,\n",
       "        1.15098953e-02, 1.00836277e-01, 5.12936115e-02, 5.20451069e-02,\n",
       "        2.07674503e-02, 2.82748938e-02, 3.72803211e-02, 3.50332260e-03,\n",
       "        2.42710114e-02, 5.22958040e-02, 1.05340123e-01, 2.40199566e-02,\n",
       "        1.27359152e-01, 8.38218927e-02, 6.00515604e-02, 1.52621269e-02,\n",
       "        8.00657272e-03, 1.92646980e-02, 3.85307074e-02, 2.00021267e-03,\n",
       "        1.05094910e-02, 3.75378132e-03, 2.37712860e-02, 1.40117407e-02,\n",
       "        1.70141459e-02, 3.17794085e-02, 4.75403070e-02, 3.27785015e-02,\n",
       "        4.37878370e-02, 2.70229578e-02, 3.77827883e-02, 3.07763815e-02,\n",
       "        2.35201120e-02, 9.78355408e-02, 1.22618675e-02, 3.57801914e-02,\n",
       "        9.25815105e-03, 5.00679016e-04, 2.37705708e-02, 1.42619610e-02,\n",
       "        9.00840759e-03, 1.12596750e-02, 4.65401411e-02, 5.50478697e-02,\n",
       "        1.37869000e-01, 3.96091223e-01, 2.12689638e-02, 1.18351460e-01,\n",
       "        1.63390040e-01, 1.67644024e-02, 4.75418568e-03, 5.61232328e-01,\n",
       "        5.90511560e-02, 9.78342295e-02, 6.25562668e-03, 6.85591698e-02,\n",
       "        4.02840376e-02, 4.10354137e-02, 4.27868366e-02, 8.10698271e-02,\n",
       "        6.93596125e-01, 1.52626038e-02, 2.67738104e-02, 2.50232220e-03,\n",
       "        2.87747383e-02, 5.60480356e-02, 7.25662708e-03, 7.45643377e-02,\n",
       "        6.13026619e-02, 9.50849056e-03, 2.05181837e-02, 8.00585747e-03,\n",
       "        6.60067797e-01, 2.72732973e-02, 4.72916365e-02, 5.72992563e-02,\n",
       "        5.75494766e-03, 1.52633190e-02, 2.79490352e-01, 8.75699520e-03,\n",
       "        9.99927521e-04, 8.85261655e-01, 6.23035431e-02, 2.97753811e-02,\n",
       "        1.12846255e-01, 4.27874327e-02, 3.72828245e-02, 7.60661364e-02,\n",
       "        7.65661001e-02, 4.99963760e-04, 7.54148245e-01, 4.70405817e-02,\n",
       "        6.20532036e-02, 5.12440443e-01, 2.25193501e-02, 6.98105097e-02,\n",
       "        1.26358151e-01, 4.07850742e-02, 3.50296497e-03, 3.10275555e-02,\n",
       "        9.70835686e-02, 5.25447130e-02, 9.83351469e-02, 5.85505962e-02,\n",
       "        1.35365844e-01, 6.63070679e-02, 3.50344181e-03, 4.52883244e-02,\n",
       "        1.50133371e-02, 2.52721310e-02, 1.95668101e-01, 5.98013401e-02,\n",
       "        1.45124197e-02, 9.83346701e-02, 6.85591698e-02, 5.50484657e-03,\n",
       "        1.30107403e-02, 3.80577564e-01, 1.23355508e-01, 8.25703144e-03,\n",
       "        5.50460815e-03, 7.10605383e-02, 2.70237923e-02, 1.95168257e-02,\n",
       "        1.35114193e-02, 3.65320444e-02, 3.67814302e-02, 1.75178051e-03,\n",
       "        1.75118446e-03, 3.64313006e-01, 1.25145912e-03, 2.97758579e-02,\n",
       "        8.00728798e-03, 1.67640448e-02, 3.60317230e-02, 1.35116577e-02,\n",
       "        2.90243626e-02, 1.97668076e-02, 6.68077469e-02, 9.63331461e-02,\n",
       "        4.02841568e-02, 2.80238390e-02, 3.95339727e-02, 2.72696018e-02,\n",
       "        7.68160820e-02, 2.50160694e-03, 1.22597218e-02, 3.45298052e-02,\n",
       "        4.67901230e-02, 4.65397835e-02, 2.10179090e-02, 5.75494766e-03,\n",
       "        2.22688913e-02, 2.22682953e-02, 6.65563345e-02, 3.15270424e-02,\n",
       "        5.50448895e-03, 4.85415459e-02, 5.80497980e-02, 4.87921238e-02,\n",
       "        6.98100328e-02, 2.00177431e-02, 3.80330086e-02, 5.15447855e-02,\n",
       "        2.25257874e-03, 3.75294685e-03, 2.25210190e-03, 3.10264826e-02,\n",
       "        2.20189095e-02, 9.00745392e-03, 6.90593719e-02, 4.12853956e-02,\n",
       "        3.97838354e-02, 6.50489330e-03, 6.78081512e-02, 5.70493937e-02,\n",
       "        1.87660098e-01, 8.25716257e-02, 1.07341886e-01, 8.78255367e-02,\n",
       "        2.10177898e-02, 3.25334072e-03, 9.25805569e-02, 4.12858725e-02,\n",
       "        3.37793827e-02, 1.95167065e-02, 1.56384826e-01, 6.00564480e-03,\n",
       "        9.75775719e-03, 1.15599632e-01, 2.30201483e-02, 3.25310230e-03,\n",
       "        7.08110332e-02, 2.50172615e-03, 4.17855978e-02, 4.25368547e-02,\n",
       "        6.18032217e-02, 8.25691223e-03, 9.20799971e-02, 7.58152008e-02,\n",
       "        9.03280973e-02, 1.48627520e-01, 1.85170174e-02, 5.77989817e-02,\n",
       "        5.30457497e-02, 1.30862117e-01, 1.56635642e-01, 6.50548935e-03,\n",
       "        3.22779417e-02, 3.70321274e-02, 2.07675695e-02, 4.72897291e-02,\n",
       "        7.23117590e-02, 2.30195522e-02, 1.17351174e-01, 7.53141642e-02,\n",
       "        4.87912893e-02, 1.31363630e-01, 5.75494766e-02, 4.50388193e-02,\n",
       "        4.87920046e-02, 3.17773819e-02, 3.82828712e-02, 6.43054247e-02,\n",
       "        6.45556450e-02, 1.49379373e-01, 2.35209465e-02, 3.17775011e-02,\n",
       "        1.50239468e-03, 6.10522032e-02, 2.95256376e-02, 3.90335321e-02,\n",
       "        6.60569668e-02, 1.60387754e-01, 3.50260735e-03, 6.18034601e-02]),\n",
       " 'mean_score_time': array([0.02151728, 0.02777457, 0.02226889, 0.02251911, 0.03252792,\n",
       "        0.02852356, 0.02802372, 0.02952588, 0.02226973, 0.02351916,\n",
       "        0.02402091, 0.02452099, 0.02151823, 0.02251959, 0.02076757,\n",
       "        0.0250212 , 0.02677369, 0.02176976, 0.02276933, 0.02577233,\n",
       "        0.02702212, 0.02326977, 0.02277064, 0.02402115, 0.02201867,\n",
       "        0.02151906, 0.02326953, 0.02377117, 0.02502513, 0.02377176,\n",
       "        0.02477181, 0.02476907, 0.02377117, 0.02452075, 0.02727306,\n",
       "        0.0257709 , 0.02301991, 0.0217694 , 0.02101743, 0.02802444,\n",
       "        0.02577233, 0.02702463, 0.02402174, 0.02427042, 0.02302015,\n",
       "        0.02376986, 0.02377045, 0.02577186, 0.02126908, 0.02151763,\n",
       "        0.02902615, 0.02477098, 0.02577448, 0.02377081, 0.03052855,\n",
       "        0.02351975, 0.02351999, 0.02251935, 0.02602208, 0.02477169,\n",
       "        0.02101755, 0.02827406, 0.02452135, 0.02376986, 0.0217663 ,\n",
       "        0.02301931, 0.02277195, 0.02602172, 0.02301955, 0.02477026,\n",
       "        0.0247699 , 0.02351987, 0.02552259, 0.02327001, 0.02226877,\n",
       "        0.02627277, 0.02702165, 0.03002238, 0.02877498, 0.02652359,\n",
       "        0.02302051, 0.02927566, 0.0367806 , 0.03678215, 0.04028428,\n",
       "        0.03152633, 0.02902412, 0.02727175, 0.02702367, 0.02627265,\n",
       "        0.02627242, 0.02577269, 0.02427006, 0.02452207, 0.02402067,\n",
       "        0.03903377, 0.02402103, 0.02126861, 0.02051795, 0.02276921,\n",
       "        0.03127646, 0.02351975, 0.02201939, 0.02251911, 0.02277017,\n",
       "        0.02327061, 0.02402091, 0.02377045, 0.0270232 , 0.02226937,\n",
       "        0.02176881, 0.02151859, 0.02302003, 0.02176845, 0.02201819,\n",
       "        0.02226818, 0.02126813, 0.02226818, 0.02377129, 0.02477133,\n",
       "        0.01951718, 0.0197674 , 0.02076805, 0.02051723, 0.02026796,\n",
       "        0.02251923, 0.02151823, 0.02477181, 0.02377057, 0.02176857,\n",
       "        0.02176797, 0.02326906, 0.02226913, 0.02051771, 0.02076781,\n",
       "        0.02927518, 0.02402043, 0.02377033, 0.02427125, 0.02201939,\n",
       "        0.02477217, 0.03327894, 0.02552116, 0.02201879, 0.02126837,\n",
       "        0.02051747, 0.02251959, 0.03127694, 0.0252707 , 0.02602243,\n",
       "        0.02301943, 0.02427089, 0.02176869, 0.02301931, 0.02577221,\n",
       "        0.02877474, 0.02502155, 0.02101803, 0.02376997, 0.03653133,\n",
       "        0.02176869, 0.02602184, 0.02702308, 0.02176905, 0.02677286,\n",
       "        0.02677321, 0.02527177, 0.02176821, 0.02151835, 0.02076817,\n",
       "        0.02226901, 0.02226925, 0.02151787, 0.02226985, 0.02301991,\n",
       "        0.02276981, 0.02276933, 0.02302051, 0.02201867, 0.02351999,\n",
       "        0.02327037, 0.02051818, 0.02151728, 0.02151906, 0.02076793,\n",
       "        0.02151918, 0.02477109, 0.02201807, 0.02201831, 0.02327061,\n",
       "        0.02527153, 0.02577221, 0.02101767, 0.01976752, 0.02201891,\n",
       "        0.02651906, 0.02251947, 0.02226973, 0.02527177, 0.02652252,\n",
       "        0.02126765, 0.02301943, 0.02201891, 0.02402103, 0.02301919,\n",
       "        0.0302757 , 0.02226925, 0.02527142, 0.02877522, 0.02176881,\n",
       "        0.02251995, 0.02126849, 0.02702308, 0.02226901, 0.02251911,\n",
       "        0.02176893, 0.02251828, 0.02151895, 0.02377045, 0.02352059,\n",
       "        0.0260222 , 0.02602267, 0.02427101, 0.02251983, 0.02702343,\n",
       "        0.03127754, 0.02502155, 0.0282743 , 0.02126729, 0.02226913,\n",
       "        0.02251971, 0.02301943, 0.04078507, 0.02902448, 0.02201807,\n",
       "        0.02101839, 0.02276993, 0.02652216, 0.02353132, 0.02151823,\n",
       "        0.02402103, 0.02001727, 0.02226949, 0.02276933, 0.02527177,\n",
       "        0.02352059, 0.02552187, 0.02377057, 0.02527201, 0.02251947,\n",
       "        0.02452016, 0.02427053, 0.02326941, 0.02426994, 0.02477086,\n",
       "        0.02376986, 0.02652299, 0.02176797, 0.0232693 , 0.02602243,\n",
       "        0.02326989, 0.0282743 , 0.02852488, 0.02552187, 0.03828168,\n",
       "        0.02226925, 0.02076805, 0.02577198, 0.02201939, 0.02226901,\n",
       "        0.0240202 , 0.0272727 , 0.02827382, 0.03127646, 0.02402055,\n",
       "        0.02552152, 0.02251947, 0.02051735, 0.02326989, 0.02176893,\n",
       "        0.02151811, 0.01876581, 0.02076721, 0.01576376, 0.01351154,\n",
       "        0.01351178, 0.01301122, 0.01276124]),\n",
       " 'std_score_time': array([4.99725342e-04, 4.25267220e-03, 2.49981880e-04, 1.00016594e-03,\n",
       "        5.00392914e-03, 6.00540638e-03, 4.50468063e-03, 4.50432301e-03,\n",
       "        7.50541687e-04, 5.01036644e-04, 1.00207329e-03, 5.96046448e-07,\n",
       "        9.53674316e-07, 5.00202179e-04, 2.50697136e-04, 1.50144100e-03,\n",
       "        3.75413895e-03, 7.49588013e-04, 7.50422478e-04, 1.75166130e-03,\n",
       "        1.00135803e-03, 1.75130367e-03, 1.25145912e-03, 2.50363350e-03,\n",
       "        9.98973846e-04, 1.50120258e-03, 1.75106525e-03, 1.75297260e-03,\n",
       "        5.05208969e-04, 2.49385834e-04, 7.49230385e-04, 2.49862671e-04,\n",
       "        2.51412392e-04, 2.00164318e-03, 2.51173973e-04, 1.74975395e-03,\n",
       "        1.50120258e-03, 2.49981880e-04, 3.57627869e-07, 4.00400162e-03,\n",
       "        1.25002861e-03, 5.00321388e-04, 1.50144100e-03, 1.25110149e-03,\n",
       "        1.50024891e-03, 1.25241280e-03, 2.49981880e-04, 2.25186348e-03,\n",
       "        2.49862671e-04, 5.96046448e-07, 9.00852680e-03, 1.24979019e-03,\n",
       "        7.48634338e-04, 7.50064850e-04, 9.00912285e-03, 1.00040436e-03,\n",
       "        1.50156021e-03, 5.00917435e-04, 1.00147724e-03, 1.25074387e-03,\n",
       "        4.99486923e-04, 3.25226784e-03, 2.50124931e-03, 1.25098228e-03,\n",
       "        7.51137733e-04, 4.99486923e-04, 1.25253201e-03, 2.50101089e-03,\n",
       "        1.00111961e-03, 2.52723694e-04, 7.50422478e-04, 5.00082970e-04,\n",
       "        4.00269032e-03, 1.75082684e-03, 2.50577927e-04, 2.75254250e-03,\n",
       "        1.00135803e-03, 1.49869919e-03, 2.48908997e-04, 2.00462341e-03,\n",
       "        4.76837158e-07, 7.49826431e-04, 4.25505638e-03, 9.75883007e-03,\n",
       "        4.25302982e-03, 9.50860977e-03, 6.00337982e-03, 1.75094604e-03,\n",
       "        1.00123882e-03, 1.25157833e-03, 7.50422478e-04, 2.50458717e-04,\n",
       "        1.25122070e-03, 4.76837158e-07, 5.00440598e-04, 9.50896740e-03,\n",
       "        2.00164318e-03, 1.75142288e-03, 5.01036644e-04, 1.25169754e-03,\n",
       "        7.25674629e-03, 1.50132179e-03, 2.38418579e-07, 5.00440598e-04,\n",
       "        2.49624252e-04, 7.50780106e-04, 3.50308418e-03, 1.75130367e-03,\n",
       "        7.00628757e-03, 2.50458717e-04, 2.50577927e-04, 5.96046448e-07,\n",
       "        1.00111961e-03, 7.50184059e-04, 4.76837158e-07, 7.50899315e-04,\n",
       "        2.50577927e-04, 2.49981880e-04, 2.50101089e-04, 1.75106525e-03,\n",
       "        2.38418579e-07, 2.50220299e-04, 2.50458717e-04, 5.00798225e-04,\n",
       "        7.50064850e-04, 1.50120258e-03, 9.53674316e-07, 1.75154209e-03,\n",
       "        1.25122070e-03, 7.50541687e-04, 1.25133991e-03, 1.25181675e-03,\n",
       "        7.50184059e-04, 5.00082970e-04, 7.50184059e-04, 2.50816345e-04,\n",
       "        1.50084496e-03, 1.25193596e-03, 1.75189972e-03, 1.50132179e-03,\n",
       "        7.51018524e-04, 9.75871086e-03, 4.00340557e-03, 5.00321388e-04,\n",
       "        7.50303268e-04, 5.00321388e-04, 5.00917435e-04, 8.75806808e-03,\n",
       "        4.75382805e-03, 2.00200081e-03, 5.01036644e-04, 1.75106525e-03,\n",
       "        2.50458717e-04, 5.00917435e-04, 3.25238705e-03, 1.75094604e-03,\n",
       "        3.00240517e-03, 1.50156021e-03, 7.50184059e-04, 1.55130625e-02,\n",
       "        2.49981880e-04, 1.00052357e-03, 2.50220299e-03, 7.50541687e-04,\n",
       "        4.25374508e-03, 4.75406647e-03, 2.75290012e-03, 2.49981880e-04,\n",
       "        1.50096416e-03, 2.50339508e-04, 1.25145912e-03, 1.25145912e-03,\n",
       "        1.19209290e-07, 2.50220299e-04, 1.00100040e-03, 1.75249577e-03,\n",
       "        2.50458717e-04, 5.00202179e-04, 1.00064278e-03, 1.00064278e-03,\n",
       "        2.25234032e-03, 1.00147724e-03, 1.00088120e-03, 1.00100040e-03,\n",
       "        7.50064850e-04, 4.99486923e-04, 1.75178051e-03, 5.00798225e-04,\n",
       "        5.01036644e-04, 7.50303268e-04, 4.25374508e-03, 2.25198269e-03,\n",
       "        5.00559807e-04, 2.50816345e-04, 2.00176239e-03, 4.50682640e-03,\n",
       "        5.00798225e-04, 7.50541687e-04, 2.25317478e-03, 3.50248814e-03,\n",
       "        2.50339508e-04, 2.00116634e-03, 2.38418579e-07, 2.00212002e-03,\n",
       "        1.50144100e-03, 6.75618649e-03, 7.49826431e-04, 2.50339508e-04,\n",
       "        6.25491142e-03, 1.25098228e-03, 1.00147724e-03, 2.50220299e-04,\n",
       "        5.50556183e-03, 2.50816345e-04, 5.00679016e-04, 2.50220299e-04,\n",
       "        1.50167942e-03, 2.38418579e-07, 3.75282764e-03, 3.00300121e-03,\n",
       "        1.50084496e-03, 4.00328636e-03, 2.25186348e-03, 2.38418579e-07,\n",
       "        3.00228596e-03, 6.25526905e-03, 2.38418579e-07, 2.75301933e-03,\n",
       "        2.75266171e-03, 2.25174427e-03, 1.19209290e-07, 2.50184536e-03,\n",
       "        2.25186348e-03, 8.00716877e-03, 1.00004673e-03, 5.00321388e-04,\n",
       "        1.75166130e-03, 6.00552559e-03, 5.11288643e-04, 7.15255737e-07,\n",
       "        1.00028515e-03, 5.00321388e-04, 1.25122070e-03, 2.50220299e-04,\n",
       "        1.75178051e-03, 5.00082970e-04, 5.01036644e-04, 1.25122070e-03,\n",
       "        1.75106525e-03, 1.19209290e-07, 1.19209290e-06, 1.25169754e-03,\n",
       "        1.75166130e-03, 4.25398350e-03, 2.51173973e-04, 1.75118446e-03,\n",
       "        1.00100040e-03, 2.50458717e-04, 7.51137733e-04, 3.00264359e-03,\n",
       "        7.50780106e-04, 2.25186348e-03, 1.50156021e-03, 1.50120258e-03,\n",
       "        6.75630569e-03, 2.50101089e-04, 2.49981880e-04, 7.51376152e-04,\n",
       "        0.00000000e+00, 1.25050545e-03, 2.00223923e-03, 2.75182724e-03,\n",
       "        2.50101089e-04, 4.25410271e-03, 1.00076199e-03, 1.50132179e-03,\n",
       "        1.00100040e-03, 1.00135803e-03, 1.25074387e-03, 7.51137733e-04,\n",
       "        4.99129295e-04, 7.49945641e-04, 2.49624252e-04, 1.25098228e-03,\n",
       "        3.57627869e-07, 1.19209290e-07, 4.76837158e-07, 2.50220299e-04]),\n",
       " 'param_classifier': masked_array(data=[MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__activation': masked_array(data=['logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__alpha': masked_array(data=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__hidden_layer_sizes': masked_array(data=[(5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__learning_rate': masked_array(data=['constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'}],\n",
       " 'split0_test_recall_micro': array([0.9928, 0.9936, 0.9952, 0.9928, 0.9956, 0.9948, 0.996 , 0.9956,\n",
       "        0.9952, 0.9964, 0.9956, 0.994 , 0.9928, 0.9952, 0.9956, 0.996 ,\n",
       "        0.9948, 0.9948, 0.9964, 0.994 , 0.9756, 0.9972, 0.9956, 0.9952,\n",
       "        0.9848, 0.9936, 0.992 , 0.9928, 0.996 , 0.9964, 0.9936, 0.9904,\n",
       "        0.9956, 0.9884, 0.9924, 0.9924, 0.984 , 0.9832, 0.984 , 0.984 ,\n",
       "        0.9836, 0.9836, 0.9836, 0.9836, 0.9836, 0.9836, 0.9836, 0.9836,\n",
       "        0.9952, 0.996 , 0.9956, 0.9944, 0.9896, 0.9952, 0.9956, 0.9936,\n",
       "        0.9948, 0.994 , 0.9952, 0.9864, 0.996 , 0.9956, 0.9952, 0.9956,\n",
       "        0.9948, 0.9932, 0.996 , 0.992 , 0.9944, 0.9956, 0.9936, 0.9948,\n",
       "        0.9948, 0.9956, 0.9928, 0.9956, 0.9944, 0.994 , 0.9948, 0.9952,\n",
       "        0.9948, 0.9928, 0.9956, 0.9952, 0.9848, 0.9844, 0.9844, 0.9844,\n",
       "        0.9848, 0.984 , 0.9848, 0.984 , 0.9836, 0.9836, 0.9848, 0.986 ,\n",
       "        0.7692, 0.2264, 0.9472, 0.936 , 0.5672, 0.9408, 0.9348, 0.5724,\n",
       "        0.9308, 0.93  , 0.5684, 0.928 , 0.954 , 0.1948, 0.762 , 0.9408,\n",
       "        0.578 , 0.9336, 0.9296, 0.2492, 0.9328, 0.9332, 0.5856, 0.9332,\n",
       "        0.9496, 0.6992, 0.946 , 0.9404, 0.5736, 0.9316, 0.9292, 0.2048,\n",
       "        0.9348, 0.9272, 0.5724, 0.9276, 0.9428, 0.5776, 0.9584, 0.95  ,\n",
       "        0.1056, 0.9476, 0.932 , 0.12  , 0.9436, 0.9332, 0.402 , 0.9348,\n",
       "        0.9332, 0.5364, 0.9332, 0.934 , 0.2768, 0.9324, 0.932 , 0.5088,\n",
       "        0.932 , 0.934 , 0.2512, 0.934 , 0.9368, 0.1176, 0.9344, 0.9332,\n",
       "        0.3276, 0.9332, 0.9344, 0.3288, 0.9312, 0.9344, 0.5708, 0.9336,\n",
       "        0.9344, 0.2512, 0.9352, 0.9352, 0.2692, 0.9352, 0.9336, 0.3912,\n",
       "        0.9352, 0.9356, 0.2288, 0.9332, 0.9316, 0.5516, 0.932 , 0.936 ,\n",
       "        0.6008, 0.9304, 0.9344, 0.5892, 0.9352, 0.9332, 0.7044, 0.934 ,\n",
       "        0.9404, 0.9388, 0.9432, 0.944 , 0.9476, 0.946 , 0.9508, 0.95  ,\n",
       "        0.9492, 0.952 , 0.9512, 0.9516, 0.9396, 0.9432, 0.942 , 0.9472,\n",
       "        0.9492, 0.9464, 0.9484, 0.9496, 0.9484, 0.9504, 0.9508, 0.9496,\n",
       "        0.9388, 0.9408, 0.9416, 0.9468, 0.9452, 0.9448, 0.9512, 0.9476,\n",
       "        0.9496, 0.9492, 0.9516, 0.9504, 0.9364, 0.9376, 0.938 , 0.9432,\n",
       "        0.9412, 0.9432, 0.9448, 0.9436, 0.9432, 0.9452, 0.946 , 0.9456,\n",
       "        0.9692, 0.9668, 0.9656, 0.9704, 0.9712, 0.9712, 0.9732, 0.9732,\n",
       "        0.9724, 0.9736, 0.9732, 0.9752, 0.9544, 0.9656, 0.9564, 0.9712,\n",
       "        0.9712, 0.968 , 0.9704, 0.9724, 0.9712, 0.9744, 0.9732, 0.9736,\n",
       "        0.9672, 0.9528, 0.9692, 0.9708, 0.9688, 0.9692, 0.9728, 0.9712,\n",
       "        0.9724, 0.9732, 0.9772, 0.9748, 0.9524, 0.9528, 0.95  , 0.9552,\n",
       "        0.9672, 0.968 , 0.9692, 0.9692, 0.968 , 0.9724, 0.9704, 0.9732]),\n",
       " 'split1_test_recall_micro': array([9.968e-01, 9.968e-01, 9.960e-01, 9.972e-01, 9.944e-01, 9.972e-01,\n",
       "        9.944e-01, 9.968e-01, 9.920e-01, 9.936e-01, 9.972e-01, 9.936e-01,\n",
       "        9.956e-01, 9.960e-01, 9.968e-01, 9.968e-01, 9.908e-01, 9.940e-01,\n",
       "        9.948e-01, 9.964e-01, 9.936e-01, 9.964e-01, 9.956e-01, 9.952e-01,\n",
       "        9.960e-01, 9.900e-01, 9.900e-01, 9.956e-01, 9.904e-01, 9.952e-01,\n",
       "        9.940e-01, 9.956e-01, 9.928e-01, 9.884e-01, 9.896e-01, 9.904e-01,\n",
       "        9.900e-01, 9.904e-01, 9.900e-01, 9.896e-01, 9.892e-01, 9.888e-01,\n",
       "        9.892e-01, 9.892e-01, 9.888e-01, 9.892e-01, 9.888e-01, 9.888e-01,\n",
       "        9.972e-01, 9.972e-01, 9.916e-01, 9.884e-01, 9.972e-01, 9.952e-01,\n",
       "        9.888e-01, 9.904e-01, 9.940e-01, 9.920e-01, 9.936e-01, 9.896e-01,\n",
       "        9.968e-01, 9.968e-01, 9.968e-01, 9.908e-01, 9.964e-01, 9.968e-01,\n",
       "        9.956e-01, 9.968e-01, 9.952e-01, 9.932e-01, 9.968e-01, 9.904e-01,\n",
       "        9.968e-01, 9.904e-01, 9.908e-01, 9.956e-01, 9.916e-01, 9.900e-01,\n",
       "        9.952e-01, 9.972e-01, 9.960e-01, 9.956e-01, 9.940e-01, 9.900e-01,\n",
       "        9.896e-01, 9.916e-01, 9.900e-01, 9.892e-01, 9.896e-01, 9.896e-01,\n",
       "        9.896e-01, 9.888e-01, 9.900e-01, 9.896e-01, 9.892e-01, 9.900e-01,\n",
       "        9.548e-01, 5.776e-01, 7.912e-01, 9.664e-01, 8.000e-04, 9.516e-01,\n",
       "        9.584e-01, 1.776e-01, 9.572e-01, 9.532e-01, 2.504e-01, 9.572e-01,\n",
       "        7.924e-01, 2.268e-01, 9.688e-01, 9.692e-01, 3.980e-01, 9.568e-01,\n",
       "        9.572e-01, 1.948e-01, 9.512e-01, 9.492e-01, 7.100e-01, 9.492e-01,\n",
       "        8.368e-01, 5.776e-01, 8.404e-01, 7.796e-01, 1.948e-01, 9.560e-01,\n",
       "        9.560e-01, 2.000e-03, 9.568e-01, 9.540e-01, 5.776e-01, 9.576e-01,\n",
       "        9.612e-01, 8.000e-04, 9.732e-01, 9.708e-01, 6.600e-01, 9.624e-01,\n",
       "        9.640e-01, 5.776e-01, 9.516e-01, 9.520e-01, 5.776e-01, 9.588e-01,\n",
       "        9.516e-01, 9.200e-03, 9.484e-01, 9.508e-01, 1.932e-01, 9.564e-01,\n",
       "        9.480e-01, 6.192e-01, 9.532e-01, 9.532e-01, 5.684e-01, 9.568e-01,\n",
       "        9.524e-01, 4.560e-02, 9.524e-01, 9.564e-01, 3.540e-01, 9.576e-01,\n",
       "        9.504e-01, 6.300e-01, 9.584e-01, 9.572e-01, 3.916e-01, 9.584e-01,\n",
       "        9.600e-01, 3.436e-01, 9.516e-01, 9.548e-01, 2.756e-01, 9.568e-01,\n",
       "        9.528e-01, 5.012e-01, 9.524e-01, 9.508e-01, 5.472e-01, 9.588e-01,\n",
       "        9.544e-01, 5.464e-01, 9.592e-01, 9.548e-01, 2.624e-01, 9.556e-01,\n",
       "        9.524e-01, 4.624e-01, 9.560e-01, 9.580e-01, 1.328e-01, 9.584e-01,\n",
       "        9.612e-01, 9.628e-01, 9.616e-01, 9.648e-01, 9.648e-01, 9.636e-01,\n",
       "        9.652e-01, 9.664e-01, 9.648e-01, 9.652e-01, 9.664e-01, 9.672e-01,\n",
       "        9.620e-01, 9.628e-01, 9.596e-01, 9.648e-01, 9.648e-01, 9.648e-01,\n",
       "        9.656e-01, 9.652e-01, 9.652e-01, 9.672e-01, 9.652e-01, 9.660e-01,\n",
       "        9.612e-01, 9.620e-01, 9.616e-01, 9.648e-01, 9.648e-01, 9.644e-01,\n",
       "        9.648e-01, 9.656e-01, 9.652e-01, 9.664e-01, 9.672e-01, 9.652e-01,\n",
       "        9.588e-01, 9.612e-01, 9.596e-01, 9.616e-01, 9.620e-01, 9.628e-01,\n",
       "        9.628e-01, 9.632e-01, 9.632e-01, 9.640e-01, 9.636e-01, 9.644e-01,\n",
       "        9.688e-01, 9.792e-01, 9.792e-01, 9.824e-01, 9.816e-01, 9.804e-01,\n",
       "        9.844e-01, 9.856e-01, 9.844e-01, 9.820e-01, 9.856e-01, 9.864e-01,\n",
       "        9.804e-01, 9.780e-01, 9.792e-01, 9.812e-01, 9.832e-01, 9.828e-01,\n",
       "        9.808e-01, 9.844e-01, 9.840e-01, 9.868e-01, 9.844e-01, 9.860e-01,\n",
       "        9.784e-01, 9.756e-01, 9.780e-01, 9.836e-01, 9.816e-01, 9.824e-01,\n",
       "        9.832e-01, 9.836e-01, 9.860e-01, 9.852e-01, 9.852e-01, 9.868e-01,\n",
       "        9.688e-01, 9.780e-01, 9.668e-01, 9.792e-01, 9.804e-01, 9.808e-01,\n",
       "        9.836e-01, 9.820e-01, 9.820e-01, 9.828e-01, 9.852e-01, 9.836e-01]),\n",
       " 'mean_test_recall_micro': array([0.9948, 0.9952, 0.9956, 0.995 , 0.995 , 0.996 , 0.9952, 0.9962,\n",
       "        0.9936, 0.995 , 0.9964, 0.9938, 0.9942, 0.9956, 0.9962, 0.9964,\n",
       "        0.9928, 0.9944, 0.9956, 0.9952, 0.9846, 0.9968, 0.9956, 0.9952,\n",
       "        0.9904, 0.9918, 0.991 , 0.9942, 0.9932, 0.9958, 0.9938, 0.993 ,\n",
       "        0.9942, 0.9884, 0.991 , 0.9914, 0.987 , 0.9868, 0.987 , 0.9868,\n",
       "        0.9864, 0.9862, 0.9864, 0.9864, 0.9862, 0.9864, 0.9862, 0.9862,\n",
       "        0.9962, 0.9966, 0.9936, 0.9914, 0.9934, 0.9952, 0.9922, 0.992 ,\n",
       "        0.9944, 0.993 , 0.9944, 0.988 , 0.9964, 0.9962, 0.996 , 0.9932,\n",
       "        0.9956, 0.995 , 0.9958, 0.9944, 0.9948, 0.9944, 0.9952, 0.9926,\n",
       "        0.9958, 0.993 , 0.9918, 0.9956, 0.993 , 0.992 , 0.995 , 0.9962,\n",
       "        0.9954, 0.9942, 0.9948, 0.9926, 0.9872, 0.988 , 0.9872, 0.9868,\n",
       "        0.9872, 0.9868, 0.9872, 0.9864, 0.9868, 0.9866, 0.987 , 0.988 ,\n",
       "        0.862 , 0.402 , 0.8692, 0.9512, 0.284 , 0.9462, 0.9466, 0.375 ,\n",
       "        0.944 , 0.9416, 0.4094, 0.9426, 0.8732, 0.2108, 0.8654, 0.955 ,\n",
       "        0.488 , 0.9452, 0.9434, 0.222 , 0.942 , 0.9412, 0.6478, 0.9412,\n",
       "        0.8932, 0.6384, 0.8932, 0.86  , 0.3842, 0.9438, 0.9426, 0.1034,\n",
       "        0.9458, 0.9406, 0.575 , 0.9426, 0.952 , 0.2892, 0.9658, 0.9604,\n",
       "        0.3828, 0.955 , 0.948 , 0.3488, 0.9476, 0.9426, 0.4898, 0.9468,\n",
       "        0.9424, 0.2728, 0.9408, 0.9424, 0.235 , 0.9444, 0.94  , 0.564 ,\n",
       "        0.9426, 0.9436, 0.4098, 0.9454, 0.9446, 0.0816, 0.9434, 0.9448,\n",
       "        0.3408, 0.9454, 0.9424, 0.4794, 0.9448, 0.9458, 0.4812, 0.946 ,\n",
       "        0.9472, 0.2974, 0.9434, 0.945 , 0.2724, 0.946 , 0.9432, 0.4462,\n",
       "        0.9438, 0.9432, 0.388 , 0.946 , 0.943 , 0.549 , 0.9456, 0.9454,\n",
       "        0.4316, 0.943 , 0.9434, 0.5258, 0.9456, 0.9456, 0.4186, 0.9462,\n",
       "        0.9508, 0.9508, 0.9524, 0.9544, 0.9562, 0.9548, 0.958 , 0.9582,\n",
       "        0.957 , 0.9586, 0.9588, 0.9594, 0.9508, 0.953 , 0.9508, 0.956 ,\n",
       "        0.957 , 0.9556, 0.957 , 0.9574, 0.9568, 0.9588, 0.958 , 0.9578,\n",
       "        0.95  , 0.9514, 0.9516, 0.9558, 0.955 , 0.9546, 0.958 , 0.9566,\n",
       "        0.9574, 0.9578, 0.9594, 0.9578, 0.9476, 0.9494, 0.9488, 0.9524,\n",
       "        0.9516, 0.953 , 0.9538, 0.9534, 0.9532, 0.9546, 0.9548, 0.955 ,\n",
       "        0.969 , 0.973 , 0.9724, 0.9764, 0.9764, 0.9758, 0.9788, 0.9794,\n",
       "        0.9784, 0.9778, 0.9794, 0.9808, 0.9674, 0.9718, 0.9678, 0.9762,\n",
       "        0.9772, 0.9754, 0.9756, 0.9784, 0.9776, 0.9806, 0.9788, 0.9798,\n",
       "        0.9728, 0.9642, 0.9736, 0.9772, 0.9752, 0.9758, 0.978 , 0.9774,\n",
       "        0.9792, 0.9792, 0.9812, 0.9808, 0.9606, 0.9654, 0.9584, 0.9672,\n",
       "        0.9738, 0.9744, 0.9764, 0.9756, 0.975 , 0.9776, 0.9778, 0.9784]),\n",
       " 'std_test_recall_micro': array([2.000e-03, 1.600e-03, 4.000e-04, 2.200e-03, 6.000e-04, 1.200e-03,\n",
       "        8.000e-04, 6.000e-04, 1.600e-03, 1.400e-03, 8.000e-04, 2.000e-04,\n",
       "        1.400e-03, 4.000e-04, 6.000e-04, 4.000e-04, 2.000e-03, 4.000e-04,\n",
       "        8.000e-04, 1.200e-03, 9.000e-03, 4.000e-04, 0.000e+00, 0.000e+00,\n",
       "        5.600e-03, 1.800e-03, 1.000e-03, 1.400e-03, 2.800e-03, 6.000e-04,\n",
       "        2.000e-04, 2.600e-03, 1.400e-03, 0.000e+00, 1.400e-03, 1.000e-03,\n",
       "        3.000e-03, 3.600e-03, 3.000e-03, 2.800e-03, 2.800e-03, 2.600e-03,\n",
       "        2.800e-03, 2.800e-03, 2.600e-03, 2.800e-03, 2.600e-03, 2.600e-03,\n",
       "        1.000e-03, 6.000e-04, 2.000e-03, 3.000e-03, 3.800e-03, 0.000e+00,\n",
       "        3.400e-03, 1.600e-03, 4.000e-04, 1.000e-03, 8.000e-04, 1.600e-03,\n",
       "        4.000e-04, 6.000e-04, 8.000e-04, 2.400e-03, 8.000e-04, 1.800e-03,\n",
       "        2.000e-04, 2.400e-03, 4.000e-04, 1.200e-03, 1.600e-03, 2.200e-03,\n",
       "        1.000e-03, 2.600e-03, 1.000e-03, 0.000e+00, 1.400e-03, 2.000e-03,\n",
       "        2.000e-04, 1.000e-03, 6.000e-04, 1.400e-03, 8.000e-04, 2.600e-03,\n",
       "        2.400e-03, 3.600e-03, 2.800e-03, 2.400e-03, 2.400e-03, 2.800e-03,\n",
       "        2.400e-03, 2.400e-03, 3.200e-03, 3.000e-03, 2.200e-03, 2.000e-03,\n",
       "        9.280e-02, 1.756e-01, 7.800e-02, 1.520e-02, 2.832e-01, 5.400e-03,\n",
       "        1.180e-02, 1.974e-01, 1.320e-02, 1.160e-02, 1.590e-01, 1.460e-02,\n",
       "        8.080e-02, 1.600e-02, 1.034e-01, 1.420e-02, 9.000e-02, 1.160e-02,\n",
       "        1.380e-02, 2.720e-02, 9.200e-03, 8.000e-03, 6.220e-02, 8.000e-03,\n",
       "        5.640e-02, 6.080e-02, 5.280e-02, 8.040e-02, 1.894e-01, 1.220e-02,\n",
       "        1.340e-02, 1.014e-01, 1.100e-02, 1.340e-02, 2.600e-03, 1.500e-02,\n",
       "        9.200e-03, 2.884e-01, 7.400e-03, 1.040e-02, 2.772e-01, 7.400e-03,\n",
       "        1.600e-02, 2.288e-01, 4.000e-03, 9.400e-03, 8.780e-02, 1.200e-02,\n",
       "        9.200e-03, 2.636e-01, 7.600e-03, 8.400e-03, 4.180e-02, 1.200e-02,\n",
       "        8.000e-03, 5.520e-02, 1.060e-02, 9.600e-03, 1.586e-01, 1.140e-02,\n",
       "        7.800e-03, 3.600e-02, 9.000e-03, 1.160e-02, 1.320e-02, 1.220e-02,\n",
       "        8.000e-03, 1.506e-01, 1.360e-02, 1.140e-02, 8.960e-02, 1.240e-02,\n",
       "        1.280e-02, 4.620e-02, 8.200e-03, 9.800e-03, 3.200e-03, 1.080e-02,\n",
       "        9.600e-03, 5.500e-02, 8.600e-03, 7.600e-03, 1.592e-01, 1.280e-02,\n",
       "        1.140e-02, 2.600e-03, 1.360e-02, 9.400e-03, 1.692e-01, 1.260e-02,\n",
       "        9.000e-03, 6.340e-02, 1.040e-02, 1.240e-02, 2.858e-01, 1.220e-02,\n",
       "        1.040e-02, 1.200e-02, 9.200e-03, 1.040e-02, 8.600e-03, 8.800e-03,\n",
       "        7.200e-03, 8.200e-03, 7.800e-03, 6.600e-03, 7.600e-03, 7.800e-03,\n",
       "        1.120e-02, 9.800e-03, 8.800e-03, 8.800e-03, 7.800e-03, 9.200e-03,\n",
       "        8.600e-03, 7.800e-03, 8.400e-03, 8.400e-03, 7.200e-03, 8.200e-03,\n",
       "        1.120e-02, 1.060e-02, 1.000e-02, 9.000e-03, 9.800e-03, 9.800e-03,\n",
       "        6.800e-03, 9.000e-03, 7.800e-03, 8.600e-03, 7.800e-03, 7.400e-03,\n",
       "        1.120e-02, 1.180e-02, 1.080e-02, 9.200e-03, 1.040e-02, 9.800e-03,\n",
       "        9.000e-03, 9.800e-03, 1.000e-02, 9.400e-03, 8.800e-03, 9.400e-03,\n",
       "        2.000e-04, 6.200e-03, 6.800e-03, 6.000e-03, 5.200e-03, 4.600e-03,\n",
       "        5.600e-03, 6.200e-03, 6.000e-03, 4.200e-03, 6.200e-03, 5.600e-03,\n",
       "        1.300e-02, 6.200e-03, 1.140e-02, 5.000e-03, 6.000e-03, 7.400e-03,\n",
       "        5.200e-03, 6.000e-03, 6.400e-03, 6.200e-03, 5.600e-03, 6.200e-03,\n",
       "        5.600e-03, 1.140e-02, 4.400e-03, 6.400e-03, 6.400e-03, 6.600e-03,\n",
       "        5.200e-03, 6.200e-03, 6.800e-03, 6.000e-03, 4.000e-03, 6.000e-03,\n",
       "        8.200e-03, 1.260e-02, 8.400e-03, 1.200e-02, 6.600e-03, 6.400e-03,\n",
       "        7.200e-03, 6.400e-03, 7.000e-03, 5.200e-03, 7.400e-03, 5.200e-03]),\n",
       " 'rank_test_recall_micro': array([ 34,  23,  16,  29,  29,  11,  25,   6,  48,  29,   3,  46,  42,\n",
       "         16,   6,   3,  57,  37,  16,  25,  96,   1,  16,  25,  69,  63,\n",
       "         67,  42,  52,  13,  46,  53,  42,  70,  67,  65,  78,  85,  78,\n",
       "         81,  87,  92,  87,  87,  92,  87,  92,  92,   6,   2,  48,  65,\n",
       "         50,  25,  60,  61,  37,  53,  37,  71,   3,   6,  11,  51,  16,\n",
       "         29,  13,  37,  36,  37,  23,  58,  13,  53,  63,  16,  53,  61,\n",
       "         29,   6,  22,  42,  34,  58,  74,  71,  74,  81,  74,  81,  74,\n",
       "         87,  81,  86,  78,  71, 255, 272, 253, 190, 281, 205, 203, 276,\n",
       "        223, 244, 271, 235, 252, 286, 254, 173, 264, 217, 227, 285, 243,\n",
       "        245, 257, 245, 250, 258, 250, 256, 274, 224, 237, 287, 209, 248,\n",
       "        259, 237, 186, 280, 141, 145, 275, 170, 198, 277, 199, 237, 263,\n",
       "        202, 240, 282, 247, 240, 284, 222, 249, 260, 235, 226, 270, 214,\n",
       "        221, 288, 227, 219, 278, 214, 240, 266, 219, 209, 265, 206, 201,\n",
       "        279, 227, 218, 283, 206, 231, 267, 224, 231, 273, 206, 233, 261,\n",
       "        211, 214, 268, 233, 227, 262, 211, 211, 269, 204, 191, 192, 184,\n",
       "        178, 166, 174, 153, 152, 161, 150, 148, 146, 192, 182, 192, 167,\n",
       "        161, 169, 161, 159, 164, 149, 153, 156, 195, 189, 187, 168, 170,\n",
       "        176, 153, 165, 159, 156, 146, 156, 199, 196, 197, 184, 187, 182,\n",
       "        179, 180, 181, 176, 174, 170, 137, 133, 135, 119, 120, 123, 106,\n",
       "        102, 108, 112, 102,  98, 139, 136, 138, 122, 118, 127, 125, 108,\n",
       "        114, 100, 106, 101, 134, 143, 132, 117, 128, 123, 111, 116, 104,\n",
       "        105,  97,  98, 144, 142, 151, 140, 131, 130, 120, 125, 129, 114,\n",
       "        112, 110]),\n",
       " 'split0_test_f1_micro': array([0.9928, 0.9936, 0.9952, 0.9928, 0.9956, 0.9948, 0.996 , 0.9956,\n",
       "        0.9952, 0.9964, 0.9956, 0.994 , 0.9928, 0.9952, 0.9956, 0.996 ,\n",
       "        0.9948, 0.9948, 0.9964, 0.994 , 0.9756, 0.9972, 0.9956, 0.9952,\n",
       "        0.9848, 0.9936, 0.992 , 0.9928, 0.996 , 0.9964, 0.9936, 0.9904,\n",
       "        0.9956, 0.9884, 0.9924, 0.9924, 0.984 , 0.9832, 0.984 , 0.984 ,\n",
       "        0.9836, 0.9836, 0.9836, 0.9836, 0.9836, 0.9836, 0.9836, 0.9836,\n",
       "        0.9952, 0.996 , 0.9956, 0.9944, 0.9896, 0.9952, 0.9956, 0.9936,\n",
       "        0.9948, 0.994 , 0.9952, 0.9864, 0.996 , 0.9956, 0.9952, 0.9956,\n",
       "        0.9948, 0.9932, 0.996 , 0.992 , 0.9944, 0.9956, 0.9936, 0.9948,\n",
       "        0.9948, 0.9956, 0.9928, 0.9956, 0.9944, 0.994 , 0.9948, 0.9952,\n",
       "        0.9948, 0.9928, 0.9956, 0.9952, 0.9848, 0.9844, 0.9844, 0.9844,\n",
       "        0.9848, 0.984 , 0.9848, 0.984 , 0.9836, 0.9836, 0.9848, 0.986 ,\n",
       "        0.7692, 0.2264, 0.9472, 0.936 , 0.5672, 0.9408, 0.9348, 0.5724,\n",
       "        0.9308, 0.93  , 0.5684, 0.928 , 0.954 , 0.1948, 0.762 , 0.9408,\n",
       "        0.578 , 0.9336, 0.9296, 0.2492, 0.9328, 0.9332, 0.5856, 0.9332,\n",
       "        0.9496, 0.6992, 0.946 , 0.9404, 0.5736, 0.9316, 0.9292, 0.2048,\n",
       "        0.9348, 0.9272, 0.5724, 0.9276, 0.9428, 0.5776, 0.9584, 0.95  ,\n",
       "        0.1056, 0.9476, 0.932 , 0.12  , 0.9436, 0.9332, 0.402 , 0.9348,\n",
       "        0.9332, 0.5364, 0.9332, 0.934 , 0.2768, 0.9324, 0.932 , 0.5088,\n",
       "        0.932 , 0.934 , 0.2512, 0.934 , 0.9368, 0.1176, 0.9344, 0.9332,\n",
       "        0.3276, 0.9332, 0.9344, 0.3288, 0.9312, 0.9344, 0.5708, 0.9336,\n",
       "        0.9344, 0.2512, 0.9352, 0.9352, 0.2692, 0.9352, 0.9336, 0.3912,\n",
       "        0.9352, 0.9356, 0.2288, 0.9332, 0.9316, 0.5516, 0.932 , 0.936 ,\n",
       "        0.6008, 0.9304, 0.9344, 0.5892, 0.9352, 0.9332, 0.7044, 0.934 ,\n",
       "        0.9404, 0.9388, 0.9432, 0.944 , 0.9476, 0.946 , 0.9508, 0.95  ,\n",
       "        0.9492, 0.952 , 0.9512, 0.9516, 0.9396, 0.9432, 0.942 , 0.9472,\n",
       "        0.9492, 0.9464, 0.9484, 0.9496, 0.9484, 0.9504, 0.9508, 0.9496,\n",
       "        0.9388, 0.9408, 0.9416, 0.9468, 0.9452, 0.9448, 0.9512, 0.9476,\n",
       "        0.9496, 0.9492, 0.9516, 0.9504, 0.9364, 0.9376, 0.938 , 0.9432,\n",
       "        0.9412, 0.9432, 0.9448, 0.9436, 0.9432, 0.9452, 0.946 , 0.9456,\n",
       "        0.9692, 0.9668, 0.9656, 0.9704, 0.9712, 0.9712, 0.9732, 0.9732,\n",
       "        0.9724, 0.9736, 0.9732, 0.9752, 0.9544, 0.9656, 0.9564, 0.9712,\n",
       "        0.9712, 0.968 , 0.9704, 0.9724, 0.9712, 0.9744, 0.9732, 0.9736,\n",
       "        0.9672, 0.9528, 0.9692, 0.9708, 0.9688, 0.9692, 0.9728, 0.9712,\n",
       "        0.9724, 0.9732, 0.9772, 0.9748, 0.9524, 0.9528, 0.95  , 0.9552,\n",
       "        0.9672, 0.968 , 0.9692, 0.9692, 0.968 , 0.9724, 0.9704, 0.9732]),\n",
       " 'split1_test_f1_micro': array([9.968e-01, 9.968e-01, 9.960e-01, 9.972e-01, 9.944e-01, 9.972e-01,\n",
       "        9.944e-01, 9.968e-01, 9.920e-01, 9.936e-01, 9.972e-01, 9.936e-01,\n",
       "        9.956e-01, 9.960e-01, 9.968e-01, 9.968e-01, 9.908e-01, 9.940e-01,\n",
       "        9.948e-01, 9.964e-01, 9.936e-01, 9.964e-01, 9.956e-01, 9.952e-01,\n",
       "        9.960e-01, 9.900e-01, 9.900e-01, 9.956e-01, 9.904e-01, 9.952e-01,\n",
       "        9.940e-01, 9.956e-01, 9.928e-01, 9.884e-01, 9.896e-01, 9.904e-01,\n",
       "        9.900e-01, 9.904e-01, 9.900e-01, 9.896e-01, 9.892e-01, 9.888e-01,\n",
       "        9.892e-01, 9.892e-01, 9.888e-01, 9.892e-01, 9.888e-01, 9.888e-01,\n",
       "        9.972e-01, 9.972e-01, 9.916e-01, 9.884e-01, 9.972e-01, 9.952e-01,\n",
       "        9.888e-01, 9.904e-01, 9.940e-01, 9.920e-01, 9.936e-01, 9.896e-01,\n",
       "        9.968e-01, 9.968e-01, 9.968e-01, 9.908e-01, 9.964e-01, 9.968e-01,\n",
       "        9.956e-01, 9.968e-01, 9.952e-01, 9.932e-01, 9.968e-01, 9.904e-01,\n",
       "        9.968e-01, 9.904e-01, 9.908e-01, 9.956e-01, 9.916e-01, 9.900e-01,\n",
       "        9.952e-01, 9.972e-01, 9.960e-01, 9.956e-01, 9.940e-01, 9.900e-01,\n",
       "        9.896e-01, 9.916e-01, 9.900e-01, 9.892e-01, 9.896e-01, 9.896e-01,\n",
       "        9.896e-01, 9.888e-01, 9.900e-01, 9.896e-01, 9.892e-01, 9.900e-01,\n",
       "        9.548e-01, 5.776e-01, 7.912e-01, 9.664e-01, 8.000e-04, 9.516e-01,\n",
       "        9.584e-01, 1.776e-01, 9.572e-01, 9.532e-01, 2.504e-01, 9.572e-01,\n",
       "        7.924e-01, 2.268e-01, 9.688e-01, 9.692e-01, 3.980e-01, 9.568e-01,\n",
       "        9.572e-01, 1.948e-01, 9.512e-01, 9.492e-01, 7.100e-01, 9.492e-01,\n",
       "        8.368e-01, 5.776e-01, 8.404e-01, 7.796e-01, 1.948e-01, 9.560e-01,\n",
       "        9.560e-01, 2.000e-03, 9.568e-01, 9.540e-01, 5.776e-01, 9.576e-01,\n",
       "        9.612e-01, 8.000e-04, 9.732e-01, 9.708e-01, 6.600e-01, 9.624e-01,\n",
       "        9.640e-01, 5.776e-01, 9.516e-01, 9.520e-01, 5.776e-01, 9.588e-01,\n",
       "        9.516e-01, 9.200e-03, 9.484e-01, 9.508e-01, 1.932e-01, 9.564e-01,\n",
       "        9.480e-01, 6.192e-01, 9.532e-01, 9.532e-01, 5.684e-01, 9.568e-01,\n",
       "        9.524e-01, 4.560e-02, 9.524e-01, 9.564e-01, 3.540e-01, 9.576e-01,\n",
       "        9.504e-01, 6.300e-01, 9.584e-01, 9.572e-01, 3.916e-01, 9.584e-01,\n",
       "        9.600e-01, 3.436e-01, 9.516e-01, 9.548e-01, 2.756e-01, 9.568e-01,\n",
       "        9.528e-01, 5.012e-01, 9.524e-01, 9.508e-01, 5.472e-01, 9.588e-01,\n",
       "        9.544e-01, 5.464e-01, 9.592e-01, 9.548e-01, 2.624e-01, 9.556e-01,\n",
       "        9.524e-01, 4.624e-01, 9.560e-01, 9.580e-01, 1.328e-01, 9.584e-01,\n",
       "        9.612e-01, 9.628e-01, 9.616e-01, 9.648e-01, 9.648e-01, 9.636e-01,\n",
       "        9.652e-01, 9.664e-01, 9.648e-01, 9.652e-01, 9.664e-01, 9.672e-01,\n",
       "        9.620e-01, 9.628e-01, 9.596e-01, 9.648e-01, 9.648e-01, 9.648e-01,\n",
       "        9.656e-01, 9.652e-01, 9.652e-01, 9.672e-01, 9.652e-01, 9.660e-01,\n",
       "        9.612e-01, 9.620e-01, 9.616e-01, 9.648e-01, 9.648e-01, 9.644e-01,\n",
       "        9.648e-01, 9.656e-01, 9.652e-01, 9.664e-01, 9.672e-01, 9.652e-01,\n",
       "        9.588e-01, 9.612e-01, 9.596e-01, 9.616e-01, 9.620e-01, 9.628e-01,\n",
       "        9.628e-01, 9.632e-01, 9.632e-01, 9.640e-01, 9.636e-01, 9.644e-01,\n",
       "        9.688e-01, 9.792e-01, 9.792e-01, 9.824e-01, 9.816e-01, 9.804e-01,\n",
       "        9.844e-01, 9.856e-01, 9.844e-01, 9.820e-01, 9.856e-01, 9.864e-01,\n",
       "        9.804e-01, 9.780e-01, 9.792e-01, 9.812e-01, 9.832e-01, 9.828e-01,\n",
       "        9.808e-01, 9.844e-01, 9.840e-01, 9.868e-01, 9.844e-01, 9.860e-01,\n",
       "        9.784e-01, 9.756e-01, 9.780e-01, 9.836e-01, 9.816e-01, 9.824e-01,\n",
       "        9.832e-01, 9.836e-01, 9.860e-01, 9.852e-01, 9.852e-01, 9.868e-01,\n",
       "        9.688e-01, 9.780e-01, 9.668e-01, 9.792e-01, 9.804e-01, 9.808e-01,\n",
       "        9.836e-01, 9.820e-01, 9.820e-01, 9.828e-01, 9.852e-01, 9.836e-01]),\n",
       " 'mean_test_f1_micro': array([0.9948, 0.9952, 0.9956, 0.995 , 0.995 , 0.996 , 0.9952, 0.9962,\n",
       "        0.9936, 0.995 , 0.9964, 0.9938, 0.9942, 0.9956, 0.9962, 0.9964,\n",
       "        0.9928, 0.9944, 0.9956, 0.9952, 0.9846, 0.9968, 0.9956, 0.9952,\n",
       "        0.9904, 0.9918, 0.991 , 0.9942, 0.9932, 0.9958, 0.9938, 0.993 ,\n",
       "        0.9942, 0.9884, 0.991 , 0.9914, 0.987 , 0.9868, 0.987 , 0.9868,\n",
       "        0.9864, 0.9862, 0.9864, 0.9864, 0.9862, 0.9864, 0.9862, 0.9862,\n",
       "        0.9962, 0.9966, 0.9936, 0.9914, 0.9934, 0.9952, 0.9922, 0.992 ,\n",
       "        0.9944, 0.993 , 0.9944, 0.988 , 0.9964, 0.9962, 0.996 , 0.9932,\n",
       "        0.9956, 0.995 , 0.9958, 0.9944, 0.9948, 0.9944, 0.9952, 0.9926,\n",
       "        0.9958, 0.993 , 0.9918, 0.9956, 0.993 , 0.992 , 0.995 , 0.9962,\n",
       "        0.9954, 0.9942, 0.9948, 0.9926, 0.9872, 0.988 , 0.9872, 0.9868,\n",
       "        0.9872, 0.9868, 0.9872, 0.9864, 0.9868, 0.9866, 0.987 , 0.988 ,\n",
       "        0.862 , 0.402 , 0.8692, 0.9512, 0.284 , 0.9462, 0.9466, 0.375 ,\n",
       "        0.944 , 0.9416, 0.4094, 0.9426, 0.8732, 0.2108, 0.8654, 0.955 ,\n",
       "        0.488 , 0.9452, 0.9434, 0.222 , 0.942 , 0.9412, 0.6478, 0.9412,\n",
       "        0.8932, 0.6384, 0.8932, 0.86  , 0.3842, 0.9438, 0.9426, 0.1034,\n",
       "        0.9458, 0.9406, 0.575 , 0.9426, 0.952 , 0.2892, 0.9658, 0.9604,\n",
       "        0.3828, 0.955 , 0.948 , 0.3488, 0.9476, 0.9426, 0.4898, 0.9468,\n",
       "        0.9424, 0.2728, 0.9408, 0.9424, 0.235 , 0.9444, 0.94  , 0.564 ,\n",
       "        0.9426, 0.9436, 0.4098, 0.9454, 0.9446, 0.0816, 0.9434, 0.9448,\n",
       "        0.3408, 0.9454, 0.9424, 0.4794, 0.9448, 0.9458, 0.4812, 0.946 ,\n",
       "        0.9472, 0.2974, 0.9434, 0.945 , 0.2724, 0.946 , 0.9432, 0.4462,\n",
       "        0.9438, 0.9432, 0.388 , 0.946 , 0.943 , 0.549 , 0.9456, 0.9454,\n",
       "        0.4316, 0.943 , 0.9434, 0.5258, 0.9456, 0.9456, 0.4186, 0.9462,\n",
       "        0.9508, 0.9508, 0.9524, 0.9544, 0.9562, 0.9548, 0.958 , 0.9582,\n",
       "        0.957 , 0.9586, 0.9588, 0.9594, 0.9508, 0.953 , 0.9508, 0.956 ,\n",
       "        0.957 , 0.9556, 0.957 , 0.9574, 0.9568, 0.9588, 0.958 , 0.9578,\n",
       "        0.95  , 0.9514, 0.9516, 0.9558, 0.955 , 0.9546, 0.958 , 0.9566,\n",
       "        0.9574, 0.9578, 0.9594, 0.9578, 0.9476, 0.9494, 0.9488, 0.9524,\n",
       "        0.9516, 0.953 , 0.9538, 0.9534, 0.9532, 0.9546, 0.9548, 0.955 ,\n",
       "        0.969 , 0.973 , 0.9724, 0.9764, 0.9764, 0.9758, 0.9788, 0.9794,\n",
       "        0.9784, 0.9778, 0.9794, 0.9808, 0.9674, 0.9718, 0.9678, 0.9762,\n",
       "        0.9772, 0.9754, 0.9756, 0.9784, 0.9776, 0.9806, 0.9788, 0.9798,\n",
       "        0.9728, 0.9642, 0.9736, 0.9772, 0.9752, 0.9758, 0.978 , 0.9774,\n",
       "        0.9792, 0.9792, 0.9812, 0.9808, 0.9606, 0.9654, 0.9584, 0.9672,\n",
       "        0.9738, 0.9744, 0.9764, 0.9756, 0.975 , 0.9776, 0.9778, 0.9784]),\n",
       " 'std_test_f1_micro': array([2.000e-03, 1.600e-03, 4.000e-04, 2.200e-03, 6.000e-04, 1.200e-03,\n",
       "        8.000e-04, 6.000e-04, 1.600e-03, 1.400e-03, 8.000e-04, 2.000e-04,\n",
       "        1.400e-03, 4.000e-04, 6.000e-04, 4.000e-04, 2.000e-03, 4.000e-04,\n",
       "        8.000e-04, 1.200e-03, 9.000e-03, 4.000e-04, 0.000e+00, 0.000e+00,\n",
       "        5.600e-03, 1.800e-03, 1.000e-03, 1.400e-03, 2.800e-03, 6.000e-04,\n",
       "        2.000e-04, 2.600e-03, 1.400e-03, 0.000e+00, 1.400e-03, 1.000e-03,\n",
       "        3.000e-03, 3.600e-03, 3.000e-03, 2.800e-03, 2.800e-03, 2.600e-03,\n",
       "        2.800e-03, 2.800e-03, 2.600e-03, 2.800e-03, 2.600e-03, 2.600e-03,\n",
       "        1.000e-03, 6.000e-04, 2.000e-03, 3.000e-03, 3.800e-03, 0.000e+00,\n",
       "        3.400e-03, 1.600e-03, 4.000e-04, 1.000e-03, 8.000e-04, 1.600e-03,\n",
       "        4.000e-04, 6.000e-04, 8.000e-04, 2.400e-03, 8.000e-04, 1.800e-03,\n",
       "        2.000e-04, 2.400e-03, 4.000e-04, 1.200e-03, 1.600e-03, 2.200e-03,\n",
       "        1.000e-03, 2.600e-03, 1.000e-03, 0.000e+00, 1.400e-03, 2.000e-03,\n",
       "        2.000e-04, 1.000e-03, 6.000e-04, 1.400e-03, 8.000e-04, 2.600e-03,\n",
       "        2.400e-03, 3.600e-03, 2.800e-03, 2.400e-03, 2.400e-03, 2.800e-03,\n",
       "        2.400e-03, 2.400e-03, 3.200e-03, 3.000e-03, 2.200e-03, 2.000e-03,\n",
       "        9.280e-02, 1.756e-01, 7.800e-02, 1.520e-02, 2.832e-01, 5.400e-03,\n",
       "        1.180e-02, 1.974e-01, 1.320e-02, 1.160e-02, 1.590e-01, 1.460e-02,\n",
       "        8.080e-02, 1.600e-02, 1.034e-01, 1.420e-02, 9.000e-02, 1.160e-02,\n",
       "        1.380e-02, 2.720e-02, 9.200e-03, 8.000e-03, 6.220e-02, 8.000e-03,\n",
       "        5.640e-02, 6.080e-02, 5.280e-02, 8.040e-02, 1.894e-01, 1.220e-02,\n",
       "        1.340e-02, 1.014e-01, 1.100e-02, 1.340e-02, 2.600e-03, 1.500e-02,\n",
       "        9.200e-03, 2.884e-01, 7.400e-03, 1.040e-02, 2.772e-01, 7.400e-03,\n",
       "        1.600e-02, 2.288e-01, 4.000e-03, 9.400e-03, 8.780e-02, 1.200e-02,\n",
       "        9.200e-03, 2.636e-01, 7.600e-03, 8.400e-03, 4.180e-02, 1.200e-02,\n",
       "        8.000e-03, 5.520e-02, 1.060e-02, 9.600e-03, 1.586e-01, 1.140e-02,\n",
       "        7.800e-03, 3.600e-02, 9.000e-03, 1.160e-02, 1.320e-02, 1.220e-02,\n",
       "        8.000e-03, 1.506e-01, 1.360e-02, 1.140e-02, 8.960e-02, 1.240e-02,\n",
       "        1.280e-02, 4.620e-02, 8.200e-03, 9.800e-03, 3.200e-03, 1.080e-02,\n",
       "        9.600e-03, 5.500e-02, 8.600e-03, 7.600e-03, 1.592e-01, 1.280e-02,\n",
       "        1.140e-02, 2.600e-03, 1.360e-02, 9.400e-03, 1.692e-01, 1.260e-02,\n",
       "        9.000e-03, 6.340e-02, 1.040e-02, 1.240e-02, 2.858e-01, 1.220e-02,\n",
       "        1.040e-02, 1.200e-02, 9.200e-03, 1.040e-02, 8.600e-03, 8.800e-03,\n",
       "        7.200e-03, 8.200e-03, 7.800e-03, 6.600e-03, 7.600e-03, 7.800e-03,\n",
       "        1.120e-02, 9.800e-03, 8.800e-03, 8.800e-03, 7.800e-03, 9.200e-03,\n",
       "        8.600e-03, 7.800e-03, 8.400e-03, 8.400e-03, 7.200e-03, 8.200e-03,\n",
       "        1.120e-02, 1.060e-02, 1.000e-02, 9.000e-03, 9.800e-03, 9.800e-03,\n",
       "        6.800e-03, 9.000e-03, 7.800e-03, 8.600e-03, 7.800e-03, 7.400e-03,\n",
       "        1.120e-02, 1.180e-02, 1.080e-02, 9.200e-03, 1.040e-02, 9.800e-03,\n",
       "        9.000e-03, 9.800e-03, 1.000e-02, 9.400e-03, 8.800e-03, 9.400e-03,\n",
       "        2.000e-04, 6.200e-03, 6.800e-03, 6.000e-03, 5.200e-03, 4.600e-03,\n",
       "        5.600e-03, 6.200e-03, 6.000e-03, 4.200e-03, 6.200e-03, 5.600e-03,\n",
       "        1.300e-02, 6.200e-03, 1.140e-02, 5.000e-03, 6.000e-03, 7.400e-03,\n",
       "        5.200e-03, 6.000e-03, 6.400e-03, 6.200e-03, 5.600e-03, 6.200e-03,\n",
       "        5.600e-03, 1.140e-02, 4.400e-03, 6.400e-03, 6.400e-03, 6.600e-03,\n",
       "        5.200e-03, 6.200e-03, 6.800e-03, 6.000e-03, 4.000e-03, 6.000e-03,\n",
       "        8.200e-03, 1.260e-02, 8.400e-03, 1.200e-02, 6.600e-03, 6.400e-03,\n",
       "        7.200e-03, 6.400e-03, 7.000e-03, 5.200e-03, 7.400e-03, 5.200e-03]),\n",
       " 'rank_test_f1_micro': array([ 34,  23,  16,  29,  29,  11,  25,   6,  48,  29,   3,  46,  42,\n",
       "         16,   6,   3,  57,  37,  16,  25,  96,   1,  16,  25,  69,  63,\n",
       "         67,  42,  52,  13,  46,  53,  42,  70,  67,  65,  78,  85,  78,\n",
       "         81,  87,  92,  87,  87,  92,  87,  92,  92,   6,   2,  48,  65,\n",
       "         50,  25,  60,  61,  37,  53,  37,  71,   3,   6,  11,  51,  16,\n",
       "         29,  13,  37,  36,  37,  23,  58,  13,  53,  63,  16,  53,  61,\n",
       "         29,   6,  22,  42,  34,  58,  74,  71,  74,  81,  74,  81,  74,\n",
       "         87,  81,  86,  78,  71, 255, 272, 253, 190, 281, 204, 203, 276,\n",
       "        223, 244, 271, 235, 252, 286, 254, 170, 264, 217, 227, 285, 243,\n",
       "        245, 257, 245, 250, 258, 250, 256, 274, 224, 237, 287, 209, 248,\n",
       "        259, 237, 186, 280, 141, 145, 275, 170, 198, 277, 199, 237, 263,\n",
       "        202, 240, 282, 247, 240, 284, 222, 249, 260, 235, 226, 270, 214,\n",
       "        221, 288, 227, 219, 278, 214, 240, 266, 219, 209, 265, 206, 201,\n",
       "        279, 227, 218, 283, 206, 231, 267, 224, 231, 273, 206, 233, 261,\n",
       "        211, 214, 268, 233, 227, 262, 211, 211, 269, 204, 191, 192, 184,\n",
       "        178, 166, 174, 153, 152, 161, 150, 148, 146, 192, 182, 192, 167,\n",
       "        161, 169, 161, 159, 164, 149, 153, 156, 195, 189, 187, 168, 170,\n",
       "        176, 153, 165, 159, 156, 146, 156, 199, 196, 197, 184, 187, 182,\n",
       "        179, 180, 181, 176, 174, 170, 137, 133, 135, 119, 120, 123, 106,\n",
       "        102, 108, 112, 102,  98, 139, 136, 138, 122, 118, 127, 125, 108,\n",
       "        114, 100, 106, 101, 134, 143, 132, 117, 128, 123, 111, 116, 104,\n",
       "        105,  97,  98, 144, 142, 151, 140, 131, 130, 120, 125, 129, 114,\n",
       "        112, 110]),\n",
       " 'split0_test_roc_auc_ovo': array([0.92275783, 0.90730268, 0.92892364, 0.93863786, 0.92263666,\n",
       "        0.96334813, 0.92882941, 0.93195521, 0.87378574, 0.9181704 ,\n",
       "        0.86681963, 0.88596101, 0.93775327, 0.94721693, 0.92345445,\n",
       "        0.89623937, 0.89218759, 0.90358113, 0.90431846, 0.90342519,\n",
       "        0.86134127, 0.90793452, 0.94303862, 0.92193668, 0.90782013,\n",
       "        0.90520203, 0.89383395, 0.92247269, 0.87412023, 0.92161471,\n",
       "        0.78920572, 0.84990627, 0.79201566, 0.76065781, 0.78968823,\n",
       "        0.83535577, 0.89520121, 0.88369452, 0.89166846, 0.85934196,\n",
       "        0.85815944, 0.88383265, 0.88779956, 0.88670681, 0.87147479,\n",
       "        0.88740075, 0.87175355, 0.83710311, 0.91382011, 0.91031598,\n",
       "        0.91676066, 0.85381109, 0.883957  , 0.86045599, 0.92875922,\n",
       "        0.87846969, 0.88388059, 0.86210695, 0.86214971, 0.88936092,\n",
       "        0.92370414, 0.92507212, 0.90573428, 0.93459552, 0.94248102,\n",
       "        0.85206036, 0.87735181, 0.90108822, 0.84960193, 0.89880281,\n",
       "        0.90671811, 0.85188124, 0.92053495, 0.87021457, 0.91389946,\n",
       "        0.91892288, 0.83106717, 0.91162561, 0.87506522, 0.8747362 ,\n",
       "        0.83760287, 0.83679568, 0.84435229, 0.90570641, 0.83565801,\n",
       "        0.90759885, 0.89772576, 0.88276056, 0.83557958, 0.91999322,\n",
       "        0.83091301, 0.84383338, 0.82096037, 0.85157425, 0.8484669 ,\n",
       "        0.83636714, 0.84442071, 0.3223971 , 0.85105381, 0.78740802,\n",
       "        0.41029361, 0.85076868, 0.8031628 , 0.52895666, 0.81580317,\n",
       "        0.84832346, 0.55119395, 0.84654088, 0.84556913, 0.3689999 ,\n",
       "        0.88534233, 0.79583709, 0.70998975, 0.79641236, 0.84666168,\n",
       "        0.47324493, 0.83100652, 0.83418688, 0.70918775, 0.84654003,\n",
       "        0.86902122, 0.62573275, 0.84704694, 0.85095451, 0.5652148 ,\n",
       "        0.8463959 , 0.84360043, 0.46389546, 0.83958508, 0.84539954,\n",
       "        0.25633024, 0.83802686, 0.8169505 , 0.29499091, 0.79213416,\n",
       "        0.78079046, 0.32208018, 0.85256049, 0.8512089 , 0.48566911,\n",
       "        0.8541773 , 0.81104076, 0.38809293, 0.85139425, 0.82090103,\n",
       "        0.46633858, 0.79451505, 0.80937536, 0.61471587, 0.81952599,\n",
       "        0.81623545, 0.48288764, 0.84937716, 0.84885858, 0.56867024,\n",
       "        0.8428841 , 0.80387216, 0.58285887, 0.79008482, 0.83806374,\n",
       "        0.5593277 , 0.86401199, 0.84389837, 0.57138856, 0.84237798,\n",
       "        0.83599723, 0.5465512 , 0.83547728, 0.80909493, 0.51454475,\n",
       "        0.8313761 , 0.81107035, 0.3699157 , 0.7968035 , 0.84767331,\n",
       "        0.58283027, 0.84977288, 0.85020675, 0.76552661, 0.83463546,\n",
       "        0.88635104, 0.38522221, 0.78989791, 0.84662716, 0.63471497,\n",
       "        0.79912608, 0.79806449, 0.61259197, 0.84682784, 0.83953796,\n",
       "        0.75188645, 0.84808283, 0.79700191, 0.80346357, 0.82632033,\n",
       "        0.78592824, 0.79748532, 0.85022861, 0.81101016, 0.79573344,\n",
       "        0.78601574, 0.80669024, 0.7951924 , 0.85480314, 0.77168762,\n",
       "        0.80165872, 0.82733193, 0.80647279, 0.82484583, 0.81748259,\n",
       "        0.8357434 , 0.79990038, 0.78600073, 0.80945894, 0.82978344,\n",
       "        0.81139888, 0.80882   , 0.85575626, 0.7979782 , 0.80395011,\n",
       "        0.80687201, 0.79040569, 0.83569915, 0.82324305, 0.83801254,\n",
       "        0.76833796, 0.80130664, 0.80070592, 0.79747378, 0.78902202,\n",
       "        0.78037779, 0.79978036, 0.84371816, 0.8080309 , 0.80691818,\n",
       "        0.77994036, 0.80010723, 0.82224883, 0.84462755, 0.80921937,\n",
       "        0.82295953, 0.79454927, 0.85127514, 0.85281092, 0.85293683,\n",
       "        0.84676253, 0.89978071, 0.85591233, 0.93985578, 0.95550179,\n",
       "        0.94367737, 0.95200145, 0.90412248, 0.8534804 , 0.84271051,\n",
       "        0.8742343 , 0.83751653, 0.85837862, 0.83283989, 0.94018477,\n",
       "        0.88465334, 0.92463085, 0.93874919, 0.8781732 , 0.85481662,\n",
       "        0.84590667, 0.79768846, 0.85247967, 0.88769417, 0.8368642 ,\n",
       "        0.94368075, 0.89219475, 0.96099863, 0.92366153, 0.93126919,\n",
       "        0.91047223, 0.88946162, 0.83215522, 0.81115161, 0.82879358,\n",
       "        0.82714648, 0.86235553, 0.92547137, 0.92415284, 0.89223354,\n",
       "        0.95630397, 0.89283797, 0.95364725]),\n",
       " 'split1_test_roc_auc_ovo': array([0.92077161, 0.91906738, 0.94990003, 0.9009305 , 0.91821841,\n",
       "        0.88838538, 0.92163729, 0.9401197 , 0.93427115, 0.9761324 ,\n",
       "        0.93344238, 0.89396731, 0.91119561, 0.93466585, 0.93708822,\n",
       "        0.88177768, 0.91022766, 0.90161117, 0.93933247, 0.88603814,\n",
       "        0.88097412, 0.88296347, 0.90054718, 0.91233031, 0.81517935,\n",
       "        0.91054186, 0.88760384, 0.89077968, 0.88230884, 0.92813693,\n",
       "        0.8852961 , 0.9098579 , 0.91331558, 0.9494116 , 0.91489362,\n",
       "        0.85254972, 0.8987021 , 0.87599462, 0.92133742, 0.84325214,\n",
       "        0.8778944 , 0.8396491 , 0.85541585, 0.8648408 , 0.87429558,\n",
       "        0.87139237, 0.85925802, 0.86143192, 0.93406233, 0.92957906,\n",
       "        0.92641143, 0.90863413, 0.8982053 , 0.92012914, 0.91348381,\n",
       "        0.86505648, 0.91586671, 0.91883662, 0.89100541, 0.92656704,\n",
       "        0.9280442 , 0.93083571, 0.9455301 , 0.90167703, 0.9645249 ,\n",
       "        0.92486099, 0.85456707, 0.91910662, 0.90538918, 0.82853887,\n",
       "        0.95593221, 0.9012962 , 0.92335228, 0.88494662, 0.91204598,\n",
       "        0.88617031, 0.91669001, 0.90302569, 0.89383822, 0.92208602,\n",
       "        0.9305469 , 0.84191005, 0.96452968, 0.84801116, 0.89274903,\n",
       "        0.90092113, 0.88623924, 0.91472234, 0.85914672, 0.88175289,\n",
       "        0.86627356, 0.89153153, 0.89933475, 0.89540436, 0.90239308,\n",
       "        0.93008614, 0.77809016, 0.60266207, 0.76931269, 0.7639728 ,\n",
       "        0.4144728 , 0.79934954, 0.77917122, 0.5138885 , 0.79658046,\n",
       "        0.78136491, 0.73924808, 0.77576756, 0.7833474 , 0.58212479,\n",
       "        0.92058538, 0.84646294, 0.47012044, 0.77444218, 0.80549765,\n",
       "        0.54961789, 0.78334318, 0.79907165, 0.58110357, 0.77332903,\n",
       "        0.75625034, 0.67692447, 0.77878808, 0.79177828, 0.36654268,\n",
       "        0.84077616, 0.77350888, 0.56570449, 0.77742292, 0.77433481,\n",
       "        0.54088769, 0.78025844, 0.78694637, 0.42807682, 0.84297907,\n",
       "        0.7789876 , 0.70919797, 0.77754632, 0.78736683, 0.50870375,\n",
       "        0.78692241, 0.83508915, 0.66104674, 0.78692652, 0.79250019,\n",
       "        0.59947794, 0.79203548, 0.812761  , 0.32679768, 0.82694975,\n",
       "        0.7983145 , 0.83958556, 0.78365888, 0.78593386, 0.49591309,\n",
       "        0.78841291, 0.7923702 , 0.42299632, 0.80535706, 0.8477546 ,\n",
       "        0.52310374, 0.80597539, 0.80011866, 0.75994368, 0.79750191,\n",
       "        0.80035692, 0.56278479, 0.82117187, 0.8144362 , 0.48098818,\n",
       "        0.79922357, 0.79509839, 0.66322232, 0.79297087, 0.81449756,\n",
       "        0.68814411, 0.79927541, 0.82563831, 0.52343204, 0.80602847,\n",
       "        0.8167135 , 0.72614322, 0.8265578 , 0.81663508, 0.6124989 ,\n",
       "        0.79667659, 0.82370158, 0.73564004, 0.78332482, 0.78915557,\n",
       "        0.50774838, 0.78036855, 0.75515876, 0.77356333, 0.7514541 ,\n",
       "        0.77699923, 0.78211909, 0.78880647, 0.7825775 , 0.80322789,\n",
       "        0.7796309 , 0.7910281 , 0.80100036, 0.80182386, 0.82301432,\n",
       "        0.78211465, 0.77107709, 0.78601634, 0.79381489, 0.78670161,\n",
       "        0.79437321, 0.78096289, 0.78314306, 0.80062829, 0.79412747,\n",
       "        0.79359987, 0.72578917, 0.78549133, 0.78762208, 0.78483182,\n",
       "        0.7904342 , 0.80106622, 0.7955345 , 0.80083407, 0.794328  ,\n",
       "        0.78764888, 0.80211756, 0.78509639, 0.79535406, 0.83285251,\n",
       "        0.78489895, 0.79113177, 0.79059593, 0.77965309, 0.79564584,\n",
       "        0.79246597, 0.79604378, 0.78860387, 0.78877536, 0.791976  ,\n",
       "        0.80986841, 0.80358334, 0.81485258, 0.88601812, 0.85686159,\n",
       "        0.80110828, 0.88649179, 0.93048792, 0.81329872, 0.94865969,\n",
       "        0.94868832, 0.88392887, 0.81100811, 0.85670186, 0.83338655,\n",
       "        0.86861515, 0.83373214, 0.87125556, 0.90201051, 0.88467084,\n",
       "        0.87637744, 0.92319408, 0.91386465, 0.92688934, 0.81073444,\n",
       "        0.84226279, 0.80678982, 0.80867424, 0.82922163, 0.86456958,\n",
       "        0.91990452, 0.85746329, 0.91984875, 0.91768146, 0.92913491,\n",
       "        0.86237898, 0.80758237, 0.78967177, 0.81719505, 0.86060881,\n",
       "        0.87087079, 0.85209498, 0.88210252, 0.86687641, 0.93114988,\n",
       "        0.89424676, 0.86912709, 0.91347658]),\n",
       " 'mean_test_roc_auc_ovo': array([0.92176472, 0.91318503, 0.93941184, 0.91978418, 0.92042753,\n",
       "        0.92586675, 0.92523335, 0.93603746, 0.90402845, 0.9471514 ,\n",
       "        0.90013101, 0.88996416, 0.92447444, 0.94094139, 0.93027133,\n",
       "        0.88900852, 0.90120763, 0.90259615, 0.92182547, 0.89473167,\n",
       "        0.8711577 , 0.89544899, 0.9217929 , 0.9171335 , 0.86149974,\n",
       "        0.90787195, 0.89071889, 0.90662619, 0.87821454, 0.92487582,\n",
       "        0.83725091, 0.87988208, 0.85266562, 0.85503471, 0.85229092,\n",
       "        0.84395274, 0.89695165, 0.87984457, 0.90650294, 0.85129705,\n",
       "        0.86802692, 0.86174088, 0.8716077 , 0.87577381, 0.87288518,\n",
       "        0.87939656, 0.86550578, 0.84926751, 0.92394122, 0.91994752,\n",
       "        0.92158604, 0.88122261, 0.89108115, 0.89029257, 0.92112151,\n",
       "        0.87176308, 0.89987365, 0.89047178, 0.87657756, 0.90796398,\n",
       "        0.92587417, 0.92795392, 0.92563219, 0.91813628, 0.95350296,\n",
       "        0.88846067, 0.86595944, 0.91009742, 0.87749556, 0.86367084,\n",
       "        0.93132516, 0.87658872, 0.92194362, 0.87758059, 0.91297272,\n",
       "        0.9025466 , 0.87387859, 0.90732565, 0.88445172, 0.89841111,\n",
       "        0.88407488, 0.83935286, 0.90444099, 0.87685878, 0.86420352,\n",
       "        0.90425999, 0.8919825 , 0.89874145, 0.84736315, 0.90087306,\n",
       "        0.84859329, 0.86768245, 0.86014756, 0.8734893 , 0.87542999,\n",
       "        0.88322664, 0.81125544, 0.46252958, 0.81018325, 0.77569041,\n",
       "        0.4123832 , 0.82505911, 0.79116701, 0.52142258, 0.80619181,\n",
       "        0.81484419, 0.64522101, 0.81115422, 0.81445826, 0.47556235,\n",
       "        0.90296385, 0.82115001, 0.5900551 , 0.78542727, 0.82607966,\n",
       "        0.51143141, 0.80717485, 0.81662927, 0.64514566, 0.80993453,\n",
       "        0.81263578, 0.65132861, 0.81291751, 0.82136639, 0.46587874,\n",
       "        0.84358603, 0.80855465, 0.51479998, 0.808504  , 0.80986718,\n",
       "        0.39860897, 0.80914265, 0.80194844, 0.36153387, 0.81755662,\n",
       "        0.77988903, 0.51563907, 0.81505341, 0.81928786, 0.49718643,\n",
       "        0.82054986, 0.82306496, 0.52456983, 0.81916038, 0.80670061,\n",
       "        0.53290826, 0.79327526, 0.81106818, 0.47075677, 0.82323787,\n",
       "        0.80727498, 0.6612366 , 0.81651802, 0.81739622, 0.53229166,\n",
       "        0.81564851, 0.79812118, 0.5029276 , 0.79772094, 0.84290917,\n",
       "        0.54121572, 0.83499369, 0.82200851, 0.66566612, 0.81993994,\n",
       "        0.81817708, 0.554668  , 0.82832458, 0.81176556, 0.49776646,\n",
       "        0.81529984, 0.80308437, 0.51656901, 0.79488718, 0.83108544,\n",
       "        0.63548719, 0.82452414, 0.83792253, 0.64447933, 0.82033197,\n",
       "        0.85153227, 0.55568272, 0.80822785, 0.83163112, 0.62360694,\n",
       "        0.79790133, 0.81088303, 0.67411601, 0.81507633, 0.81434676,\n",
       "        0.62981741, 0.81422569, 0.77608034, 0.78851345, 0.78888722,\n",
       "        0.78146374, 0.78980221, 0.81951754, 0.79679383, 0.79948066,\n",
       "        0.78282332, 0.79885917, 0.79809638, 0.8283135 , 0.79735097,\n",
       "        0.79188669, 0.79920451, 0.79624457, 0.80933036, 0.8020921 ,\n",
       "        0.81505831, 0.79043163, 0.7845719 , 0.80504361, 0.81195546,\n",
       "        0.80249938, 0.76730459, 0.82062379, 0.79280014, 0.79439096,\n",
       "        0.7986531 , 0.79573595, 0.81561683, 0.81203856, 0.81617027,\n",
       "        0.77799342, 0.8017121 , 0.79290116, 0.79641392, 0.81093727,\n",
       "        0.78263837, 0.79545607, 0.81715705, 0.793842  , 0.80128201,\n",
       "        0.78620317, 0.7980755 , 0.80542635, 0.81670145, 0.80059768,\n",
       "        0.81641397, 0.79906631, 0.83306386, 0.86941452, 0.85489921,\n",
       "        0.8239354 , 0.89313625, 0.89320013, 0.87657725, 0.95208074,\n",
       "        0.94618284, 0.91796516, 0.85756529, 0.85509113, 0.83804853,\n",
       "        0.87142472, 0.83562434, 0.86481709, 0.8674252 , 0.91242781,\n",
       "        0.88051539, 0.92391247, 0.92630692, 0.90253127, 0.83277553,\n",
       "        0.84408473, 0.80223914, 0.83057696, 0.8584579 , 0.85071689,\n",
       "        0.93179263, 0.87482902, 0.94042369, 0.9206715 , 0.93020205,\n",
       "        0.8864256 , 0.84852199, 0.81091349, 0.81417333, 0.8447012 ,\n",
       "        0.84900863, 0.85722525, 0.90378694, 0.89551463, 0.91169171,\n",
       "        0.92527537, 0.88098253, 0.93356191]),\n",
       " 'std_test_roc_auc_ovo': array([0.00099311, 0.00588235, 0.01048819, 0.01885368, 0.00220913,\n",
       "        0.03748138, 0.00359606, 0.00408224, 0.0302427 , 0.028981  ,\n",
       "        0.03331137, 0.00400315, 0.01327883, 0.00627554, 0.00681689,\n",
       "        0.00723085, 0.00902004, 0.00098498, 0.01750701, 0.00869352,\n",
       "        0.00981642, 0.01248553, 0.02124572, 0.00480318, 0.04632039,\n",
       "        0.00266992, 0.00311506, 0.0158465 , 0.0040943 , 0.00326111,\n",
       "        0.04804519, 0.02997581, 0.06064996, 0.09437689, 0.06260269,\n",
       "        0.00859697, 0.00175044, 0.00384995, 0.01483448, 0.00804491,\n",
       "        0.00986748, 0.02209178, 0.01619186, 0.01093301, 0.00141039,\n",
       "        0.00800419, 0.00624777, 0.0121644 , 0.01012111, 0.00963154,\n",
       "        0.00482539, 0.02741152, 0.00712415, 0.02983658, 0.0076377 ,\n",
       "        0.00670661, 0.01599306, 0.02836484, 0.01442785, 0.01860306,\n",
       "        0.00217003, 0.00288179, 0.01989791, 0.01645924, 0.01102194,\n",
       "        0.03640031, 0.01139237, 0.0090092 , 0.02789363, 0.03513197,\n",
       "        0.02460705, 0.02470748, 0.00140867, 0.00736602, 0.00092674,\n",
       "        0.01637629, 0.04281142, 0.00429996, 0.0093865 , 0.02367491,\n",
       "        0.04647202, 0.00255718, 0.0600887 , 0.02884762, 0.02854551,\n",
       "        0.00333886, 0.00574326, 0.01598089, 0.01178357, 0.01912016,\n",
       "        0.01768028, 0.02384908, 0.03918719, 0.02191506, 0.02696309,\n",
       "        0.0468595 , 0.03316527, 0.14013248, 0.04087056, 0.01171761,\n",
       "        0.0020896 , 0.02570957, 0.01199579, 0.00753408, 0.00961136,\n",
       "        0.03347928, 0.09402706, 0.03538666, 0.03111087, 0.10656245,\n",
       "        0.01762152, 0.02531292, 0.11993465, 0.01098509, 0.02058202,\n",
       "        0.03818648, 0.02383167, 0.01755761, 0.06404209, 0.0366055 ,\n",
       "        0.05638544, 0.02559586, 0.03412943, 0.02958812, 0.09933606,\n",
       "        0.00280987, 0.03504578, 0.05090451, 0.03108108, 0.03553237,\n",
       "        0.14227872, 0.02888421, 0.01500207, 0.06654296, 0.02542245,\n",
       "        0.00090143, 0.1935589 , 0.03750709, 0.03192103, 0.01151732,\n",
       "        0.03362745, 0.0120242 , 0.13647691, 0.03223386, 0.01420042,\n",
       "        0.06656968, 0.00123979, 0.00169282, 0.1439591 , 0.00371188,\n",
       "        0.00896047, 0.17834896, 0.03285914, 0.03146236, 0.03637857,\n",
       "        0.0272356 , 0.00575098, 0.07993127, 0.00763612, 0.00484543,\n",
       "        0.01811198, 0.0290183 , 0.02188985, 0.09427756, 0.02243803,\n",
       "        0.01782016, 0.00811679, 0.0071527 , 0.00267063, 0.01677829,\n",
       "        0.01607626, 0.00798598, 0.14665331, 0.00191632, 0.01658787,\n",
       "        0.05265692, 0.02524874, 0.01228422, 0.12104729, 0.01430349,\n",
       "        0.03481877, 0.17046051, 0.01832994, 0.01499604, 0.01110804,\n",
       "        0.00122475, 0.01281854, 0.06152403, 0.03175151, 0.02519119,\n",
       "        0.12206904, 0.03385714, 0.02092158, 0.01495012, 0.03743312,\n",
       "        0.0044645 , 0.00768311, 0.03071107, 0.01421633, 0.00374723,\n",
       "        0.00319242, 0.00783107, 0.00290398, 0.02648964, 0.02566335,\n",
       "        0.00977204, 0.02812742, 0.01022822, 0.01551547, 0.01539049,\n",
       "        0.02068509, 0.00946874, 0.00142884, 0.00441533, 0.01782798,\n",
       "        0.00889951, 0.04151541, 0.03513247, 0.00517806, 0.00955915,\n",
       "        0.00821891, 0.00533027, 0.02008233, 0.01120449, 0.02184227,\n",
       "        0.00965546, 0.00040546, 0.00780476, 0.00105986, 0.02191524,\n",
       "        0.00226058, 0.00432429, 0.02656112, 0.01418891, 0.00563617,\n",
       "        0.0062628 , 0.00203172, 0.01682248, 0.0279261 , 0.00862168,\n",
       "        0.00654556, 0.00451704, 0.01821128, 0.0166036 , 0.00196238,\n",
       "        0.02282713, 0.00664446, 0.0372878 , 0.06327853, 0.00342105,\n",
       "        0.00250547, 0.03403629, 0.04655719, 0.00161073, 0.00466198,\n",
       "        0.00280957, 0.00189219, 0.00643847, 0.03458531, 0.02775697,\n",
       "        0.00413795, 0.00071839, 0.01244227, 0.02435807, 0.02204109,\n",
       "        0.00182194, 0.00455068, 0.02190272, 0.02923627, 0.01385269,\n",
       "        0.01188812, 0.01736573, 0.02057494, 0.00299003, 0.00106714,\n",
       "        0.02404662, 0.04093962, 0.02124173, 0.00302172, 0.01590762,\n",
       "        0.02186215, 0.00513028, 0.02168442, 0.02863822, 0.01945817,\n",
       "        0.03102861, 0.01185544, 0.02008533]),\n",
       " 'rank_test_roc_auc_ovo': array([ 28,  38,   7,  34,  32,  17,  20,   8,  50,   3,  58,  73,  22,\n",
       "          5,  12,  74,  56,  53,  26,  65, 102,  64,  27,  37, 113,  44,\n",
       "         70,  46,  86,  21, 139,  83, 121, 119, 122, 133,  62,  84,  47,\n",
       "        124, 104, 112, 100,  93,  98,  85, 108, 126,  23,  33,  29,  80,\n",
       "         69,  72,  30,  99,  59,  71,  91,  43,  16,  14,  18,  35,   1,\n",
       "         75, 107,  42,  88, 111,  11,  90,  25,  87,  39,  54,  96,  45,\n",
       "         77,  61,  78, 136,  48,  89, 110,  49,  68,  60, 130,  57, 128,\n",
       "        105, 114,  97,  94,  79, 190, 285, 196, 255, 286, 150, 241, 274,\n",
       "        207, 180, 261, 191, 181, 282,  52, 157, 267, 247, 149, 278, 205,\n",
       "        170, 262, 197, 186, 260, 185, 156, 284, 134, 201, 277, 202, 198,\n",
       "        287, 200, 214, 288, 166, 252, 276, 179, 163, 281, 159, 154, 273,\n",
       "        164, 206, 271, 237, 192, 283, 153, 204, 259, 171, 167, 272, 174,\n",
       "        223, 279, 227, 135, 270, 141, 155, 258, 161, 165, 269, 147, 189,\n",
       "        280, 176, 210, 275, 234, 145, 264, 151, 138, 263, 160, 123, 268,\n",
       "        203, 144, 266, 226, 195, 257, 177, 182, 265, 183, 254, 245, 244,\n",
       "        251, 243, 162, 229, 218, 249, 221, 224, 148, 228, 240, 219, 231,\n",
       "        199, 213, 178, 242, 248, 209, 188, 211, 256, 158, 239, 235, 222,\n",
       "        232, 175, 187, 173, 253, 215, 238, 230, 193, 250, 233, 168, 236,\n",
       "        216, 246, 225, 208, 169, 217, 172, 220, 142, 103, 120, 152,  67,\n",
       "         66,  92,   2,   4,  36, 116, 118, 137, 101, 140, 109, 106,  40,\n",
       "         82,  24,  15,  55, 143, 132, 212, 146, 115, 125,  10,  95,   6,\n",
       "         31,  13,  76, 129, 194, 184, 131, 127, 117,  51,  63,  41,  19,\n",
       "         81,   9]),\n",
       " 'split0_test_neg_log_loss': array([-0.04708175, -0.03893551, -0.0661648 , -0.03854107, -0.06414112,\n",
       "        -0.04920115, -0.07364514, -0.0451323 , -0.09585518, -0.04351714,\n",
       "        -0.09373303, -0.10679069, -0.03862401, -0.03528683, -0.0443588 ,\n",
       "        -0.04641918, -0.05561783, -0.05559932, -0.0504931 , -0.05261273,\n",
       "        -0.1511875 , -0.0430877 , -0.03161378, -0.05230495, -0.06992367,\n",
       "        -0.02639807, -0.04505559, -0.04373457, -0.02989838, -0.02897478,\n",
       "        -0.03678548, -0.0368846 , -0.04161351, -0.0561146 , -0.04753548,\n",
       "        -0.0327275 , -0.05874773, -0.0597599 , -0.0587298 , -0.05923511,\n",
       "        -0.06014563, -0.05995138, -0.05950319, -0.05969999, -0.05975304,\n",
       "        -0.05952828, -0.06024356, -0.05994584, -0.03694577, -0.05423018,\n",
       "        -0.06004505, -0.0684435 , -0.13866338, -0.09801534, -0.06550633,\n",
       "        -0.13499545, -0.09334915, -0.13373376, -0.08745212, -0.14853221,\n",
       "        -0.05446647, -0.04971865, -0.03347705, -0.03129211, -0.06617113,\n",
       "        -0.09249694, -0.0821569 , -0.08196249, -0.10277448, -0.06266039,\n",
       "        -0.11554358, -0.06924691, -0.06385927, -0.04662794, -0.03188984,\n",
       "        -0.0413584 , -0.07055973, -0.04580251, -0.05566038, -0.08286294,\n",
       "        -0.07229111, -0.10446185, -0.0826628 , -0.07009012, -0.05474414,\n",
       "        -0.04626725, -0.04574273, -0.05638049, -0.05753539, -0.05511971,\n",
       "        -0.05826899, -0.06083736, -0.05924697, -0.05916568, -0.05936917,\n",
       "        -0.05976484, -0.46108532, -1.46670528, -0.44012173, -0.37209139,\n",
       "        -1.32812703, -0.39879399, -0.36998727, -1.20044991, -0.37546906,\n",
       "        -0.36025559, -1.33120202, -0.3525167 , -0.46736192, -1.66500156,\n",
       "        -0.47870106, -0.38504218, -1.21338992, -0.37600806, -0.37099854,\n",
       "        -1.24682506, -0.35955575, -0.36711904, -1.27131969, -0.35793268,\n",
       "        -0.42969857, -1.22975199, -0.41456029, -0.40130287, -1.22268005,\n",
       "        -0.37203623, -0.36190359, -1.35313672, -0.36899693, -0.35296796,\n",
       "        -1.18160829, -0.36928597, -0.41177595, -1.11151924, -0.46991871,\n",
       "        -0.41423723, -1.2724396 , -0.40287841, -0.37462432, -1.26857441,\n",
       "        -0.40103333, -0.35524168, -1.18125004, -0.36478941, -0.21218095,\n",
       "        -1.29199222, -0.20856667, -0.20184499, -1.46590707, -0.1996595 ,\n",
       "        -0.20292154, -1.19180029, -0.20040953, -0.19772845, -1.28240282,\n",
       "        -0.19727617, -0.19999232, -1.61174216, -0.20420759, -0.2029355 ,\n",
       "        -1.39916968, -0.20229185, -0.19796724, -1.32258479, -0.20355543,\n",
       "        -0.19316891, -1.13355042, -0.19985277, -0.21316505, -1.42480801,\n",
       "        -0.20491885, -0.19523444, -1.37352897, -0.19574505, -0.19841577,\n",
       "        -1.21959886, -0.19292277, -0.19515616, -1.45368818, -0.19934349,\n",
       "        -0.21611617, -1.14353977, -0.21737862, -0.19522354, -1.09089994,\n",
       "        -0.20952392, -0.19707991, -1.14108355, -0.19450387, -0.19727313,\n",
       "        -1.06436853, -0.19485053, -0.19257478, -0.19074305, -0.17640833,\n",
       "        -0.17074557, -0.15814264, -0.16074115, -0.14532111, -0.14893766,\n",
       "        -0.15009938, -0.14122446, -0.14698483, -0.14027739, -0.18729501,\n",
       "        -0.17800718, -0.19330678, -0.16015116, -0.15334552, -0.15954714,\n",
       "        -0.15320564, -0.15178534, -0.1567008 , -0.14768593, -0.14429571,\n",
       "        -0.14938862, -0.19671174, -0.18751612, -0.17989679, -0.15901155,\n",
       "        -0.16546892, -0.16436523, -0.14562362, -0.15706592, -0.14780156,\n",
       "        -0.15594883, -0.14603089, -0.14749721, -0.2094924 , -0.19932289,\n",
       "        -0.19533246, -0.17591926, -0.17885092, -0.167051  , -0.16225571,\n",
       "        -0.16919239, -0.17051332, -0.16244963, -0.16057717, -0.16262385,\n",
       "        -0.11536855, -0.12315351, -0.13048068, -0.11399799, -0.11897063,\n",
       "        -0.10513953, -0.10243442, -0.10027209, -0.11260308, -0.10266953,\n",
       "        -0.10285215, -0.10216604, -0.13681212, -0.12813304, -0.12335981,\n",
       "        -0.11677218, -0.10963236, -0.11911171, -0.12340489, -0.11045824,\n",
       "        -0.11237539, -0.10527269, -0.10106185, -0.10134668, -0.12973319,\n",
       "        -0.14166906, -0.1168166 , -0.11914835, -0.12268196, -0.11550473,\n",
       "        -0.10468162, -0.1130679 , -0.10818213, -0.10328869, -0.08861831,\n",
       "        -0.0939876 , -0.13255241, -0.13488352, -0.13985357, -0.12568457,\n",
       "        -0.12883932, -0.13011231, -0.11809674, -0.11878016, -0.12292085,\n",
       "        -0.1066336 , -0.10978527, -0.10137513]),\n",
       " 'split1_test_neg_log_loss': array([-0.02758887, -0.04021855, -0.02426008, -0.0272307 , -0.064371  ,\n",
       "        -0.04643683, -0.04570258, -0.03470344, -0.04885702, -0.09703878,\n",
       "        -0.03998405, -0.06495913, -0.03675533, -0.03818095, -0.02800841,\n",
       "        -0.03295627, -0.05718744, -0.02750026, -0.03161009, -0.03781088,\n",
       "        -0.05530675, -0.0450291 , -0.04087485, -0.05020386, -0.03337148,\n",
       "        -0.04930132, -0.04903405, -0.02713036, -0.05024908, -0.02890043,\n",
       "        -0.03172429, -0.03138063, -0.03224224, -0.04672545, -0.04451915,\n",
       "        -0.05761512, -0.04971642, -0.05072258, -0.04974393, -0.05053065,\n",
       "        -0.04969714, -0.05034944, -0.05022278, -0.05045792, -0.05037355,\n",
       "        -0.05022665, -0.05058326, -0.05075548, -0.02632428, -0.02583916,\n",
       "        -0.06605775, -0.08196969, -0.03628275, -0.02986171, -0.11638141,\n",
       "        -0.07173508, -0.06282578, -0.0697322 , -0.06298274, -0.1108802 ,\n",
       "        -0.03489376, -0.02698547, -0.02721791, -0.06025263, -0.02777461,\n",
       "        -0.02458574, -0.04673297, -0.04553048, -0.0462537 , -0.09205956,\n",
       "        -0.04584432, -0.06476882, -0.02224351, -0.05787198, -0.05253698,\n",
       "        -0.03802975, -0.04892532, -0.0560707 , -0.06021762, -0.0323099 ,\n",
       "        -0.03128106, -0.04678795, -0.03880593, -0.06163016, -0.04832909,\n",
       "        -0.03285361, -0.04740796, -0.04761855, -0.04867544, -0.04760305,\n",
       "        -0.05175904, -0.04957866, -0.04956091, -0.05199037, -0.04969552,\n",
       "        -0.04588746, -0.40906635, -1.21928016, -0.50980157, -0.41873841,\n",
       "        -1.56470313, -0.35338871, -0.3612109 , -1.30936981, -0.3592156 ,\n",
       "        -0.34946752, -1.27542581, -0.35303457, -0.47812151, -1.55363921,\n",
       "        -0.45600659, -0.40344152, -1.50566967, -0.38327101, -0.37123829,\n",
       "        -1.30528931, -0.34232115, -0.34571782, -1.26662611, -0.33895704,\n",
       "        -0.43951011, -1.28945918, -0.47493634, -0.43769129, -1.38409921,\n",
       "        -0.36568395, -0.3547925 , -1.42416483, -0.35680929, -0.35871671,\n",
       "        -1.14922947, -0.36464428, -0.41331532, -1.50699793, -0.45604434,\n",
       "        -0.39873643, -1.1643015 , -0.38186589, -0.37775715, -1.28048131,\n",
       "        -0.3516622 , -0.359368  , -1.10620618, -0.35761409, -0.18726812,\n",
       "        -1.54027146, -0.18395595, -0.16979321, -1.6297195 , -0.16772978,\n",
       "        -0.17420934, -1.13294064, -0.17115498, -0.17204535, -1.29133531,\n",
       "        -0.16417008, -0.18767593, -2.10431529, -0.18483651, -0.16613845,\n",
       "        -1.2834845 , -0.16532527, -0.16889758, -1.12521084, -0.1610852 ,\n",
       "        -0.16561677, -1.27610831, -0.16303972, -0.17796515, -1.30739188,\n",
       "        -0.18659924, -0.17241201, -1.42551625, -0.17123246, -0.16838634,\n",
       "        -1.19575385, -0.16916   , -0.168743  , -1.22963608, -0.16162345,\n",
       "        -0.18228264, -1.16423055, -0.17680908, -0.17834637, -1.34426118,\n",
       "        -0.17165869, -0.16576573, -1.25692788, -0.1652569 , -0.16419238,\n",
       "        -1.48267003, -0.16774838, -0.16422388, -0.15308669, -0.16181968,\n",
       "        -0.13214735, -0.13409497, -0.14034326, -0.12687594, -0.11578972,\n",
       "        -0.1272835 , -0.12305212, -0.11274016, -0.11406088, -0.15881841,\n",
       "        -0.16718433, -0.16997739, -0.13146662, -0.13113287, -0.13401759,\n",
       "        -0.1218957 , -0.1288491 , -0.12219906, -0.11201515, -0.12339413,\n",
       "        -0.11554593, -0.16762003, -0.15800984, -0.16357574, -0.1357697 ,\n",
       "        -0.13147956, -0.13818152, -0.1248027 , -0.12090127, -0.12467096,\n",
       "        -0.1182666 , -0.11458272, -0.12466811, -0.19002676, -0.16642212,\n",
       "        -0.16942389, -0.14966052, -0.1426633 , -0.13986281, -0.13988524,\n",
       "        -0.13798727, -0.13761055, -0.13528936, -0.13378632, -0.13149487,\n",
       "        -0.10025203, -0.10194678, -0.10297048, -0.08374706, -0.08495205,\n",
       "        -0.0911706 , -0.07591397, -0.0747737 , -0.07698348, -0.07850873,\n",
       "        -0.06654059, -0.06975626, -0.09470908, -0.10315539, -0.09825312,\n",
       "        -0.08449022, -0.07889196, -0.07714318, -0.08055988, -0.0750925 ,\n",
       "        -0.07334356, -0.06734305, -0.07391238, -0.06863567, -0.10926601,\n",
       "        -0.12075289, -0.10483242, -0.08390503, -0.08531248, -0.08313891,\n",
       "        -0.07759463, -0.0760413 , -0.06868642, -0.07136356, -0.07016225,\n",
       "        -0.06651106, -0.101897  , -0.10813718, -0.11598646, -0.09407597,\n",
       "        -0.09494682, -0.08968804, -0.07929432, -0.08605999, -0.08309808,\n",
       "        -0.07887351, -0.07659046, -0.07685593]),\n",
       " 'mean_test_neg_log_loss': array([-0.03733531, -0.03957703, -0.04521244, -0.03288588, -0.06425606,\n",
       "        -0.04781899, -0.05967386, -0.03991787, -0.0723561 , -0.07027796,\n",
       "        -0.06685854, -0.08587491, -0.03768967, -0.03673389, -0.0361836 ,\n",
       "        -0.03968773, -0.05640264, -0.04154979, -0.0410516 , -0.0452118 ,\n",
       "        -0.10324713, -0.0440584 , -0.03624431, -0.05125441, -0.05164757,\n",
       "        -0.03784969, -0.04704482, -0.03543246, -0.04007373, -0.0289376 ,\n",
       "        -0.03425488, -0.03413262, -0.03692788, -0.05142003, -0.04602731,\n",
       "        -0.04517131, -0.05423208, -0.05524124, -0.05423687, -0.05488288,\n",
       "        -0.05492138, -0.05515041, -0.05486298, -0.05507895, -0.0550633 ,\n",
       "        -0.05487747, -0.05541341, -0.05535066, -0.03163502, -0.04003467,\n",
       "        -0.0630514 , -0.0752066 , -0.08747306, -0.06393852, -0.09094387,\n",
       "        -0.10336527, -0.07808747, -0.10173298, -0.07521743, -0.12970621,\n",
       "        -0.04468011, -0.03835206, -0.03034748, -0.04577237, -0.04697287,\n",
       "        -0.05854134, -0.06444493, -0.06374648, -0.07451409, -0.07735998,\n",
       "        -0.08069395, -0.06700786, -0.04305139, -0.05224996, -0.04221341,\n",
       "        -0.03969407, -0.05974253, -0.0509366 , -0.057939  , -0.05758642,\n",
       "        -0.05178608, -0.0756249 , -0.06073436, -0.06586014, -0.05153662,\n",
       "        -0.03956043, -0.04657534, -0.05199952, -0.05310541, -0.05136138,\n",
       "        -0.05501402, -0.05520801, -0.05440394, -0.05557803, -0.05453234,\n",
       "        -0.05282615, -0.43507583, -1.34299272, -0.47496165, -0.3954149 ,\n",
       "        -1.44641508, -0.37609135, -0.36559909, -1.25490986, -0.36734233,\n",
       "        -0.35486156, -1.30331392, -0.35277564, -0.47274172, -1.60932039,\n",
       "        -0.46735382, -0.39424185, -1.35952979, -0.37963954, -0.37111842,\n",
       "        -1.27605718, -0.35093845, -0.35641843, -1.2689729 , -0.34844486,\n",
       "        -0.43460434, -1.25960559, -0.44474831, -0.41949708, -1.30338963,\n",
       "        -0.36886009, -0.35834805, -1.38865078, -0.36290311, -0.35584233,\n",
       "        -1.16541888, -0.36696512, -0.41254563, -1.30925858, -0.46298152,\n",
       "        -0.40648683, -1.21837055, -0.39237215, -0.37619073, -1.27452786,\n",
       "        -0.37634776, -0.35730484, -1.14372811, -0.36120175, -0.19972453,\n",
       "        -1.41613184, -0.19626131, -0.1858191 , -1.54781329, -0.18369464,\n",
       "        -0.18856544, -1.16237046, -0.18578226, -0.1848869 , -1.28686906,\n",
       "        -0.18072313, -0.19383413, -1.85802873, -0.19452205, -0.18453697,\n",
       "        -1.34132709, -0.18380856, -0.18343241, -1.22389781, -0.18232031,\n",
       "        -0.17939284, -1.20482936, -0.18144624, -0.1955651 , -1.36609995,\n",
       "        -0.19575905, -0.18382323, -1.39952261, -0.18348876, -0.18340106,\n",
       "        -1.20767635, -0.18104139, -0.18194958, -1.34166213, -0.18048347,\n",
       "        -0.19919941, -1.15388516, -0.19709385, -0.18678495, -1.21758056,\n",
       "        -0.19059131, -0.18142282, -1.19900571, -0.17988038, -0.18073275,\n",
       "        -1.27351928, -0.18129946, -0.17839933, -0.17191487, -0.169114  ,\n",
       "        -0.15144646, -0.14611881, -0.15054221, -0.13609852, -0.13236369,\n",
       "        -0.13869144, -0.13213829, -0.12986249, -0.12716914, -0.17305671,\n",
       "        -0.17259576, -0.18164209, -0.14580889, -0.1422392 , -0.14678236,\n",
       "        -0.13755067, -0.14031722, -0.13944993, -0.12985054, -0.13384492,\n",
       "        -0.13246728, -0.18216589, -0.17276298, -0.17173626, -0.14739063,\n",
       "        -0.14847424, -0.15127338, -0.13521316, -0.13898359, -0.13623626,\n",
       "        -0.13710771, -0.1303068 , -0.13608266, -0.19975958, -0.18287251,\n",
       "        -0.18237817, -0.16278989, -0.16075711, -0.1534569 , -0.15107048,\n",
       "        -0.15358983, -0.15406193, -0.14886949, -0.14718175, -0.14705936,\n",
       "        -0.10781029, -0.11255015, -0.11672558, -0.09887252, -0.10196134,\n",
       "        -0.09815507, -0.08917419, -0.08752289, -0.09479328, -0.09058913,\n",
       "        -0.08469637, -0.08596115, -0.1157606 , -0.11564422, -0.11080646,\n",
       "        -0.1006312 , -0.09426216, -0.09812744, -0.10198238, -0.09277537,\n",
       "        -0.09285947, -0.08630787, -0.08748711, -0.08499117, -0.1194996 ,\n",
       "        -0.13121098, -0.11082451, -0.10152669, -0.10399722, -0.09932182,\n",
       "        -0.09113812, -0.0945546 , -0.08843428, -0.08732612, -0.07939028,\n",
       "        -0.08024933, -0.11722471, -0.12151035, -0.12792002, -0.10988027,\n",
       "        -0.11189307, -0.10990017, -0.09869553, -0.10242007, -0.10300947,\n",
       "        -0.09275355, -0.09318786, -0.08911553]),\n",
       " 'std_test_neg_log_loss': array([9.74644156e-03, 6.41521395e-04, 2.09523603e-02, 5.65518493e-03,\n",
       "        1.14943814e-04, 1.38215901e-03, 1.39712788e-02, 5.21442999e-03,\n",
       "        2.34990816e-02, 2.67608224e-02, 2.68744916e-02, 2.09157835e-02,\n",
       "        9.34339789e-04, 1.44705628e-03, 8.17519563e-03, 6.73145132e-03,\n",
       "        7.84802236e-04, 1.40495293e-02, 9.44150600e-03, 7.40092362e-03,\n",
       "        4.79403751e-02, 9.70701359e-04, 4.63053700e-03, 1.05054282e-03,\n",
       "        1.82760930e-02, 1.14516236e-02, 1.98923210e-03, 8.30210546e-03,\n",
       "        1.01753519e-02, 3.71758700e-05, 2.53059644e-03, 2.75198448e-03,\n",
       "        4.68563227e-03, 4.69457633e-03, 1.50816725e-03, 1.24438090e-02,\n",
       "        4.51565069e-03, 4.51866030e-03, 4.49293597e-03, 4.35223255e-03,\n",
       "        5.22424570e-03, 4.80097323e-03, 4.64020635e-03, 4.62103687e-03,\n",
       "        4.68974823e-03, 4.65081524e-03, 4.83014895e-03, 4.59517957e-03,\n",
       "        5.31074015e-03, 1.41955087e-02, 3.00635013e-03, 6.76309581e-03,\n",
       "        5.11903165e-02, 3.40768130e-02, 2.54375368e-02, 3.16301881e-02,\n",
       "        1.52616860e-02, 3.20007827e-02, 1.22346942e-02, 1.88260073e-02,\n",
       "        9.78635860e-03, 1.13665888e-02, 3.12957253e-03, 1.44802588e-02,\n",
       "        1.91982622e-02, 3.39556017e-02, 1.77119649e-02, 1.82160030e-02,\n",
       "        2.82603909e-02, 1.46995863e-02, 3.48496287e-02, 2.23904454e-03,\n",
       "        2.08078789e-02, 5.62201853e-03, 1.03235672e-02, 1.66432340e-03,\n",
       "        1.08172072e-02, 5.13409722e-03, 2.27861626e-03, 2.52765166e-02,\n",
       "        2.05050286e-02, 2.88369522e-02, 2.19284373e-02, 4.22997931e-03,\n",
       "        3.20752346e-03, 6.70681771e-03, 8.32612420e-04, 4.38096975e-03,\n",
       "        4.42997285e-03, 3.75832989e-03, 3.25497391e-03, 5.62934954e-03,\n",
       "        4.84303237e-03, 3.58765626e-03, 4.83682499e-03, 6.93869152e-03,\n",
       "        2.60094811e-02, 1.23712559e-01, 3.48399204e-02, 2.33235104e-02,\n",
       "        1.18288052e-01, 2.27026412e-02, 4.38818269e-03, 5.44599500e-02,\n",
       "        8.12673217e-03, 5.39403613e-03, 2.78881024e-02, 2.58936886e-04,\n",
       "        5.37979535e-03, 5.56811746e-02, 1.13472347e-02, 9.19966678e-03,\n",
       "        1.46139876e-01, 3.63147822e-03, 1.19872567e-04, 2.92321219e-02,\n",
       "        8.61729959e-03, 1.07006100e-02, 2.34679025e-03, 9.48782339e-03,\n",
       "        4.90577252e-03, 2.98535965e-02, 3.01880248e-02, 1.81942070e-02,\n",
       "        8.07095812e-02, 3.17614396e-03, 3.55554573e-03, 3.55140543e-02,\n",
       "        6.09382164e-03, 2.87437781e-03, 1.61894061e-02, 2.32084792e-03,\n",
       "        7.69687608e-04, 1.97739344e-01, 6.93718457e-03, 7.75040117e-03,\n",
       "        5.40690460e-02, 1.05062628e-02, 1.56641370e-03, 5.95345051e-03,\n",
       "        2.46855646e-02, 2.06316164e-03, 3.75219290e-02, 3.58766375e-03,\n",
       "        1.24564158e-02, 1.24139621e-01, 1.23053568e-02, 1.60258889e-02,\n",
       "        8.19062137e-02, 1.59648611e-02, 1.43560988e-02, 2.94298246e-02,\n",
       "        1.46272753e-02, 1.28415460e-02, 4.46624325e-03, 1.65530471e-02,\n",
       "        6.15819487e-03, 2.46286567e-01, 9.68554274e-03, 1.83985247e-02,\n",
       "        5.78425889e-02, 1.84832891e-02, 1.45348310e-02, 9.86869746e-02,\n",
       "        2.12351196e-02, 1.37760655e-02, 7.12789447e-02, 1.84065225e-02,\n",
       "        1.75999523e-02, 5.87080631e-02, 9.15980489e-03, 1.14112151e-02,\n",
       "        2.59936374e-02, 1.22562965e-02, 1.50147129e-02, 1.19225015e-02,\n",
       "        1.18813850e-02, 1.32065798e-02, 1.12026052e-01, 1.88600170e-02,\n",
       "        1.69167664e-02, 1.03453931e-02, 2.02847680e-02, 8.43858390e-03,\n",
       "        1.26680621e-01, 1.89326157e-02, 1.56570909e-02, 5.79221625e-02,\n",
       "        1.46234855e-02, 1.65403710e-02, 2.09150751e-01, 1.35510736e-02,\n",
       "        1.41754539e-02, 1.88281779e-02, 7.29432789e-03, 1.92991083e-02,\n",
       "        1.20238341e-02, 1.01989467e-02, 9.22258737e-03, 1.65739674e-02,\n",
       "        1.14079399e-02, 9.08616982e-03, 1.71223339e-02, 1.31082548e-02,\n",
       "        1.42383022e-02, 5.41142595e-03, 1.16646949e-02, 1.43422685e-02,\n",
       "        1.11063230e-02, 1.27647782e-02, 1.56549720e-02, 1.14681210e-02,\n",
       "        1.72508722e-02, 1.78353911e-02, 1.04507890e-02, 1.69213445e-02,\n",
       "        1.45458533e-02, 1.47531388e-02, 8.16052516e-03, 1.16209253e-02,\n",
       "        1.69946836e-02, 1.30918592e-02, 1.04104612e-02, 1.80823253e-02,\n",
       "        1.15653011e-02, 1.88411171e-02, 1.57240846e-02, 1.14145499e-02,\n",
       "        9.73281643e-03, 1.64503848e-02, 1.29542836e-02, 1.31293698e-02,\n",
       "        1.80938099e-02, 1.35940978e-02, 1.11852334e-02, 1.56025637e-02,\n",
       "        1.64513874e-02, 1.35801368e-02, 1.33954242e-02, 1.55644928e-02,\n",
       "        7.55826024e-03, 1.06033689e-02, 1.37550997e-02, 1.51254651e-02,\n",
       "        1.70092870e-02, 6.98446198e-03, 1.32602271e-02, 1.27491954e-02,\n",
       "        1.78097974e-02, 1.20803988e-02, 1.81557806e-02, 1.62048898e-02,\n",
       "        2.10515234e-02, 1.24888293e-02, 1.25533417e-02, 1.61409832e-02,\n",
       "        1.53702005e-02, 2.09842663e-02, 2.14225078e-02, 1.76828734e-02,\n",
       "        1.95159174e-02, 1.89648175e-02, 1.35747332e-02, 1.63555088e-02,\n",
       "        1.02335903e-02, 1.04580862e-02, 5.99209455e-03, 1.76216600e-02,\n",
       "        1.86847437e-02, 1.61829090e-02, 1.35434952e-02, 1.85132976e-02,\n",
       "        1.97478560e-02, 1.59625665e-02, 9.22803249e-03, 1.37382745e-02,\n",
       "        1.53277048e-02, 1.33731720e-02, 1.19335544e-02, 1.58042999e-02,\n",
       "        1.69462530e-02, 2.02121337e-02, 1.94012099e-02, 1.63600856e-02,\n",
       "        1.99113859e-02, 1.38800476e-02, 1.65974039e-02, 1.22595996e-02]),\n",
       " 'rank_test_neg_log_loss': array([ 12,  17,  31,   4,  76,  37,  70,  20,  82,  81,  79,  94,  13,\n",
       "         10,   8,  18,  66,  24,  23,  30, 126,  27,   9,  39,  43,  14,\n",
       "         36,   7,  22,   1,   6,   5,  11,  41,  33,  29,  49,  62,  50,\n",
       "         55,  56,  60,  53,  59,  58,  54,  64,  63,   3,  21,  73,  84,\n",
       "         98,  75, 105, 127,  88, 121,  85, 144,  28,  15,   2,  32,  35,\n",
       "         69,  77,  74,  83,  87,  91,  80,  26,  46,  25,  19,  71,  38,\n",
       "         68,  67,  44,  86,  72,  78,  42,  16,  34,  45,  48,  40,  57,\n",
       "         61,  51,  65,  52,  47, 251, 279, 256, 246, 285, 240, 235, 267,\n",
       "        237, 228, 274, 227, 255, 287, 254, 245, 280, 243, 239, 272, 226,\n",
       "        230, 269, 225, 250, 268, 252, 249, 275, 238, 232, 282, 234, 229,\n",
       "        260, 236, 248, 276, 253, 247, 265, 244, 241, 271, 242, 231, 257,\n",
       "        233, 223, 284, 220, 212, 286, 206, 214, 259, 211, 210, 273, 191,\n",
       "        216, 288, 217, 209, 277, 207, 204, 266, 200, 188, 262, 196, 218,\n",
       "        281, 219, 208, 283, 205, 203, 263, 193, 198, 278, 190, 222, 258,\n",
       "        221, 213, 264, 215, 195, 261, 189, 192, 270, 194, 187, 183, 181,\n",
       "        175, 165, 172, 155, 150, 159, 149, 146, 142, 186, 184, 197, 164,\n",
       "        163, 166, 158, 162, 161, 145, 152, 151, 199, 185, 182, 169, 170,\n",
       "        174, 153, 160, 156, 157, 147, 154, 224, 202, 201, 180, 179, 176,\n",
       "        173, 177, 178, 171, 168, 167, 129, 135, 138, 117, 122, 115, 103,\n",
       "        100, 113, 104,  92,  95, 137, 136, 132, 119, 111, 114, 123, 108,\n",
       "        109,  96,  99,  93, 140, 148, 133, 120, 128, 118, 106, 112, 101,\n",
       "         97,  89,  90, 139, 141, 143, 130, 134, 131, 116, 124, 125, 107,\n",
       "        110, 102])}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRIAL_5_MLP.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FIVE NEG LOG LOSS RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.01,\n",
       " 'classifier__hidden_layer_sizes': (10,),\n",
       " 'classifier__learning_rate': 'adaptive',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL FIVE NEG LOG LOSS RESULTS ---------------------------------------------\")\n",
    "TRIAL_5_MLP.cv_results_['rank_test_neg_log_loss']\n",
    "TRIAL_5_MLP.cv_results_['params'][ np.argmin(TRIAL_5_MLP.cv_results_['rank_test_neg_log_loss']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FIVE F1 RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.001,\n",
       " 'classifier__hidden_layer_sizes': (20,),\n",
       " 'classifier__learning_rate': 'constant',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL FIVE F1 RESULTS ---------------------------------------------\")\n",
    "TRIAL_5_MLP.cv_results_['rank_test_f1_micro']\n",
    "TRIAL_5_MLP.cv_results_['params'][ np.argmin(TRIAL_5_MLP.cv_results_['rank_test_f1_micro']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FIVE ROC AUC RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'tanh',\n",
       " 'classifier__alpha': 0.001,\n",
       " 'classifier__hidden_layer_sizes': (10,),\n",
       " 'classifier__learning_rate': 'invscaling',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL FIVE ROC AUC RESULTS ---------------------------------------------\")\n",
    "TRIAL_5_MLP.cv_results_['rank_test_roc_auc_ovo']\n",
    "TRIAL_5_MLP.cv_results_['params'][ np.argmin(TRIAL_5_MLP.cv_results_['rank_test_roc_auc_ovo']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 5 MLP using best NEG LOG LOSS hyperparameters :0.9952666666666666\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 5 MLP using best F1 hyperparameters :0.9967166666666667\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 5 MLP using best ROC_AUC hyperparameters :0.9966166666666667\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best NEG LOG LOSS results to train on test set, \n",
    "p = TRIAL_5_MLP.cv_results_['params'][ np.argmin(TRIAL_5_MLP.cv_results_['rank_test_neg_log_loss']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train__neg_log_loss = pipe.predict(X_train)\n",
    "y_pred_test_neg_log_loss = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 5 MLP using best NEG LOG LOSS hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_neg_log_loss)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best F1 results to train on test set, LR_Trial 1\n",
    "p = TRIAL_5_MLP.cv_results_['params'][ np.argmin(TRIAL_5_MLP.cv_results_['rank_test_f1_micro']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train_roc_auc = pipe.predict(X_train)\n",
    "y_pred_test_f1 = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 5 MLP using best F1 hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_f1)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best ROC_AUC results to train on test set, LR_Trial 1\n",
    "p  = TRIAL_5_MLP.cv_results_['params'][ np.argmin(TRIAL_5_MLP.cv_results_['rank_test_roc_auc_ovo']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train_roc_auc = pipe.predict(X_train)\n",
    "y_pred_test_roc_auc = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 5 MLP using best ROC_AUC hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_roc_auc)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
