{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries;\n",
    "import numpy as np\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# use seaborn plotting defaults\n",
    "import seaborn as sns; sns.set_style('white')\n",
    "\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from numpy import genfromtxt\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import load_digits, make_blobs\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, plot_confusion_matrix\n",
    "from sklearn.model_selection import KFold, train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, StandardScaler \n",
    "from sklearn.preprocessing import RobustScaler, Normalizer, QuantileTransformer, PowerTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import seaborn as sns\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>Perimeter</th>\n",
       "      <th>MajorAxisLength</th>\n",
       "      <th>MinorAxisLength</th>\n",
       "      <th>AspectRation</th>\n",
       "      <th>Eccentricity</th>\n",
       "      <th>ConvexArea</th>\n",
       "      <th>EquivDiameter</th>\n",
       "      <th>Extent</th>\n",
       "      <th>Solidity</th>\n",
       "      <th>roundness</th>\n",
       "      <th>Compactness</th>\n",
       "      <th>ShapeFactor1</th>\n",
       "      <th>ShapeFactor2</th>\n",
       "      <th>ShapeFactor3</th>\n",
       "      <th>ShapeFactor4</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28395</td>\n",
       "      <td>610.291</td>\n",
       "      <td>208.178117</td>\n",
       "      <td>173.888747</td>\n",
       "      <td>1.197191</td>\n",
       "      <td>0.549812</td>\n",
       "      <td>28715</td>\n",
       "      <td>190.141097</td>\n",
       "      <td>0.763923</td>\n",
       "      <td>0.988856</td>\n",
       "      <td>0.958027</td>\n",
       "      <td>0.913358</td>\n",
       "      <td>0.007332</td>\n",
       "      <td>0.003147</td>\n",
       "      <td>0.834222</td>\n",
       "      <td>0.998724</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28734</td>\n",
       "      <td>638.018</td>\n",
       "      <td>200.524796</td>\n",
       "      <td>182.734419</td>\n",
       "      <td>1.097356</td>\n",
       "      <td>0.411785</td>\n",
       "      <td>29172</td>\n",
       "      <td>191.272750</td>\n",
       "      <td>0.783968</td>\n",
       "      <td>0.984986</td>\n",
       "      <td>0.887034</td>\n",
       "      <td>0.953861</td>\n",
       "      <td>0.006979</td>\n",
       "      <td>0.003564</td>\n",
       "      <td>0.909851</td>\n",
       "      <td>0.998430</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29380</td>\n",
       "      <td>624.110</td>\n",
       "      <td>212.826130</td>\n",
       "      <td>175.931143</td>\n",
       "      <td>1.209713</td>\n",
       "      <td>0.562727</td>\n",
       "      <td>29690</td>\n",
       "      <td>193.410904</td>\n",
       "      <td>0.778113</td>\n",
       "      <td>0.989559</td>\n",
       "      <td>0.947849</td>\n",
       "      <td>0.908774</td>\n",
       "      <td>0.007244</td>\n",
       "      <td>0.003048</td>\n",
       "      <td>0.825871</td>\n",
       "      <td>0.999066</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30008</td>\n",
       "      <td>645.884</td>\n",
       "      <td>210.557999</td>\n",
       "      <td>182.516516</td>\n",
       "      <td>1.153638</td>\n",
       "      <td>0.498616</td>\n",
       "      <td>30724</td>\n",
       "      <td>195.467062</td>\n",
       "      <td>0.782681</td>\n",
       "      <td>0.976696</td>\n",
       "      <td>0.903936</td>\n",
       "      <td>0.928329</td>\n",
       "      <td>0.007017</td>\n",
       "      <td>0.003215</td>\n",
       "      <td>0.861794</td>\n",
       "      <td>0.994199</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30140</td>\n",
       "      <td>620.134</td>\n",
       "      <td>201.847882</td>\n",
       "      <td>190.279279</td>\n",
       "      <td>1.060798</td>\n",
       "      <td>0.333680</td>\n",
       "      <td>30417</td>\n",
       "      <td>195.896503</td>\n",
       "      <td>0.773098</td>\n",
       "      <td>0.990893</td>\n",
       "      <td>0.984877</td>\n",
       "      <td>0.970516</td>\n",
       "      <td>0.006697</td>\n",
       "      <td>0.003665</td>\n",
       "      <td>0.941900</td>\n",
       "      <td>0.999166</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Area  Perimeter  MajorAxisLength  MinorAxisLength  AspectRation  \\\n",
       "0  28395    610.291       208.178117       173.888747      1.197191   \n",
       "1  28734    638.018       200.524796       182.734419      1.097356   \n",
       "2  29380    624.110       212.826130       175.931143      1.209713   \n",
       "3  30008    645.884       210.557999       182.516516      1.153638   \n",
       "4  30140    620.134       201.847882       190.279279      1.060798   \n",
       "\n",
       "   Eccentricity  ConvexArea  EquivDiameter    Extent  Solidity  roundness  \\\n",
       "0      0.549812       28715     190.141097  0.763923  0.988856   0.958027   \n",
       "1      0.411785       29172     191.272750  0.783968  0.984986   0.887034   \n",
       "2      0.562727       29690     193.410904  0.778113  0.989559   0.947849   \n",
       "3      0.498616       30724     195.467062  0.782681  0.976696   0.903936   \n",
       "4      0.333680       30417     195.896503  0.773098  0.990893   0.984877   \n",
       "\n",
       "   Compactness  ShapeFactor1  ShapeFactor2  ShapeFactor3  ShapeFactor4  Class  \n",
       "0     0.913358      0.007332      0.003147      0.834222      0.998724  SEKER  \n",
       "1     0.953861      0.006979      0.003564      0.909851      0.998430  SEKER  \n",
       "2     0.908774      0.007244      0.003048      0.825871      0.999066  SEKER  \n",
       "3     0.928329      0.007017      0.003215      0.861794      0.994199  SEKER  \n",
       "4     0.970516      0.006697      0.003665      0.941900      0.999166  SEKER  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beanData = pd.read_excel('Dry_Bean_Dataset.xlsx')\n",
    "\n",
    "\n",
    "X = beanData.iloc[:,0:15].values\n",
    "y = beanData.iloc[:,16].values\n",
    "beanData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encodeLabel = LabelEncoder()\n",
    "y = encodeLabel.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size= 0.63, random_state=1738)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTI LAYERED PERCEPTRON TRIAL ONE ON BEANS DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:    9.5s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:   10.7s\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:   12.0s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:   13.7s\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:   16.5s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:   17.8s\n",
      "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed:   19.9s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   23.0s\n",
      "[Parallel(n_jobs=-1)]: Done 189 tasks      | elapsed:   24.7s\n",
      "[Parallel(n_jobs=-1)]: Done 210 tasks      | elapsed:   26.6s\n",
      "[Parallel(n_jobs=-1)]: Done 233 tasks      | elapsed:   30.3s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:   32.5s\n",
      "[Parallel(n_jobs=-1)]: Done 281 tasks      | elapsed:   35.9s\n",
      "[Parallel(n_jobs=-1)]: Done 306 tasks      | elapsed:   39.1s\n",
      "[Parallel(n_jobs=-1)]: Done 333 tasks      | elapsed:   42.7s\n",
      "[Parallel(n_jobs=-1)]: Done 360 tasks      | elapsed:   46.4s\n",
      "[Parallel(n_jobs=-1)]: Done 389 tasks      | elapsed:   49.2s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:   54.2s\n",
      "[Parallel(n_jobs=-1)]: Done 449 tasks      | elapsed:   57.4s\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 513 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 546 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 581 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 616 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 653 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 690 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 729 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 809 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 850 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 893 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 936 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 981 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1026 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1073 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1120 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1169 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1269 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1320 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1373 tasks      | elapsed:  4.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL ONE RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1440 out of 1440 | elapsed:  5.0min finished\n"
     ]
    }
   ],
   "source": [
    "ITERATIONS = 1\n",
    "for i in range(1):\n",
    "    #For reach \"trial\", randomly select 5000 data points\n",
    "    random5000DataPoints = beanData.sample(n = 5000, replace = True)\n",
    "    #From that 5000, create x set and y set\n",
    "    xSet = random5000DataPoints.iloc[:,0:15].values\n",
    "    ySet = random5000DataPoints.iloc[:,16].values\n",
    "    ySet = encodeLabel.fit_transform(ySet)\n",
    "    \n",
    "    pipe = Pipeline([('std', StandardScaler()),('classifier', MLPClassifier(max_iter = 250))])\n",
    "\n",
    "    search_space = [\n",
    "                {\n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                {\n",
    "                 \n",
    "                  \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['sgd'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                \n",
    "                {\n",
    "                \n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['adam'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                ]\n",
    "    \n",
    "    #'precision_micro','roc_auc_ovo','jaccard','neg_log_loss'] JACCARD DOES NOT WORK FOR NON BINARY?\n",
    "    runHyperParamSearch = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=5), \n",
    "                          scoring=['recall_micro','f1_micro','roc_auc_ovo','neg_log_loss'], \n",
    "                          refit=False, verbose=10, n_jobs = -1)\n",
    "    TRIAL_1_MLP = runHyperParamSearch.fit(xSet, ySet)\n",
    "    print(\"---------------------------------TRIAL ONE RESULTS ---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([1.21014109, 1.23436151, 1.22375274, 1.51210175, 1.5063971 ,\n",
       "        1.52100873, 2.01012897, 2.0388546 , 2.04526067, 2.40006461,\n",
       "        2.32990408, 2.27835889, 1.49448237, 1.2925106 , 1.26929259,\n",
       "        1.62689834, 1.59607496, 1.62689881, 1.94166899, 2.08749666,\n",
       "        2.13623648, 2.29757743, 2.28666415, 2.16445804, 1.37427459,\n",
       "        1.30151739, 1.27059231, 1.58996778, 1.67123823, 1.70916996,\n",
       "        2.10401015, 2.07248187, 1.99231582, 2.31949544, 2.31338949,\n",
       "        2.20379162, 1.34976101, 1.22995682, 1.21924863, 1.45615287,\n",
       "        1.51760483, 1.53181911, 1.95588193, 2.02253985, 2.07458363,\n",
       "        2.34271598, 2.31649122, 2.34001112, 1.53923507, 1.37718449,\n",
       "        1.2901093 , 1.63720846, 1.67283921, 1.69285469, 2.20949893,\n",
       "        2.27765718, 2.26034484, 2.65448394, 2.70412421, 2.64187136,\n",
       "        1.41041408, 1.32363801, 1.31973376, 1.68454895, 1.66423106,\n",
       "        1.68615117, 2.22381091, 2.25974393, 2.41878071, 2.70292382,\n",
       "        2.68510895, 2.67249842, 1.46836281, 1.33464699, 1.34876027,\n",
       "        1.64771667, 1.64101119, 1.68244767, 2.25864244, 2.32069492,\n",
       "        2.37604322, 2.75306935, 2.87157006, 2.84374504, 1.56414537,\n",
       "        1.46125565, 1.39660153, 1.69956193, 1.69585905, 1.77952981,\n",
       "        2.1966887 , 2.28056326, 2.31789565, 2.56020255, 2.5139606 ,\n",
       "        2.43468924, 3.36459336, 0.33428712, 3.15971756, 3.33526859,\n",
       "        0.40995255, 3.44946632, 3.68567014, 0.85423503, 3.74542065,\n",
       "        3.90205536, 1.28030057, 4.02836394, 3.28952932, 0.31366978,\n",
       "        3.64803762, 3.82438936, 0.56878934, 3.6793644 , 3.98122358,\n",
       "        1.40450735, 4.37936625, 4.62537823, 1.39680085, 4.89951344,\n",
       "        3.66885509, 0.44508348, 3.55085382, 3.6596055 , 0.79858708,\n",
       "        3.63973064, 3.84971085, 0.69770031, 4.11463914, 4.13985972,\n",
       "        1.31272907, 4.34573646, 3.49790783, 0.34159355, 3.75212698,\n",
       "        3.76203523, 0.61592941, 4.37796454, 3.82659068, 0.74714284,\n",
       "        4.30840554, 4.7199595 , 1.65081964, 4.4270071 , 3.77514663,\n",
       "        3.73831477, 3.65784492, 3.82128615, 4.30059934, 4.09982586,\n",
       "        4.36555486, 4.637989  , 4.24645147, 4.33923149, 4.27587729,\n",
       "        4.46924329, 3.52373037, 3.00188193, 3.55345588, 3.7534287 ,\n",
       "        3.50551438, 3.67626114, 4.28578563, 4.04087534, 4.26216559,\n",
       "        4.68292694, 4.55751963, 4.49266319, 3.68306713, 3.88884416,\n",
       "        3.85541568, 4.08070917, 4.43050985, 4.40698996, 4.58614426,\n",
       "        4.71345372, 4.91392622, 5.06825871, 4.61917243, 4.50397363,\n",
       "        3.34437633, 3.24659162, 3.5819808 , 3.70648766, 4.39538026,\n",
       "        4.31030674, 4.34053288, 4.40438771, 4.19140439, 4.42720718,\n",
       "        4.28248301, 4.35844817, 3.82709122, 3.89134693, 3.80497298,\n",
       "        4.08931656, 4.21822777, 4.5320981 , 4.47619567, 4.54040465,\n",
       "        4.52278972, 4.34513707, 4.69133472, 4.6060617 , 3.77744851,\n",
       "        3.960005  , 4.33262587, 4.17479048, 4.45312986, 4.15367193,\n",
       "        4.48735919, 4.24975467, 4.44622374, 4.62928114, 4.46874309,\n",
       "        4.38947496, 3.56566634, 3.64203191, 3.60259833, 4.03577075,\n",
       "        3.92907896, 3.98462586, 4.14086084, 4.46343851, 4.30960665,\n",
       "        4.38777361, 4.75078578, 4.47534857, 3.8989531 , 3.79396224,\n",
       "        4.09271941, 4.15977716, 4.24024692, 4.09902492, 4.66821408,\n",
       "        4.30079842, 4.38707361, 4.95516205, 4.65780587, 4.91142406,\n",
       "        4.09922509, 4.33923101, 3.78825755, 4.59795451, 4.64639592,\n",
       "        4.51127934, 5.06845856, 4.93444338, 4.37085924, 5.2621254 ,\n",
       "        5.24721203, 4.92013087, 4.15207052, 3.85001111, 4.32782211,\n",
       "        4.78501482, 4.29759574, 4.39738188, 4.69093475, 4.48325591,\n",
       "        4.34944043, 4.70994992, 4.54180579, 4.71815724, 3.76854086,\n",
       "        3.73621311, 3.68446813, 4.04477816, 3.97111549, 4.31751323,\n",
       "        4.69754071, 4.50287275, 4.39878316, 4.89551001, 4.80573316,\n",
       "        5.2056767 , 4.27297502, 4.11754084, 4.07310257, 4.26226554,\n",
       "        4.45433106, 4.9206316 , 4.83205543, 4.47915182, 4.48025322,\n",
       "        5.01991673, 4.49416471, 4.186099  ]),\n",
       " 'std_fit_time': array([0.03511627, 0.05734397, 0.04059953, 0.03865018, 0.05907418,\n",
       "        0.05444281, 0.06358804, 0.05471498, 0.0273431 , 0.12609635,\n",
       "        0.08937906, 0.08726959, 0.07620143, 0.05501199, 0.05183852,\n",
       "        0.02915316, 0.04853865, 0.06752914, 0.03749257, 0.03458814,\n",
       "        0.08067904, 0.03934657, 0.07948614, 0.0487822 , 0.05708313,\n",
       "        0.04553165, 0.03663592, 0.10919888, 0.1261102 , 0.08861347,\n",
       "        0.078934  , 0.07900974, 0.03057615, 0.03861582, 0.01909373,\n",
       "        0.04260069, 0.09248411, 0.02174409, 0.02018868, 0.03812867,\n",
       "        0.02830353, 0.03855985, 0.09149418, 0.08393856, 0.10093443,\n",
       "        0.08746363, 0.04785103, 0.09086671, 0.14337835, 0.03208045,\n",
       "        0.02379272, 0.03516876, 0.09320993, 0.06512753, 0.10594325,\n",
       "        0.07706518, 0.04361124, 0.10595487, 0.14159529, 0.17868196,\n",
       "        0.05494997, 0.01395958, 0.01276875, 0.04994781, 0.03870885,\n",
       "        0.03900075, 0.11675445, 0.13044015, 0.13294532, 0.11301363,\n",
       "        0.05437947, 0.05696207, 0.11934697, 0.01374889, 0.0461211 ,\n",
       "        0.01237482, 0.02298123, 0.07363367, 0.07643501, 0.13730086,\n",
       "        0.07366095, 0.14833771, 0.19744922, 0.10287455, 0.11695593,\n",
       "        0.07512591, 0.07036332, 0.04200711, 0.04999619, 0.07877649,\n",
       "        0.06969951, 0.05520121, 0.10456925, 0.08539336, 0.02438544,\n",
       "        0.12457891, 0.12843163, 0.11478807, 0.0347096 , 0.07929818,\n",
       "        0.06089015, 0.08124572, 0.08945168, 0.55250966, 0.03101931,\n",
       "        0.10465198, 0.40052753, 0.1574601 , 0.11259697, 0.06667667,\n",
       "        0.24286078, 0.40425219, 0.41864092, 0.29338778, 0.2702352 ,\n",
       "        0.64375018, 0.33193445, 0.53913618, 0.72373069, 0.34011911,\n",
       "        0.28542336, 0.12368286, 0.22569473, 0.14923603, 0.3423538 ,\n",
       "        0.19339499, 0.256616  , 0.26555613, 0.26038145, 0.24470166,\n",
       "        0.84563162, 0.40041954, 0.36726335, 0.08802506, 0.40468839,\n",
       "        0.35665659, 0.17755259, 0.26575426, 0.19978335, 0.38583772,\n",
       "        0.38067361, 0.49881332, 0.6410256 , 0.43780686, 0.20118362,\n",
       "        0.64985538, 0.39934107, 0.22377268, 0.20902977, 0.13866549,\n",
       "        0.28905273, 0.31469027, 0.18068203, 0.19142368, 0.14228104,\n",
       "        0.21001417, 0.10580258, 0.62317129, 0.1885747 , 0.223755  ,\n",
       "        0.0436626 , 0.12328812, 0.29181441, 0.32524131, 0.16823471,\n",
       "        0.36019228, 0.15549452, 0.31518004, 0.35111946, 0.30825022,\n",
       "        0.55235573, 0.60512879, 0.49370681, 0.84034185, 0.61910441,\n",
       "        0.65861639, 0.86525043, 0.54406859, 0.50450273, 0.30642543,\n",
       "        0.07945915, 0.31820328, 0.2290181 , 0.23060126, 0.28185466,\n",
       "        0.50842606, 0.26479477, 0.16105486, 0.08419004, 0.22162204,\n",
       "        0.1053962 , 0.20183751, 0.26750791, 0.44897657, 0.27431926,\n",
       "        0.25485414, 0.5249917 , 0.55126105, 0.24521841, 0.43279701,\n",
       "        0.31095247, 0.20688799, 0.47949047, 0.32767236, 0.32098283,\n",
       "        0.22015729, 0.57867259, 0.4572068 , 0.56360463, 0.22688847,\n",
       "        0.25738221, 0.11141278, 0.29244813, 0.22892573, 0.2603988 ,\n",
       "        0.19179311, 0.03229778, 0.17890749, 0.19759029, 0.29390695,\n",
       "        0.2065452 , 0.29029259, 0.15607919, 0.42561111, 0.23913095,\n",
       "        0.17045394, 0.30676126, 0.16841553, 0.33100119, 0.28777465,\n",
       "        0.47147221, 0.43468821, 0.25209313, 0.29978198, 0.32682742,\n",
       "        0.2822957 , 0.31168069, 0.595898  , 0.54397121, 0.5460293 ,\n",
       "        0.48508182, 0.60244511, 0.45331458, 0.45592097, 0.40145348,\n",
       "        0.62800892, 0.6541005 , 0.54454342, 0.07707397, 0.54909132,\n",
       "        0.54857052, 0.39410787, 0.46841399, 0.28130552, 0.25702835,\n",
       "        0.75131942, 0.50404615, 0.34966624, 0.26196339, 0.21216254,\n",
       "        0.10683206, 0.21393455, 0.11653889, 0.13716706, 0.19033508,\n",
       "        0.23250118, 0.15982341, 0.15915778, 0.15413434, 0.23926353,\n",
       "        0.38420737, 0.14444396, 0.14501484, 0.31602623, 0.32675144,\n",
       "        0.46710109, 0.65566831, 0.41070615, 0.31556407, 0.32795988,\n",
       "        0.42045507, 0.52044338, 0.4909786 , 0.09715117, 0.35951684,\n",
       "        0.37658136, 0.16341359, 0.1607523 ]),\n",
       " 'mean_score_time': array([0.03833337, 0.03733211, 0.04443836, 0.03883252, 0.04223661,\n",
       "        0.04293623, 0.04804134, 0.04694047, 0.04964113, 0.05884981,\n",
       "        0.04013553, 0.0397346 , 0.05184517, 0.04013443, 0.04233637,\n",
       "        0.04644108, 0.04443684, 0.04293799, 0.06215329, 0.04603896,\n",
       "        0.05664988, 0.04884109, 0.04163661, 0.03663177, 0.04043546,\n",
       "        0.04133635, 0.04393735, 0.05064378, 0.04723964, 0.04563971,\n",
       "        0.04714189, 0.04543924, 0.04804006, 0.05224524, 0.03963356,\n",
       "        0.03723259, 0.03592811, 0.03653212, 0.03643174, 0.03723121,\n",
       "        0.04343719, 0.04393783, 0.05384703, 0.0483418 , 0.05594764,\n",
       "        0.04944267, 0.04654112, 0.03913417, 0.03711996, 0.03523049,\n",
       "        0.03713274, 0.03913326, 0.05664773, 0.04423862, 0.05054522,\n",
       "        0.04844236, 0.04703994, 0.04834104, 0.04083591, 0.03943434,\n",
       "        0.03803086, 0.04263463, 0.04163609, 0.03823195, 0.04353695,\n",
       "        0.04073639, 0.04453897, 0.05164576, 0.05124445, 0.04403744,\n",
       "        0.04453754, 0.04103503, 0.04153557, 0.03703327, 0.03853288,\n",
       "        0.04083557, 0.04243712, 0.05294585, 0.04493814, 0.04183664,\n",
       "        0.04934368, 0.0598495 , 0.04824104, 0.04644084, 0.04653907,\n",
       "        0.03843322, 0.03733196, 0.03913412, 0.04493909, 0.04223766,\n",
       "        0.04534054, 0.04373665, 0.04543805, 0.04543891, 0.044238  ,\n",
       "        0.03523073, 0.03663111, 0.03482995, 0.04023433, 0.03763261,\n",
       "        0.03482995, 0.03653207, 0.03713174, 0.03583055, 0.03913426,\n",
       "        0.04143586, 0.03673158, 0.03793244, 0.03543015, 0.03432956,\n",
       "        0.03773246, 0.04413786, 0.04463835, 0.03733201, 0.04563937,\n",
       "        0.03663149, 0.04614029, 0.05354643, 0.0474412 , 0.04183574,\n",
       "        0.04393768, 0.04653969, 0.04233589, 0.03763299, 0.03733196,\n",
       "        0.04423823, 0.04413872, 0.03923407, 0.04303646, 0.04323721,\n",
       "        0.04093499, 0.04844213, 0.04253669, 0.04523907, 0.05314569,\n",
       "        0.04794159, 0.03923421, 0.04473867, 0.0559484 , 0.03603106,\n",
       "        0.04944224, 0.04834151, 0.04183593, 0.05284538, 0.04663954,\n",
       "        0.05855021, 0.04934249, 0.04113555, 0.05084295, 0.04954267,\n",
       "        0.04734073, 0.04373765, 0.05865054, 0.04203629, 0.0429368 ,\n",
       "        0.04393816, 0.04223614, 0.03683167, 0.04413805, 0.04113498,\n",
       "        0.04463854, 0.04533954, 0.04954252, 0.04183626, 0.04634056,\n",
       "        0.04143586, 0.0467401 , 0.04463878, 0.04413843, 0.05404634,\n",
       "        0.04353752, 0.05354629, 0.04453821, 0.0603519 , 0.04513884,\n",
       "        0.05814991, 0.05284534, 0.0391336 , 0.04303699, 0.03793278,\n",
       "        0.04473858, 0.0419364 , 0.0472403 , 0.04513836, 0.04413776,\n",
       "        0.04483919, 0.04233594, 0.04123545, 0.04774117, 0.03713202,\n",
       "        0.04023438, 0.04283676, 0.04413824, 0.05064349, 0.04944301,\n",
       "        0.04784131, 0.05364647, 0.05424614, 0.03938723, 0.03943381,\n",
       "        0.05054297, 0.03793268, 0.06245341, 0.05584788, 0.04523864,\n",
       "        0.04794154, 0.05534782, 0.05314603, 0.04353776, 0.03893394,\n",
       "        0.04623961, 0.05804992, 0.05324554, 0.04183617, 0.04113531,\n",
       "        0.03893337, 0.04213648, 0.05044327, 0.04043465, 0.04213614,\n",
       "        0.0508441 , 0.04774241, 0.04403825, 0.03883362, 0.04613981,\n",
       "        0.04193625, 0.03903351, 0.04003439, 0.05755   , 0.03603129,\n",
       "        0.04323721, 0.03853354, 0.05484657, 0.04193659, 0.04143567,\n",
       "        0.04854174, 0.04333663, 0.05104427, 0.05024362, 0.04383736,\n",
       "        0.05184445, 0.04433889, 0.050844  , 0.05664873, 0.05865064,\n",
       "        0.04643984, 0.0445384 , 0.05504761, 0.05174403, 0.05374599,\n",
       "        0.05764971, 0.05214515, 0.04463873, 0.04664035, 0.05654817,\n",
       "        0.05394669, 0.04373827, 0.03733191, 0.04333687, 0.04393821,\n",
       "        0.04203596, 0.04614034, 0.04473867, 0.04163604, 0.0432374 ,\n",
       "        0.05064349, 0.04293718, 0.05284567, 0.04563918, 0.04974313,\n",
       "        0.0416358 , 0.04533877, 0.05164404, 0.04944248, 0.05294547,\n",
       "        0.04653993, 0.04593949, 0.04914198, 0.04245377, 0.04664021,\n",
       "        0.05184426, 0.05734925, 0.03843341, 0.03863306, 0.04223647,\n",
       "        0.04073534, 0.02882457, 0.02211971]),\n",
       " 'std_score_time': array([0.00472149, 0.00250487, 0.00860578, 0.00157044, 0.00492725,\n",
       "        0.00305858, 0.00589576, 0.00477222, 0.00620669, 0.01989464,\n",
       "        0.00326481, 0.0014708 , 0.01517684, 0.00410696, 0.00631098,\n",
       "        0.0151331 , 0.00517445, 0.00233464, 0.00810204, 0.00113972,\n",
       "        0.00746576, 0.00466727, 0.00400869, 0.00182852, 0.00501827,\n",
       "        0.00463654, 0.00485653, 0.01025762, 0.00596758, 0.00418077,\n",
       "        0.00159454, 0.00101946, 0.0023081 , 0.00847123, 0.00222418,\n",
       "        0.00196538, 0.00152678, 0.00217006, 0.00143041, 0.00092752,\n",
       "        0.00386857, 0.00165724, 0.00889338, 0.00696637, 0.00820848,\n",
       "        0.00378981, 0.00547151, 0.00552201, 0.00190532, 0.00103078,\n",
       "        0.00373539, 0.0029589 , 0.03252334, 0.00494904, 0.00966254,\n",
       "        0.00302537, 0.00330655, 0.00841446, 0.00132783, 0.00422713,\n",
       "        0.00336397, 0.00679126, 0.00408247, 0.00160097, 0.00491342,\n",
       "        0.00280449, 0.00151769, 0.00720025, 0.00658299, 0.00630584,\n",
       "        0.00577721, 0.00493383, 0.00442999, 0.00397778, 0.00298543,\n",
       "        0.00504944, 0.00389393, 0.01293059, 0.00481427, 0.00092961,\n",
       "        0.00427689, 0.01779412, 0.00801562, 0.01324849, 0.00935126,\n",
       "        0.00337027, 0.00387086, 0.0026752 , 0.00490944, 0.00180759,\n",
       "        0.00698964, 0.00220409, 0.00139205, 0.00373736, 0.00927376,\n",
       "        0.00156987, 0.00159536, 0.00236033, 0.00731093, 0.00239718,\n",
       "        0.00087211, 0.00063331, 0.00226869, 0.00240198, 0.00267448,\n",
       "        0.0105795 , 0.00191482, 0.00538433, 0.00066382, 0.00067877,\n",
       "        0.00282374, 0.00970043, 0.00841157, 0.00166286, 0.01090163,\n",
       "        0.00213241, 0.00464536, 0.01344299, 0.01214026, 0.00857247,\n",
       "        0.00635688, 0.01454575, 0.00356073, 0.00342861, 0.00196695,\n",
       "        0.010948  , 0.00613218, 0.00248458, 0.00598781, 0.00757348,\n",
       "        0.00365582, 0.01138294, 0.00864465, 0.00671872, 0.01276775,\n",
       "        0.01628186, 0.00727039, 0.00603493, 0.02216218, 0.00134277,\n",
       "        0.01100679, 0.01174655, 0.01337112, 0.01227668, 0.01195778,\n",
       "        0.00746282, 0.01193757, 0.00499848, 0.00503986, 0.00876573,\n",
       "        0.00703176, 0.00520644, 0.03201734, 0.00672151, 0.00814572,\n",
       "        0.00342953, 0.0041818 , 0.00211406, 0.00741852, 0.00472017,\n",
       "        0.00259848, 0.00698139, 0.00627423, 0.00533014, 0.00644386,\n",
       "        0.00443547, 0.00738579, 0.00940152, 0.00781311, 0.01245645,\n",
       "        0.0092165 , 0.01498913, 0.00958422, 0.0186639 , 0.01010973,\n",
       "        0.01423372, 0.00975836, 0.0027844 , 0.00513296, 0.00193584,\n",
       "        0.00685167, 0.00832213, 0.01255116, 0.0101446 , 0.00682794,\n",
       "        0.00563281, 0.00467947, 0.00546905, 0.00705366, 0.0010686 ,\n",
       "        0.00444925, 0.00575465, 0.00959179, 0.00883601, 0.00957731,\n",
       "        0.00379337, 0.00742536, 0.01178563, 0.00331464, 0.0042629 ,\n",
       "        0.01459363, 0.00362806, 0.02071452, 0.0135225 , 0.00738577,\n",
       "        0.01304681, 0.00627102, 0.0086289 , 0.00984706, 0.00321877,\n",
       "        0.00536729, 0.03551588, 0.01599007, 0.00463639, 0.00261813,\n",
       "        0.00335565, 0.00550361, 0.00835854, 0.00453582, 0.00595824,\n",
       "        0.01017556, 0.00791054, 0.00404082, 0.00409735, 0.00645065,\n",
       "        0.00525231, 0.00197645, 0.00448682, 0.00787467, 0.00164447,\n",
       "        0.00472199, 0.00500445, 0.00834112, 0.00891543, 0.00457972,\n",
       "        0.01409726, 0.00989017, 0.0097654 , 0.00834161, 0.00753408,\n",
       "        0.01397851, 0.01050004, 0.01425188, 0.01097078, 0.01041773,\n",
       "        0.00629361, 0.00939388, 0.01535246, 0.00548683, 0.00816588,\n",
       "        0.01823764, 0.00712224, 0.0078706 , 0.0131158 , 0.01504619,\n",
       "        0.00663474, 0.00755382, 0.00150497, 0.00447143, 0.00667951,\n",
       "        0.00672104, 0.00507783, 0.00906621, 0.00663485, 0.00385783,\n",
       "        0.0182434 , 0.01024352, 0.01578912, 0.00586537, 0.00907222,\n",
       "        0.00464493, 0.00703852, 0.01910381, 0.01356323, 0.00714373,\n",
       "        0.00971915, 0.00685002, 0.00698729, 0.00836362, 0.00894297,\n",
       "        0.01679712, 0.02292702, 0.0039579 , 0.00338508, 0.00422949,\n",
       "        0.00440334, 0.00564093, 0.00020054]),\n",
       " 'param_classifier': masked_array(data=[MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__activation': masked_array(data=['logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__alpha': masked_array(data=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__hidden_layer_sizes': masked_array(data=[(5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__learning_rate': masked_array(data=['constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'}],\n",
       " 'split0_test_recall_micro': array([0.928, 0.927, 0.925, 0.931, 0.932, 0.924, 0.926, 0.926, 0.936,\n",
       "        0.922, 0.916, 0.916, 0.928, 0.928, 0.93 , 0.925, 0.936, 0.936,\n",
       "        0.923, 0.922, 0.931, 0.931, 0.926, 0.928, 0.923, 0.925, 0.93 ,\n",
       "        0.93 , 0.932, 0.924, 0.929, 0.925, 0.936, 0.915, 0.922, 0.925,\n",
       "        0.929, 0.93 , 0.929, 0.928, 0.93 , 0.93 , 0.93 , 0.937, 0.928,\n",
       "        0.928, 0.931, 0.925, 0.932, 0.93 , 0.925, 0.926, 0.937, 0.929,\n",
       "        0.922, 0.922, 0.922, 0.918, 0.936, 0.925, 0.924, 0.933, 0.918,\n",
       "        0.929, 0.921, 0.931, 0.927, 0.926, 0.928, 0.927, 0.926, 0.936,\n",
       "        0.923, 0.932, 0.927, 0.93 , 0.923, 0.921, 0.928, 0.929, 0.926,\n",
       "        0.915, 0.919, 0.918, 0.922, 0.933, 0.93 , 0.927, 0.934, 0.925,\n",
       "        0.92 , 0.922, 0.925, 0.923, 0.925, 0.923, 0.731, 0.091, 0.746,\n",
       "        0.801, 0.307, 0.824, 0.825, 0.132, 0.814, 0.888, 0.199, 0.849,\n",
       "        0.758, 0.133, 0.743, 0.833, 0.26 , 0.82 , 0.825, 0.042, 0.827,\n",
       "        0.874, 0.112, 0.875, 0.66 , 0.211, 0.723, 0.814, 0.045, 0.819,\n",
       "        0.85 , 0.264, 0.862, 0.886, 0.188, 0.873, 0.744, 0.257, 0.687,\n",
       "        0.807, 0.133, 0.8  , 0.874, 0.189, 0.869, 0.877, 0.048, 0.839,\n",
       "        0.891, 0.19 , 0.9  , 0.906, 0.107, 0.902, 0.912, 0.357, 0.919,\n",
       "        0.922, 0.178, 0.919, 0.9  , 0.115, 0.892, 0.909, 0.357, 0.912,\n",
       "        0.917, 0.105, 0.914, 0.918, 0.541, 0.917, 0.896, 0.286, 0.905,\n",
       "        0.905, 0.111, 0.916, 0.914, 0.372, 0.916, 0.918, 0.254, 0.917,\n",
       "        0.885, 0.045, 0.891, 0.901, 0.232, 0.912, 0.912, 0.221, 0.917,\n",
       "        0.917, 0.333, 0.919, 0.866, 0.846, 0.914, 0.923, 0.926, 0.915,\n",
       "        0.926, 0.927, 0.927, 0.925, 0.931, 0.929, 0.87 , 0.873, 0.877,\n",
       "        0.924, 0.923, 0.928, 0.925, 0.927, 0.927, 0.925, 0.927, 0.924,\n",
       "        0.869, 0.868, 0.898, 0.924, 0.923, 0.918, 0.929, 0.925, 0.926,\n",
       "        0.926, 0.926, 0.925, 0.871, 0.876, 0.872, 0.925, 0.916, 0.923,\n",
       "        0.926, 0.924, 0.925, 0.927, 0.924, 0.929, 0.912, 0.921, 0.914,\n",
       "        0.929, 0.927, 0.93 , 0.93 , 0.926, 0.924, 0.929, 0.926, 0.925,\n",
       "        0.912, 0.905, 0.915, 0.927, 0.924, 0.928, 0.931, 0.925, 0.927,\n",
       "        0.929, 0.922, 0.928, 0.916, 0.875, 0.921, 0.928, 0.925, 0.925,\n",
       "        0.924, 0.93 , 0.928, 0.93 , 0.93 , 0.926, 0.909, 0.918, 0.915,\n",
       "        0.926, 0.925, 0.93 , 0.926, 0.93 , 0.929, 0.929, 0.927, 0.929]),\n",
       " 'split1_test_recall_micro': array([0.93 , 0.925, 0.931, 0.936, 0.925, 0.933, 0.926, 0.935, 0.936,\n",
       "        0.929, 0.936, 0.937, 0.93 , 0.934, 0.93 , 0.924, 0.939, 0.928,\n",
       "        0.932, 0.93 , 0.926, 0.929, 0.931, 0.928, 0.925, 0.923, 0.932,\n",
       "        0.934, 0.935, 0.931, 0.931, 0.928, 0.929, 0.927, 0.928, 0.934,\n",
       "        0.932, 0.932, 0.93 , 0.926, 0.929, 0.932, 0.928, 0.931, 0.929,\n",
       "        0.93 , 0.929, 0.936, 0.927, 0.931, 0.928, 0.929, 0.93 , 0.935,\n",
       "        0.928, 0.937, 0.925, 0.924, 0.93 , 0.93 , 0.928, 0.927, 0.926,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.929, 0.921, 0.926, 0.936,\n",
       "        0.925, 0.927, 0.928, 0.933, 0.928, 0.928, 0.934, 0.937, 0.935,\n",
       "        0.921, 0.93 , 0.924, 0.933, 0.934, 0.929, 0.929, 0.928, 0.932,\n",
       "        0.935, 0.931, 0.934, 0.93 , 0.939, 0.93 , 0.739, 0.131, 0.735,\n",
       "        0.78 , 0.133, 0.795, 0.831, 0.165, 0.829, 0.868, 0.241, 0.861,\n",
       "        0.72 , 0.26 , 0.744, 0.803, 0.246, 0.77 , 0.818, 0.257, 0.853,\n",
       "        0.871, 0.216, 0.835, 0.731, 0.189, 0.734, 0.789, 0.257, 0.807,\n",
       "        0.859, 0.257, 0.826, 0.874, 0.132, 0.872, 0.75 , 0.216, 0.689,\n",
       "        0.805, 0.289, 0.801, 0.815, 0.257, 0.848, 0.868, 0.146, 0.872,\n",
       "        0.883, 0.463, 0.908, 0.916, 0.339, 0.906, 0.917, 0.254, 0.92 ,\n",
       "        0.916, 0.378, 0.915, 0.911, 0.069, 0.911, 0.919, 0.241, 0.915,\n",
       "        0.913, 0.247, 0.915, 0.915, 0.194, 0.919, 0.911, 0.144, 0.84 ,\n",
       "        0.921, 0.199, 0.913, 0.917, 0.287, 0.914, 0.915, 0.452, 0.915,\n",
       "        0.896, 0.3  , 0.846, 0.919, 0.064, 0.916, 0.912, 0.274, 0.921,\n",
       "        0.919, 0.231, 0.915, 0.873, 0.919, 0.875, 0.92 , 0.921, 0.923,\n",
       "        0.926, 0.927, 0.923, 0.927, 0.922, 0.928, 0.876, 0.873, 0.876,\n",
       "        0.919, 0.921, 0.922, 0.926, 0.925, 0.924, 0.928, 0.931, 0.93 ,\n",
       "        0.866, 0.875, 0.876, 0.922, 0.923, 0.92 , 0.929, 0.925, 0.927,\n",
       "        0.929, 0.925, 0.925, 0.916, 0.874, 0.914, 0.92 , 0.918, 0.92 ,\n",
       "        0.921, 0.921, 0.923, 0.923, 0.926, 0.919, 0.875, 0.912, 0.918,\n",
       "        0.922, 0.925, 0.927, 0.929, 0.924, 0.927, 0.931, 0.925, 0.93 ,\n",
       "        0.922, 0.914, 0.919, 0.926, 0.93 , 0.929, 0.931, 0.93 , 0.931,\n",
       "        0.927, 0.931, 0.928, 0.917, 0.92 , 0.922, 0.922, 0.924, 0.932,\n",
       "        0.929, 0.927, 0.929, 0.934, 0.93 , 0.935, 0.916, 0.916, 0.924,\n",
       "        0.924, 0.929, 0.925, 0.933, 0.932, 0.932, 0.929, 0.93 , 0.933]),\n",
       " 'split2_test_recall_micro': array([0.938, 0.944, 0.94 , 0.937, 0.943, 0.94 , 0.932, 0.939, 0.937,\n",
       "        0.94 , 0.933, 0.933, 0.946, 0.94 , 0.938, 0.945, 0.943, 0.94 ,\n",
       "        0.937, 0.941, 0.937, 0.927, 0.926, 0.934, 0.939, 0.939, 0.945,\n",
       "        0.941, 0.937, 0.933, 0.929, 0.942, 0.937, 0.936, 0.938, 0.936,\n",
       "        0.941, 0.942, 0.942, 0.943, 0.939, 0.943, 0.942, 0.942, 0.941,\n",
       "        0.934, 0.935, 0.938, 0.939, 0.942, 0.94 , 0.944, 0.94 , 0.937,\n",
       "        0.942, 0.929, 0.942, 0.929, 0.93 , 0.941, 0.936, 0.93 , 0.942,\n",
       "        0.94 , 0.945, 0.943, 0.938, 0.941, 0.934, 0.938, 0.924, 0.936,\n",
       "        0.943, 0.939, 0.941, 0.945, 0.945, 0.939, 0.937, 0.943, 0.931,\n",
       "        0.933, 0.938, 0.939, 0.939, 0.937, 0.936, 0.937, 0.943, 0.935,\n",
       "        0.937, 0.946, 0.935, 0.937, 0.942, 0.934, 0.713, 0.217, 0.696,\n",
       "        0.796, 0.257, 0.804, 0.811, 0.147, 0.832, 0.875, 0.189, 0.866,\n",
       "        0.693, 0.143, 0.786, 0.771, 0.043, 0.787, 0.82 , 0.143, 0.825,\n",
       "        0.866, 0.257, 0.853, 0.722, 0.043, 0.727, 0.77 , 0.189, 0.774,\n",
       "        0.832, 0.143, 0.813, 0.871, 0.209, 0.861, 0.712, 0.114, 0.747,\n",
       "        0.801, 0.131, 0.786, 0.816, 0.294, 0.847, 0.852, 0.146, 0.865,\n",
       "        0.921, 0.262, 0.912, 0.922, 0.434, 0.93 , 0.926, 0.192, 0.925,\n",
       "        0.928, 0.408, 0.931, 0.915, 0.058, 0.911, 0.926, 0.371, 0.923,\n",
       "        0.926, 0.198, 0.93 , 0.925, 0.245, 0.928, 0.922, 0.274, 0.906,\n",
       "        0.92 , 0.33 , 0.923, 0.931, 0.275, 0.922, 0.929, 0.35 , 0.926,\n",
       "        0.876, 0.167, 0.915, 0.922, 0.243, 0.92 , 0.93 , 0.25 , 0.923,\n",
       "        0.926, 0.381, 0.928, 0.873, 0.919, 0.868, 0.932, 0.929, 0.926,\n",
       "        0.928, 0.927, 0.931, 0.93 , 0.933, 0.93 , 0.879, 0.878, 0.865,\n",
       "        0.933, 0.93 , 0.934, 0.935, 0.936, 0.931, 0.934, 0.933, 0.935,\n",
       "        0.88 , 0.921, 0.874, 0.931, 0.933, 0.934, 0.931, 0.93 , 0.928,\n",
       "        0.929, 0.928, 0.933, 0.926, 0.922, 0.874, 0.934, 0.928, 0.928,\n",
       "        0.934, 0.935, 0.933, 0.933, 0.938, 0.93 , 0.934, 0.928, 0.923,\n",
       "        0.931, 0.934, 0.935, 0.935, 0.934, 0.933, 0.936, 0.935, 0.932,\n",
       "        0.924, 0.923, 0.925, 0.935, 0.934, 0.934, 0.935, 0.936, 0.932,\n",
       "        0.935, 0.934, 0.935, 0.931, 0.932, 0.931, 0.932, 0.932, 0.935,\n",
       "        0.935, 0.934, 0.935, 0.935, 0.934, 0.935, 0.931, 0.928, 0.93 ,\n",
       "        0.936, 0.935, 0.932, 0.932, 0.932, 0.937, 0.935, 0.934, 0.933]),\n",
       " 'split3_test_recall_micro': array([0.924, 0.916, 0.921, 0.917, 0.923, 0.913, 0.922, 0.916, 0.926,\n",
       "        0.914, 0.919, 0.911, 0.923, 0.921, 0.918, 0.921, 0.913, 0.92 ,\n",
       "        0.914, 0.919, 0.922, 0.916, 0.901, 0.914, 0.919, 0.917, 0.92 ,\n",
       "        0.926, 0.92 , 0.915, 0.923, 0.922, 0.914, 0.914, 0.914, 0.928,\n",
       "        0.917, 0.921, 0.918, 0.916, 0.92 , 0.917, 0.923, 0.923, 0.917,\n",
       "        0.924, 0.92 , 0.912, 0.919, 0.921, 0.917, 0.917, 0.918, 0.916,\n",
       "        0.922, 0.919, 0.907, 0.92 , 0.917, 0.917, 0.924, 0.919, 0.926,\n",
       "        0.918, 0.917, 0.92 , 0.917, 0.925, 0.903, 0.915, 0.924, 0.914,\n",
       "        0.918, 0.924, 0.923, 0.921, 0.917, 0.917, 0.911, 0.922, 0.917,\n",
       "        0.912, 0.913, 0.91 , 0.917, 0.927, 0.921, 0.925, 0.923, 0.916,\n",
       "        0.917, 0.916, 0.919, 0.918, 0.915, 0.924, 0.753, 0.251, 0.769,\n",
       "        0.794, 0.071, 0.786, 0.833, 0.145, 0.825, 0.888, 0.259, 0.876,\n",
       "        0.735, 0.19 , 0.743, 0.805, 0.043, 0.796, 0.859, 0.148, 0.842,\n",
       "        0.878, 0.26 , 0.889, 0.707, 0.016, 0.755, 0.803, 0.189, 0.792,\n",
       "        0.863, 0.257, 0.858, 0.869, 0.143, 0.882, 0.746, 0.19 , 0.699,\n",
       "        0.798, 0.079, 0.808, 0.809, 0.259, 0.837, 0.882, 0.287, 0.884,\n",
       "        0.885, 0.344, 0.901, 0.915, 0.221, 0.917, 0.911, 0.264, 0.914,\n",
       "        0.917, 0.317, 0.912, 0.9  , 0.152, 0.864, 0.913, 0.142, 0.914,\n",
       "        0.914, 0.356, 0.917, 0.915, 0.466, 0.915, 0.867, 0.376, 0.882,\n",
       "        0.914, 0.092, 0.916, 0.913, 0.192, 0.917, 0.915, 0.356, 0.913,\n",
       "        0.855, 0.143, 0.913, 0.914, 0.283, 0.912, 0.913, 0.35 , 0.913,\n",
       "        0.911, 0.12 , 0.916, 0.908, 0.867, 0.91 , 0.912, 0.912, 0.912,\n",
       "        0.911, 0.913, 0.909, 0.916, 0.914, 0.913, 0.908, 0.888, 0.863,\n",
       "        0.918, 0.912, 0.911, 0.912, 0.913, 0.911, 0.916, 0.913, 0.913,\n",
       "        0.905, 0.913, 0.871, 0.916, 0.913, 0.914, 0.913, 0.911, 0.918,\n",
       "        0.912, 0.914, 0.913, 0.909, 0.916, 0.911, 0.918, 0.916, 0.914,\n",
       "        0.915, 0.914, 0.913, 0.916, 0.915, 0.914, 0.907, 0.913, 0.907,\n",
       "        0.91 , 0.91 , 0.908, 0.912, 0.911, 0.912, 0.917, 0.913, 0.912,\n",
       "        0.908, 0.914, 0.866, 0.912, 0.912, 0.914, 0.911, 0.913, 0.917,\n",
       "        0.912, 0.916, 0.912, 0.913, 0.914, 0.909, 0.914, 0.915, 0.912,\n",
       "        0.914, 0.913, 0.913, 0.916, 0.916, 0.916, 0.907, 0.911, 0.903,\n",
       "        0.912, 0.909, 0.915, 0.916, 0.914, 0.913, 0.915, 0.913, 0.914]),\n",
       " 'split4_test_recall_micro': array([0.931, 0.919, 0.929, 0.929, 0.93 , 0.931, 0.925, 0.931, 0.927,\n",
       "        0.932, 0.923, 0.919, 0.923, 0.923, 0.927, 0.933, 0.932, 0.933,\n",
       "        0.931, 0.923, 0.932, 0.925, 0.927, 0.929, 0.929, 0.921, 0.93 ,\n",
       "        0.93 , 0.931, 0.932, 0.928, 0.935, 0.922, 0.928, 0.92 , 0.937,\n",
       "        0.923, 0.928, 0.92 , 0.93 , 0.931, 0.927, 0.928, 0.927, 0.925,\n",
       "        0.924, 0.92 , 0.922, 0.93 , 0.929, 0.923, 0.929, 0.933, 0.931,\n",
       "        0.922, 0.935, 0.913, 0.921, 0.924, 0.926, 0.927, 0.927, 0.924,\n",
       "        0.923, 0.927, 0.928, 0.934, 0.923, 0.925, 0.929, 0.924, 0.921,\n",
       "        0.927, 0.925, 0.926, 0.925, 0.928, 0.928, 0.923, 0.927, 0.927,\n",
       "        0.93 , 0.926, 0.93 , 0.924, 0.922, 0.924, 0.928, 0.926, 0.928,\n",
       "        0.928, 0.929, 0.927, 0.926, 0.923, 0.912, 0.761, 0.257, 0.742,\n",
       "        0.789, 0.143, 0.797, 0.853, 0.085, 0.865, 0.86 , 0.106, 0.877,\n",
       "        0.745, 0.145, 0.735, 0.806, 0.257, 0.813, 0.844, 0.007, 0.805,\n",
       "        0.873, 0.131, 0.879, 0.752, 0.146, 0.743, 0.773, 0.286, 0.774,\n",
       "        0.812, 0.128, 0.831, 0.865, 0.256, 0.866, 0.737, 0.123, 0.726,\n",
       "        0.782, 0.173, 0.802, 0.813, 0.265, 0.836, 0.882, 0.245, 0.884,\n",
       "        0.906, 0.22 , 0.853, 0.912, 0.325, 0.917, 0.916, 0.383, 0.913,\n",
       "        0.916, 0.124, 0.918, 0.894, 0.327, 0.912, 0.913, 0.348, 0.917,\n",
       "        0.909, 0.275, 0.916, 0.918, 0.295, 0.918, 0.864, 0.048, 0.907,\n",
       "        0.914, 0.3  , 0.909, 0.914, 0.071, 0.909, 0.917, 0.339, 0.912,\n",
       "        0.909, 0.011, 0.908, 0.913, 0.145, 0.918, 0.918, 0.065, 0.918,\n",
       "        0.914, 0.456, 0.912, 0.861, 0.868, 0.911, 0.916, 0.918, 0.92 ,\n",
       "        0.916, 0.92 , 0.92 , 0.92 , 0.92 , 0.919, 0.862, 0.871, 0.919,\n",
       "        0.921, 0.918, 0.919, 0.921, 0.916, 0.919, 0.922, 0.921, 0.918,\n",
       "        0.867, 0.907, 0.868, 0.92 , 0.92 , 0.918, 0.921, 0.92 , 0.925,\n",
       "        0.92 , 0.917, 0.916, 0.866, 0.872, 0.866, 0.917, 0.92 , 0.92 ,\n",
       "        0.916, 0.916, 0.922, 0.92 , 0.922, 0.92 , 0.919, 0.913, 0.911,\n",
       "        0.923, 0.92 , 0.919, 0.922, 0.923, 0.926, 0.922, 0.926, 0.922,\n",
       "        0.921, 0.91 , 0.912, 0.913, 0.919, 0.919, 0.922, 0.921, 0.92 ,\n",
       "        0.922, 0.923, 0.921, 0.914, 0.919, 0.909, 0.92 , 0.924, 0.921,\n",
       "        0.92 , 0.924, 0.923, 0.922, 0.924, 0.92 , 0.912, 0.919, 0.912,\n",
       "        0.92 , 0.92 , 0.918, 0.92 , 0.921, 0.92 , 0.922, 0.919, 0.921]),\n",
       " 'mean_test_recall_micro': array([0.9302, 0.9262, 0.9292, 0.93  , 0.9306, 0.9282, 0.9262, 0.9294,\n",
       "        0.9324, 0.9274, 0.9254, 0.9232, 0.93  , 0.9292, 0.9286, 0.9296,\n",
       "        0.9326, 0.9314, 0.9274, 0.927 , 0.9296, 0.9256, 0.9222, 0.9266,\n",
       "        0.927 , 0.925 , 0.9314, 0.9322, 0.931 , 0.927 , 0.928 , 0.9304,\n",
       "        0.9276, 0.924 , 0.9244, 0.932 , 0.9284, 0.9306, 0.9278, 0.9286,\n",
       "        0.9298, 0.9298, 0.9302, 0.932 , 0.928 , 0.928 , 0.927 , 0.9266,\n",
       "        0.9294, 0.9306, 0.9266, 0.929 , 0.9316, 0.9296, 0.9272, 0.9284,\n",
       "        0.9218, 0.9224, 0.9274, 0.9278, 0.9278, 0.9272, 0.9272, 0.928 ,\n",
       "        0.928 , 0.9304, 0.9292, 0.929 , 0.9238, 0.926 , 0.9248, 0.9286,\n",
       "        0.9272, 0.9294, 0.929 , 0.9308, 0.9282, 0.9266, 0.9266, 0.9316,\n",
       "        0.9272, 0.9222, 0.9252, 0.9242, 0.927 , 0.9306, 0.928 , 0.9292,\n",
       "        0.9308, 0.9272, 0.9274, 0.9288, 0.928 , 0.9268, 0.9288, 0.9246,\n",
       "        0.7394, 0.1894, 0.7376, 0.792 , 0.1822, 0.8012, 0.8306, 0.1348,\n",
       "        0.833 , 0.8758, 0.1988, 0.8658, 0.7302, 0.1742, 0.7502, 0.8036,\n",
       "        0.1698, 0.7972, 0.8332, 0.1194, 0.8304, 0.8724, 0.1952, 0.8662,\n",
       "        0.7144, 0.121 , 0.7364, 0.7898, 0.1932, 0.7932, 0.8432, 0.2098,\n",
       "        0.838 , 0.873 , 0.1856, 0.8708, 0.7378, 0.18  , 0.7096, 0.7986,\n",
       "        0.161 , 0.7994, 0.8254, 0.2528, 0.8474, 0.8722, 0.1744, 0.8688,\n",
       "        0.8972, 0.2958, 0.8948, 0.9142, 0.2852, 0.9144, 0.9164, 0.29  ,\n",
       "        0.9182, 0.9198, 0.281 , 0.919 , 0.904 , 0.1442, 0.898 , 0.916 ,\n",
       "        0.2918, 0.9162, 0.9158, 0.2362, 0.9184, 0.9182, 0.3482, 0.9194,\n",
       "        0.892 , 0.2256, 0.888 , 0.9148, 0.2064, 0.9154, 0.9178, 0.2394,\n",
       "        0.9156, 0.9188, 0.3502, 0.9166, 0.8842, 0.1332, 0.8946, 0.9138,\n",
       "        0.1934, 0.9156, 0.917 , 0.232 , 0.9184, 0.9174, 0.3042, 0.918 ,\n",
       "        0.8762, 0.8838, 0.8956, 0.9206, 0.9212, 0.9192, 0.9214, 0.9228,\n",
       "        0.922 , 0.9236, 0.924 , 0.9238, 0.879 , 0.8766, 0.88  , 0.923 ,\n",
       "        0.9208, 0.9228, 0.9238, 0.9234, 0.9224, 0.925 , 0.925 , 0.924 ,\n",
       "        0.8774, 0.8968, 0.8774, 0.9226, 0.9224, 0.9208, 0.9246, 0.9222,\n",
       "        0.9248, 0.9232, 0.922 , 0.9224, 0.8976, 0.892 , 0.8874, 0.9228,\n",
       "        0.9196, 0.921 , 0.9224, 0.922 , 0.9232, 0.9238, 0.925 , 0.9224,\n",
       "        0.9094, 0.9174, 0.9146, 0.923 , 0.9232, 0.9238, 0.9256, 0.9236,\n",
       "        0.9244, 0.927 , 0.925 , 0.9242, 0.9174, 0.9132, 0.9074, 0.9226,\n",
       "        0.9238, 0.9248, 0.926 , 0.925 , 0.9254, 0.925 , 0.9252, 0.9248,\n",
       "        0.9182, 0.912 , 0.9184, 0.9232, 0.924 , 0.925 , 0.9244, 0.9256,\n",
       "        0.9256, 0.9274, 0.9268, 0.9264, 0.915 , 0.9184, 0.9168, 0.9236,\n",
       "        0.9236, 0.924 , 0.9254, 0.9258, 0.9262, 0.926 , 0.9246, 0.926 ]),\n",
       " 'std_test_recall_micro': array([0.00457821, 0.00974474, 0.0064    , 0.00715542, 0.00700286,\n",
       "        0.00915205, 0.00324962, 0.00796492, 0.00484149, 0.00884534,\n",
       "        0.00781281, 0.01004789, 0.00846168, 0.00702567, 0.00643739,\n",
       "        0.00866256, 0.01044222, 0.00691665, 0.00806474, 0.00787401,\n",
       "        0.0051614 , 0.0052    , 0.01075918, 0.00668132, 0.00681175,\n",
       "        0.00748331, 0.00798999, 0.00507543, 0.00589915, 0.00678233,\n",
       "        0.00268328, 0.00722772, 0.00868562, 0.0083666 , 0.0081388 ,\n",
       "        0.00469042, 0.0081388 , 0.0068    , 0.00854166, 0.00866256,\n",
       "        0.00604649, 0.00837616, 0.00633719, 0.00681175, 0.00774597,\n",
       "        0.00379473, 0.00603324, 0.00954149, 0.00652993, 0.00671118,\n",
       "        0.00760526, 0.00869483, 0.00760526, 0.00736478, 0.00775629,\n",
       "        0.00703136, 0.01195659, 0.00382623, 0.00643739, 0.00783326,\n",
       "        0.0044    , 0.00466476, 0.0079599 , 0.0074027 , 0.00963328,\n",
       "        0.00739189, 0.00713863, 0.00641872, 0.0107963 , 0.00774597,\n",
       "        0.0009798 , 0.00932952, 0.00844748, 0.00553534, 0.00622896,\n",
       "        0.00820731, 0.00932523, 0.00749933, 0.00917824, 0.00747262,\n",
       "        0.00601332, 0.00818291, 0.00865794, 0.00992774, 0.00792465,\n",
       "        0.00538888, 0.00517687, 0.00411825, 0.00708237, 0.00655439,\n",
       "        0.00791454, 0.01010742, 0.00593296, 0.00643117, 0.01016661,\n",
       "        0.00747262, 0.01684755, 0.06666213, 0.02372003, 0.00712741,\n",
       "        0.08659423, 0.01276558, 0.01358823, 0.02702887, 0.01712308,\n",
       "        0.01103449, 0.05312777, 0.01034215, 0.0223732 , 0.04719068,\n",
       "        0.01819231, 0.01967333, 0.10363667, 0.01797109, 0.01584172,\n",
       "        0.08824194, 0.01631686, 0.00392938, 0.06244165, 0.01953868,\n",
       "        0.03085839, 0.0780487 , 0.01151694, 0.01693989, 0.08328361,\n",
       "        0.01785945, 0.01890397, 0.06090452, 0.01894202, 0.00712741,\n",
       "        0.04516016, 0.00708237, 0.01357056, 0.0546443 , 0.02330322,\n",
       "        0.00886792, 0.07062011, 0.00725534, 0.02441803, 0.03456241,\n",
       "        0.01187603, 0.01132078, 0.08396809, 0.01658192, 0.0143722 ,\n",
       "        0.09834714, 0.02136726, 0.00523068, 0.11180054, 0.0098102 ,\n",
       "        0.00531413, 0.07030505, 0.00435431, 0.00466476, 0.11143788,\n",
       "        0.00648074, 0.00777174, 0.09738049, 0.01857956, 0.00593296,\n",
       "        0.08798045, 0.00376298, 0.00570614, 0.08324278, 0.00588558,\n",
       "        0.00365513, 0.13289605, 0.00449889, 0.02317758, 0.11559516,\n",
       "        0.0257449 , 0.00570614, 0.09620936, 0.00458694, 0.00673498,\n",
       "        0.10170664, 0.00422374, 0.00523068, 0.06289165, 0.005004  ,\n",
       "        0.01830191, 0.10175736, 0.02572625, 0.00719444, 0.07882284,\n",
       "        0.0032    , 0.00687023, 0.09383176, 0.00344093, 0.00508331,\n",
       "        0.11752344, 0.00547723, 0.0165336 , 0.0297953 , 0.0198454 ,\n",
       "        0.0068    , 0.00597997, 0.00511468, 0.00668132, 0.0056    ,\n",
       "        0.00748331, 0.005004  , 0.00707107, 0.00667533, 0.0156205 ,\n",
       "        0.00615142, 0.02029778, 0.0054037 , 0.0059127 , 0.00783326,\n",
       "        0.00746726, 0.00821219, 0.00691665, 0.006     , 0.00726636,\n",
       "        0.00792465, 0.01467787, 0.02124523, 0.01065082, 0.00496387,\n",
       "        0.00643739, 0.00688186, 0.00674092, 0.00643117, 0.00354401,\n",
       "        0.00649307, 0.00547723, 0.00714423, 0.02441803, 0.02216303,\n",
       "        0.02068429, 0.00624179, 0.00445421, 0.0045607 , 0.00700286,\n",
       "        0.0074027 , 0.0064    , 0.00584466, 0.00748331, 0.00615142,\n",
       "        0.01945867, 0.00621611, 0.00553534, 0.00734847, 0.00798499,\n",
       "        0.00945304, 0.00796492, 0.00739189, 0.00688767, 0.00672309,\n",
       "        0.00701427, 0.00705408, 0.0062482 , 0.0059127 , 0.02115278,\n",
       "        0.0088227 , 0.00780769, 0.00724983, 0.00862554, 0.00782304,\n",
       "        0.00595315, 0.0077201 , 0.00649307, 0.00778203, 0.00655439,\n",
       "        0.01942164, 0.00842852, 0.00627375, 0.0054037 , 0.00817313,\n",
       "        0.00722772, 0.00711618, 0.00736478, 0.00731027, 0.00627375,\n",
       "        0.00770973, 0.0085557 , 0.00553534, 0.00941063, 0.00783837,\n",
       "        0.0088    , 0.00660303, 0.00662118, 0.00716659, 0.00861162,\n",
       "        0.00687023, 0.00760526, 0.00742967]),\n",
       " 'rank_test_recall_micro': array([ 19,  83,  31,  21,  13,  45,  83,  28,   2,  58,  95, 136,  21,\n",
       "         32,  40,  26,   1,   8,  58,  71,  25,  92, 154,  78,  71, 101,\n",
       "          8,   3,  10,  69,  47,  17,  57, 121, 115,   4,  43,  13,  54,\n",
       "         42,  24,  23,  19,   4,  53,  47,  71,  78,  28,  13,  78,  35,\n",
       "          6,  26,  64,  43, 160, 149,  58,  54,  54,  64,  64,  47,  47,\n",
       "         17,  32,  35, 125,  86, 108,  40,  64,  28,  37,  11,  45,  78,\n",
       "         77,   6,  63, 156,  98, 118,  69,  13,  47,  32,  11,  64,  62,\n",
       "         38,  47,  75,  38, 113, 250, 276, 252, 247, 278, 242, 238, 285,\n",
       "        237, 225, 272, 232, 254, 281, 249, 241, 282, 245, 236, 288, 239,\n",
       "        227, 273, 231, 255, 287, 253, 248, 275, 246, 234, 270, 235, 226,\n",
       "        277, 229, 251, 279, 256, 244, 283, 243, 240, 265, 233, 228, 280,\n",
       "        230, 208, 260, 211, 199, 263, 198, 188, 262, 177, 167, 264, 171,\n",
       "        205, 284, 206, 190, 261, 189, 191, 267, 173, 177, 258, 169, 213,\n",
       "        269, 215, 196, 271, 194, 181, 266, 192, 172, 257, 187, 217, 286,\n",
       "        212, 200, 274, 192, 185, 268, 173, 183, 259, 180, 224, 218, 210,\n",
       "        166, 162, 170, 161, 145, 157, 131, 120, 127, 220, 223, 219, 141,\n",
       "        164, 143, 127, 135, 149, 101, 100, 121, 222, 209, 221, 146, 149,\n",
       "        164, 113, 154, 108, 139, 157, 148, 207, 213, 216, 143, 168, 163,\n",
       "        149, 157, 139, 125, 101, 149, 203, 182, 197, 141, 136, 127,  92,\n",
       "        131, 117,  71, 101, 119, 183, 201, 204, 146, 127, 108,  86, 101,\n",
       "         96, 101,  98, 108, 177, 202, 176, 136, 121, 101, 115,  91,  92,\n",
       "         58,  75,  82, 195, 173, 186, 131, 131, 121,  96,  90,  83,  86,\n",
       "        112,  86]),\n",
       " 'split0_test_f1_micro': array([0.928, 0.927, 0.925, 0.931, 0.932, 0.924, 0.926, 0.926, 0.936,\n",
       "        0.922, 0.916, 0.916, 0.928, 0.928, 0.93 , 0.925, 0.936, 0.936,\n",
       "        0.923, 0.922, 0.931, 0.931, 0.926, 0.928, 0.923, 0.925, 0.93 ,\n",
       "        0.93 , 0.932, 0.924, 0.929, 0.925, 0.936, 0.915, 0.922, 0.925,\n",
       "        0.929, 0.93 , 0.929, 0.928, 0.93 , 0.93 , 0.93 , 0.937, 0.928,\n",
       "        0.928, 0.931, 0.925, 0.932, 0.93 , 0.925, 0.926, 0.937, 0.929,\n",
       "        0.922, 0.922, 0.922, 0.918, 0.936, 0.925, 0.924, 0.933, 0.918,\n",
       "        0.929, 0.921, 0.931, 0.927, 0.926, 0.928, 0.927, 0.926, 0.936,\n",
       "        0.923, 0.932, 0.927, 0.93 , 0.923, 0.921, 0.928, 0.929, 0.926,\n",
       "        0.915, 0.919, 0.918, 0.922, 0.933, 0.93 , 0.927, 0.934, 0.925,\n",
       "        0.92 , 0.922, 0.925, 0.923, 0.925, 0.923, 0.731, 0.091, 0.746,\n",
       "        0.801, 0.307, 0.824, 0.825, 0.132, 0.814, 0.888, 0.199, 0.849,\n",
       "        0.758, 0.133, 0.743, 0.833, 0.26 , 0.82 , 0.825, 0.042, 0.827,\n",
       "        0.874, 0.112, 0.875, 0.66 , 0.211, 0.723, 0.814, 0.045, 0.819,\n",
       "        0.85 , 0.264, 0.862, 0.886, 0.188, 0.873, 0.744, 0.257, 0.687,\n",
       "        0.807, 0.133, 0.8  , 0.874, 0.189, 0.869, 0.877, 0.048, 0.839,\n",
       "        0.891, 0.19 , 0.9  , 0.906, 0.107, 0.902, 0.912, 0.357, 0.919,\n",
       "        0.922, 0.178, 0.919, 0.9  , 0.115, 0.892, 0.909, 0.357, 0.912,\n",
       "        0.917, 0.105, 0.914, 0.918, 0.541, 0.917, 0.896, 0.286, 0.905,\n",
       "        0.905, 0.111, 0.916, 0.914, 0.372, 0.916, 0.918, 0.254, 0.917,\n",
       "        0.885, 0.045, 0.891, 0.901, 0.232, 0.912, 0.912, 0.221, 0.917,\n",
       "        0.917, 0.333, 0.919, 0.866, 0.846, 0.914, 0.923, 0.926, 0.915,\n",
       "        0.926, 0.927, 0.927, 0.925, 0.931, 0.929, 0.87 , 0.873, 0.877,\n",
       "        0.924, 0.923, 0.928, 0.925, 0.927, 0.927, 0.925, 0.927, 0.924,\n",
       "        0.869, 0.868, 0.898, 0.924, 0.923, 0.918, 0.929, 0.925, 0.926,\n",
       "        0.926, 0.926, 0.925, 0.871, 0.876, 0.872, 0.925, 0.916, 0.923,\n",
       "        0.926, 0.924, 0.925, 0.927, 0.924, 0.929, 0.912, 0.921, 0.914,\n",
       "        0.929, 0.927, 0.93 , 0.93 , 0.926, 0.924, 0.929, 0.926, 0.925,\n",
       "        0.912, 0.905, 0.915, 0.927, 0.924, 0.928, 0.931, 0.925, 0.927,\n",
       "        0.929, 0.922, 0.928, 0.916, 0.875, 0.921, 0.928, 0.925, 0.925,\n",
       "        0.924, 0.93 , 0.928, 0.93 , 0.93 , 0.926, 0.909, 0.918, 0.915,\n",
       "        0.926, 0.925, 0.93 , 0.926, 0.93 , 0.929, 0.929, 0.927, 0.929]),\n",
       " 'split1_test_f1_micro': array([0.93 , 0.925, 0.931, 0.936, 0.925, 0.933, 0.926, 0.935, 0.936,\n",
       "        0.929, 0.936, 0.937, 0.93 , 0.934, 0.93 , 0.924, 0.939, 0.928,\n",
       "        0.932, 0.93 , 0.926, 0.929, 0.931, 0.928, 0.925, 0.923, 0.932,\n",
       "        0.934, 0.935, 0.931, 0.931, 0.928, 0.929, 0.927, 0.928, 0.934,\n",
       "        0.932, 0.932, 0.93 , 0.926, 0.929, 0.932, 0.928, 0.931, 0.929,\n",
       "        0.93 , 0.929, 0.936, 0.927, 0.931, 0.928, 0.929, 0.93 , 0.935,\n",
       "        0.928, 0.937, 0.925, 0.924, 0.93 , 0.93 , 0.928, 0.927, 0.926,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.929, 0.921, 0.926, 0.936,\n",
       "        0.925, 0.927, 0.928, 0.933, 0.928, 0.928, 0.934, 0.937, 0.935,\n",
       "        0.921, 0.93 , 0.924, 0.933, 0.934, 0.929, 0.929, 0.928, 0.932,\n",
       "        0.935, 0.931, 0.934, 0.93 , 0.939, 0.93 , 0.739, 0.131, 0.735,\n",
       "        0.78 , 0.133, 0.795, 0.831, 0.165, 0.829, 0.868, 0.241, 0.861,\n",
       "        0.72 , 0.26 , 0.744, 0.803, 0.246, 0.77 , 0.818, 0.257, 0.853,\n",
       "        0.871, 0.216, 0.835, 0.731, 0.189, 0.734, 0.789, 0.257, 0.807,\n",
       "        0.859, 0.257, 0.826, 0.874, 0.132, 0.872, 0.75 , 0.216, 0.689,\n",
       "        0.805, 0.289, 0.801, 0.815, 0.257, 0.848, 0.868, 0.146, 0.872,\n",
       "        0.883, 0.463, 0.908, 0.916, 0.339, 0.906, 0.917, 0.254, 0.92 ,\n",
       "        0.916, 0.378, 0.915, 0.911, 0.069, 0.911, 0.919, 0.241, 0.915,\n",
       "        0.913, 0.247, 0.915, 0.915, 0.194, 0.919, 0.911, 0.144, 0.84 ,\n",
       "        0.921, 0.199, 0.913, 0.917, 0.287, 0.914, 0.915, 0.452, 0.915,\n",
       "        0.896, 0.3  , 0.846, 0.919, 0.064, 0.916, 0.912, 0.274, 0.921,\n",
       "        0.919, 0.231, 0.915, 0.873, 0.919, 0.875, 0.92 , 0.921, 0.923,\n",
       "        0.926, 0.927, 0.923, 0.927, 0.922, 0.928, 0.876, 0.873, 0.876,\n",
       "        0.919, 0.921, 0.922, 0.926, 0.925, 0.924, 0.928, 0.931, 0.93 ,\n",
       "        0.866, 0.875, 0.876, 0.922, 0.923, 0.92 , 0.929, 0.925, 0.927,\n",
       "        0.929, 0.925, 0.925, 0.916, 0.874, 0.914, 0.92 , 0.918, 0.92 ,\n",
       "        0.921, 0.921, 0.923, 0.923, 0.926, 0.919, 0.875, 0.912, 0.918,\n",
       "        0.922, 0.925, 0.927, 0.929, 0.924, 0.927, 0.931, 0.925, 0.93 ,\n",
       "        0.922, 0.914, 0.919, 0.926, 0.93 , 0.929, 0.931, 0.93 , 0.931,\n",
       "        0.927, 0.931, 0.928, 0.917, 0.92 , 0.922, 0.922, 0.924, 0.932,\n",
       "        0.929, 0.927, 0.929, 0.934, 0.93 , 0.935, 0.916, 0.916, 0.924,\n",
       "        0.924, 0.929, 0.925, 0.933, 0.932, 0.932, 0.929, 0.93 , 0.933]),\n",
       " 'split2_test_f1_micro': array([0.938, 0.944, 0.94 , 0.937, 0.943, 0.94 , 0.932, 0.939, 0.937,\n",
       "        0.94 , 0.933, 0.933, 0.946, 0.94 , 0.938, 0.945, 0.943, 0.94 ,\n",
       "        0.937, 0.941, 0.937, 0.927, 0.926, 0.934, 0.939, 0.939, 0.945,\n",
       "        0.941, 0.937, 0.933, 0.929, 0.942, 0.937, 0.936, 0.938, 0.936,\n",
       "        0.941, 0.942, 0.942, 0.943, 0.939, 0.943, 0.942, 0.942, 0.941,\n",
       "        0.934, 0.935, 0.938, 0.939, 0.942, 0.94 , 0.944, 0.94 , 0.937,\n",
       "        0.942, 0.929, 0.942, 0.929, 0.93 , 0.941, 0.936, 0.93 , 0.942,\n",
       "        0.94 , 0.945, 0.943, 0.938, 0.941, 0.934, 0.938, 0.924, 0.936,\n",
       "        0.943, 0.939, 0.941, 0.945, 0.945, 0.939, 0.937, 0.943, 0.931,\n",
       "        0.933, 0.938, 0.939, 0.939, 0.937, 0.936, 0.937, 0.943, 0.935,\n",
       "        0.937, 0.946, 0.935, 0.937, 0.942, 0.934, 0.713, 0.217, 0.696,\n",
       "        0.796, 0.257, 0.804, 0.811, 0.147, 0.832, 0.875, 0.189, 0.866,\n",
       "        0.693, 0.143, 0.786, 0.771, 0.043, 0.787, 0.82 , 0.143, 0.825,\n",
       "        0.866, 0.257, 0.853, 0.722, 0.043, 0.727, 0.77 , 0.189, 0.774,\n",
       "        0.832, 0.143, 0.813, 0.871, 0.209, 0.861, 0.712, 0.114, 0.747,\n",
       "        0.801, 0.131, 0.786, 0.816, 0.294, 0.847, 0.852, 0.146, 0.865,\n",
       "        0.921, 0.262, 0.912, 0.922, 0.434, 0.93 , 0.926, 0.192, 0.925,\n",
       "        0.928, 0.408, 0.931, 0.915, 0.058, 0.911, 0.926, 0.371, 0.923,\n",
       "        0.926, 0.198, 0.93 , 0.925, 0.245, 0.928, 0.922, 0.274, 0.906,\n",
       "        0.92 , 0.33 , 0.923, 0.931, 0.275, 0.922, 0.929, 0.35 , 0.926,\n",
       "        0.876, 0.167, 0.915, 0.922, 0.243, 0.92 , 0.93 , 0.25 , 0.923,\n",
       "        0.926, 0.381, 0.928, 0.873, 0.919, 0.868, 0.932, 0.929, 0.926,\n",
       "        0.928, 0.927, 0.931, 0.93 , 0.933, 0.93 , 0.879, 0.878, 0.865,\n",
       "        0.933, 0.93 , 0.934, 0.935, 0.936, 0.931, 0.934, 0.933, 0.935,\n",
       "        0.88 , 0.921, 0.874, 0.931, 0.933, 0.934, 0.931, 0.93 , 0.928,\n",
       "        0.929, 0.928, 0.933, 0.926, 0.922, 0.874, 0.934, 0.928, 0.928,\n",
       "        0.934, 0.935, 0.933, 0.933, 0.938, 0.93 , 0.934, 0.928, 0.923,\n",
       "        0.931, 0.934, 0.935, 0.935, 0.934, 0.933, 0.936, 0.935, 0.932,\n",
       "        0.924, 0.923, 0.925, 0.935, 0.934, 0.934, 0.935, 0.936, 0.932,\n",
       "        0.935, 0.934, 0.935, 0.931, 0.932, 0.931, 0.932, 0.932, 0.935,\n",
       "        0.935, 0.934, 0.935, 0.935, 0.934, 0.935, 0.931, 0.928, 0.93 ,\n",
       "        0.936, 0.935, 0.932, 0.932, 0.932, 0.937, 0.935, 0.934, 0.933]),\n",
       " 'split3_test_f1_micro': array([0.924, 0.916, 0.921, 0.917, 0.923, 0.913, 0.922, 0.916, 0.926,\n",
       "        0.914, 0.919, 0.911, 0.923, 0.921, 0.918, 0.921, 0.913, 0.92 ,\n",
       "        0.914, 0.919, 0.922, 0.916, 0.901, 0.914, 0.919, 0.917, 0.92 ,\n",
       "        0.926, 0.92 , 0.915, 0.923, 0.922, 0.914, 0.914, 0.914, 0.928,\n",
       "        0.917, 0.921, 0.918, 0.916, 0.92 , 0.917, 0.923, 0.923, 0.917,\n",
       "        0.924, 0.92 , 0.912, 0.919, 0.921, 0.917, 0.917, 0.918, 0.916,\n",
       "        0.922, 0.919, 0.907, 0.92 , 0.917, 0.917, 0.924, 0.919, 0.926,\n",
       "        0.918, 0.917, 0.92 , 0.917, 0.925, 0.903, 0.915, 0.924, 0.914,\n",
       "        0.918, 0.924, 0.923, 0.921, 0.917, 0.917, 0.911, 0.922, 0.917,\n",
       "        0.912, 0.913, 0.91 , 0.917, 0.927, 0.921, 0.925, 0.923, 0.916,\n",
       "        0.917, 0.916, 0.919, 0.918, 0.915, 0.924, 0.753, 0.251, 0.769,\n",
       "        0.794, 0.071, 0.786, 0.833, 0.145, 0.825, 0.888, 0.259, 0.876,\n",
       "        0.735, 0.19 , 0.743, 0.805, 0.043, 0.796, 0.859, 0.148, 0.842,\n",
       "        0.878, 0.26 , 0.889, 0.707, 0.016, 0.755, 0.803, 0.189, 0.792,\n",
       "        0.863, 0.257, 0.858, 0.869, 0.143, 0.882, 0.746, 0.19 , 0.699,\n",
       "        0.798, 0.079, 0.808, 0.809, 0.259, 0.837, 0.882, 0.287, 0.884,\n",
       "        0.885, 0.344, 0.901, 0.915, 0.221, 0.917, 0.911, 0.264, 0.914,\n",
       "        0.917, 0.317, 0.912, 0.9  , 0.152, 0.864, 0.913, 0.142, 0.914,\n",
       "        0.914, 0.356, 0.917, 0.915, 0.466, 0.915, 0.867, 0.376, 0.882,\n",
       "        0.914, 0.092, 0.916, 0.913, 0.192, 0.917, 0.915, 0.356, 0.913,\n",
       "        0.855, 0.143, 0.913, 0.914, 0.283, 0.912, 0.913, 0.35 , 0.913,\n",
       "        0.911, 0.12 , 0.916, 0.908, 0.867, 0.91 , 0.912, 0.912, 0.912,\n",
       "        0.911, 0.913, 0.909, 0.916, 0.914, 0.913, 0.908, 0.888, 0.863,\n",
       "        0.918, 0.912, 0.911, 0.912, 0.913, 0.911, 0.916, 0.913, 0.913,\n",
       "        0.905, 0.913, 0.871, 0.916, 0.913, 0.914, 0.913, 0.911, 0.918,\n",
       "        0.912, 0.914, 0.913, 0.909, 0.916, 0.911, 0.918, 0.916, 0.914,\n",
       "        0.915, 0.914, 0.913, 0.916, 0.915, 0.914, 0.907, 0.913, 0.907,\n",
       "        0.91 , 0.91 , 0.908, 0.912, 0.911, 0.912, 0.917, 0.913, 0.912,\n",
       "        0.908, 0.914, 0.866, 0.912, 0.912, 0.914, 0.911, 0.913, 0.917,\n",
       "        0.912, 0.916, 0.912, 0.913, 0.914, 0.909, 0.914, 0.915, 0.912,\n",
       "        0.914, 0.913, 0.913, 0.916, 0.916, 0.916, 0.907, 0.911, 0.903,\n",
       "        0.912, 0.909, 0.915, 0.916, 0.914, 0.913, 0.915, 0.913, 0.914]),\n",
       " 'split4_test_f1_micro': array([0.931, 0.919, 0.929, 0.929, 0.93 , 0.931, 0.925, 0.931, 0.927,\n",
       "        0.932, 0.923, 0.919, 0.923, 0.923, 0.927, 0.933, 0.932, 0.933,\n",
       "        0.931, 0.923, 0.932, 0.925, 0.927, 0.929, 0.929, 0.921, 0.93 ,\n",
       "        0.93 , 0.931, 0.932, 0.928, 0.935, 0.922, 0.928, 0.92 , 0.937,\n",
       "        0.923, 0.928, 0.92 , 0.93 , 0.931, 0.927, 0.928, 0.927, 0.925,\n",
       "        0.924, 0.92 , 0.922, 0.93 , 0.929, 0.923, 0.929, 0.933, 0.931,\n",
       "        0.922, 0.935, 0.913, 0.921, 0.924, 0.926, 0.927, 0.927, 0.924,\n",
       "        0.923, 0.927, 0.928, 0.934, 0.923, 0.925, 0.929, 0.924, 0.921,\n",
       "        0.927, 0.925, 0.926, 0.925, 0.928, 0.928, 0.923, 0.927, 0.927,\n",
       "        0.93 , 0.926, 0.93 , 0.924, 0.922, 0.924, 0.928, 0.926, 0.928,\n",
       "        0.928, 0.929, 0.927, 0.926, 0.923, 0.912, 0.761, 0.257, 0.742,\n",
       "        0.789, 0.143, 0.797, 0.853, 0.085, 0.865, 0.86 , 0.106, 0.877,\n",
       "        0.745, 0.145, 0.735, 0.806, 0.257, 0.813, 0.844, 0.007, 0.805,\n",
       "        0.873, 0.131, 0.879, 0.752, 0.146, 0.743, 0.773, 0.286, 0.774,\n",
       "        0.812, 0.128, 0.831, 0.865, 0.256, 0.866, 0.737, 0.123, 0.726,\n",
       "        0.782, 0.173, 0.802, 0.813, 0.265, 0.836, 0.882, 0.245, 0.884,\n",
       "        0.906, 0.22 , 0.853, 0.912, 0.325, 0.917, 0.916, 0.383, 0.913,\n",
       "        0.916, 0.124, 0.918, 0.894, 0.327, 0.912, 0.913, 0.348, 0.917,\n",
       "        0.909, 0.275, 0.916, 0.918, 0.295, 0.918, 0.864, 0.048, 0.907,\n",
       "        0.914, 0.3  , 0.909, 0.914, 0.071, 0.909, 0.917, 0.339, 0.912,\n",
       "        0.909, 0.011, 0.908, 0.913, 0.145, 0.918, 0.918, 0.065, 0.918,\n",
       "        0.914, 0.456, 0.912, 0.861, 0.868, 0.911, 0.916, 0.918, 0.92 ,\n",
       "        0.916, 0.92 , 0.92 , 0.92 , 0.92 , 0.919, 0.862, 0.871, 0.919,\n",
       "        0.921, 0.918, 0.919, 0.921, 0.916, 0.919, 0.922, 0.921, 0.918,\n",
       "        0.867, 0.907, 0.868, 0.92 , 0.92 , 0.918, 0.921, 0.92 , 0.925,\n",
       "        0.92 , 0.917, 0.916, 0.866, 0.872, 0.866, 0.917, 0.92 , 0.92 ,\n",
       "        0.916, 0.916, 0.922, 0.92 , 0.922, 0.92 , 0.919, 0.913, 0.911,\n",
       "        0.923, 0.92 , 0.919, 0.922, 0.923, 0.926, 0.922, 0.926, 0.922,\n",
       "        0.921, 0.91 , 0.912, 0.913, 0.919, 0.919, 0.922, 0.921, 0.92 ,\n",
       "        0.922, 0.923, 0.921, 0.914, 0.919, 0.909, 0.92 , 0.924, 0.921,\n",
       "        0.92 , 0.924, 0.923, 0.922, 0.924, 0.92 , 0.912, 0.919, 0.912,\n",
       "        0.92 , 0.92 , 0.918, 0.92 , 0.921, 0.92 , 0.922, 0.919, 0.921]),\n",
       " 'mean_test_f1_micro': array([0.9302, 0.9262, 0.9292, 0.93  , 0.9306, 0.9282, 0.9262, 0.9294,\n",
       "        0.9324, 0.9274, 0.9254, 0.9232, 0.93  , 0.9292, 0.9286, 0.9296,\n",
       "        0.9326, 0.9314, 0.9274, 0.927 , 0.9296, 0.9256, 0.9222, 0.9266,\n",
       "        0.927 , 0.925 , 0.9314, 0.9322, 0.931 , 0.927 , 0.928 , 0.9304,\n",
       "        0.9276, 0.924 , 0.9244, 0.932 , 0.9284, 0.9306, 0.9278, 0.9286,\n",
       "        0.9298, 0.9298, 0.9302, 0.932 , 0.928 , 0.928 , 0.927 , 0.9266,\n",
       "        0.9294, 0.9306, 0.9266, 0.929 , 0.9316, 0.9296, 0.9272, 0.9284,\n",
       "        0.9218, 0.9224, 0.9274, 0.9278, 0.9278, 0.9272, 0.9272, 0.928 ,\n",
       "        0.928 , 0.9304, 0.9292, 0.929 , 0.9238, 0.926 , 0.9248, 0.9286,\n",
       "        0.9272, 0.9294, 0.929 , 0.9308, 0.9282, 0.9266, 0.9266, 0.9316,\n",
       "        0.9272, 0.9222, 0.9252, 0.9242, 0.927 , 0.9306, 0.928 , 0.9292,\n",
       "        0.9308, 0.9272, 0.9274, 0.9288, 0.928 , 0.9268, 0.9288, 0.9246,\n",
       "        0.7394, 0.1894, 0.7376, 0.792 , 0.1822, 0.8012, 0.8306, 0.1348,\n",
       "        0.833 , 0.8758, 0.1988, 0.8658, 0.7302, 0.1742, 0.7502, 0.8036,\n",
       "        0.1698, 0.7972, 0.8332, 0.1194, 0.8304, 0.8724, 0.1952, 0.8662,\n",
       "        0.7144, 0.121 , 0.7364, 0.7898, 0.1932, 0.7932, 0.8432, 0.2098,\n",
       "        0.838 , 0.873 , 0.1856, 0.8708, 0.7378, 0.18  , 0.7096, 0.7986,\n",
       "        0.161 , 0.7994, 0.8254, 0.2528, 0.8474, 0.8722, 0.1744, 0.8688,\n",
       "        0.8972, 0.2958, 0.8948, 0.9142, 0.2852, 0.9144, 0.9164, 0.29  ,\n",
       "        0.9182, 0.9198, 0.281 , 0.919 , 0.904 , 0.1442, 0.898 , 0.916 ,\n",
       "        0.2918, 0.9162, 0.9158, 0.2362, 0.9184, 0.9182, 0.3482, 0.9194,\n",
       "        0.892 , 0.2256, 0.888 , 0.9148, 0.2064, 0.9154, 0.9178, 0.2394,\n",
       "        0.9156, 0.9188, 0.3502, 0.9166, 0.8842, 0.1332, 0.8946, 0.9138,\n",
       "        0.1934, 0.9156, 0.917 , 0.232 , 0.9184, 0.9174, 0.3042, 0.918 ,\n",
       "        0.8762, 0.8838, 0.8956, 0.9206, 0.9212, 0.9192, 0.9214, 0.9228,\n",
       "        0.922 , 0.9236, 0.924 , 0.9238, 0.879 , 0.8766, 0.88  , 0.923 ,\n",
       "        0.9208, 0.9228, 0.9238, 0.9234, 0.9224, 0.925 , 0.925 , 0.924 ,\n",
       "        0.8774, 0.8968, 0.8774, 0.9226, 0.9224, 0.9208, 0.9246, 0.9222,\n",
       "        0.9248, 0.9232, 0.922 , 0.9224, 0.8976, 0.892 , 0.8874, 0.9228,\n",
       "        0.9196, 0.921 , 0.9224, 0.922 , 0.9232, 0.9238, 0.925 , 0.9224,\n",
       "        0.9094, 0.9174, 0.9146, 0.923 , 0.9232, 0.9238, 0.9256, 0.9236,\n",
       "        0.9244, 0.927 , 0.925 , 0.9242, 0.9174, 0.9132, 0.9074, 0.9226,\n",
       "        0.9238, 0.9248, 0.926 , 0.925 , 0.9254, 0.925 , 0.9252, 0.9248,\n",
       "        0.9182, 0.912 , 0.9184, 0.9232, 0.924 , 0.925 , 0.9244, 0.9256,\n",
       "        0.9256, 0.9274, 0.9268, 0.9264, 0.915 , 0.9184, 0.9168, 0.9236,\n",
       "        0.9236, 0.924 , 0.9254, 0.9258, 0.9262, 0.926 , 0.9246, 0.926 ]),\n",
       " 'std_test_f1_micro': array([0.00457821, 0.00974474, 0.0064    , 0.00715542, 0.00700286,\n",
       "        0.00915205, 0.00324962, 0.00796492, 0.00484149, 0.00884534,\n",
       "        0.00781281, 0.01004789, 0.00846168, 0.00702567, 0.00643739,\n",
       "        0.00866256, 0.01044222, 0.00691665, 0.00806474, 0.00787401,\n",
       "        0.0051614 , 0.0052    , 0.01075918, 0.00668132, 0.00681175,\n",
       "        0.00748331, 0.00798999, 0.00507543, 0.00589915, 0.00678233,\n",
       "        0.00268328, 0.00722772, 0.00868562, 0.0083666 , 0.0081388 ,\n",
       "        0.00469042, 0.0081388 , 0.0068    , 0.00854166, 0.00866256,\n",
       "        0.00604649, 0.00837616, 0.00633719, 0.00681175, 0.00774597,\n",
       "        0.00379473, 0.00603324, 0.00954149, 0.00652993, 0.00671118,\n",
       "        0.00760526, 0.00869483, 0.00760526, 0.00736478, 0.00775629,\n",
       "        0.00703136, 0.01195659, 0.00382623, 0.00643739, 0.00783326,\n",
       "        0.0044    , 0.00466476, 0.0079599 , 0.0074027 , 0.00963328,\n",
       "        0.00739189, 0.00713863, 0.00641872, 0.0107963 , 0.00774597,\n",
       "        0.0009798 , 0.00932952, 0.00844748, 0.00553534, 0.00622896,\n",
       "        0.00820731, 0.00932523, 0.00749933, 0.00917824, 0.00747262,\n",
       "        0.00601332, 0.00818291, 0.00865794, 0.00992774, 0.00792465,\n",
       "        0.00538888, 0.00517687, 0.00411825, 0.00708237, 0.00655439,\n",
       "        0.00791454, 0.01010742, 0.00593296, 0.00643117, 0.01016661,\n",
       "        0.00747262, 0.01684755, 0.06666213, 0.02372003, 0.00712741,\n",
       "        0.08659423, 0.01276558, 0.01358823, 0.02702887, 0.01712308,\n",
       "        0.01103449, 0.05312777, 0.01034215, 0.0223732 , 0.04719068,\n",
       "        0.01819231, 0.01967333, 0.10363667, 0.01797109, 0.01584172,\n",
       "        0.08824194, 0.01631686, 0.00392938, 0.06244165, 0.01953868,\n",
       "        0.03085839, 0.0780487 , 0.01151694, 0.01693989, 0.08328361,\n",
       "        0.01785945, 0.01890397, 0.06090452, 0.01894202, 0.00712741,\n",
       "        0.04516016, 0.00708237, 0.01357056, 0.0546443 , 0.02330322,\n",
       "        0.00886792, 0.07062011, 0.00725534, 0.02441803, 0.03456241,\n",
       "        0.01187603, 0.01132078, 0.08396809, 0.01658192, 0.0143722 ,\n",
       "        0.09834714, 0.02136726, 0.00523068, 0.11180054, 0.0098102 ,\n",
       "        0.00531413, 0.07030505, 0.00435431, 0.00466476, 0.11143788,\n",
       "        0.00648074, 0.00777174, 0.09738049, 0.01857956, 0.00593296,\n",
       "        0.08798045, 0.00376298, 0.00570614, 0.08324278, 0.00588558,\n",
       "        0.00365513, 0.13289605, 0.00449889, 0.02317758, 0.11559516,\n",
       "        0.0257449 , 0.00570614, 0.09620936, 0.00458694, 0.00673498,\n",
       "        0.10170664, 0.00422374, 0.00523068, 0.06289165, 0.005004  ,\n",
       "        0.01830191, 0.10175736, 0.02572625, 0.00719444, 0.07882284,\n",
       "        0.0032    , 0.00687023, 0.09383176, 0.00344093, 0.00508331,\n",
       "        0.11752344, 0.00547723, 0.0165336 , 0.0297953 , 0.0198454 ,\n",
       "        0.0068    , 0.00597997, 0.00511468, 0.00668132, 0.0056    ,\n",
       "        0.00748331, 0.005004  , 0.00707107, 0.00667533, 0.0156205 ,\n",
       "        0.00615142, 0.02029778, 0.0054037 , 0.0059127 , 0.00783326,\n",
       "        0.00746726, 0.00821219, 0.00691665, 0.006     , 0.00726636,\n",
       "        0.00792465, 0.01467787, 0.02124523, 0.01065082, 0.00496387,\n",
       "        0.00643739, 0.00688186, 0.00674092, 0.00643117, 0.00354401,\n",
       "        0.00649307, 0.00547723, 0.00714423, 0.02441803, 0.02216303,\n",
       "        0.02068429, 0.00624179, 0.00445421, 0.0045607 , 0.00700286,\n",
       "        0.0074027 , 0.0064    , 0.00584466, 0.00748331, 0.00615142,\n",
       "        0.01945867, 0.00621611, 0.00553534, 0.00734847, 0.00798499,\n",
       "        0.00945304, 0.00796492, 0.00739189, 0.00688767, 0.00672309,\n",
       "        0.00701427, 0.00705408, 0.0062482 , 0.0059127 , 0.02115278,\n",
       "        0.0088227 , 0.00780769, 0.00724983, 0.00862554, 0.00782304,\n",
       "        0.00595315, 0.0077201 , 0.00649307, 0.00778203, 0.00655439,\n",
       "        0.01942164, 0.00842852, 0.00627375, 0.0054037 , 0.00817313,\n",
       "        0.00722772, 0.00711618, 0.00736478, 0.00731027, 0.00627375,\n",
       "        0.00770973, 0.0085557 , 0.00553534, 0.00941063, 0.00783837,\n",
       "        0.0088    , 0.00660303, 0.00662118, 0.00716659, 0.00861162,\n",
       "        0.00687023, 0.00760526, 0.00742967]),\n",
       " 'rank_test_f1_micro': array([ 19,  83,  31,  21,  13,  45,  83,  28,   2,  58,  95, 136,  21,\n",
       "         31,  40,  26,   1,   8,  58,  71,  25,  92, 154,  78,  71, 101,\n",
       "          8,   3,  10,  69,  47,  17,  57, 121, 115,   4,  43,  13,  54,\n",
       "         42,  24,  23,  19,   4,  52,  47,  71,  78,  28,  16,  78,  35,\n",
       "          6,  26,  64,  43, 160, 149,  58,  54,  54,  64,  64,  47,  47,\n",
       "         17,  31,  35, 125,  86, 108,  40,  64,  28,  37,  11,  45,  78,\n",
       "         77,   6,  63, 156,  98, 118,  69,  13,  52,  31,  11,  64,  62,\n",
       "         38,  47,  75,  38, 113, 250, 276, 252, 247, 278, 242, 238, 285,\n",
       "        237, 225, 272, 232, 254, 281, 249, 241, 282, 245, 236, 288, 239,\n",
       "        227, 273, 231, 255, 287, 253, 248, 275, 246, 234, 270, 235, 226,\n",
       "        277, 229, 251, 279, 256, 244, 283, 243, 240, 265, 233, 228, 280,\n",
       "        230, 208, 260, 211, 199, 263, 198, 188, 262, 177, 167, 264, 171,\n",
       "        205, 284, 206, 190, 261, 189, 191, 267, 173, 177, 258, 169, 213,\n",
       "        269, 215, 196, 271, 194, 181, 266, 192, 172, 257, 187, 217, 286,\n",
       "        212, 200, 274, 192, 185, 268, 173, 183, 259, 180, 224, 218, 210,\n",
       "        166, 162, 170, 161, 143, 157, 131, 120, 125, 220, 223, 219, 141,\n",
       "        164, 143, 128, 135, 149, 101, 100, 121, 222, 209, 221, 146, 149,\n",
       "        164, 113, 154, 108, 140, 157, 148, 207, 213, 216, 143, 168, 163,\n",
       "        149, 157, 136, 125, 101, 149, 203, 182, 197, 141, 136, 128,  92,\n",
       "        131, 117,  71, 101, 119, 183, 201, 204, 146, 128, 108,  86, 101,\n",
       "         96, 101,  98, 108, 177, 202, 176, 136, 121, 101, 115,  91,  92,\n",
       "         58,  75,  82, 195, 173, 186, 131, 131, 121,  96,  90,  83,  86,\n",
       "        112,  86]),\n",
       " 'split0_test_roc_auc_ovo': array([0.99561662, 0.99471584, 0.99477403, 0.99491548, 0.99417965,\n",
       "        0.99471403, 0.99442043, 0.995157  , 0.99537173, 0.99350053,\n",
       "        0.99353418, 0.99159785, 0.9953074 , 0.99465586, 0.99492804,\n",
       "        0.99456164, 0.9944083 , 0.99436699, 0.9954174 , 0.99360152,\n",
       "        0.99473658, 0.99495446, 0.9937589 , 0.99408901, 0.99428117,\n",
       "        0.9948225 , 0.99424726, 0.99522341, 0.9946126 , 0.994882  ,\n",
       "        0.99469487, 0.99445177, 0.99439567, 0.99364852, 0.99433321,\n",
       "        0.99436586, 0.99440043, 0.99417118, 0.99492773, 0.99478427,\n",
       "        0.9948967 , 0.99464003, 0.99438744, 0.99500341, 0.99468987,\n",
       "        0.99483802, 0.99457698, 0.99500904, 0.99491362, 0.99386713,\n",
       "        0.9950688 , 0.995093  , 0.99453908, 0.99510115, 0.99335476,\n",
       "        0.99444255, 0.99478425, 0.99382439, 0.99471286, 0.99368191,\n",
       "        0.9941905 , 0.99481154, 0.99533757, 0.99511322, 0.99371853,\n",
       "        0.99547792, 0.99251009, 0.99473887, 0.99463119, 0.9934784 ,\n",
       "        0.99226422, 0.99464647, 0.99454495, 0.99451588, 0.9953001 ,\n",
       "        0.99494345, 0.99421922, 0.99581883, 0.9951734 , 0.99429753,\n",
       "        0.99396238, 0.99271854, 0.99423058, 0.99354451, 0.9948405 ,\n",
       "        0.99477694, 0.99461998, 0.99489124, 0.99474646, 0.99575091,\n",
       "        0.99494419, 0.99469906, 0.99375023, 0.99511254, 0.99325003,\n",
       "        0.99308787, 0.93150102, 0.58519356, 0.94179831, 0.96476359,\n",
       "        0.45561465, 0.97715223, 0.96937878, 0.68284778, 0.96915377,\n",
       "        0.98577055, 0.50464125, 0.9822948 , 0.95792593, 0.77338051,\n",
       "        0.95407627, 0.97584104, 0.62674111, 0.98037266, 0.98200867,\n",
       "        0.33813029, 0.97378435, 0.98477443, 0.68463473, 0.98375927,\n",
       "        0.94664272, 0.59060612, 0.95141084, 0.96632198, 0.54720211,\n",
       "        0.96885025, 0.98022663, 0.59553464, 0.98090073, 0.98372919,\n",
       "        0.78794111, 0.97992808, 0.94652392, 0.30866932, 0.92995638,\n",
       "        0.96349866, 0.51178407, 0.97638947, 0.97911055, 0.62817166,\n",
       "        0.98293309, 0.98021756, 0.6888567 , 0.98068171, 0.98925069,\n",
       "        0.51041491, 0.99072793, 0.99191811, 0.58146495, 0.99164038,\n",
       "        0.9931894 , 0.76329064, 0.99274771, 0.99333622, 0.6482793 ,\n",
       "        0.99367099, 0.99069998, 0.647032  , 0.985997  , 0.99096803,\n",
       "        0.40098094, 0.99197889, 0.99220074, 0.46047548, 0.99309812,\n",
       "        0.99311364, 0.74247828, 0.99267625, 0.98783734, 0.61137236,\n",
       "        0.98932243, 0.99203265, 0.64515717, 0.99265269, 0.99288918,\n",
       "        0.8084952 , 0.99279049, 0.99290777, 0.68139908, 0.99313012,\n",
       "        0.98942898, 0.386242  , 0.98751631, 0.99220032, 0.5081079 ,\n",
       "        0.99158294, 0.99281545, 0.71842738, 0.99237665, 0.99287696,\n",
       "        0.61495127, 0.99338009, 0.98021217, 0.96723472, 0.99015446,\n",
       "        0.99318601, 0.99359572, 0.99343919, 0.9938641 , 0.99397536,\n",
       "        0.9936591 , 0.99368363, 0.99433344, 0.99377343, 0.98129728,\n",
       "        0.97715285, 0.98140029, 0.99353347, 0.99279469, 0.99393539,\n",
       "        0.99409915, 0.99396862, 0.99373993, 0.99421729, 0.99414104,\n",
       "        0.99405898, 0.97312144, 0.97262086, 0.98999031, 0.99358735,\n",
       "        0.99191455, 0.99300782, 0.99374898, 0.99351933, 0.99369553,\n",
       "        0.99415727, 0.9941009 , 0.99393761, 0.97455908, 0.97994374,\n",
       "        0.96711298, 0.99324928, 0.99322942, 0.99342929, 0.99385064,\n",
       "        0.99350944, 0.99344624, 0.99412195, 0.99390754, 0.99418181,\n",
       "        0.99221422, 0.99340546, 0.99293644, 0.99355649, 0.99398579,\n",
       "        0.9945303 , 0.99429933, 0.99451628, 0.99419145, 0.99459238,\n",
       "        0.99465824, 0.99421861, 0.99141873, 0.9911409 , 0.99129133,\n",
       "        0.99396994, 0.99433589, 0.99360007, 0.99460052, 0.99438015,\n",
       "        0.99494162, 0.99409407, 0.99466561, 0.99441548, 0.99143995,\n",
       "        0.98925663, 0.99280767, 0.99360093, 0.99445582, 0.99423175,\n",
       "        0.99405921, 0.99493957, 0.99442723, 0.99450119, 0.99434152,\n",
       "        0.99434125, 0.99169786, 0.99250848, 0.99195478, 0.99413964,\n",
       "        0.99344132, 0.99422194, 0.99427017, 0.99474565, 0.99448458,\n",
       "        0.99455818, 0.99458398, 0.99426066]),\n",
       " 'split1_test_roc_auc_ovo': array([0.99609646, 0.99551001, 0.9957217 , 0.99665721, 0.99532011,\n",
       "        0.99620157, 0.99639918, 0.99507254, 0.99581958, 0.99491089,\n",
       "        0.9957318 , 0.99529578, 0.99561085, 0.99587581, 0.99504165,\n",
       "        0.99542429, 0.9961207 , 0.99605842, 0.99536225, 0.99653549,\n",
       "        0.99552997, 0.99336578, 0.99438217, 0.99387932, 0.99590415,\n",
       "        0.99509736, 0.99616858, 0.99613419, 0.99624671, 0.99586954,\n",
       "        0.99475298, 0.99598805, 0.99536713, 0.99540867, 0.99512933,\n",
       "        0.99524079, 0.99504778, 0.99561857, 0.99577485, 0.99534293,\n",
       "        0.99600256, 0.99581423, 0.99523171, 0.99567294, 0.99585966,\n",
       "        0.99644997, 0.99564862, 0.99474717, 0.99483034, 0.99534332,\n",
       "        0.99595291, 0.99573849, 0.9952314 , 0.99575825, 0.99497632,\n",
       "        0.99645917, 0.99558538, 0.99392771, 0.99432723, 0.99479504,\n",
       "        0.9960234 , 0.99575278, 0.99569365, 0.99568186, 0.99569156,\n",
       "        0.99534367, 0.99590683, 0.9962942 , 0.99618745, 0.99334551,\n",
       "        0.99487631, 0.99477293, 0.99588258, 0.99461845, 0.99575818,\n",
       "        0.99600125, 0.99592883, 0.99544325, 0.99584504, 0.99593999,\n",
       "        0.99621715, 0.9930624 , 0.99514746, 0.99457536, 0.99551233,\n",
       "        0.99570304, 0.99562126, 0.99589103, 0.99593635, 0.99595569,\n",
       "        0.99538387, 0.99596385, 0.99539288, 0.99516729, 0.99553003,\n",
       "        0.9945694 , 0.9636527 , 0.58765591, 0.9517043 , 0.97647631,\n",
       "        0.60791102, 0.97085913, 0.98202263, 0.61392401, 0.97778079,\n",
       "        0.98422967, 0.52881971, 0.98422872, 0.95728442, 0.62859642,\n",
       "        0.94088511, 0.98508639, 0.43193505, 0.97893908, 0.9769197 ,\n",
       "        0.62483617, 0.98237528, 0.98530551, 0.38903651, 0.98568572,\n",
       "        0.95169993, 0.68713283, 0.9540862 , 0.97362407, 0.52486332,\n",
       "        0.98384138, 0.97972908, 0.55780642, 0.98196596, 0.98576536,\n",
       "        0.38261109, 0.98652626, 0.93534992, 0.73245111, 0.9423711 ,\n",
       "        0.97505728, 0.65623096, 0.96800525, 0.97196837, 0.68350232,\n",
       "        0.98543   , 0.98617889, 0.78771192, 0.98172194, 0.98521976,\n",
       "        0.67625479, 0.98645422, 0.99399764, 0.70213887, 0.99230252,\n",
       "        0.99319446, 0.55245468, 0.9927537 , 0.99405118, 0.68240499,\n",
       "        0.9939798 , 0.99235646, 0.51684358, 0.990845  , 0.99359953,\n",
       "        0.64749007, 0.99194406, 0.99235619, 0.6921668 , 0.99242055,\n",
       "        0.99400123, 0.71008272, 0.99390854, 0.98735698, 0.53806602,\n",
       "        0.96938646, 0.99233693, 0.4275971 , 0.99317372, 0.99366437,\n",
       "        0.71112056, 0.99303711, 0.99442435, 0.69570606, 0.99372442,\n",
       "        0.98891134, 0.75204076, 0.97911694, 0.99406535, 0.46763834,\n",
       "        0.99274638, 0.99287723, 0.60991544, 0.99365244, 0.99406604,\n",
       "        0.80885045, 0.99384499, 0.97666479, 0.99140809, 0.96741013,\n",
       "        0.99410729, 0.99400526, 0.99245979, 0.99472336, 0.99494697,\n",
       "        0.99492537, 0.99493629, 0.99497136, 0.99482555, 0.96658828,\n",
       "        0.98401694, 0.98015631, 0.99415472, 0.99412434, 0.99354176,\n",
       "        0.99473786, 0.99447858, 0.99468217, 0.99488793, 0.99519714,\n",
       "        0.9951574 , 0.97190275, 0.98506888, 0.98316234, 0.9935685 ,\n",
       "        0.99341813, 0.99406614, 0.99473287, 0.99490108, 0.99462058,\n",
       "        0.99495027, 0.99500925, 0.99524883, 0.98697735, 0.9723441 ,\n",
       "        0.99071466, 0.99380797, 0.99309306, 0.99384545, 0.99482828,\n",
       "        0.99457949, 0.9947638 , 0.99483947, 0.99483307, 0.99474505,\n",
       "        0.98340741, 0.99422566, 0.99264075, 0.99510311, 0.9953245 ,\n",
       "        0.99520406, 0.99520872, 0.99522196, 0.99543799, 0.99532257,\n",
       "        0.99537185, 0.99558465, 0.99383616, 0.9894445 , 0.99367628,\n",
       "        0.99528299, 0.99494004, 0.9955551 , 0.99538682, 0.99524924,\n",
       "        0.99542415, 0.99554243, 0.99561929, 0.99531569, 0.99152005,\n",
       "        0.99374517, 0.99419271, 0.99505415, 0.99527484, 0.99548724,\n",
       "        0.99548128, 0.99542187, 0.99560158, 0.99555645, 0.99543996,\n",
       "        0.99554059, 0.99229205, 0.99159773, 0.99493592, 0.99503738,\n",
       "        0.99514005, 0.99498128, 0.99555159, 0.99545664, 0.99550652,\n",
       "        0.9954669 , 0.99556293, 0.99539583]),\n",
       " 'split2_test_roc_auc_ovo': array([0.99686781, 0.99693503, 0.99614459, 0.99683062, 0.99669398,\n",
       "        0.99672298, 0.99529894, 0.99603489, 0.99677383, 0.99568605,\n",
       "        0.99577088, 0.9959523 , 0.99711592, 0.99698884, 0.99692538,\n",
       "        0.9967878 , 0.99733957, 0.9965927 , 0.99645703, 0.99616074,\n",
       "        0.99687046, 0.99611676, 0.99629683, 0.99461604, 0.99712076,\n",
       "        0.99686137, 0.99698292, 0.99727097, 0.99708786, 0.99648713,\n",
       "        0.9969406 , 0.99650735, 0.99571627, 0.99688588, 0.99644936,\n",
       "        0.99581928, 0.99699699, 0.99689581, 0.99687589, 0.99703615,\n",
       "        0.99710111, 0.99696169, 0.99679245, 0.99703756, 0.99657671,\n",
       "        0.9967255 , 0.99603206, 0.9962717 , 0.99750378, 0.99653041,\n",
       "        0.99703343, 0.9960642 , 0.9958248 , 0.99739532, 0.99606772,\n",
       "        0.99528048, 0.99543017, 0.99524294, 0.99489491, 0.99526538,\n",
       "        0.99717141, 0.99716291, 0.99700437, 0.99671092, 0.99694952,\n",
       "        0.99705064, 0.9955259 , 0.99551004, 0.99632518, 0.99462799,\n",
       "        0.99499993, 0.99584867, 0.99670731, 0.99571939, 0.99737109,\n",
       "        0.99633611, 0.99723251, 0.9968293 , 0.99612236, 0.99657944,\n",
       "        0.99526183, 0.99487401, 0.99508671, 0.99515191, 0.99684807,\n",
       "        0.9972367 , 0.99719199, 0.99622115, 0.99727922, 0.99667018,\n",
       "        0.99631782, 0.99565898, 0.99674635, 0.99618446, 0.99599912,\n",
       "        0.99594951, 0.95947297, 0.47946046, 0.95416372, 0.9671516 ,\n",
       "        0.36996635, 0.97564871, 0.97849173, 0.47277559, 0.98406894,\n",
       "        0.98899178, 0.4874099 , 0.98720137, 0.93755552, 0.35814354,\n",
       "        0.97453895, 0.95395117, 0.6537901 , 0.97837158, 0.98302228,\n",
       "        0.33877483, 0.98519353, 0.98784634, 0.34218555, 0.98513975,\n",
       "        0.95132175, 0.53342068, 0.94056044, 0.96510866, 0.58442926,\n",
       "        0.9701486 , 0.97721075, 0.49120744, 0.97693743, 0.98648009,\n",
       "        0.70490949, 0.98712839, 0.93457632, 0.5463341 , 0.94742213,\n",
       "        0.96999053, 0.53834156, 0.96845678, 0.98141138, 0.60881282,\n",
       "        0.98802675, 0.98723682, 0.61588362, 0.9860594 , 0.99117012,\n",
       "        0.56822364, 0.99322541, 0.99390698, 0.57125609, 0.99560109,\n",
       "        0.9950546 , 0.5631431 , 0.99550529, 0.99552258, 0.84870286,\n",
       "        0.99583132, 0.99084606, 0.50512768, 0.98852906, 0.99501294,\n",
       "        0.71061907, 0.99504395, 0.99533919, 0.58965571, 0.99545097,\n",
       "        0.99553346, 0.7485274 , 0.99572145, 0.99448777, 0.65822153,\n",
       "        0.99044979, 0.99425004, 0.70992861, 0.99517264, 0.99548409,\n",
       "        0.69016723, 0.99439926, 0.99553681, 0.69564537, 0.99581609,\n",
       "        0.9927972 , 0.70269228, 0.99055987, 0.9939822 , 0.71789616,\n",
       "        0.99402237, 0.99545969, 0.5509285 , 0.99487973, 0.99567757,\n",
       "        0.71246847, 0.99572947, 0.97729107, 0.99188846, 0.97868318,\n",
       "        0.995502  , 0.99503568, 0.99520063, 0.99577176, 0.99619819,\n",
       "        0.99631806, 0.99644359, 0.99629246, 0.99627989, 0.97261816,\n",
       "        0.97035721, 0.98812116, 0.99608994, 0.9951585 , 0.9959672 ,\n",
       "        0.99642197, 0.99623903, 0.99564952, 0.99645282, 0.99625588,\n",
       "        0.99653706, 0.98272695, 0.99059501, 0.96953098, 0.99583151,\n",
       "        0.99553239, 0.99524482, 0.99625661, 0.99603537, 0.99617819,\n",
       "        0.99640189, 0.99627008, 0.99652424, 0.99206388, 0.99256585,\n",
       "        0.96988733, 0.99579474, 0.99562576, 0.99531337, 0.99624912,\n",
       "        0.99578422, 0.99580105, 0.99613735, 0.9961988 , 0.99606957,\n",
       "        0.9957931 , 0.99522922, 0.99386034, 0.99646006, 0.99665675,\n",
       "        0.99677543, 0.99688715, 0.99646258, 0.99695388, 0.99674352,\n",
       "        0.99702605, 0.99680514, 0.99539009, 0.99599927, 0.99526192,\n",
       "        0.99627618, 0.99628305, 0.99654143, 0.99687214, 0.99676934,\n",
       "        0.99683765, 0.9971033 , 0.99674004, 0.99699722, 0.99545218,\n",
       "        0.99499907, 0.99442786, 0.99661648, 0.99684294, 0.99658533,\n",
       "        0.99721884, 0.99648811, 0.99663641, 0.99686046, 0.99671219,\n",
       "        0.99710956, 0.99474945, 0.99606923, 0.99540307, 0.99636841,\n",
       "        0.99691758, 0.99647965, 0.9967906 , 0.99667462, 0.99653556,\n",
       "        0.99698052, 0.99672918, 0.99678988]),\n",
       " 'split3_test_roc_auc_ovo': array([0.99521174, 0.99382026, 0.9945424 , 0.99386408, 0.99408124,\n",
       "        0.99533336, 0.99304312, 0.9930217 , 0.99421216, 0.9916659 ,\n",
       "        0.99347689, 0.99307098, 0.99418099, 0.99378556, 0.99373505,\n",
       "        0.99358407, 0.99266351, 0.99356686, 0.99430236, 0.99322887,\n",
       "        0.99392255, 0.99316867, 0.99158058, 0.99141759, 0.99453001,\n",
       "        0.99406898, 0.99475238, 0.99406258, 0.99323436, 0.99525255,\n",
       "        0.99282981, 0.99411806, 0.99274386, 0.99331682, 0.99442559,\n",
       "        0.99152781, 0.99388554, 0.99402761, 0.99387659, 0.9932716 ,\n",
       "        0.99375132, 0.99447987, 0.99400592, 0.99423081, 0.99291278,\n",
       "        0.99374585, 0.99361929, 0.99426735, 0.99440418, 0.99505145,\n",
       "        0.99509231, 0.99512011, 0.99537533, 0.99414338, 0.99358699,\n",
       "        0.99189937, 0.99175449, 0.99142806, 0.98883714, 0.99180118,\n",
       "        0.99463426, 0.99392184, 0.99519938, 0.99446864, 0.99491326,\n",
       "        0.99518514, 0.99222985, 0.99311058, 0.99277219, 0.99379515,\n",
       "        0.99180276, 0.99170418, 0.9940841 , 0.99509648, 0.99338077,\n",
       "        0.99495585, 0.99453174, 0.99431814, 0.99286538, 0.99252515,\n",
       "        0.99238613, 0.992524  , 0.9918909 , 0.99224519, 0.99465415,\n",
       "        0.99482662, 0.99475758, 0.99385353, 0.99454418, 0.99522852,\n",
       "        0.99448339, 0.99375922, 0.99454103, 0.99368281, 0.99218918,\n",
       "        0.99433927, 0.94428865, 0.47502814, 0.95838068, 0.96685717,\n",
       "        0.59018961, 0.97200128, 0.97600583, 0.6055728 , 0.97438555,\n",
       "        0.98369066, 0.80152812, 0.98144092, 0.94175827, 0.48050367,\n",
       "        0.97441234, 0.9671501 , 0.69185017, 0.96572178, 0.97880942,\n",
       "        0.71096981, 0.97829618, 0.98282008, 0.67187149, 0.98275841,\n",
       "        0.96860075, 0.25843429, 0.9647825 , 0.97065022, 0.55121337,\n",
       "        0.95576831, 0.97991528, 0.35004273, 0.97551745, 0.98252939,\n",
       "        0.46263743, 0.98376516, 0.9573504 , 0.47115803, 0.95243065,\n",
       "        0.96709587, 0.4686996 , 0.97797506, 0.97468455, 0.47255388,\n",
       "        0.98023662, 0.98195402, 0.55428512, 0.98522812, 0.98535411,\n",
       "        0.6545352 , 0.98798295, 0.98988652, 0.6551228 , 0.99044027,\n",
       "        0.99088843, 0.51900052, 0.99117824, 0.99147386, 0.61961867,\n",
       "        0.99093949, 0.9890461 , 0.56709794, 0.97609574, 0.99010053,\n",
       "        0.63649909, 0.9905947 , 0.99163194, 0.78296624, 0.99166465,\n",
       "        0.99157554, 0.79323059, 0.99167596, 0.98229933, 0.75036112,\n",
       "        0.9839528 , 0.99025101, 0.53890688, 0.99164779, 0.99136155,\n",
       "        0.61227977, 0.99162012, 0.99130125, 0.64299621, 0.9915852 ,\n",
       "        0.97493329, 0.5230654 , 0.98842767, 0.99088233, 0.70466015,\n",
       "        0.99069437, 0.99095835, 0.77291216, 0.99065893, 0.99160113,\n",
       "        0.48266984, 0.99168829, 0.98448679, 0.97219469, 0.98714038,\n",
       "        0.99242387, 0.99174288, 0.99104325, 0.99314209, 0.99326259,\n",
       "        0.99255633, 0.99295875, 0.99287852, 0.99308779, 0.98586275,\n",
       "        0.98802705, 0.97259598, 0.99220289, 0.99131344, 0.99216743,\n",
       "        0.99305391, 0.99225206, 0.99274952, 0.99241062, 0.99278954,\n",
       "        0.99295094, 0.98753779, 0.98885361, 0.9719817 , 0.99060346,\n",
       "        0.99088435, 0.99244675, 0.99242505, 0.99291762, 0.99251928,\n",
       "        0.99309766, 0.99299158, 0.99280986, 0.98640433, 0.98722758,\n",
       "        0.98827913, 0.99166458, 0.99228105, 0.99032416, 0.99238497,\n",
       "        0.99242584, 0.99237476, 0.99280511, 0.99262767, 0.99278449,\n",
       "        0.99095165, 0.9915853 , 0.99258174, 0.99298486, 0.99381755,\n",
       "        0.99247195, 0.99376381, 0.99311828, 0.9940874 , 0.99391119,\n",
       "        0.99407402, 0.99353911, 0.99059646, 0.99071322, 0.98480974,\n",
       "        0.99298679, 0.99282487, 0.99329845, 0.99379052, 0.99368379,\n",
       "        0.99375034, 0.99366246, 0.99411303, 0.99400717, 0.99093285,\n",
       "        0.99135858, 0.99055193, 0.99387748, 0.99350279, 0.99319119,\n",
       "        0.99392714, 0.99328559, 0.99352674, 0.99365258, 0.99408844,\n",
       "        0.99359765, 0.99026617, 0.99171115, 0.99221439, 0.99284352,\n",
       "        0.99278327, 0.99270557, 0.99343932, 0.99332996, 0.99314947,\n",
       "        0.99316189, 0.99349637, 0.99328627]),\n",
       " 'split4_test_roc_auc_ovo': array([0.9970543 , 0.99649771, 0.99711735, 0.99728605, 0.99691969,\n",
       "        0.99716082, 0.99695171, 0.99701199, 0.99683111, 0.99655928,\n",
       "        0.99645329, 0.99646072, 0.99693373, 0.99673918, 0.99696936,\n",
       "        0.99730007, 0.99716429, 0.99718883, 0.99740596, 0.99684613,\n",
       "        0.99711811, 0.99621201, 0.99603963, 0.99711863, 0.996664  ,\n",
       "        0.99664485, 0.99715164, 0.99742311, 0.99656597, 0.99693105,\n",
       "        0.997423  , 0.99735067, 0.99705394, 0.99596572, 0.99618844,\n",
       "        0.99715632, 0.99686465, 0.99712632, 0.99646581, 0.99730588,\n",
       "        0.99749337, 0.99742875, 0.99749664, 0.99731389, 0.9971133 ,\n",
       "        0.9968358 , 0.99725423, 0.99699884, 0.99695582, 0.99708476,\n",
       "        0.99674212, 0.99744953, 0.99723062, 0.99706566, 0.99620983,\n",
       "        0.99702966, 0.99560847, 0.99595536, 0.99650229, 0.9964305 ,\n",
       "        0.99702554, 0.99633346, 0.99706209, 0.99666754, 0.99705896,\n",
       "        0.99720595, 0.99677325, 0.99688559, 0.99569123, 0.99551485,\n",
       "        0.99692917, 0.99652467, 0.99717547, 0.99700704, 0.99676042,\n",
       "        0.9971866 , 0.99717303, 0.99737937, 0.99683384, 0.99706849,\n",
       "        0.99680985, 0.99696682, 0.99648297, 0.99649324, 0.99695974,\n",
       "        0.99695651, 0.99662561, 0.99748945, 0.99717697, 0.99669305,\n",
       "        0.99667008, 0.99706016, 0.99723014, 0.99698583, 0.99672277,\n",
       "        0.99661317, 0.96273007, 0.54394357, 0.95418315, 0.98048176,\n",
       "        0.56556544, 0.97825807, 0.98602555, 0.67946416, 0.98424048,\n",
       "        0.98797331, 0.48136954, 0.98711228, 0.94429363, 0.69899298,\n",
       "        0.96059358, 0.97141691, 0.64243296, 0.97793254, 0.98260935,\n",
       "        0.3184885 , 0.97857917, 0.98817065, 0.26121454, 0.98861712,\n",
       "        0.94521373, 0.61795845, 0.93924767, 0.97362966, 0.55591534,\n",
       "        0.96385772, 0.98227978, 0.45883099, 0.9852096 , 0.98305061,\n",
       "        0.47233137, 0.98793023, 0.93866125, 0.51631424, 0.95075142,\n",
       "        0.96644317, 0.50918864, 0.97428854, 0.97977688, 0.51443048,\n",
       "        0.98659378, 0.98676216, 0.68941397, 0.98953736, 0.99209451,\n",
       "        0.65589846, 0.9794757 , 0.99372768, 0.82126226, 0.99431015,\n",
       "        0.9950826 , 0.83733911, 0.99502119, 0.99540169, 0.51991677,\n",
       "        0.99511323, 0.98775724, 0.77942877, 0.993097  , 0.9945131 ,\n",
       "        0.6229472 , 0.99439003, 0.99504687, 0.81277168, 0.99533385,\n",
       "        0.9952035 , 0.8087197 , 0.99502805, 0.98157629, 0.56819775,\n",
       "        0.99088919, 0.99507856, 0.67726462, 0.99420313, 0.99507076,\n",
       "        0.6610707 , 0.99473832, 0.99524607, 0.64468049, 0.99561003,\n",
       "        0.99062849, 0.54645722, 0.99334916, 0.99423834, 0.59879915,\n",
       "        0.99471419, 0.99527517, 0.58672119, 0.99489949, 0.99480818,\n",
       "        0.7243211 , 0.99531445, 0.97199689, 0.98523466, 0.99421454,\n",
       "        0.99577375, 0.99511061, 0.99528322, 0.99608244, 0.99606154,\n",
       "        0.99611941, 0.99632287, 0.99590803, 0.99625202, 0.98089072,\n",
       "        0.9712134 , 0.99335091, 0.99565121, 0.99568238, 0.99514168,\n",
       "        0.9959006 , 0.99577673, 0.99599139, 0.99624536, 0.99630622,\n",
       "        0.99612835, 0.98807625, 0.9928003 , 0.98276064, 0.99534399,\n",
       "        0.99540526, 0.99538924, 0.99583538, 0.9959936 , 0.99579553,\n",
       "        0.99609624, 0.99609111, 0.99603164, 0.97281605, 0.96620233,\n",
       "        0.97641316, 0.99545059, 0.99495126, 0.99518785, 0.99558023,\n",
       "        0.9956405 , 0.99594706, 0.99568919, 0.99577036, 0.99588931,\n",
       "        0.99520183, 0.99522769, 0.99476282, 0.99667929, 0.99636241,\n",
       "        0.99677178, 0.99648596, 0.99663082, 0.99683743, 0.99694619,\n",
       "        0.99662587, 0.99691664, 0.99422526, 0.99509057, 0.99491236,\n",
       "        0.99596474, 0.99638313, 0.99658126, 0.99647826, 0.99663889,\n",
       "        0.99672357, 0.99690314, 0.99694369, 0.9968856 , 0.99551334,\n",
       "        0.9952838 , 0.99437439, 0.99583067, 0.99617876, 0.9965331 ,\n",
       "        0.99671246, 0.99639431, 0.99682604, 0.99690294, 0.99690802,\n",
       "        0.9968965 , 0.99472423, 0.99564051, 0.995451  , 0.99648443,\n",
       "        0.99589627, 0.99597703, 0.99670139, 0.99673002, 0.99656709,\n",
       "        0.99650154, 0.99666246, 0.9965441 ]),\n",
       " 'mean_test_roc_auc_ovo': array([0.99616939, 0.99549577, 0.99566001, 0.99591069, 0.99543894,\n",
       "        0.99602655, 0.99522268, 0.99525963, 0.99580168, 0.99446453,\n",
       "        0.99499341, 0.99447553, 0.99582978, 0.99560905, 0.9955199 ,\n",
       "        0.99553157, 0.99553927, 0.99555476, 0.995789  , 0.99527455,\n",
       "        0.99563553, 0.99476353, 0.99441162, 0.99422412, 0.99570002,\n",
       "        0.99549901, 0.99586056, 0.99602285, 0.9955495 , 0.99588446,\n",
       "        0.99532825, 0.99568318, 0.99505537, 0.99504512, 0.99530519,\n",
       "        0.99482201, 0.99543908, 0.9955679 , 0.99558417, 0.99554817,\n",
       "        0.99584901, 0.99586492, 0.99558283, 0.99585172, 0.99543046,\n",
       "        0.99571903, 0.99542624, 0.99545882, 0.99572155, 0.99557541,\n",
       "        0.99597791, 0.99589307, 0.99564025, 0.99589275, 0.99483913,\n",
       "        0.99502225, 0.99463255, 0.99407569, 0.99385489, 0.9943948 ,\n",
       "        0.99580902, 0.99559651, 0.99605941, 0.99572844, 0.99566637,\n",
       "        0.99605266, 0.99458918, 0.99530786, 0.99512145, 0.99415238,\n",
       "        0.99417448, 0.99469938, 0.99567888, 0.99539145, 0.99571411,\n",
       "        0.99588465, 0.99581706, 0.99595778, 0.995368  , 0.99528212,\n",
       "        0.99492747, 0.99402915, 0.99456772, 0.99440204, 0.99576296,\n",
       "        0.99589996, 0.99576329, 0.99566928, 0.99593664, 0.99605967,\n",
       "        0.99555987, 0.99542825, 0.99553213, 0.99542659, 0.99473822,\n",
       "        0.99491184, 0.95232908, 0.53425633, 0.95204603, 0.97114609,\n",
       "        0.51784941, 0.97478388, 0.97838491, 0.61091687, 0.97792591,\n",
       "        0.98613119, 0.5607537 , 0.98445562, 0.94776356, 0.58792342,\n",
       "        0.96090125, 0.97068912, 0.60934988, 0.97626753, 0.98067389,\n",
       "        0.46623992, 0.9796457 , 0.9857834 , 0.46978856, 0.98519205,\n",
       "        0.95269578, 0.53751047, 0.95001753, 0.96986692, 0.55272468,\n",
       "        0.96849325, 0.9798723 , 0.49068444, 0.98010624, 0.98431093,\n",
       "        0.5620861 , 0.98505563, 0.94249236, 0.51498536, 0.94458634,\n",
       "        0.9684171 , 0.53684896, 0.97302302, 0.97739034, 0.58149423,\n",
       "        0.98464405, 0.98446989, 0.66723027, 0.98464571, 0.98861784,\n",
       "        0.6130654 , 0.98757324, 0.99268739, 0.66624899, 0.99285888,\n",
       "        0.9934819 , 0.64704561, 0.99344123, 0.99395711, 0.66378451,\n",
       "        0.99390697, 0.99014117, 0.60310599, 0.98691276, 0.99283883,\n",
       "        0.60370728, 0.99279032, 0.99331499, 0.66760718, 0.99359363,\n",
       "        0.99388547, 0.76060774, 0.99380205, 0.98671154, 0.62524376,\n",
       "        0.98480013, 0.99278984, 0.59977087, 0.99336999, 0.99369399,\n",
       "        0.69662669, 0.99331706, 0.99388325, 0.67208544, 0.99397317,\n",
       "        0.98733986, 0.58209953, 0.98779399, 0.99307371, 0.59942034,\n",
       "        0.99275205, 0.99347718, 0.64778093, 0.99329345, 0.99380598,\n",
       "        0.66865223, 0.99399146, 0.97813034, 0.98159212, 0.98352054,\n",
       "        0.99419858, 0.99389803, 0.99348522, 0.99471675, 0.99488893,\n",
       "        0.99471566, 0.99486903, 0.99487676, 0.99484374, 0.97745144,\n",
       "        0.97815349, 0.98312493, 0.99432645, 0.99381467, 0.99415069,\n",
       "        0.9948427 , 0.99454301, 0.99456251, 0.99484281, 0.99493796,\n",
       "        0.99496654, 0.98067304, 0.98598773, 0.97948519, 0.99378696,\n",
       "        0.99343093, 0.99403095, 0.99459978, 0.9946734 , 0.99456182,\n",
       "        0.99494067, 0.99489258, 0.99491044, 0.98256414, 0.97965672,\n",
       "        0.97848145, 0.99399343, 0.99383611, 0.99362002, 0.99457865,\n",
       "        0.9943879 , 0.99446658, 0.99471861, 0.99466749, 0.99473404,\n",
       "        0.99151364, 0.99393467, 0.99335642, 0.99495676, 0.9952294 ,\n",
       "        0.9951507 , 0.99532899, 0.99518998, 0.99550163, 0.99550317,\n",
       "        0.9955512 , 0.99541283, 0.99309334, 0.99247769, 0.99199033,\n",
       "        0.99489613, 0.9949534 , 0.99511526, 0.99542565, 0.99534428,\n",
       "        0.99553547, 0.99546108, 0.99561633, 0.99552423, 0.99297168,\n",
       "        0.99292865, 0.99327091, 0.99499594, 0.99525103, 0.99520572,\n",
       "        0.99547978, 0.99530589, 0.9954036 , 0.99549472, 0.99549803,\n",
       "        0.99549711, 0.99274595, 0.99350542, 0.99399183, 0.99497468,\n",
       "        0.9948357 , 0.99487309, 0.99535061, 0.99538738, 0.99524864,\n",
       "        0.9953338 , 0.99540698, 0.99525535]),\n",
       " 'std_test_roc_auc_ovo': array([0.00070694, 0.0011394 , 0.00093771, 0.00130186, 0.00120092,\n",
       "        0.00089503, 0.00139811, 0.00132104, 0.00097124, 0.00172284,\n",
       "        0.00124178, 0.00184653, 0.0010874 , 0.00122359, 0.00125228,\n",
       "        0.00137482, 0.00177589, 0.00136848, 0.00105746, 0.00153811,\n",
       "        0.00122284, 0.001301  , 0.0017115 , 0.00181909, 0.00112885,\n",
       "        0.00108009, 0.00117066, 0.00126591, 0.00142292, 0.00075671,\n",
       "        0.00167104, 0.00122627, 0.00143573, 0.00136423, 0.0008762 ,\n",
       "        0.00188036, 0.00127316, 0.00130521, 0.00107972, 0.00149077,\n",
       "        0.00138582, 0.0011891 , 0.00135386, 0.0011767 , 0.00149781,\n",
       "        0.00122181, 0.00124454, 0.00101595, 0.00125557, 0.00113462,\n",
       "        0.00081358, 0.00086178, 0.00089591, 0.00121148, 0.00119819,\n",
       "        0.00180275, 0.00146988, 0.00154859, 0.00261639, 0.00156827,\n",
       "        0.00121504, 0.00113506, 0.00081152, 0.00087349, 0.00126077,\n",
       "        0.00088449, 0.00185865, 0.00131566, 0.00131676, 0.0008143 ,\n",
       "        0.00189928, 0.00165118, 0.00119734, 0.00091311, 0.00137537,\n",
       "        0.00085569, 0.00126963, 0.00107265, 0.00135987, 0.00166597,\n",
       "        0.00159426, 0.00168887, 0.00152005, 0.00143894, 0.00097498,\n",
       "        0.00103491, 0.00101228, 0.00123037, 0.00115727, 0.00056051,\n",
       "        0.00082169, 0.00112433, 0.00130648, 0.00111465, 0.00172439,\n",
       "        0.00124445, 0.01253712, 0.04909282, 0.0055554 , 0.006175  ,\n",
       "        0.09095448, 0.00288352, 0.0056284 , 0.07614907, 0.00578129,\n",
       "        0.00206293, 0.12150874, 0.00238353, 0.0083214 , 0.15014851,\n",
       "        0.01277386, 0.01026681, 0.09127343, 0.00533669, 0.00239219,\n",
       "        0.16705412, 0.00388698, 0.00199912, 0.17510165, 0.00199765,\n",
       "        0.00834827, 0.14803927, 0.00940134, 0.00358038, 0.01910183,\n",
       "        0.00918526, 0.00161417, 0.08519851, 0.0034995 , 0.00154411,\n",
       "        0.15592706, 0.00292162, 0.00854834, 0.13620793, 0.00808116,\n",
       "        0.00390898, 0.06370552, 0.00408622, 0.00350835, 0.07706657,\n",
       "        0.00276458, 0.00283701, 0.07857319, 0.00317892, 0.00287057,\n",
       "        0.06339593, 0.00466667, 0.00159518, 0.09127209, 0.00185848,\n",
       "        0.00154466, 0.12814612, 0.00160196, 0.00148986, 0.10717321,\n",
       "        0.00167482, 0.00158739, 0.10134767, 0.00590197, 0.00195483,\n",
       "        0.10572654, 0.00166332, 0.00155501, 0.12958672, 0.00153758,\n",
       "        0.00144208, 0.03579497, 0.00148231, 0.00464741, 0.07455   ,\n",
       "        0.00809624, 0.00170848, 0.10348735, 0.00122169, 0.00149592,\n",
       "        0.06503075, 0.0011337 , 0.0015813 , 0.02365148, 0.00158434,\n",
       "        0.00634594, 0.13155851, 0.00478079, 0.00132136, 0.10081347,\n",
       "        0.00148759, 0.00169148, 0.08389425, 0.00161281, 0.00143523,\n",
       "        0.11148782, 0.00144712, 0.00412819, 0.01010032, 0.0095333 ,\n",
       "        0.00129331, 0.00122574, 0.00162433, 0.00111218, 0.00114648,\n",
       "        0.00143953, 0.00138937, 0.00121366, 0.00128647, 0.00690996,\n",
       "        0.00695436, 0.00710182, 0.00141652, 0.0015929 , 0.00131394,\n",
       "        0.00121465, 0.00141288, 0.0012001 , 0.00147432, 0.00133619,\n",
       "        0.00132205, 0.00692958, 0.00714659, 0.00761572, 0.00183449,\n",
       "        0.00184926, 0.0011727 , 0.00139591, 0.00126999, 0.00134615,\n",
       "        0.00122319, 0.00123192, 0.00136637, 0.00753093, 0.00957644,\n",
       "        0.00951867, 0.00150862, 0.00124778, 0.00180418, 0.00135633,\n",
       "        0.00127881, 0.00137679, 0.00118211, 0.00129051, 0.00120201,\n",
       "        0.00443698, 0.00135882, 0.00083912, 0.0014898 , 0.00117218,\n",
       "        0.00160194, 0.00120755, 0.00129941, 0.00123402, 0.00118468,\n",
       "        0.00112621, 0.00135399, 0.00179704, 0.00258183, 0.00385054,\n",
       "        0.00124093, 0.00132093, 0.00141232, 0.0011451 , 0.00121677,\n",
       "        0.00115404, 0.00140618, 0.00111251, 0.00123266, 0.00206025,\n",
       "        0.00229995, 0.00148414, 0.00114222, 0.0011903 , 0.00132299,\n",
       "        0.00133959, 0.00116718, 0.00126951, 0.00128318, 0.00116534,\n",
       "        0.0013787 , 0.00175394, 0.00194855, 0.00156978, 0.00137586,\n",
       "        0.00153026, 0.00133668, 0.00132355, 0.00127352, 0.0013001 ,\n",
       "        0.00137199, 0.00123907, 0.00133432]),\n",
       " 'rank_test_roc_auc_ovo': array([  1,  62,  36,  10,  68,   5,  95,  90,  23, 146, 105, 144,  20,\n",
       "         40,  56,  54,  51,  47,  24,  89,  38, 127, 147, 152,  31,  59,\n",
       "         17,   6,  49,  15,  84,  32, 101, 102,  87, 126,  67,  45,  42,\n",
       "         50,  19,  16,  43,  18,  69,  29,  72,  66,  28,  44,   7,  12,\n",
       "         37,  13, 124, 103, 136, 157, 170, 149,  22,  41,   3,  27,  35,\n",
       "          4, 138,  85,  99, 155, 154, 133,  33,  77,  30,  14,  21,   8,\n",
       "         79,  88, 112, 159, 140, 148,  26,  11,  25,  34,   9,   2,  46,\n",
       "         70,  53,  71, 128, 113, 251, 283, 252, 244, 284, 242, 235, 269,\n",
       "        238, 212, 279, 221, 254, 275, 249, 245, 270, 241, 227, 288, 232,\n",
       "        214, 287, 215, 250, 281, 253, 246, 280, 247, 230, 286, 229, 222,\n",
       "        278, 216, 256, 285, 255, 248, 282, 243, 240, 277, 219, 220, 262,\n",
       "        218, 206, 268, 208, 201, 263, 195, 181, 266, 183, 164, 264, 166,\n",
       "        205, 272, 210, 196, 271, 197, 188, 261, 178, 168, 257, 174, 211,\n",
       "        267, 217, 198, 273, 185, 176, 258, 187, 169, 259, 163, 209, 276,\n",
       "        207, 192, 274, 199, 182, 265, 189, 173, 260, 162, 237, 226, 223,\n",
       "        153, 167, 180, 131, 117, 132, 120, 118, 121, 239, 236, 224, 151,\n",
       "        172, 156, 123, 143, 141, 122, 111, 107, 228, 213, 233, 175, 184,\n",
       "        158, 137, 134, 142, 110, 116, 114, 225, 231, 234, 160, 171, 177,\n",
       "        139, 150, 145, 130, 135, 129, 204, 165, 186, 108,  94,  98,  83,\n",
       "         97,  58,  57,  48,  74, 191, 202, 203, 115, 109, 100,  73,  81,\n",
       "         52,  65,  39,  55, 193, 194, 190, 104,  92,  96,  64,  86,  76,\n",
       "         63,  60,  61, 200, 179, 161, 106, 125, 119,  80,  78,  93,  82,\n",
       "         75,  91]),\n",
       " 'split0_test_neg_log_loss': array([-0.21442711, -0.23139251, -0.22588172, -0.25208903, -0.27236992,\n",
       "        -0.2395948 , -0.27555097, -0.24231169, -0.24415569, -0.31192631,\n",
       "        -0.30474937, -0.34205879, -0.22441812, -0.21670915, -0.22880987,\n",
       "        -0.23412275, -0.25109402, -0.23166363, -0.24221528, -0.28552265,\n",
       "        -0.2600095 , -0.25203527, -0.28655364, -0.27061035, -0.23997262,\n",
       "        -0.23630047, -0.2300278 , -0.22761856, -0.2354405 , -0.23399399,\n",
       "        -0.24287063, -0.26105797, -0.25603896, -0.29234594, -0.27893525,\n",
       "        -0.26556615, -0.22308569, -0.2273372 , -0.22410973, -0.22370556,\n",
       "        -0.21329406, -0.2190053 , -0.23529041, -0.22118858, -0.22453334,\n",
       "        -0.23562247, -0.23039852, -0.22615214, -0.22171226, -0.22657302,\n",
       "        -0.22191737, -0.23253778, -0.21827123, -0.22870107, -0.29998599,\n",
       "        -0.26306388, -0.26327347, -0.29595971, -0.26584158, -0.34559846,\n",
       "        -0.2433475 , -0.22256652, -0.22804312, -0.23000953, -0.2685709 ,\n",
       "        -0.21866426, -0.35353792, -0.27224811, -0.25506326, -0.35976274,\n",
       "        -0.35331148, -0.26107206, -0.23961556, -0.23818314, -0.21904811,\n",
       "        -0.22850652, -0.23946127, -0.22581169, -0.23932804, -0.26812808,\n",
       "        -0.27791312, -0.3785683 , -0.32098744, -0.31327598, -0.21552242,\n",
       "        -0.23164577, -0.21833002, -0.21647765, -0.2216032 , -0.2187408 ,\n",
       "        -0.23828163, -0.23060294, -0.26368552, -0.24736858, -0.28738359,\n",
       "        -0.29945008, -0.88373323, -2.01771182, -0.93343991, -0.70175224,\n",
       "        -1.94668898, -0.67312157, -0.63480907, -2.05585879, -0.62278254,\n",
       "        -0.56708053, -1.9898525 , -0.58061342, -0.82516074, -1.90986945,\n",
       "        -0.89400311, -0.66942037, -1.86499941, -0.67961232, -0.60539328,\n",
       "        -2.07798781, -0.62864654, -0.54373289, -1.96255084, -0.56723382,\n",
       "        -0.94205501, -2.07323223, -0.83639228, -0.65940289, -2.06916132,\n",
       "        -0.68376879, -0.61879062, -1.83639724, -0.5875055 , -0.56055484,\n",
       "        -1.8559025 , -0.57762454, -0.92772376, -1.94978271, -0.90097255,\n",
       "        -0.67862464, -1.98309446, -0.67564545, -0.59253993, -1.93938978,\n",
       "        -0.61177891, -0.56993053, -1.97483624, -0.5750745 , -0.38621996,\n",
       "        -2.02829067, -0.35390517, -0.32023145, -2.05631847, -0.32349695,\n",
       "        -0.28052697, -1.68099926, -0.27970612, -0.2762585 , -1.92451695,\n",
       "        -0.26832875, -0.36846833, -1.81922512, -0.40576872, -0.31604435,\n",
       "        -2.13766938, -0.3027872 , -0.29384091, -2.07431308, -0.28356888,\n",
       "        -0.28042729, -1.42991777, -0.28051885, -0.4025336 , -1.9375347 ,\n",
       "        -0.37284196, -0.30563769, -2.028565  , -0.2919295 , -0.29885237,\n",
       "        -1.61973628, -0.2861962 , -0.28167659, -1.78590632, -0.27707083,\n",
       "        -0.40762557, -2.23355778, -0.39004079, -0.31667487, -2.03440395,\n",
       "        -0.3069955 , -0.29144614, -1.90569453, -0.28777119, -0.28194015,\n",
       "        -1.78837505, -0.27765481, -0.36867687, -0.46105309, -0.31842781,\n",
       "        -0.25409546, -0.25112343, -0.25386649, -0.24232837, -0.24315663,\n",
       "        -0.24315162, -0.24351338, -0.23367266, -0.23785883, -0.36268582,\n",
       "        -0.38189701, -0.38568069, -0.25331251, -0.25791856, -0.2534984 ,\n",
       "        -0.24241118, -0.24136391, -0.24307783, -0.23888415, -0.23419096,\n",
       "        -0.24029804, -0.41376387, -0.37539547, -0.32738607, -0.25222288,\n",
       "        -0.27214781, -0.25457221, -0.24856562, -0.24810336, -0.24115845,\n",
       "        -0.23636929, -0.24044408, -0.24196139, -0.40602871, -0.38985249,\n",
       "        -0.40730262, -0.26565125, -0.27243049, -0.2598077 , -0.24941696,\n",
       "        -0.25330803, -0.25515405, -0.24903617, -0.25107979, -0.24552511,\n",
       "        -0.27912901, -0.25630873, -0.26410794, -0.23795522, -0.2357324 ,\n",
       "        -0.22732616, -0.22754304, -0.22767448, -0.23167756, -0.22755474,\n",
       "        -0.22639637, -0.22877346, -0.27242157, -0.30115   , -0.3162449 ,\n",
       "        -0.23256736, -0.22901291, -0.23538792, -0.22314454, -0.23053908,\n",
       "        -0.22644305, -0.2354901 , -0.22716615, -0.22722594, -0.28165597,\n",
       "        -0.32478948, -0.26143332, -0.23851696, -0.23181973, -0.23394718,\n",
       "        -0.23469311, -0.22522072, -0.23156024, -0.22527957, -0.23136822,\n",
       "        -0.23164945, -0.27388718, -0.26420993, -0.28015671, -0.23422591,\n",
       "        -0.24364065, -0.2335135 , -0.23313212, -0.22585193, -0.22900607,\n",
       "        -0.22700244, -0.23206961, -0.22811951]),\n",
       " 'split1_test_neg_log_loss': array([-0.20120174, -0.2121051 , -0.21040668, -0.18774132, -0.22065108,\n",
       "        -0.1980023 , -0.2043553 , -0.23011744, -0.21458204, -0.24376656,\n",
       "        -0.22775964, -0.22833819, -0.21081735, -0.1982103 , -0.21606539,\n",
       "        -0.21384211, -0.19988772, -0.19962491, -0.2181365 , -0.19526233,\n",
       "        -0.21668016, -0.29090386, -0.2741501 , -0.26097667, -0.20730357,\n",
       "        -0.22196988, -0.19497889, -0.19612288, -0.19450058, -0.2035703 ,\n",
       "        -0.22975014, -0.2075494 , -0.21679438, -0.22959563, -0.22640441,\n",
       "        -0.2218066 , -0.20840724, -0.20257377, -0.20133515, -0.21213178,\n",
       "        -0.20155456, -0.20159643, -0.21725951, -0.20447517, -0.20595782,\n",
       "        -0.19385363, -0.21074985, -0.21898817, -0.22191676, -0.21827241,\n",
       "        -0.20347352, -0.20515961, -0.20816949, -0.20344086, -0.23082641,\n",
       "        -0.19380374, -0.22618688, -0.28906283, -0.25600538, -0.26610021,\n",
       "        -0.2004528 , -0.20569683, -0.2048505 , -0.21666335, -0.20719267,\n",
       "        -0.21328029, -0.20410503, -0.20377696, -0.20160056, -0.31430661,\n",
       "        -0.26483428, -0.24287198, -0.20787538, -0.22847408, -0.20994396,\n",
       "        -0.20753734, -0.20906056, -0.21690971, -0.20696182, -0.21909435,\n",
       "        -0.19774397, -0.31410155, -0.24675865, -0.27935563, -0.20856018,\n",
       "        -0.20539703, -0.20729815, -0.20020852, -0.19593737, -0.19553311,\n",
       "        -0.21941977, -0.20944532, -0.21925366, -0.21870637, -0.21581927,\n",
       "        -0.23614025, -0.76552138, -2.00711235, -0.88203022, -0.66653467,\n",
       "        -1.92370686, -0.67094664, -0.58042013, -1.85648303, -0.58153752,\n",
       "        -0.54403619, -1.9305446 , -0.56461329, -0.8319898 , -1.90775903,\n",
       "        -0.96388227, -0.66390803, -1.90651655, -0.70616685, -0.60230463,\n",
       "        -1.99438491, -0.59657759, -0.55091729, -1.92561906, -0.5535278 ,\n",
       "        -0.82813717, -1.89835838, -0.95183765, -0.66604941, -1.92152662,\n",
       "        -0.64658209, -0.60844203, -1.90624177, -0.58798573, -0.54229578,\n",
       "        -1.99726028, -0.55050959, -0.84710802, -1.99074972, -0.91972441,\n",
       "        -0.64350696, -1.90841424, -0.66593411, -0.61393656, -1.89559048,\n",
       "        -0.61058598, -0.5476313 , -1.96069584, -0.53745845, -0.4184855 ,\n",
       "        -1.75302215, -0.3742155 , -0.26760779, -1.74509967, -0.29434   ,\n",
       "        -0.27333147, -2.10377872, -0.26625832, -0.25201712, -1.61389689,\n",
       "        -0.26245861, -0.3231034 , -2.05935977, -0.32145714, -0.27645508,\n",
       "        -2.12620436, -0.28564401, -0.27833296, -1.92164555, -0.26979114,\n",
       "        -0.26294022, -1.74640087, -0.26123277, -0.39855365, -1.98101096,\n",
       "        -0.44154923, -0.27826302, -2.03962811, -0.2841517 , -0.26491834,\n",
       "        -1.79712197, -0.26703313, -0.25851593, -1.46652518, -0.26177571,\n",
       "        -0.40130225, -1.75912367, -0.45516678, -0.27054126, -2.30474755,\n",
       "        -0.28611795, -0.27421861, -1.87580337, -0.26766376, -0.26188154,\n",
       "        -1.71024326, -0.26143383, -0.36102787, -0.2921742 , -0.39355917,\n",
       "        -0.24061525, -0.24966818, -0.25640023, -0.23391777, -0.23083221,\n",
       "        -0.22622946, -0.22845947, -0.23111663, -0.22822056, -0.38794605,\n",
       "        -0.36020768, -0.36264902, -0.24486388, -0.24105857, -0.23547147,\n",
       "        -0.22946713, -0.23225444, -0.23206411, -0.22885653, -0.22406118,\n",
       "        -0.22532709, -0.36806649, -0.38459257, -0.36872864, -0.2454308 ,\n",
       "        -0.24594237, -0.24512083, -0.23097251, -0.22973486, -0.2321297 ,\n",
       "        -0.22445364, -0.22504976, -0.22425904, -0.34618086, -0.37896602,\n",
       "        -0.31157723, -0.25590948, -0.25663196, -0.24967753, -0.23586429,\n",
       "        -0.24159382, -0.23834303, -0.2353644 , -0.23431595, -0.23710453,\n",
       "        -0.31208741, -0.25962216, -0.2658543 , -0.22707891, -0.21997169,\n",
       "        -0.22647792, -0.21919123, -0.2216959 , -0.21625372, -0.21650582,\n",
       "        -0.21825244, -0.21304931, -0.25070884, -0.30259915, -0.25059502,\n",
       "        -0.21856798, -0.22665688, -0.2164933 , -0.21953358, -0.21855685,\n",
       "        -0.21655232, -0.21586583, -0.21145298, -0.21889368, -0.26173952,\n",
       "        -0.24799067, -0.24876968, -0.22599974, -0.22122582, -0.21854491,\n",
       "        -0.21687379, -0.21921783, -0.21546026, -0.21357177, -0.21547423,\n",
       "        -0.21335829, -0.25146248, -0.26556138, -0.23629896, -0.22911522,\n",
       "        -0.22067434, -0.22590941, -0.2166458 , -0.21501123, -0.21738032,\n",
       "        -0.21494012, -0.21437739, -0.2145551 ]),\n",
       " 'split2_test_neg_log_loss': array([-0.18405842, -0.18370543, -0.19865385, -0.18969202, -0.18500157,\n",
       "        -0.17853765, -0.24557225, -0.22352195, -0.19549273, -0.24626863,\n",
       "        -0.2552648 , -0.23265725, -0.17214551, -0.17644304, -0.17471092,\n",
       "        -0.18975113, -0.16676685, -0.19626501, -0.20816215, -0.20129952,\n",
       "        -0.19211656, -0.23626221, -0.21491973, -0.26330773, -0.17297432,\n",
       "        -0.18079504, -0.17830312, -0.17619217, -0.18140832, -0.19169994,\n",
       "        -0.19046558, -0.20055987, -0.2149958 , -0.19955695, -0.20277683,\n",
       "        -0.2310779 , -0.17829972, -0.17940705, -0.18022578, -0.1724441 ,\n",
       "        -0.17365433, -0.17433899, -0.18239968, -0.17843768, -0.1896314 ,\n",
       "        -0.18543526, -0.2045737 , -0.19542096, -0.16719085, -0.18930436,\n",
       "        -0.17608349, -0.18646131, -0.21130013, -0.17991523, -0.21509418,\n",
       "        -0.27016398, -0.22324453, -0.24796064, -0.26661765, -0.28359764,\n",
       "        -0.17886476, -0.17859241, -0.18267237, -0.19954757, -0.1900164 ,\n",
       "        -0.18349305, -0.25595786, -0.24138503, -0.21705091, -0.27299679,\n",
       "        -0.26292043, -0.26621868, -0.18339314, -0.205778  , -0.1726165 ,\n",
       "        -0.19682327, -0.17093982, -0.18108176, -0.22271541, -0.20102643,\n",
       "        -0.23569888, -0.26437356, -0.25318481, -0.2560572 , -0.1838734 ,\n",
       "        -0.17549991, -0.17467893, -0.200656  , -0.1663598 , -0.18130085,\n",
       "        -0.19949381, -0.21377666, -0.18929564, -0.21493226, -0.21186349,\n",
       "        -0.22456553, -0.86338028, -1.936368  , -0.81713349, -0.67292928,\n",
       "        -1.93675958, -0.66972439, -0.59998934, -1.90822443, -0.57173464,\n",
       "        -0.52267994, -1.99706196, -0.53418772, -0.99889841, -2.00781001,\n",
       "        -0.78502275, -0.72575775, -1.99306124, -0.64681154, -0.6126056 ,\n",
       "        -1.97706312, -0.60695945, -0.53287634, -1.90801877, -0.53833   ,\n",
       "        -0.80984612, -2.089915  , -0.85740572, -0.68600626, -1.88581113,\n",
       "        -0.65513194, -0.59180095, -1.96543461, -0.60650774, -0.53058069,\n",
       "        -1.88957888, -0.54028978, -0.82572632, -1.96775672, -0.8207195 ,\n",
       "        -0.66367578, -1.96745327, -0.70410346, -0.59632574, -1.98275541,\n",
       "        -0.5805914 , -0.55808151, -1.9638461 , -0.53842472, -0.31606911,\n",
       "        -2.00115199, -0.33486797, -0.28117342, -1.78351001, -0.26558453,\n",
       "        -0.25325149, -2.03771397, -0.25221996, -0.24189886, -1.63599187,\n",
       "        -0.23454385, -0.33122057, -2.25428642, -0.344951  , -0.2627779 ,\n",
       "        -1.82017546, -0.2678746 , -0.24848063, -1.97509851, -0.24387893,\n",
       "        -0.24601242, -1.64052927, -0.2390354 , -0.3228122 , -1.88864348,\n",
       "        -0.34915452, -0.27192022, -1.69700443, -0.27269327, -0.2449476 ,\n",
       "        -1.61030995, -0.2598739 , -0.24364982, -1.7448462 , -0.24075877,\n",
       "        -0.42019926, -1.95829324, -0.35124216, -0.29026554, -1.74279946,\n",
       "        -0.26750173, -0.25685481, -1.95137732, -0.258601  , -0.24745646,\n",
       "        -1.75470172, -0.23709894, -0.35966806, -0.30723779, -0.37759992,\n",
       "        -0.22488138, -0.23118183, -0.23442076, -0.21943123, -0.21087308,\n",
       "        -0.20711747, -0.20220945, -0.20496803, -0.2039372 , -0.36339008,\n",
       "        -0.37255487, -0.35913441, -0.2133867 , -0.23007364, -0.21986838,\n",
       "        -0.20296323, -0.20876164, -0.21739139, -0.20131553, -0.20313147,\n",
       "        -0.19597018, -0.34978149, -0.30573612, -0.37272827, -0.21990533,\n",
       "        -0.22305014, -0.22694399, -0.20666418, -0.21339264, -0.21234745,\n",
       "        -0.20408279, -0.20488037, -0.199183  , -0.30251804, -0.30041027,\n",
       "        -0.38560577, -0.23034991, -0.23597028, -0.24468646, -0.21705286,\n",
       "        -0.22363356, -0.22241114, -0.21762888, -0.21263123, -0.21639044,\n",
       "        -0.21482351, -0.23706559, -0.27298344, -0.19769548, -0.19189955,\n",
       "        -0.18674273, -0.18461329, -0.19324809, -0.18266098, -0.18662114,\n",
       "        -0.18485348, -0.18683686, -0.22934371, -0.21159589, -0.23229073,\n",
       "        -0.19517345, -0.19974537, -0.19248182, -0.18406083, -0.187016  ,\n",
       "        -0.1843825 , -0.18236652, -0.18588647, -0.1832656 , -0.22279156,\n",
       "        -0.22699511, -0.22973796, -0.19408836, -0.18975605, -0.19397617,\n",
       "        -0.18193065, -0.19268189, -0.18865288, -0.18549343, -0.18275356,\n",
       "        -0.18077428, -0.23601192, -0.21996802, -0.22651027, -0.19654541,\n",
       "        -0.19154375, -0.20003368, -0.18968282, -0.19228798, -0.19401968,\n",
       "        -0.18865179, -0.18948147, -0.18872136]),\n",
       " 'split3_test_neg_log_loss': array([-0.22283187, -0.2475463 , -0.24402008, -0.25141014, -0.25410207,\n",
       "        -0.22991896, -0.29189156, -0.28257007, -0.26395183, -0.36454304,\n",
       "        -0.27430645, -0.29296251, -0.24506593, -0.23796497, -0.25203169,\n",
       "        -0.2535471 , -0.27944012, -0.25592861, -0.26704202, -0.28306471,\n",
       "        -0.26931209, -0.29966205, -0.37124974, -0.35833335, -0.24084003,\n",
       "        -0.24686559, -0.23550623, -0.24112329, -0.26443978, -0.23189878,\n",
       "        -0.28769281, -0.24470585, -0.28159113, -0.28546752, -0.28063412,\n",
       "        -0.33412021, -0.23898758, -0.23417806, -0.23749714, -0.24516275,\n",
       "        -0.24253193, -0.23715838, -0.24288793, -0.24157536, -0.25542606,\n",
       "        -0.24495018, -0.24686328, -0.24906268, -0.2365624 , -0.23902483,\n",
       "        -0.22720143, -0.23024516, -0.22953955, -0.2518899 , -0.27644734,\n",
       "        -0.30144411, -0.33815942, -0.38391455, -0.46970338, -0.3721107 ,\n",
       "        -0.2432902 , -0.25302233, -0.23048762, -0.23480365, -0.24732352,\n",
       "        -0.23568617, -0.29217505, -0.29011342, -0.31623935, -0.31217428,\n",
       "        -0.38373505, -0.36379583, -0.24373037, -0.22356599, -0.24863412,\n",
       "        -0.22966432, -0.24910088, -0.24587749, -0.29026839, -0.32322767,\n",
       "        -0.33792439, -0.34404401, -0.38311515, -0.33717608, -0.23913216,\n",
       "        -0.22219234, -0.23341445, -0.23810717, -0.24474393, -0.23575683,\n",
       "        -0.25469598, -0.27260804, -0.24211065, -0.27108158, -0.31014234,\n",
       "        -0.26547311, -0.86445106, -1.91660345, -0.80599481, -0.70786465,\n",
       "        -1.93160685, -0.73158383, -0.61130574, -1.94095888, -0.60085466,\n",
       "        -0.54736307, -1.87324633, -0.5779743 , -0.89615397, -1.93093785,\n",
       "        -0.83373735, -0.68022342, -2.05125013, -0.67311773, -0.61351772,\n",
       "        -1.93308213, -0.61136264, -0.57150748, -1.86488163, -0.55050123,\n",
       "        -0.87719051, -1.9144843 , -0.88107347, -0.69814875, -2.01699197,\n",
       "        -0.74718849, -0.61899634, -1.96599691, -0.62278482, -0.59107585,\n",
       "        -2.00145953, -0.56280226, -0.81124336, -1.95635856, -0.82529517,\n",
       "        -0.71558047, -1.92056284, -0.68011502, -0.63888582, -1.8880177 ,\n",
       "        -0.62521016, -0.56327044, -1.91459688, -0.56932457, -0.38005611,\n",
       "        -2.05459863, -0.359567  , -0.2937953 , -1.73221987, -0.30670616,\n",
       "        -0.28625366, -1.89317636, -0.28128138, -0.27739266, -1.79117385,\n",
       "        -0.27984446, -0.37555732, -2.18120788, -0.39015094, -0.30237614,\n",
       "        -1.94290461, -0.30057543, -0.27273889, -1.70487208, -0.28061616,\n",
       "        -0.27616705, -1.68132798, -0.27842399, -0.41025389, -1.68904114,\n",
       "        -0.39208512, -0.30206584, -2.03980608, -0.28980183, -0.28295187,\n",
       "        -1.78121877, -0.2851601 , -0.27840575, -1.73056369, -0.27361324,\n",
       "        -0.41182221, -2.01980955, -0.32872674, -0.29642173, -1.65529139,\n",
       "        -0.30546242, -0.29479566, -1.67799105, -0.28705499, -0.28050697,\n",
       "        -2.117208  , -0.28018728, -0.35629241, -0.36943803, -0.3252852 ,\n",
       "        -0.25613944, -0.26589743, -0.26835994, -0.24836004, -0.24899409,\n",
       "        -0.24817927, -0.24723869, -0.24970487, -0.24791273, -0.34989183,\n",
       "        -0.34037272, -0.37984022, -0.25536321, -0.26223196, -0.25310206,\n",
       "        -0.24976618, -0.25577462, -0.25215419, -0.25572672, -0.25442378,\n",
       "        -0.24891068, -0.32785418, -0.31604452, -0.39631386, -0.27115399,\n",
       "        -0.27069395, -0.25594663, -0.25136791, -0.24969946, -0.25539442,\n",
       "        -0.24529444, -0.24755773, -0.24830766, -0.34650692, -0.33713058,\n",
       "        -0.33819411, -0.262754  , -0.2638089 , -0.28348771, -0.25809014,\n",
       "        -0.26060734, -0.25822763, -0.25276391, -0.2535244 , -0.25367976,\n",
       "        -0.27845847, -0.25776957, -0.26277479, -0.25211249, -0.24626671,\n",
       "        -0.25962372, -0.24581721, -0.25077091, -0.24457029, -0.24375281,\n",
       "        -0.24323602, -0.25045012, -0.28589959, -0.27189414, -0.35005247,\n",
       "        -0.25259938, -0.25104895, -0.25067561, -0.24623201, -0.24753876,\n",
       "        -0.24422691, -0.24614022, -0.24172326, -0.24552535, -0.26918408,\n",
       "        -0.27369914, -0.28384372, -0.24550328, -0.24436894, -0.24853932,\n",
       "        -0.24551012, -0.24868841, -0.24968608, -0.24468701, -0.24505217,\n",
       "        -0.24675307, -0.27719511, -0.26884319, -0.27402872, -0.24656134,\n",
       "        -0.25359261, -0.25346055, -0.24555469, -0.24424578, -0.24719512,\n",
       "        -0.24893191, -0.24655047, -0.24661065]),\n",
       " 'split4_test_neg_log_loss': array([-0.18880603, -0.20266585, -0.18769385, -0.18265168, -0.18556261,\n",
       "        -0.18405865, -0.19509135, -0.18562155, -0.19980533, -0.19363861,\n",
       "        -0.21501383, -0.21283197, -0.19113845, -0.19986651, -0.19454759,\n",
       "        -0.17914739, -0.1858675 , -0.18756026, -0.17332757, -0.19367963,\n",
       "        -0.18561634, -0.21277081, -0.22732494, -0.19530204, -0.19657832,\n",
       "        -0.20125281, -0.1863858 , -0.17906084, -0.19912497, -0.18411732,\n",
       "        -0.18400259, -0.17454704, -0.19210251, -0.22043527, -0.21772817,\n",
       "        -0.18054529, -0.19470929, -0.18816447, -0.20785481, -0.18335088,\n",
       "        -0.17859886, -0.18237572, -0.17877631, -0.18182273, -0.19047465,\n",
       "        -0.19214686, -0.18344763, -0.19158562, -0.1912635 , -0.1880619 ,\n",
       "        -0.19566214, -0.17656368, -0.18077814, -0.18649922, -0.21184052,\n",
       "        -0.18709544, -0.24179378, -0.2280472 , -0.20968204, -0.21491258,\n",
       "        -0.18750679, -0.19856624, -0.1900859 , -0.20307815, -0.18431837,\n",
       "        -0.18153057, -0.19321357, -0.19010935, -0.21817348, -0.23257611,\n",
       "        -0.19866925, -0.22176231, -0.18461932, -0.19023092, -0.19282143,\n",
       "        -0.18182924, -0.18556716, -0.17923267, -0.20371518, -0.19109989,\n",
       "        -0.19205694, -0.20303338, -0.20918924, -0.20424115, -0.19335011,\n",
       "        -0.19245644, -0.20121605, -0.17502895, -0.18868137, -0.18417587,\n",
       "        -0.20318112, -0.19103634, -0.17828494, -0.19022495, -0.19544793,\n",
       "        -0.20881805, -0.85854672, -1.90939654, -0.88453598, -0.63471393,\n",
       "        -1.93736931, -0.67555916, -0.61860245, -1.86444749, -0.58388974,\n",
       "        -0.55780039, -1.92329784, -0.55957513, -0.83969603, -1.96475326,\n",
       "        -0.90647789, -0.64433776, -1.85796718, -0.66028029, -0.60356869,\n",
       "        -1.95889191, -0.6351798 , -0.55333251, -2.04799657, -0.54354469,\n",
       "        -0.95857258, -1.95557625, -0.81913942, -0.67791016, -1.93733038,\n",
       "        -0.65509735, -0.62803548, -2.00892764, -0.58293755, -0.55715187,\n",
       "        -1.89640467, -0.5506713 , -0.86097974, -1.91373763, -0.82319594,\n",
       "        -0.68174948, -1.97270668, -0.70272762, -0.61254637, -1.87864046,\n",
       "        -0.60150308, -0.5451046 , -1.89765949, -0.54390462, -0.34420147,\n",
       "        -2.00435197, -0.41239677, -0.30258053, -1.62819175, -0.28025406,\n",
       "        -0.25602918, -1.59735994, -0.26221963, -0.24507629, -2.23524559,\n",
       "        -0.25188438, -0.39806106, -1.62484032, -0.35334742, -0.30041114,\n",
       "        -1.88017953, -0.27386836, -0.25606695, -1.57519068, -0.25361567,\n",
       "        -0.25289513, -1.7120956 , -0.25922305, -0.40832846, -2.0388538 ,\n",
       "        -0.35206479, -0.28361965, -1.94339495, -0.29250099, -0.26050248,\n",
       "        -1.79560409, -0.26735414, -0.25470621, -1.73536958, -0.24931157,\n",
       "        -0.38604893, -2.10095326, -0.34890336, -0.28501725, -1.9075852 ,\n",
       "        -0.2759755 , -0.25623996, -2.12853103, -0.26690812, -0.26041208,\n",
       "        -1.71139086, -0.25397731, -0.3813938 , -0.34550212, -0.29441506,\n",
       "        -0.22981715, -0.24530422, -0.23809595, -0.22173203, -0.21809001,\n",
       "        -0.21746337, -0.209919  , -0.21683203, -0.21233753, -0.3828995 ,\n",
       "        -0.36048539, -0.3215087 , -0.22973805, -0.23425912, -0.24074642,\n",
       "        -0.2211598 , -0.22172489, -0.21997683, -0.21118003, -0.21264781,\n",
       "        -0.2146392 , -0.3521932 , -0.3169587 , -0.33869061, -0.24062174,\n",
       "        -0.23827678, -0.24063778, -0.22185999, -0.21922038, -0.22149342,\n",
       "        -0.21548649, -0.21560464, -0.21859519, -0.40140001, -0.3831249 ,\n",
       "        -0.38051591, -0.24666008, -0.25451507, -0.2464412 , -0.23647867,\n",
       "        -0.2339491 , -0.2318176 , -0.23017015, -0.22769098, -0.22588468,\n",
       "        -0.23696033, -0.23615379, -0.25114772, -0.20353213, -0.21079461,\n",
       "        -0.20098536, -0.20702412, -0.20328412, -0.20259909, -0.19565986,\n",
       "        -0.20119356, -0.19797256, -0.25850428, -0.24094467, -0.25079074,\n",
       "        -0.21824874, -0.21000128, -0.20667428, -0.20943468, -0.20443048,\n",
       "        -0.20324995, -0.19878519, -0.1954907 , -0.19816099, -0.23188087,\n",
       "        -0.23214993, -0.2534413 , -0.21821963, -0.20736309, -0.20994364,\n",
       "        -0.20425169, -0.20711259, -0.20033049, -0.19664581, -0.1982849 ,\n",
       "        -0.19864102, -0.24407283, -0.23675537, -0.23842563, -0.21014775,\n",
       "        -0.21849944, -0.21677001, -0.20402787, -0.20321483, -0.20638102,\n",
       "        -0.20525739, -0.20544965, -0.20836965]),\n",
       " 'mean_test_neg_log_loss': array([-0.20226503, -0.21548304, -0.21333124, -0.21271684, -0.22353745,\n",
       "        -0.20602247, -0.24249229, -0.23282854, -0.22359752, -0.27202863,\n",
       "        -0.25541882, -0.26176974, -0.20871707, -0.20583879, -0.21323309,\n",
       "        -0.2140821 , -0.21661124, -0.21420849, -0.22177671, -0.23176577,\n",
       "        -0.22474693, -0.25832684, -0.27483963, -0.26970603, -0.21153377,\n",
       "        -0.21743676, -0.20504037, -0.20402355, -0.21498283, -0.20905607,\n",
       "        -0.22695635, -0.21768402, -0.23230456, -0.24548026, -0.24129576,\n",
       "        -0.24662323, -0.2086979 , -0.20633211, -0.21020452, -0.20735902,\n",
       "        -0.20192675, -0.20289496, -0.21132277, -0.2054999 , -0.21320466,\n",
       "        -0.21040168, -0.2152066 , -0.21624191, -0.20772915, -0.2122473 ,\n",
       "        -0.20486759, -0.20619351, -0.20961171, -0.21008926, -0.24683889,\n",
       "        -0.24311423, -0.25853161, -0.28898898, -0.29357001, -0.29646392,\n",
       "        -0.21069241, -0.21168887, -0.2072279 , -0.21682045, -0.21948437,\n",
       "        -0.20653087, -0.25979788, -0.23952657, -0.24162551, -0.29836331,\n",
       "        -0.2926941 , -0.27114417, -0.21184675, -0.21724642, -0.20861282,\n",
       "        -0.20887214, -0.21082594, -0.20978266, -0.23259777, -0.24051529,\n",
       "        -0.24826746, -0.30082416, -0.28264706, -0.27802121, -0.20808766,\n",
       "        -0.2054383 , -0.20698752, -0.20609566, -0.20346513, -0.20310149,\n",
       "        -0.22301446, -0.22349386, -0.21852608, -0.22846275, -0.24413132,\n",
       "        -0.2468894 , -0.84712654, -1.95743843, -0.86462688, -0.67675895,\n",
       "        -1.93522631, -0.68418712, -0.60902535, -1.92519453, -0.59215982,\n",
       "        -0.54779202, -1.94280065, -0.56339277, -0.87837979, -1.94422592,\n",
       "        -0.87662467, -0.67672947, -1.9347589 , -0.67319774, -0.60747799,\n",
       "        -1.98828197, -0.61574521, -0.5504733 , -1.94181337, -0.55062751,\n",
       "        -0.88316028, -1.98631323, -0.86916971, -0.67750349, -1.96616428,\n",
       "        -0.67755373, -0.61321308, -1.93659963, -0.59754427, -0.5563318 ,\n",
       "        -1.92812117, -0.55637949, -0.85455624, -1.95567707, -0.85798151,\n",
       "        -0.67662747, -1.9504463 , -0.68570513, -0.61084688, -1.91687877,\n",
       "        -0.6059339 , -0.55680368, -1.94232691, -0.55283737, -0.36900643,\n",
       "        -1.96828308, -0.36699048, -0.2930777 , -1.78906795, -0.29407634,\n",
       "        -0.26987855, -1.86260565, -0.26833708, -0.25852869, -1.84016503,\n",
       "        -0.25941201, -0.35928214, -1.9877839 , -0.36313504, -0.29161292,\n",
       "        -1.98142667, -0.28614992, -0.26989207, -1.85022398, -0.26629415,\n",
       "        -0.26368842, -1.6420543 , -0.26368681, -0.38849636, -1.90701682,\n",
       "        -0.38153912, -0.28830128, -1.94967971, -0.28621546, -0.27043453,\n",
       "        -1.72079821, -0.27312349, -0.26339086, -1.6926422 , -0.26050603,\n",
       "        -0.40539965, -2.0143475 , -0.37481597, -0.29178413, -1.92896551,\n",
       "        -0.28841062, -0.27471104, -1.90787946, -0.27359981, -0.26643944,\n",
       "        -1.81638378, -0.26207043, -0.3654118 , -0.35508104, -0.34185743,\n",
       "        -0.24110973, -0.24863502, -0.25022867, -0.23315389, -0.2303892 ,\n",
       "        -0.22842824, -0.226268  , -0.22725885, -0.22605337, -0.36936266,\n",
       "        -0.36310353, -0.36176261, -0.23933287, -0.24510837, -0.24053735,\n",
       "        -0.22915351, -0.2319759 , -0.23293287, -0.22719259, -0.22569104,\n",
       "        -0.22502904, -0.36233185, -0.33974548, -0.36076949, -0.24586695,\n",
       "        -0.25002221, -0.24464429, -0.23188604, -0.23203014, -0.23250469,\n",
       "        -0.22513733, -0.22670732, -0.22646126, -0.36052691, -0.35789685,\n",
       "        -0.36463913, -0.25226494, -0.25667134, -0.25682012, -0.23938058,\n",
       "        -0.24261837, -0.24119069, -0.2369927 , -0.23584847, -0.2357169 ,\n",
       "        -0.26429175, -0.24938397, -0.26337364, -0.22367485, -0.22093299,\n",
       "        -0.22023118, -0.21683778, -0.2193347 , -0.21555233, -0.21401887,\n",
       "        -0.21478637, -0.21541646, -0.2593756 , -0.26563677, -0.27999477,\n",
       "        -0.22343138, -0.22329308, -0.22034258, -0.21648113, -0.21761624,\n",
       "        -0.21497095, -0.21572957, -0.21234391, -0.21461431, -0.2534504 ,\n",
       "        -0.26112487, -0.2554452 , -0.22446559, -0.21890672, -0.22099025,\n",
       "        -0.21665187, -0.21858429, -0.21713799, -0.21313552, -0.21458662,\n",
       "        -0.21423522, -0.2565259 , -0.25106758, -0.25108406, -0.22331913,\n",
       "        -0.22559016, -0.22593743, -0.21780866, -0.21612235, -0.21879644,\n",
       "        -0.21695673, -0.21758572, -0.21727525]),\n",
       " 'std_test_neg_log_loss': array([0.01472866, 0.02221113, 0.01990263, 0.03195364, 0.03536733,\n",
       "        0.02449609, 0.03806292, 0.03126216, 0.02641339, 0.0595954 ,\n",
       "        0.03221138, 0.04853747, 0.02537188, 0.02053401, 0.02679214,\n",
       "        0.02746489, 0.0420615 , 0.02565162, 0.03166263, 0.04297123,\n",
       "        0.03432352, 0.03277696, 0.05528226, 0.05197829, 0.02606153,\n",
       "        0.02386202, 0.02330943, 0.02605094, 0.03053768, 0.02047906,\n",
       "        0.03775338, 0.03120328, 0.03209003, 0.03683448, 0.03232686,\n",
       "        0.05145556, 0.02118626, 0.0213811 , 0.0196018 , 0.02651246,\n",
       "        0.02498662, 0.02311577, 0.02646458, 0.02383945, 0.02464084,\n",
       "        0.02473898, 0.02179217, 0.02109219, 0.02507047, 0.02034638,\n",
       "        0.01846458, 0.02254266, 0.0161766 , 0.02685091, 0.03518604,\n",
       "        0.04494815, 0.04227195, 0.05377116, 0.09051691, 0.0563467 ,\n",
       "        0.02751142, 0.02502104, 0.01937401, 0.01403264, 0.03298667,\n",
       "        0.02096932, 0.05896675, 0.03834913, 0.04124004, 0.04286313,\n",
       "        0.06703169, 0.04888338, 0.02590129, 0.01712136, 0.0255244 ,\n",
       "        0.01841792, 0.03005654, 0.02595229, 0.03149687, 0.04912008,\n",
       "        0.0543828 , 0.06161181, 0.06183183, 0.04619501, 0.01909338,\n",
       "        0.02016119, 0.01951556, 0.0207938 , 0.0271476 , 0.02098421,\n",
       "        0.02095857, 0.02759717, 0.03184307, 0.02797923, 0.04569698,\n",
       "        0.03215993, 0.04169939, 0.04586908, 0.04717589, 0.02637883,\n",
       "        0.00753836, 0.02378158, 0.01822981, 0.07214031, 0.0179542 ,\n",
       "        0.01494738, 0.04588363, 0.01660223, 0.06530144, 0.0378182 ,\n",
       "        0.0617047 , 0.02714043, 0.07553854, 0.01995599, 0.00467254,\n",
       "        0.04923625, 0.01419818, 0.01269563, 0.06168912, 0.00985487,\n",
       "        0.05931905, 0.08016041, 0.04624169, 0.01383847, 0.06704087,\n",
       "        0.03702403, 0.01237336, 0.05981491, 0.01498618, 0.02042442,\n",
       "        0.05977577, 0.0127947 , 0.04039157, 0.02517989, 0.04319098,\n",
       "        0.02371613, 0.03003458, 0.01517632, 0.01639893, 0.03897383,\n",
       "        0.01475977, 0.0093463 , 0.03040225, 0.01606454, 0.03545201,\n",
       "        0.10933424, 0.02596968, 0.01799207, 0.14317807, 0.02013948,\n",
       "        0.01312798, 0.19651876, 0.01093907, 0.01529781, 0.22737106,\n",
       "        0.01537118, 0.02810489, 0.23396566, 0.03069526, 0.0192425 ,\n",
       "        0.12892575, 0.0139282 , 0.01613991, 0.18312898, 0.01535546,\n",
       "        0.013156  , 0.11166078, 0.01506082, 0.03310379, 0.11969832,\n",
       "        0.03378038, 0.01327432, 0.13139305, 0.00737738, 0.01867038,\n",
       "        0.08659452, 0.01059911, 0.01448248, 0.11473102, 0.01387857,\n",
       "        0.01145837, 0.1573943 , 0.04480949, 0.0151024 , 0.22905975,\n",
       "        0.01570498, 0.01639344, 0.1445417 , 0.01172049, 0.01308147,\n",
       "        0.15321469, 0.01587811, 0.00896056, 0.05962248, 0.03748322,\n",
       "        0.01253513, 0.01114413, 0.01246722, 0.01126722, 0.01443256,\n",
       "        0.0154019 , 0.01782178, 0.01526724, 0.01611922, 0.0140567 ,\n",
       "        0.01396032, 0.02248086, 0.01580037, 0.01278646, 0.01248018,\n",
       "        0.01643519, 0.01611568, 0.01328556, 0.01938908, 0.01777378,\n",
       "        0.01873996, 0.02873031, 0.03322587, 0.02478585, 0.01662252,\n",
       "        0.01896816, 0.01054406, 0.01670479, 0.01474631, 0.0150102 ,\n",
       "        0.0146244 , 0.01565804, 0.0174834 , 0.03875158, 0.03416862,\n",
       "        0.03471815, 0.01276072, 0.01210356, 0.01432516, 0.01392902,\n",
       "        0.01322498, 0.01366652, 0.01279009, 0.01518134, 0.01334898,\n",
       "        0.03434932, 0.01048686, 0.00705511, 0.02051766, 0.019013  ,\n",
       "        0.02503202, 0.0204679 , 0.02000747, 0.02168956, 0.02079039,\n",
       "        0.02018381, 0.02250535, 0.01924321, 0.0352042 , 0.04519967,\n",
       "        0.01888524, 0.01758173, 0.02061221, 0.02019684, 0.02084937,\n",
       "        0.02031284, 0.02330455, 0.02033551, 0.02184633, 0.02243667,\n",
       "        0.03574406, 0.01761689, 0.01791183, 0.01899192, 0.01889233,\n",
       "        0.02243952, 0.01872557, 0.02174437, 0.02087659, 0.02231434,\n",
       "        0.02334201, 0.01631055, 0.01934314, 0.02169969, 0.01779365,\n",
       "        0.02164975, 0.01771985, 0.01993477, 0.01800838, 0.01833443,\n",
       "        0.02032457, 0.01998082, 0.01939174]),\n",
       " 'rank_test_neg_log_loss': array([  2,  56,  44,  40,  91,  12, 134, 119,  92, 183, 154, 167,  24,\n",
       "         11,  43,  46,  62,  47,  85, 112,  95, 159, 187, 178,  35,  70,\n",
       "          8,   6,  53,  26, 105,  73, 116, 140, 132, 142,  23,  15,  30,\n",
       "         19,   1,   3,  34,  10,  42,  31,  54,  60,  20,  38,   7,  14,\n",
       "         27,  29, 143, 136, 161, 195, 200, 202,  32,  36,  18,  64,  80,\n",
       "         16, 164, 127, 133, 203, 198, 182,  37,  68,  22,  25,  33,  28,\n",
       "        118, 128, 145, 204, 190, 188,  21,   9,  17,  13,   5,   4,  86,\n",
       "         90,  75, 109, 137, 144, 249, 281, 252, 244, 272, 247, 237, 268,\n",
       "        233, 225, 276, 232, 255, 277, 254, 243, 271, 241, 236, 287, 240,\n",
       "        226, 274, 227, 256, 285, 253, 245, 282, 246, 239, 273, 234, 229,\n",
       "        269, 230, 250, 280, 251, 242, 279, 248, 238, 267, 235, 231, 275,\n",
       "        228, 219, 283, 218, 199, 260, 201, 179, 264, 177, 160, 262, 163,\n",
       "        209, 286, 215, 196, 284, 191, 180, 263, 175, 172, 257, 171, 223,\n",
       "        265, 222, 193, 278, 192, 181, 259, 184, 170, 258, 165, 224, 288,\n",
       "        221, 197, 270, 194, 186, 266, 185, 176, 261, 168, 217, 207, 206,\n",
       "        130, 146, 149, 121, 111, 108, 102, 107, 101, 220, 214, 212, 125,\n",
       "        139, 129, 110, 114, 120, 106,  99,  96, 213, 205, 211, 141, 148,\n",
       "        138, 113, 115, 117,  97, 104, 103, 210, 208, 216, 152, 157, 158,\n",
       "        126, 135, 131, 124, 123, 122, 173, 147, 169,  93,  83,  81,  65,\n",
       "         79,  57,  45,  51,  55, 162, 174, 189,  89,  87,  82,  61,  72,\n",
       "         52,  58,  39,  50, 153, 166, 155,  94,  78,  84,  63,  76,  67,\n",
       "         41,  49,  48, 156, 150, 151,  88,  98, 100,  74,  59,  77,  66,\n",
       "         71,  69])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRIAL_1_MLP.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL ONE NEG LOG LOSS RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.1,\n",
       " 'classifier__hidden_layer_sizes': (10,),\n",
       " 'classifier__learning_rate': 'invscaling',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL ONE NEG LOG LOSS RESULTS ---------------------------------------------\")\n",
    "TRIAL_1_MLP.cv_results_['rank_test_neg_log_loss']\n",
    "TRIAL_1_MLP.cv_results_['params'][ np.argmin(TRIAL_1_MLP.cv_results_['rank_test_neg_log_loss']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL ONE F1 RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.001,\n",
       " 'classifier__hidden_layer_sizes': (10,),\n",
       " 'classifier__learning_rate': 'invscaling',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL ONE F1 RESULTS ---------------------------------------------\")\n",
    "TRIAL_1_MLP.cv_results_['rank_test_f1_micro']\n",
    "TRIAL_1_MLP.cv_results_['params'][ np.argmin(TRIAL_1_MLP.cv_results_['rank_test_f1_micro']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL ONE ROC AUC RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.0001,\n",
       " 'classifier__hidden_layer_sizes': (5,),\n",
       " 'classifier__learning_rate': 'constant',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL ONE ROC AUC RESULTS ---------------------------------------------\")\n",
    "TRIAL_1_MLP.cv_results_['rank_test_roc_auc_ovo']\n",
    "TRIAL_1_MLP.cv_results_['params'][ np.argmin(TRIAL_1_MLP.cv_results_['rank_test_roc_auc_ovo']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 1 MLP using best NEG LOG LOSS hyperparameters :0.9303790087463557\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 1 MLP using best F1 hyperparameters :0.9259475218658892\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 1 MLP using best ROC_AUC hyperparameters :0.9281632653061225\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best NEG LOG LOSS results to train on test set, \n",
    "p = TRIAL_1_MLP.cv_results_['params'][ np.argmin(TRIAL_1_MLP.cv_results_['rank_test_neg_log_loss']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train__neg_log_loss = pipe.predict(X_train)\n",
    "y_pred_test_neg_log_loss = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 1 MLP using best NEG LOG LOSS hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_neg_log_loss)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best F1 results to train on test set, LR_Trial 1\n",
    "p = TRIAL_1_MLP.cv_results_['params'][ np.argmin(TRIAL_1_MLP.cv_results_['rank_test_f1_micro']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train_roc_auc = pipe.predict(X_train)\n",
    "y_pred_test_f1 = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 1 MLP using best F1 hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_f1)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best ROC_AUC results to train on test set, LR_Trial 1\n",
    "p  = TRIAL_1_MLP.cv_results_['params'][ np.argmin(TRIAL_1_MLP.cv_results_['rank_test_roc_auc_ovo']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train_roc_auc = pipe.predict(X_train)\n",
    "y_pred_test_roc_auc = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 1 MLP using best ROC_AUC hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_roc_auc)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTI LAYERED PERCEPTRON TRIAL TWO ON GAMMA PARTICLES DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    8.8s\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:   10.3s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:   12.3s\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:   14.8s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:   16.6s\n",
      "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed:   18.6s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   22.0s\n",
      "[Parallel(n_jobs=-1)]: Done 189 tasks      | elapsed:   24.3s\n",
      "[Parallel(n_jobs=-1)]: Done 210 tasks      | elapsed:   26.5s\n",
      "[Parallel(n_jobs=-1)]: Done 233 tasks      | elapsed:   30.2s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:   32.5s\n",
      "[Parallel(n_jobs=-1)]: Done 281 tasks      | elapsed:   36.1s\n",
      "[Parallel(n_jobs=-1)]: Done 306 tasks      | elapsed:   39.7s\n",
      "[Parallel(n_jobs=-1)]: Done 333 tasks      | elapsed:   43.6s\n",
      "[Parallel(n_jobs=-1)]: Done 360 tasks      | elapsed:   47.3s\n",
      "[Parallel(n_jobs=-1)]: Done 389 tasks      | elapsed:   50.7s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:   56.0s\n",
      "[Parallel(n_jobs=-1)]: Done 449 tasks      | elapsed:   59.2s\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 513 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 546 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 581 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 616 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 653 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 690 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 729 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 809 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 850 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 893 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 936 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 981 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1026 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1073 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1120 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1169 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1269 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1320 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1373 tasks      | elapsed:  4.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL TWO RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1440 out of 1440 | elapsed:  5.1min finished\n"
     ]
    }
   ],
   "source": [
    "ITERATIONS = 1\n",
    "for i in range(1):\n",
    "    #For reach \"trial\", randomly select 5000 data points\n",
    "    random5000DataPoints = beanData.sample(n = 5000, replace = True)\n",
    "    #From that 5000, create x set and y set\n",
    "    xSet = random5000DataPoints.iloc[:,0:15].values\n",
    "    ySet = random5000DataPoints.iloc[:,16].values\n",
    "    ySet = encodeLabel.fit_transform(ySet)\n",
    "    pipe = Pipeline([('std', StandardScaler()),('classifier', MLPClassifier(max_iter = 250))])\n",
    "\n",
    "    search_space = [\n",
    "                {\n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                {\n",
    "                 \n",
    "                  \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['sgd'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                \n",
    "                {\n",
    "                \n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['adam'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                ]\n",
    "    \n",
    "    #'precision_micro','roc_auc_ovo','jaccard','neg_log_loss'] JACCARD DOES NOT WORK FOR NON BINARY?\n",
    "    runHyperParamSearch = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=5), \n",
    "                          scoring=['recall_micro','f1_micro','roc_auc_ovo','neg_log_loss'], \n",
    "                          refit=False, verbose=10, n_jobs = -1)\n",
    "    TRIAL_2_MLP = runHyperParamSearch.fit(xSet, ySet)\n",
    "    print(\"---------------------------------TRIAL TWO RESULTS ---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([1.42993031, 1.50169201, 1.57945867, 1.63300481, 1.66042938,\n",
       "        1.70086474, 2.03404913, 2.02033625, 2.15004973, 2.46632137,\n",
       "        2.30317879, 2.37163777, 1.48137274, 1.34535675, 1.31793509,\n",
       "        1.63350573, 1.62850099, 1.65102086, 2.14274287, 2.1083128 ,\n",
       "        2.2067986 , 2.53898344, 2.32349911, 2.39996238, 1.5431252 ,\n",
       "        1.4486454 , 1.51970754, 1.72628455, 1.83658013, 1.64411554,\n",
       "        2.30528398, 2.38595214, 2.41817999, 2.517765  , 2.61825113,\n",
       "        2.46872234, 1.44724398, 1.37097783, 1.37688327, 1.59056797,\n",
       "        1.60478039, 1.59717412, 1.97000437, 2.08459377, 2.15565286,\n",
       "        2.48603754, 2.40566778, 2.33260322, 1.53982344, 1.44003782,\n",
       "        1.34375582, 1.74329925, 1.76281652, 1.90814228, 2.26955237,\n",
       "        2.30348129, 2.48924065, 2.92091184, 2.93882556, 2.82633028,\n",
       "        1.6613277 , 1.5859633 , 1.45044847, 1.93146114, 1.8703095 ,\n",
       "        1.91524644, 2.50475492, 2.57521553, 2.50705843, 2.95824409,\n",
       "        2.79019961, 2.76177192, 1.5679472 , 1.40330701, 1.43433347,\n",
       "        1.83407745, 1.80365114, 1.91774969, 2.33030539, 2.4396008 ,\n",
       "        2.59983435, 2.83713994, 2.92171259, 2.91490707, 1.71247253,\n",
       "        1.51820564, 1.42202339, 1.75831261, 1.75731306, 1.75240765,\n",
       "        2.37664471, 2.6271606 , 2.57121167, 2.91160359, 2.7512671 ,\n",
       "        2.71043   , 3.52643213, 0.48271384, 3.3010396 , 3.68526945,\n",
       "        0.88365998, 3.55325589, 3.85651617, 0.96402917, 4.07520504,\n",
       "        4.13375468, 1.62920117, 4.4713449 , 3.62601786, 0.57879796,\n",
       "        3.34517617, 3.65864596, 0.63524618, 3.86772671, 4.04577971,\n",
       "        0.85143223, 4.21532507, 4.2634666 , 2.09780412, 4.06779814,\n",
       "        3.2652082 , 0.39113674, 3.69007363, 3.69357643, 0.63784776,\n",
       "        3.65073986, 4.10993485, 1.16960664, 4.30238314, 4.41169357,\n",
       "        2.33500834, 4.32702084, 3.48759923, 0.42196279, 3.43495378,\n",
       "        4.02476115, 0.51354132, 3.94199038, 4.3293231 , 0.77616615,\n",
       "        4.33853106, 4.83305578, 1.7727241 , 4.70064154, 4.43020873,\n",
       "        4.22733588, 4.43161077, 4.44161983, 4.29449344, 4.21022077,\n",
       "        4.34143362, 4.57173176, 4.55781965, 4.58144016, 4.5636251 ,\n",
       "        4.78301344, 3.86932731, 3.51872587, 3.69858112, 3.93308253,\n",
       "        4.04487891, 3.94529281, 4.24645166, 4.34916363, 4.28808775,\n",
       "        4.62858047, 4.74988446, 4.57823758, 3.62431641, 3.20989633,\n",
       "        3.53133707, 4.04257646, 4.03997455, 4.15857587, 4.45723305,\n",
       "        4.41990132, 4.82414908, 4.82104611, 4.84226413, 4.63058147,\n",
       "        3.88003612, 3.73471169, 3.9372858 , 4.11073503, 4.1906045 ,\n",
       "        3.86192145, 4.30280027, 4.33833084, 4.53700252, 4.58944707,\n",
       "        4.38727341, 4.45623188, 3.84911342, 3.94681535, 3.77235851,\n",
       "        3.95890427, 3.96140709, 3.8803369 , 4.34553704, 4.31360993,\n",
       "        4.41559782, 4.46473994, 4.34883952, 4.32291775, 3.64703703,\n",
       "        3.64893799, 3.83810072, 3.98883004, 4.08511324, 4.15907717,\n",
       "        4.33422899, 4.46854281, 4.37646365, 4.62667899, 4.65089931,\n",
       "        4.64989896, 3.79546404, 3.79856663, 3.82809234, 3.91496677,\n",
       "        4.04988327, 4.17398992, 4.41779895, 4.40158534, 4.43161087,\n",
       "        4.4277081 , 4.60586138, 4.50987821, 3.76493788, 3.7438189 ,\n",
       "        3.59499211, 3.90615916, 3.96210723, 4.04998307, 4.33672929,\n",
       "        4.32882304, 4.36905742, 4.61727133, 4.43161111, 4.40358686,\n",
       "        3.56886883, 3.52713323, 3.60910454, 3.88333998, 4.0002398 ,\n",
       "        4.10853376, 4.42090206, 4.57453399, 4.60165758, 4.79292221,\n",
       "        4.810537  , 4.91012249, 3.82819185, 3.67696209, 3.67756243,\n",
       "        4.03947382, 4.09261875, 4.14676614, 4.49826851, 4.49836917,\n",
       "        4.5545167 , 4.88580141, 4.76930194, 4.86558409, 3.91556764,\n",
       "        3.82589006, 3.84730883, 4.15997763, 4.11714082, 4.16888537,\n",
       "        4.46093698, 4.88470082, 4.74327874, 5.01681418, 4.86098056,\n",
       "        4.66010756, 3.66765389, 3.71079154, 3.7354126 , 3.90816097,\n",
       "        3.9122643 , 3.95199862, 4.31581144, 4.3773644 , 4.62938156,\n",
       "        4.64299331, 4.35274291, 3.27881961]),\n",
       " 'std_fit_time': array([0.19979946, 0.20087419, 0.22180507, 0.09192302, 0.10277186,\n",
       "        0.09476133, 0.0591763 , 0.06209093, 0.14794099, 0.1141912 ,\n",
       "        0.08922503, 0.09119823, 0.12116419, 0.08695649, 0.07659162,\n",
       "        0.05237752, 0.16245652, 0.07416569, 0.15987355, 0.08008602,\n",
       "        0.18647127, 0.20982684, 0.05044365, 0.08550709, 0.11288007,\n",
       "        0.07095835, 0.12449613, 0.13236248, 0.14560419, 0.07726969,\n",
       "        0.15326338, 0.18749222, 0.16559327, 0.26613235, 0.17452193,\n",
       "        0.09846231, 0.07049552, 0.07313875, 0.06493697, 0.0780884 ,\n",
       "        0.0911195 , 0.02778003, 0.07690941, 0.03939028, 0.11514307,\n",
       "        0.11837855, 0.16805692, 0.06721796, 0.14170318, 0.08391684,\n",
       "        0.03865358, 0.04788032, 0.05497041, 0.11992767, 0.05750125,\n",
       "        0.06047842, 0.08198131, 0.16413333, 0.1060205 , 0.19097636,\n",
       "        0.15552121, 0.08514923, 0.11820136, 0.12256896, 0.16752741,\n",
       "        0.0251716 , 0.20178734, 0.18410526, 0.14687139, 0.26774792,\n",
       "        0.12342656, 0.16715694, 0.16862078, 0.04229488, 0.07379859,\n",
       "        0.0993711 , 0.08129161, 0.09065103, 0.14742722, 0.12956442,\n",
       "        0.1756176 , 0.12144082, 0.20776742, 0.19590852, 0.11052635,\n",
       "        0.13668424, 0.02705876, 0.10778925, 0.05279763, 0.07126685,\n",
       "        0.13014387, 0.10407951, 0.29480504, 0.20689303, 0.12743993,\n",
       "        0.03689394, 0.20150372, 0.14074048, 0.11334155, 0.16296345,\n",
       "        0.41319108, 0.06729711, 0.18957526, 0.70393573, 0.20477649,\n",
       "        0.30546783, 0.70741609, 0.43314579, 0.30648721, 0.3090034 ,\n",
       "        0.11236392, 0.21835226, 0.19620304, 0.27542818, 0.31690247,\n",
       "        0.25680441, 0.1700415 , 0.21712151, 0.85552778, 0.22277475,\n",
       "        0.04538071, 0.09862078, 0.16082274, 0.26007738, 0.27402449,\n",
       "        0.22556994, 0.33997579, 0.57858352, 0.40440325, 0.2155102 ,\n",
       "        0.62921215, 0.12545131, 0.21026224, 0.06218186, 0.20583932,\n",
       "        0.19492335, 0.15673201, 0.35185952, 0.27209332, 0.28881723,\n",
       "        0.34332096, 0.24501574, 0.79889163, 0.3023313 , 0.36314911,\n",
       "        0.31303924, 0.17316672, 0.23329198, 0.19112646, 0.22282178,\n",
       "        0.3254793 , 0.27141213, 0.18028891, 0.14649992, 0.24809531,\n",
       "        0.22311656, 0.09911787, 0.36558937, 0.25710518, 0.16037989,\n",
       "        0.0708626 , 0.10587667, 0.13027269, 0.11105336, 0.22331336,\n",
       "        0.15441484, 0.24548197, 0.17081786, 0.09145033, 0.38746081,\n",
       "        0.08980307, 0.15344699, 0.10718612, 0.19287838, 0.26565145,\n",
       "        0.24050505, 0.28752793, 0.48761515, 0.37079452, 0.2770702 ,\n",
       "        0.11809021, 0.66609191, 0.4618814 , 0.25348899, 0.13860532,\n",
       "        0.17393857, 0.29775456, 0.1729131 , 0.33941448, 0.20279646,\n",
       "        0.10799947, 0.18904828, 0.12690158, 0.21532749, 0.31639658,\n",
       "        0.1787599 , 0.09891472, 0.13049754, 0.1026187 , 0.22625747,\n",
       "        0.19478776, 0.14117408, 0.03280257, 0.09680146, 0.06169016,\n",
       "        0.1192723 , 0.18664613, 0.20611605, 0.10177703, 0.18328976,\n",
       "        0.20046797, 0.13945098, 0.25194887, 0.15141313, 0.25619962,\n",
       "        0.09831063, 0.10395233, 0.16302084, 0.17185246, 0.17778593,\n",
       "        0.17008582, 0.2103876 , 0.22210882, 0.14671982, 0.07919763,\n",
       "        0.08483998, 0.06286165, 0.17039244, 0.26703233, 0.1442361 ,\n",
       "        0.09803385, 0.14794193, 0.11512425, 0.11788277, 0.13249363,\n",
       "        0.13977224, 0.23075711, 0.13386378, 0.16465838, 0.15056819,\n",
       "        0.04335324, 0.004424  , 0.10789425, 0.06368108, 0.08739516,\n",
       "        0.15340357, 0.16175233, 0.21510025, 0.10417897, 0.20362679,\n",
       "        0.23923597, 0.18169019, 0.18763045, 0.10334596, 0.09776893,\n",
       "        0.12661827, 0.10609136, 0.21140442, 0.10558833, 0.14597638,\n",
       "        0.05943465, 0.17317089, 0.16776647, 0.21507554, 0.13440902,\n",
       "        0.11039149, 0.16014738, 0.18704017, 0.13921176, 0.18124026,\n",
       "        0.21139609, 0.09950915, 0.24214507, 0.28459186, 0.06778318,\n",
       "        0.07691957, 0.05061697, 0.17770195, 0.11886779, 0.11194809,\n",
       "        0.1302689 , 0.03935133, 0.05688787, 0.06638071, 0.11479391,\n",
       "        0.11415215, 0.15715943, 0.1536285 ]),\n",
       " 'mean_score_time': array([0.05054312, 0.04624019, 0.04824276, 0.04463916, 0.04994183,\n",
       "        0.05574718, 0.05034447, 0.05064549, 0.057551  , 0.05774951,\n",
       "        0.04714141, 0.04373875, 0.04313674, 0.03773241, 0.04573822,\n",
       "        0.04523797, 0.05034318, 0.0459404 , 0.06775851, 0.07256241,\n",
       "        0.05314488, 0.05935087, 0.05644646, 0.06375508, 0.04994292,\n",
       "        0.06045222, 0.043538  , 0.04473853, 0.05835028, 0.05284538,\n",
       "        0.07856641, 0.06345601, 0.06125278, 0.06025057, 0.05344615,\n",
       "        0.04593968, 0.04143581, 0.04613919, 0.04523988, 0.04383807,\n",
       "        0.04794202, 0.04834056, 0.04733086, 0.05154762, 0.05374746,\n",
       "        0.05634832, 0.06665716, 0.05865045, 0.05134459, 0.04644041,\n",
       "        0.04003434, 0.04523926, 0.04974246, 0.05434632, 0.05324512,\n",
       "        0.05544877, 0.0856751 , 0.06015091, 0.05564942, 0.06405482,\n",
       "        0.05334601, 0.05234532, 0.05244451, 0.04724069, 0.04643927,\n",
       "        0.04844112, 0.06015134, 0.05684867, 0.06615639, 0.05845165,\n",
       "        0.05334773, 0.0522449 , 0.03973441, 0.04283619, 0.04313726,\n",
       "        0.05775051, 0.05304646, 0.0551486 , 0.05895052, 0.05864816,\n",
       "        0.06515713, 0.06505651, 0.05474696, 0.04784036, 0.04974236,\n",
       "        0.04224038, 0.04293647, 0.04644074, 0.04653921, 0.04814062,\n",
       "        0.06555638, 0.07186284, 0.06195183, 0.05254531, 0.04804063,\n",
       "        0.04213543, 0.04243579, 0.03953371, 0.04183555, 0.0377326 ,\n",
       "        0.03973408, 0.03893366, 0.04253664, 0.03823285, 0.05334549,\n",
       "        0.04073529, 0.03963423, 0.04093575, 0.04163713, 0.03933334,\n",
       "        0.0411356 , 0.04573941, 0.03933406, 0.05544744, 0.0465404 ,\n",
       "        0.03963399, 0.03933353, 0.04073486, 0.03893309, 0.0410347 ,\n",
       "        0.0424367 , 0.04253669, 0.0476408 , 0.05084414, 0.03512993,\n",
       "        0.04443812, 0.05684862, 0.0426362 , 0.04383779, 0.03903356,\n",
       "        0.0399343 , 0.04694061, 0.04363804, 0.03933372, 0.04263678,\n",
       "        0.04794111, 0.04293647, 0.04323688, 0.05384636, 0.04383802,\n",
       "        0.04944282, 0.05534763, 0.04343762, 0.06615758, 0.05865045,\n",
       "        0.05824933, 0.05054507, 0.04954252, 0.03923383, 0.04844189,\n",
       "        0.0454391 , 0.04674015, 0.04674063, 0.05384679, 0.0463398 ,\n",
       "        0.04093509, 0.0433373 , 0.04533892, 0.04493876, 0.04143591,\n",
       "        0.03903384, 0.03773293, 0.04223661, 0.04231319, 0.04413819,\n",
       "        0.04724097, 0.04453878, 0.04023395, 0.03743229, 0.03869743,\n",
       "        0.0509438 , 0.04193707, 0.04463854, 0.05815005, 0.04293718,\n",
       "        0.05164452, 0.05714912, 0.04684014, 0.04844174, 0.04604006,\n",
       "        0.04013629, 0.04623966, 0.05244503, 0.04173608, 0.05074353,\n",
       "        0.04493899, 0.04664025, 0.04393811, 0.04103503, 0.03933434,\n",
       "        0.03853302, 0.04093542, 0.04213247, 0.04842095, 0.04252243,\n",
       "        0.03693199, 0.04193625, 0.03853283, 0.03793263, 0.04433851,\n",
       "        0.04293661, 0.03773227, 0.03963447, 0.0478415 , 0.04273686,\n",
       "        0.04143567, 0.03953404, 0.04523916, 0.04053459, 0.04003425,\n",
       "        0.04123421, 0.04453893, 0.04133544, 0.04283652, 0.04694057,\n",
       "        0.03933325, 0.03893328, 0.04393845, 0.04654026, 0.04223776,\n",
       "        0.04463859, 0.04053469, 0.04103546, 0.04153557, 0.04343767,\n",
       "        0.03963418, 0.0471405 , 0.03823276, 0.03643126, 0.03523035,\n",
       "        0.04423771, 0.04253678, 0.04213638, 0.04714026, 0.04543953,\n",
       "        0.04023433, 0.03973398, 0.04023457, 0.03813276, 0.04203639,\n",
       "        0.03593097, 0.03653159, 0.04233637, 0.04243646, 0.03893385,\n",
       "        0.04333692, 0.04433813, 0.04103546, 0.04694018, 0.04193621,\n",
       "        0.04393778, 0.04043527, 0.03633142, 0.03983417, 0.0353303 ,\n",
       "        0.03873353, 0.04393835, 0.0389339 , 0.04193635, 0.03843279,\n",
       "        0.04113531, 0.03783288, 0.0461391 , 0.04934292, 0.03963389,\n",
       "        0.04193668, 0.04033465, 0.04543939, 0.0382329 , 0.05844984,\n",
       "        0.04493818, 0.04373775, 0.04403825, 0.04754071, 0.03743186,\n",
       "        0.04063482, 0.03693204, 0.04694004, 0.03633113, 0.03653188,\n",
       "        0.04303708, 0.04083529, 0.04233665, 0.03983459, 0.03983417,\n",
       "        0.03472915, 0.02752428, 0.02392058]),\n",
       " 'std_score_time': array([0.00716193, 0.00636629, 0.00785183, 0.00697903, 0.0096179 ,\n",
       "        0.00671146, 0.00518619, 0.00687845, 0.00874701, 0.01006627,\n",
       "        0.00382963, 0.00309541, 0.00357228, 0.00201613, 0.00307709,\n",
       "        0.00535935, 0.00824588, 0.00403245, 0.02351193, 0.02389574,\n",
       "        0.00428484, 0.00461553, 0.00596516, 0.00476443, 0.00865255,\n",
       "        0.01383975, 0.00461954, 0.00564977, 0.00902239, 0.00756574,\n",
       "        0.02625803, 0.01502257, 0.01291289, 0.02528291, 0.01333558,\n",
       "        0.0070292 , 0.00338536, 0.00290806, 0.00499013, 0.00434642,\n",
       "        0.00553957, 0.0054881 , 0.00263132, 0.00541676, 0.00348904,\n",
       "        0.01332631, 0.03648675, 0.016355  , 0.01026836, 0.00584857,\n",
       "        0.00381108, 0.00705349, 0.01401883, 0.00571104, 0.01000071,\n",
       "        0.00882409, 0.02284384, 0.00808969, 0.02301286, 0.0145206 ,\n",
       "        0.00623958, 0.01347505, 0.01345653, 0.00623115, 0.00459152,\n",
       "        0.00806551, 0.00554901, 0.00899418, 0.01205364, 0.0150198 ,\n",
       "        0.00577101, 0.00934964, 0.00240258, 0.00527226, 0.00583992,\n",
       "        0.00952854, 0.00621767, 0.00358859, 0.01237824, 0.01291951,\n",
       "        0.00782621, 0.01218907, 0.00398555, 0.00699605, 0.01110288,\n",
       "        0.00543165, 0.00570973, 0.00370891, 0.00663034, 0.00700053,\n",
       "        0.01261256, 0.02786314, 0.00853504, 0.00802548, 0.00626491,\n",
       "        0.00529033, 0.00838302, 0.00549979, 0.00543202, 0.00180704,\n",
       "        0.00430039, 0.00560314, 0.00585308, 0.00285887, 0.0071238 ,\n",
       "        0.00260205, 0.00442416, 0.00600035, 0.00381673, 0.00307829,\n",
       "        0.0054951 , 0.00639765, 0.00453834, 0.00556743, 0.00691262,\n",
       "        0.00560304, 0.00462561, 0.00449358, 0.00323412, 0.00439719,\n",
       "        0.00440158, 0.00562629, 0.01212427, 0.0079228 , 0.00086083,\n",
       "        0.01315048, 0.00985026, 0.00562098, 0.00700271, 0.00322763,\n",
       "        0.00527129, 0.01074404, 0.00799147, 0.00415809, 0.00460198,\n",
       "        0.01099305, 0.00739807, 0.00367216, 0.00738635, 0.00369927,\n",
       "        0.0069801 , 0.02389418, 0.00408275, 0.00800323, 0.0117973 ,\n",
       "        0.00758632, 0.01158179, 0.01842191, 0.00186139, 0.01088792,\n",
       "        0.00506779, 0.00605947, 0.0102486 , 0.01861421, 0.00765249,\n",
       "        0.00428651, 0.00661349, 0.00734535, 0.00663529, 0.00305841,\n",
       "        0.00432831, 0.00172179, 0.00611742, 0.00273164, 0.00715716,\n",
       "        0.01047132, 0.00696299, 0.00160171, 0.00139431, 0.00431257,\n",
       "        0.00923493, 0.00694344, 0.00481417, 0.01915136, 0.00430904,\n",
       "        0.00765137, 0.01789667, 0.00477472, 0.00384259, 0.00258993,\n",
       "        0.00398547, 0.00560477, 0.01213254, 0.00231696, 0.01038881,\n",
       "        0.00670181, 0.01024295, 0.00622924, 0.00879407, 0.0021609 ,\n",
       "        0.00187268, 0.00757868, 0.00700851, 0.00872792, 0.01075563,\n",
       "        0.00102049, 0.00433273, 0.00266708, 0.00196143, 0.00637419,\n",
       "        0.0073639 , 0.00160153, 0.00419125, 0.00900556, 0.00537689,\n",
       "        0.00331032, 0.00405322, 0.00780774, 0.00424613, 0.00681024,\n",
       "        0.00492698, 0.00777208, 0.0046792 , 0.00383221, 0.00723354,\n",
       "        0.00546034, 0.00345846, 0.00534745, 0.00677277, 0.00933249,\n",
       "        0.00794078, 0.00406482, 0.00413897, 0.00533412, 0.00388131,\n",
       "        0.002247  , 0.01202872, 0.00147118, 0.00165632, 0.00120946,\n",
       "        0.00761304, 0.01038184, 0.00606662, 0.00310775, 0.00302562,\n",
       "        0.00501051, 0.0037791 , 0.00794754, 0.00237688, 0.00510326,\n",
       "        0.00139364, 0.00284837, 0.0076524 , 0.01132939, 0.00385614,\n",
       "        0.00430095, 0.01033662, 0.00277648, 0.01067398, 0.00386848,\n",
       "        0.00700185, 0.00461281, 0.00169317, 0.00446035, 0.0005105 ,\n",
       "        0.00356115, 0.00440154, 0.00174507, 0.00589061, 0.00384272,\n",
       "        0.00728898, 0.00175076, 0.00769067, 0.00579812, 0.00315556,\n",
       "        0.00446958, 0.00294536, 0.00948092, 0.0019148 , 0.01747752,\n",
       "        0.00681284, 0.00238074, 0.0068468 , 0.00949439, 0.00162604,\n",
       "        0.00490729, 0.00231296, 0.01280674, 0.00242302, 0.00063294,\n",
       "        0.00697048, 0.0049501 , 0.00717274, 0.00269783, 0.00273332,\n",
       "        0.00332865, 0.00509354, 0.00239796]),\n",
       " 'param_classifier': masked_array(data=[MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__activation': masked_array(data=['logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__alpha': masked_array(data=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__hidden_layer_sizes': masked_array(data=[(5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__learning_rate': masked_array(data=['constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'}],\n",
       " 'split0_test_recall_micro': array([0.918, 0.917, 0.911, 0.918, 0.924, 0.917, 0.914, 0.902, 0.925,\n",
       "        0.917, 0.919, 0.921, 0.917, 0.919, 0.919, 0.92 , 0.923, 0.916,\n",
       "        0.914, 0.921, 0.918, 0.916, 0.921, 0.92 , 0.913, 0.913, 0.918,\n",
       "        0.919, 0.915, 0.914, 0.916, 0.912, 0.916, 0.908, 0.913, 0.914,\n",
       "        0.911, 0.92 , 0.916, 0.915, 0.92 , 0.923, 0.915, 0.918, 0.917,\n",
       "        0.916, 0.92 , 0.911, 0.917, 0.91 , 0.913, 0.914, 0.923, 0.917,\n",
       "        0.929, 0.912, 0.92 , 0.927, 0.914, 0.923, 0.916, 0.919, 0.913,\n",
       "        0.906, 0.919, 0.914, 0.921, 0.926, 0.927, 0.908, 0.921, 0.918,\n",
       "        0.914, 0.915, 0.903, 0.926, 0.927, 0.913, 0.924, 0.915, 0.92 ,\n",
       "        0.92 , 0.914, 0.914, 0.917, 0.916, 0.914, 0.924, 0.919, 0.923,\n",
       "        0.925, 0.927, 0.921, 0.925, 0.912, 0.91 , 0.707, 0.154, 0.744,\n",
       "        0.799, 0.116, 0.795, 0.829, 0.167, 0.85 , 0.844, 0.201, 0.863,\n",
       "        0.755, 0.198, 0.743, 0.807, 0.178, 0.821, 0.835, 0.092, 0.836,\n",
       "        0.842, 0.143, 0.863, 0.7  , 0.048, 0.727, 0.789, 0.071, 0.794,\n",
       "        0.838, 0.296, 0.841, 0.864, 0.269, 0.846, 0.746, 0.135, 0.742,\n",
       "        0.8  , 0.145, 0.821, 0.829, 0.269, 0.839, 0.868, 0.103, 0.85 ,\n",
       "        0.87 , 0.173, 0.899, 0.903, 0.323, 0.911, 0.909, 0.442, 0.909,\n",
       "        0.906, 0.147, 0.904, 0.85 , 0.149, 0.861, 0.898, 0.315, 0.904,\n",
       "        0.906, 0.139, 0.904, 0.907, 0.379, 0.904, 0.854, 0.145, 0.897,\n",
       "        0.903, 0.296, 0.902, 0.907, 0.191, 0.901, 0.904, 0.357, 0.904,\n",
       "        0.868, 0.155, 0.904, 0.899, 0.067, 0.9  , 0.907, 0.106, 0.904,\n",
       "        0.906, 0.357, 0.907, 0.865, 0.85 , 0.861, 0.91 , 0.911, 0.911,\n",
       "        0.914, 0.913, 0.917, 0.913, 0.913, 0.915, 0.899, 0.861, 0.856,\n",
       "        0.907, 0.908, 0.911, 0.914, 0.912, 0.914, 0.912, 0.915, 0.912,\n",
       "        0.9  , 0.902, 0.861, 0.913, 0.913, 0.906, 0.911, 0.911, 0.913,\n",
       "        0.911, 0.913, 0.913, 0.872, 0.898, 0.861, 0.912, 0.908, 0.912,\n",
       "        0.914, 0.914, 0.909, 0.912, 0.913, 0.906, 0.904, 0.91 , 0.909,\n",
       "        0.909, 0.908, 0.912, 0.908, 0.91 , 0.91 , 0.913, 0.91 , 0.914,\n",
       "        0.906, 0.909, 0.91 , 0.91 , 0.908, 0.912, 0.911, 0.909, 0.913,\n",
       "        0.913, 0.913, 0.912, 0.905, 0.899, 0.904, 0.909, 0.907, 0.91 ,\n",
       "        0.912, 0.916, 0.912, 0.91 , 0.912, 0.913, 0.914, 0.904, 0.91 ,\n",
       "        0.91 , 0.912, 0.91 , 0.909, 0.914, 0.914, 0.912, 0.911, 0.917]),\n",
       " 'split1_test_recall_micro': array([0.918, 0.917, 0.916, 0.913, 0.909, 0.916, 0.908, 0.91 , 0.914,\n",
       "        0.911, 0.908, 0.914, 0.917, 0.919, 0.92 , 0.917, 0.91 , 0.914,\n",
       "        0.909, 0.913, 0.917, 0.916, 0.908, 0.908, 0.92 , 0.91 , 0.916,\n",
       "        0.906, 0.918, 0.92 , 0.909, 0.912, 0.916, 0.914, 0.907, 0.913,\n",
       "        0.917, 0.914, 0.91 , 0.917, 0.914, 0.909, 0.918, 0.915, 0.918,\n",
       "        0.916, 0.91 , 0.913, 0.916, 0.913, 0.92 , 0.915, 0.911, 0.911,\n",
       "        0.916, 0.906, 0.911, 0.912, 0.915, 0.913, 0.917, 0.922, 0.915,\n",
       "        0.919, 0.91 , 0.915, 0.908, 0.908, 0.917, 0.911, 0.91 , 0.91 ,\n",
       "        0.919, 0.918, 0.919, 0.913, 0.923, 0.911, 0.914, 0.917, 0.919,\n",
       "        0.909, 0.905, 0.911, 0.921, 0.915, 0.917, 0.912, 0.911, 0.912,\n",
       "        0.919, 0.918, 0.911, 0.911, 0.914, 0.913, 0.758, 0.022, 0.782,\n",
       "        0.803, 0.269, 0.807, 0.83 , 0.096, 0.815, 0.828, 0.104, 0.844,\n",
       "        0.734, 0.04 , 0.657, 0.807, 0.14 , 0.804, 0.807, 0.134, 0.817,\n",
       "        0.856, 0.098, 0.834, 0.756, 0.117, 0.762, 0.805, 0.134, 0.77 ,\n",
       "        0.827, 0.096, 0.827, 0.837, 0.183, 0.846, 0.756, 0.307, 0.728,\n",
       "        0.814, 0.265, 0.79 , 0.822, 0.141, 0.828, 0.864, 0.073, 0.864,\n",
       "        0.889, 0.032, 0.878, 0.894, 0.274, 0.894, 0.893, 0.182, 0.902,\n",
       "        0.901, 0.477, 0.898, 0.898, 0.248, 0.87 , 0.893, 0.227, 0.897,\n",
       "        0.898, 0.39 , 0.897, 0.902, 0.364, 0.902, 0.828, 0.158, 0.849,\n",
       "        0.898, 0.147, 0.887, 0.89 , 0.374, 0.901, 0.901, 0.221, 0.898,\n",
       "        0.83 , 0.248, 0.891, 0.893, 0.185, 0.893, 0.897, 0.25 , 0.903,\n",
       "        0.899, 0.398, 0.897, 0.856, 0.861, 0.857, 0.906, 0.897, 0.904,\n",
       "        0.906, 0.907, 0.905, 0.905, 0.907, 0.907, 0.897, 0.854, 0.854,\n",
       "        0.905, 0.907, 0.903, 0.91 , 0.905, 0.903, 0.906, 0.911, 0.909,\n",
       "        0.896, 0.888, 0.9  , 0.904, 0.907, 0.903, 0.9  , 0.908, 0.905,\n",
       "        0.912, 0.906, 0.909, 0.849, 0.891, 0.854, 0.898, 0.9  , 0.903,\n",
       "        0.907, 0.908, 0.904, 0.907, 0.906, 0.906, 0.9  , 0.891, 0.899,\n",
       "        0.909, 0.901, 0.908, 0.909, 0.91 , 0.907, 0.908, 0.907, 0.909,\n",
       "        0.909, 0.902, 0.907, 0.91 , 0.909, 0.91 , 0.911, 0.912, 0.91 ,\n",
       "        0.905, 0.909, 0.91 , 0.903, 0.899, 0.904, 0.906, 0.911, 0.908,\n",
       "        0.911, 0.91 , 0.907, 0.907, 0.911, 0.912, 0.896, 0.893, 0.905,\n",
       "        0.907, 0.908, 0.911, 0.914, 0.907, 0.911, 0.909, 0.913, 0.911]),\n",
       " 'split2_test_recall_micro': array([0.909, 0.91 , 0.912, 0.911, 0.908, 0.913, 0.914, 0.912, 0.92 ,\n",
       "        0.92 , 0.905, 0.91 , 0.911, 0.909, 0.904, 0.903, 0.91 , 0.908,\n",
       "        0.917, 0.9  , 0.913, 0.916, 0.909, 0.903, 0.913, 0.912, 0.91 ,\n",
       "        0.913, 0.911, 0.916, 0.918, 0.912, 0.92 , 0.909, 0.916, 0.905,\n",
       "        0.908, 0.908, 0.906, 0.915, 0.912, 0.912, 0.907, 0.905, 0.914,\n",
       "        0.913, 0.915, 0.912, 0.912, 0.904, 0.91 , 0.908, 0.911, 0.912,\n",
       "        0.912, 0.919, 0.914, 0.916, 0.917, 0.899, 0.91 , 0.907, 0.913,\n",
       "        0.918, 0.916, 0.91 , 0.909, 0.914, 0.91 , 0.908, 0.916, 0.908,\n",
       "        0.907, 0.901, 0.905, 0.919, 0.912, 0.914, 0.914, 0.91 , 0.917,\n",
       "        0.903, 0.906, 0.902, 0.908, 0.915, 0.907, 0.915, 0.912, 0.921,\n",
       "        0.907, 0.919, 0.915, 0.913, 0.917, 0.918, 0.723, 0.322, 0.701,\n",
       "        0.788, 0.268, 0.793, 0.812, 0.198, 0.812, 0.846, 0.2  , 0.83 ,\n",
       "        0.711, 0.27 , 0.689, 0.799, 0.26 , 0.77 , 0.818, 0.308, 0.824,\n",
       "        0.838, 0.097, 0.835, 0.755, 0.199, 0.722, 0.775, 0.14 , 0.794,\n",
       "        0.802, 0.414, 0.813, 0.85 , 0.145, 0.829, 0.708, 0.138, 0.741,\n",
       "        0.784, 0.143, 0.771, 0.803, 0.266, 0.821, 0.835, 0.266, 0.833,\n",
       "        0.883, 0.083, 0.885, 0.893, 0.291, 0.898, 0.897, 0.338, 0.901,\n",
       "        0.9  , 0.464, 0.896, 0.85 , 0.41 , 0.895, 0.886, 0.195, 0.895,\n",
       "        0.897, 0.135, 0.898, 0.896, 0.201, 0.901, 0.86 , 0.274, 0.877,\n",
       "        0.896, 0.311, 0.898, 0.895, 0.28 , 0.899, 0.899, 0.308, 0.896,\n",
       "        0.878, 0.186, 0.85 , 0.894, 0.154, 0.895, 0.902, 0.185, 0.897,\n",
       "        0.897, 0.341, 0.897, 0.861, 0.877, 0.896, 0.9  , 0.9  , 0.899,\n",
       "        0.896, 0.9  , 0.904, 0.901, 0.9  , 0.899, 0.86 , 0.891, 0.865,\n",
       "        0.892, 0.905, 0.898, 0.901, 0.903, 0.899, 0.899, 0.899, 0.901,\n",
       "        0.857, 0.893, 0.856, 0.899, 0.898, 0.902, 0.898, 0.902, 0.904,\n",
       "        0.898, 0.9  , 0.901, 0.861, 0.857, 0.848, 0.902, 0.899, 0.9  ,\n",
       "        0.901, 0.896, 0.899, 0.903, 0.902, 0.9  , 0.899, 0.893, 0.898,\n",
       "        0.904, 0.902, 0.903, 0.905, 0.905, 0.902, 0.899, 0.898, 0.899,\n",
       "        0.897, 0.896, 0.888, 0.901, 0.9  , 0.903, 0.899, 0.903, 0.898,\n",
       "        0.902, 0.901, 0.899, 0.893, 0.898, 0.903, 0.903, 0.9  , 0.896,\n",
       "        0.899, 0.9  , 0.909, 0.903, 0.901, 0.902, 0.9  , 0.89 , 0.892,\n",
       "        0.901, 0.904, 0.904, 0.899, 0.904, 0.898, 0.902, 0.898, 0.898]),\n",
       " 'split3_test_recall_micro': array([0.918, 0.923, 0.917, 0.922, 0.92 , 0.928, 0.924, 0.92 , 0.92 ,\n",
       "        0.918, 0.928, 0.921, 0.924, 0.928, 0.923, 0.922, 0.927, 0.926,\n",
       "        0.923, 0.917, 0.926, 0.929, 0.917, 0.923, 0.924, 0.915, 0.922,\n",
       "        0.926, 0.918, 0.924, 0.921, 0.925, 0.928, 0.925, 0.926, 0.922,\n",
       "        0.921, 0.923, 0.926, 0.929, 0.92 , 0.929, 0.924, 0.924, 0.934,\n",
       "        0.932, 0.924, 0.925, 0.923, 0.917, 0.921, 0.929, 0.914, 0.926,\n",
       "        0.922, 0.928, 0.917, 0.928, 0.915, 0.919, 0.917, 0.923, 0.924,\n",
       "        0.92 , 0.919, 0.929, 0.929, 0.927, 0.927, 0.92 , 0.912, 0.928,\n",
       "        0.92 , 0.924, 0.921, 0.917, 0.924, 0.922, 0.92 , 0.926, 0.919,\n",
       "        0.921, 0.928, 0.92 , 0.925, 0.927, 0.923, 0.925, 0.923, 0.929,\n",
       "        0.92 , 0.926, 0.92 , 0.931, 0.922, 0.918, 0.786, 0.043, 0.745,\n",
       "        0.802, 0.091, 0.807, 0.835, 0.239, 0.815, 0.863, 0.075, 0.823,\n",
       "        0.737, 0.036, 0.684, 0.807, 0.199, 0.807, 0.812, 0.269, 0.838,\n",
       "        0.86 , 0.006, 0.83 , 0.676, 0.269, 0.74 , 0.795, 0.056, 0.805,\n",
       "        0.828, 0.142, 0.815, 0.829, 0.14 , 0.84 , 0.743, 0.207, 0.742,\n",
       "        0.8  , 0.175, 0.809, 0.821, 0.142, 0.827, 0.86 , 0.078, 0.829,\n",
       "        0.866, 0.245, 0.897, 0.902, 0.134, 0.904, 0.916, 0.401, 0.904,\n",
       "        0.913, 0.3  , 0.916, 0.899, 0.097, 0.881, 0.91 , 0.358, 0.908,\n",
       "        0.909, 0.248, 0.907, 0.912, 0.378, 0.912, 0.894, 0.107, 0.865,\n",
       "        0.909, 0.172, 0.909, 0.907, 0.521, 0.912, 0.916, 0.373, 0.913,\n",
       "        0.894, 0.069, 0.844, 0.907, 0.248, 0.894, 0.911, 0.381, 0.907,\n",
       "        0.913, 0.177, 0.919, 0.859, 0.86 , 0.865, 0.907, 0.915, 0.908,\n",
       "        0.917, 0.913, 0.914, 0.913, 0.915, 0.913, 0.864, 0.853, 0.865,\n",
       "        0.916, 0.911, 0.912, 0.911, 0.911, 0.91 , 0.915, 0.913, 0.911,\n",
       "        0.855, 0.9  , 0.896, 0.916, 0.912, 0.916, 0.913, 0.911, 0.916,\n",
       "        0.911, 0.915, 0.911, 0.895, 0.863, 0.858, 0.913, 0.913, 0.911,\n",
       "        0.91 , 0.911, 0.91 , 0.913, 0.91 , 0.915, 0.91 , 0.906, 0.908,\n",
       "        0.91 , 0.913, 0.919, 0.922, 0.918, 0.915, 0.918, 0.921, 0.916,\n",
       "        0.91 , 0.897, 0.906, 0.914, 0.915, 0.911, 0.919, 0.912, 0.913,\n",
       "        0.921, 0.915, 0.916, 0.905, 0.904, 0.907, 0.913, 0.918, 0.914,\n",
       "        0.914, 0.919, 0.916, 0.918, 0.916, 0.915, 0.909, 0.9  , 0.904,\n",
       "        0.911, 0.914, 0.912, 0.918, 0.916, 0.915, 0.919, 0.92 , 0.919]),\n",
       " 'split4_test_recall_micro': array([0.923, 0.919, 0.92 , 0.907, 0.915, 0.916, 0.913, 0.913, 0.915,\n",
       "        0.916, 0.914, 0.908, 0.917, 0.917, 0.917, 0.916, 0.907, 0.916,\n",
       "        0.907, 0.911, 0.915, 0.914, 0.918, 0.913, 0.923, 0.92 , 0.925,\n",
       "        0.913, 0.91 , 0.913, 0.915, 0.918, 0.908, 0.921, 0.913, 0.909,\n",
       "        0.917, 0.92 , 0.914, 0.916, 0.918, 0.909, 0.911, 0.908, 0.914,\n",
       "        0.91 , 0.917, 0.916, 0.922, 0.923, 0.924, 0.918, 0.917, 0.915,\n",
       "        0.912, 0.91 , 0.91 , 0.922, 0.916, 0.919, 0.913, 0.915, 0.917,\n",
       "        0.918, 0.906, 0.913, 0.914, 0.913, 0.915, 0.913, 0.915, 0.908,\n",
       "        0.919, 0.919, 0.925, 0.917, 0.919, 0.918, 0.912, 0.912, 0.904,\n",
       "        0.91 , 0.91 , 0.913, 0.914, 0.916, 0.913, 0.916, 0.915, 0.917,\n",
       "        0.915, 0.914, 0.912, 0.911, 0.909, 0.906, 0.75 , 0.072, 0.722,\n",
       "        0.795, 0.107, 0.797, 0.822, 0.174, 0.816, 0.848, 0.321, 0.831,\n",
       "        0.756, 0.135, 0.754, 0.8  , 0.189, 0.814, 0.833, 0.275, 0.821,\n",
       "        0.824, 0.144, 0.835, 0.711, 0.095, 0.764, 0.8  , 0.199, 0.77 ,\n",
       "        0.807, 0.095, 0.834, 0.838, 0.137, 0.84 , 0.734, 0.225, 0.753,\n",
       "        0.804, 0.144, 0.775, 0.813, 0.299, 0.816, 0.835, 0.274, 0.831,\n",
       "        0.858, 0.234, 0.857, 0.903, 0.402, 0.896, 0.907, 0.13 , 0.909,\n",
       "        0.904, 0.291, 0.91 , 0.863, 0.105, 0.857, 0.908, 0.164, 0.907,\n",
       "        0.905, 0.367, 0.906, 0.908, 0.204, 0.908, 0.802, 0.301, 0.899,\n",
       "        0.898, 0.126, 0.9  , 0.9  , 0.207, 0.907, 0.905, 0.286, 0.902,\n",
       "        0.864, 0.138, 0.863, 0.898, 0.144, 0.894, 0.912, 0.321, 0.899,\n",
       "        0.908, 0.46 , 0.901, 0.86 , 0.863, 0.861, 0.91 , 0.911, 0.908,\n",
       "        0.913, 0.909, 0.91 , 0.913, 0.913, 0.912, 0.868, 0.862, 0.863,\n",
       "        0.907, 0.911, 0.912, 0.912, 0.911, 0.914, 0.907, 0.914, 0.913,\n",
       "        0.903, 0.861, 0.859, 0.908, 0.9  , 0.915, 0.912, 0.916, 0.912,\n",
       "        0.911, 0.915, 0.91 , 0.865, 0.895, 0.861, 0.908, 0.909, 0.907,\n",
       "        0.914, 0.911, 0.913, 0.912, 0.913, 0.915, 0.909, 0.905, 0.904,\n",
       "        0.909, 0.911, 0.909, 0.908, 0.906, 0.911, 0.912, 0.91 , 0.911,\n",
       "        0.871, 0.908, 0.904, 0.912, 0.909, 0.914, 0.912, 0.911, 0.912,\n",
       "        0.914, 0.914, 0.907, 0.907, 0.908, 0.904, 0.91 , 0.911, 0.918,\n",
       "        0.912, 0.913, 0.914, 0.914, 0.909, 0.909, 0.91 , 0.899, 0.907,\n",
       "        0.914, 0.91 , 0.913, 0.91 , 0.907, 0.916, 0.909, 0.907, 0.909]),\n",
       " 'mean_test_recall_micro': array([0.9172, 0.9172, 0.9152, 0.9142, 0.9152, 0.918 , 0.9146, 0.9114,\n",
       "        0.9188, 0.9164, 0.9148, 0.9148, 0.9172, 0.9184, 0.9166, 0.9156,\n",
       "        0.9154, 0.916 , 0.914 , 0.9124, 0.9178, 0.9182, 0.9146, 0.9134,\n",
       "        0.9186, 0.914 , 0.9182, 0.9154, 0.9144, 0.9174, 0.9158, 0.9158,\n",
       "        0.9176, 0.9154, 0.915 , 0.9126, 0.9148, 0.917 , 0.9144, 0.9184,\n",
       "        0.9168, 0.9164, 0.915 , 0.914 , 0.9194, 0.9174, 0.9172, 0.9154,\n",
       "        0.918 , 0.9134, 0.9176, 0.9168, 0.9152, 0.9162, 0.9182, 0.915 ,\n",
       "        0.9144, 0.921 , 0.9154, 0.9146, 0.9146, 0.9172, 0.9164, 0.9162,\n",
       "        0.914 , 0.9162, 0.9162, 0.9176, 0.9192, 0.912 , 0.9148, 0.9144,\n",
       "        0.9158, 0.9154, 0.9146, 0.9184, 0.921 , 0.9156, 0.9168, 0.916 ,\n",
       "        0.9158, 0.9126, 0.9126, 0.912 , 0.917 , 0.9178, 0.9148, 0.9184,\n",
       "        0.916 , 0.9204, 0.9172, 0.9208, 0.9158, 0.9182, 0.9148, 0.913 ,\n",
       "        0.7448, 0.1226, 0.7388, 0.7974, 0.1702, 0.7998, 0.8256, 0.1748,\n",
       "        0.8216, 0.8458, 0.1802, 0.8382, 0.7386, 0.1358, 0.7054, 0.804 ,\n",
       "        0.1932, 0.8032, 0.821 , 0.2156, 0.8272, 0.844 , 0.0976, 0.8394,\n",
       "        0.7196, 0.1456, 0.743 , 0.7928, 0.12  , 0.7866, 0.8204, 0.2086,\n",
       "        0.826 , 0.8436, 0.1748, 0.8402, 0.7374, 0.2024, 0.7412, 0.8004,\n",
       "        0.1744, 0.7932, 0.8176, 0.2234, 0.8262, 0.8524, 0.1588, 0.8414,\n",
       "        0.8732, 0.1534, 0.8832, 0.899 , 0.2848, 0.9006, 0.9044, 0.2986,\n",
       "        0.905 , 0.9048, 0.3358, 0.9048, 0.872 , 0.2018, 0.8728, 0.899 ,\n",
       "        0.2518, 0.9022, 0.903 , 0.2558, 0.9024, 0.905 , 0.3052, 0.9054,\n",
       "        0.8476, 0.197 , 0.8774, 0.9008, 0.2104, 0.8992, 0.8998, 0.3146,\n",
       "        0.904 , 0.905 , 0.309 , 0.9026, 0.8668, 0.1592, 0.8704, 0.8982,\n",
       "        0.1596, 0.8952, 0.9058, 0.2486, 0.902 , 0.9046, 0.3466, 0.9042,\n",
       "        0.8602, 0.8622, 0.868 , 0.9066, 0.9068, 0.906 , 0.9092, 0.9084,\n",
       "        0.91  , 0.909 , 0.9096, 0.9092, 0.8776, 0.8642, 0.8606, 0.9054,\n",
       "        0.9084, 0.9072, 0.9096, 0.9084, 0.908 , 0.9078, 0.9104, 0.9092,\n",
       "        0.8822, 0.8888, 0.8744, 0.908 , 0.906 , 0.9084, 0.9068, 0.9096,\n",
       "        0.91  , 0.9086, 0.9098, 0.9088, 0.8684, 0.8808, 0.8564, 0.9066,\n",
       "        0.9058, 0.9066, 0.9092, 0.908 , 0.907 , 0.9094, 0.9088, 0.9084,\n",
       "        0.9044, 0.901 , 0.9036, 0.9082, 0.907 , 0.9102, 0.9104, 0.9098,\n",
       "        0.909 , 0.91  , 0.9092, 0.9098, 0.8986, 0.9024, 0.903 , 0.9094,\n",
       "        0.9082, 0.91  , 0.9104, 0.9094, 0.9092, 0.911 , 0.9104, 0.9088,\n",
       "        0.9026, 0.9016, 0.9044, 0.9082, 0.9094, 0.9092, 0.9096, 0.9116,\n",
       "        0.9116, 0.9104, 0.9098, 0.9102, 0.9058, 0.8972, 0.9036, 0.9086,\n",
       "        0.9096, 0.91  , 0.91  , 0.9096, 0.9108, 0.9102, 0.9098, 0.9108]),\n",
       " 'std_test_recall_micro': array([0.00453431, 0.00421426, 0.00331059, 0.00526878, 0.00617738,\n",
       "        0.00517687, 0.0052    , 0.00578273, 0.00396989, 0.00300666,\n",
       "        0.00818291, 0.00541849, 0.00411825, 0.0060531 , 0.0065909 ,\n",
       "        0.00665132, 0.00801499, 0.00579655, 0.00572713, 0.00708802,\n",
       "        0.00444522, 0.00545527, 0.0051614 , 0.00739189, 0.00475815,\n",
       "        0.00340588, 0.00515364, 0.00671118, 0.00338231, 0.00407922,\n",
       "        0.00396989, 0.00515364, 0.00649923, 0.00665132, 0.00622896,\n",
       "        0.00567803, 0.00466476, 0.00536656, 0.00674092, 0.00535164,\n",
       "        0.00324962, 0.0081388 , 0.00583095, 0.00684105, 0.00747262,\n",
       "        0.00763151, 0.00470744, 0.00508331, 0.00404969, 0.00640625,\n",
       "        0.00523832, 0.00691086, 0.00448999, 0.00534416, 0.0065238 ,\n",
       "        0.00774597, 0.00372022, 0.00619677, 0.0010198 , 0.00842852,\n",
       "        0.00272764, 0.00581034, 0.00407922, 0.00515364, 0.00517687,\n",
       "        0.00661513, 0.00788416, 0.00755248, 0.00676461, 0.00442719,\n",
       "        0.00376298, 0.00773563, 0.00487442, 0.00776144, 0.00889044,\n",
       "        0.00427083, 0.00517687, 0.00392938, 0.00448999, 0.00554977,\n",
       "        0.00597997, 0.00688767, 0.00833307, 0.00583095, 0.00583095,\n",
       "        0.00462169, 0.00523068, 0.0051614 , 0.00447214, 0.00571314,\n",
       "        0.00601332, 0.0049558 , 0.0040694 , 0.00825591, 0.00444522,\n",
       "        0.00464758, 0.02757825, 0.10932813, 0.02699185, 0.0054626 ,\n",
       "        0.08066077, 0.00601332, 0.00796492, 0.04675639, 0.01426324,\n",
       "        0.01114271, 0.08662886, 0.01413365, 0.01647544, 0.0905724 ,\n",
       "        0.03700054, 0.00368782, 0.03892249, 0.01761136, 0.01118928,\n",
       "        0.08585243, 0.00832827, 0.01296148, 0.05020996, 0.0119432 ,\n",
       "        0.03142356, 0.07871874, 0.01736663, 0.01036147, 0.05164107,\n",
       "        0.01413648, 0.01363231, 0.12637025, 0.01077033, 0.01220819,\n",
       "        0.04993756, 0.00620967, 0.01629233, 0.06350307, 0.00793473,\n",
       "        0.00966644, 0.04686833, 0.0192707 , 0.00889044, 0.06786045,\n",
       "        0.00773046, 0.01443052, 0.09139672, 0.013544  , 0.0113031 ,\n",
       "        0.0836244 , 0.01521052, 0.00451664, 0.08728207, 0.00618385,\n",
       "        0.00833307, 0.12216644, 0.00340588, 0.00462169, 0.12272473,\n",
       "        0.00744043, 0.02215401, 0.11715187, 0.01383329, 0.00903327,\n",
       "        0.07324316, 0.00526878, 0.00469042, 0.10832433, 0.00412795,\n",
       "        0.00551362, 0.08402714, 0.00407922, 0.03101999, 0.07624959,\n",
       "        0.0190326 , 0.00470744, 0.07754379, 0.00713863, 0.00667533,\n",
       "        0.12180411, 0.00481664, 0.00589915, 0.05417379, 0.00591946,\n",
       "        0.02111303, 0.05866311, 0.02332895, 0.0049558 , 0.0588374 ,\n",
       "        0.00248193, 0.0056356 , 0.09711148, 0.00357771, 0.00588558,\n",
       "        0.09423927, 0.00825591, 0.00292575, 0.00865794, 0.01422674,\n",
       "        0.00366606, 0.00699714, 0.00414729, 0.00752064, 0.0048    ,\n",
       "        0.00501996, 0.00505964, 0.00549909, 0.00574108, 0.01685942,\n",
       "        0.0138766 , 0.00467333, 0.00770973, 0.00233238, 0.00570614,\n",
       "        0.00449889, 0.00366606, 0.00603324, 0.00549181, 0.0058515 ,\n",
       "        0.00430813, 0.02151651, 0.01477024, 0.01937627, 0.00609918,\n",
       "        0.00609918, 0.00595315, 0.00643117, 0.00458694, 0.00469042,\n",
       "        0.00531413, 0.0059127 , 0.00411825, 0.01525254, 0.01723253,\n",
       "        0.00492341, 0.00578273, 0.00541849, 0.00458694, 0.00487442,\n",
       "        0.00629285, 0.00493964, 0.00382623, 0.00426146, 0.00581722,\n",
       "        0.00449889, 0.00756307, 0.00449889, 0.00213542, 0.00477493,\n",
       "        0.00526878, 0.00595315, 0.00457821, 0.0043359 , 0.0063561 ,\n",
       "        0.00735935, 0.0059127 , 0.01454098, 0.00538888, 0.00774597,\n",
       "        0.00445421, 0.00479166, 0.00374166, 0.00643739, 0.00338231,\n",
       "        0.00570614, 0.00678233, 0.0051225 , 0.00570614, 0.00496387,\n",
       "        0.00382623, 0.00135647, 0.00342929, 0.00588558, 0.00744043,\n",
       "        0.00538888, 0.00652993, 0.0032619 , 0.00523832, 0.0049558 ,\n",
       "        0.00453431, 0.00670522, 0.00503587, 0.00615142, 0.00440908,\n",
       "        0.00344093, 0.00316228, 0.0063561 , 0.00458694, 0.00661513,\n",
       "        0.00549181, 0.00724983, 0.00738647]),\n",
       " 'rank_test_recall_micro': array([ 26,  26,  63,  82,  61,  17,  73,  98,   7,  38,  69,  67,  26,\n",
       "         11,  37,  53,  58,  45,  83,  93,  19,  13,  73,  87,   8,  83,\n",
       "         13,  55,  78,  24,  48,  48,  22,  58,  64,  90,  69,  32,  78,\n",
       "          9,  36,  39,  64,  83,   5,  24,  26,  55,  18,  87,  21,  34,\n",
       "         61,  41,  13,  64,  78,   2,  55,  73,  73,  26,  39,  41,  83,\n",
       "         41,  41,  22,   6,  95,  69,  78,  48,  58,  73,  11,   1,  53,\n",
       "         34,  45,  48,  90,  92,  94,  32,  19,  69,   9,  45,   4,  26,\n",
       "          3,  52,  13,  67,  89, 249, 286, 252, 245, 279, 244, 236, 277,\n",
       "        237, 226, 275, 232, 253, 285, 256, 241, 274, 242, 238, 268, 233,\n",
       "        227, 288, 231, 255, 284, 250, 247, 287, 248, 239, 270, 235, 228,\n",
       "        276, 230, 254, 271, 251, 243, 278, 246, 240, 267, 234, 224, 282,\n",
       "        229, 212, 283, 206, 199, 263, 196, 178, 262, 174, 175, 258, 175,\n",
       "        214, 272, 213, 200, 265, 191, 185, 264, 189, 172, 261, 170, 225,\n",
       "        273, 210, 195, 269, 198, 197, 259, 182, 172, 260, 187, 218, 281,\n",
       "        215, 202, 280, 204, 167, 266, 192, 177, 257, 181, 222, 220, 217,\n",
       "        162, 160, 165, 131, 145, 111, 138, 121, 131, 209, 219, 221, 170,\n",
       "        145, 157, 121, 145, 153, 156, 105, 131, 207, 205, 211, 153, 165,\n",
       "        145, 161, 121, 111, 143, 118, 142, 216, 208, 223, 162, 167, 164,\n",
       "        131, 153, 158, 130, 140, 145, 178, 194, 183, 150, 158, 107, 102,\n",
       "        116, 139, 110, 131, 118, 201, 189, 185, 127, 150, 111, 102, 127,\n",
       "        131,  99, 105, 140, 187, 193, 178, 150, 127, 131, 121,  96,  96,\n",
       "        102, 116, 107, 167, 203, 183, 143, 121, 111, 111, 121, 100, 107,\n",
       "        118, 100]),\n",
       " 'split0_test_f1_micro': array([0.918, 0.917, 0.911, 0.918, 0.924, 0.917, 0.914, 0.902, 0.925,\n",
       "        0.917, 0.919, 0.921, 0.917, 0.919, 0.919, 0.92 , 0.923, 0.916,\n",
       "        0.914, 0.921, 0.918, 0.916, 0.921, 0.92 , 0.913, 0.913, 0.918,\n",
       "        0.919, 0.915, 0.914, 0.916, 0.912, 0.916, 0.908, 0.913, 0.914,\n",
       "        0.911, 0.92 , 0.916, 0.915, 0.92 , 0.923, 0.915, 0.918, 0.917,\n",
       "        0.916, 0.92 , 0.911, 0.917, 0.91 , 0.913, 0.914, 0.923, 0.917,\n",
       "        0.929, 0.912, 0.92 , 0.927, 0.914, 0.923, 0.916, 0.919, 0.913,\n",
       "        0.906, 0.919, 0.914, 0.921, 0.926, 0.927, 0.908, 0.921, 0.918,\n",
       "        0.914, 0.915, 0.903, 0.926, 0.927, 0.913, 0.924, 0.915, 0.92 ,\n",
       "        0.92 , 0.914, 0.914, 0.917, 0.916, 0.914, 0.924, 0.919, 0.923,\n",
       "        0.925, 0.927, 0.921, 0.925, 0.912, 0.91 , 0.707, 0.154, 0.744,\n",
       "        0.799, 0.116, 0.795, 0.829, 0.167, 0.85 , 0.844, 0.201, 0.863,\n",
       "        0.755, 0.198, 0.743, 0.807, 0.178, 0.821, 0.835, 0.092, 0.836,\n",
       "        0.842, 0.143, 0.863, 0.7  , 0.048, 0.727, 0.789, 0.071, 0.794,\n",
       "        0.838, 0.296, 0.841, 0.864, 0.269, 0.846, 0.746, 0.135, 0.742,\n",
       "        0.8  , 0.145, 0.821, 0.829, 0.269, 0.839, 0.868, 0.103, 0.85 ,\n",
       "        0.87 , 0.173, 0.899, 0.903, 0.323, 0.911, 0.909, 0.442, 0.909,\n",
       "        0.906, 0.147, 0.904, 0.85 , 0.149, 0.861, 0.898, 0.315, 0.904,\n",
       "        0.906, 0.139, 0.904, 0.907, 0.379, 0.904, 0.854, 0.145, 0.897,\n",
       "        0.903, 0.296, 0.902, 0.907, 0.191, 0.901, 0.904, 0.357, 0.904,\n",
       "        0.868, 0.155, 0.904, 0.899, 0.067, 0.9  , 0.907, 0.106, 0.904,\n",
       "        0.906, 0.357, 0.907, 0.865, 0.85 , 0.861, 0.91 , 0.911, 0.911,\n",
       "        0.914, 0.913, 0.917, 0.913, 0.913, 0.915, 0.899, 0.861, 0.856,\n",
       "        0.907, 0.908, 0.911, 0.914, 0.912, 0.914, 0.912, 0.915, 0.912,\n",
       "        0.9  , 0.902, 0.861, 0.913, 0.913, 0.906, 0.911, 0.911, 0.913,\n",
       "        0.911, 0.913, 0.913, 0.872, 0.898, 0.861, 0.912, 0.908, 0.912,\n",
       "        0.914, 0.914, 0.909, 0.912, 0.913, 0.906, 0.904, 0.91 , 0.909,\n",
       "        0.909, 0.908, 0.912, 0.908, 0.91 , 0.91 , 0.913, 0.91 , 0.914,\n",
       "        0.906, 0.909, 0.91 , 0.91 , 0.908, 0.912, 0.911, 0.909, 0.913,\n",
       "        0.913, 0.913, 0.912, 0.905, 0.899, 0.904, 0.909, 0.907, 0.91 ,\n",
       "        0.912, 0.916, 0.912, 0.91 , 0.912, 0.913, 0.914, 0.904, 0.91 ,\n",
       "        0.91 , 0.912, 0.91 , 0.909, 0.914, 0.914, 0.912, 0.911, 0.917]),\n",
       " 'split1_test_f1_micro': array([0.918, 0.917, 0.916, 0.913, 0.909, 0.916, 0.908, 0.91 , 0.914,\n",
       "        0.911, 0.908, 0.914, 0.917, 0.919, 0.92 , 0.917, 0.91 , 0.914,\n",
       "        0.909, 0.913, 0.917, 0.916, 0.908, 0.908, 0.92 , 0.91 , 0.916,\n",
       "        0.906, 0.918, 0.92 , 0.909, 0.912, 0.916, 0.914, 0.907, 0.913,\n",
       "        0.917, 0.914, 0.91 , 0.917, 0.914, 0.909, 0.918, 0.915, 0.918,\n",
       "        0.916, 0.91 , 0.913, 0.916, 0.913, 0.92 , 0.915, 0.911, 0.911,\n",
       "        0.916, 0.906, 0.911, 0.912, 0.915, 0.913, 0.917, 0.922, 0.915,\n",
       "        0.919, 0.91 , 0.915, 0.908, 0.908, 0.917, 0.911, 0.91 , 0.91 ,\n",
       "        0.919, 0.918, 0.919, 0.913, 0.923, 0.911, 0.914, 0.917, 0.919,\n",
       "        0.909, 0.905, 0.911, 0.921, 0.915, 0.917, 0.912, 0.911, 0.912,\n",
       "        0.919, 0.918, 0.911, 0.911, 0.914, 0.913, 0.758, 0.022, 0.782,\n",
       "        0.803, 0.269, 0.807, 0.83 , 0.096, 0.815, 0.828, 0.104, 0.844,\n",
       "        0.734, 0.04 , 0.657, 0.807, 0.14 , 0.804, 0.807, 0.134, 0.817,\n",
       "        0.856, 0.098, 0.834, 0.756, 0.117, 0.762, 0.805, 0.134, 0.77 ,\n",
       "        0.827, 0.096, 0.827, 0.837, 0.183, 0.846, 0.756, 0.307, 0.728,\n",
       "        0.814, 0.265, 0.79 , 0.822, 0.141, 0.828, 0.864, 0.073, 0.864,\n",
       "        0.889, 0.032, 0.878, 0.894, 0.274, 0.894, 0.893, 0.182, 0.902,\n",
       "        0.901, 0.477, 0.898, 0.898, 0.248, 0.87 , 0.893, 0.227, 0.897,\n",
       "        0.898, 0.39 , 0.897, 0.902, 0.364, 0.902, 0.828, 0.158, 0.849,\n",
       "        0.898, 0.147, 0.887, 0.89 , 0.374, 0.901, 0.901, 0.221, 0.898,\n",
       "        0.83 , 0.248, 0.891, 0.893, 0.185, 0.893, 0.897, 0.25 , 0.903,\n",
       "        0.899, 0.398, 0.897, 0.856, 0.861, 0.857, 0.906, 0.897, 0.904,\n",
       "        0.906, 0.907, 0.905, 0.905, 0.907, 0.907, 0.897, 0.854, 0.854,\n",
       "        0.905, 0.907, 0.903, 0.91 , 0.905, 0.903, 0.906, 0.911, 0.909,\n",
       "        0.896, 0.888, 0.9  , 0.904, 0.907, 0.903, 0.9  , 0.908, 0.905,\n",
       "        0.912, 0.906, 0.909, 0.849, 0.891, 0.854, 0.898, 0.9  , 0.903,\n",
       "        0.907, 0.908, 0.904, 0.907, 0.906, 0.906, 0.9  , 0.891, 0.899,\n",
       "        0.909, 0.901, 0.908, 0.909, 0.91 , 0.907, 0.908, 0.907, 0.909,\n",
       "        0.909, 0.902, 0.907, 0.91 , 0.909, 0.91 , 0.911, 0.912, 0.91 ,\n",
       "        0.905, 0.909, 0.91 , 0.903, 0.899, 0.904, 0.906, 0.911, 0.908,\n",
       "        0.911, 0.91 , 0.907, 0.907, 0.911, 0.912, 0.896, 0.893, 0.905,\n",
       "        0.907, 0.908, 0.911, 0.914, 0.907, 0.911, 0.909, 0.913, 0.911]),\n",
       " 'split2_test_f1_micro': array([0.909, 0.91 , 0.912, 0.911, 0.908, 0.913, 0.914, 0.912, 0.92 ,\n",
       "        0.92 , 0.905, 0.91 , 0.911, 0.909, 0.904, 0.903, 0.91 , 0.908,\n",
       "        0.917, 0.9  , 0.913, 0.916, 0.909, 0.903, 0.913, 0.912, 0.91 ,\n",
       "        0.913, 0.911, 0.916, 0.918, 0.912, 0.92 , 0.909, 0.916, 0.905,\n",
       "        0.908, 0.908, 0.906, 0.915, 0.912, 0.912, 0.907, 0.905, 0.914,\n",
       "        0.913, 0.915, 0.912, 0.912, 0.904, 0.91 , 0.908, 0.911, 0.912,\n",
       "        0.912, 0.919, 0.914, 0.916, 0.917, 0.899, 0.91 , 0.907, 0.913,\n",
       "        0.918, 0.916, 0.91 , 0.909, 0.914, 0.91 , 0.908, 0.916, 0.908,\n",
       "        0.907, 0.901, 0.905, 0.919, 0.912, 0.914, 0.914, 0.91 , 0.917,\n",
       "        0.903, 0.906, 0.902, 0.908, 0.915, 0.907, 0.915, 0.912, 0.921,\n",
       "        0.907, 0.919, 0.915, 0.913, 0.917, 0.918, 0.723, 0.322, 0.701,\n",
       "        0.788, 0.268, 0.793, 0.812, 0.198, 0.812, 0.846, 0.2  , 0.83 ,\n",
       "        0.711, 0.27 , 0.689, 0.799, 0.26 , 0.77 , 0.818, 0.308, 0.824,\n",
       "        0.838, 0.097, 0.835, 0.755, 0.199, 0.722, 0.775, 0.14 , 0.794,\n",
       "        0.802, 0.414, 0.813, 0.85 , 0.145, 0.829, 0.708, 0.138, 0.741,\n",
       "        0.784, 0.143, 0.771, 0.803, 0.266, 0.821, 0.835, 0.266, 0.833,\n",
       "        0.883, 0.083, 0.885, 0.893, 0.291, 0.898, 0.897, 0.338, 0.901,\n",
       "        0.9  , 0.464, 0.896, 0.85 , 0.41 , 0.895, 0.886, 0.195, 0.895,\n",
       "        0.897, 0.135, 0.898, 0.896, 0.201, 0.901, 0.86 , 0.274, 0.877,\n",
       "        0.896, 0.311, 0.898, 0.895, 0.28 , 0.899, 0.899, 0.308, 0.896,\n",
       "        0.878, 0.186, 0.85 , 0.894, 0.154, 0.895, 0.902, 0.185, 0.897,\n",
       "        0.897, 0.341, 0.897, 0.861, 0.877, 0.896, 0.9  , 0.9  , 0.899,\n",
       "        0.896, 0.9  , 0.904, 0.901, 0.9  , 0.899, 0.86 , 0.891, 0.865,\n",
       "        0.892, 0.905, 0.898, 0.901, 0.903, 0.899, 0.899, 0.899, 0.901,\n",
       "        0.857, 0.893, 0.856, 0.899, 0.898, 0.902, 0.898, 0.902, 0.904,\n",
       "        0.898, 0.9  , 0.901, 0.861, 0.857, 0.848, 0.902, 0.899, 0.9  ,\n",
       "        0.901, 0.896, 0.899, 0.903, 0.902, 0.9  , 0.899, 0.893, 0.898,\n",
       "        0.904, 0.902, 0.903, 0.905, 0.905, 0.902, 0.899, 0.898, 0.899,\n",
       "        0.897, 0.896, 0.888, 0.901, 0.9  , 0.903, 0.899, 0.903, 0.898,\n",
       "        0.902, 0.901, 0.899, 0.893, 0.898, 0.903, 0.903, 0.9  , 0.896,\n",
       "        0.899, 0.9  , 0.909, 0.903, 0.901, 0.902, 0.9  , 0.89 , 0.892,\n",
       "        0.901, 0.904, 0.904, 0.899, 0.904, 0.898, 0.902, 0.898, 0.898]),\n",
       " 'split3_test_f1_micro': array([0.918, 0.923, 0.917, 0.922, 0.92 , 0.928, 0.924, 0.92 , 0.92 ,\n",
       "        0.918, 0.928, 0.921, 0.924, 0.928, 0.923, 0.922, 0.927, 0.926,\n",
       "        0.923, 0.917, 0.926, 0.929, 0.917, 0.923, 0.924, 0.915, 0.922,\n",
       "        0.926, 0.918, 0.924, 0.921, 0.925, 0.928, 0.925, 0.926, 0.922,\n",
       "        0.921, 0.923, 0.926, 0.929, 0.92 , 0.929, 0.924, 0.924, 0.934,\n",
       "        0.932, 0.924, 0.925, 0.923, 0.917, 0.921, 0.929, 0.914, 0.926,\n",
       "        0.922, 0.928, 0.917, 0.928, 0.915, 0.919, 0.917, 0.923, 0.924,\n",
       "        0.92 , 0.919, 0.929, 0.929, 0.927, 0.927, 0.92 , 0.912, 0.928,\n",
       "        0.92 , 0.924, 0.921, 0.917, 0.924, 0.922, 0.92 , 0.926, 0.919,\n",
       "        0.921, 0.928, 0.92 , 0.925, 0.927, 0.923, 0.925, 0.923, 0.929,\n",
       "        0.92 , 0.926, 0.92 , 0.931, 0.922, 0.918, 0.786, 0.043, 0.745,\n",
       "        0.802, 0.091, 0.807, 0.835, 0.239, 0.815, 0.863, 0.075, 0.823,\n",
       "        0.737, 0.036, 0.684, 0.807, 0.199, 0.807, 0.812, 0.269, 0.838,\n",
       "        0.86 , 0.006, 0.83 , 0.676, 0.269, 0.74 , 0.795, 0.056, 0.805,\n",
       "        0.828, 0.142, 0.815, 0.829, 0.14 , 0.84 , 0.743, 0.207, 0.742,\n",
       "        0.8  , 0.175, 0.809, 0.821, 0.142, 0.827, 0.86 , 0.078, 0.829,\n",
       "        0.866, 0.245, 0.897, 0.902, 0.134, 0.904, 0.916, 0.401, 0.904,\n",
       "        0.913, 0.3  , 0.916, 0.899, 0.097, 0.881, 0.91 , 0.358, 0.908,\n",
       "        0.909, 0.248, 0.907, 0.912, 0.378, 0.912, 0.894, 0.107, 0.865,\n",
       "        0.909, 0.172, 0.909, 0.907, 0.521, 0.912, 0.916, 0.373, 0.913,\n",
       "        0.894, 0.069, 0.844, 0.907, 0.248, 0.894, 0.911, 0.381, 0.907,\n",
       "        0.913, 0.177, 0.919, 0.859, 0.86 , 0.865, 0.907, 0.915, 0.908,\n",
       "        0.917, 0.913, 0.914, 0.913, 0.915, 0.913, 0.864, 0.853, 0.865,\n",
       "        0.916, 0.911, 0.912, 0.911, 0.911, 0.91 , 0.915, 0.913, 0.911,\n",
       "        0.855, 0.9  , 0.896, 0.916, 0.912, 0.916, 0.913, 0.911, 0.916,\n",
       "        0.911, 0.915, 0.911, 0.895, 0.863, 0.858, 0.913, 0.913, 0.911,\n",
       "        0.91 , 0.911, 0.91 , 0.913, 0.91 , 0.915, 0.91 , 0.906, 0.908,\n",
       "        0.91 , 0.913, 0.919, 0.922, 0.918, 0.915, 0.918, 0.921, 0.916,\n",
       "        0.91 , 0.897, 0.906, 0.914, 0.915, 0.911, 0.919, 0.912, 0.913,\n",
       "        0.921, 0.915, 0.916, 0.905, 0.904, 0.907, 0.913, 0.918, 0.914,\n",
       "        0.914, 0.919, 0.916, 0.918, 0.916, 0.915, 0.909, 0.9  , 0.904,\n",
       "        0.911, 0.914, 0.912, 0.918, 0.916, 0.915, 0.919, 0.92 , 0.919]),\n",
       " 'split4_test_f1_micro': array([0.923, 0.919, 0.92 , 0.907, 0.915, 0.916, 0.913, 0.913, 0.915,\n",
       "        0.916, 0.914, 0.908, 0.917, 0.917, 0.917, 0.916, 0.907, 0.916,\n",
       "        0.907, 0.911, 0.915, 0.914, 0.918, 0.913, 0.923, 0.92 , 0.925,\n",
       "        0.913, 0.91 , 0.913, 0.915, 0.918, 0.908, 0.921, 0.913, 0.909,\n",
       "        0.917, 0.92 , 0.914, 0.916, 0.918, 0.909, 0.911, 0.908, 0.914,\n",
       "        0.91 , 0.917, 0.916, 0.922, 0.923, 0.924, 0.918, 0.917, 0.915,\n",
       "        0.912, 0.91 , 0.91 , 0.922, 0.916, 0.919, 0.913, 0.915, 0.917,\n",
       "        0.918, 0.906, 0.913, 0.914, 0.913, 0.915, 0.913, 0.915, 0.908,\n",
       "        0.919, 0.919, 0.925, 0.917, 0.919, 0.918, 0.912, 0.912, 0.904,\n",
       "        0.91 , 0.91 , 0.913, 0.914, 0.916, 0.913, 0.916, 0.915, 0.917,\n",
       "        0.915, 0.914, 0.912, 0.911, 0.909, 0.906, 0.75 , 0.072, 0.722,\n",
       "        0.795, 0.107, 0.797, 0.822, 0.174, 0.816, 0.848, 0.321, 0.831,\n",
       "        0.756, 0.135, 0.754, 0.8  , 0.189, 0.814, 0.833, 0.275, 0.821,\n",
       "        0.824, 0.144, 0.835, 0.711, 0.095, 0.764, 0.8  , 0.199, 0.77 ,\n",
       "        0.807, 0.095, 0.834, 0.838, 0.137, 0.84 , 0.734, 0.225, 0.753,\n",
       "        0.804, 0.144, 0.775, 0.813, 0.299, 0.816, 0.835, 0.274, 0.831,\n",
       "        0.858, 0.234, 0.857, 0.903, 0.402, 0.896, 0.907, 0.13 , 0.909,\n",
       "        0.904, 0.291, 0.91 , 0.863, 0.105, 0.857, 0.908, 0.164, 0.907,\n",
       "        0.905, 0.367, 0.906, 0.908, 0.204, 0.908, 0.802, 0.301, 0.899,\n",
       "        0.898, 0.126, 0.9  , 0.9  , 0.207, 0.907, 0.905, 0.286, 0.902,\n",
       "        0.864, 0.138, 0.863, 0.898, 0.144, 0.894, 0.912, 0.321, 0.899,\n",
       "        0.908, 0.46 , 0.901, 0.86 , 0.863, 0.861, 0.91 , 0.911, 0.908,\n",
       "        0.913, 0.909, 0.91 , 0.913, 0.913, 0.912, 0.868, 0.862, 0.863,\n",
       "        0.907, 0.911, 0.912, 0.912, 0.911, 0.914, 0.907, 0.914, 0.913,\n",
       "        0.903, 0.861, 0.859, 0.908, 0.9  , 0.915, 0.912, 0.916, 0.912,\n",
       "        0.911, 0.915, 0.91 , 0.865, 0.895, 0.861, 0.908, 0.909, 0.907,\n",
       "        0.914, 0.911, 0.913, 0.912, 0.913, 0.915, 0.909, 0.905, 0.904,\n",
       "        0.909, 0.911, 0.909, 0.908, 0.906, 0.911, 0.912, 0.91 , 0.911,\n",
       "        0.871, 0.908, 0.904, 0.912, 0.909, 0.914, 0.912, 0.911, 0.912,\n",
       "        0.914, 0.914, 0.907, 0.907, 0.908, 0.904, 0.91 , 0.911, 0.918,\n",
       "        0.912, 0.913, 0.914, 0.914, 0.909, 0.909, 0.91 , 0.899, 0.907,\n",
       "        0.914, 0.91 , 0.913, 0.91 , 0.907, 0.916, 0.909, 0.907, 0.909]),\n",
       " 'mean_test_f1_micro': array([0.9172, 0.9172, 0.9152, 0.9142, 0.9152, 0.918 , 0.9146, 0.9114,\n",
       "        0.9188, 0.9164, 0.9148, 0.9148, 0.9172, 0.9184, 0.9166, 0.9156,\n",
       "        0.9154, 0.916 , 0.914 , 0.9124, 0.9178, 0.9182, 0.9146, 0.9134,\n",
       "        0.9186, 0.914 , 0.9182, 0.9154, 0.9144, 0.9174, 0.9158, 0.9158,\n",
       "        0.9176, 0.9154, 0.915 , 0.9126, 0.9148, 0.917 , 0.9144, 0.9184,\n",
       "        0.9168, 0.9164, 0.915 , 0.914 , 0.9194, 0.9174, 0.9172, 0.9154,\n",
       "        0.918 , 0.9134, 0.9176, 0.9168, 0.9152, 0.9162, 0.9182, 0.915 ,\n",
       "        0.9144, 0.921 , 0.9154, 0.9146, 0.9146, 0.9172, 0.9164, 0.9162,\n",
       "        0.914 , 0.9162, 0.9162, 0.9176, 0.9192, 0.912 , 0.9148, 0.9144,\n",
       "        0.9158, 0.9154, 0.9146, 0.9184, 0.921 , 0.9156, 0.9168, 0.916 ,\n",
       "        0.9158, 0.9126, 0.9126, 0.912 , 0.917 , 0.9178, 0.9148, 0.9184,\n",
       "        0.916 , 0.9204, 0.9172, 0.9208, 0.9158, 0.9182, 0.9148, 0.913 ,\n",
       "        0.7448, 0.1226, 0.7388, 0.7974, 0.1702, 0.7998, 0.8256, 0.1748,\n",
       "        0.8216, 0.8458, 0.1802, 0.8382, 0.7386, 0.1358, 0.7054, 0.804 ,\n",
       "        0.1932, 0.8032, 0.821 , 0.2156, 0.8272, 0.844 , 0.0976, 0.8394,\n",
       "        0.7196, 0.1456, 0.743 , 0.7928, 0.12  , 0.7866, 0.8204, 0.2086,\n",
       "        0.826 , 0.8436, 0.1748, 0.8402, 0.7374, 0.2024, 0.7412, 0.8004,\n",
       "        0.1744, 0.7932, 0.8176, 0.2234, 0.8262, 0.8524, 0.1588, 0.8414,\n",
       "        0.8732, 0.1534, 0.8832, 0.899 , 0.2848, 0.9006, 0.9044, 0.2986,\n",
       "        0.905 , 0.9048, 0.3358, 0.9048, 0.872 , 0.2018, 0.8728, 0.899 ,\n",
       "        0.2518, 0.9022, 0.903 , 0.2558, 0.9024, 0.905 , 0.3052, 0.9054,\n",
       "        0.8476, 0.197 , 0.8774, 0.9008, 0.2104, 0.8992, 0.8998, 0.3146,\n",
       "        0.904 , 0.905 , 0.309 , 0.9026, 0.8668, 0.1592, 0.8704, 0.8982,\n",
       "        0.1596, 0.8952, 0.9058, 0.2486, 0.902 , 0.9046, 0.3466, 0.9042,\n",
       "        0.8602, 0.8622, 0.868 , 0.9066, 0.9068, 0.906 , 0.9092, 0.9084,\n",
       "        0.91  , 0.909 , 0.9096, 0.9092, 0.8776, 0.8642, 0.8606, 0.9054,\n",
       "        0.9084, 0.9072, 0.9096, 0.9084, 0.908 , 0.9078, 0.9104, 0.9092,\n",
       "        0.8822, 0.8888, 0.8744, 0.908 , 0.906 , 0.9084, 0.9068, 0.9096,\n",
       "        0.91  , 0.9086, 0.9098, 0.9088, 0.8684, 0.8808, 0.8564, 0.9066,\n",
       "        0.9058, 0.9066, 0.9092, 0.908 , 0.907 , 0.9094, 0.9088, 0.9084,\n",
       "        0.9044, 0.901 , 0.9036, 0.9082, 0.907 , 0.9102, 0.9104, 0.9098,\n",
       "        0.909 , 0.91  , 0.9092, 0.9098, 0.8986, 0.9024, 0.903 , 0.9094,\n",
       "        0.9082, 0.91  , 0.9104, 0.9094, 0.9092, 0.911 , 0.9104, 0.9088,\n",
       "        0.9026, 0.9016, 0.9044, 0.9082, 0.9094, 0.9092, 0.9096, 0.9116,\n",
       "        0.9116, 0.9104, 0.9098, 0.9102, 0.9058, 0.8972, 0.9036, 0.9086,\n",
       "        0.9096, 0.91  , 0.91  , 0.9096, 0.9108, 0.9102, 0.9098, 0.9108]),\n",
       " 'std_test_f1_micro': array([0.00453431, 0.00421426, 0.00331059, 0.00526878, 0.00617738,\n",
       "        0.00517687, 0.0052    , 0.00578273, 0.00396989, 0.00300666,\n",
       "        0.00818291, 0.00541849, 0.00411825, 0.0060531 , 0.0065909 ,\n",
       "        0.00665132, 0.00801499, 0.00579655, 0.00572713, 0.00708802,\n",
       "        0.00444522, 0.00545527, 0.0051614 , 0.00739189, 0.00475815,\n",
       "        0.00340588, 0.00515364, 0.00671118, 0.00338231, 0.00407922,\n",
       "        0.00396989, 0.00515364, 0.00649923, 0.00665132, 0.00622896,\n",
       "        0.00567803, 0.00466476, 0.00536656, 0.00674092, 0.00535164,\n",
       "        0.00324962, 0.0081388 , 0.00583095, 0.00684105, 0.00747262,\n",
       "        0.00763151, 0.00470744, 0.00508331, 0.00404969, 0.00640625,\n",
       "        0.00523832, 0.00691086, 0.00448999, 0.00534416, 0.0065238 ,\n",
       "        0.00774597, 0.00372022, 0.00619677, 0.0010198 , 0.00842852,\n",
       "        0.00272764, 0.00581034, 0.00407922, 0.00515364, 0.00517687,\n",
       "        0.00661513, 0.00788416, 0.00755248, 0.00676461, 0.00442719,\n",
       "        0.00376298, 0.00773563, 0.00487442, 0.00776144, 0.00889044,\n",
       "        0.00427083, 0.00517687, 0.00392938, 0.00448999, 0.00554977,\n",
       "        0.00597997, 0.00688767, 0.00833307, 0.00583095, 0.00583095,\n",
       "        0.00462169, 0.00523068, 0.0051614 , 0.00447214, 0.00571314,\n",
       "        0.00601332, 0.0049558 , 0.0040694 , 0.00825591, 0.00444522,\n",
       "        0.00464758, 0.02757825, 0.10932813, 0.02699185, 0.0054626 ,\n",
       "        0.08066077, 0.00601332, 0.00796492, 0.04675639, 0.01426324,\n",
       "        0.01114271, 0.08662886, 0.01413365, 0.01647544, 0.0905724 ,\n",
       "        0.03700054, 0.00368782, 0.03892249, 0.01761136, 0.01118928,\n",
       "        0.08585243, 0.00832827, 0.01296148, 0.05020996, 0.0119432 ,\n",
       "        0.03142356, 0.07871874, 0.01736663, 0.01036147, 0.05164107,\n",
       "        0.01413648, 0.01363231, 0.12637025, 0.01077033, 0.01220819,\n",
       "        0.04993756, 0.00620967, 0.01629233, 0.06350307, 0.00793473,\n",
       "        0.00966644, 0.04686833, 0.0192707 , 0.00889044, 0.06786045,\n",
       "        0.00773046, 0.01443052, 0.09139672, 0.013544  , 0.0113031 ,\n",
       "        0.0836244 , 0.01521052, 0.00451664, 0.08728207, 0.00618385,\n",
       "        0.00833307, 0.12216644, 0.00340588, 0.00462169, 0.12272473,\n",
       "        0.00744043, 0.02215401, 0.11715187, 0.01383329, 0.00903327,\n",
       "        0.07324316, 0.00526878, 0.00469042, 0.10832433, 0.00412795,\n",
       "        0.00551362, 0.08402714, 0.00407922, 0.03101999, 0.07624959,\n",
       "        0.0190326 , 0.00470744, 0.07754379, 0.00713863, 0.00667533,\n",
       "        0.12180411, 0.00481664, 0.00589915, 0.05417379, 0.00591946,\n",
       "        0.02111303, 0.05866311, 0.02332895, 0.0049558 , 0.0588374 ,\n",
       "        0.00248193, 0.0056356 , 0.09711148, 0.00357771, 0.00588558,\n",
       "        0.09423927, 0.00825591, 0.00292575, 0.00865794, 0.01422674,\n",
       "        0.00366606, 0.00699714, 0.00414729, 0.00752064, 0.0048    ,\n",
       "        0.00501996, 0.00505964, 0.00549909, 0.00574108, 0.01685942,\n",
       "        0.0138766 , 0.00467333, 0.00770973, 0.00233238, 0.00570614,\n",
       "        0.00449889, 0.00366606, 0.00603324, 0.00549181, 0.0058515 ,\n",
       "        0.00430813, 0.02151651, 0.01477024, 0.01937627, 0.00609918,\n",
       "        0.00609918, 0.00595315, 0.00643117, 0.00458694, 0.00469042,\n",
       "        0.00531413, 0.0059127 , 0.00411825, 0.01525254, 0.01723253,\n",
       "        0.00492341, 0.00578273, 0.00541849, 0.00458694, 0.00487442,\n",
       "        0.00629285, 0.00493964, 0.00382623, 0.00426146, 0.00581722,\n",
       "        0.00449889, 0.00756307, 0.00449889, 0.00213542, 0.00477493,\n",
       "        0.00526878, 0.00595315, 0.00457821, 0.0043359 , 0.0063561 ,\n",
       "        0.00735935, 0.0059127 , 0.01454098, 0.00538888, 0.00774597,\n",
       "        0.00445421, 0.00479166, 0.00374166, 0.00643739, 0.00338231,\n",
       "        0.00570614, 0.00678233, 0.0051225 , 0.00570614, 0.00496387,\n",
       "        0.00382623, 0.00135647, 0.00342929, 0.00588558, 0.00744043,\n",
       "        0.00538888, 0.00652993, 0.0032619 , 0.00523832, 0.0049558 ,\n",
       "        0.00453431, 0.00670522, 0.00503587, 0.00615142, 0.00440908,\n",
       "        0.00344093, 0.00316228, 0.0063561 , 0.00458694, 0.00661513,\n",
       "        0.00549181, 0.00724983, 0.00738647]),\n",
       " 'rank_test_f1_micro': array([ 26,  26,  63,  82,  61,  17,  73,  98,   7,  38,  68,  68,  26,\n",
       "         11,  37,  53,  58,  45,  83,  93,  19,  13,  73,  87,   8,  83,\n",
       "         13,  55,  78,  24,  48,  48,  21,  58,  64,  90,  68,  32,  78,\n",
       "          9,  36,  40,  64,  83,   5,  24,  26,  55,  18,  87,  21,  34,\n",
       "         61,  41,  13,  64,  78,   2,  55,  73,  73,  26,  38,  41,  83,\n",
       "         41,  44,  21,   6,  95,  68,  78,  48,  58,  73,  11,   1,  53,\n",
       "         34,  45,  48,  91,  91,  94,  32,  19,  68,   9,  45,   4,  26,\n",
       "          3,  52,  13,  67,  89, 249, 286, 252, 245, 279, 244, 236, 277,\n",
       "        237, 226, 275, 232, 253, 285, 256, 241, 274, 242, 238, 268, 233,\n",
       "        227, 288, 231, 255, 284, 250, 247, 287, 248, 239, 270, 235, 228,\n",
       "        276, 230, 254, 271, 251, 243, 278, 246, 240, 267, 234, 224, 282,\n",
       "        229, 212, 283, 206, 199, 263, 196, 178, 262, 174, 175, 258, 175,\n",
       "        214, 272, 213, 200, 265, 191, 185, 264, 189, 172, 261, 170, 225,\n",
       "        273, 210, 195, 269, 198, 197, 259, 182, 172, 260, 187, 218, 281,\n",
       "        215, 202, 280, 204, 167, 266, 192, 177, 257, 181, 222, 220, 217,\n",
       "        162, 160, 165, 131, 145, 111, 138, 121, 131, 209, 219, 221, 170,\n",
       "        146, 157, 121, 146, 153, 156, 102, 131, 207, 205, 211, 153, 165,\n",
       "        146, 160, 121, 111, 143, 116, 142, 216, 208, 223, 162, 167, 164,\n",
       "        131, 153, 158, 127, 140, 146, 178, 194, 183, 150, 158, 107, 102,\n",
       "        116, 139, 110, 131, 120, 201, 189, 185, 127, 150, 111, 102, 127,\n",
       "        131,  99, 106, 140, 188, 193, 178, 150, 127, 131, 121,  96,  96,\n",
       "        102, 116, 107, 167, 203, 183, 143, 121, 111, 111, 121, 100, 107,\n",
       "        116, 100]),\n",
       " 'split0_test_roc_auc_ovo': array([0.99634839, 0.99535972, 0.99552833, 0.99535252, 0.99415433,\n",
       "        0.9958172 , 0.99481507, 0.99552172, 0.99573179, 0.99513525,\n",
       "        0.99460386, 0.99398756, 0.99580134, 0.99624853, 0.99596659,\n",
       "        0.99525602, 0.99629007, 0.99585168, 0.99593099, 0.99565251,\n",
       "        0.99625047, 0.99505294, 0.99556219, 0.99481572, 0.99573489,\n",
       "        0.99535322, 0.99636312, 0.99557899, 0.99640187, 0.99558281,\n",
       "        0.99574688, 0.99618397, 0.9956304 , 0.9945648 , 0.99547353,\n",
       "        0.99415019, 0.99540368, 0.99608058, 0.9955957 , 0.99623414,\n",
       "        0.99534596, 0.99663474, 0.99581405, 0.99637273, 0.99625088,\n",
       "        0.99556872, 0.99536461, 0.99559746, 0.99579843, 0.99570998,\n",
       "        0.99587619, 0.99535361, 0.99633868, 0.99585316, 0.99558565,\n",
       "        0.99508845, 0.99587417, 0.99535832, 0.99425384, 0.99494115,\n",
       "        0.9956384 , 0.99556482, 0.9963744 , 0.99528267, 0.99646527,\n",
       "        0.99599573, 0.99516287, 0.99537823, 0.9958243 , 0.99424372,\n",
       "        0.99539273, 0.99431032, 0.99603749, 0.99579128, 0.99501161,\n",
       "        0.99558888, 0.995991  , 0.99601081, 0.99661148, 0.99533449,\n",
       "        0.99533512, 0.99548906, 0.99543596, 0.99527799, 0.99605476,\n",
       "        0.99612661, 0.99597821, 0.99570146, 0.9957451 , 0.99514335,\n",
       "        0.99594087, 0.99651482, 0.99637522, 0.99552847, 0.99547198,\n",
       "        0.99490657, 0.96160425, 0.49332632, 0.95809239, 0.9796856 ,\n",
       "        0.69890922, 0.97210837, 0.9779162 , 0.69537316, 0.98504187,\n",
       "        0.97941781, 0.4295771 , 0.98434749, 0.94676409, 0.52294198,\n",
       "        0.93475507, 0.9580479 , 0.55124273, 0.96495253, 0.98131146,\n",
       "        0.40815442, 0.97357793, 0.98372533, 0.57371522, 0.98484537,\n",
       "        0.94929836, 0.49993788, 0.93016682, 0.9747932 , 0.5319892 ,\n",
       "        0.95942504, 0.98000171, 0.74111339, 0.97782362, 0.9835937 ,\n",
       "        0.45555902, 0.98264285, 0.94771097, 0.55774946, 0.95838787,\n",
       "        0.96556501, 0.62914311, 0.97968923, 0.97928427, 0.60004254,\n",
       "        0.98012979, 0.98563748, 0.26389549, 0.98649068, 0.98449153,\n",
       "        0.55810051, 0.99238503, 0.99413195, 0.66988922, 0.9944707 ,\n",
       "        0.99420804, 0.72055444, 0.99498925, 0.99461513, 0.77849982,\n",
       "        0.99449748, 0.98719569, 0.72187334, 0.97609624, 0.99281312,\n",
       "        0.45767541, 0.99419626, 0.99449797, 0.53691857, 0.99371475,\n",
       "        0.99455709, 0.8971765 , 0.99467595, 0.98389113, 0.53549531,\n",
       "        0.99290459, 0.99328007, 0.63892162, 0.99350683, 0.99440101,\n",
       "        0.66159402, 0.99448911, 0.99409223, 0.83380152, 0.99416842,\n",
       "        0.98993089, 0.37637328, 0.99118913, 0.99427839, 0.55928325,\n",
       "        0.99368933, 0.9943461 , 0.43023716, 0.99442363, 0.99427361,\n",
       "        0.80730663, 0.99450348, 0.96912472, 0.96801584, 0.97817842,\n",
       "        0.99506443, 0.99423683, 0.99456867, 0.9953442 , 0.99536706,\n",
       "        0.99527183, 0.99546567, 0.99545182, 0.99541249, 0.98933923,\n",
       "        0.97569848, 0.96949334, 0.99387326, 0.99491667, 0.9939487 ,\n",
       "        0.99535353, 0.99535895, 0.99530169, 0.99530661, 0.99524549,\n",
       "        0.99558618, 0.99055653, 0.99299125, 0.96951913, 0.99449945,\n",
       "        0.99442052, 0.99458063, 0.99509496, 0.99520845, 0.99522621,\n",
       "        0.99513818, 0.99536591, 0.99548717, 0.98522017, 0.99115404,\n",
       "        0.98193351, 0.99473698, 0.99489078, 0.99485634, 0.99507233,\n",
       "        0.99535758, 0.99498216, 0.99512074, 0.99554212, 0.9952481 ,\n",
       "        0.99446163, 0.99435593, 0.99511415, 0.99507696, 0.99571774,\n",
       "        0.99590815, 0.99598473, 0.99555535, 0.99560676, 0.99570343,\n",
       "        0.99559452, 0.99604242, 0.99431484, 0.99470601, 0.99415103,\n",
       "        0.99570075, 0.99605148, 0.99576213, 0.99533344, 0.99577937,\n",
       "        0.99626277, 0.99622754, 0.99591401, 0.99620146, 0.99344426,\n",
       "        0.99336809, 0.99416467, 0.99552871, 0.99505008, 0.99548798,\n",
       "        0.99589483, 0.99616939, 0.99627875, 0.99612164, 0.99602192,\n",
       "        0.995991  , 0.99468507, 0.99327284, 0.99398468, 0.99533314,\n",
       "        0.99581409, 0.99550985, 0.99572476, 0.99574997, 0.99569992,\n",
       "        0.99591179, 0.99585125, 0.99629143]),\n",
       " 'split1_test_roc_auc_ovo': array([0.99350864, 0.99390608, 0.99409646, 0.99366202, 0.99431221,\n",
       "        0.99259466, 0.99319326, 0.99327499, 0.99287792, 0.99309423,\n",
       "        0.99351855, 0.99512789, 0.99437576, 0.99418725, 0.99451317,\n",
       "        0.99537702, 0.99351798, 0.99368562, 0.99318449, 0.99336407,\n",
       "        0.99387972, 0.99384033, 0.99324516, 0.99434931, 0.99483549,\n",
       "        0.9938389 , 0.99451043, 0.99436801, 0.99401857, 0.99548286,\n",
       "        0.99418689, 0.99369833, 0.9934985 , 0.99313671, 0.9940495 ,\n",
       "        0.99409058, 0.99334497, 0.99344122, 0.99347919, 0.99369693,\n",
       "        0.99368534, 0.99432982, 0.99464363, 0.99440436, 0.9950056 ,\n",
       "        0.99457968, 0.99451338, 0.99439125, 0.99351893, 0.99402386,\n",
       "        0.99405682, 0.99442143, 0.99355543, 0.99472498, 0.99271291,\n",
       "        0.99397809, 0.99279895, 0.99474226, 0.99404954, 0.99348391,\n",
       "        0.99449524, 0.99405345, 0.99254725, 0.99547157, 0.9943263 ,\n",
       "        0.99416311, 0.99286282, 0.99230125, 0.99495621, 0.9926574 ,\n",
       "        0.99374106, 0.99357134, 0.99442983, 0.99363364, 0.99499306,\n",
       "        0.99390779, 0.99350762, 0.99334066, 0.99352896, 0.99301347,\n",
       "        0.99373584, 0.99343417, 0.99263392, 0.99343424, 0.99446783,\n",
       "        0.99422657, 0.99282249, 0.99337774, 0.99363847, 0.99460529,\n",
       "        0.99441211, 0.99452829, 0.99388402, 0.99301133, 0.99473691,\n",
       "        0.99408676, 0.94120558, 0.50970929, 0.9527858 , 0.96361048,\n",
       "        0.38379973, 0.9669858 , 0.97207508, 0.65139189, 0.96821823,\n",
       "        0.97917444, 0.49821679, 0.97108082, 0.914489  , 0.4005171 ,\n",
       "        0.93620747, 0.9619593 , 0.43878992, 0.97431513, 0.97020506,\n",
       "        0.51647658, 0.97480907, 0.98023975, 0.48528508, 0.97413828,\n",
       "        0.95864127, 0.72326136, 0.95376643, 0.96737135, 0.67617408,\n",
       "        0.97482868, 0.97711948, 0.65295406, 0.97074457, 0.98029707,\n",
       "        0.598023  , 0.97978942, 0.9576403 , 0.546534  , 0.89090894,\n",
       "        0.95751327, 0.41960075, 0.96036473, 0.97354156, 0.55808076,\n",
       "        0.97494304, 0.98258513, 0.41891329, 0.97985899, 0.98534554,\n",
       "        0.5901773 , 0.98555172, 0.99090906, 0.57982258, 0.98957307,\n",
       "        0.99033892, 0.58533286, 0.9914101 , 0.99153678, 0.66138459,\n",
       "        0.99148367, 0.98827042, 0.49167778, 0.98568842, 0.98915039,\n",
       "        0.63126754, 0.98964815, 0.9912887 , 0.60884473, 0.99067927,\n",
       "        0.99079029, 0.76065639, 0.99184105, 0.97581071, 0.60406832,\n",
       "        0.94948627, 0.99120444, 0.41759809, 0.98997624, 0.9905735 ,\n",
       "        0.84370255, 0.99091394, 0.99171132, 0.59904642, 0.99135602,\n",
       "        0.96858194, 0.43243756, 0.98510172, 0.99071314, 0.73843073,\n",
       "        0.98971096, 0.99060326, 0.65009959, 0.99089635, 0.9912514 ,\n",
       "        0.7888854 , 0.99160717, 0.96535744, 0.98170637, 0.97660632,\n",
       "        0.99135731, 0.99128903, 0.99146388, 0.99211285, 0.99219981,\n",
       "        0.99195797, 0.99277713, 0.99250857, 0.99254569, 0.98969787,\n",
       "        0.97518483, 0.9666873 , 0.99134182, 0.9915217 , 0.99138928,\n",
       "        0.99214127, 0.99211064, 0.99246007, 0.99250255, 0.9928911 ,\n",
       "        0.9924048 , 0.98559286, 0.98775755, 0.98761528, 0.9912471 ,\n",
       "        0.99199956, 0.99083447, 0.99163955, 0.99245621, 0.99269393,\n",
       "        0.99232612, 0.99232649, 0.99286161, 0.96323099, 0.98495254,\n",
       "        0.96890079, 0.9903486 , 0.99111572, 0.99115019, 0.99243834,\n",
       "        0.99214789, 0.99147037, 0.99171355, 0.99222459, 0.99213393,\n",
       "        0.99008624, 0.98999991, 0.99085597, 0.99299815, 0.99259302,\n",
       "        0.99321444, 0.9929264 , 0.99328867, 0.99329941, 0.99359479,\n",
       "        0.9933349 , 0.99342208, 0.99133746, 0.99116057, 0.99138749,\n",
       "        0.99302858, 0.99314105, 0.99267671, 0.9932296 , 0.99345092,\n",
       "        0.99313473, 0.99339887, 0.99350837, 0.99343936, 0.99192045,\n",
       "        0.9914905 , 0.99129776, 0.99240165, 0.99319601, 0.99270404,\n",
       "        0.99303576, 0.99334898, 0.99334302, 0.99331614, 0.99328213,\n",
       "        0.99312486, 0.99062744, 0.98994078, 0.99054498, 0.99294566,\n",
       "        0.99284055, 0.99303568, 0.99308894, 0.99282189, 0.99340673,\n",
       "        0.99319328, 0.99317349, 0.99319257]),\n",
       " 'split2_test_roc_auc_ovo': array([0.99491576, 0.994791  , 0.99455345, 0.99525377, 0.99475893,\n",
       "        0.99488341, 0.99484038, 0.99403187, 0.99519195, 0.99486128,\n",
       "        0.99320333, 0.99300911, 0.99443446, 0.99428325, 0.9933975 ,\n",
       "        0.99481867, 0.99512974, 0.99535732, 0.99466666, 0.99517193,\n",
       "        0.99396794, 0.99400879, 0.99476461, 0.99461301, 0.99450173,\n",
       "        0.99437212, 0.9949365 , 0.99533122, 0.99523874, 0.99488223,\n",
       "        0.99509693, 0.99464503, 0.99453562, 0.99431535, 0.9945982 ,\n",
       "        0.99375126, 0.99428162, 0.99442438, 0.99371934, 0.99481305,\n",
       "        0.99464956, 0.99429731, 0.99432138, 0.99468133, 0.99506947,\n",
       "        0.9949185 , 0.99455496, 0.99437998, 0.99410453, 0.9947071 ,\n",
       "        0.99483803, 0.99474792, 0.99455945, 0.99451757, 0.99472874,\n",
       "        0.99501264, 0.99486865, 0.99420158, 0.99435883, 0.99213346,\n",
       "        0.99496136, 0.99486719, 0.99507086, 0.99509672, 0.9949975 ,\n",
       "        0.99520869, 0.99542714, 0.99419232, 0.99418084, 0.99393758,\n",
       "        0.99412542, 0.99311704, 0.99450023, 0.99397785, 0.99498675,\n",
       "        0.99504778, 0.9953027 , 0.99512123, 0.99503666, 0.99382093,\n",
       "        0.99478577, 0.99402106, 0.99341709, 0.99324678, 0.99442067,\n",
       "        0.99469724, 0.994584  , 0.99549141, 0.99507443, 0.99523269,\n",
       "        0.9950442 , 0.99478699, 0.99452466, 0.9945529 , 0.99449955,\n",
       "        0.99467013, 0.93958641, 0.65880618, 0.95558678, 0.95679431,\n",
       "        0.47404794, 0.96535354, 0.97792171, 0.45492221, 0.97557865,\n",
       "        0.98066358, 0.67023258, 0.98123876, 0.91950118, 0.48883221,\n",
       "        0.95271218, 0.96586309, 0.56813185, 0.95851693, 0.97815633,\n",
       "        0.70469601, 0.97427517, 0.98219057, 0.34521907, 0.97781655,\n",
       "        0.95821584, 0.64992942, 0.96490615, 0.94982539, 0.40710569,\n",
       "        0.97219958, 0.97474858, 0.74726848, 0.9779734 , 0.97936457,\n",
       "        0.56489544, 0.98223357, 0.92720412, 0.6571703 , 0.9358815 ,\n",
       "        0.95835294, 0.41922565, 0.97156894, 0.97630494, 0.67919065,\n",
       "        0.97708577, 0.97880756, 0.27427181, 0.97948966, 0.98908833,\n",
       "        0.41583569, 0.98656076, 0.99187179, 0.68256592, 0.99179897,\n",
       "        0.99202762, 0.73170909, 0.99081103, 0.99216066, 0.71846946,\n",
       "        0.99217104, 0.97619076, 0.49166919, 0.98701819, 0.99029842,\n",
       "        0.70597966, 0.99009584, 0.99189942, 0.58641635, 0.99199483,\n",
       "        0.9912376 , 0.63730871, 0.99157928, 0.96689041, 0.56843761,\n",
       "        0.9880992 , 0.99183471, 0.7554324 , 0.99040334, 0.99097261,\n",
       "        0.57981009, 0.99186132, 0.99224792, 0.62176132, 0.99128121,\n",
       "        0.98056008, 0.58265278, 0.96567351, 0.99002374, 0.49261245,\n",
       "        0.9905334 , 0.99078353, 0.66821928, 0.99190942, 0.99147989,\n",
       "        0.582919  , 0.99099736, 0.97876987, 0.98499704, 0.98402609,\n",
       "        0.9908603 , 0.992242  , 0.99170157, 0.99309766, 0.99317693,\n",
       "        0.99279884, 0.99292962, 0.99321803, 0.99321264, 0.9693586 ,\n",
       "        0.98886266, 0.97215454, 0.99287872, 0.9922785 , 0.99209734,\n",
       "        0.99300587, 0.99315905, 0.99268993, 0.99344991, 0.99305605,\n",
       "        0.99346161, 0.9646882 , 0.98902118, 0.97277   , 0.99204833,\n",
       "        0.99168482, 0.99236448, 0.99307154, 0.9932617 , 0.99333096,\n",
       "        0.99316034, 0.99346861, 0.99328493, 0.97018765, 0.96796347,\n",
       "        0.96502749, 0.99277386, 0.99223099, 0.99215848, 0.99298013,\n",
       "        0.99276343, 0.99293566, 0.99287729, 0.99327004, 0.99267351,\n",
       "        0.9906711 , 0.9919158 , 0.99183472, 0.99368601, 0.99273516,\n",
       "        0.9936816 , 0.99387714, 0.99394413, 0.99392939, 0.99398646,\n",
       "        0.9939056 , 0.99433588, 0.99101552, 0.99200603, 0.99088265,\n",
       "        0.9931666 , 0.99313744, 0.99329008, 0.99408615, 0.99411202,\n",
       "        0.99386048, 0.99412168, 0.99411062, 0.99426603, 0.99090647,\n",
       "        0.99099294, 0.99136812, 0.99360262, 0.99357776, 0.9936645 ,\n",
       "        0.99390779, 0.99397661, 0.99403566, 0.99421868, 0.99410306,\n",
       "        0.99432441, 0.99166395, 0.99086412, 0.9898963 , 0.99308225,\n",
       "        0.99343081, 0.99331004, 0.99360054, 0.99369613, 0.99380916,\n",
       "        0.99376409, 0.99410911, 0.9941309 ]),\n",
       " 'split3_test_roc_auc_ovo': array([0.99510052, 0.99582824, 0.99545743, 0.99562306, 0.99580021,\n",
       "        0.9958567 , 0.99440258, 0.99530769, 0.99440991, 0.99384217,\n",
       "        0.99281388, 0.99427994, 0.99568903, 0.99593909, 0.99531645,\n",
       "        0.99541164, 0.99505945, 0.99561753, 0.9952641 , 0.99466746,\n",
       "        0.99497886, 0.9947459 , 0.9951909 , 0.99484151, 0.9956636 ,\n",
       "        0.99513896, 0.99544659, 0.996345  , 0.99565036, 0.99551494,\n",
       "        0.9955537 , 0.99482343, 0.99526808, 0.99451986, 0.99480784,\n",
       "        0.99485497, 0.99535068, 0.99526029, 0.99562544, 0.99578219,\n",
       "        0.99534213, 0.9957563 , 0.99582845, 0.99597001, 0.99583259,\n",
       "        0.99527454, 0.99547382, 0.99530553, 0.99573097, 0.99545161,\n",
       "        0.99523611, 0.99562916, 0.99581541, 0.99546047, 0.99416068,\n",
       "        0.99495254, 0.99501578, 0.99354531, 0.99472207, 0.99466483,\n",
       "        0.99568667, 0.99570644, 0.99551828, 0.99550547, 0.994788  ,\n",
       "        0.99609552, 0.99453699, 0.99564686, 0.99490319, 0.99362434,\n",
       "        0.99367018, 0.99473607, 0.99606686, 0.9955695 , 0.99556158,\n",
       "        0.99531436, 0.9957023 , 0.99473515, 0.99382621, 0.99572856,\n",
       "        0.99527934, 0.99428114, 0.99453038, 0.99515492, 0.99568649,\n",
       "        0.99591181, 0.99573493, 0.99592662, 0.99585689, 0.99567007,\n",
       "        0.99587327, 0.99599787, 0.99531481, 0.99586011, 0.99436663,\n",
       "        0.99534404, 0.95651343, 0.52130882, 0.92728744, 0.96675715,\n",
       "        0.52163846, 0.96332939, 0.97641069, 0.53203525, 0.97218652,\n",
       "        0.9828132 , 0.43193967, 0.97328183, 0.94889929, 0.37468062,\n",
       "        0.93096123, 0.97305764, 0.57330461, 0.97226119, 0.97218364,\n",
       "        0.44356105, 0.97928641, 0.983294  , 0.68008719, 0.9744274 ,\n",
       "        0.9435561 , 0.3969722 , 0.94719902, 0.96852389, 0.43838108,\n",
       "        0.95256111, 0.97558348, 0.53063396, 0.97559496, 0.97869428,\n",
       "        0.61319167, 0.9794337 , 0.92451041, 0.63406428, 0.94560541,\n",
       "        0.96049825, 0.3340536 , 0.95902825, 0.97349432, 0.60212642,\n",
       "        0.97341643, 0.98143078, 0.58575086, 0.97733455, 0.98185426,\n",
       "        0.67334924, 0.98771298, 0.99019492, 0.74254404, 0.99086637,\n",
       "        0.99111709, 0.58936765, 0.9914084 , 0.99278762, 0.67775426,\n",
       "        0.99248219, 0.98909726, 0.46911109, 0.9881794 , 0.99189947,\n",
       "        0.70836238, 0.99104869, 0.99231911, 0.60831996, 0.99241772,\n",
       "        0.99253702, 0.59188974, 0.99259592, 0.98484973, 0.58269925,\n",
       "        0.96905734, 0.99222746, 0.66241553, 0.99213229, 0.99232132,\n",
       "        0.86728525, 0.99213274, 0.99279408, 0.54881227, 0.99209888,\n",
       "        0.9875138 , 0.54119189, 0.97486069, 0.99105883, 0.66044839,\n",
       "        0.98762256, 0.9925195 , 0.58530162, 0.99085392, 0.99266923,\n",
       "        0.58380499, 0.99348858, 0.96656927, 0.96623236, 0.97744526,\n",
       "        0.99335651, 0.99179966, 0.99213865, 0.99427975, 0.99376938,\n",
       "        0.99391494, 0.99385111, 0.99424685, 0.99399977, 0.96850903,\n",
       "        0.96548017, 0.96721586, 0.99257188, 0.99266746, 0.99164862,\n",
       "        0.99362162, 0.9941875 , 0.99322456, 0.99433394, 0.99396702,\n",
       "        0.99409285, 0.96514897, 0.98874439, 0.98757715, 0.99248927,\n",
       "        0.9914097 , 0.99317545, 0.9936917 , 0.99383994, 0.9931334 ,\n",
       "        0.99383478, 0.99420777, 0.99423655, 0.98925883, 0.96698411,\n",
       "        0.97468558, 0.99215455, 0.99295447, 0.99233443, 0.99360002,\n",
       "        0.99327289, 0.99375166, 0.99409077, 0.99381206, 0.99400185,\n",
       "        0.9909954 , 0.99386924, 0.99293539, 0.9941611 , 0.99492838,\n",
       "        0.99457679, 0.9948414 , 0.9946848 , 0.99484376, 0.9947434 ,\n",
       "        0.99477447, 0.99488415, 0.9927819 , 0.99202585, 0.9926467 ,\n",
       "        0.99417219, 0.99460877, 0.99400557, 0.99470815, 0.99510868,\n",
       "        0.99477941, 0.99478975, 0.99493768, 0.99484773, 0.99234884,\n",
       "        0.99280021, 0.9885003 , 0.99432884, 0.99440293, 0.99453047,\n",
       "        0.99461744, 0.99500983, 0.99476857, 0.9951246 , 0.99492625,\n",
       "        0.99500158, 0.99296409, 0.99194249, 0.99330723, 0.99385908,\n",
       "        0.99433719, 0.99427812, 0.99460893, 0.9946549 , 0.9943224 ,\n",
       "        0.99478306, 0.99505429, 0.99488658]),\n",
       " 'split4_test_roc_auc_ovo': array([0.99416295, 0.99287429, 0.99313457, 0.99335487, 0.99378991,\n",
       "        0.99427207, 0.99445519, 0.99404055, 0.99292543, 0.99413565,\n",
       "        0.99277416, 0.99395564, 0.99296125, 0.99419626, 0.99439077,\n",
       "        0.99314734, 0.99442757, 0.99391381, 0.99397174, 0.99356405,\n",
       "        0.99394569, 0.99347957, 0.99378319, 0.99296963, 0.99349458,\n",
       "        0.99393731, 0.99370007, 0.99436579, 0.993829  , 0.99432701,\n",
       "        0.99486194, 0.99412367, 0.99330052, 0.9943678 , 0.99409075,\n",
       "        0.99342994, 0.9937106 , 0.99358229, 0.99341295, 0.99440438,\n",
       "        0.9946696 , 0.99470545, 0.99417577, 0.99350557, 0.9941581 ,\n",
       "        0.99447225, 0.99494893, 0.99443042, 0.99474202, 0.9945912 ,\n",
       "        0.99381389, 0.99443986, 0.99232139, 0.99521575, 0.9940916 ,\n",
       "        0.99358231, 0.99387265, 0.9947995 , 0.99470641, 0.99392429,\n",
       "        0.99401923, 0.99383129, 0.99493731, 0.99509689, 0.99280691,\n",
       "        0.9947331 , 0.99456477, 0.99452428, 0.99510187, 0.99413963,\n",
       "        0.99421403, 0.99246483, 0.99468999, 0.9942063 , 0.99522304,\n",
       "        0.99424662, 0.99409302, 0.9945529 , 0.9943485 , 0.99429312,\n",
       "        0.99384707, 0.99305619, 0.99391592, 0.99269699, 0.9941747 ,\n",
       "        0.99371495, 0.99431146, 0.99527123, 0.99399337, 0.99463566,\n",
       "        0.99483462, 0.99497086, 0.99433313, 0.99407112, 0.99460328,\n",
       "        0.99346636, 0.89980588, 0.20308137, 0.94336832, 0.97202199,\n",
       "        0.46645843, 0.97090487, 0.97951549, 0.50372946, 0.98205165,\n",
       "        0.97928904, 0.74281895, 0.98207028, 0.95642837, 0.54414779,\n",
       "        0.95679064, 0.96754319, 0.64354115, 0.9651526 , 0.96445464,\n",
       "        0.60829141, 0.9773356 , 0.9757058 , 0.70189919, 0.97956796,\n",
       "        0.95595638, 0.25962687, 0.94355153, 0.95951442, 0.41645678,\n",
       "        0.96540763, 0.96922377, 0.60347172, 0.97513526, 0.98138494,\n",
       "        0.40341476, 0.97711079, 0.95787253, 0.75539648, 0.95302328,\n",
       "        0.95780092, 0.4697199 , 0.96666994, 0.97824643, 0.6726509 ,\n",
       "        0.97896815, 0.97992116, 0.52839182, 0.97657612, 0.97507752,\n",
       "        0.51676894, 0.9681835 , 0.99093301, 0.63031575, 0.99055247,\n",
       "        0.99133141, 0.5402173 , 0.99222478, 0.99178693, 0.73974464,\n",
       "        0.99168782, 0.96739976, 0.5714083 , 0.97378809, 0.99128713,\n",
       "        0.56795789, 0.99116604, 0.99147852, 0.70947264, 0.99143179,\n",
       "        0.99152948, 0.62059375, 0.99197057, 0.95305961, 0.64957356,\n",
       "        0.98922076, 0.98981266, 0.56881694, 0.99092265, 0.99190017,\n",
       "        0.58907072, 0.99143604, 0.99199391, 0.68260797, 0.99161048,\n",
       "        0.98391471, 0.56509581, 0.98030421, 0.99054101, 0.35048712,\n",
       "        0.99123128, 0.99129231, 0.61884806, 0.99152674, 0.9920006 ,\n",
       "        0.75351644, 0.99174388, 0.96571876, 0.96566946, 0.97219879,\n",
       "        0.99138976, 0.99237808, 0.99227474, 0.99271059, 0.99206529,\n",
       "        0.99258283, 0.9928709 , 0.99306851, 0.99315944, 0.96647536,\n",
       "        0.95227321, 0.96627222, 0.9920994 , 0.99207153, 0.99183899,\n",
       "        0.99254583, 0.99244266, 0.99225865, 0.99291345, 0.99298295,\n",
       "        0.99294819, 0.98733869, 0.96572224, 0.97755813, 0.99146588,\n",
       "        0.99187659, 0.99266932, 0.99260457, 0.99261927, 0.99259594,\n",
       "        0.99278909, 0.99287447, 0.99281831, 0.97338747, 0.98480621,\n",
       "        0.97964577, 0.9921124 , 0.99228422, 0.99160504, 0.99232682,\n",
       "        0.992224  , 0.99271056, 0.99274229, 0.99300819, 0.99278374,\n",
       "        0.99194782, 0.99060653, 0.99155617, 0.9933811 , 0.99311438,\n",
       "        0.99320665, 0.99377136, 0.99358908, 0.99347906, 0.99381392,\n",
       "        0.993898  , 0.99376873, 0.98662537, 0.99164142, 0.99107485,\n",
       "        0.99250501, 0.99265247, 0.99361574, 0.99314963, 0.99391623,\n",
       "        0.9930689 , 0.99384146, 0.99348229, 0.99397756, 0.98827569,\n",
       "        0.99173255, 0.99002248, 0.99368151, 0.99322871, 0.99326927,\n",
       "        0.99362976, 0.9935032 , 0.99378241, 0.99341913, 0.99370319,\n",
       "        0.99388817, 0.99219453, 0.99148228, 0.99165082, 0.9933639 ,\n",
       "        0.99250173, 0.99303325, 0.99313594, 0.99302974, 0.99322269,\n",
       "        0.99337533, 0.99300608, 0.99382721]),\n",
       " 'mean_test_roc_auc_ovo': array([0.99480725, 0.99455186, 0.99455405, 0.99464925, 0.99456312,\n",
       "        0.99468481, 0.9943413 , 0.99443536, 0.9942274 , 0.99421372,\n",
       "        0.99338276, 0.99407203, 0.99465237, 0.99497087, 0.9947169 ,\n",
       "        0.99480214, 0.99488496, 0.99488519, 0.9946036 , 0.994484  ,\n",
       "        0.99460454, 0.9942255 , 0.99450921, 0.99431783, 0.99484606,\n",
       "        0.9945281 , 0.99499134, 0.9951978 , 0.99502771, 0.99515797,\n",
       "        0.99508927, 0.99469489, 0.99444662, 0.9941809 , 0.99460396,\n",
       "        0.99405539, 0.99441831, 0.99455775, 0.99436652, 0.99498614,\n",
       "        0.99473852, 0.99514473, 0.99495665, 0.9949868 , 0.99526333,\n",
       "        0.99496274, 0.99497114, 0.99482093, 0.99477898, 0.99489675,\n",
       "        0.99476421, 0.9949184 , 0.99451807, 0.99515439, 0.99425592,\n",
       "        0.9945228 , 0.99448604, 0.9945294 , 0.99441814, 0.99382953,\n",
       "        0.99496018, 0.99480464, 0.99488962, 0.99529066, 0.9946768 ,\n",
       "        0.99523923, 0.99451092, 0.99440859, 0.99499328, 0.99372053,\n",
       "        0.99422868, 0.99363992, 0.99514488, 0.99463572, 0.99515521,\n",
       "        0.99482109, 0.99491933, 0.99475215, 0.99467036, 0.99443812,\n",
       "        0.99459663, 0.99405633, 0.99398665, 0.99396218, 0.99496089,\n",
       "        0.99493544, 0.99468622, 0.99515369, 0.99486165, 0.99505741,\n",
       "        0.99522101, 0.99535977, 0.99488637, 0.99460478, 0.99473567,\n",
       "        0.99449477, 0.93974311, 0.47724639, 0.94742415, 0.96777391,\n",
       "        0.50897075, 0.96773639, 0.97676784, 0.56749039, 0.97661538,\n",
       "        0.98027161, 0.55455702, 0.97840384, 0.93721639, 0.46622394,\n",
       "        0.94228532, 0.96529422, 0.55500205, 0.96703968, 0.97326223,\n",
       "        0.53623589, 0.97585684, 0.98103109, 0.55724115, 0.97815911,\n",
       "        0.95313359, 0.50594555, 0.94791799, 0.96400565, 0.49402136,\n",
       "        0.96488441, 0.9753354 , 0.65508832, 0.97545436, 0.98066691,\n",
       "        0.52701678, 0.98024206, 0.94298767, 0.6301829 , 0.9367614 ,\n",
       "        0.95994608, 0.4543486 , 0.96746422, 0.9761743 , 0.62241825,\n",
       "        0.97690864, 0.98167642, 0.41424465, 0.97995   , 0.98317144,\n",
       "        0.55084634, 0.9840788 , 0.99160815, 0.6610275 , 0.99145232,\n",
       "        0.99180462, 0.63343627, 0.99216871, 0.99257742, 0.71517055,\n",
       "        0.99246444, 0.98163078, 0.54914794, 0.98215407, 0.99108971,\n",
       "        0.61424858, 0.991231  , 0.99229674, 0.60999445, 0.99204767,\n",
       "        0.9921303 , 0.70152502, 0.99253255, 0.97290032, 0.58805481,\n",
       "        0.97775363, 0.99167187, 0.60863692, 0.99138827, 0.99203372,\n",
       "        0.70829252, 0.99216663, 0.99256789, 0.6572059 , 0.992103  ,\n",
       "        0.98210028, 0.49955026, 0.97942585, 0.99132302, 0.56025239,\n",
       "        0.9905575 , 0.99190894, 0.59054114, 0.99192201, 0.99233495,\n",
       "        0.70328649, 0.99246809, 0.96910801, 0.97332421, 0.97769098,\n",
       "        0.99240566, 0.99238912, 0.9924295 , 0.99350901, 0.99331569,\n",
       "        0.99330528, 0.99357889, 0.99369876, 0.99366601, 0.97667602,\n",
       "        0.97149987, 0.96836465, 0.99255302, 0.99269117, 0.99218459,\n",
       "        0.99333362, 0.99345176, 0.99318698, 0.99370129, 0.99362852,\n",
       "        0.99369873, 0.97866505, 0.98484732, 0.97900794, 0.99235001,\n",
       "        0.99227824, 0.99272487, 0.99322047, 0.99347711, 0.99339609,\n",
       "        0.9934497 , 0.99364865, 0.99373771, 0.97625702, 0.97917207,\n",
       "        0.97403863, 0.99242528, 0.99269523, 0.99242089, 0.99328353,\n",
       "        0.99315316, 0.99317008, 0.99330893, 0.9935714 , 0.99336823,\n",
       "        0.99163244, 0.99214948, 0.99245928, 0.99386066, 0.99381774,\n",
       "        0.99411753, 0.99428021, 0.9942124 , 0.99423168, 0.9943684 ,\n",
       "        0.9943015 , 0.99449065, 0.99121502, 0.99230798, 0.99202854,\n",
       "        0.99371463, 0.99391824, 0.99387005, 0.99410139, 0.99447344,\n",
       "        0.99422126, 0.99447586, 0.9943906 , 0.99454643, 0.99137914,\n",
       "        0.99207686, 0.99107067, 0.99390867, 0.9938911 , 0.99393125,\n",
       "        0.99421712, 0.9944016 , 0.99444168, 0.99444003, 0.99440731,\n",
       "        0.994466  , 0.99242702, 0.9915005 , 0.9918768 , 0.9937168 ,\n",
       "        0.99378487, 0.99383339, 0.99403182, 0.99399053, 0.99409218,\n",
       "        0.99420551, 0.99423885, 0.99446574]),\n",
       " 'std_test_roc_auc_ovo': array([0.00095649, 0.00105548, 0.00089327, 0.00094429, 0.00069234,\n",
       "        0.00120284, 0.00060138, 0.00084927, 0.0011613 , 0.0007302 ,\n",
       "        0.00066869, 0.00067949, 0.00103718, 0.00092269, 0.00087291,\n",
       "        0.00085401, 0.00091022, 0.00090287, 0.00096076, 0.00089139,\n",
       "        0.00091803, 0.00058433, 0.0008676 , 0.00069684, 0.00082505,\n",
       "        0.00061682, 0.00089339, 0.0007563 , 0.00097733, 0.0004858 ,\n",
       "        0.0005503 , 0.0008433 , 0.00092699, 0.00053021, 0.00052318,\n",
       "        0.00047607, 0.00083807, 0.00100285, 0.00102091, 0.00091888,\n",
       "        0.00060913, 0.00091316, 0.00072201, 0.00105015, 0.00072455,\n",
       "        0.00041321, 0.00039768, 0.00052334, 0.00089325, 0.00061   ,\n",
       "        0.00075738, 0.00048982, 0.00146639, 0.0004849 , 0.00093901,\n",
       "        0.00062061, 0.00105607, 0.00061339, 0.00026148, 0.00099383,\n",
       "        0.00064642, 0.00076246, 0.00127449, 0.00017555, 0.00117755,\n",
       "        0.00073765, 0.00089267, 0.00118106, 0.00052382, 0.00057201,\n",
       "        0.00061896, 0.00081362, 0.00074573, 0.00087506, 0.00022137,\n",
       "        0.00064005, 0.00095752, 0.00086634, 0.00109737, 0.00098998,\n",
       "        0.00068557, 0.00083569, 0.00095451, 0.00105311, 0.00075843,\n",
       "        0.00094032, 0.00113079, 0.00091426, 0.00090182, 0.00039893,\n",
       "        0.00059641, 0.00076315, 0.00087657, 0.00102524, 0.00038772,\n",
       "        0.00065509, 0.02170852, 0.14925436, 0.01123498, 0.00773116,\n",
       "        0.10481582, 0.00331109, 0.00254359, 0.09098086, 0.00619291,\n",
       "        0.00137924, 0.12857108, 0.00522816, 0.01689445, 0.06707871,\n",
       "        0.01040196, 0.00508494, 0.06613008, 0.00567011, 0.00594686,\n",
       "        0.10858612, 0.00213268, 0.00292264, 0.13146827, 0.00392176,\n",
       "        0.00584071, 0.16765541, 0.01146725, 0.00859493, 0.10127942,\n",
       "        0.00817731, 0.00354162, 0.08252904, 0.00261774, 0.0017208 ,\n",
       "        0.08280877, 0.00201956, 0.01448515, 0.07571733, 0.02413956,\n",
       "        0.00299786, 0.09767427, 0.00759604, 0.0023705 , 0.04647186,\n",
       "        0.00247822, 0.00236167, 0.13012534, 0.00349927, 0.00466316,\n",
       "        0.08484769, 0.00828481, 0.00136973, 0.05426754, 0.00166826,\n",
       "        0.00131689, 0.07770828, 0.00148023, 0.00110254, 0.04223333,\n",
       "        0.00107561, 0.00851959, 0.09310436, 0.0059857 , 0.00126901,\n",
       "        0.09404215, 0.00158897, 0.00115667, 0.05620685, 0.00101693,\n",
       "        0.00134226, 0.11355133, 0.00112258, 0.01184761, 0.03799212,\n",
       "        0.01638747, 0.00114862, 0.11264074, 0.00128239, 0.00133887,\n",
       "        0.12370682, 0.00123217, 0.00084146, 0.09820316, 0.00107162,\n",
       "        0.0074704 , 0.08076256, 0.00873303, 0.00151497, 0.13443536,\n",
       "        0.00197957, 0.00139026, 0.08497906, 0.00131204, 0.00108509,\n",
       "        0.09943297, 0.00131247, 0.00500701, 0.00828956, 0.00378873,\n",
       "        0.00158066, 0.00099935, 0.00110877, 0.00115913, 0.00120349,\n",
       "        0.00116899, 0.00101977, 0.00104101, 0.00098781, 0.01052828,\n",
       "        0.01215381, 0.00219767, 0.00083932, 0.00117263, 0.00091202,\n",
       "        0.00112363, 0.00119006, 0.0011055 , 0.00100977, 0.00089638,\n",
       "        0.0010965 , 0.01133729, 0.009728  , 0.00746417, 0.00116005,\n",
       "        0.00108948, 0.00121272, 0.00115209, 0.00099536, 0.00095455,\n",
       "        0.00097794, 0.00106221, 0.00101265, 0.00963467, 0.00982756,\n",
       "        0.00635273, 0.00141062, 0.00124635, 0.00128748, 0.00100216,\n",
       "        0.0011747 , 0.00116422, 0.00117869, 0.00111027, 0.001121  ,\n",
       "        0.00153783, 0.00172522, 0.00148669, 0.0007175 , 0.00126571,\n",
       "        0.00102494, 0.00104638, 0.00081737, 0.00087091, 0.00077133,\n",
       "        0.00079366, 0.00092193, 0.00257618, 0.00123953, 0.00122643,\n",
       "        0.00113033, 0.00125247, 0.00104148, 0.00084287, 0.00084801,\n",
       "        0.00119314, 0.00098536, 0.00092756, 0.00094422, 0.0017523 ,\n",
       "        0.00087496, 0.00186646, 0.00102151, 0.00072461, 0.00097958,\n",
       "        0.00098094, 0.00105743, 0.00102842, 0.00106309, 0.00097269,\n",
       "        0.00097623, 0.00136103, 0.00111127, 0.00156438, 0.00086656,\n",
       "        0.00119049, 0.00095486, 0.00100764, 0.00108743, 0.00088812,\n",
       "        0.00101524, 0.00108962, 0.00106311]),\n",
       " 'rank_test_roc_auc_ovo': array([ 38,  63,  62,  53,  60,  49,  92,  83, 100, 104, 146, 111,  52,\n",
       "         21,  46,  40,  33,  32,  58,  74,  56, 101,  70,  93,  35,  66,\n",
       "         17,   6,  15,   7,  13,  47,  79, 107,  57, 113,  84,  61,  91,\n",
       "         19,  44,  12,  25,  18,   3,  22,  20,  37,  41,  29,  42,  28,\n",
       "         68,   9,  96,  67,  73,  65,  85, 125,  24,  39,  30,   2,  50,\n",
       "          4,  69,  86,  16, 129,  99, 137,  11,  54,   8,  36,  27,  43,\n",
       "         51,  82,  59, 112, 116, 117,  23,  26,  48,  10,  34,  14,   5,\n",
       "          1,  31,  55,  45,  71, 254, 285, 251, 241, 281, 242, 226, 272,\n",
       "        228, 214, 276, 221, 255, 286, 253, 245, 275, 244, 236, 279, 231,\n",
       "        212, 274, 222, 249, 282, 250, 247, 284, 246, 233, 263, 232, 213,\n",
       "        280, 215, 252, 265, 256, 248, 287, 243, 230, 266, 225, 210, 288,\n",
       "        216, 207, 277, 206, 194, 261, 196, 191, 264, 179, 160, 257, 165,\n",
       "        211, 278, 208, 202, 267, 200, 176, 268, 185, 182, 260, 163, 237,\n",
       "        271, 223, 192, 269, 197, 186, 258, 180, 161, 262, 183, 209, 283,\n",
       "        217, 199, 273, 204, 189, 270, 188, 174, 259, 164, 239, 235, 224,\n",
       "        171, 172, 167, 141, 149, 151, 139, 133, 135, 227, 238, 240, 162,\n",
       "        159, 178, 148, 143, 154, 132, 138, 134, 220, 205, 219, 173, 177,\n",
       "        157, 153, 142, 145, 144, 136, 128, 229, 218, 234, 169, 158, 170,\n",
       "        152, 156, 155, 150, 140, 147, 193, 181, 166, 123, 126, 108,  95,\n",
       "        105,  98,  90,  94,  72, 201, 175, 187, 131, 119, 122, 109,  76,\n",
       "        102,  75,  89,  64, 198, 184, 203, 120, 121, 118, 103,  88,  80,\n",
       "         81,  87,  77, 168, 195, 190, 130, 127, 124, 114, 115, 110, 106,\n",
       "         97,  78]),\n",
       " 'split0_test_neg_log_loss': array([-0.21154054, -0.22557797, -0.22900381, -0.22812738, -0.23457645,\n",
       "        -0.20607701, -0.25664744, -0.24550917, -0.22078682, -0.24069693,\n",
       "        -0.26023485, -0.27018723, -0.22359636, -0.20438865, -0.21705336,\n",
       "        -0.22437729, -0.2093454 , -0.21921687, -0.22118347, -0.21542   ,\n",
       "        -0.20550172, -0.23907203, -0.23482691, -0.26171685, -0.221556  ,\n",
       "        -0.2179468 , -0.21371841, -0.21934713, -0.20468008, -0.22137003,\n",
       "        -0.2278467 , -0.21234473, -0.22194237, -0.2603953 , -0.24335014,\n",
       "        -0.25574826, -0.22291828, -0.21342558, -0.21899585, -0.21168907,\n",
       "        -0.21724095, -0.19933606, -0.20929003, -0.20473076, -0.20658475,\n",
       "        -0.21765461, -0.22321434, -0.22539207, -0.22407303, -0.22165813,\n",
       "        -0.22316588, -0.22972383, -0.2062933 , -0.22696536, -0.23024615,\n",
       "        -0.23363589, -0.22944067, -0.24322612, -0.30180775, -0.27647127,\n",
       "        -0.22025332, -0.20941315, -0.21364004, -0.22303558, -0.20514343,\n",
       "        -0.22135713, -0.24296946, -0.23393851, -0.24118301, -0.31254186,\n",
       "        -0.25399427, -0.31069515, -0.21235538, -0.22265285, -0.23260973,\n",
       "        -0.21670913, -0.20896714, -0.21759102, -0.2030698 , -0.23874407,\n",
       "        -0.23375707, -0.25158983, -0.24677847, -0.24730194, -0.20855044,\n",
       "        -0.213462  , -0.20850745, -0.21150938, -0.21608336, -0.22084732,\n",
       "        -0.2187478 , -0.20643776, -0.20768293, -0.22183038, -0.22962521,\n",
       "        -0.25218063, -0.93053267, -1.89120188, -0.86324024, -0.69978289,\n",
       "        -1.93173132, -0.66794682, -0.60196499, -1.88604129, -0.57197189,\n",
       "        -0.55003892, -1.89783953, -0.55202872, -0.91543015, -1.99432143,\n",
       "        -0.77731294, -0.69322888, -1.96234404, -0.69494219, -0.58039022,\n",
       "        -1.91777088, -0.60248071, -0.57384377, -1.88342526, -0.55655965,\n",
       "        -0.89616053, -1.98200927, -0.86168259, -0.66746393, -1.92868682,\n",
       "        -0.69524509, -0.58512125, -1.87059964, -0.5849253 , -0.55785457,\n",
       "        -1.95337728, -0.55852468, -0.81625973, -1.94537345, -0.78953547,\n",
       "        -0.67380315, -1.88659074, -0.65917216, -0.59346618, -1.93166825,\n",
       "        -0.59978274, -0.54983303, -2.00225142, -0.55629468, -0.42880995,\n",
       "        -2.05782853, -0.3498048 , -0.2954548 , -1.71982135, -0.28648279,\n",
       "        -0.2798995 , -1.66889925, -0.26824951, -0.26807529, -1.89593917,\n",
       "        -0.26820191, -0.45311093, -1.78560978, -0.41835249, -0.30961264,\n",
       "        -2.01248633, -0.28971609, -0.27449913, -2.07209559, -0.28859101,\n",
       "        -0.26860967, -1.51163058, -0.27068502, -0.46281525, -1.94665862,\n",
       "        -0.34294972, -0.30370021, -1.84152501, -0.2932656 , -0.2741406 ,\n",
       "        -1.90682105, -0.27981741, -0.2736922 , -1.61467407, -0.27422002,\n",
       "        -0.38640569, -2.16089239, -0.33233535, -0.30187566, -2.07047844,\n",
       "        -0.30443703, -0.27984273, -2.06713591, -0.27592293, -0.27734452,\n",
       "        -1.5568965 , -0.27079472, -0.38502144, -0.41249019, -0.38085117,\n",
       "        -0.2459908 , -0.25156384, -0.25371308, -0.23209964, -0.23735119,\n",
       "        -0.23144488, -0.22998449, -0.22917617, -0.22966959, -0.34801879,\n",
       "        -0.37752038, -0.4098642 , -0.26735535, -0.25035563, -0.26143943,\n",
       "        -0.23222016, -0.23237533, -0.23391242, -0.22809239, -0.23149421,\n",
       "        -0.22848914, -0.33118051, -0.30372846, -0.38130421, -0.25329811,\n",
       "        -0.25201203, -0.24804932, -0.23688726, -0.23438765, -0.23088057,\n",
       "        -0.23382582, -0.22995174, -0.23006777, -0.38382667, -0.35546769,\n",
       "        -0.37845295, -0.2604196 , -0.25580955, -0.25627002, -0.24369872,\n",
       "        -0.24292791, -0.25254502, -0.24376649, -0.23780147, -0.24352088,\n",
       "        -0.25541956, -0.25565704, -0.24447667, -0.23825561, -0.22269517,\n",
       "        -0.22547882, -0.22121342, -0.22186032, -0.22268275, -0.2230848 ,\n",
       "        -0.22040157, -0.21743667, -0.26435049, -0.25200811, -0.25492173,\n",
       "        -0.2236511 , -0.22387627, -0.22206526, -0.23571438, -0.22664323,\n",
       "        -0.21634978, -0.21602353, -0.22009624, -0.21649431, -0.27720098,\n",
       "        -0.2776849 , -0.25603887, -0.22527518, -0.23394241, -0.2292363 ,\n",
       "        -0.22315184, -0.21771441, -0.21434309, -0.22050741, -0.21830738,\n",
       "        -0.22178704, -0.25152698, -0.27590462, -0.26617528, -0.23334592,\n",
       "        -0.22619837, -0.22920118, -0.22760655, -0.22374575, -0.23225494,\n",
       "        -0.22122723, -0.22503262, -0.21558082]),\n",
       " 'split1_test_neg_log_loss': array([-0.25808335, -0.25011102, -0.25130093, -0.25861186, -0.25019947,\n",
       "        -0.27226597, -0.28587518, -0.27053874, -0.27460405, -0.30781875,\n",
       "        -0.30901329, -0.24741083, -0.24871369, -0.24660082, -0.24534715,\n",
       "        -0.22672674, -0.25904316, -0.25745856, -0.28381251, -0.27083232,\n",
       "        -0.26393433, -0.28389987, -0.29550001, -0.25992855, -0.23903256,\n",
       "        -0.25581255, -0.24572997, -0.24276082, -0.25376231, -0.22686334,\n",
       "        -0.26897495, -0.26455913, -0.27542778, -0.29211942, -0.27354551,\n",
       "        -0.26366799, -0.25841689, -0.26010933, -0.25464536, -0.25039976,\n",
       "        -0.24518311, -0.24762604, -0.24527416, -0.24194871, -0.2322318 ,\n",
       "        -0.24222513, -0.24513465, -0.24796005, -0.26049001, -0.25571882,\n",
       "        -0.25128749, -0.25796203, -0.2562834 , -0.24853772, -0.26957147,\n",
       "        -0.27718824, -0.29906274, -0.27586841, -0.28961808, -0.3016532 ,\n",
       "        -0.25167329, -0.25281936, -0.27474646, -0.2277977 , -0.25795805,\n",
       "        -0.25779747, -0.29216766, -0.33149673, -0.23831609, -0.31281172,\n",
       "        -0.29789593, -0.30053379, -0.24735958, -0.24652173, -0.23852491,\n",
       "        -0.26359682, -0.2562619 , -0.26846385, -0.26150456, -0.28884961,\n",
       "        -0.26993334, -0.31613215, -0.35288805, -0.30663323, -0.24438654,\n",
       "        -0.24926002, -0.25970446, -0.25842489, -0.25366794, -0.24509504,\n",
       "        -0.24141164, -0.24699603, -0.26843567, -0.27778069, -0.25346782,\n",
       "        -0.26446237, -0.88018431, -2.06679733, -0.85277563, -0.68092401,\n",
       "        -1.91673023, -0.65594215, -0.60896238, -2.07379773, -0.62478541,\n",
       "        -0.56313165, -1.9504879 , -0.57711798, -0.85411099, -2.04590214,\n",
       "        -0.99770233, -0.66976814, -1.94318881, -0.68066457, -0.63056505,\n",
       "        -1.91154601, -0.61658096, -0.56560067, -1.97603915, -0.56766011,\n",
       "        -0.83870906, -1.99071726, -0.82487058, -0.66628642, -1.98404444,\n",
       "        -0.73891825, -0.59391705, -1.99835707, -0.61895743, -0.55843869,\n",
       "        -1.95390023, -0.55917069, -0.83222346, -1.99578399, -0.80014067,\n",
       "        -0.66061983, -1.89598635, -0.72163445, -0.62930077, -1.92082033,\n",
       "        -0.60457854, -0.54934898, -2.00579005, -0.55482351, -0.40905959,\n",
       "        -2.2281602 , -0.40096868, -0.34397473, -1.83822058, -0.32562431,\n",
       "        -0.31509531, -1.9366858 , -0.30487693, -0.29680488, -1.68511742,\n",
       "        -0.29724167, -0.39936339, -2.18624263, -0.39793075, -0.33804608,\n",
       "        -1.85754836, -0.32254988, -0.30541199, -1.73542686, -0.30966607,\n",
       "        -0.30160928, -1.57846854, -0.29347768, -0.47671612, -1.85894207,\n",
       "        -0.44887755, -0.31278066, -2.17039596, -0.34107089, -0.31203729,\n",
       "        -1.68370031, -0.31028068, -0.29455564, -1.83399364, -0.29528316,\n",
       "        -0.49025205, -1.96698617, -0.40482524, -0.3240584 , -1.83924018,\n",
       "        -0.3396965 , -0.31233663, -1.93603092, -0.30749746, -0.30594887,\n",
       "        -1.51639976, -0.29771589, -0.41927164, -0.39685306, -0.38741325,\n",
       "        -0.28515569, -0.28876541, -0.28849977, -0.27786573, -0.27371191,\n",
       "        -0.27427532, -0.26879943, -0.26815679, -0.26939433, -0.33530063,\n",
       "        -0.39685922, -0.40553378, -0.29023177, -0.28793858, -0.28736292,\n",
       "        -0.27431383, -0.27423484, -0.27243615, -0.27054335, -0.26596559,\n",
       "        -0.27056792, -0.36937038, -0.3486466 , -0.35330872, -0.28941026,\n",
       "        -0.27704023, -0.29127345, -0.27815195, -0.26909738, -0.26903802,\n",
       "        -0.27154742, -0.27066848, -0.26692351, -0.44584533, -0.38298474,\n",
       "        -0.42794804, -0.30758657, -0.29919461, -0.29190572, -0.27443347,\n",
       "        -0.27813146, -0.28725607, -0.27984324, -0.27622499, -0.27709897,\n",
       "        -0.31763535, -0.31902001, -0.2954434 , -0.26233992, -0.26848889,\n",
       "        -0.26182997, -0.27117855, -0.2597859 , -0.26194187, -0.25914707,\n",
       "        -0.26050419, -0.2590971 , -0.28814133, -0.30261512, -0.29951803,\n",
       "        -0.26467361, -0.26125432, -0.26746888, -0.26118277, -0.25821771,\n",
       "        -0.26348364, -0.26692359, -0.26331506, -0.25987421, -0.28713858,\n",
       "        -0.2854484 , -0.29094033, -0.2708428 , -0.26040205, -0.27234138,\n",
       "        -0.26305885, -0.26347409, -0.26099847, -0.26113792, -0.26406004,\n",
       "        -0.26402848, -0.30173594, -0.30583837, -0.30455055, -0.26579467,\n",
       "        -0.26723565, -0.26611881, -0.26115451, -0.26780216, -0.26201678,\n",
       "        -0.26151453, -0.25826405, -0.26069824]),\n",
       " 'split2_test_neg_log_loss': array([-0.25067754, -0.24683554, -0.24465089, -0.23588007, -0.25861974,\n",
       "        -0.24241833, -0.25897633, -0.27158955, -0.24508093, -0.26329892,\n",
       "        -0.33817051, -0.29794362, -0.25024651, -0.24691228, -0.27293204,\n",
       "        -0.24538568, -0.24359385, -0.23365993, -0.26012078, -0.25417269,\n",
       "        -0.27080598, -0.27827063, -0.26312034, -0.2793797 , -0.24112428,\n",
       "        -0.2469549 , -0.24462581, -0.23191256, -0.23784567, -0.24502018,\n",
       "        -0.24030742, -0.26547014, -0.25545145, -0.27235644, -0.26290637,\n",
       "        -0.29799888, -0.24831672, -0.24522755, -0.25640762, -0.23788103,\n",
       "        -0.23832801, -0.24935437, -0.25002033, -0.24758241, -0.23153373,\n",
       "        -0.23478959, -0.24401623, -0.24382209, -0.24927674, -0.24468461,\n",
       "        -0.24481629, -0.24892123, -0.27573563, -0.2529016 , -0.2726163 ,\n",
       "        -0.25798037, -0.26549337, -0.30001593, -0.28939878, -0.45390004,\n",
       "        -0.24525384, -0.24215157, -0.24167843, -0.23758234, -0.24644916,\n",
       "        -0.24140534, -0.23841024, -0.27732303, -0.26803576, -0.3153263 ,\n",
       "        -0.28927113, -0.35202403, -0.24942233, -0.2593329 , -0.23863881,\n",
       "        -0.24281238, -0.24053252, -0.23927627, -0.25936707, -0.29099887,\n",
       "        -0.26462121, -0.31213876, -0.33347742, -0.3191797 , -0.24528014,\n",
       "        -0.24263997, -0.24205754, -0.22833389, -0.23922019, -0.23569294,\n",
       "        -0.24683753, -0.24909549, -0.25725918, -0.26292966, -0.2578866 ,\n",
       "        -0.26344157, -0.87488398, -1.93651949, -0.889883  , -0.67383679,\n",
       "        -1.99564855, -0.68670308, -0.63668117, -1.87965922, -0.61842861,\n",
       "        -0.57759647, -1.91425769, -0.57110107, -0.88417968, -1.8801683 ,\n",
       "        -0.92697968, -0.67709694, -2.01626739, -0.72381553, -0.60303228,\n",
       "        -1.95786085, -0.6166134 , -0.56085988, -1.97248396, -0.5724394 ,\n",
       "        -0.83247443, -1.96300563, -0.89039469, -0.69181636, -1.96542732,\n",
       "        -0.66094323, -0.63772754, -1.84437285, -0.62517174, -0.57777618,\n",
       "        -2.01969507, -0.57730296, -0.88936837, -2.00566385, -0.81668418,\n",
       "        -0.69750393, -1.92718529, -0.71196338, -0.62425128, -1.90264769,\n",
       "        -0.61285304, -0.58115009, -1.98347997, -0.58341085, -0.40746477,\n",
       "        -2.23607171, -0.38137669, -0.31826113, -1.78528122, -0.31670627,\n",
       "        -0.31107999, -1.66615742, -0.30809079, -0.29680113, -1.5935452 ,\n",
       "        -0.29972344, -0.45589057, -1.86299951, -0.3679906 , -0.34055365,\n",
       "        -1.75986841, -0.32702594, -0.30982241, -1.96296576, -0.30206163,\n",
       "        -0.30909944, -1.88547391, -0.30227634, -0.42340521, -1.8536442 ,\n",
       "        -0.41004698, -0.31450378, -1.8044733 , -0.3208859 , -0.32255718,\n",
       "        -1.86869862, -0.3072129 , -0.29806516, -1.82792257, -0.30560062,\n",
       "        -0.43819456, -1.98579576, -0.45621032, -0.32851975, -2.28351417,\n",
       "        -0.34871427, -0.31391228, -1.96581924, -0.30933537, -0.30515646,\n",
       "        -1.7796812 , -0.31346874, -0.38517637, -0.39074357, -0.37301542,\n",
       "        -0.2954543 , -0.2816545 , -0.28284258, -0.27032647, -0.26795269,\n",
       "        -0.27578231, -0.27597877, -0.26819932, -0.26639441, -0.39578145,\n",
       "        -0.35562209, -0.40472633, -0.28160399, -0.28902415, -0.29184783,\n",
       "        -0.27254089, -0.26751209, -0.27457037, -0.26622971, -0.26909814,\n",
       "        -0.26424413, -0.41745751, -0.33975973, -0.42325676, -0.292556  ,\n",
       "        -0.29474186, -0.28156813, -0.26922042, -0.27166866, -0.26702879,\n",
       "        -0.26567249, -0.26432135, -0.26722588, -0.39877076, -0.41022211,\n",
       "        -0.42936916, -0.28687608, -0.29555854, -0.2953527 , -0.28444313,\n",
       "        -0.28195832, -0.27874653, -0.27977813, -0.27288686, -0.28077319,\n",
       "        -0.29998919, -0.29596186, -0.29132789, -0.26166465, -0.27055367,\n",
       "        -0.26328771, -0.2586126 , -0.25948683, -0.25410656, -0.25547951,\n",
       "        -0.26109306, -0.24946409, -0.29595091, -0.29330485, -0.31032029,\n",
       "        -0.26630275, -0.26928823, -0.26403654, -0.25578507, -0.25393863,\n",
       "        -0.2549927 , -0.25149896, -0.25298037, -0.25073476, -0.29898573,\n",
       "        -0.31289524, -0.29387375, -0.26061439, -0.26547872, -0.26364449,\n",
       "        -0.2574026 , -0.25863301, -0.25357275, -0.2525998 , -0.25325988,\n",
       "        -0.25258461, -0.2863062 , -0.30407077, -0.31874401, -0.27161789,\n",
       "        -0.26666956, -0.26474008, -0.25853384, -0.26115303, -0.26007702,\n",
       "        -0.25796421, -0.25559308, -0.25561612]),\n",
       " 'split3_test_neg_log_loss': array([-0.23184782, -0.2114003 , -0.22340739, -0.20918619, -0.21979336,\n",
       "        -0.2098426 , -0.24059293, -0.23198062, -0.24339645, -0.26407828,\n",
       "        -0.27651135, -0.26417947, -0.21679698, -0.20934328, -0.2221442 ,\n",
       "        -0.2225728 , -0.2175011 , -0.2087637 , -0.20825629, -0.2398085 ,\n",
       "        -0.22809888, -0.25971864, -0.24462364, -0.24327706, -0.21271659,\n",
       "        -0.23325267, -0.21869373, -0.20106495, -0.21199706, -0.21848853,\n",
       "        -0.22572756, -0.2235305 , -0.21466872, -0.24983583, -0.23215465,\n",
       "        -0.24425918, -0.22150675, -0.2226494 , -0.21122904, -0.20736935,\n",
       "        -0.21536225, -0.20552011, -0.21207492, -0.20545534, -0.20321154,\n",
       "        -0.21181384, -0.21670713, -0.21959393, -0.21586871, -0.21980338,\n",
       "        -0.22299481, -0.2182248 , -0.21128467, -0.21279786, -0.25911367,\n",
       "        -0.22493942, -0.22644108, -0.28607956, -0.264577  , -0.25766168,\n",
       "        -0.21613409, -0.21751562, -0.21652778, -0.22022144, -0.23113399,\n",
       "        -0.2061339 , -0.23978716, -0.22243709, -0.24978279, -0.30069906,\n",
       "        -0.2777497 , -0.23976301, -0.20318068, -0.21500149, -0.21471181,\n",
       "        -0.22834462, -0.2124979 , -0.22407681, -0.27276877, -0.21861699,\n",
       "        -0.2337261 , -0.28044508, -0.27044088, -0.2575184 , -0.21728506,\n",
       "        -0.20546757, -0.21254883, -0.20492384, -0.20905577, -0.20703499,\n",
       "        -0.21094484, -0.20836088, -0.2285629 , -0.20984945, -0.25063157,\n",
       "        -0.23241586, -0.7923612 , -1.92005045, -0.91149867, -0.70652861,\n",
       "        -2.0070522 , -0.69603387, -0.59439241, -1.90134982, -0.62674942,\n",
       "        -0.55167773, -1.99481651, -0.56483633, -0.83742403, -2.10679669,\n",
       "        -0.94972026, -0.6784529 , -1.93062369, -0.64508552, -0.62218875,\n",
       "        -1.94299096, -0.60252567, -0.53389076, -1.98688603, -0.55653904,\n",
       "        -0.92765853, -1.89012742, -0.8974623 , -0.67716365, -1.97966847,\n",
       "        -0.67991095, -0.59461915, -1.92108949, -0.64493673, -0.56660305,\n",
       "        -1.97775993, -0.57762934, -0.87263264, -1.98690434, -0.82132645,\n",
       "        -0.68225622, -2.06783375, -0.6983631 , -0.61333259, -1.92674909,\n",
       "        -0.62099574, -0.57575572, -1.90781373, -0.57631196, -0.42023783,\n",
       "        -1.7541674 , -0.39552668, -0.3214873 , -1.78052851, -0.31558338,\n",
       "        -0.29699172, -1.73922058, -0.28910566, -0.27868234, -1.69354721,\n",
       "        -0.28027004, -0.37369904, -2.32683413, -0.38214784, -0.2977744 ,\n",
       "        -1.73163598, -0.30698851, -0.29532081, -1.78714517, -0.29626704,\n",
       "        -0.2775364 , -1.74444421, -0.27614684, -0.39625342, -1.89216344,\n",
       "        -0.4631714 , -0.29581817, -1.97770591, -0.30589023, -0.28673115,\n",
       "        -1.51799332, -0.28057742, -0.27423171, -1.87218127, -0.27779819,\n",
       "        -0.38211761, -2.02194418, -0.45281919, -0.3124164 , -2.04265446,\n",
       "        -0.35094791, -0.29478643, -1.66983341, -0.29744154, -0.27721371,\n",
       "        -1.84429886, -0.27180925, -0.39466587, -0.40223665, -0.38308546,\n",
       "        -0.26100395, -0.27232607, -0.27202068, -0.24692622, -0.25129281,\n",
       "        -0.24826765, -0.24636742, -0.24495929, -0.24660923, -0.38453564,\n",
       "        -0.4431275 , -0.40090113, -0.26707944, -0.27386952, -0.27431909,\n",
       "        -0.25837221, -0.25271439, -0.25878584, -0.24593781, -0.25112224,\n",
       "        -0.24648943, -0.43027135, -0.34705203, -0.34433584, -0.27057452,\n",
       "        -0.29938943, -0.2659238 , -0.25475898, -0.25554215, -0.25668322,\n",
       "        -0.24879411, -0.2473777 , -0.243464  , -0.34483138, -0.4195465 ,\n",
       "        -0.40182254, -0.27953065, -0.27363094, -0.27703816, -0.26014863,\n",
       "        -0.26236009, -0.25947847, -0.25134637, -0.2583504 , -0.2531153 ,\n",
       "        -0.29839885, -0.2663656 , -0.26628624, -0.24543401, -0.2336342 ,\n",
       "        -0.23433257, -0.23226767, -0.22965763, -0.22947863, -0.23085271,\n",
       "        -0.23020007, -0.22795922, -0.26824045, -0.2793903 , -0.28239968,\n",
       "        -0.24392262, -0.23349038, -0.24373523, -0.23308403, -0.2318735 ,\n",
       "        -0.23214494, -0.22814971, -0.22866152, -0.22726951, -0.26955648,\n",
       "        -0.27796131, -0.3143593 , -0.24053527, -0.23830299, -0.23692406,\n",
       "        -0.23433567, -0.2241195 , -0.2290553 , -0.22664939, -0.22556929,\n",
       "        -0.22658801, -0.27399165, -0.28153294, -0.26527019, -0.24781306,\n",
       "        -0.23885175, -0.24129333, -0.23433861, -0.23367018, -0.23746212,\n",
       "        -0.23023751, -0.22534068, -0.22562395]),\n",
       " 'split4_test_neg_log_loss': array([-0.23912937, -0.24556733, -0.24425664, -0.25992845, -0.26230691,\n",
       "        -0.2475449 , -0.26574359, -0.2711491 , -0.28475847, -0.27699285,\n",
       "        -0.31524098, -0.29112218, -0.25152099, -0.23469958, -0.232076  ,\n",
       "        -0.26955242, -0.24789819, -0.24686974, -0.27737574, -0.28976187,\n",
       "        -0.27643288, -0.2822248 , -0.2847688 , -0.32708828, -0.23576171,\n",
       "        -0.23287843, -0.2374197 , -0.24135949, -0.25118894, -0.24402808,\n",
       "        -0.25048628, -0.26077982, -0.29464862, -0.26668795, -0.27130894,\n",
       "        -0.29999191, -0.24288066, -0.24007553, -0.237182  , -0.23212687,\n",
       "        -0.23182721, -0.2338154 , -0.24270462, -0.26143568, -0.23549909,\n",
       "        -0.24723345, -0.23305454, -0.24570554, -0.22831693, -0.23745991,\n",
       "        -0.23545467, -0.25092181, -0.29111988, -0.23588394, -0.27690227,\n",
       "        -0.2984195 , -0.26516296, -0.27711231, -0.28596215, -0.28489495,\n",
       "        -0.23670938, -0.24977443, -0.23234858, -0.23521752, -0.27416547,\n",
       "        -0.24265743, -0.25521845, -0.25439958, -0.24938427, -0.29359454,\n",
       "        -0.28818895, -0.34273926, -0.23692287, -0.24076148, -0.22165646,\n",
       "        -0.24933439, -0.25264319, -0.24284947, -0.26048926, -0.26220587,\n",
       "        -0.27825768, -0.31671199, -0.31466775, -0.3708998 , -0.23816555,\n",
       "        -0.24077893, -0.23740409, -0.22608456, -0.24277952, -0.23556008,\n",
       "        -0.247159  , -0.24903577, -0.26047049, -0.25948717, -0.26422081,\n",
       "        -0.29034818, -0.80784094, -2.033937  , -0.89314364, -0.71831525,\n",
       "        -1.99107598, -0.65513779, -0.60471044, -1.86764475, -0.60438746,\n",
       "        -0.56250341, -1.86642358, -0.57832966, -0.8292035 , -1.93589808,\n",
       "        -0.84224838, -0.7504168 , -1.88599212, -0.64459439, -0.61206202,\n",
       "        -1.90692009, -0.62775555, -0.58050035, -1.92001276, -0.56608516,\n",
       "        -0.93364135, -2.03014359, -0.86022364, -0.65648175, -1.95686783,\n",
       "        -0.71848507, -0.62029262, -1.96208204, -0.61062293, -0.57019504,\n",
       "        -2.02875982, -0.56364615, -0.93620183, -1.93970803, -0.80207898,\n",
       "        -0.707075  , -1.9265869 , -0.72248589, -0.60113171, -1.84395803,\n",
       "        -0.61593548, -0.57117165, -1.86964407, -0.56837047, -0.4135078 ,\n",
       "        -1.91348045, -0.4173544 , -0.30832127, -1.9876768 , -0.31505386,\n",
       "        -0.29493158, -2.21270145, -0.28497952, -0.28486309, -1.75979722,\n",
       "        -0.28643815, -0.39145707, -1.9547795 , -0.41159522, -0.31070593,\n",
       "        -1.88596247, -0.30519606, -0.28449551, -1.73063021, -0.28790694,\n",
       "        -0.286241  , -1.8520728 , -0.28197099, -0.51327423, -1.79615143,\n",
       "        -0.36191013, -0.31921714, -2.16677463, -0.31208342, -0.28529429,\n",
       "        -1.9026405 , -0.29050463, -0.2777516 , -1.7360431 , -0.28325052,\n",
       "        -0.41487112, -1.95456926, -0.47234869, -0.32007454, -2.25262925,\n",
       "        -0.31567289, -0.30285855, -1.74894665, -0.29029047, -0.28062115,\n",
       "        -1.51109123, -0.28720492, -0.41506251, -0.39969547, -0.39855084,\n",
       "        -0.27552171, -0.26660008, -0.26556106, -0.25397123, -0.26022339,\n",
       "        -0.24966857, -0.24703194, -0.24609172, -0.24484005, -0.3896164 ,\n",
       "        -0.40499718, -0.40653494, -0.26769695, -0.2674811 , -0.26982124,\n",
       "        -0.25693955, -0.25368004, -0.25220246, -0.24878495, -0.25192834,\n",
       "        -0.25007748, -0.34384299, -0.40807295, -0.3956572 , -0.26883101,\n",
       "        -0.26928305, -0.25822819, -0.2526775 , -0.24789009, -0.25327421,\n",
       "        -0.24828061, -0.24964195, -0.25190868, -0.40211856, -0.39658422,\n",
       "        -0.39705326, -0.2792501 , -0.27853059, -0.27607995, -0.26204168,\n",
       "        -0.26399091, -0.26039326, -0.25541103, -0.25256107, -0.2540965 ,\n",
       "        -0.26799343, -0.28613112, -0.27687913, -0.24209029, -0.24504736,\n",
       "        -0.24381612, -0.24059507, -0.24126935, -0.24640536, -0.24143919,\n",
       "        -0.23859006, -0.24023317, -0.32165614, -0.27492484, -0.27647521,\n",
       "        -0.25027713, -0.24873944, -0.24194945, -0.24710706, -0.23856514,\n",
       "        -0.24531513, -0.24461908, -0.24561562, -0.24019321, -0.31677623,\n",
       "        -0.26887629, -0.30806016, -0.24559895, -0.24627771, -0.24505885,\n",
       "        -0.24181462, -0.23937029, -0.24182333, -0.24362026, -0.24379655,\n",
       "        -0.23849897, -0.25674301, -0.27580781, -0.27937278, -0.24443633,\n",
       "        -0.25171225, -0.24631923, -0.24403963, -0.24551837, -0.23959421,\n",
       "        -0.24039265, -0.24411304, -0.23815378]),\n",
       " 'mean_test_neg_log_loss': array([-0.23825572, -0.23589843, -0.23852393, -0.23834679, -0.24509918,\n",
       "        -0.23562976, -0.2615671 , -0.25815344, -0.25372535, -0.27057714,\n",
       "        -0.2998342 , -0.27416866, -0.23817491, -0.22838892, -0.23791055,\n",
       "        -0.23772299, -0.23547634, -0.23319376, -0.25014976, -0.25399907,\n",
       "        -0.24895476, -0.26863719, -0.26456794, -0.27427809, -0.23003823,\n",
       "        -0.23736907, -0.23203752, -0.22728899, -0.23189481, -0.23115403,\n",
       "        -0.24266858, -0.24533687, -0.25242779, -0.26827899, -0.25665312,\n",
       "        -0.27233324, -0.23880786, -0.23629748, -0.23569197, -0.22789322,\n",
       "        -0.2295883 , -0.2271304 , -0.23187281, -0.23223058, -0.22181218,\n",
       "        -0.23074333, -0.23242538, -0.23649474, -0.23560509, -0.23586497,\n",
       "        -0.23554383, -0.24115074, -0.24814338, -0.2354173 , -0.26168997,\n",
       "        -0.25843269, -0.25712016, -0.27646047, -0.28627275, -0.31491623,\n",
       "        -0.23400478, -0.23433483, -0.23578826, -0.22877092, -0.24297002,\n",
       "        -0.23387025, -0.25371059, -0.26391899, -0.24934038, -0.30699469,\n",
       "        -0.28142   , -0.30915105, -0.22984817, -0.23685409, -0.22922835,\n",
       "        -0.24015947, -0.23418053, -0.23845148, -0.25143989, -0.25988308,\n",
       "        -0.25605908, -0.29540356, -0.30365051, -0.30030662, -0.23073355,\n",
       "        -0.2303217 , -0.23204447, -0.22585531, -0.23216135, -0.22884608,\n",
       "        -0.23302016, -0.23198519, -0.24448224, -0.24637547, -0.2511664 ,\n",
       "        -0.26056972, -0.85716062, -1.96970123, -0.88210823, -0.69587751,\n",
       "        -1.96844766, -0.67235274, -0.60934228, -1.92169856, -0.60926456,\n",
       "        -0.56098964, -1.92476504, -0.56868275, -0.86406967, -1.99261732,\n",
       "        -0.89879272, -0.69379273, -1.94768321, -0.67782044, -0.60964766,\n",
       "        -1.92741776, -0.61319126, -0.56293909, -1.94776943, -0.56385667,\n",
       "        -0.88572878, -1.97120063, -0.86692676, -0.67184242, -1.96293898,\n",
       "        -0.69870052, -0.60633552, -1.91930022, -0.61692283, -0.56617351,\n",
       "        -1.98669847, -0.56725477, -0.86933721, -1.97468673, -0.80595315,\n",
       "        -0.68425163, -1.94083661, -0.7027238 , -0.61229651, -1.90516868,\n",
       "        -0.61082911, -0.5654519 , -1.95379585, -0.56784229, -0.41581599,\n",
       "        -2.03794166, -0.38900625, -0.31749985, -1.82230569, -0.31189012,\n",
       "        -0.29959962, -1.8447329 , -0.29106048, -0.28504535, -1.72558924,\n",
       "        -0.28637504, -0.4147042 , -2.02329311, -0.39560338, -0.31933854,\n",
       "        -1.84950031, -0.3102953 , -0.29390997, -1.85765272, -0.29689854,\n",
       "        -0.28861916, -1.71441801, -0.28491138, -0.45449285, -1.86951195,\n",
       "        -0.40539116, -0.30920399, -1.99217496, -0.31463921, -0.2961521 ,\n",
       "        -1.77597076, -0.29367861, -0.28365926, -1.77696293, -0.2872305 ,\n",
       "        -0.42236821, -2.01803755, -0.42370776, -0.31738895, -2.0977033 ,\n",
       "        -0.33189372, -0.30074732, -1.87755323, -0.29609755, -0.28925694,\n",
       "        -1.64167351, -0.28819871, -0.39983956, -0.40040379, -0.38458323,\n",
       "        -0.27262529, -0.27218198, -0.27252743, -0.25623786, -0.2581064 ,\n",
       "        -0.25588775, -0.25363241, -0.25131666, -0.25138152, -0.37065058,\n",
       "        -0.39562527, -0.40551208, -0.2747935 , -0.2737338 , -0.2769581 ,\n",
       "        -0.25887733, -0.25610334, -0.25838145, -0.25191764, -0.2539217 ,\n",
       "        -0.25197362, -0.37842455, -0.34945195, -0.37957255, -0.27493398,\n",
       "        -0.27849332, -0.26900858, -0.25833922, -0.25571718, -0.25538096,\n",
       "        -0.25362409, -0.25239224, -0.25191797, -0.39507854, -0.39296105,\n",
       "        -0.40692919, -0.2827326 , -0.28054485, -0.27932931, -0.26495312,\n",
       "        -0.26587374, -0.26768387, -0.26202905, -0.25956496, -0.26172097,\n",
       "        -0.28788728, -0.28462713, -0.27488267, -0.2499569 , -0.24808386,\n",
       "        -0.24574904, -0.24477346, -0.24241201, -0.24292303, -0.24200066,\n",
       "        -0.24215779, -0.23883805, -0.28766786, -0.28044865, -0.28472699,\n",
       "        -0.24976544, -0.24732973, -0.24785107, -0.24657466, -0.24184764,\n",
       "        -0.24245724, -0.24144298, -0.24213376, -0.2389132 , -0.2899316 ,\n",
       "        -0.28457323, -0.29265448, -0.24857332, -0.24888078, -0.24944102,\n",
       "        -0.24395272, -0.24066226, -0.23995859, -0.24090296, -0.24099863,\n",
       "        -0.24069742, -0.27406076, -0.2886309 , -0.28682256, -0.25260157,\n",
       "        -0.25013352, -0.24953453, -0.24513463, -0.2463779 , -0.24628101,\n",
       "        -0.24226723, -0.24166869, -0.23913458]),\n",
       " 'std_test_neg_log_loss': array([0.01614287, 0.01497849, 0.01051473, 0.01916926, 0.01584952,\n",
       "        0.02477341, 0.0146912 , 0.01641771, 0.02308824, 0.02198242,\n",
       "        0.02794051, 0.01835368, 0.01486242, 0.0181844 , 0.01999725,\n",
       "        0.01789068, 0.01887605, 0.01770995, 0.0302257 , 0.02550327,\n",
       "        0.02749562, 0.01711149, 0.02301854, 0.02877395, 0.01103217,\n",
       "        0.01301065, 0.01333089, 0.01555318, 0.02011181, 0.01124778,\n",
       "        0.01590851, 0.02270386, 0.03058055, 0.01407001, 0.01622586,\n",
       "        0.02263638, 0.01444526, 0.01655547, 0.01826354, 0.01617423,\n",
       "        0.01165702, 0.02096757, 0.01748253, 0.0230487 , 0.01391594,\n",
       "        0.01378237, 0.01120871, 0.01165237, 0.0166241 , 0.01366978,\n",
       "        0.01135378, 0.01479626, 0.03401343, 0.01456803, 0.01678254,\n",
       "        0.02715732, 0.0268378 , 0.01872263, 0.01210869, 0.07092219,\n",
       "        0.01381663, 0.01757917, 0.02202624, 0.0067251 , 0.02358323,\n",
       "        0.01806869, 0.0201224 , 0.03861765, 0.01037104, 0.00840504,\n",
       "        0.01512982, 0.03964241, 0.01874567, 0.01608253, 0.00953394,\n",
       "        0.01630973, 0.01987287, 0.01767798, 0.02466015, 0.02815101,\n",
       "        0.01873352, 0.02571844, 0.03942757, 0.04476593, 0.01500796,\n",
       "        0.01744552, 0.01912137, 0.01849814, 0.01683731, 0.01338424,\n",
       "        0.01518067, 0.02009771, 0.02280463, 0.02595711, 0.01170627,\n",
       "        0.01883175, 0.05071304, 0.06823821, 0.02127171, 0.01638017,\n",
       "        0.03678321, 0.01644264, 0.01447022, 0.0768229 , 0.02022437,\n",
       "        0.00989238, 0.04428993, 0.00961474, 0.03183434, 0.07967554,\n",
       "        0.07889972, 0.0293184 , 0.04251058, 0.03030623, 0.01732285,\n",
       "        0.01966945, 0.0096311 , 0.01601576, 0.03962499, 0.00632281,\n",
       "        0.04291717, 0.04606956, 0.02578495, 0.01194374, 0.01970572,\n",
       "        0.02756627, 0.01960508, 0.05669097, 0.01959843, 0.00748347,\n",
       "        0.03201205, 0.00852275, 0.042609  , 0.02696933, 0.01157435,\n",
       "        0.01654173, 0.06553144, 0.02344903, 0.01350829, 0.03214347,\n",
       "        0.00767624, 0.01333095, 0.05500611, 0.01111063, 0.00786277,\n",
       "        0.18539284, 0.0227341 , 0.01604761, 0.09079656, 0.01327407,\n",
       "        0.01255868, 0.20870827, 0.01443652, 0.0109999 , 0.10028915,\n",
       "        0.01152216, 0.03355171, 0.20276647, 0.01855907, 0.01693624,\n",
       "        0.09988451, 0.01334564, 0.01306067, 0.13647108, 0.00824536,\n",
       "        0.01494689, 0.14745544, 0.01151326, 0.04093082, 0.04941654,\n",
       "        0.04699113, 0.00837395, 0.15516729, 0.01598476, 0.01811475,\n",
       "        0.15286051, 0.01290468, 0.0104822 , 0.09265854, 0.01163089,\n",
       "        0.03956844, 0.07496235, 0.05094173, 0.00939124, 0.16068757,\n",
       "        0.01856805, 0.01252693, 0.14617749, 0.01222899, 0.01336366,\n",
       "        0.14144648, 0.01613476, 0.0146335 , 0.0071536 , 0.00840344,\n",
       "        0.01749866, 0.01281425, 0.01236353, 0.0163753 , 0.01282316,\n",
       "        0.01690026, 0.01664313, 0.01501021, 0.01474328, 0.02427293,\n",
       "        0.02923648, 0.00289365, 0.00948596, 0.01429142, 0.01121332,\n",
       "        0.01509697, 0.01441726, 0.01481008, 0.01526412, 0.01334303,\n",
       "        0.01470718, 0.03929764, 0.03354819, 0.02864067, 0.0144531 ,\n",
       "        0.01725608, 0.01561083, 0.01425485, 0.01378203, 0.01362784,\n",
       "        0.01349268, 0.01423103, 0.0142014 , 0.03253258, 0.02246391,\n",
       "        0.01938996, 0.0151996 , 0.01572844, 0.0138741 , 0.01380613,\n",
       "        0.01379396, 0.01307761, 0.01499209, 0.01399435, 0.01458015,\n",
       "        0.02276759, 0.02229958, 0.0184152 , 0.01009625, 0.01888839,\n",
       "        0.01490742, 0.0179871 , 0.01536068, 0.01476032, 0.01384255,\n",
       "        0.0162738 , 0.01484356, 0.02071309, 0.01730394, 0.01916065,\n",
       "        0.01556981, 0.01685022, 0.01651366, 0.01094034, 0.01229311,\n",
       "        0.01670544, 0.01779512, 0.01578337, 0.01561367, 0.01665909,\n",
       "        0.01510273, 0.02026604, 0.01585672, 0.01224787, 0.0161881 ,\n",
       "        0.01466751, 0.01813981, 0.01677221, 0.01531533, 0.01698795,\n",
       "        0.01579338, 0.01856635, 0.01350006, 0.02135217, 0.01411424,\n",
       "        0.0159285 , 0.01412761, 0.01312374, 0.01644228, 0.01230578,\n",
       "        0.01554197, 0.01427328, 0.01717505]),\n",
       " 'rank_test_neg_log_loss': array([ 47,  39,  50,  48,  77,  35, 132, 124, 112, 144, 190, 151,  46,\n",
       "          6,  45,  44,  32,  26,  98, 114,  91, 142, 137, 152,  12,  43,\n",
       "         20,   4,  18,  16,  71,  79, 107, 141, 121, 146,  51,  40,  36,\n",
       "          5,  10,   3,  17,  23,   1,  15,  24,  41,  34,  38,  33,  61,\n",
       "         88,  31, 133, 127, 122, 156, 170, 200,  28,  30,  37,   7,  73,\n",
       "         27, 111, 136,  92, 194, 162, 195,  11,  42,   9,  56,  29,  49,\n",
       "        102, 130, 118, 185, 193, 191,  14,  13,  21,   2,  22,   8,  25,\n",
       "         19,  75,  82,  99, 131, 250, 279, 254, 246, 278, 242, 235, 270,\n",
       "        234, 225, 271, 232, 251, 284, 256, 245, 274, 243, 236, 272, 239,\n",
       "        226, 275, 227, 255, 280, 252, 241, 277, 247, 233, 269, 240, 229,\n",
       "        282, 230, 253, 281, 249, 244, 273, 248, 238, 268, 237, 228, 276,\n",
       "        231, 221, 287, 210, 202, 262, 198, 189, 263, 181, 169, 259, 171,\n",
       "        220, 286, 213, 203, 264, 197, 184, 265, 188, 177, 258, 168, 224,\n",
       "        266, 217, 196, 283, 199, 187, 260, 183, 164, 261, 173, 222, 285,\n",
       "        223, 201, 288, 204, 192, 267, 186, 179, 257, 176, 215, 216, 209,\n",
       "        148, 145, 147, 120, 123, 117, 110, 100, 101, 206, 214, 218, 153,\n",
       "        149, 157, 128, 119, 126, 103, 113, 105, 207, 205, 208, 155, 158,\n",
       "        143, 125, 116, 115, 109, 106, 104, 212, 211, 219, 163, 161, 159,\n",
       "        138, 139, 140, 135, 129, 134, 175, 166, 154,  96,  87,  80,  76,\n",
       "         69,  72,  65,  67,  52, 174, 160, 167,  95,  85,  86,  84,  64,\n",
       "         70,  62,  66,  53, 180, 165, 182,  89,  90,  93,  74,  57,  55,\n",
       "         59,  60,  58, 150, 178, 172, 108,  97,  94,  78,  83,  81,  68,\n",
       "         63,  54])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRIAL_2_MLP.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL TWO NEG LOG LOSS RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.1,\n",
       " 'classifier__hidden_layer_sizes': (15,),\n",
       " 'classifier__learning_rate': 'adaptive',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL TWO NEG LOG LOSS RESULTS ---------------------------------------------\")\n",
    "TRIAL_2_MLP.cv_results_['rank_test_neg_log_loss']\n",
    "TRIAL_2_MLP.cv_results_['params'][ np.argmin(TRIAL_2_MLP.cv_results_['rank_test_neg_log_loss']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL TWO F1 RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'tanh',\n",
       " 'classifier__alpha': 0.01,\n",
       " 'classifier__hidden_layer_sizes': (10,),\n",
       " 'classifier__learning_rate': 'invscaling',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL TWO F1 RESULTS ---------------------------------------------\")\n",
    "TRIAL_2_MLP.cv_results_['rank_test_f1_micro']\n",
    "TRIAL_2_MLP.cv_results_['params'][ np.argmin(TRIAL_2_MLP.cv_results_['rank_test_f1_micro']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL TWO ROC AUC RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'tanh',\n",
       " 'classifier__alpha': 0.1,\n",
       " 'classifier__hidden_layer_sizes': (15,),\n",
       " 'classifier__learning_rate': 'invscaling',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL TWO ROC AUC RESULTS ---------------------------------------------\")\n",
    "TRIAL_2_MLP.cv_results_['rank_test_roc_auc_ovo']\n",
    "TRIAL_2_MLP.cv_results_['params'][ np.argmin(TRIAL_2_MLP.cv_results_['rank_test_roc_auc_ovo']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 2 MLP using best NEG LOG LOSS hyperparameters :0.9286297376093294\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 2 MLP using best F1 hyperparameters :0.9286297376093294\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 2 MLP using best ROC_AUC hyperparameters :0.927463556851312\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best NEG LOG LOSS results to train on test set, \n",
    "p = TRIAL_2_MLP.cv_results_['params'][ np.argmin(TRIAL_2_MLP.cv_results_['rank_test_neg_log_loss']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train__neg_log_loss = pipe.predict(X_train)\n",
    "y_pred_test_neg_log_loss = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 2 MLP using best NEG LOG LOSS hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_neg_log_loss)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best F1 results to train on test set, LR_Trial 1\n",
    "p = TRIAL_2_MLP.cv_results_['params'][ np.argmin(TRIAL_2_MLP.cv_results_['rank_test_f1_micro']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train_roc_auc = pipe.predict(X_train)\n",
    "y_pred_test_f1 = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 2 MLP using best F1 hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_f1)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best ROC_AUC results to train on test set, LR_Trial 1\n",
    "p  = TRIAL_2_MLP.cv_results_['params'][ np.argmin(TRIAL_2_MLP.cv_results_['rank_test_roc_auc_ovo']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train_roc_auc = pipe.predict(X_train)\n",
    "y_pred_test_roc_auc = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 2 MLP using best ROC_AUC hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_roc_auc)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTI LAYERED PERCEPTRON TRIAL THREE ON GAMMA PARTICLES DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:   10.0s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:   11.5s\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:   14.0s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:   15.7s\n",
      "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed:   17.7s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   20.8s\n",
      "[Parallel(n_jobs=-1)]: Done 189 tasks      | elapsed:   22.6s\n",
      "[Parallel(n_jobs=-1)]: Done 210 tasks      | elapsed:   24.7s\n",
      "[Parallel(n_jobs=-1)]: Done 233 tasks      | elapsed:   28.6s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:   30.9s\n",
      "[Parallel(n_jobs=-1)]: Done 281 tasks      | elapsed:   34.4s\n",
      "[Parallel(n_jobs=-1)]: Done 306 tasks      | elapsed:   38.0s\n",
      "[Parallel(n_jobs=-1)]: Done 333 tasks      | elapsed:   41.4s\n",
      "[Parallel(n_jobs=-1)]: Done 360 tasks      | elapsed:   45.3s\n",
      "[Parallel(n_jobs=-1)]: Done 389 tasks      | elapsed:   48.3s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:   53.3s\n",
      "[Parallel(n_jobs=-1)]: Done 449 tasks      | elapsed:   56.4s\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 513 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 546 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 581 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 616 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 653 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 690 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 729 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 809 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 850 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 893 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 936 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 981 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1026 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1073 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1120 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1169 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1269 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1320 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1373 tasks      | elapsed:  4.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL THREE RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1440 out of 1440 | elapsed:  4.8min finished\n"
     ]
    }
   ],
   "source": [
    "ITERATIONS = 1\n",
    "for i in range(1):\n",
    "    #For reach \"trial\", randomly select 5000 data points\n",
    "    random5000DataPoints = beanData.sample(n = 5000, replace = True)\n",
    "    #From that 5000, create x set and y set\n",
    "    xSet = random5000DataPoints.iloc[:,0:15].values\n",
    "    ySet = random5000DataPoints.iloc[:,16].values\n",
    "    ySet = encodeLabel.fit_transform(ySet)\n",
    "    # Run GridSearch to find best hyperparameters per 5000 data points \n",
    "    \n",
    "    pipe = Pipeline([('std', StandardScaler()),('classifier', MLPClassifier(max_iter = 250))])\n",
    "\n",
    "    search_space = [\n",
    "                {\n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                {\n",
    "                 \n",
    "                  \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['sgd'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                \n",
    "                {\n",
    "                \n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['adam'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                ]\n",
    "    \n",
    "    #'precision_micro','roc_auc_ovo','jaccard','neg_log_loss'] JACCARD DOES NOT WORK FOR NON BINARY?\n",
    "    runHyperParamSearch = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=5), \n",
    "                          scoring=['recall_micro','f1_micro','roc_auc_ovo','neg_log_loss'], \n",
    "                          refit=False, verbose=10, n_jobs = -1)\n",
    "    TRIAL_3_MLP = runHyperParamSearch.fit(xSet, ySet)\n",
    "    print(\"---------------------------------TRIAL THREE RESULTS ---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([1.2388648 , 1.23826561, 1.26939092, 1.50539508, 1.55073342,\n",
       "        1.58046036, 2.01273355, 2.07508492, 2.0775876 , 2.34161129,\n",
       "        2.29817657, 2.261443  , 1.44614334, 1.29120984, 1.35496402,\n",
       "        1.62619901, 1.66783595, 1.5723516 , 2.00162287, 2.09080048,\n",
       "        2.10791245, 2.35372276, 2.34101353, 2.27095284, 1.40791049,\n",
       "        1.30942745, 1.35316358, 1.66323042, 1.55693941, 1.65532393,\n",
       "        2.03935528, 2.07688837, 2.22921944, 2.32249808, 2.30187864,\n",
       "        2.24232693, 1.37197952, 1.30642304, 1.32684112, 1.55453725,\n",
       "        1.64411359, 1.63901072, 2.05486841, 2.19778986, 2.24913526,\n",
       "        2.59923449, 2.41167231, 2.3364079 , 1.47916999, 1.37758389,\n",
       "        1.3495595 , 1.74299965, 1.82877364, 1.90173573, 2.28996954,\n",
       "        2.34781799, 2.48443203, 2.79820738, 2.65658441, 2.60644016,\n",
       "        1.57115073, 1.42852774, 1.40160618, 1.77722831, 1.78653679,\n",
       "        1.81055856, 2.28126144, 2.49184389, 2.43149028, 2.69631877,\n",
       "        2.61895275, 2.62605691, 1.49898949, 1.40400782, 1.4176178 ,\n",
       "        1.7884376 , 1.8486917 , 1.77652831, 2.33891306, 2.33841071,\n",
       "        2.56000233, 2.9269166 , 2.89388919, 2.69681835, 1.49308324,\n",
       "        1.34885902, 1.33244424, 1.66943717, 1.6690351 , 1.77422571,\n",
       "        2.23572607, 2.32509937, 2.49574633, 2.76007276, 2.62445745,\n",
       "        2.6089416 , 3.65624423, 0.35080194, 3.3690968 , 3.77985015,\n",
       "        0.73853512, 3.63442559, 3.89164648, 0.72482376, 3.83750029,\n",
       "        3.91906977, 1.09063764, 4.04057479, 3.35518517, 0.38453074,\n",
       "        3.46958337, 3.64193168, 0.72772593, 3.63482571, 3.91156421,\n",
       "        0.6796844 , 4.1753911 , 4.27918   , 1.36827693, 4.16678333,\n",
       "        3.53674159, 0.36251206, 3.36459327, 3.8208859 , 0.64785709,\n",
       "        3.61881227, 3.9227736 , 1.18591995, 4.20571723, 4.35704689,\n",
       "        1.41681819, 4.31340957, 3.46157675, 0.38593216, 3.42484503,\n",
       "        3.70048232, 1.00566497, 3.55005307, 4.04337735, 0.69219532,\n",
       "        3.91896996, 4.10432978, 1.09334044, 4.0124505 , 3.37370067,\n",
       "        2.20609732, 3.49620662, 3.67355943, 3.5790782 , 3.75633049,\n",
       "        4.0302659 , 3.95450044, 4.27827997, 4.48395581, 4.32391825,\n",
       "        4.30099821, 3.35398393, 2.73905544, 3.45567183, 3.75833206,\n",
       "        3.72350183, 3.725704  , 4.07010055, 3.95810409, 4.134656  ,\n",
       "        4.4047883 , 4.33212533, 4.37225986, 3.70118275, 2.95554171,\n",
       "        3.50661621, 3.83840127, 3.80277081, 4.21472473, 4.15397243,\n",
       "        4.29669528, 4.23534293, 4.45743356, 4.38206916, 4.60375834,\n",
       "        3.50481348, 3.37820539, 3.32485924, 3.58392262, 3.63992991,\n",
       "        3.82198668, 4.29249182, 4.2095202 , 4.32642088, 4.55161448,\n",
       "        4.41209431, 4.44001865, 3.79416289, 3.82078581, 3.82198706,\n",
       "        3.94889579, 4.0855134 , 4.0830112 , 4.41769905, 4.40098505,\n",
       "        4.43081083, 4.65780549, 4.57293229, 4.59375043, 3.68556862,\n",
       "        3.69798031, 3.85061159, 4.22443275, 4.06569667, 3.98102384,\n",
       "        4.22393312, 4.20161338, 4.18379774, 4.46734281, 4.37256031,\n",
       "        4.24094725, 3.63102212, 3.5037128 , 3.47949257, 3.69417663,\n",
       "        3.66074824, 3.71339374, 4.0195569 , 3.98662891, 4.08811584,\n",
       "        4.26426735, 4.20982051, 4.1849988 , 3.48229399, 3.41533661,\n",
       "        3.42454443, 3.66074834, 3.70618725, 3.79296179, 4.02776384,\n",
       "        4.28628621, 4.21732702, 4.24154716, 4.22183065, 4.15627418,\n",
       "        3.5585597 , 3.5715713 , 3.58538332, 3.81397986, 3.88834391,\n",
       "        3.85741715, 4.21002092, 4.28678651, 4.27117338, 4.50947795,\n",
       "        4.47654953, 4.5418057 , 3.56076183, 3.54705014, 3.52252936,\n",
       "        3.80437212, 3.862922  , 3.8591186 , 4.24434958, 4.21222224,\n",
       "        4.3043016 , 4.52362413, 4.49076176, 4.45933499, 3.57067051,\n",
       "        3.56606655, 3.54044476, 3.87373123, 4.05018291, 3.99173322,\n",
       "        4.40178599, 4.34313478, 4.45042734, 4.68783126, 4.83085456,\n",
       "        4.80493212, 3.76583829, 3.68316736, 3.60229793, 3.8330966 ,\n",
       "        3.84210434, 3.90235577, 4.25616794, 4.24625134, 4.26877084,\n",
       "        4.37986674, 3.93338189, 3.4382565 ]),\n",
       " 'std_fit_time': array([0.02835863, 0.04576203, 0.04072309, 0.02053437, 0.06918268,\n",
       "        0.09097868, 0.13133981, 0.07510668, 0.08334126, 0.09788362,\n",
       "        0.09890495, 0.06191554, 0.08209525, 0.03289989, 0.07791581,\n",
       "        0.07395557, 0.12552015, 0.02721782, 0.09713869, 0.05419148,\n",
       "        0.071957  , 0.10450216, 0.07264432, 0.07693776, 0.07192721,\n",
       "        0.02533864, 0.07470117, 0.02535622, 0.08073677, 0.03564172,\n",
       "        0.06563365, 0.03645525, 0.08143463, 0.05140876, 0.03175782,\n",
       "        0.07480754, 0.02961364, 0.02287799, 0.0447443 , 0.07370529,\n",
       "        0.04463098, 0.03347564, 0.08658772, 0.1163895 , 0.08363755,\n",
       "        0.06551063, 0.13056666, 0.11319934, 0.09216667, 0.04393673,\n",
       "        0.05810869, 0.07191413, 0.08443916, 0.13487756, 0.10180572,\n",
       "        0.11712937, 0.13200207, 0.11277239, 0.04803978, 0.06128068,\n",
       "        0.05598729, 0.06025516, 0.05242721, 0.07228516, 0.07918584,\n",
       "        0.02980294, 0.10257156, 0.02772235, 0.06205726, 0.05946098,\n",
       "        0.10410508, 0.09183213, 0.05041492, 0.04905944, 0.07213454,\n",
       "        0.04409337, 0.08109041, 0.10206524, 0.13454805, 0.10456915,\n",
       "        0.07489244, 0.2079849 , 0.09205023, 0.16541121, 0.14144919,\n",
       "        0.02906726, 0.04516483, 0.04595199, 0.00823711, 0.0323186 ,\n",
       "        0.08643973, 0.06864739, 0.18225525, 0.09564413, 0.02965306,\n",
       "        0.03017483, 0.17332628, 0.07294978, 0.1529494 , 0.24773416,\n",
       "        0.23193439, 0.16804399, 0.12802223, 0.3554368 , 0.11875145,\n",
       "        0.04202509, 0.34568348, 0.11741978, 0.08724684, 0.14557716,\n",
       "        0.05420713, 0.17999099, 0.28542459, 0.06054816, 0.1284044 ,\n",
       "        0.30972097, 0.17569729, 0.21067593, 0.67936376, 0.21670617,\n",
       "        0.10779979, 0.03200974, 0.11093163, 0.11717744, 0.23358744,\n",
       "        0.1547421 , 0.17651391, 0.49836508, 0.29222214, 0.2308577 ,\n",
       "        0.74616487, 0.1737474 , 0.10405637, 0.08471545, 0.22856607,\n",
       "        0.03067372, 0.89307334, 0.12769723, 0.12639232, 0.21067253,\n",
       "        0.15209266, 0.13111188, 0.79139334, 0.08093284, 0.05143031,\n",
       "        0.95845571, 0.14754712, 0.17806985, 0.39640471, 0.06171615,\n",
       "        0.09671235, 0.10049056, 0.14442352, 0.11451257, 0.08489123,\n",
       "        0.1550927 , 0.06514016, 0.83226553, 0.11247733, 0.20631857,\n",
       "        0.10097636, 0.06448592, 0.09150008, 0.09223737, 0.077777  ,\n",
       "        0.18129683, 0.1537231 , 0.09802121, 0.12796591, 0.65877099,\n",
       "        0.16440791, 0.29537783, 0.20391474, 0.3950989 , 0.25232704,\n",
       "        0.1115154 , 0.22643729, 0.28300319, 0.17160719, 0.15306802,\n",
       "        0.15048653, 0.24819225, 0.12767302, 0.07254519, 0.091112  ,\n",
       "        0.11561524, 0.17656024, 0.13612253, 0.13618679, 0.20239127,\n",
       "        0.18167935, 0.18859278, 0.07814729, 0.11369962, 0.13533441,\n",
       "        0.20555175, 0.15323253, 0.17581072, 0.07167794, 0.14552404,\n",
       "        0.25408399, 0.13470827, 0.23495088, 0.12904746, 0.07662086,\n",
       "        0.14125355, 0.17720067, 0.1342253 , 0.15828037, 0.13840931,\n",
       "        0.10150691, 0.08910838, 0.08124428, 0.09985958, 0.11369057,\n",
       "        0.13423327, 0.12193511, 0.13071755, 0.1206006 , 0.056335  ,\n",
       "        0.04226129, 0.0350067 , 0.06536463, 0.03936953, 0.04522329,\n",
       "        0.07832876, 0.08086897, 0.07042778, 0.03523217, 0.02294017,\n",
       "        0.05286375, 0.04524887, 0.04639506, 0.06621072, 0.02836604,\n",
       "        0.18726018, 0.18199157, 0.04107609, 0.05028214, 0.01182764,\n",
       "        0.02572586, 0.02817541, 0.06192018, 0.02434746, 0.07047439,\n",
       "        0.02154797, 0.05265097, 0.08079544, 0.05453784, 0.07214758,\n",
       "        0.05448805, 0.05904714, 0.05605411, 0.03894645, 0.03713564,\n",
       "        0.04035813, 0.05717808, 0.03364835, 0.0656084 , 0.01922188,\n",
       "        0.10591031, 0.07402955, 0.03220412, 0.09238513, 0.08030871,\n",
       "        0.09033848, 0.06867185, 0.07343015, 0.11141554, 0.07560985,\n",
       "        0.07997255, 0.03478409, 0.17446581, 0.12583986, 0.21350612,\n",
       "        0.15145893, 0.117845  , 0.07187261, 0.04319098, 0.04904266,\n",
       "        0.04232749, 0.08988808, 0.07500053, 0.05885625, 0.04761238,\n",
       "        0.08707248, 0.18740884, 0.13059192]),\n",
       " 'mean_score_time': array([0.03773341, 0.04603963, 0.04023547, 0.03953385, 0.03933411,\n",
       "        0.04103584, 0.04533744, 0.04613938, 0.05294557, 0.04914312,\n",
       "        0.04173594, 0.04343705, 0.04474006, 0.03973436, 0.04053445,\n",
       "        0.04684057, 0.04313645, 0.03923473, 0.05104351, 0.0471385 ,\n",
       "        0.05074506, 0.05124526, 0.04974322, 0.04774055, 0.0479414 ,\n",
       "        0.04013352, 0.03863316, 0.04243598, 0.04684086, 0.04884276,\n",
       "        0.05504932, 0.04954209, 0.05714636, 0.04984245, 0.04844117,\n",
       "        0.03973475, 0.0383327 , 0.04253621, 0.04083495, 0.03913584,\n",
       "        0.05094423, 0.0473402 , 0.0587533 , 0.05544825, 0.05774908,\n",
       "        0.05444708, 0.04643927, 0.03903346, 0.04463768, 0.04413953,\n",
       "        0.04614034, 0.04844122, 0.04733906, 0.04333615, 0.05354676,\n",
       "        0.05034332, 0.04904385, 0.0529448 , 0.05314479, 0.04053469,\n",
       "        0.04253688, 0.04343815, 0.04333711, 0.04784169, 0.04013529,\n",
       "        0.04593768, 0.06155348, 0.05014415, 0.04854169, 0.04894142,\n",
       "        0.04443903, 0.04443817, 0.04283528, 0.03953366, 0.0427371 ,\n",
       "        0.05324721, 0.04543915, 0.05264549, 0.05234451, 0.05654945,\n",
       "        0.04974322, 0.05244651, 0.04754076, 0.0403348 , 0.04213657,\n",
       "        0.03793211, 0.03823438, 0.04083395, 0.039534  , 0.04704146,\n",
       "        0.0521431 , 0.05694938, 0.05094438, 0.05054469, 0.04734073,\n",
       "        0.04353786, 0.04413767, 0.04463744, 0.04874229, 0.03993487,\n",
       "        0.03693166, 0.03693204, 0.03823285, 0.04433794, 0.0364315 ,\n",
       "        0.04213672, 0.03703213, 0.044238  , 0.03953447, 0.03763285,\n",
       "        0.03823309, 0.04553957, 0.03803253, 0.03733211, 0.04764123,\n",
       "        0.03813272, 0.03973413, 0.04483886, 0.03833342, 0.04563913,\n",
       "        0.04143581, 0.03903351, 0.04133573, 0.03913388, 0.04203606,\n",
       "        0.04443836, 0.05174484, 0.03733234, 0.04313717, 0.05074372,\n",
       "        0.04093537, 0.04033446, 0.03733211, 0.04183583, 0.03983479,\n",
       "        0.04073524, 0.03693171, 0.04063511, 0.04223638, 0.03873358,\n",
       "        0.04033465, 0.03863344, 0.03923368, 0.04603915, 0.04143596,\n",
       "        0.04033456, 0.04453807, 0.0359313 , 0.0362309 , 0.04193597,\n",
       "        0.0391336 , 0.04894233, 0.04083495, 0.04013515, 0.03673129,\n",
       "        0.03633151, 0.04173613, 0.04814124, 0.04263706, 0.04023452,\n",
       "        0.03673186, 0.03723197, 0.03753238, 0.04614029, 0.04744115,\n",
       "        0.03783259, 0.04373779, 0.03763242, 0.03963404, 0.03623137,\n",
       "        0.04633956, 0.04123573, 0.03863339, 0.04563909, 0.04273696,\n",
       "        0.04473815, 0.04013391, 0.04824128, 0.04503818, 0.04453878,\n",
       "        0.03673182, 0.04764099, 0.04023499, 0.04289632, 0.05014348,\n",
       "        0.04313726, 0.04323711, 0.04693999, 0.04443808, 0.0417357 ,\n",
       "        0.04173622, 0.03983397, 0.04033489, 0.04253669, 0.03763223,\n",
       "        0.03703184, 0.04293776, 0.04313712, 0.04183578, 0.04774079,\n",
       "        0.04824142, 0.04734106, 0.04243646, 0.04003453, 0.04463849,\n",
       "        0.04103503, 0.04203625, 0.04543934, 0.03943419, 0.04013476,\n",
       "        0.0432373 , 0.04143572, 0.03663177, 0.03643045, 0.03893328,\n",
       "        0.03813281, 0.03603134, 0.03933415, 0.03503041, 0.038133  ,\n",
       "        0.03583097, 0.03783236, 0.03823314, 0.04653997, 0.03723187,\n",
       "        0.03803277, 0.03673172, 0.03563089, 0.03623118, 0.03523064,\n",
       "        0.03553076, 0.03593087, 0.0368319 , 0.05614843, 0.03833318,\n",
       "        0.03653183, 0.03773251, 0.03903432, 0.03753219, 0.035531  ,\n",
       "        0.03613143, 0.04363775, 0.03653164, 0.03853292, 0.03703198,\n",
       "        0.03593111, 0.03703218, 0.04373813, 0.03993454, 0.03893313,\n",
       "        0.0366313 , 0.04533911, 0.04243665, 0.0415359 , 0.03693175,\n",
       "        0.03633075, 0.0376328 , 0.03603182, 0.03963437, 0.03703184,\n",
       "        0.03913379, 0.03759871, 0.03753257, 0.04213634, 0.03613133,\n",
       "        0.0368319 , 0.03673191, 0.04573965, 0.03963451, 0.03823285,\n",
       "        0.03613076, 0.03903403, 0.05374641, 0.04063544, 0.04113545,\n",
       "        0.03773274, 0.0373322 , 0.0376327 , 0.03553047, 0.0363317 ,\n",
       "        0.03763204, 0.03623176, 0.0392262 , 0.03723183, 0.04243646,\n",
       "        0.03272839, 0.02542219, 0.02111821]),\n",
       " 'std_score_time': array([0.00150457, 0.00893509, 0.0041221 , 0.00247217, 0.00294391,\n",
       "        0.00274203, 0.00256179, 0.00584874, 0.00732358, 0.00711502,\n",
       "        0.00068055, 0.00321796, 0.00785343, 0.00560546, 0.00243182,\n",
       "        0.00898806, 0.00586556, 0.00301213, 0.00700854, 0.00419093,\n",
       "        0.00919887, 0.00817817, 0.0117918 , 0.00843695, 0.01560014,\n",
       "        0.00643503, 0.00313984, 0.00538474, 0.00537648, 0.00529996,\n",
       "        0.01046318, 0.00701345, 0.013275  , 0.00792165, 0.01823795,\n",
       "        0.00153782, 0.00434659, 0.00392775, 0.00765209, 0.00246013,\n",
       "        0.01123161, 0.00377738, 0.00383247, 0.01067634, 0.00482794,\n",
       "        0.00636439, 0.00476284, 0.00293544, 0.00860028, 0.01678132,\n",
       "        0.01717215, 0.00957273, 0.01014153, 0.00424189, 0.00982691,\n",
       "        0.00699716, 0.00476646, 0.00361487, 0.01606802, 0.00137965,\n",
       "        0.00303667, 0.00591594, 0.01591153, 0.00673326, 0.0018284 ,\n",
       "        0.00615771, 0.01772657, 0.00599159, 0.0063538 , 0.00365395,\n",
       "        0.00888128, 0.00685687, 0.00354969, 0.0054986 , 0.01134311,\n",
       "        0.01654241, 0.00233441, 0.01444692, 0.00998142, 0.01615398,\n",
       "        0.00989113, 0.00675426, 0.00534147, 0.00297871, 0.00533769,\n",
       "        0.0022466 , 0.00381871, 0.00398642, 0.00266643, 0.00628994,\n",
       "        0.01133862, 0.01008533, 0.00733521, 0.00889632, 0.00435881,\n",
       "        0.00749593, 0.00986439, 0.00990404, 0.00724885, 0.00422756,\n",
       "        0.00213263, 0.00086099, 0.00254324, 0.01098028, 0.00086076,\n",
       "        0.00692272, 0.00301903, 0.00552336, 0.00321209, 0.00271186,\n",
       "        0.00479595, 0.00738158, 0.00141536, 0.00297929, 0.01100195,\n",
       "        0.00473062, 0.00254403, 0.00568445, 0.00278838, 0.00804706,\n",
       "        0.00464542, 0.00522954, 0.00634277, 0.0035582 , 0.00746991,\n",
       "        0.0048456 , 0.01337426, 0.00194098, 0.00677657, 0.00923637,\n",
       "        0.00744494, 0.00527318, 0.00229533, 0.00992103, 0.00470042,\n",
       "        0.0055419 , 0.00355826, 0.00476215, 0.00426553, 0.00525439,\n",
       "        0.00740653, 0.00451377, 0.00196652, 0.01432355, 0.00661164,\n",
       "        0.00797908, 0.00800661, 0.00073551, 0.00196649, 0.0042036 ,\n",
       "        0.00379015, 0.00924057, 0.00294479, 0.00253991, 0.00107771,\n",
       "        0.00051026, 0.00683676, 0.00805305, 0.00534705, 0.00568446,\n",
       "        0.00196625, 0.00160103, 0.00114087, 0.00966439, 0.00977283,\n",
       "        0.00188799, 0.00574607, 0.00159516, 0.00435606, 0.00074932,\n",
       "        0.00628701, 0.00633482, 0.0034589 , 0.0079277 , 0.00620691,\n",
       "        0.00987547, 0.00360044, 0.01068883, 0.00489335, 0.01028978,\n",
       "        0.00060047, 0.01262208, 0.00835343, 0.00742415, 0.00420364,\n",
       "        0.00422746, 0.00947146, 0.00922975, 0.00569191, 0.00186174,\n",
       "        0.00694525, 0.00262138, 0.00443776, 0.00579266, 0.00521405,\n",
       "        0.00134307, 0.00572674, 0.00667938, 0.00404842, 0.00826884,\n",
       "        0.0072632 , 0.01124181, 0.00539328, 0.00447563, 0.00872749,\n",
       "        0.00396595, 0.00592083, 0.00819496, 0.00190993, 0.00789599,\n",
       "        0.00661327, 0.00594953, 0.00086107, 0.0005837 , 0.00400819,\n",
       "        0.00408254, 0.00083731, 0.00787216, 0.00083765, 0.00257922,\n",
       "        0.00067905, 0.00244324, 0.00360333, 0.01131939, 0.00092827,\n",
       "        0.00288403, 0.00103064, 0.00073569, 0.00225105, 0.00081283,\n",
       "        0.00130501, 0.00073536, 0.00291105, 0.01501427, 0.00240214,\n",
       "        0.00063358, 0.00180691, 0.00331928, 0.00270453, 0.00130466,\n",
       "        0.00193557, 0.01238527, 0.00226045, 0.00455323, 0.00173377,\n",
       "        0.00159475, 0.00070733, 0.01234147, 0.00588242, 0.00364185,\n",
       "        0.00058435, 0.0143951 , 0.00758513, 0.00905754, 0.0035726 ,\n",
       "        0.00116735, 0.00255969, 0.00104985, 0.00745875, 0.00266655,\n",
       "        0.00312366, 0.00102617, 0.00109641, 0.00719919, 0.00111436,\n",
       "        0.0012895 , 0.00271514, 0.00440405, 0.00477222, 0.00291144,\n",
       "        0.00111474, 0.00353872, 0.01066551, 0.00670193, 0.00425086,\n",
       "        0.00250418, 0.00074888, 0.00339999, 0.00070776, 0.00186154,\n",
       "        0.00206082, 0.00098025, 0.00380737, 0.00098046, 0.00496794,\n",
       "        0.0039095 , 0.00397115, 0.00058372]),\n",
       " 'param_classifier': masked_array(data=[MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__activation': masked_array(data=['logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__alpha': masked_array(data=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__hidden_layer_sizes': masked_array(data=[(5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__learning_rate': masked_array(data=['constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'}],\n",
       " 'split0_test_recall_micro': array([0.927, 0.922, 0.92 , 0.914, 0.929, 0.916, 0.906, 0.92 , 0.915,\n",
       "        0.917, 0.927, 0.914, 0.916, 0.932, 0.923, 0.921, 0.917, 0.918,\n",
       "        0.931, 0.92 , 0.916, 0.923, 0.919, 0.923, 0.922, 0.909, 0.915,\n",
       "        0.927, 0.912, 0.919, 0.915, 0.921, 0.921, 0.912, 0.909, 0.918,\n",
       "        0.923, 0.921, 0.92 , 0.925, 0.92 , 0.918, 0.927, 0.924, 0.927,\n",
       "        0.923, 0.926, 0.922, 0.923, 0.92 , 0.92 , 0.922, 0.927, 0.921,\n",
       "        0.92 , 0.917, 0.915, 0.909, 0.92 , 0.915, 0.916, 0.917, 0.915,\n",
       "        0.922, 0.918, 0.92 , 0.918, 0.923, 0.911, 0.918, 0.917, 0.912,\n",
       "        0.92 , 0.922, 0.921, 0.934, 0.922, 0.921, 0.927, 0.904, 0.924,\n",
       "        0.919, 0.914, 0.917, 0.926, 0.923, 0.922, 0.916, 0.922, 0.92 ,\n",
       "        0.925, 0.919, 0.916, 0.92 , 0.92 , 0.91 , 0.748, 0.252, 0.734,\n",
       "        0.819, 0.163, 0.813, 0.806, 0.199, 0.875, 0.877, 0.158, 0.88 ,\n",
       "        0.784, 0.149, 0.745, 0.797, 0.158, 0.816, 0.862, 0.101, 0.83 ,\n",
       "        0.855, 0.295, 0.875, 0.758, 0.119, 0.712, 0.813, 0.113, 0.801,\n",
       "        0.866, 0.198, 0.861, 0.88 , 0.253, 0.872, 0.672, 0.06 , 0.758,\n",
       "        0.81 , 0.198, 0.791, 0.863, 0.296, 0.838, 0.851, 0.198, 0.883,\n",
       "        0.911, 0.28 , 0.896, 0.909, 0.372, 0.915, 0.91 , 0.242, 0.916,\n",
       "        0.914, 0.379, 0.923, 0.908, 0.157, 0.857, 0.912, 0.213, 0.909,\n",
       "        0.915, 0.329, 0.91 , 0.915, 0.34 , 0.914, 0.84 , 0.323, 0.905,\n",
       "        0.919, 0.403, 0.899, 0.918, 0.213, 0.912, 0.914, 0.36 , 0.918,\n",
       "        0.864, 0.23 , 0.9  , 0.907, 0.245, 0.915, 0.916, 0.357, 0.921,\n",
       "        0.914, 0.319, 0.907, 0.859, 0.859, 0.859, 0.91 , 0.913, 0.914,\n",
       "        0.916, 0.913, 0.914, 0.915, 0.916, 0.914, 0.856, 0.854, 0.881,\n",
       "        0.91 , 0.91 , 0.913, 0.908, 0.913, 0.911, 0.914, 0.915, 0.914,\n",
       "        0.871, 0.884, 0.897, 0.913, 0.921, 0.911, 0.915, 0.912, 0.914,\n",
       "        0.912, 0.911, 0.915, 0.91 , 0.908, 0.901, 0.917, 0.911, 0.911,\n",
       "        0.917, 0.91 , 0.915, 0.918, 0.914, 0.918, 0.909, 0.913, 0.909,\n",
       "        0.913, 0.92 , 0.915, 0.916, 0.923, 0.922, 0.911, 0.921, 0.921,\n",
       "        0.91 , 0.909, 0.915, 0.922, 0.915, 0.921, 0.922, 0.921, 0.912,\n",
       "        0.917, 0.92 , 0.918, 0.912, 0.907, 0.908, 0.92 , 0.923, 0.921,\n",
       "        0.918, 0.914, 0.914, 0.922, 0.919, 0.917, 0.907, 0.905, 0.906,\n",
       "        0.919, 0.914, 0.917, 0.92 , 0.919, 0.919, 0.918, 0.921, 0.919]),\n",
       " 'split1_test_recall_micro': array([0.937, 0.943, 0.938, 0.937, 0.942, 0.937, 0.933, 0.932, 0.93 ,\n",
       "        0.937, 0.931, 0.932, 0.936, 0.94 , 0.939, 0.938, 0.938, 0.94 ,\n",
       "        0.936, 0.939, 0.938, 0.938, 0.931, 0.929, 0.933, 0.935, 0.94 ,\n",
       "        0.943, 0.943, 0.94 , 0.936, 0.934, 0.928, 0.935, 0.934, 0.928,\n",
       "        0.937, 0.938, 0.931, 0.941, 0.943, 0.935, 0.941, 0.93 , 0.933,\n",
       "        0.939, 0.931, 0.931, 0.945, 0.932, 0.94 , 0.936, 0.941, 0.937,\n",
       "        0.937, 0.944, 0.935, 0.936, 0.929, 0.934, 0.942, 0.941, 0.939,\n",
       "        0.935, 0.933, 0.941, 0.94 , 0.936, 0.933, 0.931, 0.929, 0.929,\n",
       "        0.942, 0.938, 0.931, 0.939, 0.937, 0.94 , 0.929, 0.935, 0.93 ,\n",
       "        0.931, 0.93 , 0.929, 0.937, 0.939, 0.946, 0.94 , 0.945, 0.937,\n",
       "        0.937, 0.939, 0.935, 0.933, 0.936, 0.934, 0.718, 0.261, 0.782,\n",
       "        0.809, 0.129, 0.821, 0.861, 0.252, 0.837, 0.877, 0.275, 0.886,\n",
       "        0.726, 0.198, 0.789, 0.823, 0.268, 0.809, 0.835, 0.274, 0.875,\n",
       "        0.882, 0.202, 0.882, 0.763, 0.198, 0.698, 0.806, 0.198, 0.822,\n",
       "        0.841, 0.113, 0.844, 0.884, 0.254, 0.887, 0.745, 0.28 , 0.73 ,\n",
       "        0.825, 0.129, 0.823, 0.843, 0.071, 0.875, 0.871, 0.252, 0.889,\n",
       "        0.851, 0.228, 0.9  , 0.91 , 0.137, 0.907, 0.911, 0.34 , 0.915,\n",
       "        0.914, 0.231, 0.917, 0.912, 0.316, 0.897, 0.917, 0.336, 0.906,\n",
       "        0.913, 0.342, 0.914, 0.917, 0.374, 0.918, 0.849, 0.412, 0.847,\n",
       "        0.907, 0.043, 0.912, 0.917, 0.427, 0.913, 0.91 , 0.39 , 0.906,\n",
       "        0.911, 0.184, 0.857, 0.913, 0.339, 0.915, 0.915, 0.281, 0.911,\n",
       "        0.913, 0.398, 0.918, 0.868, 0.86 , 0.862, 0.918, 0.921, 0.919,\n",
       "        0.926, 0.92 , 0.925, 0.924, 0.923, 0.922, 0.87 , 0.869, 0.909,\n",
       "        0.917, 0.919, 0.917, 0.923, 0.923, 0.924, 0.922, 0.924, 0.926,\n",
       "        0.868, 0.869, 0.904, 0.921, 0.915, 0.922, 0.921, 0.923, 0.925,\n",
       "        0.923, 0.924, 0.922, 0.862, 0.86 , 0.9  , 0.918, 0.915, 0.919,\n",
       "        0.921, 0.917, 0.922, 0.919, 0.921, 0.919, 0.92 , 0.918, 0.922,\n",
       "        0.922, 0.93 , 0.93 , 0.933, 0.927, 0.932, 0.934, 0.928, 0.927,\n",
       "        0.912, 0.92 , 0.913, 0.932, 0.931, 0.932, 0.928, 0.925, 0.928,\n",
       "        0.933, 0.926, 0.926, 0.924, 0.915, 0.914, 0.925, 0.924, 0.923,\n",
       "        0.932, 0.931, 0.925, 0.926, 0.927, 0.926, 0.922, 0.921, 0.859,\n",
       "        0.927, 0.93 , 0.924, 0.923, 0.93 , 0.927, 0.926, 0.931, 0.922]),\n",
       " 'split2_test_recall_micro': array([0.911, 0.921, 0.923, 0.93 , 0.928, 0.93 , 0.919, 0.917, 0.931,\n",
       "        0.907, 0.927, 0.926, 0.922, 0.922, 0.925, 0.931, 0.926, 0.928,\n",
       "        0.921, 0.926, 0.93 , 0.917, 0.927, 0.919, 0.926, 0.929, 0.923,\n",
       "        0.923, 0.92 , 0.928, 0.916, 0.921, 0.926, 0.926, 0.924, 0.92 ,\n",
       "        0.918, 0.921, 0.921, 0.929, 0.929, 0.923, 0.924, 0.924, 0.917,\n",
       "        0.918, 0.92 , 0.913, 0.925, 0.921, 0.923, 0.921, 0.92 , 0.924,\n",
       "        0.929, 0.928, 0.924, 0.914, 0.922, 0.918, 0.926, 0.919, 0.918,\n",
       "        0.924, 0.917, 0.936, 0.929, 0.928, 0.934, 0.926, 0.928, 0.916,\n",
       "        0.924, 0.918, 0.923, 0.923, 0.918, 0.931, 0.921, 0.923, 0.927,\n",
       "        0.918, 0.923, 0.921, 0.923, 0.926, 0.917, 0.926, 0.928, 0.92 ,\n",
       "        0.924, 0.923, 0.918, 0.933, 0.921, 0.925, 0.728, 0.178, 0.76 ,\n",
       "        0.795, 0.044, 0.808, 0.859, 0.223, 0.842, 0.872, 0.308, 0.87 ,\n",
       "        0.73 , 0.252, 0.773, 0.793, 0.099, 0.795, 0.839, 0.169, 0.867,\n",
       "        0.879, 0.114, 0.874, 0.728, 0.119, 0.721, 0.8  , 0.03 , 0.812,\n",
       "        0.864, 0.333, 0.854, 0.866, 0.124, 0.863, 0.742, 0.111, 0.755,\n",
       "        0.808, 0.184, 0.787, 0.86 , 0.204, 0.862, 0.874, 0.198, 0.868,\n",
       "        0.885, 0.255, 0.896, 0.904, 0.254, 0.887, 0.908, 0.101, 0.897,\n",
       "        0.906, 0.468, 0.9  , 0.86 , 0.208, 0.882, 0.9  , 0.193, 0.895,\n",
       "        0.901, 0.268, 0.9  , 0.903, 0.273, 0.904, 0.901, 0.284, 0.898,\n",
       "        0.902, 0.164, 0.9  , 0.904, 0.027, 0.907, 0.903, 0.085, 0.904,\n",
       "        0.886, 0.201, 0.896, 0.902, 0.265, 0.905, 0.907, 0.431, 0.902,\n",
       "        0.903, 0.138, 0.904, 0.903, 0.857, 0.864, 0.906, 0.907, 0.909,\n",
       "        0.91 , 0.908, 0.908, 0.911, 0.908, 0.911, 0.816, 0.862, 0.859,\n",
       "        0.908, 0.908, 0.907, 0.907, 0.911, 0.907, 0.909, 0.913, 0.912,\n",
       "        0.856, 0.858, 0.854, 0.912, 0.908, 0.909, 0.912, 0.914, 0.917,\n",
       "        0.913, 0.91 , 0.912, 0.903, 0.855, 0.857, 0.914, 0.908, 0.903,\n",
       "        0.906, 0.903, 0.912, 0.907, 0.91 , 0.908, 0.9  , 0.903, 0.898,\n",
       "        0.922, 0.914, 0.91 , 0.923, 0.912, 0.918, 0.915, 0.918, 0.917,\n",
       "        0.9  , 0.903, 0.903, 0.917, 0.919, 0.918, 0.921, 0.911, 0.917,\n",
       "        0.921, 0.911, 0.917, 0.901, 0.897, 0.914, 0.921, 0.916, 0.92 ,\n",
       "        0.913, 0.916, 0.922, 0.916, 0.914, 0.918, 0.902, 0.904, 0.9  ,\n",
       "        0.918, 0.908, 0.915, 0.913, 0.92 , 0.917, 0.913, 0.914, 0.918]),\n",
       " 'split3_test_recall_micro': array([0.931, 0.926, 0.929, 0.933, 0.935, 0.927, 0.929, 0.933, 0.928,\n",
       "        0.925, 0.926, 0.929, 0.934, 0.931, 0.933, 0.933, 0.931, 0.931,\n",
       "        0.927, 0.931, 0.936, 0.934, 0.93 , 0.93 , 0.927, 0.938, 0.935,\n",
       "        0.933, 0.932, 0.937, 0.932, 0.931, 0.931, 0.939, 0.928, 0.919,\n",
       "        0.933, 0.931, 0.932, 0.933, 0.932, 0.936, 0.938, 0.938, 0.936,\n",
       "        0.931, 0.934, 0.942, 0.931, 0.928, 0.937, 0.927, 0.937, 0.931,\n",
       "        0.931, 0.933, 0.934, 0.925, 0.921, 0.925, 0.936, 0.931, 0.931,\n",
       "        0.93 , 0.933, 0.929, 0.926, 0.924, 0.927, 0.928, 0.924, 0.927,\n",
       "        0.932, 0.933, 0.929, 0.935, 0.929, 0.932, 0.932, 0.925, 0.93 ,\n",
       "        0.93 , 0.925, 0.924, 0.937, 0.933, 0.929, 0.929, 0.936, 0.931,\n",
       "        0.927, 0.926, 0.931, 0.931, 0.924, 0.927, 0.773, 0.298, 0.739,\n",
       "        0.806, 0.106, 0.775, 0.829, 0.158, 0.859, 0.87 , 0.158, 0.871,\n",
       "        0.746, 0.252, 0.744, 0.791, 0.09 , 0.784, 0.832, 0.255, 0.824,\n",
       "        0.869, 0.252, 0.872, 0.754, 0.171, 0.729, 0.825, 0.101, 0.798,\n",
       "        0.823, 0.258, 0.851, 0.871, 0.187, 0.885, 0.734, 0.064, 0.743,\n",
       "        0.832, 0.044, 0.814, 0.859, 0.112, 0.864, 0.88 , 0.156, 0.872,\n",
       "        0.865, 0.337, 0.889, 0.928, 0.202, 0.92 , 0.928, 0.168, 0.92 ,\n",
       "        0.925, 0.212, 0.918, 0.859, 0.146, 0.906, 0.92 , 0.35 , 0.914,\n",
       "        0.923, 0.323, 0.921, 0.923, 0.352, 0.92 , 0.895, 0.27 , 0.914,\n",
       "        0.924, 0.235, 0.914, 0.924, 0.249, 0.922, 0.921, 0.389, 0.927,\n",
       "        0.897, 0.231, 0.859, 0.909, 0.128, 0.921, 0.918, 0.464, 0.914,\n",
       "        0.923, 0.163, 0.922, 0.868, 0.841, 0.906, 0.923, 0.927, 0.927,\n",
       "        0.923, 0.922, 0.924, 0.923, 0.925, 0.925, 0.86 , 0.878, 0.865,\n",
       "        0.919, 0.926, 0.922, 0.928, 0.923, 0.93 , 0.927, 0.927, 0.925,\n",
       "        0.867, 0.861, 0.91 , 0.928, 0.927, 0.924, 0.922, 0.923, 0.927,\n",
       "        0.922, 0.924, 0.926, 0.872, 0.868, 0.917, 0.924, 0.921, 0.915,\n",
       "        0.924, 0.919, 0.923, 0.925, 0.927, 0.923, 0.916, 0.92 , 0.919,\n",
       "        0.927, 0.924, 0.924, 0.928, 0.93 , 0.93 , 0.927, 0.933, 0.928,\n",
       "        0.919, 0.914, 0.92 , 0.924, 0.928, 0.925, 0.931, 0.928, 0.928,\n",
       "        0.929, 0.928, 0.93 , 0.919, 0.923, 0.917, 0.931, 0.921, 0.93 ,\n",
       "        0.929, 0.934, 0.928, 0.931, 0.932, 0.933, 0.911, 0.918, 0.91 ,\n",
       "        0.923, 0.929, 0.929, 0.926, 0.928, 0.932, 0.927, 0.93 , 0.93 ]),\n",
       " 'split4_test_recall_micro': array([0.928, 0.929, 0.936, 0.927, 0.929, 0.928, 0.935, 0.922, 0.936,\n",
       "        0.918, 0.919, 0.931, 0.928, 0.924, 0.922, 0.922, 0.935, 0.925,\n",
       "        0.928, 0.927, 0.912, 0.931, 0.923, 0.914, 0.924, 0.928, 0.926,\n",
       "        0.932, 0.928, 0.933, 0.923, 0.931, 0.924, 0.927, 0.922, 0.922,\n",
       "        0.926, 0.93 , 0.928, 0.93 , 0.928, 0.931, 0.929, 0.92 , 0.931,\n",
       "        0.926, 0.919, 0.927, 0.932, 0.929, 0.927, 0.92 , 0.924, 0.927,\n",
       "        0.92 , 0.918, 0.914, 0.923, 0.929, 0.911, 0.925, 0.927, 0.928,\n",
       "        0.926, 0.929, 0.924, 0.926, 0.921, 0.924, 0.928, 0.923, 0.919,\n",
       "        0.925, 0.932, 0.924, 0.928, 0.919, 0.929, 0.936, 0.93 , 0.924,\n",
       "        0.926, 0.909, 0.923, 0.923, 0.925, 0.92 , 0.932, 0.928, 0.928,\n",
       "        0.922, 0.925, 0.925, 0.92 , 0.922, 0.929, 0.729, 0.105, 0.777,\n",
       "        0.813, 0.092, 0.787, 0.835, 0.129, 0.857, 0.868, 0.101, 0.873,\n",
       "        0.806, 0.197, 0.722, 0.819, 0.03 , 0.813, 0.864, 0.063, 0.841,\n",
       "        0.875, 0.112, 0.875, 0.743, 0.197, 0.736, 0.814, 0.256, 0.824,\n",
       "        0.858, 0.023, 0.873, 0.868, 0.256, 0.879, 0.801, 0.197, 0.744,\n",
       "        0.801, 0.197, 0.809, 0.835, 0.109, 0.862, 0.88 , 0.253, 0.873,\n",
       "        0.856, 0.085, 0.87 , 0.91 , 0.201, 0.91 , 0.918, 0.235, 0.913,\n",
       "        0.918, 0.17 , 0.917, 0.905, 0.067, 0.903, 0.913, 0.204, 0.918,\n",
       "        0.914, 0.335, 0.914, 0.916, 0.278, 0.912, 0.892, 0.243, 0.807,\n",
       "        0.917, 0.425, 0.914, 0.913, 0.432, 0.916, 0.915, 0.224, 0.911,\n",
       "        0.908, 0.103, 0.868, 0.914, 0.361, 0.916, 0.915, 0.348, 0.917,\n",
       "        0.918, 0.427, 0.915, 0.839, 0.898, 0.865, 0.915, 0.922, 0.92 ,\n",
       "        0.917, 0.916, 0.92 , 0.92 , 0.923, 0.921, 0.864, 0.865, 0.901,\n",
       "        0.918, 0.92 , 0.922, 0.924, 0.918, 0.912, 0.922, 0.922, 0.921,\n",
       "        0.912, 0.901, 0.867, 0.914, 0.915, 0.918, 0.923, 0.922, 0.924,\n",
       "        0.921, 0.921, 0.915, 0.865, 0.864, 0.887, 0.921, 0.921, 0.914,\n",
       "        0.918, 0.921, 0.918, 0.918, 0.918, 0.922, 0.903, 0.913, 0.916,\n",
       "        0.917, 0.923, 0.923, 0.928, 0.923, 0.918, 0.918, 0.919, 0.921,\n",
       "        0.914, 0.905, 0.909, 0.919, 0.922, 0.921, 0.914, 0.917, 0.916,\n",
       "        0.921, 0.921, 0.926, 0.915, 0.907, 0.921, 0.92 , 0.923, 0.919,\n",
       "        0.918, 0.923, 0.917, 0.925, 0.92 , 0.92 , 0.91 , 0.918, 0.907,\n",
       "        0.92 , 0.921, 0.916, 0.918, 0.917, 0.919, 0.921, 0.921, 0.923]),\n",
       " 'mean_test_recall_micro': array([0.9268, 0.9282, 0.9292, 0.9282, 0.9326, 0.9276, 0.9244, 0.9248,\n",
       "        0.928 , 0.9208, 0.926 , 0.9264, 0.9272, 0.9298, 0.9284, 0.929 ,\n",
       "        0.9294, 0.9284, 0.9286, 0.9286, 0.9264, 0.9286, 0.926 , 0.923 ,\n",
       "        0.9264, 0.9278, 0.9278, 0.9316, 0.927 , 0.9314, 0.9244, 0.9276,\n",
       "        0.926 , 0.9278, 0.9234, 0.9214, 0.9274, 0.9282, 0.9264, 0.9316,\n",
       "        0.9304, 0.9286, 0.9318, 0.9272, 0.9288, 0.9274, 0.926 , 0.927 ,\n",
       "        0.9312, 0.926 , 0.9294, 0.9252, 0.9298, 0.928 , 0.9274, 0.928 ,\n",
       "        0.9244, 0.9214, 0.9242, 0.9206, 0.929 , 0.927 , 0.9262, 0.9274,\n",
       "        0.926 , 0.93  , 0.9278, 0.9264, 0.9258, 0.9262, 0.9242, 0.9206,\n",
       "        0.9286, 0.9286, 0.9256, 0.9318, 0.925 , 0.9306, 0.929 , 0.9234,\n",
       "        0.927 , 0.9248, 0.9202, 0.9228, 0.9292, 0.9292, 0.9268, 0.9286,\n",
       "        0.9318, 0.9272, 0.927 , 0.9264, 0.925 , 0.9274, 0.9246, 0.925 ,\n",
       "        0.7392, 0.2188, 0.7584, 0.8084, 0.1068, 0.8008, 0.838 , 0.1922,\n",
       "        0.854 , 0.8728, 0.2   , 0.876 , 0.7584, 0.2096, 0.7546, 0.8046,\n",
       "        0.129 , 0.8034, 0.8464, 0.1724, 0.8474, 0.872 , 0.195 , 0.8756,\n",
       "        0.7492, 0.1608, 0.7192, 0.8116, 0.1396, 0.8114, 0.8504, 0.185 ,\n",
       "        0.8566, 0.8738, 0.2148, 0.8772, 0.7388, 0.1424, 0.746 , 0.8152,\n",
       "        0.1504, 0.8048, 0.852 , 0.1584, 0.8602, 0.8712, 0.2114, 0.877 ,\n",
       "        0.8736, 0.237 , 0.8902, 0.9122, 0.2332, 0.9078, 0.915 , 0.2172,\n",
       "        0.9122, 0.9154, 0.292 , 0.915 , 0.8888, 0.1788, 0.889 , 0.9124,\n",
       "        0.2592, 0.9084, 0.9132, 0.3194, 0.9118, 0.9148, 0.3234, 0.9136,\n",
       "        0.8754, 0.3064, 0.8742, 0.9138, 0.254 , 0.9078, 0.9152, 0.2696,\n",
       "        0.914 , 0.9126, 0.2896, 0.9132, 0.8932, 0.1898, 0.876 , 0.909 ,\n",
       "        0.2676, 0.9144, 0.9142, 0.3762, 0.913 , 0.9142, 0.289 , 0.9132,\n",
       "        0.8674, 0.863 , 0.8712, 0.9144, 0.918 , 0.9178, 0.9184, 0.9158,\n",
       "        0.9182, 0.9186, 0.919 , 0.9186, 0.8532, 0.8656, 0.883 , 0.9144,\n",
       "        0.9166, 0.9162, 0.918 , 0.9176, 0.9168, 0.9188, 0.9202, 0.9196,\n",
       "        0.8748, 0.8746, 0.8864, 0.9176, 0.9172, 0.9168, 0.9186, 0.9188,\n",
       "        0.9214, 0.9182, 0.918 , 0.918 , 0.8824, 0.871 , 0.8924, 0.9188,\n",
       "        0.9152, 0.9124, 0.9172, 0.914 , 0.918 , 0.9174, 0.918 , 0.918 ,\n",
       "        0.9096, 0.9134, 0.9128, 0.9202, 0.9222, 0.9204, 0.9256, 0.923 ,\n",
       "        0.924 , 0.921 , 0.9238, 0.9228, 0.911 , 0.9102, 0.912 , 0.9228,\n",
       "        0.923 , 0.9234, 0.9232, 0.9204, 0.9202, 0.9242, 0.9212, 0.9234,\n",
       "        0.9142, 0.9098, 0.9148, 0.9234, 0.9214, 0.9226, 0.922 , 0.9236,\n",
       "        0.9212, 0.924 , 0.9224, 0.9228, 0.9104, 0.9132, 0.8964, 0.9214,\n",
       "        0.9204, 0.9202, 0.92  , 0.9228, 0.9228, 0.921 , 0.9234, 0.9224]),\n",
       " 'std_test_recall_micro': array([0.00863481, 0.00793473, 0.00702567, 0.00783326, 0.00531413,\n",
       "        0.00677052, 0.01072567, 0.00649307, 0.00701427, 0.00992774,\n",
       "        0.00389872, 0.00652993, 0.00744043, 0.0064    , 0.00656049,\n",
       "        0.00654217, 0.00739189, 0.00722772, 0.00492341, 0.00628013,\n",
       "        0.01053755, 0.00760526, 0.00447214, 0.00603324, 0.00372022,\n",
       "        0.01010742, 0.00884081, 0.00674092, 0.01054514, 0.00739189,\n",
       "        0.00840476, 0.00549909, 0.00340588, 0.00928224, 0.00828493,\n",
       "        0.00355528, 0.00682935, 0.00649307, 0.005004  , 0.00535164,\n",
       "        0.0074458 , 0.00700286, 0.00655439, 0.00627375, 0.00658483,\n",
       "        0.00717217, 0.00589915, 0.00961249, 0.00770454, 0.00469042,\n",
       "        0.00781281, 0.0059127 , 0.00793473, 0.0055857 , 0.0065909 ,\n",
       "        0.01001998, 0.00895768, 0.00935094, 0.00396989, 0.00811419,\n",
       "        0.00907744, 0.00867179, 0.00874986, 0.00463033, 0.0070993 ,\n",
       "        0.00766812, 0.00711056, 0.00531413, 0.0082801 , 0.0044    ,\n",
       "        0.00426146, 0.00646838, 0.00773563, 0.00741889, 0.00377359,\n",
       "        0.0056356 , 0.00712741, 0.00608605, 0.00501996, 0.01055651,\n",
       "        0.00268328, 0.00541849, 0.00762627, 0.00391918, 0.0064622 ,\n",
       "        0.00594643, 0.01038075, 0.00783837, 0.0079599 , 0.00655439,\n",
       "        0.00525357, 0.00674092, 0.00729383, 0.00608605, 0.0058515 ,\n",
       "        0.00807465, 0.01948743, 0.06895042, 0.01937627, 0.00798999,\n",
       "        0.03953429, 0.01711607, 0.02041568, 0.04412437, 0.0134759 ,\n",
       "        0.00365513, 0.07825343, 0.00609918, 0.03140446, 0.03888753,\n",
       "        0.02361864, 0.01358823, 0.08047857, 0.01207642, 0.01374918,\n",
       "        0.08273234, 0.02018514, 0.00954987, 0.07314096, 0.00338231,\n",
       "        0.01248038, 0.03547619, 0.01328759, 0.00840476, 0.07895467,\n",
       "        0.01057544, 0.01628005, 0.10847119, 0.00985089, 0.00699714,\n",
       "        0.05236564, 0.00881816, 0.04099463, 0.08466073, 0.00993982,\n",
       "        0.01147868, 0.05889856, 0.01371714, 0.01099091, 0.08154165,\n",
       "        0.0121062 , 0.01068457, 0.0368977 , 0.00777174, 0.02201454,\n",
       "        0.08409281, 0.01070327, 0.00820731, 0.07869282, 0.0113031 ,\n",
       "        0.0073212 , 0.07991846, 0.00793473, 0.00618385, 0.11273864,\n",
       "        0.00782304, 0.02402832, 0.08213014, 0.01801111, 0.00682935,\n",
       "        0.06885753, 0.00786384, 0.00705408, 0.02646205, 0.00688186,\n",
       "        0.0065238 , 0.04063299, 0.00557136, 0.02555465, 0.05877619,\n",
       "        0.04087738, 0.00808455, 0.14451574, 0.00682349, 0.00661513,\n",
       "        0.15075225, 0.00493964, 0.00595315, 0.11923187, 0.00842378,\n",
       "        0.01705755, 0.04691865, 0.01838478, 0.0043359 , 0.08225229,\n",
       "        0.0052    , 0.00376298, 0.0647129 , 0.00641872, 0.00661513,\n",
       "        0.11874511, 0.00673498, 0.02071328, 0.01881489, 0.01752027,\n",
       "        0.00595315, 0.0070993 , 0.00604649, 0.00560714, 0.004996  ,\n",
       "        0.0064    , 0.00492341, 0.00629285, 0.00523832, 0.01916664,\n",
       "        0.00791454, 0.0195141 , 0.00449889, 0.00668132, 0.00570614,\n",
       "        0.00874071, 0.00496387, 0.00870402, 0.00643117, 0.00534416,\n",
       "        0.00567803, 0.01928108, 0.01598249, 0.02193262, 0.00608605,\n",
       "        0.0064    , 0.0059127 , 0.00431741, 0.00479166, 0.005004  ,\n",
       "        0.00470744, 0.00622896, 0.00517687, 0.02006589, 0.01899474,\n",
       "        0.02009577, 0.00342929, 0.00523068, 0.00535164, 0.00611228,\n",
       "        0.00663325, 0.00414729, 0.00581722, 0.00583095, 0.00532917,\n",
       "        0.00755248, 0.00588558, 0.00856505, 0.00479166, 0.00523068,\n",
       "        0.00705975, 0.00574804, 0.00609918, 0.00593296, 0.0083666 ,\n",
       "        0.00577581, 0.00411825, 0.00626099, 0.00617738, 0.00572713,\n",
       "        0.0051923 , 0.00583095, 0.00484149, 0.0059127 , 0.00598665,\n",
       "        0.00658483, 0.00587878, 0.0059127 , 0.00504381, 0.00773046,\n",
       "        0.00872697, 0.00426146, 0.00422374, 0.00287054, 0.00392938,\n",
       "        0.00723878, 0.00791454, 0.00511468, 0.00493964, 0.0063435 ,\n",
       "        0.00597997, 0.0065909 , 0.00719444, 0.01897999, 0.0032619 ,\n",
       "        0.00849941, 0.00541849, 0.00442719, 0.0051923 , 0.00574108,\n",
       "        0.00517687, 0.0063435 , 0.00422374]),\n",
       " 'rank_test_recall_micro': array([ 57,  32,  16,  32,   1,  42,  83,  81,  35, 123,  67,  59,  49,\n",
       "         12,  31,  21,  14,  30,  23,  23,  59,  23,  67, 100,  59,  38,\n",
       "         38,   5,  53,   7,  83,  43,  67,  38,  94, 114,  44,  32,  59,\n",
       "          6,  10,  23,   3,  49,  22,  44,  67,  53,   8,  71,  14,  76,\n",
       "         12,  35,  44,  35,  83, 114,  86, 124,  19,  53,  65,  44,  71,\n",
       "         11,  38,  64,  73,  65,  86, 124,  29,  23,  74,   2,  77,   9,\n",
       "         19,  94,  52,  80, 130, 103,  16,  16,  57,  23,   3,  49,  53,\n",
       "         59,  77,  44,  82,  77, 254, 270, 250, 244, 288, 248, 240, 277,\n",
       "        234, 224, 275, 215, 249, 274, 251, 246, 287, 247, 239, 281, 238,\n",
       "        225, 276, 217, 252, 282, 256, 242, 286, 243, 237, 279, 233, 222,\n",
       "        272, 213, 255, 285, 253, 241, 284, 245, 236, 283, 232, 226, 273,\n",
       "        214, 223, 268, 207, 191, 269, 202, 167, 271, 191, 164, 261, 167,\n",
       "        209, 280, 208, 189, 266, 201, 184, 259, 194, 169, 258, 180, 218,\n",
       "        260, 221, 179, 267, 202, 166, 264, 177, 188, 262, 182, 205, 278,\n",
       "        215, 200, 265, 171, 174, 257, 186, 174, 263, 182, 229, 231, 226,\n",
       "        172, 146, 153, 143, 163, 144, 140, 136, 140, 235, 230, 211, 172,\n",
       "        161, 162, 146, 154, 159, 137, 130, 135, 219, 220, 210, 154, 157,\n",
       "        160, 140, 137, 114, 144, 146, 146, 212, 228, 206, 137, 165, 189,\n",
       "        157, 177, 146, 156, 146, 146, 199, 181, 187, 130, 112, 126,  75,\n",
       "        100,  89, 121,  91, 104, 195, 197, 193, 104, 100,  93,  99, 126,\n",
       "        130,  86, 119,  94, 174, 198, 169,  94, 114, 109, 113,  92, 119,\n",
       "         89, 110, 104, 196, 184, 204, 114, 126, 129, 134, 104, 104, 121,\n",
       "         94, 110]),\n",
       " 'split0_test_f1_micro': array([0.927, 0.922, 0.92 , 0.914, 0.929, 0.916, 0.906, 0.92 , 0.915,\n",
       "        0.917, 0.927, 0.914, 0.916, 0.932, 0.923, 0.921, 0.917, 0.918,\n",
       "        0.931, 0.92 , 0.916, 0.923, 0.919, 0.923, 0.922, 0.909, 0.915,\n",
       "        0.927, 0.912, 0.919, 0.915, 0.921, 0.921, 0.912, 0.909, 0.918,\n",
       "        0.923, 0.921, 0.92 , 0.925, 0.92 , 0.918, 0.927, 0.924, 0.927,\n",
       "        0.923, 0.926, 0.922, 0.923, 0.92 , 0.92 , 0.922, 0.927, 0.921,\n",
       "        0.92 , 0.917, 0.915, 0.909, 0.92 , 0.915, 0.916, 0.917, 0.915,\n",
       "        0.922, 0.918, 0.92 , 0.918, 0.923, 0.911, 0.918, 0.917, 0.912,\n",
       "        0.92 , 0.922, 0.921, 0.934, 0.922, 0.921, 0.927, 0.904, 0.924,\n",
       "        0.919, 0.914, 0.917, 0.926, 0.923, 0.922, 0.916, 0.922, 0.92 ,\n",
       "        0.925, 0.919, 0.916, 0.92 , 0.92 , 0.91 , 0.748, 0.252, 0.734,\n",
       "        0.819, 0.163, 0.813, 0.806, 0.199, 0.875, 0.877, 0.158, 0.88 ,\n",
       "        0.784, 0.149, 0.745, 0.797, 0.158, 0.816, 0.862, 0.101, 0.83 ,\n",
       "        0.855, 0.295, 0.875, 0.758, 0.119, 0.712, 0.813, 0.113, 0.801,\n",
       "        0.866, 0.198, 0.861, 0.88 , 0.253, 0.872, 0.672, 0.06 , 0.758,\n",
       "        0.81 , 0.198, 0.791, 0.863, 0.296, 0.838, 0.851, 0.198, 0.883,\n",
       "        0.911, 0.28 , 0.896, 0.909, 0.372, 0.915, 0.91 , 0.242, 0.916,\n",
       "        0.914, 0.379, 0.923, 0.908, 0.157, 0.857, 0.912, 0.213, 0.909,\n",
       "        0.915, 0.329, 0.91 , 0.915, 0.34 , 0.914, 0.84 , 0.323, 0.905,\n",
       "        0.919, 0.403, 0.899, 0.918, 0.213, 0.912, 0.914, 0.36 , 0.918,\n",
       "        0.864, 0.23 , 0.9  , 0.907, 0.245, 0.915, 0.916, 0.357, 0.921,\n",
       "        0.914, 0.319, 0.907, 0.859, 0.859, 0.859, 0.91 , 0.913, 0.914,\n",
       "        0.916, 0.913, 0.914, 0.915, 0.916, 0.914, 0.856, 0.854, 0.881,\n",
       "        0.91 , 0.91 , 0.913, 0.908, 0.913, 0.911, 0.914, 0.915, 0.914,\n",
       "        0.871, 0.884, 0.897, 0.913, 0.921, 0.911, 0.915, 0.912, 0.914,\n",
       "        0.912, 0.911, 0.915, 0.91 , 0.908, 0.901, 0.917, 0.911, 0.911,\n",
       "        0.917, 0.91 , 0.915, 0.918, 0.914, 0.918, 0.909, 0.913, 0.909,\n",
       "        0.913, 0.92 , 0.915, 0.916, 0.923, 0.922, 0.911, 0.921, 0.921,\n",
       "        0.91 , 0.909, 0.915, 0.922, 0.915, 0.921, 0.922, 0.921, 0.912,\n",
       "        0.917, 0.92 , 0.918, 0.912, 0.907, 0.908, 0.92 , 0.923, 0.921,\n",
       "        0.918, 0.914, 0.914, 0.922, 0.919, 0.917, 0.907, 0.905, 0.906,\n",
       "        0.919, 0.914, 0.917, 0.92 , 0.919, 0.919, 0.918, 0.921, 0.919]),\n",
       " 'split1_test_f1_micro': array([0.937, 0.943, 0.938, 0.937, 0.942, 0.937, 0.933, 0.932, 0.93 ,\n",
       "        0.937, 0.931, 0.932, 0.936, 0.94 , 0.939, 0.938, 0.938, 0.94 ,\n",
       "        0.936, 0.939, 0.938, 0.938, 0.931, 0.929, 0.933, 0.935, 0.94 ,\n",
       "        0.943, 0.943, 0.94 , 0.936, 0.934, 0.928, 0.935, 0.934, 0.928,\n",
       "        0.937, 0.938, 0.931, 0.941, 0.943, 0.935, 0.941, 0.93 , 0.933,\n",
       "        0.939, 0.931, 0.931, 0.945, 0.932, 0.94 , 0.936, 0.941, 0.937,\n",
       "        0.937, 0.944, 0.935, 0.936, 0.929, 0.934, 0.942, 0.941, 0.939,\n",
       "        0.935, 0.933, 0.941, 0.94 , 0.936, 0.933, 0.931, 0.929, 0.929,\n",
       "        0.942, 0.938, 0.931, 0.939, 0.937, 0.94 , 0.929, 0.935, 0.93 ,\n",
       "        0.931, 0.93 , 0.929, 0.937, 0.939, 0.946, 0.94 , 0.945, 0.937,\n",
       "        0.937, 0.939, 0.935, 0.933, 0.936, 0.934, 0.718, 0.261, 0.782,\n",
       "        0.809, 0.129, 0.821, 0.861, 0.252, 0.837, 0.877, 0.275, 0.886,\n",
       "        0.726, 0.198, 0.789, 0.823, 0.268, 0.809, 0.835, 0.274, 0.875,\n",
       "        0.882, 0.202, 0.882, 0.763, 0.198, 0.698, 0.806, 0.198, 0.822,\n",
       "        0.841, 0.113, 0.844, 0.884, 0.254, 0.887, 0.745, 0.28 , 0.73 ,\n",
       "        0.825, 0.129, 0.823, 0.843, 0.071, 0.875, 0.871, 0.252, 0.889,\n",
       "        0.851, 0.228, 0.9  , 0.91 , 0.137, 0.907, 0.911, 0.34 , 0.915,\n",
       "        0.914, 0.231, 0.917, 0.912, 0.316, 0.897, 0.917, 0.336, 0.906,\n",
       "        0.913, 0.342, 0.914, 0.917, 0.374, 0.918, 0.849, 0.412, 0.847,\n",
       "        0.907, 0.043, 0.912, 0.917, 0.427, 0.913, 0.91 , 0.39 , 0.906,\n",
       "        0.911, 0.184, 0.857, 0.913, 0.339, 0.915, 0.915, 0.281, 0.911,\n",
       "        0.913, 0.398, 0.918, 0.868, 0.86 , 0.862, 0.918, 0.921, 0.919,\n",
       "        0.926, 0.92 , 0.925, 0.924, 0.923, 0.922, 0.87 , 0.869, 0.909,\n",
       "        0.917, 0.919, 0.917, 0.923, 0.923, 0.924, 0.922, 0.924, 0.926,\n",
       "        0.868, 0.869, 0.904, 0.921, 0.915, 0.922, 0.921, 0.923, 0.925,\n",
       "        0.923, 0.924, 0.922, 0.862, 0.86 , 0.9  , 0.918, 0.915, 0.919,\n",
       "        0.921, 0.917, 0.922, 0.919, 0.921, 0.919, 0.92 , 0.918, 0.922,\n",
       "        0.922, 0.93 , 0.93 , 0.933, 0.927, 0.932, 0.934, 0.928, 0.927,\n",
       "        0.912, 0.92 , 0.913, 0.932, 0.931, 0.932, 0.928, 0.925, 0.928,\n",
       "        0.933, 0.926, 0.926, 0.924, 0.915, 0.914, 0.925, 0.924, 0.923,\n",
       "        0.932, 0.931, 0.925, 0.926, 0.927, 0.926, 0.922, 0.921, 0.859,\n",
       "        0.927, 0.93 , 0.924, 0.923, 0.93 , 0.927, 0.926, 0.931, 0.922]),\n",
       " 'split2_test_f1_micro': array([0.911, 0.921, 0.923, 0.93 , 0.928, 0.93 , 0.919, 0.917, 0.931,\n",
       "        0.907, 0.927, 0.926, 0.922, 0.922, 0.925, 0.931, 0.926, 0.928,\n",
       "        0.921, 0.926, 0.93 , 0.917, 0.927, 0.919, 0.926, 0.929, 0.923,\n",
       "        0.923, 0.92 , 0.928, 0.916, 0.921, 0.926, 0.926, 0.924, 0.92 ,\n",
       "        0.918, 0.921, 0.921, 0.929, 0.929, 0.923, 0.924, 0.924, 0.917,\n",
       "        0.918, 0.92 , 0.913, 0.925, 0.921, 0.923, 0.921, 0.92 , 0.924,\n",
       "        0.929, 0.928, 0.924, 0.914, 0.922, 0.918, 0.926, 0.919, 0.918,\n",
       "        0.924, 0.917, 0.936, 0.929, 0.928, 0.934, 0.926, 0.928, 0.916,\n",
       "        0.924, 0.918, 0.923, 0.923, 0.918, 0.931, 0.921, 0.923, 0.927,\n",
       "        0.918, 0.923, 0.921, 0.923, 0.926, 0.917, 0.926, 0.928, 0.92 ,\n",
       "        0.924, 0.923, 0.918, 0.933, 0.921, 0.925, 0.728, 0.178, 0.76 ,\n",
       "        0.795, 0.044, 0.808, 0.859, 0.223, 0.842, 0.872, 0.308, 0.87 ,\n",
       "        0.73 , 0.252, 0.773, 0.793, 0.099, 0.795, 0.839, 0.169, 0.867,\n",
       "        0.879, 0.114, 0.874, 0.728, 0.119, 0.721, 0.8  , 0.03 , 0.812,\n",
       "        0.864, 0.333, 0.854, 0.866, 0.124, 0.863, 0.742, 0.111, 0.755,\n",
       "        0.808, 0.184, 0.787, 0.86 , 0.204, 0.862, 0.874, 0.198, 0.868,\n",
       "        0.885, 0.255, 0.896, 0.904, 0.254, 0.887, 0.908, 0.101, 0.897,\n",
       "        0.906, 0.468, 0.9  , 0.86 , 0.208, 0.882, 0.9  , 0.193, 0.895,\n",
       "        0.901, 0.268, 0.9  , 0.903, 0.273, 0.904, 0.901, 0.284, 0.898,\n",
       "        0.902, 0.164, 0.9  , 0.904, 0.027, 0.907, 0.903, 0.085, 0.904,\n",
       "        0.886, 0.201, 0.896, 0.902, 0.265, 0.905, 0.907, 0.431, 0.902,\n",
       "        0.903, 0.138, 0.904, 0.903, 0.857, 0.864, 0.906, 0.907, 0.909,\n",
       "        0.91 , 0.908, 0.908, 0.911, 0.908, 0.911, 0.816, 0.862, 0.859,\n",
       "        0.908, 0.908, 0.907, 0.907, 0.911, 0.907, 0.909, 0.913, 0.912,\n",
       "        0.856, 0.858, 0.854, 0.912, 0.908, 0.909, 0.912, 0.914, 0.917,\n",
       "        0.913, 0.91 , 0.912, 0.903, 0.855, 0.857, 0.914, 0.908, 0.903,\n",
       "        0.906, 0.903, 0.912, 0.907, 0.91 , 0.908, 0.9  , 0.903, 0.898,\n",
       "        0.922, 0.914, 0.91 , 0.923, 0.912, 0.918, 0.915, 0.918, 0.917,\n",
       "        0.9  , 0.903, 0.903, 0.917, 0.919, 0.918, 0.921, 0.911, 0.917,\n",
       "        0.921, 0.911, 0.917, 0.901, 0.897, 0.914, 0.921, 0.916, 0.92 ,\n",
       "        0.913, 0.916, 0.922, 0.916, 0.914, 0.918, 0.902, 0.904, 0.9  ,\n",
       "        0.918, 0.908, 0.915, 0.913, 0.92 , 0.917, 0.913, 0.914, 0.918]),\n",
       " 'split3_test_f1_micro': array([0.931, 0.926, 0.929, 0.933, 0.935, 0.927, 0.929, 0.933, 0.928,\n",
       "        0.925, 0.926, 0.929, 0.934, 0.931, 0.933, 0.933, 0.931, 0.931,\n",
       "        0.927, 0.931, 0.936, 0.934, 0.93 , 0.93 , 0.927, 0.938, 0.935,\n",
       "        0.933, 0.932, 0.937, 0.932, 0.931, 0.931, 0.939, 0.928, 0.919,\n",
       "        0.933, 0.931, 0.932, 0.933, 0.932, 0.936, 0.938, 0.938, 0.936,\n",
       "        0.931, 0.934, 0.942, 0.931, 0.928, 0.937, 0.927, 0.937, 0.931,\n",
       "        0.931, 0.933, 0.934, 0.925, 0.921, 0.925, 0.936, 0.931, 0.931,\n",
       "        0.93 , 0.933, 0.929, 0.926, 0.924, 0.927, 0.928, 0.924, 0.927,\n",
       "        0.932, 0.933, 0.929, 0.935, 0.929, 0.932, 0.932, 0.925, 0.93 ,\n",
       "        0.93 , 0.925, 0.924, 0.937, 0.933, 0.929, 0.929, 0.936, 0.931,\n",
       "        0.927, 0.926, 0.931, 0.931, 0.924, 0.927, 0.773, 0.298, 0.739,\n",
       "        0.806, 0.106, 0.775, 0.829, 0.158, 0.859, 0.87 , 0.158, 0.871,\n",
       "        0.746, 0.252, 0.744, 0.791, 0.09 , 0.784, 0.832, 0.255, 0.824,\n",
       "        0.869, 0.252, 0.872, 0.754, 0.171, 0.729, 0.825, 0.101, 0.798,\n",
       "        0.823, 0.258, 0.851, 0.871, 0.187, 0.885, 0.734, 0.064, 0.743,\n",
       "        0.832, 0.044, 0.814, 0.859, 0.112, 0.864, 0.88 , 0.156, 0.872,\n",
       "        0.865, 0.337, 0.889, 0.928, 0.202, 0.92 , 0.928, 0.168, 0.92 ,\n",
       "        0.925, 0.212, 0.918, 0.859, 0.146, 0.906, 0.92 , 0.35 , 0.914,\n",
       "        0.923, 0.323, 0.921, 0.923, 0.352, 0.92 , 0.895, 0.27 , 0.914,\n",
       "        0.924, 0.235, 0.914, 0.924, 0.249, 0.922, 0.921, 0.389, 0.927,\n",
       "        0.897, 0.231, 0.859, 0.909, 0.128, 0.921, 0.918, 0.464, 0.914,\n",
       "        0.923, 0.163, 0.922, 0.868, 0.841, 0.906, 0.923, 0.927, 0.927,\n",
       "        0.923, 0.922, 0.924, 0.923, 0.925, 0.925, 0.86 , 0.878, 0.865,\n",
       "        0.919, 0.926, 0.922, 0.928, 0.923, 0.93 , 0.927, 0.927, 0.925,\n",
       "        0.867, 0.861, 0.91 , 0.928, 0.927, 0.924, 0.922, 0.923, 0.927,\n",
       "        0.922, 0.924, 0.926, 0.872, 0.868, 0.917, 0.924, 0.921, 0.915,\n",
       "        0.924, 0.919, 0.923, 0.925, 0.927, 0.923, 0.916, 0.92 , 0.919,\n",
       "        0.927, 0.924, 0.924, 0.928, 0.93 , 0.93 , 0.927, 0.933, 0.928,\n",
       "        0.919, 0.914, 0.92 , 0.924, 0.928, 0.925, 0.931, 0.928, 0.928,\n",
       "        0.929, 0.928, 0.93 , 0.919, 0.923, 0.917, 0.931, 0.921, 0.93 ,\n",
       "        0.929, 0.934, 0.928, 0.931, 0.932, 0.933, 0.911, 0.918, 0.91 ,\n",
       "        0.923, 0.929, 0.929, 0.926, 0.928, 0.932, 0.927, 0.93 , 0.93 ]),\n",
       " 'split4_test_f1_micro': array([0.928, 0.929, 0.936, 0.927, 0.929, 0.928, 0.935, 0.922, 0.936,\n",
       "        0.918, 0.919, 0.931, 0.928, 0.924, 0.922, 0.922, 0.935, 0.925,\n",
       "        0.928, 0.927, 0.912, 0.931, 0.923, 0.914, 0.924, 0.928, 0.926,\n",
       "        0.932, 0.928, 0.933, 0.923, 0.931, 0.924, 0.927, 0.922, 0.922,\n",
       "        0.926, 0.93 , 0.928, 0.93 , 0.928, 0.931, 0.929, 0.92 , 0.931,\n",
       "        0.926, 0.919, 0.927, 0.932, 0.929, 0.927, 0.92 , 0.924, 0.927,\n",
       "        0.92 , 0.918, 0.914, 0.923, 0.929, 0.911, 0.925, 0.927, 0.928,\n",
       "        0.926, 0.929, 0.924, 0.926, 0.921, 0.924, 0.928, 0.923, 0.919,\n",
       "        0.925, 0.932, 0.924, 0.928, 0.919, 0.929, 0.936, 0.93 , 0.924,\n",
       "        0.926, 0.909, 0.923, 0.923, 0.925, 0.92 , 0.932, 0.928, 0.928,\n",
       "        0.922, 0.925, 0.925, 0.92 , 0.922, 0.929, 0.729, 0.105, 0.777,\n",
       "        0.813, 0.092, 0.787, 0.835, 0.129, 0.857, 0.868, 0.101, 0.873,\n",
       "        0.806, 0.197, 0.722, 0.819, 0.03 , 0.813, 0.864, 0.063, 0.841,\n",
       "        0.875, 0.112, 0.875, 0.743, 0.197, 0.736, 0.814, 0.256, 0.824,\n",
       "        0.858, 0.023, 0.873, 0.868, 0.256, 0.879, 0.801, 0.197, 0.744,\n",
       "        0.801, 0.197, 0.809, 0.835, 0.109, 0.862, 0.88 , 0.253, 0.873,\n",
       "        0.856, 0.085, 0.87 , 0.91 , 0.201, 0.91 , 0.918, 0.235, 0.913,\n",
       "        0.918, 0.17 , 0.917, 0.905, 0.067, 0.903, 0.913, 0.204, 0.918,\n",
       "        0.914, 0.335, 0.914, 0.916, 0.278, 0.912, 0.892, 0.243, 0.807,\n",
       "        0.917, 0.425, 0.914, 0.913, 0.432, 0.916, 0.915, 0.224, 0.911,\n",
       "        0.908, 0.103, 0.868, 0.914, 0.361, 0.916, 0.915, 0.348, 0.917,\n",
       "        0.918, 0.427, 0.915, 0.839, 0.898, 0.865, 0.915, 0.922, 0.92 ,\n",
       "        0.917, 0.916, 0.92 , 0.92 , 0.923, 0.921, 0.864, 0.865, 0.901,\n",
       "        0.918, 0.92 , 0.922, 0.924, 0.918, 0.912, 0.922, 0.922, 0.921,\n",
       "        0.912, 0.901, 0.867, 0.914, 0.915, 0.918, 0.923, 0.922, 0.924,\n",
       "        0.921, 0.921, 0.915, 0.865, 0.864, 0.887, 0.921, 0.921, 0.914,\n",
       "        0.918, 0.921, 0.918, 0.918, 0.918, 0.922, 0.903, 0.913, 0.916,\n",
       "        0.917, 0.923, 0.923, 0.928, 0.923, 0.918, 0.918, 0.919, 0.921,\n",
       "        0.914, 0.905, 0.909, 0.919, 0.922, 0.921, 0.914, 0.917, 0.916,\n",
       "        0.921, 0.921, 0.926, 0.915, 0.907, 0.921, 0.92 , 0.923, 0.919,\n",
       "        0.918, 0.923, 0.917, 0.925, 0.92 , 0.92 , 0.91 , 0.918, 0.907,\n",
       "        0.92 , 0.921, 0.916, 0.918, 0.917, 0.919, 0.921, 0.921, 0.923]),\n",
       " 'mean_test_f1_micro': array([0.9268, 0.9282, 0.9292, 0.9282, 0.9326, 0.9276, 0.9244, 0.9248,\n",
       "        0.928 , 0.9208, 0.926 , 0.9264, 0.9272, 0.9298, 0.9284, 0.929 ,\n",
       "        0.9294, 0.9284, 0.9286, 0.9286, 0.9264, 0.9286, 0.926 , 0.923 ,\n",
       "        0.9264, 0.9278, 0.9278, 0.9316, 0.927 , 0.9314, 0.9244, 0.9276,\n",
       "        0.926 , 0.9278, 0.9234, 0.9214, 0.9274, 0.9282, 0.9264, 0.9316,\n",
       "        0.9304, 0.9286, 0.9318, 0.9272, 0.9288, 0.9274, 0.926 , 0.927 ,\n",
       "        0.9312, 0.926 , 0.9294, 0.9252, 0.9298, 0.928 , 0.9274, 0.928 ,\n",
       "        0.9244, 0.9214, 0.9242, 0.9206, 0.929 , 0.927 , 0.9262, 0.9274,\n",
       "        0.926 , 0.93  , 0.9278, 0.9264, 0.9258, 0.9262, 0.9242, 0.9206,\n",
       "        0.9286, 0.9286, 0.9256, 0.9318, 0.925 , 0.9306, 0.929 , 0.9234,\n",
       "        0.927 , 0.9248, 0.9202, 0.9228, 0.9292, 0.9292, 0.9268, 0.9286,\n",
       "        0.9318, 0.9272, 0.927 , 0.9264, 0.925 , 0.9274, 0.9246, 0.925 ,\n",
       "        0.7392, 0.2188, 0.7584, 0.8084, 0.1068, 0.8008, 0.838 , 0.1922,\n",
       "        0.854 , 0.8728, 0.2   , 0.876 , 0.7584, 0.2096, 0.7546, 0.8046,\n",
       "        0.129 , 0.8034, 0.8464, 0.1724, 0.8474, 0.872 , 0.195 , 0.8756,\n",
       "        0.7492, 0.1608, 0.7192, 0.8116, 0.1396, 0.8114, 0.8504, 0.185 ,\n",
       "        0.8566, 0.8738, 0.2148, 0.8772, 0.7388, 0.1424, 0.746 , 0.8152,\n",
       "        0.1504, 0.8048, 0.852 , 0.1584, 0.8602, 0.8712, 0.2114, 0.877 ,\n",
       "        0.8736, 0.237 , 0.8902, 0.9122, 0.2332, 0.9078, 0.915 , 0.2172,\n",
       "        0.9122, 0.9154, 0.292 , 0.915 , 0.8888, 0.1788, 0.889 , 0.9124,\n",
       "        0.2592, 0.9084, 0.9132, 0.3194, 0.9118, 0.9148, 0.3234, 0.9136,\n",
       "        0.8754, 0.3064, 0.8742, 0.9138, 0.254 , 0.9078, 0.9152, 0.2696,\n",
       "        0.914 , 0.9126, 0.2896, 0.9132, 0.8932, 0.1898, 0.876 , 0.909 ,\n",
       "        0.2676, 0.9144, 0.9142, 0.3762, 0.913 , 0.9142, 0.289 , 0.9132,\n",
       "        0.8674, 0.863 , 0.8712, 0.9144, 0.918 , 0.9178, 0.9184, 0.9158,\n",
       "        0.9182, 0.9186, 0.919 , 0.9186, 0.8532, 0.8656, 0.883 , 0.9144,\n",
       "        0.9166, 0.9162, 0.918 , 0.9176, 0.9168, 0.9188, 0.9202, 0.9196,\n",
       "        0.8748, 0.8746, 0.8864, 0.9176, 0.9172, 0.9168, 0.9186, 0.9188,\n",
       "        0.9214, 0.9182, 0.918 , 0.918 , 0.8824, 0.871 , 0.8924, 0.9188,\n",
       "        0.9152, 0.9124, 0.9172, 0.914 , 0.918 , 0.9174, 0.918 , 0.918 ,\n",
       "        0.9096, 0.9134, 0.9128, 0.9202, 0.9222, 0.9204, 0.9256, 0.923 ,\n",
       "        0.924 , 0.921 , 0.9238, 0.9228, 0.911 , 0.9102, 0.912 , 0.9228,\n",
       "        0.923 , 0.9234, 0.9232, 0.9204, 0.9202, 0.9242, 0.9212, 0.9234,\n",
       "        0.9142, 0.9098, 0.9148, 0.9234, 0.9214, 0.9226, 0.922 , 0.9236,\n",
       "        0.9212, 0.924 , 0.9224, 0.9228, 0.9104, 0.9132, 0.8964, 0.9214,\n",
       "        0.9204, 0.9202, 0.92  , 0.9228, 0.9228, 0.921 , 0.9234, 0.9224]),\n",
       " 'std_test_f1_micro': array([0.00863481, 0.00793473, 0.00702567, 0.00783326, 0.00531413,\n",
       "        0.00677052, 0.01072567, 0.00649307, 0.00701427, 0.00992774,\n",
       "        0.00389872, 0.00652993, 0.00744043, 0.0064    , 0.00656049,\n",
       "        0.00654217, 0.00739189, 0.00722772, 0.00492341, 0.00628013,\n",
       "        0.01053755, 0.00760526, 0.00447214, 0.00603324, 0.00372022,\n",
       "        0.01010742, 0.00884081, 0.00674092, 0.01054514, 0.00739189,\n",
       "        0.00840476, 0.00549909, 0.00340588, 0.00928224, 0.00828493,\n",
       "        0.00355528, 0.00682935, 0.00649307, 0.005004  , 0.00535164,\n",
       "        0.0074458 , 0.00700286, 0.00655439, 0.00627375, 0.00658483,\n",
       "        0.00717217, 0.00589915, 0.00961249, 0.00770454, 0.00469042,\n",
       "        0.00781281, 0.0059127 , 0.00793473, 0.0055857 , 0.0065909 ,\n",
       "        0.01001998, 0.00895768, 0.00935094, 0.00396989, 0.00811419,\n",
       "        0.00907744, 0.00867179, 0.00874986, 0.00463033, 0.0070993 ,\n",
       "        0.00766812, 0.00711056, 0.00531413, 0.0082801 , 0.0044    ,\n",
       "        0.00426146, 0.00646838, 0.00773563, 0.00741889, 0.00377359,\n",
       "        0.0056356 , 0.00712741, 0.00608605, 0.00501996, 0.01055651,\n",
       "        0.00268328, 0.00541849, 0.00762627, 0.00391918, 0.0064622 ,\n",
       "        0.00594643, 0.01038075, 0.00783837, 0.0079599 , 0.00655439,\n",
       "        0.00525357, 0.00674092, 0.00729383, 0.00608605, 0.0058515 ,\n",
       "        0.00807465, 0.01948743, 0.06895042, 0.01937627, 0.00798999,\n",
       "        0.03953429, 0.01711607, 0.02041568, 0.04412437, 0.0134759 ,\n",
       "        0.00365513, 0.07825343, 0.00609918, 0.03140446, 0.03888753,\n",
       "        0.02361864, 0.01358823, 0.08047857, 0.01207642, 0.01374918,\n",
       "        0.08273234, 0.02018514, 0.00954987, 0.07314096, 0.00338231,\n",
       "        0.01248038, 0.03547619, 0.01328759, 0.00840476, 0.07895467,\n",
       "        0.01057544, 0.01628005, 0.10847119, 0.00985089, 0.00699714,\n",
       "        0.05236564, 0.00881816, 0.04099463, 0.08466073, 0.00993982,\n",
       "        0.01147868, 0.05889856, 0.01371714, 0.01099091, 0.08154165,\n",
       "        0.0121062 , 0.01068457, 0.0368977 , 0.00777174, 0.02201454,\n",
       "        0.08409281, 0.01070327, 0.00820731, 0.07869282, 0.0113031 ,\n",
       "        0.0073212 , 0.07991846, 0.00793473, 0.00618385, 0.11273864,\n",
       "        0.00782304, 0.02402832, 0.08213014, 0.01801111, 0.00682935,\n",
       "        0.06885753, 0.00786384, 0.00705408, 0.02646205, 0.00688186,\n",
       "        0.0065238 , 0.04063299, 0.00557136, 0.02555465, 0.05877619,\n",
       "        0.04087738, 0.00808455, 0.14451574, 0.00682349, 0.00661513,\n",
       "        0.15075225, 0.00493964, 0.00595315, 0.11923187, 0.00842378,\n",
       "        0.01705755, 0.04691865, 0.01838478, 0.0043359 , 0.08225229,\n",
       "        0.0052    , 0.00376298, 0.0647129 , 0.00641872, 0.00661513,\n",
       "        0.11874511, 0.00673498, 0.02071328, 0.01881489, 0.01752027,\n",
       "        0.00595315, 0.0070993 , 0.00604649, 0.00560714, 0.004996  ,\n",
       "        0.0064    , 0.00492341, 0.00629285, 0.00523832, 0.01916664,\n",
       "        0.00791454, 0.0195141 , 0.00449889, 0.00668132, 0.00570614,\n",
       "        0.00874071, 0.00496387, 0.00870402, 0.00643117, 0.00534416,\n",
       "        0.00567803, 0.01928108, 0.01598249, 0.02193262, 0.00608605,\n",
       "        0.0064    , 0.0059127 , 0.00431741, 0.00479166, 0.005004  ,\n",
       "        0.00470744, 0.00622896, 0.00517687, 0.02006589, 0.01899474,\n",
       "        0.02009577, 0.00342929, 0.00523068, 0.00535164, 0.00611228,\n",
       "        0.00663325, 0.00414729, 0.00581722, 0.00583095, 0.00532917,\n",
       "        0.00755248, 0.00588558, 0.00856505, 0.00479166, 0.00523068,\n",
       "        0.00705975, 0.00574804, 0.00609918, 0.00593296, 0.0083666 ,\n",
       "        0.00577581, 0.00411825, 0.00626099, 0.00617738, 0.00572713,\n",
       "        0.0051923 , 0.00583095, 0.00484149, 0.0059127 , 0.00598665,\n",
       "        0.00658483, 0.00587878, 0.0059127 , 0.00504381, 0.00773046,\n",
       "        0.00872697, 0.00426146, 0.00422374, 0.00287054, 0.00392938,\n",
       "        0.00723878, 0.00791454, 0.00511468, 0.00493964, 0.0063435 ,\n",
       "        0.00597997, 0.0065909 , 0.00719444, 0.01897999, 0.0032619 ,\n",
       "        0.00849941, 0.00541849, 0.00442719, 0.0051923 , 0.00574108,\n",
       "        0.00517687, 0.0063435 , 0.00422374]),\n",
       " 'rank_test_f1_micro': array([ 57,  32,  16,  32,   1,  42,  83,  81,  35, 123,  67,  59,  49,\n",
       "         12,  31,  21,  14,  30,  28,  23,  59,  23,  67, 100,  59,  38,\n",
       "         38,   5,  53,   7,  83,  43,  70,  38,  93, 114,  44,  32,  63,\n",
       "          6,  10,  23,   3,  49,  22,  44,  67,  53,   8,  70,  14,  76,\n",
       "         12,  35,  44,  35,  83, 114,  86, 124,  19,  53,  65,  44,  70,\n",
       "         11,  38,  63,  73,  65,  86, 124,  28,  23,  74,   2,  77,   9,\n",
       "         19,  93,  52,  80, 130, 103,  16,  16,  57,  23,   3,  49,  53,\n",
       "         59,  77,  44,  82,  77, 254, 270, 250, 244, 288, 248, 240, 277,\n",
       "        234, 224, 275, 216, 249, 274, 251, 246, 287, 247, 239, 281, 238,\n",
       "        225, 276, 217, 252, 282, 256, 242, 286, 243, 237, 279, 233, 222,\n",
       "        272, 213, 255, 285, 253, 241, 284, 245, 236, 283, 232, 226, 273,\n",
       "        214, 223, 268, 207, 191, 269, 202, 167, 271, 191, 164, 261, 167,\n",
       "        209, 280, 208, 189, 266, 201, 184, 259, 194, 169, 258, 180, 218,\n",
       "        260, 221, 179, 267, 202, 165, 264, 177, 188, 262, 182, 205, 278,\n",
       "        215, 200, 265, 171, 175, 257, 186, 174, 263, 182, 229, 231, 226,\n",
       "        171, 146, 153, 143, 163, 144, 140, 136, 140, 235, 230, 211, 171,\n",
       "        161, 162, 146, 154, 159, 137, 130, 135, 219, 220, 210, 154, 157,\n",
       "        160, 140, 137, 114, 144, 146, 146, 212, 228, 206, 137, 165, 189,\n",
       "        157, 177, 146, 156, 146, 146, 199, 181, 187, 130, 112, 126,  74,\n",
       "        100,  89, 121,  91, 103, 195, 197, 193, 103, 100,  93,  99, 126,\n",
       "        130,  86, 119,  93, 175, 198, 169,  93, 118, 109, 113,  92, 119,\n",
       "         89, 110, 103, 196, 184, 204, 114, 126, 129, 134, 103, 103, 121,\n",
       "         93, 110]),\n",
       " 'split0_test_roc_auc_ovo': array([0.99553121, 0.99530791, 0.99518945, 0.9941417 , 0.99501027,\n",
       "        0.99490434, 0.99494899, 0.99442379, 0.99486914, 0.99449339,\n",
       "        0.99430551, 0.99432526, 0.99479224, 0.99533912, 0.99541343,\n",
       "        0.99578684, 0.99540668, 0.99534191, 0.99471493, 0.99492906,\n",
       "        0.99301304, 0.99439531, 0.99361237, 0.99429622, 0.9951637 ,\n",
       "        0.99385707, 0.99481308, 0.99529189, 0.99548929, 0.99540308,\n",
       "        0.99502434, 0.99570853, 0.99504179, 0.99354915, 0.99435094,\n",
       "        0.99559362, 0.99505864, 0.99482369, 0.99486162, 0.99533789,\n",
       "        0.9944809 , 0.9951001 , 0.99463967, 0.99513358, 0.99453114,\n",
       "        0.99514912, 0.99548723, 0.99411487, 0.995364  , 0.99509275,\n",
       "        0.99581227, 0.99556152, 0.99576678, 0.99544717, 0.99529384,\n",
       "        0.9945008 , 0.9951521 , 0.99284846, 0.99412557, 0.99351419,\n",
       "        0.99517605, 0.99526801, 0.9953575 , 0.99593394, 0.99557115,\n",
       "        0.99589462, 0.9933018 , 0.9959305 , 0.99543188, 0.9946105 ,\n",
       "        0.99247886, 0.99411698, 0.99507989, 0.99505155, 0.99463816,\n",
       "        0.9959151 , 0.99534045, 0.99581248, 0.99523931, 0.99217917,\n",
       "        0.99563797, 0.99322428, 0.99417752, 0.99459304, 0.99488801,\n",
       "        0.99539025, 0.99525318, 0.99480559, 0.99473738, 0.99525847,\n",
       "        0.99563454, 0.99455948, 0.99511157, 0.99553203, 0.99519554,\n",
       "        0.99484517, 0.95671607, 0.36844715, 0.94345762, 0.98111099,\n",
       "        0.54692449, 0.9645724 , 0.9673578 , 0.5056521 , 0.98667324,\n",
       "        0.98542264, 0.38600305, 0.98530718, 0.94981771, 0.58699733,\n",
       "        0.95830179, 0.97027152, 0.56246984, 0.981303  , 0.98483738,\n",
       "        0.51653342, 0.97898768, 0.98214436, 0.45836675, 0.98582383,\n",
       "        0.9649858 , 0.36144126, 0.95054863, 0.96902928, 0.45797594,\n",
       "        0.97646634, 0.98422635, 0.521673  , 0.98388877, 0.98772159,\n",
       "        0.57213385, 0.98686825, 0.95180558, 0.37823079, 0.95890043,\n",
       "        0.95971076, 0.47419799, 0.95704926, 0.98230159, 0.55688409,\n",
       "        0.98275096, 0.9834459 , 0.59676213, 0.98885584, 0.99194395,\n",
       "        0.76287543, 0.99068558, 0.99366302, 0.82973312, 0.99350125,\n",
       "        0.99361716, 0.72048683, 0.99370009, 0.99377029, 0.77138424,\n",
       "        0.99397109, 0.99248361, 0.45837966, 0.98485255, 0.99351557,\n",
       "        0.60681014, 0.99292978, 0.99399007, 0.70627058, 0.993335  ,\n",
       "        0.99375504, 0.75372274, 0.99385034, 0.97603054, 0.73593508,\n",
       "        0.99184226, 0.99339618, 0.59791225, 0.99245191, 0.99369385,\n",
       "        0.50214083, 0.99338807, 0.99378695, 0.71139084, 0.99407686,\n",
       "        0.9679894 , 0.37558177, 0.99175709, 0.99329494, 0.57873093,\n",
       "        0.99323644, 0.99352569, 0.64006768, 0.99381067, 0.99374766,\n",
       "        0.73900406, 0.99374615, 0.98014062, 0.96901317, 0.97682216,\n",
       "        0.99368582, 0.99381923, 0.99389013, 0.99414025, 0.99414107,\n",
       "        0.99404633, 0.99425648, 0.99412013, 0.99423443, 0.97903945,\n",
       "        0.96810214, 0.98864791, 0.99383344, 0.99352311, 0.99371518,\n",
       "        0.99400782, 0.99392076, 0.99391634, 0.99415145, 0.9941971 ,\n",
       "        0.99422059, 0.98214136, 0.98870304, 0.99058917, 0.9936701 ,\n",
       "        0.99401676, 0.99381744, 0.99409482, 0.9941238 , 0.99402747,\n",
       "        0.99430285, 0.99424397, 0.99434989, 0.9926757 , 0.99117577,\n",
       "        0.98886341, 0.9938106 , 0.99335622, 0.99389054, 0.99404852,\n",
       "        0.99405986, 0.99389292, 0.99410116, 0.99419628, 0.99404697,\n",
       "        0.99251376, 0.992892  , 0.99342539, 0.99396707, 0.99419958,\n",
       "        0.99404879, 0.99487753, 0.99476175, 0.99431986, 0.99505622,\n",
       "        0.99489864, 0.99536831, 0.99360919, 0.99304397, 0.99319826,\n",
       "        0.99439062, 0.99424549, 0.99447617, 0.99469707, 0.99504001,\n",
       "        0.99485077, 0.99533284, 0.99507685, 0.99508875, 0.99353921,\n",
       "        0.99272256, 0.99356844, 0.99404058, 0.99490776, 0.9943533 ,\n",
       "        0.99414311, 0.9950199 , 0.99478952, 0.99533058, 0.99495826,\n",
       "        0.99486866, 0.99304239, 0.99280907, 0.99223524, 0.99433738,\n",
       "        0.99423865, 0.99450443, 0.99474712, 0.99499132, 0.99412246,\n",
       "        0.99441258, 0.99464456, 0.99497303]),\n",
       " 'split1_test_roc_auc_ovo': array([0.99620393, 0.99670656, 0.99685089, 0.99716553, 0.99746786,\n",
       "        0.99645894, 0.99614449, 0.99547509, 0.99683287, 0.99601457,\n",
       "        0.99555456, 0.99462348, 0.99669376, 0.9963239 , 0.99647228,\n",
       "        0.9961682 , 0.99642882, 0.99665898, 0.99581049, 0.99735511,\n",
       "        0.99594186, 0.99672844, 0.99646771, 0.9948108 , 0.9964502 ,\n",
       "        0.99700251, 0.99689661, 0.99765132, 0.99761006, 0.99766612,\n",
       "        0.99616742, 0.99571615, 0.99613925, 0.99618241, 0.99607084,\n",
       "        0.99568526, 0.99730497, 0.99728406, 0.99681233, 0.99755886,\n",
       "        0.99758262, 0.99717201, 0.99700476, 0.99686817, 0.99625562,\n",
       "        0.99740929, 0.99599238, 0.99678891, 0.99741068, 0.99601682,\n",
       "        0.99693612, 0.99635584, 0.99712899, 0.99645365, 0.99740117,\n",
       "        0.99673842, 0.99626166, 0.99620237, 0.99531264, 0.99515576,\n",
       "        0.99737142, 0.99649184, 0.99685206, 0.99589335, 0.99655647,\n",
       "        0.99657948, 0.99610057, 0.9968702 , 0.99570094, 0.99699256,\n",
       "        0.99498209, 0.99555143, 0.99716399, 0.99646684, 0.99723553,\n",
       "        0.99759983, 0.99631042, 0.99710856, 0.99630955, 0.99574969,\n",
       "        0.99644643, 0.99534206, 0.99521379, 0.99624265, 0.99639934,\n",
       "        0.99725912, 0.99747275, 0.99617959, 0.99680151, 0.99674734,\n",
       "        0.99627911, 0.99689441, 0.9962647 , 0.99500172, 0.99575976,\n",
       "        0.99620274, 0.95471092, 0.41756547, 0.95489096, 0.97079777,\n",
       "        0.55736485, 0.96729252, 0.97538868, 0.3590646 , 0.97996817,\n",
       "        0.98154975, 0.63921608, 0.98488946, 0.93680838, 0.62219564,\n",
       "        0.93387401, 0.96828223, 0.56246312, 0.9589065 , 0.98291156,\n",
       "        0.57966865, 0.98084303, 0.98255358, 0.70015023, 0.9795982 ,\n",
       "        0.94693888, 0.45224944, 0.92899348, 0.96895681, 0.61356948,\n",
       "        0.97658402, 0.98075003, 0.64272146, 0.97556918, 0.98253112,\n",
       "        0.72252001, 0.98348309, 0.94352757, 0.6883061 , 0.94866983,\n",
       "        0.96605221, 0.39946925, 0.96747377, 0.97716884, 0.47141261,\n",
       "        0.98158909, 0.97927762, 0.3815108 , 0.98505316, 0.95281114,\n",
       "        0.60938864, 0.98912182, 0.99519778, 0.56368459, 0.99436234,\n",
       "        0.99482198, 0.67890468, 0.99551597, 0.99566747, 0.67253236,\n",
       "        0.99517217, 0.99370885, 0.78393537, 0.99275267, 0.99514875,\n",
       "        0.65424616, 0.9943742 , 0.99545272, 0.76950979, 0.99562495,\n",
       "        0.99561051, 0.70774189, 0.99532964, 0.96490359, 0.58646123,\n",
       "        0.97558784, 0.99556829, 0.52773615, 0.99397071, 0.99533765,\n",
       "        0.62913184, 0.99537403, 0.99568368, 0.72093829, 0.99533112,\n",
       "        0.99351858, 0.76143471, 0.98409985, 0.99537438, 0.57890003,\n",
       "        0.99508427, 0.99509881, 0.5962554 , 0.9953619 , 0.99534479,\n",
       "        0.65751993, 0.99587999, 0.98574044, 0.98329643, 0.97685985,\n",
       "        0.99539019, 0.99556846, 0.99550253, 0.99637947, 0.99619422,\n",
       "        0.99658684, 0.99631971, 0.99640218, 0.99648901, 0.98704175,\n",
       "        0.96302391, 0.99232461, 0.9952562 , 0.99568916, 0.99564711,\n",
       "        0.99626845, 0.99616817, 0.99617275, 0.99635863, 0.99629687,\n",
       "        0.9966474 , 0.98449036, 0.98042069, 0.99112059, 0.99573412,\n",
       "        0.99512015, 0.99581658, 0.99627097, 0.99638582, 0.996109  ,\n",
       "        0.99664977, 0.99641976, 0.99649072, 0.97192481, 0.98425364,\n",
       "        0.98634688, 0.99527948, 0.99563491, 0.99489844, 0.99608174,\n",
       "        0.99596584, 0.99600922, 0.99632294, 0.9960142 , 0.99633971,\n",
       "        0.99636541, 0.99539449, 0.99473489, 0.99663365, 0.99655762,\n",
       "        0.9966809 , 0.99679966, 0.99664007, 0.99686667, 0.99715491,\n",
       "        0.99704692, 0.99707058, 0.99317468, 0.99589809, 0.99505323,\n",
       "        0.99666847, 0.99679356, 0.99687397, 0.99690968, 0.99662118,\n",
       "        0.99702715, 0.99723363, 0.99691284, 0.99705877, 0.99455037,\n",
       "        0.99434001, 0.99504163, 0.9966048 , 0.99672144, 0.99695858,\n",
       "        0.99699797, 0.99707204, 0.99690249, 0.99720895, 0.99702616,\n",
       "        0.99722647, 0.99522042, 0.99498469, 0.98404348, 0.99685789,\n",
       "        0.99656875, 0.99658847, 0.99700625, 0.9968591 , 0.996822  ,\n",
       "        0.99691027, 0.99715216, 0.99689462]),\n",
       " 'split2_test_roc_auc_ovo': array([0.99483464, 0.99662028, 0.99540884, 0.99594094, 0.99702258,\n",
       "        0.99730379, 0.996436  , 0.99562904, 0.99639298, 0.99559217,\n",
       "        0.99545491, 0.99688082, 0.99635694, 0.99592381, 0.99691789,\n",
       "        0.99690511, 0.99726489, 0.99708498, 0.99637734, 0.99665397,\n",
       "        0.99686764, 0.99643777, 0.99610504, 0.99630521, 0.99633945,\n",
       "        0.9963371 , 0.99642569, 0.99598666, 0.99661728, 0.99691462,\n",
       "        0.99651645, 0.99626805, 0.99668192, 0.99665796, 0.99615734,\n",
       "        0.99555666, 0.99592607, 0.99577798, 0.99642275, 0.99665976,\n",
       "        0.99656958, 0.99712502, 0.99695358, 0.99675012, 0.9961642 ,\n",
       "        0.99618593, 0.99597031, 0.9962911 , 0.99651808, 0.99629687,\n",
       "        0.99612863, 0.99681375, 0.99676341, 0.99669003, 0.99704811,\n",
       "        0.99679892, 0.99682138, 0.99514842, 0.996245  , 0.9949388 ,\n",
       "        0.99618408, 0.99602492, 0.99604177, 0.99700758, 0.99586539,\n",
       "        0.9970806 , 0.99681809, 0.99695008, 0.99679153, 0.99665641,\n",
       "        0.99603943, 0.99597367, 0.99640529, 0.99657176, 0.99654416,\n",
       "        0.99669492, 0.99607265, 0.997021  , 0.99600295, 0.99595363,\n",
       "        0.99692745, 0.9964565 , 0.99529319, 0.99525789, 0.99615164,\n",
       "        0.99622949, 0.99572972, 0.99680672, 0.99684381, 0.99684812,\n",
       "        0.99639551, 0.99653209, 0.99650487, 0.99702361, 0.99553182,\n",
       "        0.99627317, 0.95398111, 0.62526925, 0.95587573, 0.97217984,\n",
       "        0.62990495, 0.97786185, 0.98556999, 0.56710198, 0.98171239,\n",
       "        0.98501663, 0.52013061, 0.98841557, 0.94195138, 0.65257792,\n",
       "        0.95914449, 0.96432147, 0.48319033, 0.97497978, 0.98246254,\n",
       "        0.73596523, 0.98547956, 0.98683919, 0.60819487, 0.98631131,\n",
       "        0.954668  , 0.71567044, 0.96057664, 0.9740681 , 0.51024454,\n",
       "        0.96263485, 0.98603132, 0.73178459, 0.98608531, 0.98590278,\n",
       "        0.46553194, 0.98624967, 0.91753774, 0.44632719, 0.95519585,\n",
       "        0.98037495, 0.58120696, 0.96195013, 0.98699751, 0.73158584,\n",
       "        0.98578793, 0.98650103, 0.47364591, 0.98444223, 0.98547736,\n",
       "        0.72214828, 0.99045404, 0.99235371, 0.65019234, 0.98803922,\n",
       "        0.99369934, 0.5552566 , 0.99249436, 0.99408512, 0.77390501,\n",
       "        0.99361338, 0.97291059, 0.63139622, 0.98875686, 0.99300907,\n",
       "        0.69232026, 0.99243907, 0.99414432, 0.66740505, 0.993716  ,\n",
       "        0.99314101, 0.59804256, 0.99388367, 0.98923445, 0.60988884,\n",
       "        0.99010077, 0.9932739 , 0.5483898 , 0.9927324 , 0.99382465,\n",
       "        0.50462099, 0.99334429, 0.99393449, 0.58964761, 0.99398321,\n",
       "        0.98722885, 0.44763487, 0.99207502, 0.99194882, 0.55315481,\n",
       "        0.99347501, 0.99333621, 0.64570377, 0.99296799, 0.9934559 ,\n",
       "        0.57738879, 0.99386665, 0.99247539, 0.97756637, 0.98300274,\n",
       "        0.9942002 , 0.99412599, 0.99386467, 0.99456425, 0.99464431,\n",
       "        0.99474427, 0.99505006, 0.99490145, 0.99489577, 0.95841494,\n",
       "        0.97716518, 0.97301941, 0.99373042, 0.99347701, 0.99350962,\n",
       "        0.99450885, 0.99468135, 0.99472918, 0.99487987, 0.99512977,\n",
       "        0.99504264, 0.96911807, 0.96835522, 0.98416029, 0.99320684,\n",
       "        0.99432687, 0.99421765, 0.99470205, 0.994925  , 0.99484379,\n",
       "        0.99508572, 0.99482378, 0.9948533 , 0.99171645, 0.9599208 ,\n",
       "        0.97809377, 0.99423102, 0.99422846, 0.99302069, 0.99418435,\n",
       "        0.99383112, 0.99395541, 0.99478262, 0.99475442, 0.99463094,\n",
       "        0.99340556, 0.99403525, 0.99360351, 0.99525179, 0.99503906,\n",
       "        0.99509811, 0.99563722, 0.99538095, 0.99540965, 0.99577093,\n",
       "        0.9957028 , 0.99571526, 0.99375047, 0.99314061, 0.99286422,\n",
       "        0.995261  , 0.9953237 , 0.99522118, 0.99564745, 0.99562332,\n",
       "        0.99561057, 0.9954399 , 0.99568114, 0.99558704, 0.99435178,\n",
       "        0.99257761, 0.99433839, 0.99529911, 0.99475767, 0.99566076,\n",
       "        0.99550268, 0.99546844, 0.99546901, 0.99556907, 0.99577864,\n",
       "        0.99575858, 0.99399975, 0.99285355, 0.99332546, 0.99510886,\n",
       "        0.99511701, 0.99510885, 0.9951623 , 0.99529326, 0.9952569 ,\n",
       "        0.99539943, 0.99521978, 0.99545199]),\n",
       " 'split3_test_roc_auc_ovo': array([0.9931351 , 0.99235406, 0.99307094, 0.99306144, 0.99133544,\n",
       "        0.99309537, 0.99177471, 0.99312394, 0.99100744, 0.9909071 ,\n",
       "        0.99175891, 0.99178189, 0.99376744, 0.99352837, 0.99274449,\n",
       "        0.99338335, 0.99266796, 0.99219005, 0.99217277, 0.99276554,\n",
       "        0.99181462, 0.99209111, 0.99246365, 0.99120261, 0.99273916,\n",
       "        0.99334886, 0.99376097, 0.99335113, 0.99331867, 0.99415263,\n",
       "        0.99242654, 0.99236106, 0.99093303, 0.99280236, 0.99224298,\n",
       "        0.99178085, 0.99320525, 0.99403713, 0.99345769, 0.99347549,\n",
       "        0.99286751, 0.99368766, 0.99367301, 0.99349081, 0.99356947,\n",
       "        0.99326518, 0.99321686, 0.99339517, 0.99440786, 0.99429644,\n",
       "        0.99384885, 0.99330635, 0.99277081, 0.99322958, 0.99182907,\n",
       "        0.993508  , 0.99363137, 0.99074891, 0.9903282 , 0.99172311,\n",
       "        0.99300541, 0.99343251, 0.99351188, 0.99413511, 0.99272013,\n",
       "        0.99344819, 0.99263881, 0.99178151, 0.99446846, 0.99289813,\n",
       "        0.99320678, 0.99281148, 0.99329477, 0.99397738, 0.99434066,\n",
       "        0.9932034 , 0.99391964, 0.99355568, 0.99330611, 0.9920441 ,\n",
       "        0.99232781, 0.99058341, 0.99096783, 0.99002   , 0.99430286,\n",
       "        0.99408467, 0.99402558, 0.99200945, 0.99352169, 0.99250325,\n",
       "        0.99313436, 0.99206863, 0.99220909, 0.9926016 , 0.99269023,\n",
       "        0.99233122, 0.93646434, 0.67980561, 0.92477904, 0.9598504 ,\n",
       "        0.39111537, 0.96407939, 0.97670403, 0.64777565, 0.97642149,\n",
       "        0.97997597, 0.61491435, 0.98154902, 0.93716724, 0.50698464,\n",
       "        0.95520311, 0.97087522, 0.64706222, 0.97128872, 0.97823189,\n",
       "        0.67017891, 0.97621717, 0.97659708, 0.69791623, 0.98290469,\n",
       "        0.95250993, 0.46409129, 0.93323623, 0.97248614, 0.53352315,\n",
       "        0.95166666, 0.97263214, 0.52066243, 0.97449687, 0.98537241,\n",
       "        0.63440245, 0.98427615, 0.94731525, 0.38225491, 0.94898175,\n",
       "        0.97298909, 0.27548982, 0.97154746, 0.97819762, 0.47753878,\n",
       "        0.9798143 , 0.98055636, 0.57244352, 0.97935663, 0.96803644,\n",
       "        0.68832571, 0.9795644 , 0.99085016, 0.47167673, 0.99092605,\n",
       "        0.99097924, 0.66518403, 0.99135525, 0.9917568 , 0.72333332,\n",
       "        0.99086741, 0.98184637, 0.41496517, 0.98704707, 0.98988179,\n",
       "        0.64551547, 0.99004919, 0.99090425, 0.73894241, 0.99183813,\n",
       "        0.99120073, 0.78749949, 0.99120673, 0.98335103, 0.77739618,\n",
       "        0.98861819, 0.99034561, 0.79917034, 0.99162071, 0.99112899,\n",
       "        0.77804943, 0.9914393 , 0.99165715, 0.60455845, 0.99081778,\n",
       "        0.98137906, 0.58374588, 0.9736721 , 0.98679294, 0.46017547,\n",
       "        0.98998495, 0.99174862, 0.71519735, 0.99119239, 0.9909342 ,\n",
       "        0.69836043, 0.99180109, 0.96741911, 0.96244208, 0.98694301,\n",
       "        0.99169048, 0.99183762, 0.99151016, 0.99263661, 0.99278837,\n",
       "        0.9928206 , 0.99286486, 0.99322142, 0.99311775, 0.97312684,\n",
       "        0.98182729, 0.96603449, 0.9917573 , 0.99187816, 0.99237416,\n",
       "        0.9929158 , 0.99225808, 0.99237017, 0.99305024, 0.99324422,\n",
       "        0.99321994, 0.96450499, 0.98030984, 0.98558593, 0.99215435,\n",
       "        0.99126513, 0.99132595, 0.99304309, 0.99280989, 0.99261589,\n",
       "        0.99319387, 0.99279797, 0.99284587, 0.97730515, 0.97646924,\n",
       "        0.98823794, 0.99074413, 0.99076457, 0.99057428, 0.99210155,\n",
       "        0.99235752, 0.99242527, 0.99265813, 0.99319001, 0.99293448,\n",
       "        0.99164018, 0.99151221, 0.99234665, 0.99366328, 0.99335646,\n",
       "        0.99316837, 0.9938754 , 0.99377343, 0.99367824, 0.99366155,\n",
       "        0.99341068, 0.99365069, 0.98963459, 0.99010421, 0.99158531,\n",
       "        0.99353062, 0.99374458, 0.99340952, 0.99386279, 0.99343094,\n",
       "        0.99320767, 0.99372206, 0.99350899, 0.9932869 , 0.99203329,\n",
       "        0.99183154, 0.9913225 , 0.99310113, 0.99350026, 0.99298982,\n",
       "        0.99350708, 0.99333873, 0.99319126, 0.99365911, 0.99364966,\n",
       "        0.99374532, 0.99175802, 0.98971692, 0.9907863 , 0.99360733,\n",
       "        0.99291905, 0.99338706, 0.99353796, 0.99316665, 0.99360933,\n",
       "        0.99352484, 0.99361042, 0.99378816]),\n",
       " 'split4_test_roc_auc_ovo': array([0.99586377, 0.99631984, 0.99634291, 0.99677649, 0.99660513,\n",
       "        0.99602546, 0.99648594, 0.99610396, 0.99670728, 0.9955616 ,\n",
       "        0.99598042, 0.99613206, 0.99664589, 0.99618956, 0.99644058,\n",
       "        0.9958435 , 0.99642779, 0.99616236, 0.99544929, 0.99592416,\n",
       "        0.99542404, 0.99631587, 0.99604472, 0.99458821, 0.99606185,\n",
       "        0.99613721, 0.99622657, 0.99672572, 0.99629813, 0.9965672 ,\n",
       "        0.9958687 , 0.99635688, 0.9960291 , 0.99517383, 0.99504735,\n",
       "        0.99608456, 0.99592279, 0.99639262, 0.99614959, 0.99620402,\n",
       "        0.99679328, 0.99635469, 0.99554453, 0.99594546, 0.99602926,\n",
       "        0.99585426, 0.99613716, 0.9960767 , 0.99647438, 0.99626268,\n",
       "        0.99634069, 0.99610419, 0.99595359, 0.99686016, 0.99604957,\n",
       "        0.99599563, 0.99541695, 0.99561953, 0.995754  , 0.99493239,\n",
       "        0.99653577, 0.99622436, 0.99625104, 0.99587279, 0.99650709,\n",
       "        0.99665123, 0.99562889, 0.99598189, 0.99542927, 0.99560723,\n",
       "        0.99551834, 0.99589041, 0.99624299, 0.99622926, 0.99617198,\n",
       "        0.99586346, 0.99610843, 0.99659678, 0.99576206, 0.99661772,\n",
       "        0.99631611, 0.99641932, 0.99466328, 0.99537754, 0.99634019,\n",
       "        0.99639819, 0.99590744, 0.99673762, 0.99639247, 0.9965042 ,\n",
       "        0.99598475, 0.99635723, 0.99596596, 0.99560044, 0.99608845,\n",
       "        0.99621368, 0.93738453, 0.27471167, 0.95237629, 0.98213557,\n",
       "        0.34750858, 0.97161987, 0.98273627, 0.62553796, 0.98469158,\n",
       "        0.98511895, 0.45914711, 0.98844843, 0.96472452, 0.5146492 ,\n",
       "        0.96076668, 0.96907164, 0.51168513, 0.97939384, 0.98651139,\n",
       "        0.47918775, 0.98089546, 0.98956556, 0.47218584, 0.98954042,\n",
       "        0.96819208, 0.58358107, 0.95665495, 0.96890467, 0.46384939,\n",
       "        0.98366849, 0.98164257, 0.52689845, 0.98891537, 0.98897164,\n",
       "        0.6361107 , 0.98983706, 0.95538862, 0.57357583, 0.94434442,\n",
       "        0.97412048, 0.54965331, 0.97937144, 0.97800442, 0.46332291,\n",
       "        0.98470524, 0.99056678, 0.53380224, 0.98407236, 0.97616954,\n",
       "        0.40090259, 0.97436384, 0.99413066, 0.47319418, 0.99408634,\n",
       "        0.99478057, 0.74838932, 0.99468379, 0.99487539, 0.76044463,\n",
       "        0.99505529, 0.99083759, 0.57222154, 0.99352034, 0.9945943 ,\n",
       "        0.66740204, 0.99434125, 0.99464896, 0.70362141, 0.99453693,\n",
       "        0.99485758, 0.57273551, 0.99474041, 0.991881  , 0.70782286,\n",
       "        0.97321305, 0.99469498, 0.70435931, 0.9945758 , 0.99468622,\n",
       "        0.85553546, 0.99454016, 0.99503876, 0.7335831 , 0.99465607,\n",
       "        0.99351353, 0.53098944, 0.98557211, 0.99418589, 0.76591599,\n",
       "        0.99459153, 0.99470611, 0.69711135, 0.9948791 , 0.99502376,\n",
       "        0.78526748, 0.99495396, 0.97623531, 0.99231524, 0.98981702,\n",
       "        0.99472847, 0.99419879, 0.99521358, 0.99559078, 0.99542053,\n",
       "        0.99548351, 0.99547545, 0.99553699, 0.99553403, 0.97004959,\n",
       "        0.98707254, 0.99176798, 0.99477355, 0.99508623, 0.99534032,\n",
       "        0.99541715, 0.99519514, 0.99558243, 0.99542471, 0.99554361,\n",
       "        0.99533751, 0.99276909, 0.98904202, 0.97354563, 0.99459575,\n",
       "        0.99507762, 0.99501092, 0.99543386, 0.99544412, 0.99548209,\n",
       "        0.99576684, 0.99559027, 0.9955942 , 0.98267846, 0.9877106 ,\n",
       "        0.99002447, 0.99472397, 0.99441122, 0.99517926, 0.99534711,\n",
       "        0.9953745 , 0.99516306, 0.99526274, 0.99533792, 0.99544449,\n",
       "        0.99447084, 0.99484733, 0.99394455, 0.99537444, 0.99556898,\n",
       "        0.99562406, 0.99575896, 0.99593806, 0.99558288, 0.99580544,\n",
       "        0.99590769, 0.99560849, 0.99436764, 0.99421072, 0.99224254,\n",
       "        0.9954753 , 0.9954342 , 0.99563411, 0.99572151, 0.99560324,\n",
       "        0.99586124, 0.99570469, 0.99559704, 0.99586392, 0.99445167,\n",
       "        0.99514023, 0.99480945, 0.99549015, 0.99577462, 0.99554115,\n",
       "        0.99552484, 0.99560324, 0.99553199, 0.99583662, 0.99556645,\n",
       "        0.9955287 , 0.99411332, 0.99290097, 0.99455658, 0.99575767,\n",
       "        0.99559044, 0.99548041, 0.99574789, 0.99561567, 0.99562184,\n",
       "        0.99559361, 0.9956815 , 0.99576849]),\n",
       " 'mean_test_roc_auc_ovo': array([0.99511373, 0.99546173, 0.9953726 , 0.99541722, 0.99548825,\n",
       "        0.99555758, 0.99515803, 0.99495116, 0.99516194, 0.99451377,\n",
       "        0.99461086, 0.9947487 , 0.99565125, 0.99546095, 0.99559773,\n",
       "        0.9956174 , 0.99563923, 0.99548765, 0.99490496, 0.99552557,\n",
       "        0.99461224, 0.9951937 , 0.9949387 , 0.99424061, 0.99535087,\n",
       "        0.99533655, 0.99562458, 0.99580134, 0.99586669, 0.99614073,\n",
       "        0.99520069, 0.99528213, 0.99496502, 0.99487314, 0.99477389,\n",
       "        0.99494019, 0.99548355, 0.9956631 , 0.99554079, 0.9958472 ,\n",
       "        0.99565878, 0.9958879 , 0.99556311, 0.99563763, 0.99530994,\n",
       "        0.99557276, 0.99536079, 0.99533335, 0.996035  , 0.99559311,\n",
       "        0.99581331, 0.99562833, 0.99567671, 0.99573612, 0.99552435,\n",
       "        0.99550835, 0.99545669, 0.99411354, 0.99435308, 0.99405285,\n",
       "        0.99565455, 0.99548833, 0.99560285, 0.99576856, 0.99544404,\n",
       "        0.99593082, 0.99489763, 0.99550284, 0.99556442, 0.99535296,\n",
       "        0.9944451 , 0.99486879, 0.99563739, 0.99565936, 0.9957861 ,\n",
       "        0.99585534, 0.99555032, 0.9960189 , 0.995324  , 0.99450886,\n",
       "        0.99553115, 0.99440511, 0.99406312, 0.99429823, 0.99561641,\n",
       "        0.99587234, 0.99567774, 0.99530779, 0.99565937, 0.99557228,\n",
       "        0.99548566, 0.99528237, 0.99521124, 0.99515188, 0.99505316,\n",
       "        0.9951732 , 0.94785139, 0.47315983, 0.94627593, 0.97321491,\n",
       "        0.49456365, 0.96908521, 0.97755135, 0.54102646, 0.98189337,\n",
       "        0.98341679, 0.52388224, 0.98572193, 0.94609384, 0.57668094,\n",
       "        0.95345802, 0.96856442, 0.55337413, 0.97317437, 0.98299095,\n",
       "        0.59630679, 0.98048458, 0.98353995, 0.58736278, 0.98483569,\n",
       "        0.95745894, 0.5154067 , 0.94600198, 0.970689  , 0.5158325 ,\n",
       "        0.97020407, 0.98105648, 0.58874798, 0.9817911 , 0.98609991,\n",
       "        0.60613979, 0.98614284, 0.94311495, 0.49373896, 0.95121846,\n",
       "        0.9706495 , 0.45600347, 0.96747841, 0.980534  , 0.54014884,\n",
       "        0.9829295 , 0.98406954, 0.51163292, 0.98435604, 0.97488769,\n",
       "        0.63672813, 0.98483794, 0.99323907, 0.59769619, 0.99218304,\n",
       "        0.99357966, 0.67364429, 0.99354989, 0.99403101, 0.74031991,\n",
       "        0.99373587, 0.9863574 , 0.57217959, 0.9893859 , 0.99322989,\n",
       "        0.65325881, 0.9928267 , 0.99382806, 0.71714985, 0.9938102 ,\n",
       "        0.99371297, 0.68394844, 0.99380216, 0.98108012, 0.68350084,\n",
       "        0.98387242, 0.99345579, 0.63551357, 0.99307031, 0.99373427,\n",
       "        0.65389571, 0.99361717, 0.9940202 , 0.67202366, 0.99377301,\n",
       "        0.98472588, 0.53987733, 0.98543523, 0.99231939, 0.58737545,\n",
       "        0.99327444, 0.99368309, 0.65886711, 0.99364241, 0.99370126,\n",
       "        0.69150814, 0.99404957, 0.98040217, 0.97692666, 0.98268896,\n",
       "        0.99393903, 0.99391002, 0.99399621, 0.99466227, 0.9946377 ,\n",
       "        0.99473631, 0.99479331, 0.99483643, 0.9948542 , 0.97353452,\n",
       "        0.97543821, 0.98235888, 0.99387018, 0.99393074, 0.99411728,\n",
       "        0.99462362, 0.9944447 , 0.99455417, 0.99477298, 0.99488231,\n",
       "        0.99489362, 0.97860478, 0.98136616, 0.98500032, 0.99387223,\n",
       "        0.9939613 , 0.99403771, 0.99470896, 0.99473773, 0.99461565,\n",
       "        0.99499981, 0.99477515, 0.9948268 , 0.98326012, 0.97990601,\n",
       "        0.9863133 , 0.99375784, 0.99367908, 0.99351264, 0.99435265,\n",
       "        0.99431777, 0.99428918, 0.99462552, 0.99469857, 0.99467932,\n",
       "        0.99367915, 0.99373626, 0.993611  , 0.99497805, 0.99494434,\n",
       "        0.99492405, 0.99538975, 0.99529885, 0.99517146, 0.99548981,\n",
       "        0.99539335, 0.99548267, 0.99290731, 0.99327952, 0.99298871,\n",
       "        0.9950652 , 0.99510831, 0.99512299, 0.9953677 , 0.99526374,\n",
       "        0.99531148, 0.99548663, 0.99535537, 0.99537708, 0.99378527,\n",
       "        0.99332239, 0.99381608, 0.99490715, 0.99513235, 0.99510072,\n",
       "        0.99513514, 0.99530047, 0.99517685, 0.99552086, 0.99539583,\n",
       "        0.99542554, 0.99362678, 0.99265304, 0.99098941, 0.99513383,\n",
       "        0.99488678, 0.99501384, 0.99524031, 0.9951852 , 0.9950865 ,\n",
       "        0.99516814, 0.99526169, 0.99537526]),\n",
       " 'std_test_roc_auc_ovo': array([0.00108796, 0.00163153, 0.00130072, 0.00157195, 0.00223583,\n",
       "        0.00145395, 0.00178102, 0.00106582, 0.00219277, 0.00187199,\n",
       "        0.00153007, 0.00175821, 0.00117139, 0.00102367, 0.0015095 ,\n",
       "        0.00118595, 0.00159816, 0.00174781, 0.00146868, 0.00159706,\n",
       "        0.00189198, 0.00175647, 0.00160041, 0.00166979, 0.00138184,\n",
       "        0.00145309, 0.00116222, 0.00145426, 0.00144386, 0.00123323,\n",
       "        0.00147235, 0.00148526, 0.00208431, 0.00148534, 0.00143214,\n",
       "        0.0015908 , 0.00134741, 0.00114204, 0.00123011, 0.00138577,\n",
       "        0.00173184, 0.00133109, 0.00129902, 0.00124174, 0.00107492,\n",
       "        0.0013662 , 0.00109413, 0.00132889, 0.00104084, 0.00078165,\n",
       "        0.00104856, 0.00122939, 0.00153747, 0.0013457 , 0.0019914 ,\n",
       "        0.00129866, 0.00108963, 0.00203093, 0.00213139, 0.00130328,\n",
       "        0.0015001 , 0.00110559, 0.00114941, 0.00092273, 0.00141287,\n",
       "        0.00129819, 0.00163211, 0.00190918, 0.00074315, 0.00148522,\n",
       "        0.00136979, 0.00122705, 0.00134801, 0.00100077, 0.00111638,\n",
       "        0.00146864, 0.00087923, 0.0013142 , 0.00106801, 0.00197873,\n",
       "        0.00165387, 0.00224245, 0.00159945, 0.00220258, 0.00085782,\n",
       "        0.00107289, 0.00111242, 0.00179903, 0.0013164 , 0.00163673,\n",
       "        0.00120476, 0.00179814, 0.00157326, 0.00144052, 0.00121688,\n",
       "        0.00151901, 0.00897139, 0.15445188, 0.01160731, 0.00809259,\n",
       "        0.10707909, 0.00513987, 0.00633223, 0.10351713, 0.00358769,\n",
       "        0.00222735, 0.09466163, 0.00256779, 0.01043104, 0.05769554,\n",
       "        0.00995784, 0.00230623, 0.05586866, 0.00793906, 0.00278341,\n",
       "        0.09520405, 0.0030232 , 0.00443434, 0.10515013, 0.00336001,\n",
       "        0.00793483, 0.12254042, 0.01263987, 0.00217196, 0.05647358,\n",
       "        0.01150773, 0.00460878, 0.08524397, 0.0057535 , 0.00219914,\n",
       "        0.085051  , 0.00222458, 0.01340281, 0.12020733, 0.00516881,\n",
       "        0.00711192, 0.11005321, 0.00771143, 0.00369007, 0.10146438,\n",
       "        0.00213808, 0.00409439, 0.0772155 , 0.00302708, 0.01370412,\n",
       "        0.12824444, 0.00665739, 0.00150346, 0.13351811, 0.00240253,\n",
       "        0.00139739, 0.06617543, 0.00149026, 0.00131407, 0.03842305,\n",
       "        0.00155578, 0.00790964, 0.13115938, 0.00331197, 0.00183759,\n",
       "        0.02808644, 0.00158518, 0.00154844, 0.03461907, 0.00126057,\n",
       "        0.00151994, 0.08474142, 0.00141129, 0.00975179, 0.0734747 ,\n",
       "        0.00783696, 0.00177246, 0.10212116, 0.00106538, 0.00143348,\n",
       "        0.1428275 , 0.00132729, 0.0013748 , 0.06175654, 0.00155418,\n",
       "        0.00950905, 0.13160002, 0.00669665, 0.00298153, 0.0993722 ,\n",
       "        0.00178172, 0.00117797, 0.04262074, 0.00148134, 0.00156008,\n",
       "        0.07109438, 0.00136745, 0.00848824, 0.01049017, 0.00524218,\n",
       "        0.00125836, 0.00119863, 0.00141138, 0.00128034, 0.00115778,\n",
       "        0.00127505, 0.00117203, 0.00110163, 0.00114332, 0.00952536,\n",
       "        0.00879879, 0.01078063, 0.00120186, 0.00134256, 0.00121701,\n",
       "        0.00115275, 0.00131494, 0.001333  , 0.00112291, 0.00106324,\n",
       "        0.00114448, 0.01035932, 0.00753715, 0.00633958, 0.00121859,\n",
       "        0.00141387, 0.00151961, 0.00110614, 0.00121192, 0.00121473,\n",
       "        0.00118872, 0.00122987, 0.00122554, 0.00805546, 0.01111792,\n",
       "        0.00427883, 0.0015848 , 0.0016286 , 0.00165631, 0.00135403,\n",
       "        0.00126323, 0.00122187, 0.00122127, 0.00096652, 0.00116525,\n",
       "        0.0016391 , 0.00139519, 0.00077556, 0.00106993, 0.00110253,\n",
       "        0.00122109, 0.00097376, 0.00098281, 0.00110042, 0.00113857,\n",
       "        0.00120585, 0.00109184, 0.00168038, 0.00189111, 0.0011703 ,\n",
       "        0.00105707, 0.00105801, 0.00115682, 0.00102914, 0.00104847,\n",
       "        0.00126281, 0.00111706, 0.00110245, 0.00122973, 0.00094661,\n",
       "        0.00122289, 0.00134468, 0.00121558, 0.00107631, 0.00133969,\n",
       "        0.00121588, 0.00119879, 0.00120681, 0.00113613, 0.0011022 ,\n",
       "        0.00114002, 0.00116183, 0.00168424, 0.00368839, 0.00112487,\n",
       "        0.00123849, 0.00105944, 0.00114223, 0.00119256, 0.00113481,\n",
       "        0.00114375, 0.00117128, 0.00101486]),\n",
       " 'rank_test_roc_auc_ovo': array([ 99,  54,  65,  59,  48,  37,  93, 109,  92, 143, 141, 129,  22,\n",
       "         55,  31,  28,  23,  49, 115,  41, 140,  86, 112, 153,  70,  71,\n",
       "         27,  11,   7,   1,  85,  80, 108, 120, 127, 111,  52,  17,  39,\n",
       "          9,  20,   5,  36,  24,  75,  33,  67,  72,   2,  32,  10,  26,\n",
       "         16,  14,  42,  44,  56, 155, 148, 157,  21,  47,  30,  13,  57,\n",
       "          4, 116,  45,  35,  69, 145, 121,  25,  19,  12,   8,  38,   3,\n",
       "         73, 144,  40, 147, 156, 151,  29,   6,  15,  76,  18,  34,  51,\n",
       "         79,  84,  94, 104,  89, 252, 287, 253, 241, 285, 246, 236, 278,\n",
       "        226, 220, 281, 210, 254, 275, 250, 247, 277, 242, 222, 271, 232,\n",
       "        219, 274, 214, 249, 283, 255, 243, 282, 245, 230, 272, 227, 209,\n",
       "        269, 208, 256, 286, 251, 244, 288, 248, 231, 279, 223, 217, 284,\n",
       "        216, 239, 267, 213, 195, 270, 203, 188, 262, 189, 160, 257, 177,\n",
       "        206, 276, 205, 196, 266, 200, 169, 258, 171, 179, 260, 172, 229,\n",
       "        261, 218, 191, 268, 197, 178, 265, 186, 161, 263, 174, 215, 280,\n",
       "        211, 202, 273, 194, 181, 264, 184, 180, 259, 158, 233, 237, 224,\n",
       "        164, 166, 162, 135, 136, 131, 125, 123, 122, 240, 238, 225, 168,\n",
       "        165, 154, 138, 146, 142, 128, 119, 117, 235, 228, 212, 167, 163,\n",
       "        159, 132, 130, 139, 106, 126, 124, 221, 234, 207, 175, 183, 190,\n",
       "        149, 150, 152, 137, 133, 134, 182, 176, 187, 107, 110, 113,  62,\n",
       "         78,  90,  46,  61,  53, 199, 193, 198, 103, 100,  98,  66,  81,\n",
       "         74,  50,  68,  63, 173, 192, 170, 114,  97, 101,  95,  77,  88,\n",
       "         43,  60,  58, 185, 201, 204,  96, 118, 105,  83,  87, 102,  91,\n",
       "         82,  64]),\n",
       " 'split0_test_neg_log_loss': array([-0.21364279, -0.22408668, -0.2173891 , -0.2474475 , -0.22379569,\n",
       "        -0.24003322, -0.24754443, -0.24942579, -0.24716292, -0.27271734,\n",
       "        -0.27008236, -0.29818733, -0.2285104 , -0.21991833, -0.22167317,\n",
       "        -0.22236038, -0.22896224, -0.22932718, -0.23575532, -0.2290875 ,\n",
       "        -0.29690527, -0.27312549, -0.29142051, -0.26189133, -0.21862677,\n",
       "        -0.25744991, -0.23849382, -0.23037185, -0.22008621, -0.23464969,\n",
       "        -0.24611031, -0.22637932, -0.23999244, -0.28504437, -0.27989946,\n",
       "        -0.24172496, -0.21638942, -0.2217073 , -0.22157485, -0.21803682,\n",
       "        -0.22891117, -0.22528391, -0.22417681, -0.22040606, -0.22653824,\n",
       "        -0.22943278, -0.21933894, -0.2387317 , -0.21561833, -0.22603176,\n",
       "        -0.21561951, -0.22560212, -0.2170242 , -0.23128428, -0.23983652,\n",
       "        -0.26070236, -0.25391434, -0.33217109, -0.33739886, -0.31247496,\n",
       "        -0.2267749 , -0.22314729, -0.21947258, -0.23015309, -0.2309281 ,\n",
       "        -0.21719593, -0.2776707 , -0.21771432, -0.25059696, -0.27428515,\n",
       "        -0.34492724, -0.27640197, -0.22600845, -0.22039652, -0.2258679 ,\n",
       "        -0.20936631, -0.23182769, -0.21377419, -0.22504237, -0.3234763 ,\n",
       "        -0.23406789, -0.33927969, -0.29280829, -0.26646389, -0.22010169,\n",
       "        -0.21622916, -0.21891437, -0.24106754, -0.22951929, -0.22180375,\n",
       "        -0.2213213 , -0.23917027, -0.2405891 , -0.2401911 , -0.2434494 ,\n",
       "        -0.25860602, -0.8090949 , -2.00089171, -0.83081174, -0.66214089,\n",
       "        -1.93105803, -0.63864211, -0.61523116, -1.98809865, -0.57457641,\n",
       "        -0.56889289, -1.96250942, -0.55847029, -0.85881279, -2.11609936,\n",
       "        -0.82608895, -0.68668176, -2.05692379, -0.66209625, -0.57984348,\n",
       "        -1.9572188 , -0.62232744, -0.58471493, -1.89346231, -0.55080978,\n",
       "        -0.87985759, -1.994096  , -0.85119762, -0.66923815, -1.94633873,\n",
       "        -0.67511474, -0.57949816, -1.94083152, -0.59696004, -0.5431766 ,\n",
       "        -1.92068124, -0.56232859, -0.95520553, -1.98829992, -0.78138641,\n",
       "        -0.68513746, -1.93047505, -0.70800945, -0.61711158, -1.85427818,\n",
       "        -0.62305598, -0.59649109, -1.8867556 , -0.55930533, -0.36686078,\n",
       "        -1.73242428, -0.36396838, -0.28754084, -1.52590013, -0.28039512,\n",
       "        -0.27697135, -1.76443369, -0.2757551 , -0.27002801, -1.56644803,\n",
       "        -0.26108597, -0.33497536, -2.04905246, -0.46754718, -0.28476449,\n",
       "        -1.84538553, -0.29659597, -0.27007503, -1.74184304, -0.28467923,\n",
       "        -0.26651999, -1.67962292, -0.27141336, -0.45041075, -1.81205745,\n",
       "        -0.34638849, -0.29754671, -1.87154106, -0.31848341, -0.27266572,\n",
       "        -2.04511473, -0.2808484 , -0.26788556, -1.78423589, -0.26215421,\n",
       "        -0.40756972, -2.0596033 , -0.35278027, -0.29978331, -1.84603029,\n",
       "        -0.31021159, -0.28340079, -1.69472767, -0.27978926, -0.26597688,\n",
       "        -1.72225415, -0.2764604 , -0.36290954, -0.40318793, -0.39078392,\n",
       "        -0.255015  , -0.24545216, -0.25344934, -0.23688725, -0.24076698,\n",
       "        -0.24475165, -0.23262657, -0.23431709, -0.23793062, -0.39090636,\n",
       "        -0.40846418, -0.38656152, -0.24863468, -0.25802696, -0.25864113,\n",
       "        -0.23786019, -0.24339984, -0.24337034, -0.23608734, -0.23611729,\n",
       "        -0.2361608 , -0.35967025, -0.34641461, -0.35427073, -0.25663107,\n",
       "        -0.24265581, -0.24942073, -0.2403279 , -0.24188496, -0.24512986,\n",
       "        -0.23670268, -0.23526954, -0.23715413, -0.31787624, -0.34343893,\n",
       "        -0.37792784, -0.26041447, -0.27479658, -0.26286975, -0.24876962,\n",
       "        -0.25040421, -0.25230906, -0.24289745, -0.24648993, -0.245164  ,\n",
       "        -0.27039505, -0.25689798, -0.25337461, -0.2369444 , -0.23485549,\n",
       "        -0.23813655, -0.22672745, -0.2257917 , -0.22989296, -0.22890715,\n",
       "        -0.22793387, -0.22119109, -0.25750132, -0.26591425, -0.2549721 ,\n",
       "        -0.2282775 , -0.23483836, -0.22884867, -0.22741282, -0.22428674,\n",
       "        -0.2253416 , -0.22286222, -0.22558822, -0.22406942, -0.25203924,\n",
       "        -0.2722971 , -0.26477856, -0.23571638, -0.22615664, -0.22965531,\n",
       "        -0.2296224 , -0.23021424, -0.23041288, -0.22142919, -0.22782585,\n",
       "        -0.22933277, -0.26709795, -0.26351728, -0.27483594, -0.23651882,\n",
       "        -0.23367263, -0.23271669, -0.2282511 , -0.22819839, -0.23194154,\n",
       "        -0.23095451, -0.22655354, -0.22431308]),\n",
       " 'split1_test_neg_log_loss': array([-0.17958205, -0.17038259, -0.1747961 , -0.17084256, -0.16104221,\n",
       "        -0.17779611, -0.18575998, -0.19877693, -0.17980793, -0.19715739,\n",
       "        -0.22582316, -0.23976003, -0.1750491 , -0.17927887, -0.17713202,\n",
       "        -0.17886236, -0.17285861, -0.17648333, -0.20244484, -0.17027811,\n",
       "        -0.1808197 , -0.19777311, -0.20435944, -0.24251885, -0.18455483,\n",
       "        -0.18027193, -0.18546483, -0.16114001, -0.16292093, -0.1609445 ,\n",
       "        -0.19584178, -0.20029757, -0.20648288, -0.20030724, -0.20717136,\n",
       "        -0.22170095, -0.1712191 , -0.17500968, -0.18408007, -0.16611809,\n",
       "        -0.16311674, -0.16751898, -0.1751036 , -0.17481631, -0.18118571,\n",
       "        -0.17314352, -0.1966431 , -0.18488787, -0.16540006, -0.19851011,\n",
       "        -0.1727038 , -0.17871533, -0.16740173, -0.17240363, -0.17441426,\n",
       "        -0.16971957, -0.19952275, -0.20643446, -0.23730449, -0.24661069,\n",
       "        -0.16397011, -0.17332913, -0.17442743, -0.18775599, -0.19055593,\n",
       "        -0.17831471, -0.19538625, -0.18448155, -0.1958746 , -0.1869912 ,\n",
       "        -0.256506  , -0.2311578 , -0.16653335, -0.1735723 , -0.17428491,\n",
       "        -0.16221397, -0.18171852, -0.17014239, -0.20167841, -0.19834454,\n",
       "        -0.20247124, -0.23125351, -0.22977521, -0.20905493, -0.17750547,\n",
       "        -0.16852471, -0.16933867, -0.1758293 , -0.16543525, -0.16678126,\n",
       "        -0.18404793, -0.17334191, -0.18355196, -0.22367404, -0.21151286,\n",
       "        -0.20087232, -0.90105074, -2.00683188, -0.80804896, -0.69001697,\n",
       "        -1.97029129, -0.66332279, -0.58952796, -1.92489363, -0.58968965,\n",
       "        -0.5769137 , -1.93737704, -0.55332219, -0.90021726, -1.98264238,\n",
       "        -0.8045807 , -0.66227416, -1.92182539, -0.67719944, -0.60905966,\n",
       "        -1.90859512, -0.59199774, -0.54367172, -1.91642581, -0.54290606,\n",
       "        -0.895808  , -1.98604181, -0.97099288, -0.68148119, -1.87182067,\n",
       "        -0.66527616, -0.60242536, -1.98473858, -0.61224014, -0.55473888,\n",
       "        -1.80614174, -0.5609725 , -0.88121066, -1.91068098, -0.84914755,\n",
       "        -0.65513868, -2.0471002 , -0.66593747, -0.60163506, -1.96258174,\n",
       "        -0.59930771, -0.56295345, -1.96856891, -0.5462373 , -0.45623152,\n",
       "        -2.03735161, -0.39984496, -0.2722797 , -1.96806605, -0.27959827,\n",
       "        -0.25827051, -1.75822399, -0.25275978, -0.24052524, -1.80806828,\n",
       "        -0.25308538, -0.31647335, -1.67005823, -0.35000849, -0.27516089,\n",
       "        -1.7206916 , -0.29370429, -0.25774138, -1.68816939, -0.25113684,\n",
       "        -0.24541246, -1.6788882 , -0.24384073, -0.42692727, -1.85345341,\n",
       "        -0.40378923, -0.26652546, -2.10318693, -0.27491713, -0.25593092,\n",
       "        -1.71524572, -0.25551604, -0.24610525, -1.65575745, -0.25480777,\n",
       "        -0.3257985 , -1.86116358, -0.40993117, -0.27014113, -1.77725864,\n",
       "        -0.27437148, -0.25687815, -1.91018195, -0.25817655, -0.25348883,\n",
       "        -1.70621675, -0.23645949, -0.35891549, -0.3699008 , -0.35845289,\n",
       "        -0.2336049 , -0.23214983, -0.23140353, -0.20343208, -0.21093422,\n",
       "        -0.20535088, -0.2084798 , -0.20730769, -0.20104711, -0.3590925 ,\n",
       "        -0.35643453, -0.30073595, -0.23278131, -0.22414206, -0.22516798,\n",
       "        -0.20607934, -0.21000375, -0.21144673, -0.20502054, -0.20617512,\n",
       "        -0.19893278, -0.37290654, -0.36180648, -0.30780503, -0.22486485,\n",
       "        -0.24325448, -0.22306507, -0.21123895, -0.20620456, -0.21176217,\n",
       "        -0.20179492, -0.20465317, -0.20279862, -0.40662687, -0.37177206,\n",
       "        -0.36408575, -0.23922689, -0.23202931, -0.25050728, -0.2247121 ,\n",
       "        -0.22441959, -0.22330028, -0.21582646, -0.2205265 , -0.21890355,\n",
       "        -0.21496376, -0.22028112, -0.23062405, -0.19753025, -0.19084971,\n",
       "        -0.19120133, -0.18713504, -0.19151968, -0.18805446, -0.18198113,\n",
       "        -0.18449046, -0.18576581, -0.25359526, -0.22404238, -0.23355174,\n",
       "        -0.18710982, -0.19344938, -0.19213774, -0.18984206, -0.19817258,\n",
       "        -0.18947742, -0.1808298 , -0.18907542, -0.18396817, -0.25397848,\n",
       "        -0.24025656, -0.23961061, -0.19883278, -0.19614117, -0.19153847,\n",
       "        -0.18821889, -0.18639201, -0.18863515, -0.18197967, -0.18554841,\n",
       "        -0.18208181, -0.2329276 , -0.23395475, -0.31101208, -0.19505671,\n",
       "        -0.19528438, -0.20136673, -0.19081033, -0.19284536, -0.19324855,\n",
       "        -0.19035354, -0.18542362, -0.19130693]),\n",
       " 'split2_test_neg_log_loss': array([-0.24163093, -0.1908157 , -0.21679496, -0.20065082, -0.18082524,\n",
       "        -0.1738185 , -0.20133979, -0.22367561, -0.18915341, -0.22929767,\n",
       "        -0.22993004, -0.18916172, -0.19570065, -0.21501038, -0.18530565,\n",
       "        -0.18641963, -0.17302039, -0.18237137, -0.19673219, -0.19103845,\n",
       "        -0.17831926, -0.20398848, -0.21605294, -0.20934681, -0.20376027,\n",
       "        -0.19877929, -0.1984878 , -0.20575945, -0.19034343, -0.18092396,\n",
       "        -0.20584279, -0.20707677, -0.1962722 , -0.19560833, -0.20478099,\n",
       "        -0.22238532, -0.20809286, -0.20962575, -0.19933172, -0.18669596,\n",
       "        -0.19221276, -0.17857582, -0.18338474, -0.18751664, -0.20558578,\n",
       "        -0.20932246, -0.20673901, -0.20592504, -0.20083141, -0.1999758 ,\n",
       "        -0.19582566, -0.18886134, -0.19263445, -0.18546082, -0.17969279,\n",
       "        -0.18718767, -0.18805791, -0.25414712, -0.2142376 , -0.2458036 ,\n",
       "        -0.20630092, -0.21149348, -0.21321764, -0.18085639, -0.2104782 ,\n",
       "        -0.17996883, -0.18915208, -0.18501052, -0.18455736, -0.19749625,\n",
       "        -0.21834298, -0.22466125, -0.1995789 , -0.19443799, -0.20190822,\n",
       "        -0.19325644, -0.20804625, -0.18185871, -0.21193123, -0.21674472,\n",
       "        -0.18455614, -0.20793267, -0.23908464, -0.24779047, -0.20296146,\n",
       "        -0.20205401, -0.21261601, -0.18934939, -0.18792843, -0.18666126,\n",
       "        -0.19340393, -0.19288559, -0.1961505 , -0.18392109, -0.22248866,\n",
       "        -0.20659009, -0.82978536, -1.87621071, -0.83071429, -0.67000625,\n",
       "        -2.00140035, -0.65319899, -0.58813507, -1.86883809, -0.6177678 ,\n",
       "        -0.55711277, -1.89498919, -0.54670299, -0.83729546, -1.96848549,\n",
       "        -0.77221192, -0.66933362, -1.94171402, -0.66501445, -0.60899449,\n",
       "        -1.89518671, -0.58660357, -0.53699675, -2.02412082, -0.55732986,\n",
       "        -0.8973876 , -1.93486082, -0.8094775 , -0.65700443, -2.01584436,\n",
       "        -0.6749013 , -0.56999615, -1.88891979, -0.60989082, -0.55273618,\n",
       "        -1.9239737 , -0.55446407, -0.8441628 , -1.94172849, -0.91825933,\n",
       "        -0.6512015 , -1.89548702, -0.70094039, -0.58961568, -1.86933002,\n",
       "        -0.58569017, -0.54984789, -1.93508248, -0.56237547, -0.42626308,\n",
       "        -1.79969193, -0.38870385, -0.30198843, -2.00266271, -0.35078442,\n",
       "        -0.27206468, -1.96478951, -0.29593204, -0.26310738, -1.6553265 ,\n",
       "        -0.27080967, -0.40284022, -1.79470622, -0.35891598, -0.29087412,\n",
       "        -1.84111973, -0.29952954, -0.26685026, -1.80240914, -0.2793438 ,\n",
       "        -0.27475717, -1.91786783, -0.26715365, -0.34217893, -1.8516843 ,\n",
       "        -0.37998452, -0.29311004, -1.94646837, -0.29689708, -0.28316822,\n",
       "        -2.12592152, -0.28220486, -0.27068979, -1.91930357, -0.27313455,\n",
       "        -0.3717979 , -2.04867381, -0.35368339, -0.30712473, -1.90437267,\n",
       "        -0.28006342, -0.2799518 , -1.82544228, -0.28769272, -0.27458742,\n",
       "        -1.96717502, -0.27262609, -0.29915019, -0.43785689, -0.36384744,\n",
       "        -0.24307517, -0.24883879, -0.24976991, -0.23639388, -0.23374951,\n",
       "        -0.23350806, -0.22712187, -0.23105579, -0.22978271, -0.48396778,\n",
       "        -0.38948936, -0.37148169, -0.25679976, -0.25126935, -0.25598243,\n",
       "        -0.23424144, -0.23424374, -0.23492307, -0.23149334, -0.22613209,\n",
       "        -0.22852   , -0.37867829, -0.37899415, -0.37752924, -0.25387218,\n",
       "        -0.24061269, -0.24389093, -0.23389682, -0.23073362, -0.22916021,\n",
       "        -0.22854595, -0.23495712, -0.22849174, -0.32520003, -0.41880355,\n",
       "        -0.41319821, -0.25363654, -0.25764339, -0.27220018, -0.2512352 ,\n",
       "        -0.2507374 , -0.24874381, -0.23913124, -0.23963907, -0.24058098,\n",
       "        -0.26970656, -0.25448175, -0.26174924, -0.21879079, -0.23018761,\n",
       "        -0.2263034 , -0.21581406, -0.2197205 , -0.21636137, -0.21245309,\n",
       "        -0.21700965, -0.21306355, -0.26087704, -0.26151834, -0.27408303,\n",
       "        -0.22184583, -0.22145196, -0.21786274, -0.21258469, -0.21666647,\n",
       "        -0.21349092, -0.21720521, -0.21317851, -0.21398589, -0.24796776,\n",
       "        -0.2818662 , -0.24127179, -0.21584151, -0.22666042, -0.21479953,\n",
       "        -0.21658736, -0.21726037, -0.2131181 , -0.21397764, -0.21287982,\n",
       "        -0.21250181, -0.26231036, -0.28321624, -0.26114001, -0.2312527 ,\n",
       "        -0.23047252, -0.22488171, -0.22472437, -0.22358167, -0.22166333,\n",
       "        -0.2203617 , -0.22183397, -0.21853657]),\n",
       " 'split3_test_neg_log_loss': array([-0.25734648, -0.24980179, -0.24648431, -0.27598597, -0.2873185 ,\n",
       "        -0.27241247, -0.31999372, -0.29010588, -0.34648602, -0.34984008,\n",
       "        -0.34594784, -0.33631924, -0.24931325, -0.26286378, -0.2647152 ,\n",
       "        -0.25897483, -0.2766549 , -0.28227619, -0.31387557, -0.27592315,\n",
       "        -0.32840514, -0.3123333 , -0.31983534, -0.3803697 , -0.25848194,\n",
       "        -0.24539749, -0.23712686, -0.26070492, -0.26807393, -0.23990596,\n",
       "        -0.2827173 , -0.28714931, -0.31722588, -0.29768098, -0.30622693,\n",
       "        -0.32776335, -0.23968863, -0.23100441, -0.23524426, -0.23392302,\n",
       "        -0.24416275, -0.23729277, -0.23482165, -0.24076234, -0.24062184,\n",
       "        -0.24845191, -0.25381383, -0.24793444, -0.22954206, -0.24263665,\n",
       "        -0.24941822, -0.28486037, -0.27145686, -0.2845201 , -0.34048605,\n",
       "        -0.28891167, -0.29015063, -0.3974452 , -0.45010235, -0.37193629,\n",
       "        -0.2543841 , -0.24576274, -0.24083952, -0.24428406, -0.28244239,\n",
       "        -0.2619414 , -0.32012602, -0.35274924, -0.26971898, -0.34444601,\n",
       "        -0.36306033, -0.35768199, -0.24474518, -0.24602388, -0.23935373,\n",
       "        -0.27651407, -0.27752892, -0.27705986, -0.28841843, -0.34149591,\n",
       "        -0.31284586, -0.38723336, -0.41869687, -0.41679512, -0.22906731,\n",
       "        -0.23683331, -0.23776062, -0.26152671, -0.2511341 , -0.27141302,\n",
       "        -0.2733602 , -0.29866345, -0.2826299 , -0.29410461, -0.31271865,\n",
       "        -0.29167735, -0.76701871, -1.94676756, -0.83611256, -0.71710787,\n",
       "        -1.99238786, -0.69440588, -0.58425999, -1.87601047, -0.59701715,\n",
       "        -0.54672212, -1.87861307, -0.54821157, -0.7773043 , -1.8680707 ,\n",
       "        -0.84035395, -0.71238601, -1.95621464, -0.72211287, -0.60682403,\n",
       "        -1.86280853, -0.60657905, -0.55683211, -1.91802746, -0.5498742 ,\n",
       "        -0.78190273, -1.97851881, -0.84239198, -0.68240504, -2.01106007,\n",
       "        -0.70365207, -0.61011403, -1.90662637, -0.61492104, -0.53610444,\n",
       "        -2.02583475, -0.53258845, -0.88297963, -1.95064389, -0.81916773,\n",
       "        -0.66566039, -2.21517904, -0.6384574 , -0.59395467, -1.9574097 ,\n",
       "        -0.5892103 , -0.56109769, -2.00792791, -0.56335374, -0.43493535,\n",
       "        -1.77249424, -0.41383875, -0.28754614, -2.41866083, -0.285452  ,\n",
       "        -0.28000844, -1.90526064, -0.28363861, -0.26961683, -1.81481317,\n",
       "        -0.27553119, -0.39898318, -2.09435492, -0.36668963, -0.29756347,\n",
       "        -1.73165902, -0.30311313, -0.28151405, -1.71928679, -0.28028166,\n",
       "        -0.27482838, -1.63636565, -0.27680073, -0.39703575, -1.52450989,\n",
       "        -0.34446128, -0.30125532, -1.6485537 , -0.29336263, -0.28034465,\n",
       "        -1.81881723, -0.27774378, -0.2715953 , -1.81158794, -0.27388089,\n",
       "        -0.38776664, -1.87184148, -0.42438439, -0.34468158, -2.39980678,\n",
       "        -0.31826763, -0.27419306, -1.59439241, -0.27685205, -0.27872147,\n",
       "        -1.88321761, -0.26849313, -0.39622011, -0.44996397, -0.32321151,\n",
       "        -0.25925522, -0.26305238, -0.25253914, -0.24712003, -0.24664284,\n",
       "        -0.24662364, -0.24360874, -0.23643473, -0.23984629, -0.40234555,\n",
       "        -0.37047799, -0.38102383, -0.2632017 , -0.24759322, -0.26141623,\n",
       "        -0.24410431, -0.24940439, -0.24473263, -0.23939351, -0.2401987 ,\n",
       "        -0.23622787, -0.40833491, -0.39377915, -0.35413513, -0.25147633,\n",
       "        -0.25872226, -0.26024477, -0.2458867 , -0.24465387, -0.24673056,\n",
       "        -0.24244887, -0.24271526, -0.2419592 , -0.38143285, -0.38711382,\n",
       "        -0.3203838 , -0.2690269 , -0.27120586, -0.2732234 , -0.25258442,\n",
       "        -0.25532044, -0.25162836, -0.24938373, -0.2438122 , -0.242292  ,\n",
       "        -0.26905049, -0.26709961, -0.25904437, -0.23321859, -0.24151304,\n",
       "        -0.24285525, -0.22869497, -0.22993067, -0.23347741, -0.23479633,\n",
       "        -0.2351342 , -0.23441928, -0.27839387, -0.30740622, -0.25099479,\n",
       "        -0.23510946, -0.2324357 , -0.23430927, -0.22898711, -0.23600615,\n",
       "        -0.23720303, -0.23438383, -0.23611583, -0.23798275, -0.2608655 ,\n",
       "        -0.25841714, -0.26664437, -0.23705846, -0.23663478, -0.24126809,\n",
       "        -0.23965131, -0.23490828, -0.23906142, -0.23511414, -0.23319693,\n",
       "        -0.22911021, -0.27855453, -0.27831995, -0.27754684, -0.23525212,\n",
       "        -0.23797018, -0.23347862, -0.2320902 , -0.23921308, -0.23239686,\n",
       "        -0.23428229, -0.23040082, -0.23000652]),\n",
       " 'split4_test_neg_log_loss': array([-0.20836014, -0.19419496, -0.19311386, -0.18882426, -0.19699909,\n",
       "        -0.20210776, -0.19036591, -0.20538497, -0.18863599, -0.23402855,\n",
       "        -0.21214813, -0.2075527 , -0.19371553, -0.19953493, -0.20300552,\n",
       "        -0.21457891, -0.1980465 , -0.20314217, -0.21799783, -0.22044422,\n",
       "        -0.22791796, -0.20768209, -0.22109899, -0.25909283, -0.20302454,\n",
       "        -0.20003587, -0.1931572 , -0.18516783, -0.19355466, -0.19026662,\n",
       "        -0.21966959, -0.19511437, -0.20750388, -0.22914684, -0.24146776,\n",
       "        -0.2096869 , -0.20295383, -0.19336103, -0.19848508, -0.19512562,\n",
       "        -0.18374173, -0.19513717, -0.21071587, -0.20365469, -0.19784971,\n",
       "        -0.20476044, -0.20269389, -0.2028506 , -0.19075412, -0.19522339,\n",
       "        -0.19746773, -0.20606234, -0.20854369, -0.18632084, -0.21461107,\n",
       "        -0.21451499, -0.2560826 , -0.22575002, -0.23449716, -0.26249878,\n",
       "        -0.19829945, -0.19594086, -0.19814069, -0.21264461, -0.19382424,\n",
       "        -0.19263569, -0.22326179, -0.22567249, -0.23319863, -0.2319986 ,\n",
       "        -0.24249188, -0.23378215, -0.20013899, -0.20282716, -0.20128636,\n",
       "        -0.20455896, -0.20515519, -0.19387523, -0.22393388, -0.1937932 ,\n",
       "        -0.20432111, -0.20708497, -0.27807534, -0.23494629, -0.19605835,\n",
       "        -0.19768017, -0.20881474, -0.18596173, -0.1948803 , -0.1904465 ,\n",
       "        -0.21582311, -0.20260711, -0.21156696, -0.22550526, -0.21242479,\n",
       "        -0.20206549, -0.8723312 , -2.01760184, -0.78602805, -0.69284897,\n",
       "        -1.97212948, -0.70846243, -0.60486075, -1.97410048, -0.57928293,\n",
       "        -0.55821823, -1.98881924, -0.55961649, -0.78895393, -1.9883821 ,\n",
       "        -0.78217807, -0.67555814, -1.98717086, -0.65714863, -0.5787439 ,\n",
       "        -1.92973939, -0.58118866, -0.52627815, -1.94414134, -0.5276123 ,\n",
       "        -0.79441137, -1.97847958, -0.85230776, -0.66455821, -1.96133515,\n",
       "        -0.65845149, -0.57834357, -2.00618514, -0.56631024, -0.55417034,\n",
       "        -1.96945571, -0.55357093, -0.83483669, -1.97131174, -0.87717808,\n",
       "        -0.6717611 , -1.94467623, -0.64779113, -0.60344127, -1.92754671,\n",
       "        -0.59214944, -0.53953028, -1.91160872, -0.56208407, -0.42481896,\n",
       "        -2.12320764, -0.39668208, -0.29366765, -1.92906534, -0.29452216,\n",
       "        -0.25571374, -1.79887693, -0.26498377, -0.2589805 , -1.76821207,\n",
       "        -0.25197733, -0.34989684, -2.03251466, -0.32019742, -0.28320703,\n",
       "        -1.70360826, -0.28350556, -0.26812025, -1.65588806, -0.26472413,\n",
       "        -0.25664355, -1.96054564, -0.26156943, -0.3515736 , -1.81228955,\n",
       "        -0.47510077, -0.27811648, -1.77306533, -0.28672314, -0.26434514,\n",
       "        -1.55956524, -0.27025495, -0.25337805, -1.80878678, -0.2592577 ,\n",
       "        -0.3312002 , -2.08083803, -0.44708696, -0.29255323, -1.65881114,\n",
       "        -0.28281239, -0.26599915, -1.71953837, -0.25943192, -0.25601392,\n",
       "        -1.5521864 , -0.26512636, -0.43599069, -0.31750598, -0.35148223,\n",
       "        -0.24866081, -0.24830123, -0.23113513, -0.22197412, -0.2260781 ,\n",
       "        -0.22545015, -0.21924195, -0.21711443, -0.22015309, -0.38430118,\n",
       "        -0.34590562, -0.3145086 , -0.24255995, -0.23635972, -0.23228456,\n",
       "        -0.22112381, -0.225603  , -0.22580979, -0.21765089, -0.21768977,\n",
       "        -0.22193026, -0.29645463, -0.35162434, -0.37932164, -0.24758153,\n",
       "        -0.23470693, -0.23481387, -0.22193814, -0.23007685, -0.22337362,\n",
       "        -0.21449956, -0.2180425 , -0.21803919, -0.40961754, -0.35774185,\n",
       "        -0.34936241, -0.25043502, -0.25787533, -0.25513475, -0.23637802,\n",
       "        -0.23572571, -0.23439346, -0.23431444, -0.23148351, -0.23015143,\n",
       "        -0.25559047, -0.2325448 , -0.24704983, -0.21724559, -0.21038616,\n",
       "        -0.21066464, -0.20801422, -0.20536686, -0.21272564, -0.20828573,\n",
       "        -0.20465071, -0.21074959, -0.24330264, -0.25804469, -0.28183202,\n",
       "        -0.21769379, -0.21703484, -0.2132137 , -0.21139106, -0.21083816,\n",
       "        -0.20946791, -0.20729234, -0.20890724, -0.20539848, -0.25522583,\n",
       "        -0.23823847, -0.23533617, -0.21769469, -0.20872971, -0.21358456,\n",
       "        -0.21235393, -0.21164618, -0.2114304 , -0.20638003, -0.21008609,\n",
       "        -0.20967097, -0.25963142, -0.28738629, -0.24513876, -0.21292114,\n",
       "        -0.21555284, -0.21929965, -0.21143662, -0.2135954 , -0.21626491,\n",
       "        -0.21407771, -0.21065415, -0.21146627]),\n",
       " 'mean_test_neg_log_loss': array([-0.22011248, -0.20585634, -0.20971567, -0.21675022, -0.20999615,\n",
       "        -0.21323361, -0.22900077, -0.23347384, -0.23024925, -0.25660821,\n",
       "        -0.25678631, -0.2541962 , -0.20845778, -0.21532126, -0.21036631,\n",
       "        -0.21223922, -0.20990853, -0.21472005, -0.23336115, -0.21735429,\n",
       "        -0.24247347, -0.23898049, -0.25055344, -0.2706439 , -0.21368967,\n",
       "        -0.2163869 , -0.2105461 , -0.20862881, -0.20699583, -0.20133815,\n",
       "        -0.23003635, -0.22320347, -0.23349546, -0.24155755, -0.2479093 ,\n",
       "        -0.2446523 , -0.20766877, -0.20614163, -0.2077432 , -0.1999799 ,\n",
       "        -0.20242903, -0.20076173, -0.20564054, -0.20543121, -0.21035625,\n",
       "        -0.21302222, -0.21584575, -0.21606593, -0.2004292 , -0.21247554,\n",
       "        -0.20620698, -0.2168203 , -0.21141219, -0.21199794, -0.22980814,\n",
       "        -0.22420725, -0.23754565, -0.28318958, -0.29470809, -0.28786486,\n",
       "        -0.2099459 , -0.2099347 , -0.20921957, -0.21113883, -0.22164577,\n",
       "        -0.20601131, -0.24111937, -0.23312562, -0.2267893 , -0.24704344,\n",
       "        -0.28506569, -0.26473703, -0.20740097, -0.20745157, -0.20854022,\n",
       "        -0.20918195, -0.22085531, -0.20734208, -0.23020086, -0.25477094,\n",
       "        -0.22765245, -0.27455684, -0.29168807, -0.27501014, -0.20513886,\n",
       "        -0.20426427, -0.20948888, -0.21074693, -0.20577947, -0.20742116,\n",
       "        -0.21759129, -0.22133367, -0.22289768, -0.23347922, -0.24051887,\n",
       "        -0.23196225, -0.83585618, -1.96966074, -0.81834312, -0.68642419,\n",
       "        -1.9734534 , -0.67160644, -0.59640299, -1.92638826, -0.59166679,\n",
       "        -0.56157194, -1.93246159, -0.55326471, -0.83251675, -1.98473601,\n",
       "        -0.80508272, -0.68124674, -1.97276974, -0.67671433, -0.59669311,\n",
       "        -1.91070971, -0.59773929, -0.54969873, -1.93923555, -0.54570644,\n",
       "        -0.84987346, -1.9743994 , -0.86527355, -0.67093741, -1.9612798 ,\n",
       "        -0.67547915, -0.58807545, -1.94546028, -0.60006446, -0.54818529,\n",
       "        -1.92921743, -0.55278491, -0.87967906, -1.952533  , -0.84902782,\n",
       "        -0.66577983, -2.00658351, -0.67222717, -0.60115165, -1.91422927,\n",
       "        -0.59788272, -0.56198408, -1.94198872, -0.55867118, -0.42182194,\n",
       "        -1.89303394, -0.3926076 , -0.28860455, -1.96887101, -0.29815039,\n",
       "        -0.26860574, -1.83831696, -0.27461386, -0.26045159, -1.72257361,\n",
       "        -0.26249791, -0.36063379, -1.9281373 , -0.37267174, -0.286314  ,\n",
       "        -1.76849283, -0.2952897 , -0.26886019, -1.72151929, -0.27203313,\n",
       "        -0.26363231, -1.77465805, -0.26415558, -0.39362526, -1.77079892,\n",
       "        -0.38994486, -0.2873108 , -1.86856308, -0.29407668, -0.27129093,\n",
       "        -1.85293289, -0.27331361, -0.26193079, -1.79593432, -0.26464702,\n",
       "        -0.36482659, -1.98442404, -0.39757324, -0.30285679, -1.9172559 ,\n",
       "        -0.2931453 , -0.27208459, -1.74885654, -0.2723885 , -0.2657577 ,\n",
       "        -1.76620999, -0.26383309, -0.3706372 , -0.39568311, -0.3575556 ,\n",
       "        -0.24792222, -0.24755888, -0.24365941, -0.22916147, -0.23163433,\n",
       "        -0.23113688, -0.22621578, -0.22524595, -0.22575196, -0.40412267,\n",
       "        -0.37415434, -0.35086232, -0.24879548, -0.24347826, -0.24669847,\n",
       "        -0.22868182, -0.23253094, -0.23205651, -0.22592912, -0.22526259,\n",
       "        -0.22435434, -0.36320892, -0.36652374, -0.35461235, -0.24688519,\n",
       "        -0.24399043, -0.24228707, -0.2306577 , -0.23071077, -0.23123128,\n",
       "        -0.2247984 , -0.22712752, -0.22568858, -0.36815071, -0.37577404,\n",
       "        -0.3649916 , -0.25454796, -0.25871009, -0.26278707, -0.24273587,\n",
       "        -0.24332147, -0.24207499, -0.23631066, -0.23639024, -0.23541839,\n",
       "        -0.25594127, -0.24626105, -0.25036842, -0.22074592, -0.2215584 ,\n",
       "        -0.22183223, -0.21327715, -0.21446588, -0.21610237, -0.21328469,\n",
       "        -0.21384378, -0.21303787, -0.25873403, -0.26338518, -0.25908674,\n",
       "        -0.21800728, -0.21984205, -0.21727442, -0.21404355, -0.21719402,\n",
       "        -0.21499618, -0.21251468, -0.21457305, -0.21308094, -0.25401536,\n",
       "        -0.2582151 , -0.2495283 , -0.22102876, -0.21886454, -0.21816919,\n",
       "        -0.21728678, -0.21608422, -0.21653159, -0.21177613, -0.21390742,\n",
       "        -0.21253952, -0.26010437, -0.2692789 , -0.27393473, -0.2222003 ,\n",
       "        -0.22259051, -0.22234868, -0.21746252, -0.21948678, -0.21910304,\n",
       "        -0.21800595, -0.21497322, -0.21512587]),\n",
       " 'std_test_neg_log_loss': array([0.0271002 , 0.02787165, 0.02431431, 0.03898143, 0.04378476,\n",
       "        0.03782462, 0.05049977, 0.03331822, 0.06286798, 0.05242479,\n",
       "        0.04857799, 0.05530446, 0.0267114 , 0.02767592, 0.03120379,\n",
       "        0.02854048, 0.03922311, 0.03852202, 0.04247784, 0.03601727,\n",
       "        0.06078797, 0.04571344, 0.04617756, 0.0579623 , 0.02486724,\n",
       "        0.02969667, 0.02264711, 0.03463674, 0.0355034 , 0.03088074,\n",
       "        0.03129128, 0.03368354, 0.04437073, 0.04244499, 0.03996359,\n",
       "        0.04280462, 0.02214638, 0.02002563, 0.01824299, 0.02376904,\n",
       "        0.02980621, 0.02670125, 0.02301614, 0.0233833 , 0.02101406,\n",
       "        0.02528567, 0.02038835, 0.02357496, 0.02190469, 0.0186675 ,\n",
       "        0.02554375, 0.03756593, 0.03445173, 0.04138277, 0.06026798,\n",
       "        0.04461357, 0.03814083, 0.07139699, 0.08873095, 0.04855544,\n",
       "        0.03006881, 0.02448038, 0.02215475, 0.02419173, 0.03360869,\n",
       "        0.03123297, 0.0503767 , 0.06210342, 0.03221831, 0.05747506,\n",
       "        0.05787344, 0.0499333 , 0.0265515 , 0.02447014, 0.02244883,\n",
       "        0.03745566, 0.03247936, 0.03772669, 0.03033968, 0.06417143,\n",
       "        0.04546386, 0.07450028, 0.06770247, 0.07331048, 0.0181508 ,\n",
       "        0.02249201, 0.02240716, 0.03402742, 0.03061284, 0.03653089,\n",
       "        0.03110433, 0.04418422, 0.03541979, 0.03559837, 0.03788696,\n",
       "        0.03682372, 0.04705821, 0.05274926, 0.01883878, 0.01926085,\n",
       "        0.02428442, 0.02597028, 0.01173977, 0.04886063, 0.01522655,\n",
       "        0.01039822, 0.04100503, 0.00521607, 0.04526325, 0.07892893,\n",
       "        0.02565418, 0.01751248, 0.04716599, 0.02364172, 0.01423359,\n",
       "        0.03182823, 0.01492388, 0.02011571, 0.04537584, 0.01013665,\n",
       "        0.05091697, 0.02059329, 0.05509916, 0.00980191, 0.05230973,\n",
       "        0.01541572, 0.01540623, 0.04462425, 0.01796946, 0.00735547,\n",
       "        0.07239063, 0.01067195, 0.04240219, 0.02647192, 0.04701583,\n",
       "        0.01213482, 0.11586297, 0.02786359, 0.00943473, 0.04469886,\n",
       "        0.01336011, 0.01920375, 0.04260475, 0.00636081, 0.02968305,\n",
       "        0.15674536, 0.01646321, 0.00973155, 0.28328823, 0.02684841,\n",
       "        0.00984876, 0.08234792, 0.01488323, 0.01078875, 0.09674473,\n",
       "        0.00938322, 0.0345712 , 0.16586263, 0.04998706, 0.00753381,\n",
       "        0.06170713, 0.00666736, 0.0076133 , 0.04976119, 0.01242163,\n",
       "        0.01129831, 0.13593496, 0.01132206, 0.04185765, 0.12446384,\n",
       "        0.04797566, 0.01303396, 0.15404571, 0.01432003, 0.01009063,\n",
       "        0.20863502, 0.0098139 , 0.01028766, 0.08416247, 0.00760753,\n",
       "        0.03179857, 0.09689539, 0.03809434, 0.02430413, 0.25474981,\n",
       "        0.01762243, 0.00961939, 0.10912371, 0.01165132, 0.00991502,\n",
       "        0.14519995, 0.01420893, 0.04523952, 0.04807859, 0.02172978,\n",
       "        0.00903072, 0.00983987, 0.01018916, 0.01515364, 0.01242981,\n",
       "        0.01502053, 0.0118965 , 0.01121723, 0.0141767 , 0.04236432,\n",
       "        0.0225292 , 0.03589862, 0.01064865, 0.01194873, 0.01494493,\n",
       "        0.01357203, 0.01386543, 0.01233441, 0.01281513, 0.01235595,\n",
       "        0.01378302, 0.03698204, 0.01758972, 0.02579536, 0.01140403,\n",
       "        0.00796169, 0.01265918, 0.01256221, 0.01356635, 0.01325501,\n",
       "        0.01484939, 0.01384346, 0.01404645, 0.03936651, 0.02594943,\n",
       "        0.03074466, 0.00996532, 0.0150241 , 0.00902088, 0.01068567,\n",
       "        0.01152487, 0.01141904, 0.01136516, 0.00941678, 0.00968872,\n",
       "        0.02121097, 0.01719338, 0.0110877 , 0.01394991, 0.01853271,\n",
       "        0.01892489, 0.01507424, 0.01417297, 0.01606379, 0.01850625,\n",
       "        0.01793302, 0.01595395, 0.01146434, 0.02654848, 0.01718536,\n",
       "        0.01653752, 0.01476844, 0.01464812, 0.01412037, 0.01270191,\n",
       "        0.01602627, 0.01810269, 0.01593487, 0.01815621, 0.00421591,\n",
       "        0.01720054, 0.0133677 , 0.01416589, 0.01448308, 0.01677533,\n",
       "        0.01744766, 0.01706874, 0.01741361, 0.01766147, 0.01664752,\n",
       "        0.01727717, 0.01505314, 0.01941576, 0.02182431, 0.01600121,\n",
       "        0.01559889, 0.01172456, 0.01503162, 0.01566053, 0.01431515,\n",
       "        0.0156091 , 0.0161927 , 0.01340303]),\n",
       " 'rank_test_neg_log_loss': array([ 83,  11,  28,  68,  32,  48, 109, 125, 114, 161, 162, 157,  22,\n",
       "         61,  34,  41,  29,  57, 124,  73, 138, 132, 155, 182,  51,  66,\n",
       "         35,  24,  15,   4, 112,  95, 127, 135, 150, 144,  20,  13,  21,\n",
       "          1,   5,   3,   9,   8,  33,  45,  62,  63,   2,  42,  14,  69,\n",
       "         38,  40, 111,  96, 131, 192, 201, 196,  31,  30,  26,  37,  89,\n",
       "         12, 134, 123, 105, 148, 193, 177,  17,  19,  23,  25,  85,  16,\n",
       "        113, 159, 107, 189, 198, 191,   7,   6,  27,  36,  10,  18,  75,\n",
       "         87,  94, 126, 133, 120, 252, 282, 250, 248, 284, 243, 235, 272,\n",
       "        234, 231, 275, 229, 251, 287, 249, 247, 283, 246, 236, 269, 237,\n",
       "        227, 276, 225, 254, 285, 255, 242, 280, 245, 233, 278, 239, 226,\n",
       "        274, 228, 256, 279, 253, 241, 288, 244, 240, 270, 238, 232, 277,\n",
       "        230, 224, 268, 219, 197, 281, 203, 179, 265, 190, 168, 258, 170,\n",
       "        208, 273, 215, 194, 261, 202, 180, 257, 184, 173, 263, 175, 220,\n",
       "        262, 218, 195, 267, 200, 183, 266, 187, 169, 264, 176, 210, 286,\n",
       "        222, 204, 271, 199, 185, 259, 186, 178, 260, 174, 214, 221, 207,\n",
       "        151, 149, 142, 110, 119, 117, 104,  99, 102, 223, 216, 205, 152,\n",
       "        141, 146, 108, 122, 121, 103, 100,  97, 209, 212, 206, 147, 143,\n",
       "        137, 115, 116, 118,  98, 106, 101, 213, 217, 211, 158, 164, 171,\n",
       "        139, 140, 136, 129, 130, 128, 160, 145, 154,  84,  88,  90,  49,\n",
       "         55,  65,  50,  52,  46, 165, 172, 166,  77,  82,  71,  54,  70,\n",
       "         59,  43,  56,  47, 156, 163, 153,  86,  79,  78,  72,  64,  67,\n",
       "         39,  53,  44, 167, 181, 188,  91,  93,  92,  74,  81,  80,  76,\n",
       "         58,  60])}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRIAL_3_MLP.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL THREE NEG LOG LOSS RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.1,\n",
       " 'classifier__hidden_layer_sizes': (10,),\n",
       " 'classifier__learning_rate': 'constant',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL THREE NEG LOG LOSS RESULTS ---------------------------------------------\")\n",
    "TRIAL_3_MLP.cv_results_['rank_test_neg_log_loss']\n",
    "TRIAL_3_MLP.cv_results_['params'][ np.argmin(TRIAL_3_MLP.cv_results_['rank_test_neg_log_loss']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL THREE F1 RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.0001,\n",
       " 'classifier__hidden_layer_sizes': (10,),\n",
       " 'classifier__learning_rate': 'invscaling',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL THREE F1 RESULTS ---------------------------------------------\")\n",
    "TRIAL_3_MLP.cv_results_['rank_test_f1_micro']\n",
    "TRIAL_3_MLP.cv_results_['params'][ np.argmin(TRIAL_3_MLP.cv_results_['rank_test_f1_micro']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL THREE ROC AUC RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.01,\n",
       " 'classifier__hidden_layer_sizes': (10,),\n",
       " 'classifier__learning_rate': 'adaptive',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL THREE ROC AUC RESULTS ---------------------------------------------\")\n",
    "TRIAL_3_MLP.cv_results_['rank_test_roc_auc_ovo']\n",
    "TRIAL_3_MLP.cv_results_['params'][ np.argmin(TRIAL_3_MLP.cv_results_['rank_test_roc_auc_ovo']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 3 MLP using best NEG LOG LOSS hyperparameters :0.9293294460641399\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 3 MLP using best F1 hyperparameters :0.9262973760932944\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 3 MLP using best ROC_AUC hyperparameters :0.9266472303206997\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best NEG LOG LOSS results to train on test set, \n",
    "p = TRIAL_3_MLP.cv_results_['params'][ np.argmin(TRIAL_3_MLP.cv_results_['rank_test_neg_log_loss']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train__neg_log_loss = pipe.predict(X_train)\n",
    "y_pred_test_neg_log_loss = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 3 MLP using best NEG LOG LOSS hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_neg_log_loss)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best F1 results to train on test set, LR_Trial 1\n",
    "p = TRIAL_3_MLP.cv_results_['params'][ np.argmin(TRIAL_3_MLP.cv_results_['rank_test_f1_micro']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train_roc_auc = pipe.predict(X_train)\n",
    "y_pred_test_f1 = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 3 MLP using best F1 hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_f1)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best ROC_AUC results to train on test set, LR_Trial 1\n",
    "p  = TRIAL_3_MLP.cv_results_['params'][ np.argmin(TRIAL_3_MLP.cv_results_['rank_test_roc_auc_ovo']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train_roc_auc = pipe.predict(X_train)\n",
    "y_pred_test_roc_auc = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 3 MLP using best ROC_AUC hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_roc_auc)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTI LAYERED PERCEPTRON TRIAL FOUR ON GAMMA PARTICLES DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    9.6s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:   11.1s\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:   14.1s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:   15.5s\n",
      "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed:   17.5s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   20.4s\n",
      "[Parallel(n_jobs=-1)]: Done 189 tasks      | elapsed:   22.1s\n",
      "[Parallel(n_jobs=-1)]: Done 210 tasks      | elapsed:   24.4s\n",
      "[Parallel(n_jobs=-1)]: Done 233 tasks      | elapsed:   27.8s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:   29.9s\n",
      "[Parallel(n_jobs=-1)]: Done 281 tasks      | elapsed:   33.5s\n",
      "[Parallel(n_jobs=-1)]: Done 306 tasks      | elapsed:   36.9s\n",
      "[Parallel(n_jobs=-1)]: Done 333 tasks      | elapsed:   40.4s\n",
      "[Parallel(n_jobs=-1)]: Done 360 tasks      | elapsed:   44.3s\n",
      "[Parallel(n_jobs=-1)]: Done 389 tasks      | elapsed:   47.4s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:   52.3s\n",
      "[Parallel(n_jobs=-1)]: Done 449 tasks      | elapsed:   55.5s\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 513 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 546 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 581 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 616 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 653 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 690 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 729 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 809 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 850 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 893 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 936 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 981 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1026 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1073 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1120 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1169 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1269 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1320 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1373 tasks      | elapsed:  4.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FOUR RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1440 out of 1440 | elapsed:  4.6min finished\n"
     ]
    }
   ],
   "source": [
    "ITERATIONS = 1\n",
    "for i in range(1):\n",
    "    #For reach \"trial\", randomly select 5000 data points\n",
    "    random5000DataPoints = beanData.sample(n = 5000, replace = True)\n",
    "    #From that 5000, create x set and y set\n",
    "    xSet = random5000DataPoints.iloc[:,0:15].values\n",
    "    ySet = random5000DataPoints.iloc[:,16].values\n",
    "    ySet = encodeLabel.fit_transform(ySet)\n",
    "    # Run GridSearch to find best hyperparameters per 5000 data points \n",
    "    \n",
    "    pipe = Pipeline([('std', StandardScaler()),('classifier', MLPClassifier(max_iter = 250))])\n",
    "\n",
    "    search_space = [\n",
    "                {\n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                {\n",
    "                 \n",
    "                  \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['sgd'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                \n",
    "                {\n",
    "                \n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['adam'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                ]\n",
    "    \n",
    "    #'precision_micro','roc_auc_ovo','jaccard','neg_log_loss'] JACCARD DOES NOT WORK FOR NON BINARY?\n",
    "    runHyperParamSearch = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=5), \n",
    "                          scoring=['recall_micro','f1_micro','roc_auc_ovo','neg_log_loss'], \n",
    "                          refit=False, verbose=10, n_jobs = -1)\n",
    "    TRIAL_4_MLP = runHyperParamSearch.fit(xSet, ySet)\n",
    "    print(\"---------------------------------TRIAL FOUR RESULTS ---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([1.22245255, 1.24937563, 1.26038599, 1.51330233, 1.51930618,\n",
       "        1.57615585, 1.99291425, 2.04766221, 2.02504296, 2.27355576,\n",
       "        2.28206325, 2.23772292, 1.38348823, 1.26038189, 1.2644876 ,\n",
       "        1.51870656, 1.56404552, 1.59006844, 1.99461503, 2.073384  ,\n",
       "        2.0701808 , 2.45050821, 2.35292211, 2.34921894, 1.43903713,\n",
       "        1.32233744, 1.2929122 , 1.49969063, 1.54342904, 1.59887528,\n",
       "        1.97800336, 2.00102262, 2.07488494, 2.31639237, 2.30618086,\n",
       "        2.1787715 , 1.3370491 , 1.28170128, 1.27239347, 1.50929799,\n",
       "        1.55894032, 1.57985816, 1.96709261, 1.96729355, 2.03304825,\n",
       "        2.3140903 , 2.32099452, 2.2337203 , 1.42492504, 1.3708776 ,\n",
       "        1.39219708, 1.79083943, 1.88532171, 1.91895032, 2.37334223,\n",
       "        2.37203975, 2.35482507, 2.60443974, 2.57861814, 2.54418802,\n",
       "        1.45535026, 1.37878542, 1.34065237, 1.67373967, 1.75881205,\n",
       "        1.81646242, 2.32299881, 2.35512719, 2.31939397, 2.78139248,\n",
       "        2.7770864 , 2.71323266, 1.50168991, 1.40500803, 1.3214355 ,\n",
       "        1.67043653, 1.7643189 , 1.8359798 , 2.38535171, 2.40637026,\n",
       "        2.50285325, 2.87427077, 2.72684069, 2.8554544 , 1.59106565,\n",
       "        1.40921106, 1.39960361, 1.65902734, 1.69465799, 1.73729458,\n",
       "        2.22431564, 2.25333714, 2.29687462, 2.70342455, 2.68340764,\n",
       "        2.62465825, 3.46698093, 0.51063795, 3.21596479, 3.43485394,\n",
       "        0.3827292 , 3.61731095, 3.94269018, 1.13667769, 3.97571898,\n",
       "        4.04137592, 1.11195655, 3.92557549, 3.29483337, 0.39994416,\n",
       "        3.2104609 , 3.4314508 , 0.72902689, 3.4369669 , 3.67756243,\n",
       "        0.91708875, 3.80497174, 3.90305657, 0.99025183, 3.91907024,\n",
       "        3.19194465, 0.34900041, 3.19714952, 3.38661203, 0.75625052,\n",
       "        3.41513739, 3.71109152, 1.05650792, 3.77104273, 3.92797756,\n",
       "        1.37508249, 3.89424901, 3.21206217, 0.42646699, 3.18193669,\n",
       "        3.40783072, 0.49092231, 3.40713396, 3.67405963, 0.86824675,\n",
       "        3.75242662, 3.87263021, 0.83431783, 3.88474069, 3.340973  ,\n",
       "        2.76918163, 3.2469923 , 3.52092776, 3.48379569, 3.61721072,\n",
       "        3.87012801, 3.92137184, 3.97702003, 4.17408967, 4.09552135,\n",
       "        4.11834173, 3.25359755, 3.12558761, 3.24489064, 3.51992726,\n",
       "        3.49400516, 3.64293284, 3.97581916, 3.85241265, 3.97281609,\n",
       "        4.18029518, 4.05788984, 4.15907664, 3.23923669, 3.01449256,\n",
       "        3.32756162, 3.56976967, 3.52823544, 3.59499154, 3.98863034,\n",
       "        3.99463568, 4.04778056, 4.56732745, 4.40408759, 4.53990407,\n",
       "        3.34417605, 3.03981485, 3.25099607, 3.54404793, 3.54264688,\n",
       "        3.67085729, 3.92437468, 3.94459229, 3.98062344, 4.16838512,\n",
       "        4.06990056, 4.12654834, 3.46327777, 3.4127346 , 3.50801678,\n",
       "        3.67456026, 3.7114922 , 3.76033392, 3.95229921, 4.01495323,\n",
       "        3.99743814, 4.19971118, 4.31511054, 4.22983785, 3.55039515,\n",
       "        3.47879181, 3.42234263, 3.70028214, 3.69747992, 3.75542974,\n",
       "        3.97291646, 4.02616272, 4.07750649, 4.28678689, 4.21222248,\n",
       "        4.1946064 , 3.44606295, 3.51998467, 3.460776  , 3.65914702,\n",
       "        3.75172634, 3.74692211, 4.09682355, 4.04748106, 4.06339502,\n",
       "        4.26126475, 4.30700359, 4.21752691, 3.48839946, 3.4862978 ,\n",
       "        3.45497141, 3.65824609, 3.68456893, 3.72330194, 3.97391763,\n",
       "        4.0888164 , 4.08421254, 4.21482468, 4.23994598, 4.20121326,\n",
       "        3.52603221, 3.49710732, 3.50461373, 3.81978483, 3.92467542,\n",
       "        3.80917568, 4.30720434, 4.26666956, 4.39117641, 4.53419914,\n",
       "        4.45803337, 4.47414761, 3.61130557, 3.57637534, 3.58908625,\n",
       "        3.87283058, 3.86592417, 3.93588486, 4.32642074, 4.18129606,\n",
       "        4.37165966, 4.80923605, 4.41920042, 4.61366735, 3.59719343,\n",
       "        3.52322979, 3.5317369 , 3.87613344, 3.91917019, 3.84710917,\n",
       "        4.2582623 , 4.31641197, 4.2860857 , 4.53770242, 4.5629241 ,\n",
       "        4.43841686, 3.56436467, 3.54184561, 3.67446008, 4.32111611,\n",
       "        4.10523052, 4.53219748, 4.59275045, 4.61536918, 4.53299804,\n",
       "        4.3570477 , 4.10362926, 3.40803013]),\n",
       " 'std_fit_time': array([0.0430068 , 0.01504821, 0.02151647, 0.02183324, 0.01418655,\n",
       "        0.04685086, 0.1240954 , 0.03029607, 0.02704926, 0.04035563,\n",
       "        0.07496356, 0.05717456, 0.01547368, 0.04534055, 0.04790711,\n",
       "        0.04860393, 0.02521518, 0.06436064, 0.10021114, 0.11563771,\n",
       "        0.11218864, 0.09715963, 0.09752642, 0.0739825 , 0.0617339 ,\n",
       "        0.02793906, 0.02063453, 0.02975141, 0.03524464, 0.06102216,\n",
       "        0.05464525, 0.02691015, 0.01906377, 0.03838088, 0.0249711 ,\n",
       "        0.08867271, 0.04987012, 0.03471305, 0.02227787, 0.05543133,\n",
       "        0.09025115, 0.08034585, 0.03398876, 0.02811575, 0.02608065,\n",
       "        0.07058237, 0.07479514, 0.06947524, 0.10856241, 0.02771883,\n",
       "        0.05601875, 0.08476589, 0.0383627 , 0.06033785, 0.09894963,\n",
       "        0.04793199, 0.05549715, 0.08239281, 0.05179554, 0.04411953,\n",
       "        0.02630759, 0.03444427, 0.02764093, 0.04956287, 0.06047282,\n",
       "        0.07338559, 0.07481849, 0.06320613, 0.06472987, 0.11099562,\n",
       "        0.18075025, 0.07397119, 0.03973556, 0.04204811, 0.02573808,\n",
       "        0.029732  , 0.05549617, 0.07030614, 0.14119973, 0.12464553,\n",
       "        0.07348132, 0.20710651, 0.14516607, 0.17009374, 0.08935947,\n",
       "        0.08664979, 0.04275957, 0.03737171, 0.01674359, 0.03507982,\n",
       "        0.03362979, 0.02955341, 0.08682933, 0.05877104, 0.09075266,\n",
       "        0.05270892, 0.20172065, 0.26010708, 0.05014659, 0.0507199 ,\n",
       "        0.14330846, 0.11230117, 0.12976064, 0.26291074, 0.16104917,\n",
       "        0.09049967, 0.32173345, 0.14795972, 0.09986752, 0.15070419,\n",
       "        0.05260376, 0.05318044, 0.42737498, 0.01373956, 0.01580365,\n",
       "        0.66085096, 0.09926766, 0.083815  , 0.36848639, 0.076616  ,\n",
       "        0.0390741 , 0.10096072, 0.02661886, 0.03088744, 0.39755143,\n",
       "        0.03221917, 0.05258049, 0.35190192, 0.05556922, 0.09436518,\n",
       "        0.44898533, 0.08627344, 0.03049399, 0.16257302, 0.01902039,\n",
       "        0.03403824, 0.11185645, 0.02544648, 0.04578164, 0.40020097,\n",
       "        0.05056025, 0.04377472, 0.33832514, 0.04283694, 0.06953902,\n",
       "        0.92712775, 0.03364516, 0.05023732, 0.03079438, 0.06254084,\n",
       "        0.03319145, 0.05444097, 0.05034759, 0.0730667 , 0.08163685,\n",
       "        0.0281586 , 0.0342435 , 0.21208471, 0.02934307, 0.06647913,\n",
       "        0.05860678, 0.07982873, 0.10614924, 0.0341368 , 0.06641887,\n",
       "        0.05454071, 0.06993656, 0.05684486, 0.04374356, 0.4405797 ,\n",
       "        0.07703047, 0.05278869, 0.03820094, 0.03431427, 0.08719333,\n",
       "        0.07782471, 0.11067427, 0.14089562, 0.0987569 , 0.16045835,\n",
       "        0.12278986, 0.38714642, 0.06235205, 0.01465853, 0.07499519,\n",
       "        0.04828563, 0.02347897, 0.07464115, 0.05080457, 0.04504795,\n",
       "        0.04196131, 0.06471758, 0.0313262 , 0.04378223, 0.0676178 ,\n",
       "        0.08723177, 0.05324761, 0.0321201 , 0.02751999, 0.02091318,\n",
       "        0.0275648 , 0.05057205, 0.10627386, 0.09894469, 0.10598512,\n",
       "        0.06796511, 0.03640079, 0.07681043, 0.04391098, 0.04976671,\n",
       "        0.05923306, 0.05564741, 0.05393914, 0.09339485, 0.04412832,\n",
       "        0.07554835, 0.04716109, 0.09704468, 0.06902246, 0.04242275,\n",
       "        0.08060388, 0.05539165, 0.12104083, 0.0757496 , 0.05288019,\n",
       "        0.09146008, 0.06556471, 0.09226674, 0.05804195, 0.0802464 ,\n",
       "        0.05979203, 0.05798041, 0.05325497, 0.04343832, 0.0796127 ,\n",
       "        0.16379518, 0.14870299, 0.05836378, 0.08851016, 0.07113928,\n",
       "        0.05486773, 0.03857569, 0.04063833, 0.09951403, 0.08919483,\n",
       "        0.05997588, 0.1716111 , 0.16273529, 0.15787828, 0.08400814,\n",
       "        0.0496609 , 0.06413512, 0.08911081, 0.1291679 , 0.13784356,\n",
       "        0.0508486 , 0.18576009, 0.17690103, 0.11556037, 0.05458584,\n",
       "        0.19690097, 0.35577533, 0.10224336, 0.10121399, 0.18206774,\n",
       "        0.07565765, 0.10732136, 0.11047865, 0.1410423 , 0.08166012,\n",
       "        0.04928953, 0.10763734, 0.14303947, 0.08926906, 0.15176797,\n",
       "        0.08006354, 0.05690101, 0.11235793, 0.12349887, 0.17994711,\n",
       "        0.16165769, 0.03407418, 0.12475365, 0.08203016, 0.06106266,\n",
       "        0.1848465 , 0.12175886, 0.20348178]),\n",
       " 'mean_score_time': array([0.03723249, 0.03523026, 0.03713112, 0.03843265, 0.03843336,\n",
       "        0.04243765, 0.04453931, 0.04323735, 0.04423733, 0.04553914,\n",
       "        0.03903489, 0.03763242, 0.0433373 , 0.03683205, 0.036832  ,\n",
       "        0.04123492, 0.04313807, 0.0545475 , 0.04874196, 0.05895228,\n",
       "        0.04904237, 0.05835123, 0.0431376 , 0.03623133, 0.03993382,\n",
       "        0.04273648, 0.03923459, 0.03933315, 0.04573817, 0.04193554,\n",
       "        0.04473653, 0.04603729, 0.05024457, 0.04433742, 0.04333768,\n",
       "        0.03823366, 0.03653116, 0.03653078, 0.04093523, 0.03913345,\n",
       "        0.04123664, 0.05124402, 0.04634156, 0.04824214, 0.04704065,\n",
       "        0.05364604, 0.04113665, 0.03703089, 0.04153547, 0.04013515,\n",
       "        0.05764933, 0.04553971, 0.04864221, 0.04473825, 0.04864082,\n",
       "        0.04363708, 0.04804044, 0.04463887, 0.0393333 , 0.03993235,\n",
       "        0.04053464, 0.03753233, 0.04083538, 0.04143572, 0.04273791,\n",
       "        0.04533811, 0.05204597, 0.05224409, 0.05234504, 0.05354605,\n",
       "        0.04613919, 0.03873224, 0.0364315 , 0.03512907, 0.04383769,\n",
       "        0.04643989, 0.04413762, 0.05344639, 0.04744101, 0.04593906,\n",
       "        0.05114441, 0.05484781, 0.05444751, 0.04463778, 0.04043632,\n",
       "        0.03973379, 0.03903399, 0.03843269, 0.03953462, 0.04493833,\n",
       "        0.0505415 , 0.05334668, 0.04644051, 0.05294428, 0.04643874,\n",
       "        0.04353518, 0.03593063, 0.03643188, 0.0384335 , 0.04623947,\n",
       "        0.03563085, 0.03883362, 0.03763275, 0.03933372, 0.05014343,\n",
       "        0.03683176, 0.03703189, 0.03953428, 0.04043479, 0.03553019,\n",
       "        0.03753262, 0.03583102, 0.03543043, 0.03652005, 0.03693218,\n",
       "        0.03573084, 0.03643193, 0.03733201, 0.03603129, 0.03693175,\n",
       "        0.03843331, 0.03873324, 0.03473039, 0.03913393, 0.03472981,\n",
       "        0.03513036, 0.03783259, 0.03593106, 0.03823285, 0.03733225,\n",
       "        0.03563085, 0.03753262, 0.03673148, 0.03512983, 0.03603096,\n",
       "        0.03893404, 0.03633156, 0.03712821, 0.03823323, 0.03643174,\n",
       "        0.03843322, 0.03863349, 0.03563056, 0.03603125, 0.03452988,\n",
       "        0.03613129, 0.03653159, 0.03833313, 0.03683176, 0.0407352 ,\n",
       "        0.03903418, 0.04213662, 0.03823271, 0.03643184, 0.03803329,\n",
       "        0.03643169, 0.0365315 , 0.03763227, 0.03753214, 0.03643155,\n",
       "        0.03623118, 0.03813281, 0.03943419, 0.03823304, 0.03753204,\n",
       "        0.03883348, 0.03813267, 0.03643155, 0.03988309, 0.04073548,\n",
       "        0.0389338 , 0.03653164, 0.03623028, 0.0368319 , 0.03933392,\n",
       "        0.04173541, 0.04393826, 0.04293694, 0.03643103, 0.03743234,\n",
       "        0.0353302 , 0.03563018, 0.04223609, 0.04333725, 0.04083486,\n",
       "        0.0373323 , 0.0410358 , 0.03793221, 0.04053488, 0.03833251,\n",
       "        0.03693185, 0.03673177, 0.03563151, 0.03763256, 0.03753233,\n",
       "        0.03593059, 0.03683181, 0.03653173, 0.0369319 , 0.03653121,\n",
       "        0.03713183, 0.03833361, 0.04243631, 0.04413838, 0.03588896,\n",
       "        0.03563066, 0.03693199, 0.03633108, 0.03723211, 0.03613114,\n",
       "        0.03853307, 0.04653983, 0.04343739, 0.03713255, 0.04333758,\n",
       "        0.0359314 , 0.03603134, 0.04348016, 0.03633156, 0.03653121,\n",
       "        0.04263682, 0.03893356, 0.0369319 , 0.03663158, 0.03763232,\n",
       "        0.04573998, 0.04003448, 0.03693185, 0.03593078, 0.0377326 ,\n",
       "        0.03623099, 0.03533053, 0.03553057, 0.03963432, 0.03963389,\n",
       "        0.03883352, 0.03973441, 0.03663158, 0.03973398, 0.038833  ,\n",
       "        0.03613105, 0.03793278, 0.03603101, 0.04974289, 0.03913302,\n",
       "        0.03933396, 0.03773265, 0.03633146, 0.03813267, 0.03923392,\n",
       "        0.03723216, 0.04213624, 0.03693213, 0.04243665, 0.04213629,\n",
       "        0.03593049, 0.04233656, 0.03733177, 0.04403782, 0.03653178,\n",
       "        0.0412354 , 0.03943396, 0.03983445, 0.03633113, 0.03683171,\n",
       "        0.03703203, 0.04083509, 0.03593092, 0.03943419, 0.03593097,\n",
       "        0.04043469, 0.03833337, 0.03983474, 0.0432373 , 0.03993449,\n",
       "        0.03673201, 0.04063535, 0.04213614, 0.04123545, 0.03953419,\n",
       "        0.04583974, 0.05154419, 0.04333696, 0.03903365, 0.03793287,\n",
       "        0.03382854, 0.02612171, 0.02151856]),\n",
       " 'std_score_time': array([0.00248336, 0.00081287, 0.00193625, 0.00317206, 0.00165618,\n",
       "        0.00328072, 0.00181767, 0.00314206, 0.00537611, 0.00482097,\n",
       "        0.00376005, 0.00132024, 0.00406207, 0.00175044, 0.00206474,\n",
       "        0.00341786, 0.00342987, 0.01206191, 0.00564995, 0.01121519,\n",
       "        0.00491428, 0.01239676, 0.00334051, 0.00250448, 0.0036557 ,\n",
       "        0.00329879, 0.00248435, 0.00160108, 0.00950847, 0.0014992 ,\n",
       "        0.00103049, 0.00230436, 0.00262251, 0.00201626, 0.00796755,\n",
       "        0.00353283, 0.00070716, 0.00070743, 0.00650525, 0.00139417,\n",
       "        0.00376602, 0.01092551, 0.00614328, 0.00486869, 0.00400454,\n",
       "        0.00975708, 0.00278494, 0.00184614, 0.00496415, 0.00521388,\n",
       "        0.01223134, 0.00465155, 0.00545822, 0.00216017, 0.00334149,\n",
       "        0.00213314, 0.00440747, 0.00156382, 0.00376731, 0.00365448,\n",
       "        0.00179031, 0.00164396, 0.01091618, 0.00255839, 0.00388338,\n",
       "        0.00441648, 0.00330463, 0.0048057 , 0.01128233, 0.01040624,\n",
       "        0.00791476, 0.00201656, 0.00177484, 0.00213128, 0.00735222,\n",
       "        0.00489784, 0.00487579, 0.00796634, 0.00440179, 0.00324875,\n",
       "        0.00921282, 0.01038496, 0.01097941, 0.00667228, 0.00567364,\n",
       "        0.00387089, 0.00339453, 0.00106803, 0.00114034, 0.00638127,\n",
       "        0.00519265, 0.00972901, 0.00182909, 0.01100286, 0.00826746,\n",
       "        0.00811946, 0.00124173, 0.00146425, 0.00487658, 0.01845683,\n",
       "        0.00058374, 0.00380508, 0.00180177, 0.00623918, 0.0140241 ,\n",
       "        0.00120948, 0.00274111, 0.00537133, 0.00664183, 0.00054838,\n",
       "        0.00255201, 0.00060031, 0.00080085, 0.00072296, 0.00106864,\n",
       "        0.00081269, 0.00049015, 0.0005103 , 0.0004475 , 0.00139375,\n",
       "        0.00360027, 0.00537703, 0.00040009, 0.00361417, 0.00060023,\n",
       "        0.00058379, 0.0006789 , 0.0015631 , 0.00350411, 0.00153746,\n",
       "        0.00086105, 0.00187252, 0.00116716, 0.00115876, 0.00141529,\n",
       "        0.00381593, 0.00265908, 0.00208969, 0.00299564, 0.00135771,\n",
       "        0.00432181, 0.00529037, 0.00073567, 0.00104973, 0.00089506,\n",
       "        0.00159513, 0.00137986, 0.006628  , 0.00107784, 0.00801075,\n",
       "        0.00603023, 0.00696551, 0.00103005, 0.00058324, 0.00141558,\n",
       "        0.00086089, 0.00134252, 0.00376354, 0.00381069, 0.00128207,\n",
       "        0.00087258, 0.00292477, 0.00317076, 0.00183456, 0.0017902 ,\n",
       "        0.00369948, 0.00058323, 0.00073568, 0.00692743, 0.00722816,\n",
       "        0.00708001, 0.00044729, 0.00067717, 0.00128919, 0.00326794,\n",
       "        0.00393529, 0.00545855, 0.00507772, 0.00115882, 0.00259819,\n",
       "        0.00060066, 0.00073543, 0.0080669 , 0.00854889, 0.00646031,\n",
       "        0.00120973, 0.00709134, 0.00135747, 0.0065401 , 0.00081311,\n",
       "        0.00120056, 0.00124979, 0.00080102, 0.0049982 , 0.00207552,\n",
       "        0.00180163, 0.00136463, 0.00063339, 0.00174507, 0.0007078 ,\n",
       "        0.00037415, 0.00269674, 0.00818258, 0.00806565, 0.00171067,\n",
       "        0.00086094, 0.00241871, 0.00074904, 0.0007493 , 0.00097075,\n",
       "        0.00358109, 0.01383221, 0.00839996, 0.0005838 , 0.00719406,\n",
       "        0.00106865, 0.00104998, 0.0158117 , 0.00188838, 0.00192487,\n",
       "        0.01095661, 0.0036006 , 0.00073473, 0.00086104, 0.00263658,\n",
       "        0.0123374 , 0.00260974, 0.00168672, 0.00233413, 0.00546981,\n",
       "        0.00248393, 0.00051008, 0.00044793, 0.0067542 , 0.00493798,\n",
       "        0.00372554, 0.00615851, 0.0009709 , 0.00647542, 0.00566719,\n",
       "        0.00124208, 0.00409478, 0.0015179 , 0.01782982, 0.00504797,\n",
       "        0.0050901 , 0.0015698 , 0.00051033, 0.0025    , 0.00304565,\n",
       "        0.0010305 , 0.00562074, 0.00486585, 0.00825606, 0.00717134,\n",
       "        0.00231265, 0.00762632, 0.002251  , 0.00727217, 0.00148509,\n",
       "        0.00611748, 0.00655874, 0.00394786, 0.00120966, 0.0025042 ,\n",
       "        0.00324323, 0.01087469, 0.00097086, 0.00407031, 0.00080092,\n",
       "        0.00740532, 0.00262118, 0.00573736, 0.0124825 , 0.00588208,\n",
       "        0.00401041, 0.0071995 , 0.00835212, 0.00622314, 0.00419877,\n",
       "        0.00484785, 0.01402649, 0.01240179, 0.00378507, 0.00135748,\n",
       "        0.00369884, 0.00285573, 0.00054825]),\n",
       " 'param_classifier': masked_array(data=[MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__activation': masked_array(data=['logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__alpha': masked_array(data=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__hidden_layer_sizes': masked_array(data=[(5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__learning_rate': masked_array(data=['constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'}],\n",
       " 'split0_test_recall_micro': array([0.932, 0.929, 0.937, 0.932, 0.933, 0.929, 0.923, 0.933, 0.923,\n",
       "        0.931, 0.927, 0.922, 0.933, 0.936, 0.935, 0.93 , 0.941, 0.929,\n",
       "        0.936, 0.926, 0.933, 0.924, 0.925, 0.927, 0.935, 0.931, 0.934,\n",
       "        0.934, 0.934, 0.932, 0.931, 0.939, 0.935, 0.928, 0.931, 0.931,\n",
       "        0.933, 0.931, 0.934, 0.937, 0.932, 0.933, 0.927, 0.925, 0.93 ,\n",
       "        0.925, 0.929, 0.927, 0.934, 0.932, 0.936, 0.929, 0.932, 0.928,\n",
       "        0.933, 0.923, 0.936, 0.923, 0.93 , 0.927, 0.937, 0.928, 0.934,\n",
       "        0.931, 0.937, 0.93 , 0.924, 0.929, 0.922, 0.919, 0.925, 0.929,\n",
       "        0.932, 0.929, 0.933, 0.929, 0.931, 0.933, 0.922, 0.923, 0.934,\n",
       "        0.93 , 0.928, 0.933, 0.934, 0.93 , 0.932, 0.93 , 0.93 , 0.933,\n",
       "        0.93 , 0.925, 0.924, 0.921, 0.937, 0.939, 0.735, 0.037, 0.764,\n",
       "        0.803, 0.152, 0.805, 0.837, 0.172, 0.827, 0.882, 0.191, 0.864,\n",
       "        0.639, 0.196, 0.753, 0.809, 0.273, 0.802, 0.835, 0.139, 0.849,\n",
       "        0.847, 0.107, 0.87 , 0.723, 0.25 , 0.755, 0.805, 0.261, 0.826,\n",
       "        0.836, 0.211, 0.874, 0.887, 0.192, 0.864, 0.745, 0.169, 0.745,\n",
       "        0.801, 0.189, 0.817, 0.83 , 0.335, 0.838, 0.84 , 0.191, 0.858,\n",
       "        0.907, 0.171, 0.923, 0.917, 0.352, 0.923, 0.917, 0.504, 0.914,\n",
       "        0.914, 0.245, 0.926, 0.882, 0.23 , 0.917, 0.925, 0.011, 0.919,\n",
       "        0.919, 0.108, 0.914, 0.925, 0.523, 0.92 , 0.906, 0.482, 0.881,\n",
       "        0.915, 0.375, 0.92 , 0.925, 0.213, 0.924, 0.919, 0.427, 0.924,\n",
       "        0.912, 0.146, 0.911, 0.914, 0.326, 0.915, 0.919, 0.404, 0.919,\n",
       "        0.926, 0.406, 0.919, 0.884, 0.866, 0.91 , 0.915, 0.921, 0.933,\n",
       "        0.924, 0.93 , 0.929, 0.931, 0.937, 0.932, 0.874, 0.878, 0.912,\n",
       "        0.926, 0.926, 0.925, 0.929, 0.928, 0.929, 0.929, 0.928, 0.931,\n",
       "        0.878, 0.891, 0.879, 0.927, 0.921, 0.924, 0.929, 0.924, 0.926,\n",
       "        0.931, 0.931, 0.929, 0.876, 0.879, 0.912, 0.919, 0.923, 0.921,\n",
       "        0.924, 0.926, 0.927, 0.924, 0.926, 0.927, 0.915, 0.917, 0.922,\n",
       "        0.928, 0.931, 0.928, 0.931, 0.931, 0.931, 0.932, 0.932, 0.932,\n",
       "        0.918, 0.918, 0.912, 0.925, 0.929, 0.937, 0.929, 0.928, 0.934,\n",
       "        0.932, 0.933, 0.934, 0.915, 0.915, 0.915, 0.93 , 0.925, 0.931,\n",
       "        0.932, 0.929, 0.931, 0.931, 0.937, 0.933, 0.921, 0.926, 0.914,\n",
       "        0.93 , 0.924, 0.931, 0.929, 0.932, 0.934, 0.938, 0.934, 0.933]),\n",
       " 'split1_test_recall_micro': array([0.926, 0.924, 0.92 , 0.927, 0.924, 0.923, 0.922, 0.916, 0.925,\n",
       "        0.924, 0.922, 0.924, 0.917, 0.925, 0.924, 0.925, 0.923, 0.922,\n",
       "        0.924, 0.928, 0.931, 0.924, 0.926, 0.919, 0.924, 0.927, 0.929,\n",
       "        0.928, 0.931, 0.92 , 0.934, 0.927, 0.932, 0.93 , 0.925, 0.927,\n",
       "        0.926, 0.927, 0.925, 0.928, 0.927, 0.925, 0.93 , 0.929, 0.93 ,\n",
       "        0.93 , 0.925, 0.928, 0.921, 0.917, 0.924, 0.925, 0.926, 0.924,\n",
       "        0.932, 0.92 , 0.919, 0.926, 0.92 , 0.922, 0.928, 0.924, 0.927,\n",
       "        0.923, 0.929, 0.923, 0.924, 0.933, 0.923, 0.927, 0.919, 0.928,\n",
       "        0.921, 0.923, 0.925, 0.927, 0.928, 0.925, 0.925, 0.922, 0.926,\n",
       "        0.927, 0.927, 0.938, 0.931, 0.92 , 0.927, 0.933, 0.925, 0.924,\n",
       "        0.931, 0.93 , 0.922, 0.931, 0.921, 0.923, 0.767, 0.14 , 0.723,\n",
       "        0.823, 0.26 , 0.818, 0.849, 0.033, 0.866, 0.89 , 0.258, 0.899,\n",
       "        0.728, 0.153, 0.699, 0.807, 0.093, 0.84 , 0.834, 0.148, 0.869,\n",
       "        0.886, 0.268, 0.894, 0.771, 0.148, 0.737, 0.788, 0.098, 0.826,\n",
       "        0.849, 0.14 , 0.834, 0.861, 0.26 , 0.893, 0.737, 0.039, 0.725,\n",
       "        0.819, 0.081, 0.818, 0.835, 0.26 , 0.838, 0.87 , 0.26 , 0.862,\n",
       "        0.871, 0.408, 0.872, 0.917, 0.343, 0.917, 0.917, 0.425, 0.912,\n",
       "        0.916, 0.49 , 0.926, 0.905, 0.17 , 0.915, 0.918, 0.151, 0.916,\n",
       "        0.925, 0.269, 0.917, 0.923, 0.287, 0.918, 0.868, 0.239, 0.872,\n",
       "        0.921, 0.089, 0.916, 0.913, 0.065, 0.919, 0.929, 0.309, 0.923,\n",
       "        0.9  , 0.099, 0.867, 0.921, 0.104, 0.92 , 0.922, 0.397, 0.914,\n",
       "        0.922, 0.189, 0.918, 0.881, 0.883, 0.872, 0.924, 0.921, 0.921,\n",
       "        0.925, 0.926, 0.926, 0.926, 0.924, 0.927, 0.878, 0.865, 0.918,\n",
       "        0.925, 0.923, 0.927, 0.924, 0.925, 0.927, 0.925, 0.926, 0.928,\n",
       "        0.914, 0.873, 0.907, 0.924, 0.918, 0.923, 0.927, 0.924, 0.926,\n",
       "        0.925, 0.928, 0.923, 0.878, 0.875, 0.882, 0.924, 0.926, 0.924,\n",
       "        0.924, 0.925, 0.92 , 0.931, 0.924, 0.925, 0.918, 0.922, 0.921,\n",
       "        0.921, 0.926, 0.927, 0.925, 0.927, 0.925, 0.924, 0.926, 0.924,\n",
       "        0.917, 0.924, 0.923, 0.927, 0.927, 0.923, 0.927, 0.925, 0.925,\n",
       "        0.931, 0.927, 0.925, 0.92 , 0.918, 0.917, 0.925, 0.921, 0.922,\n",
       "        0.93 , 0.925, 0.927, 0.92 , 0.928, 0.927, 0.918, 0.926, 0.918,\n",
       "        0.925, 0.931, 0.923, 0.923, 0.922, 0.928, 0.928, 0.922, 0.925]),\n",
       " 'split2_test_recall_micro': array([0.931, 0.924, 0.93 , 0.924, 0.934, 0.926, 0.924, 0.939, 0.926,\n",
       "        0.932, 0.924, 0.934, 0.927, 0.93 , 0.924, 0.934, 0.923, 0.924,\n",
       "        0.923, 0.93 , 0.926, 0.916, 0.928, 0.919, 0.923, 0.931, 0.926,\n",
       "        0.931, 0.929, 0.927, 0.925, 0.926, 0.926, 0.926, 0.915, 0.934,\n",
       "        0.924, 0.928, 0.928, 0.928, 0.929, 0.927, 0.93 , 0.923, 0.932,\n",
       "        0.93 , 0.927, 0.929, 0.921, 0.928, 0.925, 0.927, 0.927, 0.928,\n",
       "        0.937, 0.938, 0.93 , 0.928, 0.93 , 0.931, 0.93 , 0.926, 0.92 ,\n",
       "        0.928, 0.93 , 0.924, 0.929, 0.933, 0.928, 0.94 , 0.922, 0.935,\n",
       "        0.926, 0.928, 0.922, 0.932, 0.934, 0.929, 0.93 , 0.921, 0.937,\n",
       "        0.937, 0.923, 0.922, 0.925, 0.927, 0.927, 0.927, 0.93 , 0.929,\n",
       "        0.93 , 0.929, 0.933, 0.927, 0.929, 0.924, 0.754, 0.114, 0.696,\n",
       "        0.828, 0.141, 0.813, 0.841, 0.078, 0.838, 0.866, 0.114, 0.883,\n",
       "        0.743, 0.04 , 0.746, 0.793, 0.265, 0.821, 0.846, 0.039, 0.822,\n",
       "        0.878, 0.26 , 0.87 , 0.756, 0.19 , 0.755, 0.821, 0.181, 0.806,\n",
       "        0.847, 0.055, 0.843, 0.845, 0.138, 0.848, 0.752, 0.149, 0.737,\n",
       "        0.844, 0.2  , 0.788, 0.833, 0.045, 0.807, 0.842, 0.256, 0.845,\n",
       "        0.883, 0.074, 0.892, 0.913, 0.332, 0.911, 0.911, 0.498, 0.912,\n",
       "        0.916, 0.418, 0.912, 0.898, 0.393, 0.91 , 0.91 , 0.245, 0.909,\n",
       "        0.909, 0.261, 0.909, 0.914, 0.435, 0.916, 0.908, 0.141, 0.868,\n",
       "        0.911, 0.248, 0.908, 0.913, 0.328, 0.908, 0.913, 0.358, 0.911,\n",
       "        0.88 , 0.204, 0.905, 0.925, 0.273, 0.915, 0.907, 0.341, 0.917,\n",
       "        0.912, 0.332, 0.913, 0.876, 0.882, 0.882, 0.92 , 0.916, 0.918,\n",
       "        0.918, 0.92 , 0.916, 0.92 , 0.919, 0.922, 0.912, 0.871, 0.866,\n",
       "        0.917, 0.921, 0.912, 0.916, 0.916, 0.921, 0.921, 0.922, 0.919,\n",
       "        0.913, 0.864, 0.901, 0.913, 0.91 , 0.916, 0.914, 0.915, 0.915,\n",
       "        0.919, 0.919, 0.92 , 0.913, 0.913, 0.862, 0.918, 0.916, 0.921,\n",
       "        0.916, 0.913, 0.916, 0.916, 0.918, 0.921, 0.874, 0.917, 0.915,\n",
       "        0.924, 0.924, 0.92 , 0.924, 0.924, 0.921, 0.923, 0.919, 0.918,\n",
       "        0.917, 0.916, 0.922, 0.921, 0.92 , 0.918, 0.922, 0.92 , 0.925,\n",
       "        0.92 , 0.927, 0.92 , 0.916, 0.915, 0.917, 0.917, 0.921, 0.921,\n",
       "        0.922, 0.921, 0.919, 0.919, 0.924, 0.923, 0.915, 0.918, 0.916,\n",
       "        0.916, 0.92 , 0.922, 0.922, 0.921, 0.921, 0.919, 0.92 , 0.919]),\n",
       " 'split3_test_recall_micro': array([0.933, 0.931, 0.94 , 0.933, 0.947, 0.933, 0.938, 0.943, 0.937,\n",
       "        0.93 , 0.935, 0.93 , 0.944, 0.931, 0.936, 0.943, 0.937, 0.937,\n",
       "        0.937, 0.927, 0.937, 0.934, 0.937, 0.936, 0.93 , 0.937, 0.933,\n",
       "        0.932, 0.943, 0.939, 0.934, 0.937, 0.941, 0.924, 0.937, 0.936,\n",
       "        0.943, 0.932, 0.936, 0.935, 0.934, 0.932, 0.932, 0.941, 0.941,\n",
       "        0.942, 0.941, 0.941, 0.942, 0.942, 0.936, 0.936, 0.944, 0.941,\n",
       "        0.94 , 0.929, 0.936, 0.937, 0.934, 0.937, 0.94 , 0.935, 0.931,\n",
       "        0.93 , 0.936, 0.938, 0.938, 0.937, 0.937, 0.929, 0.936, 0.943,\n",
       "        0.933, 0.941, 0.93 , 0.941, 0.936, 0.939, 0.937, 0.934, 0.94 ,\n",
       "        0.938, 0.937, 0.919, 0.933, 0.941, 0.935, 0.936, 0.943, 0.941,\n",
       "        0.941, 0.95 , 0.934, 0.938, 0.935, 0.946, 0.772, 0.124, 0.758,\n",
       "        0.809, 0.442, 0.827, 0.871, 0.197, 0.848, 0.888, 0.037, 0.896,\n",
       "        0.726, 0.141, 0.792, 0.84 , 0.25 , 0.826, 0.877, 0.26 , 0.831,\n",
       "        0.859, 0.19 , 0.853, 0.763, 0.26 , 0.745, 0.824, 0.135, 0.831,\n",
       "        0.875, 0.242, 0.845, 0.87 , 0.141, 0.879, 0.805, 0.089, 0.754,\n",
       "        0.824, 0.103, 0.828, 0.849, 0.099, 0.872, 0.9  , 0.26 , 0.887,\n",
       "        0.911, 0.196, 0.87 , 0.92 , 0.261, 0.915, 0.926, 0.277, 0.923,\n",
       "        0.929, 0.434, 0.924, 0.916, 0.23 , 0.865, 0.926, 0.283, 0.92 ,\n",
       "        0.925, 0.32 , 0.924, 0.926, 0.271, 0.925, 0.902, 0.312, 0.878,\n",
       "        0.916, 0.229, 0.921, 0.926, 0.298, 0.928, 0.924, 0.557, 0.926,\n",
       "        0.914, 0.16 , 0.92 , 0.912, 0.175, 0.924, 0.927, 0.29 , 0.923,\n",
       "        0.926, 0.288, 0.926, 0.879, 0.917, 0.878, 0.929, 0.923, 0.928,\n",
       "        0.928, 0.929, 0.927, 0.925, 0.926, 0.928, 0.876, 0.875, 0.868,\n",
       "        0.927, 0.926, 0.927, 0.929, 0.926, 0.927, 0.927, 0.927, 0.924,\n",
       "        0.916, 0.878, 0.886, 0.925, 0.93 , 0.925, 0.928, 0.926, 0.923,\n",
       "        0.928, 0.928, 0.926, 0.88 , 0.917, 0.874, 0.923, 0.923, 0.925,\n",
       "        0.927, 0.928, 0.928, 0.928, 0.925, 0.929, 0.923, 0.923, 0.92 ,\n",
       "        0.934, 0.928, 0.928, 0.936, 0.932, 0.932, 0.935, 0.928, 0.933,\n",
       "        0.921, 0.921, 0.921, 0.927, 0.933, 0.93 , 0.931, 0.931, 0.93 ,\n",
       "        0.931, 0.93 , 0.935, 0.923, 0.92 , 0.922, 0.929, 0.933, 0.924,\n",
       "        0.929, 0.929, 0.929, 0.933, 0.934, 0.932, 0.923, 0.921, 0.92 ,\n",
       "        0.931, 0.929, 0.931, 0.93 , 0.929, 0.93 , 0.928, 0.933, 0.931]),\n",
       " 'split4_test_recall_micro': array([0.944, 0.946, 0.944, 0.934, 0.944, 0.939, 0.933, 0.937, 0.942,\n",
       "        0.931, 0.946, 0.938, 0.943, 0.942, 0.935, 0.947, 0.939, 0.944,\n",
       "        0.938, 0.946, 0.94 , 0.932, 0.937, 0.935, 0.94 , 0.941, 0.938,\n",
       "        0.94 , 0.944, 0.94 , 0.939, 0.941, 0.931, 0.938, 0.939, 0.937,\n",
       "        0.939, 0.935, 0.933, 0.938, 0.941, 0.94 , 0.939, 0.935, 0.938,\n",
       "        0.934, 0.933, 0.936, 0.945, 0.936, 0.943, 0.939, 0.939, 0.94 ,\n",
       "        0.941, 0.947, 0.938, 0.934, 0.927, 0.936, 0.941, 0.932, 0.933,\n",
       "        0.945, 0.946, 0.944, 0.934, 0.946, 0.939, 0.926, 0.937, 0.934,\n",
       "        0.94 , 0.946, 0.93 , 0.942, 0.946, 0.942, 0.945, 0.941, 0.938,\n",
       "        0.937, 0.931, 0.94 , 0.939, 0.938, 0.936, 0.94 , 0.935, 0.945,\n",
       "        0.939, 0.938, 0.943, 0.934, 0.942, 0.935, 0.74 , 0.099, 0.773,\n",
       "        0.815, 0.26 , 0.828, 0.845, 0.15 , 0.851, 0.891, 0.288, 0.862,\n",
       "        0.72 , 0.145, 0.75 , 0.828, 0.084, 0.829, 0.866, 0.259, 0.851,\n",
       "        0.892, 0.157, 0.876, 0.761, 0.141, 0.723, 0.826, 0.124, 0.844,\n",
       "        0.825, 0.099, 0.882, 0.867, 0.008, 0.887, 0.782, 0.262, 0.774,\n",
       "        0.813, 0.164, 0.837, 0.834, 0.165, 0.854, 0.901, 0.259, 0.895,\n",
       "        0.912, 0.125, 0.876, 0.923, 0.155, 0.918, 0.928, 0.421, 0.923,\n",
       "        0.92 , 0.472, 0.919, 0.91 , 0.043, 0.872, 0.924, 0.269, 0.919,\n",
       "        0.924, 0.066, 0.923, 0.925, 0.347, 0.922, 0.871, 0.092, 0.909,\n",
       "        0.918, 0.073, 0.925, 0.918, 0.382, 0.921, 0.921, 0.285, 0.922,\n",
       "        0.835, 0.268, 0.872, 0.917, 0.256, 0.922, 0.924, 0.461, 0.926,\n",
       "        0.92 , 0.285, 0.921, 0.88 , 0.92 , 0.875, 0.921, 0.922, 0.922,\n",
       "        0.919, 0.922, 0.922, 0.92 , 0.924, 0.922, 0.872, 0.874, 0.876,\n",
       "        0.921, 0.922, 0.926, 0.92 , 0.921, 0.916, 0.922, 0.919, 0.922,\n",
       "        0.874, 0.879, 0.881, 0.928, 0.919, 0.921, 0.917, 0.92 , 0.921,\n",
       "        0.921, 0.921, 0.926, 0.877, 0.873, 0.857, 0.921, 0.924, 0.924,\n",
       "        0.921, 0.923, 0.92 , 0.922, 0.924, 0.922, 0.916, 0.919, 0.92 ,\n",
       "        0.923, 0.923, 0.931, 0.931, 0.927, 0.926, 0.936, 0.933, 0.93 ,\n",
       "        0.918, 0.918, 0.923, 0.931, 0.925, 0.925, 0.929, 0.931, 0.931,\n",
       "        0.927, 0.929, 0.932, 0.913, 0.916, 0.914, 0.927, 0.93 , 0.925,\n",
       "        0.93 , 0.927, 0.928, 0.928, 0.931, 0.925, 0.918, 0.916, 0.914,\n",
       "        0.924, 0.922, 0.919, 0.927, 0.928, 0.922, 0.925, 0.931, 0.929]),\n",
       " 'mean_test_recall_micro': array([0.9332, 0.9308, 0.9342, 0.93  , 0.9364, 0.93  , 0.928 , 0.9336,\n",
       "        0.9306, 0.9296, 0.9308, 0.9296, 0.9328, 0.9328, 0.9308, 0.9358,\n",
       "        0.9326, 0.9312, 0.9316, 0.9314, 0.9334, 0.926 , 0.9306, 0.9272,\n",
       "        0.9304, 0.9334, 0.932 , 0.933 , 0.9362, 0.9316, 0.9326, 0.934 ,\n",
       "        0.933 , 0.9292, 0.9294, 0.933 , 0.933 , 0.9306, 0.9312, 0.9332,\n",
       "        0.9326, 0.9314, 0.9316, 0.9306, 0.9342, 0.9322, 0.931 , 0.9322,\n",
       "        0.9326, 0.931 , 0.9328, 0.9312, 0.9336, 0.9322, 0.9366, 0.9314,\n",
       "        0.9318, 0.9296, 0.9282, 0.9306, 0.9352, 0.929 , 0.929 , 0.9314,\n",
       "        0.9356, 0.9318, 0.9298, 0.9356, 0.9298, 0.9282, 0.9278, 0.9338,\n",
       "        0.9304, 0.9334, 0.928 , 0.9342, 0.935 , 0.9336, 0.9318, 0.9282,\n",
       "        0.935 , 0.9338, 0.9292, 0.9304, 0.9324, 0.9312, 0.9314, 0.9332,\n",
       "        0.9326, 0.9344, 0.9342, 0.9344, 0.9312, 0.9302, 0.9328, 0.9334,\n",
       "        0.7536, 0.1028, 0.7428, 0.8156, 0.251 , 0.8182, 0.8486, 0.126 ,\n",
       "        0.846 , 0.8834, 0.1776, 0.8808, 0.7112, 0.135 , 0.748 , 0.8154,\n",
       "        0.193 , 0.8236, 0.8516, 0.169 , 0.8444, 0.8724, 0.1964, 0.8726,\n",
       "        0.7548, 0.1978, 0.743 , 0.8128, 0.1598, 0.8266, 0.8464, 0.1494,\n",
       "        0.8556, 0.866 , 0.1478, 0.8742, 0.7642, 0.1416, 0.747 , 0.8202,\n",
       "        0.1474, 0.8176, 0.8362, 0.1808, 0.8418, 0.8706, 0.2452, 0.8694,\n",
       "        0.8968, 0.1948, 0.8866, 0.918 , 0.2886, 0.9168, 0.9198, 0.425 ,\n",
       "        0.9168, 0.919 , 0.4118, 0.9214, 0.9022, 0.2132, 0.8958, 0.9206,\n",
       "        0.1918, 0.9166, 0.9204, 0.2048, 0.9174, 0.9226, 0.3726, 0.9202,\n",
       "        0.891 , 0.2532, 0.8816, 0.9162, 0.2028, 0.918 , 0.919 , 0.2572,\n",
       "        0.92  , 0.9212, 0.3872, 0.9212, 0.8882, 0.1754, 0.895 , 0.9178,\n",
       "        0.2268, 0.9192, 0.9198, 0.3786, 0.9198, 0.9212, 0.3   , 0.9194,\n",
       "        0.88  , 0.8936, 0.8834, 0.9218, 0.9206, 0.9244, 0.9228, 0.9254,\n",
       "        0.924 , 0.9244, 0.926 , 0.9262, 0.8824, 0.8726, 0.888 , 0.9232,\n",
       "        0.9236, 0.9234, 0.9236, 0.9232, 0.924 , 0.9248, 0.9244, 0.9248,\n",
       "        0.899 , 0.877 , 0.8908, 0.9234, 0.9196, 0.9218, 0.923 , 0.9218,\n",
       "        0.9222, 0.9248, 0.9254, 0.9248, 0.8848, 0.8914, 0.8774, 0.921 ,\n",
       "        0.9224, 0.923 , 0.9224, 0.923 , 0.9222, 0.9242, 0.9234, 0.9248,\n",
       "        0.9092, 0.9196, 0.9196, 0.926 , 0.9264, 0.9268, 0.9294, 0.9282,\n",
       "        0.927 , 0.93  , 0.9276, 0.9274, 0.9182, 0.9194, 0.9202, 0.9262,\n",
       "        0.9268, 0.9266, 0.9276, 0.927 , 0.929 , 0.9282, 0.9292, 0.9292,\n",
       "        0.9174, 0.9168, 0.917 , 0.9256, 0.926 , 0.9246, 0.9286, 0.9262,\n",
       "        0.9268, 0.9262, 0.9308, 0.928 , 0.919 , 0.9214, 0.9164, 0.9252,\n",
       "        0.9252, 0.9252, 0.9262, 0.9264, 0.927 , 0.9276, 0.928 , 0.9274]),\n",
       " 'std_test_recall_micro': array([0.0059127 , 0.00808455, 0.00844748, 0.00384708, 0.00826075,\n",
       "        0.0055857 , 0.0063561 , 0.0093723 , 0.00749933, 0.00287054,\n",
       "        0.00879545, 0.00598665, 0.01012719, 0.00577581, 0.00556417,\n",
       "        0.00813388, 0.00793977, 0.00823165, 0.00665132, 0.00741889,\n",
       "        0.00484149, 0.00644981, 0.00531413, 0.00738647, 0.00646838,\n",
       "        0.00496387, 0.00414729, 0.004     , 0.00617738, 0.00749933,\n",
       "        0.00458694, 0.00626099, 0.00493964, 0.00483322, 0.00870862,\n",
       "        0.00363318, 0.00729383, 0.00287054, 0.0040694 , 0.00435431,\n",
       "        0.00484149, 0.00523832, 0.00402989, 0.00662118, 0.00448999,\n",
       "        0.00567098, 0.00565685, 0.00541849, 0.01013114, 0.00839047,\n",
       "        0.00724983, 0.00538145, 0.0069455 , 0.00693974, 0.00361109,\n",
       "        0.00993177, 0.00693974, 0.0051614 , 0.00466476, 0.00560714,\n",
       "        0.00526878, 0.004     , 0.00509902, 0.00733757, 0.00608605,\n",
       "        0.00810925, 0.00552811, 0.00578273, 0.00702567, 0.00679412,\n",
       "        0.00735935, 0.00534416, 0.00646838, 0.00863944, 0.00394968,\n",
       "        0.00617738, 0.00613188, 0.0062482 , 0.00832827, 0.00793473,\n",
       "        0.00489898, 0.00444522, 0.00466476, 0.00845222, 0.00454313,\n",
       "        0.00757364, 0.00382623, 0.00453431, 0.00608605, 0.00768375,\n",
       "        0.00479166, 0.00886792, 0.00757364, 0.00584466, 0.00722219,\n",
       "        0.0088227 , 0.01448586, 0.03550437, 0.02889567, 0.00906863,\n",
       "        0.10820721, 0.00865794, 0.01189285, 0.06113264, 0.01306905,\n",
       "        0.00924338, 0.0923528 , 0.01551   , 0.03688577, 0.05139261,\n",
       "        0.02956349, 0.01659639, 0.08569014, 0.01246756, 0.01714176,\n",
       "        0.08321298, 0.01643898, 0.01688313, 0.061249  , 0.01316966,\n",
       "        0.01661806, 0.04972082, 0.01206648, 0.01444161, 0.05728316,\n",
       "        0.01222457, 0.01668053, 0.06915085, 0.01883189, 0.01359412,\n",
       "        0.08273911, 0.01631441, 0.02545113, 0.07562433, 0.01652876,\n",
       "        0.01416192, 0.04722965, 0.0164997 , 0.00661513, 0.10529273,\n",
       "        0.02145134, 0.02662029, 0.02713964, 0.01868261, 0.01669012,\n",
       "        0.1144437 , 0.01977473, 0.00334664, 0.07412584, 0.00391918,\n",
       "        0.00630555, 0.08184131, 0.00511468, 0.00536656, 0.08728436,\n",
       "        0.00535164, 0.01170299, 0.11289535, 0.02251577, 0.00598665,\n",
       "        0.10145028, 0.00402989, 0.00611882, 0.09918347, 0.00560714,\n",
       "        0.00440908, 0.09467756, 0.0031241 , 0.01768615, 0.13753749,\n",
       "        0.01443052, 0.00331059, 0.11151753, 0.00576194, 0.00562139,\n",
       "        0.11058644, 0.00672309, 0.0053066 , 0.0978047 , 0.00526878,\n",
       "        0.02921917, 0.05715103, 0.02141962, 0.00470744, 0.07821867,\n",
       "        0.00365513, 0.00691086, 0.05837328, 0.00426146, 0.00515364,\n",
       "        0.07063993, 0.00422374, 0.00260768, 0.02122828, 0.01370547,\n",
       "        0.00462169, 0.00241661, 0.00538888, 0.00376298, 0.00387814,\n",
       "        0.00460435, 0.00412795, 0.00596657, 0.00381576, 0.01493452,\n",
       "        0.00440908, 0.02237856, 0.00370945, 0.00205913, 0.00574804,\n",
       "        0.00508331, 0.00426146, 0.00481664, 0.00299333, 0.00338231,\n",
       "        0.00426146, 0.01884675, 0.00878635, 0.01117855, 0.00538888,\n",
       "        0.00640625, 0.00318748, 0.00622896, 0.00391918, 0.0040694 ,\n",
       "        0.0044    , 0.00458694, 0.00305941, 0.01416192, 0.01940722,\n",
       "        0.01940722, 0.00228035, 0.00338231, 0.00167332, 0.00372022,\n",
       "        0.00525357, 0.00457821, 0.00515364, 0.0028    , 0.00299333,\n",
       "        0.0178146 , 0.002498  , 0.00241661, 0.00460435, 0.00287054,\n",
       "        0.00365513, 0.00440908, 0.00292575, 0.00404969, 0.00547723,\n",
       "        0.005004  , 0.00564269, 0.00146969, 0.0028    , 0.00416653,\n",
       "        0.00324962, 0.00430813, 0.00646838, 0.00307246, 0.00414729,\n",
       "        0.00352136, 0.00444522, 0.00222711, 0.00577581, 0.00361109,\n",
       "        0.00193907, 0.00275681, 0.00463033, 0.00481664, 0.00349857,\n",
       "        0.00344093, 0.00299333, 0.00411825, 0.00570614, 0.00453431,\n",
       "        0.00389872, 0.00275681, 0.00407922, 0.00233238, 0.00534416,\n",
       "        0.00416653, 0.00491528, 0.00318748, 0.00422374, 0.00489898,\n",
       "        0.00615142, 0.00583095, 0.00496387]),\n",
       " 'rank_test_recall_micro': array([ 26,  65,  12,  78,   2,  78, 104,  19,  69,  85,  65,  83,  34,\n",
       "         33,  65,   4,  37,  58,  50,  53,  22, 126,  69, 111,  75,  22,\n",
       "         46,  29,   3,  52,  37,  16,  29,  88,  86,  29,  29,  69,  58,\n",
       "         26,  37,  53,  50,  69,  12,  43,  63,  44,  37,  63,  34,  58,\n",
       "         19,  44,   1,  53,  48,  83,  96,  69,   7,  92,  92,  53,   5,\n",
       "         48,  81,   5,  81,  96, 105,  17,  74,  22, 101,  12,   8,  19,\n",
       "         47,  96,   8,  17,  88,  75,  42,  58,  53,  26,  37,  10,  12,\n",
       "         11,  58,  77,  34,  22, 251, 288, 255, 246, 266, 244, 235, 287,\n",
       "        237, 218, 278, 222, 256, 286, 252, 247, 275, 242, 234, 280, 238,\n",
       "        229, 273, 227, 250, 272, 254, 248, 281, 241, 236, 282, 233, 232,\n",
       "        283, 226, 249, 285, 253, 243, 284, 245, 240, 277, 239, 230, 267,\n",
       "        231, 207, 274, 216, 192, 263, 198, 179, 257, 200, 189, 258, 167,\n",
       "        205, 269, 208, 173, 276, 201, 175, 270, 195, 159, 261, 176, 212,\n",
       "        265, 221, 203, 271, 192, 188, 264, 178, 170, 259, 170, 214, 279,\n",
       "        209, 194, 268, 187, 179, 260, 179, 169, 262, 185, 223, 210, 218,\n",
       "        164, 174, 142, 158, 131, 146, 142, 126, 121, 220, 228, 215, 154,\n",
       "        148, 151, 148, 153, 146, 136, 142, 139, 206, 225, 213, 151, 182,\n",
       "        164, 155, 164, 162, 136, 131, 136, 217, 211, 224, 172, 160, 155,\n",
       "        160, 155, 163, 145, 150, 139, 204, 182, 184, 126, 120, 115,  86,\n",
       "         96, 113,  78, 106, 110, 191, 185, 176, 121, 115, 118, 106, 113,\n",
       "         94,  96,  88,  88, 195, 198, 197, 130, 129, 141,  95, 121, 115,\n",
       "        121,  65, 101, 189, 167, 202, 133, 135, 133, 121, 119, 112, 106,\n",
       "        101, 109]),\n",
       " 'split0_test_f1_micro': array([0.932, 0.929, 0.937, 0.932, 0.933, 0.929, 0.923, 0.933, 0.923,\n",
       "        0.931, 0.927, 0.922, 0.933, 0.936, 0.935, 0.93 , 0.941, 0.929,\n",
       "        0.936, 0.926, 0.933, 0.924, 0.925, 0.927, 0.935, 0.931, 0.934,\n",
       "        0.934, 0.934, 0.932, 0.931, 0.939, 0.935, 0.928, 0.931, 0.931,\n",
       "        0.933, 0.931, 0.934, 0.937, 0.932, 0.933, 0.927, 0.925, 0.93 ,\n",
       "        0.925, 0.929, 0.927, 0.934, 0.932, 0.936, 0.929, 0.932, 0.928,\n",
       "        0.933, 0.923, 0.936, 0.923, 0.93 , 0.927, 0.937, 0.928, 0.934,\n",
       "        0.931, 0.937, 0.93 , 0.924, 0.929, 0.922, 0.919, 0.925, 0.929,\n",
       "        0.932, 0.929, 0.933, 0.929, 0.931, 0.933, 0.922, 0.923, 0.934,\n",
       "        0.93 , 0.928, 0.933, 0.934, 0.93 , 0.932, 0.93 , 0.93 , 0.933,\n",
       "        0.93 , 0.925, 0.924, 0.921, 0.937, 0.939, 0.735, 0.037, 0.764,\n",
       "        0.803, 0.152, 0.805, 0.837, 0.172, 0.827, 0.882, 0.191, 0.864,\n",
       "        0.639, 0.196, 0.753, 0.809, 0.273, 0.802, 0.835, 0.139, 0.849,\n",
       "        0.847, 0.107, 0.87 , 0.723, 0.25 , 0.755, 0.805, 0.261, 0.826,\n",
       "        0.836, 0.211, 0.874, 0.887, 0.192, 0.864, 0.745, 0.169, 0.745,\n",
       "        0.801, 0.189, 0.817, 0.83 , 0.335, 0.838, 0.84 , 0.191, 0.858,\n",
       "        0.907, 0.171, 0.923, 0.917, 0.352, 0.923, 0.917, 0.504, 0.914,\n",
       "        0.914, 0.245, 0.926, 0.882, 0.23 , 0.917, 0.925, 0.011, 0.919,\n",
       "        0.919, 0.108, 0.914, 0.925, 0.523, 0.92 , 0.906, 0.482, 0.881,\n",
       "        0.915, 0.375, 0.92 , 0.925, 0.213, 0.924, 0.919, 0.427, 0.924,\n",
       "        0.912, 0.146, 0.911, 0.914, 0.326, 0.915, 0.919, 0.404, 0.919,\n",
       "        0.926, 0.406, 0.919, 0.884, 0.866, 0.91 , 0.915, 0.921, 0.933,\n",
       "        0.924, 0.93 , 0.929, 0.931, 0.937, 0.932, 0.874, 0.878, 0.912,\n",
       "        0.926, 0.926, 0.925, 0.929, 0.928, 0.929, 0.929, 0.928, 0.931,\n",
       "        0.878, 0.891, 0.879, 0.927, 0.921, 0.924, 0.929, 0.924, 0.926,\n",
       "        0.931, 0.931, 0.929, 0.876, 0.879, 0.912, 0.919, 0.923, 0.921,\n",
       "        0.924, 0.926, 0.927, 0.924, 0.926, 0.927, 0.915, 0.917, 0.922,\n",
       "        0.928, 0.931, 0.928, 0.931, 0.931, 0.931, 0.932, 0.932, 0.932,\n",
       "        0.918, 0.918, 0.912, 0.925, 0.929, 0.937, 0.929, 0.928, 0.934,\n",
       "        0.932, 0.933, 0.934, 0.915, 0.915, 0.915, 0.93 , 0.925, 0.931,\n",
       "        0.932, 0.929, 0.931, 0.931, 0.937, 0.933, 0.921, 0.926, 0.914,\n",
       "        0.93 , 0.924, 0.931, 0.929, 0.932, 0.934, 0.938, 0.934, 0.933]),\n",
       " 'split1_test_f1_micro': array([0.926, 0.924, 0.92 , 0.927, 0.924, 0.923, 0.922, 0.916, 0.925,\n",
       "        0.924, 0.922, 0.924, 0.917, 0.925, 0.924, 0.925, 0.923, 0.922,\n",
       "        0.924, 0.928, 0.931, 0.924, 0.926, 0.919, 0.924, 0.927, 0.929,\n",
       "        0.928, 0.931, 0.92 , 0.934, 0.927, 0.932, 0.93 , 0.925, 0.927,\n",
       "        0.926, 0.927, 0.925, 0.928, 0.927, 0.925, 0.93 , 0.929, 0.93 ,\n",
       "        0.93 , 0.925, 0.928, 0.921, 0.917, 0.924, 0.925, 0.926, 0.924,\n",
       "        0.932, 0.92 , 0.919, 0.926, 0.92 , 0.922, 0.928, 0.924, 0.927,\n",
       "        0.923, 0.929, 0.923, 0.924, 0.933, 0.923, 0.927, 0.919, 0.928,\n",
       "        0.921, 0.923, 0.925, 0.927, 0.928, 0.925, 0.925, 0.922, 0.926,\n",
       "        0.927, 0.927, 0.938, 0.931, 0.92 , 0.927, 0.933, 0.925, 0.924,\n",
       "        0.931, 0.93 , 0.922, 0.931, 0.921, 0.923, 0.767, 0.14 , 0.723,\n",
       "        0.823, 0.26 , 0.818, 0.849, 0.033, 0.866, 0.89 , 0.258, 0.899,\n",
       "        0.728, 0.153, 0.699, 0.807, 0.093, 0.84 , 0.834, 0.148, 0.869,\n",
       "        0.886, 0.268, 0.894, 0.771, 0.148, 0.737, 0.788, 0.098, 0.826,\n",
       "        0.849, 0.14 , 0.834, 0.861, 0.26 , 0.893, 0.737, 0.039, 0.725,\n",
       "        0.819, 0.081, 0.818, 0.835, 0.26 , 0.838, 0.87 , 0.26 , 0.862,\n",
       "        0.871, 0.408, 0.872, 0.917, 0.343, 0.917, 0.917, 0.425, 0.912,\n",
       "        0.916, 0.49 , 0.926, 0.905, 0.17 , 0.915, 0.918, 0.151, 0.916,\n",
       "        0.925, 0.269, 0.917, 0.923, 0.287, 0.918, 0.868, 0.239, 0.872,\n",
       "        0.921, 0.089, 0.916, 0.913, 0.065, 0.919, 0.929, 0.309, 0.923,\n",
       "        0.9  , 0.099, 0.867, 0.921, 0.104, 0.92 , 0.922, 0.397, 0.914,\n",
       "        0.922, 0.189, 0.918, 0.881, 0.883, 0.872, 0.924, 0.921, 0.921,\n",
       "        0.925, 0.926, 0.926, 0.926, 0.924, 0.927, 0.878, 0.865, 0.918,\n",
       "        0.925, 0.923, 0.927, 0.924, 0.925, 0.927, 0.925, 0.926, 0.928,\n",
       "        0.914, 0.873, 0.907, 0.924, 0.918, 0.923, 0.927, 0.924, 0.926,\n",
       "        0.925, 0.928, 0.923, 0.878, 0.875, 0.882, 0.924, 0.926, 0.924,\n",
       "        0.924, 0.925, 0.92 , 0.931, 0.924, 0.925, 0.918, 0.922, 0.921,\n",
       "        0.921, 0.926, 0.927, 0.925, 0.927, 0.925, 0.924, 0.926, 0.924,\n",
       "        0.917, 0.924, 0.923, 0.927, 0.927, 0.923, 0.927, 0.925, 0.925,\n",
       "        0.931, 0.927, 0.925, 0.92 , 0.918, 0.917, 0.925, 0.921, 0.922,\n",
       "        0.93 , 0.925, 0.927, 0.92 , 0.928, 0.927, 0.918, 0.926, 0.918,\n",
       "        0.925, 0.931, 0.923, 0.923, 0.922, 0.928, 0.928, 0.922, 0.925]),\n",
       " 'split2_test_f1_micro': array([0.931, 0.924, 0.93 , 0.924, 0.934, 0.926, 0.924, 0.939, 0.926,\n",
       "        0.932, 0.924, 0.934, 0.927, 0.93 , 0.924, 0.934, 0.923, 0.924,\n",
       "        0.923, 0.93 , 0.926, 0.916, 0.928, 0.919, 0.923, 0.931, 0.926,\n",
       "        0.931, 0.929, 0.927, 0.925, 0.926, 0.926, 0.926, 0.915, 0.934,\n",
       "        0.924, 0.928, 0.928, 0.928, 0.929, 0.927, 0.93 , 0.923, 0.932,\n",
       "        0.93 , 0.927, 0.929, 0.921, 0.928, 0.925, 0.927, 0.927, 0.928,\n",
       "        0.937, 0.938, 0.93 , 0.928, 0.93 , 0.931, 0.93 , 0.926, 0.92 ,\n",
       "        0.928, 0.93 , 0.924, 0.929, 0.933, 0.928, 0.94 , 0.922, 0.935,\n",
       "        0.926, 0.928, 0.922, 0.932, 0.934, 0.929, 0.93 , 0.921, 0.937,\n",
       "        0.937, 0.923, 0.922, 0.925, 0.927, 0.927, 0.927, 0.93 , 0.929,\n",
       "        0.93 , 0.929, 0.933, 0.927, 0.929, 0.924, 0.754, 0.114, 0.696,\n",
       "        0.828, 0.141, 0.813, 0.841, 0.078, 0.838, 0.866, 0.114, 0.883,\n",
       "        0.743, 0.04 , 0.746, 0.793, 0.265, 0.821, 0.846, 0.039, 0.822,\n",
       "        0.878, 0.26 , 0.87 , 0.756, 0.19 , 0.755, 0.821, 0.181, 0.806,\n",
       "        0.847, 0.055, 0.843, 0.845, 0.138, 0.848, 0.752, 0.149, 0.737,\n",
       "        0.844, 0.2  , 0.788, 0.833, 0.045, 0.807, 0.842, 0.256, 0.845,\n",
       "        0.883, 0.074, 0.892, 0.913, 0.332, 0.911, 0.911, 0.498, 0.912,\n",
       "        0.916, 0.418, 0.912, 0.898, 0.393, 0.91 , 0.91 , 0.245, 0.909,\n",
       "        0.909, 0.261, 0.909, 0.914, 0.435, 0.916, 0.908, 0.141, 0.868,\n",
       "        0.911, 0.248, 0.908, 0.913, 0.328, 0.908, 0.913, 0.358, 0.911,\n",
       "        0.88 , 0.204, 0.905, 0.925, 0.273, 0.915, 0.907, 0.341, 0.917,\n",
       "        0.912, 0.332, 0.913, 0.876, 0.882, 0.882, 0.92 , 0.916, 0.918,\n",
       "        0.918, 0.92 , 0.916, 0.92 , 0.919, 0.922, 0.912, 0.871, 0.866,\n",
       "        0.917, 0.921, 0.912, 0.916, 0.916, 0.921, 0.921, 0.922, 0.919,\n",
       "        0.913, 0.864, 0.901, 0.913, 0.91 , 0.916, 0.914, 0.915, 0.915,\n",
       "        0.919, 0.919, 0.92 , 0.913, 0.913, 0.862, 0.918, 0.916, 0.921,\n",
       "        0.916, 0.913, 0.916, 0.916, 0.918, 0.921, 0.874, 0.917, 0.915,\n",
       "        0.924, 0.924, 0.92 , 0.924, 0.924, 0.921, 0.923, 0.919, 0.918,\n",
       "        0.917, 0.916, 0.922, 0.921, 0.92 , 0.918, 0.922, 0.92 , 0.925,\n",
       "        0.92 , 0.927, 0.92 , 0.916, 0.915, 0.917, 0.917, 0.921, 0.921,\n",
       "        0.922, 0.921, 0.919, 0.919, 0.924, 0.923, 0.915, 0.918, 0.916,\n",
       "        0.916, 0.92 , 0.922, 0.922, 0.921, 0.921, 0.919, 0.92 , 0.919]),\n",
       " 'split3_test_f1_micro': array([0.933, 0.931, 0.94 , 0.933, 0.947, 0.933, 0.938, 0.943, 0.937,\n",
       "        0.93 , 0.935, 0.93 , 0.944, 0.931, 0.936, 0.943, 0.937, 0.937,\n",
       "        0.937, 0.927, 0.937, 0.934, 0.937, 0.936, 0.93 , 0.937, 0.933,\n",
       "        0.932, 0.943, 0.939, 0.934, 0.937, 0.941, 0.924, 0.937, 0.936,\n",
       "        0.943, 0.932, 0.936, 0.935, 0.934, 0.932, 0.932, 0.941, 0.941,\n",
       "        0.942, 0.941, 0.941, 0.942, 0.942, 0.936, 0.936, 0.944, 0.941,\n",
       "        0.94 , 0.929, 0.936, 0.937, 0.934, 0.937, 0.94 , 0.935, 0.931,\n",
       "        0.93 , 0.936, 0.938, 0.938, 0.937, 0.937, 0.929, 0.936, 0.943,\n",
       "        0.933, 0.941, 0.93 , 0.941, 0.936, 0.939, 0.937, 0.934, 0.94 ,\n",
       "        0.938, 0.937, 0.919, 0.933, 0.941, 0.935, 0.936, 0.943, 0.941,\n",
       "        0.941, 0.95 , 0.934, 0.938, 0.935, 0.946, 0.772, 0.124, 0.758,\n",
       "        0.809, 0.442, 0.827, 0.871, 0.197, 0.848, 0.888, 0.037, 0.896,\n",
       "        0.726, 0.141, 0.792, 0.84 , 0.25 , 0.826, 0.877, 0.26 , 0.831,\n",
       "        0.859, 0.19 , 0.853, 0.763, 0.26 , 0.745, 0.824, 0.135, 0.831,\n",
       "        0.875, 0.242, 0.845, 0.87 , 0.141, 0.879, 0.805, 0.089, 0.754,\n",
       "        0.824, 0.103, 0.828, 0.849, 0.099, 0.872, 0.9  , 0.26 , 0.887,\n",
       "        0.911, 0.196, 0.87 , 0.92 , 0.261, 0.915, 0.926, 0.277, 0.923,\n",
       "        0.929, 0.434, 0.924, 0.916, 0.23 , 0.865, 0.926, 0.283, 0.92 ,\n",
       "        0.925, 0.32 , 0.924, 0.926, 0.271, 0.925, 0.902, 0.312, 0.878,\n",
       "        0.916, 0.229, 0.921, 0.926, 0.298, 0.928, 0.924, 0.557, 0.926,\n",
       "        0.914, 0.16 , 0.92 , 0.912, 0.175, 0.924, 0.927, 0.29 , 0.923,\n",
       "        0.926, 0.288, 0.926, 0.879, 0.917, 0.878, 0.929, 0.923, 0.928,\n",
       "        0.928, 0.929, 0.927, 0.925, 0.926, 0.928, 0.876, 0.875, 0.868,\n",
       "        0.927, 0.926, 0.927, 0.929, 0.926, 0.927, 0.927, 0.927, 0.924,\n",
       "        0.916, 0.878, 0.886, 0.925, 0.93 , 0.925, 0.928, 0.926, 0.923,\n",
       "        0.928, 0.928, 0.926, 0.88 , 0.917, 0.874, 0.923, 0.923, 0.925,\n",
       "        0.927, 0.928, 0.928, 0.928, 0.925, 0.929, 0.923, 0.923, 0.92 ,\n",
       "        0.934, 0.928, 0.928, 0.936, 0.932, 0.932, 0.935, 0.928, 0.933,\n",
       "        0.921, 0.921, 0.921, 0.927, 0.933, 0.93 , 0.931, 0.931, 0.93 ,\n",
       "        0.931, 0.93 , 0.935, 0.923, 0.92 , 0.922, 0.929, 0.933, 0.924,\n",
       "        0.929, 0.929, 0.929, 0.933, 0.934, 0.932, 0.923, 0.921, 0.92 ,\n",
       "        0.931, 0.929, 0.931, 0.93 , 0.929, 0.93 , 0.928, 0.933, 0.931]),\n",
       " 'split4_test_f1_micro': array([0.944, 0.946, 0.944, 0.934, 0.944, 0.939, 0.933, 0.937, 0.942,\n",
       "        0.931, 0.946, 0.938, 0.943, 0.942, 0.935, 0.947, 0.939, 0.944,\n",
       "        0.938, 0.946, 0.94 , 0.932, 0.937, 0.935, 0.94 , 0.941, 0.938,\n",
       "        0.94 , 0.944, 0.94 , 0.939, 0.941, 0.931, 0.938, 0.939, 0.937,\n",
       "        0.939, 0.935, 0.933, 0.938, 0.941, 0.94 , 0.939, 0.935, 0.938,\n",
       "        0.934, 0.933, 0.936, 0.945, 0.936, 0.943, 0.939, 0.939, 0.94 ,\n",
       "        0.941, 0.947, 0.938, 0.934, 0.927, 0.936, 0.941, 0.932, 0.933,\n",
       "        0.945, 0.946, 0.944, 0.934, 0.946, 0.939, 0.926, 0.937, 0.934,\n",
       "        0.94 , 0.946, 0.93 , 0.942, 0.946, 0.942, 0.945, 0.941, 0.938,\n",
       "        0.937, 0.931, 0.94 , 0.939, 0.938, 0.936, 0.94 , 0.935, 0.945,\n",
       "        0.939, 0.938, 0.943, 0.934, 0.942, 0.935, 0.74 , 0.099, 0.773,\n",
       "        0.815, 0.26 , 0.828, 0.845, 0.15 , 0.851, 0.891, 0.288, 0.862,\n",
       "        0.72 , 0.145, 0.75 , 0.828, 0.084, 0.829, 0.866, 0.259, 0.851,\n",
       "        0.892, 0.157, 0.876, 0.761, 0.141, 0.723, 0.826, 0.124, 0.844,\n",
       "        0.825, 0.099, 0.882, 0.867, 0.008, 0.887, 0.782, 0.262, 0.774,\n",
       "        0.813, 0.164, 0.837, 0.834, 0.165, 0.854, 0.901, 0.259, 0.895,\n",
       "        0.912, 0.125, 0.876, 0.923, 0.155, 0.918, 0.928, 0.421, 0.923,\n",
       "        0.92 , 0.472, 0.919, 0.91 , 0.043, 0.872, 0.924, 0.269, 0.919,\n",
       "        0.924, 0.066, 0.923, 0.925, 0.347, 0.922, 0.871, 0.092, 0.909,\n",
       "        0.918, 0.073, 0.925, 0.918, 0.382, 0.921, 0.921, 0.285, 0.922,\n",
       "        0.835, 0.268, 0.872, 0.917, 0.256, 0.922, 0.924, 0.461, 0.926,\n",
       "        0.92 , 0.285, 0.921, 0.88 , 0.92 , 0.875, 0.921, 0.922, 0.922,\n",
       "        0.919, 0.922, 0.922, 0.92 , 0.924, 0.922, 0.872, 0.874, 0.876,\n",
       "        0.921, 0.922, 0.926, 0.92 , 0.921, 0.916, 0.922, 0.919, 0.922,\n",
       "        0.874, 0.879, 0.881, 0.928, 0.919, 0.921, 0.917, 0.92 , 0.921,\n",
       "        0.921, 0.921, 0.926, 0.877, 0.873, 0.857, 0.921, 0.924, 0.924,\n",
       "        0.921, 0.923, 0.92 , 0.922, 0.924, 0.922, 0.916, 0.919, 0.92 ,\n",
       "        0.923, 0.923, 0.931, 0.931, 0.927, 0.926, 0.936, 0.933, 0.93 ,\n",
       "        0.918, 0.918, 0.923, 0.931, 0.925, 0.925, 0.929, 0.931, 0.931,\n",
       "        0.927, 0.929, 0.932, 0.913, 0.916, 0.914, 0.927, 0.93 , 0.925,\n",
       "        0.93 , 0.927, 0.928, 0.928, 0.931, 0.925, 0.918, 0.916, 0.914,\n",
       "        0.924, 0.922, 0.919, 0.927, 0.928, 0.922, 0.925, 0.931, 0.929]),\n",
       " 'mean_test_f1_micro': array([0.9332, 0.9308, 0.9342, 0.93  , 0.9364, 0.93  , 0.928 , 0.9336,\n",
       "        0.9306, 0.9296, 0.9308, 0.9296, 0.9328, 0.9328, 0.9308, 0.9358,\n",
       "        0.9326, 0.9312, 0.9316, 0.9314, 0.9334, 0.926 , 0.9306, 0.9272,\n",
       "        0.9304, 0.9334, 0.932 , 0.933 , 0.9362, 0.9316, 0.9326, 0.934 ,\n",
       "        0.933 , 0.9292, 0.9294, 0.933 , 0.933 , 0.9306, 0.9312, 0.9332,\n",
       "        0.9326, 0.9314, 0.9316, 0.9306, 0.9342, 0.9322, 0.931 , 0.9322,\n",
       "        0.9326, 0.931 , 0.9328, 0.9312, 0.9336, 0.9322, 0.9366, 0.9314,\n",
       "        0.9318, 0.9296, 0.9282, 0.9306, 0.9352, 0.929 , 0.929 , 0.9314,\n",
       "        0.9356, 0.9318, 0.9298, 0.9356, 0.9298, 0.9282, 0.9278, 0.9338,\n",
       "        0.9304, 0.9334, 0.928 , 0.9342, 0.935 , 0.9336, 0.9318, 0.9282,\n",
       "        0.935 , 0.9338, 0.9292, 0.9304, 0.9324, 0.9312, 0.9314, 0.9332,\n",
       "        0.9326, 0.9344, 0.9342, 0.9344, 0.9312, 0.9302, 0.9328, 0.9334,\n",
       "        0.7536, 0.1028, 0.7428, 0.8156, 0.251 , 0.8182, 0.8486, 0.126 ,\n",
       "        0.846 , 0.8834, 0.1776, 0.8808, 0.7112, 0.135 , 0.748 , 0.8154,\n",
       "        0.193 , 0.8236, 0.8516, 0.169 , 0.8444, 0.8724, 0.1964, 0.8726,\n",
       "        0.7548, 0.1978, 0.743 , 0.8128, 0.1598, 0.8266, 0.8464, 0.1494,\n",
       "        0.8556, 0.866 , 0.1478, 0.8742, 0.7642, 0.1416, 0.747 , 0.8202,\n",
       "        0.1474, 0.8176, 0.8362, 0.1808, 0.8418, 0.8706, 0.2452, 0.8694,\n",
       "        0.8968, 0.1948, 0.8866, 0.918 , 0.2886, 0.9168, 0.9198, 0.425 ,\n",
       "        0.9168, 0.919 , 0.4118, 0.9214, 0.9022, 0.2132, 0.8958, 0.9206,\n",
       "        0.1918, 0.9166, 0.9204, 0.2048, 0.9174, 0.9226, 0.3726, 0.9202,\n",
       "        0.891 , 0.2532, 0.8816, 0.9162, 0.2028, 0.918 , 0.919 , 0.2572,\n",
       "        0.92  , 0.9212, 0.3872, 0.9212, 0.8882, 0.1754, 0.895 , 0.9178,\n",
       "        0.2268, 0.9192, 0.9198, 0.3786, 0.9198, 0.9212, 0.3   , 0.9194,\n",
       "        0.88  , 0.8936, 0.8834, 0.9218, 0.9206, 0.9244, 0.9228, 0.9254,\n",
       "        0.924 , 0.9244, 0.926 , 0.9262, 0.8824, 0.8726, 0.888 , 0.9232,\n",
       "        0.9236, 0.9234, 0.9236, 0.9232, 0.924 , 0.9248, 0.9244, 0.9248,\n",
       "        0.899 , 0.877 , 0.8908, 0.9234, 0.9196, 0.9218, 0.923 , 0.9218,\n",
       "        0.9222, 0.9248, 0.9254, 0.9248, 0.8848, 0.8914, 0.8774, 0.921 ,\n",
       "        0.9224, 0.923 , 0.9224, 0.923 , 0.9222, 0.9242, 0.9234, 0.9248,\n",
       "        0.9092, 0.9196, 0.9196, 0.926 , 0.9264, 0.9268, 0.9294, 0.9282,\n",
       "        0.927 , 0.93  , 0.9276, 0.9274, 0.9182, 0.9194, 0.9202, 0.9262,\n",
       "        0.9268, 0.9266, 0.9276, 0.927 , 0.929 , 0.9282, 0.9292, 0.9292,\n",
       "        0.9174, 0.9168, 0.917 , 0.9256, 0.926 , 0.9246, 0.9286, 0.9262,\n",
       "        0.9268, 0.9262, 0.9308, 0.928 , 0.919 , 0.9214, 0.9164, 0.9252,\n",
       "        0.9252, 0.9252, 0.9262, 0.9264, 0.927 , 0.9276, 0.928 , 0.9274]),\n",
       " 'std_test_f1_micro': array([0.0059127 , 0.00808455, 0.00844748, 0.00384708, 0.00826075,\n",
       "        0.0055857 , 0.0063561 , 0.0093723 , 0.00749933, 0.00287054,\n",
       "        0.00879545, 0.00598665, 0.01012719, 0.00577581, 0.00556417,\n",
       "        0.00813388, 0.00793977, 0.00823165, 0.00665132, 0.00741889,\n",
       "        0.00484149, 0.00644981, 0.00531413, 0.00738647, 0.00646838,\n",
       "        0.00496387, 0.00414729, 0.004     , 0.00617738, 0.00749933,\n",
       "        0.00458694, 0.00626099, 0.00493964, 0.00483322, 0.00870862,\n",
       "        0.00363318, 0.00729383, 0.00287054, 0.0040694 , 0.00435431,\n",
       "        0.00484149, 0.00523832, 0.00402989, 0.00662118, 0.00448999,\n",
       "        0.00567098, 0.00565685, 0.00541849, 0.01013114, 0.00839047,\n",
       "        0.00724983, 0.00538145, 0.0069455 , 0.00693974, 0.00361109,\n",
       "        0.00993177, 0.00693974, 0.0051614 , 0.00466476, 0.00560714,\n",
       "        0.00526878, 0.004     , 0.00509902, 0.00733757, 0.00608605,\n",
       "        0.00810925, 0.00552811, 0.00578273, 0.00702567, 0.00679412,\n",
       "        0.00735935, 0.00534416, 0.00646838, 0.00863944, 0.00394968,\n",
       "        0.00617738, 0.00613188, 0.0062482 , 0.00832827, 0.00793473,\n",
       "        0.00489898, 0.00444522, 0.00466476, 0.00845222, 0.00454313,\n",
       "        0.00757364, 0.00382623, 0.00453431, 0.00608605, 0.00768375,\n",
       "        0.00479166, 0.00886792, 0.00757364, 0.00584466, 0.00722219,\n",
       "        0.0088227 , 0.01448586, 0.03550437, 0.02889567, 0.00906863,\n",
       "        0.10820721, 0.00865794, 0.01189285, 0.06113264, 0.01306905,\n",
       "        0.00924338, 0.0923528 , 0.01551   , 0.03688577, 0.05139261,\n",
       "        0.02956349, 0.01659639, 0.08569014, 0.01246756, 0.01714176,\n",
       "        0.08321298, 0.01643898, 0.01688313, 0.061249  , 0.01316966,\n",
       "        0.01661806, 0.04972082, 0.01206648, 0.01444161, 0.05728316,\n",
       "        0.01222457, 0.01668053, 0.06915085, 0.01883189, 0.01359412,\n",
       "        0.08273911, 0.01631441, 0.02545113, 0.07562433, 0.01652876,\n",
       "        0.01416192, 0.04722965, 0.0164997 , 0.00661513, 0.10529273,\n",
       "        0.02145134, 0.02662029, 0.02713964, 0.01868261, 0.01669012,\n",
       "        0.1144437 , 0.01977473, 0.00334664, 0.07412584, 0.00391918,\n",
       "        0.00630555, 0.08184131, 0.00511468, 0.00536656, 0.08728436,\n",
       "        0.00535164, 0.01170299, 0.11289535, 0.02251577, 0.00598665,\n",
       "        0.10145028, 0.00402989, 0.00611882, 0.09918347, 0.00560714,\n",
       "        0.00440908, 0.09467756, 0.0031241 , 0.01768615, 0.13753749,\n",
       "        0.01443052, 0.00331059, 0.11151753, 0.00576194, 0.00562139,\n",
       "        0.11058644, 0.00672309, 0.0053066 , 0.0978047 , 0.00526878,\n",
       "        0.02921917, 0.05715103, 0.02141962, 0.00470744, 0.07821867,\n",
       "        0.00365513, 0.00691086, 0.05837328, 0.00426146, 0.00515364,\n",
       "        0.07063993, 0.00422374, 0.00260768, 0.02122828, 0.01370547,\n",
       "        0.00462169, 0.00241661, 0.00538888, 0.00376298, 0.00387814,\n",
       "        0.00460435, 0.00412795, 0.00596657, 0.00381576, 0.01493452,\n",
       "        0.00440908, 0.02237856, 0.00370945, 0.00205913, 0.00574804,\n",
       "        0.00508331, 0.00426146, 0.00481664, 0.00299333, 0.00338231,\n",
       "        0.00426146, 0.01884675, 0.00878635, 0.01117855, 0.00538888,\n",
       "        0.00640625, 0.00318748, 0.00622896, 0.00391918, 0.0040694 ,\n",
       "        0.0044    , 0.00458694, 0.00305941, 0.01416192, 0.01940722,\n",
       "        0.01940722, 0.00228035, 0.00338231, 0.00167332, 0.00372022,\n",
       "        0.00525357, 0.00457821, 0.00515364, 0.0028    , 0.00299333,\n",
       "        0.0178146 , 0.002498  , 0.00241661, 0.00460435, 0.00287054,\n",
       "        0.00365513, 0.00440908, 0.00292575, 0.00404969, 0.00547723,\n",
       "        0.005004  , 0.00564269, 0.00146969, 0.0028    , 0.00416653,\n",
       "        0.00324962, 0.00430813, 0.00646838, 0.00307246, 0.00414729,\n",
       "        0.00352136, 0.00444522, 0.00222711, 0.00577581, 0.00361109,\n",
       "        0.00193907, 0.00275681, 0.00463033, 0.00481664, 0.00349857,\n",
       "        0.00344093, 0.00299333, 0.00411825, 0.00570614, 0.00453431,\n",
       "        0.00389872, 0.00275681, 0.00407922, 0.00233238, 0.00534416,\n",
       "        0.00416653, 0.00491528, 0.00318748, 0.00422374, 0.00489898,\n",
       "        0.00615142, 0.00583095, 0.00496387]),\n",
       " 'rank_test_f1_micro': array([ 26,  65,  12,  78,   2,  78, 104,  19,  69,  85,  65,  83,  34,\n",
       "         33,  65,   4,  37,  58,  50,  53,  22, 126,  69, 111,  74,  22,\n",
       "         46,  29,   3,  52,  37,  16,  29,  88,  86,  29,  29,  69,  58,\n",
       "         26,  37,  53,  50,  69,  12,  43,  63,  44,  37,  63,  34,  58,\n",
       "         19,  44,   1,  53,  48,  83,  96,  69,   7,  92,  92,  53,   5,\n",
       "         48,  81,   5,  81,  96, 105,  17,  76,  22, 101,  12,   8,  19,\n",
       "         47,  96,   8,  17,  88,  74,  42,  58,  53,  26,  37,  10,  12,\n",
       "         11,  58,  77,  34,  22, 251, 288, 255, 246, 266, 244, 235, 287,\n",
       "        237, 218, 278, 222, 256, 286, 252, 247, 275, 242, 234, 280, 238,\n",
       "        229, 273, 227, 250, 272, 254, 248, 281, 241, 236, 282, 233, 232,\n",
       "        283, 226, 249, 285, 253, 243, 284, 245, 240, 277, 239, 230, 267,\n",
       "        231, 207, 274, 216, 192, 263, 198, 179, 257, 200, 189, 258, 167,\n",
       "        205, 269, 208, 173, 276, 201, 175, 270, 195, 159, 261, 176, 212,\n",
       "        265, 221, 203, 271, 192, 188, 264, 178, 170, 259, 170, 214, 279,\n",
       "        209, 194, 268, 187, 179, 260, 179, 169, 262, 185, 223, 210, 218,\n",
       "        164, 174, 142, 158, 131, 146, 142, 126, 121, 220, 227, 215, 154,\n",
       "        148, 151, 148, 153, 146, 138, 142, 138, 206, 225, 213, 151, 182,\n",
       "        164, 155, 164, 162, 136, 131, 136, 217, 211, 224, 172, 160, 155,\n",
       "        160, 155, 163, 145, 150, 138, 204, 182, 184, 126, 119, 115,  86,\n",
       "         96, 112,  78, 106, 110, 191, 186, 176, 121, 115, 118, 106, 112,\n",
       "         94,  96,  88,  88, 195, 198, 197, 130, 129, 141,  95, 121, 115,\n",
       "        121,  65, 101, 189, 167, 202, 133, 135, 133, 121, 119, 112, 106,\n",
       "        101, 109]),\n",
       " 'split0_test_roc_auc_ovo': array([0.99550007, 0.9959366 , 0.99589549, 0.99526434, 0.99632493,\n",
       "        0.99508621, 0.9937855 , 0.99517284, 0.99422597, 0.99349847,\n",
       "        0.99392009, 0.99395657, 0.99495688, 0.99489042, 0.99577992,\n",
       "        0.99431401, 0.99554119, 0.99481402, 0.99561219, 0.99485934,\n",
       "        0.99533049, 0.99344403, 0.99415638, 0.99335894, 0.99572869,\n",
       "        0.99467987, 0.99555094, 0.99449406, 0.99552489, 0.99481903,\n",
       "        0.99435695, 0.99535234, 0.99427037, 0.99426296, 0.99446362,\n",
       "        0.99337373, 0.9956052 , 0.99559691, 0.99518455, 0.99457908,\n",
       "        0.99523474, 0.99496757, 0.99570995, 0.99495873, 0.99469765,\n",
       "        0.99473499, 0.99361575, 0.99392343, 0.99572368, 0.99572487,\n",
       "        0.99543707, 0.99498764, 0.99442988, 0.99641316, 0.99440031,\n",
       "        0.99463473, 0.9943939 , 0.99260125, 0.99457229, 0.9944627 ,\n",
       "        0.99615959, 0.99587906, 0.99523591, 0.99318408, 0.99581827,\n",
       "        0.99414657, 0.99441757, 0.99434451, 0.99389022, 0.99228161,\n",
       "        0.99479127, 0.99175812, 0.99578486, 0.9951279 , 0.9956811 ,\n",
       "        0.99557841, 0.99588975, 0.9954351 , 0.99399674, 0.99448029,\n",
       "        0.99510146, 0.99367024, 0.99430003, 0.99378518, 0.9964778 ,\n",
       "        0.99535951, 0.99574746, 0.99415371, 0.99614274, 0.99443838,\n",
       "        0.99455034, 0.99498248, 0.99451972, 0.99416063, 0.99359721,\n",
       "        0.99587918, 0.93743275, 0.48397493, 0.9632964 , 0.97072991,\n",
       "        0.56777149, 0.97110308, 0.97873533, 0.53818337, 0.97592717,\n",
       "        0.98590819, 0.45904039, 0.9819792 , 0.92553463, 0.52880073,\n",
       "        0.94811517, 0.96043995, 0.61542838, 0.9748372 , 0.97811218,\n",
       "        0.50261109, 0.98332938, 0.9834082 , 0.5992364 , 0.98340624,\n",
       "        0.94013879, 0.37691125, 0.95466029, 0.97590398, 0.67397067,\n",
       "        0.9812687 , 0.97574757, 0.58608126, 0.98534633, 0.98718979,\n",
       "        0.67380077, 0.98194309, 0.94704397, 0.65430287, 0.93666276,\n",
       "        0.97009631, 0.43674382, 0.96667215, 0.97797051, 0.46969357,\n",
       "        0.98043137, 0.98363574, 0.40446401, 0.98044764, 0.99004542,\n",
       "        0.48750327, 0.99269635, 0.99200423, 0.71377557, 0.99300515,\n",
       "        0.99321189, 0.74966892, 0.99359268, 0.99416675, 0.6186473 ,\n",
       "        0.99423584, 0.98734304, 0.69776606, 0.99140555, 0.99388522,\n",
       "        0.41360486, 0.99118047, 0.99410419, 0.48870207, 0.99363422,\n",
       "        0.99418451, 0.81125622, 0.99368703, 0.98932194, 0.77220001,\n",
       "        0.97246411, 0.99250387, 0.49775081, 0.99275133, 0.99396999,\n",
       "        0.6856067 , 0.99373143, 0.99436898, 0.75872031, 0.99431483,\n",
       "        0.99251657, 0.65359586, 0.98820894, 0.99307955, 0.80413893,\n",
       "        0.99336567, 0.9938664 , 0.72615879, 0.99355353, 0.99401339,\n",
       "        0.79774676, 0.99375866, 0.98060583, 0.97204567, 0.9872852 ,\n",
       "        0.99211262, 0.99392383, 0.99355307, 0.99468494, 0.99504705,\n",
       "        0.99504908, 0.99520633, 0.99522144, 0.99522413, 0.97819747,\n",
       "        0.96156473, 0.9865053 , 0.99458441, 0.9937217 , 0.99292417,\n",
       "        0.99504441, 0.9943422 , 0.99470409, 0.9951598 , 0.99533368,\n",
       "        0.99519104, 0.98608203, 0.98550125, 0.95877973, 0.99284859,\n",
       "        0.99441205, 0.99440951, 0.9949345 , 0.99491168, 0.99493911,\n",
       "        0.99509339, 0.99517476, 0.99517022, 0.98132524, 0.97982092,\n",
       "        0.98849702, 0.99030627, 0.99335864, 0.99437109, 0.99461762,\n",
       "        0.99435399, 0.99482334, 0.99488514, 0.99501982, 0.99508249,\n",
       "        0.99184011, 0.99387552, 0.99125802, 0.99556975, 0.99557866,\n",
       "        0.99471684, 0.99555221, 0.99539571, 0.99532124, 0.9954627 ,\n",
       "        0.99569844, 0.99540696, 0.99465369, 0.99325298, 0.99193096,\n",
       "        0.99521485, 0.99554674, 0.99552519, 0.99526883, 0.99519637,\n",
       "        0.99589857, 0.99519491, 0.99558944, 0.9955411 , 0.99325814,\n",
       "        0.9944607 , 0.99197304, 0.99507816, 0.99512471, 0.99520895,\n",
       "        0.99529695, 0.99551183, 0.99545022, 0.99542705, 0.99545693,\n",
       "        0.99569829, 0.99286267, 0.99461975, 0.99216267, 0.99515575,\n",
       "        0.99465026, 0.99538195, 0.99505719, 0.99509935, 0.99510039,\n",
       "        0.99552072, 0.99536785, 0.99557514]),\n",
       " 'split1_test_roc_auc_ovo': array([0.99479882, 0.99529686, 0.9955973 , 0.99523118, 0.99371537,\n",
       "        0.99399808, 0.99505736, 0.99424595, 0.99328533, 0.99457141,\n",
       "        0.99278477, 0.99373366, 0.99569221, 0.9952309 , 0.99447012,\n",
       "        0.99365718, 0.99497205, 0.99577745, 0.99399878, 0.99469261,\n",
       "        0.99607964, 0.99323756, 0.99278357, 0.9929913 , 0.99499993,\n",
       "        0.99541629, 0.9957401 , 0.99448821, 0.99497084, 0.99364052,\n",
       "        0.99398615, 0.99351614, 0.99381929, 0.99521796, 0.99365915,\n",
       "        0.99466427, 0.99453932, 0.99483489, 0.99517274, 0.99293399,\n",
       "        0.99429398, 0.99381886, 0.99375336, 0.99456989, 0.9934946 ,\n",
       "        0.99314862, 0.99408942, 0.99513361, 0.99484956, 0.9954071 ,\n",
       "        0.99445648, 0.9946426 , 0.99350875, 0.99297412, 0.99444524,\n",
       "        0.99255194, 0.99281042, 0.99441274, 0.99195059, 0.99439716,\n",
       "        0.99416021, 0.99550489, 0.99540207, 0.99468228, 0.99519697,\n",
       "        0.99500637, 0.99403456, 0.99500595, 0.99446541, 0.993954  ,\n",
       "        0.99400183, 0.99328051, 0.99513275, 0.99591774, 0.99499991,\n",
       "        0.99581743, 0.99474244, 0.99453703, 0.99532128, 0.99454461,\n",
       "        0.99569683, 0.99269263, 0.99430408, 0.99486656, 0.99464651,\n",
       "        0.99461212, 0.99374457, 0.99548237, 0.99473513, 0.99478155,\n",
       "        0.99547118, 0.99451504, 0.99425668, 0.99502855, 0.99504456,\n",
       "        0.99486282, 0.96219081, 0.58915202, 0.95236431, 0.97482438,\n",
       "        0.65978271, 0.98054045, 0.97993826, 0.52415524, 0.98352841,\n",
       "        0.98521533, 0.58087784, 0.98521815, 0.94033135, 0.45856801,\n",
       "        0.95888923, 0.96822871, 0.51657331, 0.97020384, 0.97620939,\n",
       "        0.6107465 , 0.9848708 , 0.98579314, 0.55156541, 0.98523446,\n",
       "        0.96851563, 0.57978077, 0.9577141 , 0.97658196, 0.52102227,\n",
       "        0.9794189 , 0.98325516, 0.47737071, 0.98117368, 0.98267134,\n",
       "        0.41028859, 0.98543064, 0.96468453, 0.47237903, 0.97375732,\n",
       "        0.96304628, 0.55473979, 0.96837279, 0.97778117, 0.57275811,\n",
       "        0.98244282, 0.98423061, 0.37755291, 0.98460672, 0.96338838,\n",
       "        0.69778449, 0.97532772, 0.99223089, 0.76447294, 0.98979585,\n",
       "        0.99207355, 0.78069548, 0.99239916, 0.99179842, 0.70506238,\n",
       "        0.99240756, 0.98833841, 0.52735182, 0.98714055, 0.99235162,\n",
       "        0.51498787, 0.98862772, 0.99282929, 0.70691881, 0.99101692,\n",
       "        0.99262374, 0.59181461, 0.99256591, 0.98314988, 0.48963194,\n",
       "        0.96782031, 0.99076029, 0.49691952, 0.99117435, 0.99216252,\n",
       "        0.56358005, 0.99214294, 0.99257303, 0.58550221, 0.99229285,\n",
       "        0.98444352, 0.59668878, 0.96674822, 0.9918424 , 0.5843235 ,\n",
       "        0.99101106, 0.99169868, 0.68584784, 0.99222422, 0.99194973,\n",
       "        0.61200482, 0.99241455, 0.98180355, 0.97336624, 0.9675251 ,\n",
       "        0.99096917, 0.99208339, 0.99180482, 0.99292964, 0.99215884,\n",
       "        0.9934113 , 0.99330374, 0.99360339, 0.99387678, 0.98197944,\n",
       "        0.97279613, 0.98775293, 0.99257888, 0.99162423, 0.99201165,\n",
       "        0.99337658, 0.99317697, 0.99268327, 0.99349935, 0.9935603 ,\n",
       "        0.99260554, 0.98700117, 0.97822543, 0.98929694, 0.99329545,\n",
       "        0.99272897, 0.99269017, 0.99285436, 0.99223404, 0.99288109,\n",
       "        0.99356868, 0.99332885, 0.99330604, 0.97046686, 0.97779721,\n",
       "        0.9808699 , 0.99311322, 0.99090576, 0.99224091, 0.99326439,\n",
       "        0.99322713, 0.99329229, 0.99345874, 0.99350697, 0.99347001,\n",
       "        0.9922048 , 0.99280814, 0.99055095, 0.9938955 , 0.9942434 ,\n",
       "        0.9932376 , 0.99337665, 0.99352422, 0.99404934, 0.99357758,\n",
       "        0.99407092, 0.99356643, 0.98806703, 0.99351756, 0.99171817,\n",
       "        0.99374507, 0.99392806, 0.99375348, 0.99356314, 0.99383373,\n",
       "        0.99386785, 0.99395722, 0.99369645, 0.99394077, 0.99274043,\n",
       "        0.99340102, 0.99275585, 0.99378153, 0.99312921, 0.9934852 ,\n",
       "        0.99353889, 0.993853  , 0.99381355, 0.99415567, 0.99380311,\n",
       "        0.99350431, 0.9924155 , 0.99099937, 0.99115496, 0.99345929,\n",
       "        0.99305337, 0.99326869, 0.99347859, 0.99353222, 0.99407999,\n",
       "        0.99395858, 0.99320068, 0.99371747]),\n",
       " 'split2_test_roc_auc_ovo': array([0.99644638, 0.99604083, 0.99564534, 0.99573779, 0.99673268,\n",
       "        0.99639956, 0.99541639, 0.99605637, 0.99623533, 0.99562864,\n",
       "        0.99517566, 0.99582862, 0.99519131, 0.9962225 , 0.99581527,\n",
       "        0.99601247, 0.99607118, 0.99527598, 0.99586548, 0.9956807 ,\n",
       "        0.99595239, 0.99558656, 0.99675296, 0.99562728, 0.99544002,\n",
       "        0.99524789, 0.99623384, 0.995813  , 0.99658176, 0.9956621 ,\n",
       "        0.99522684, 0.99568335, 0.9960593 , 0.99553481, 0.99471808,\n",
       "        0.99626547, 0.99539186, 0.99537099, 0.99536824, 0.99621029,\n",
       "        0.99580794, 0.9956519 , 0.99592385, 0.9967078 , 0.99652029,\n",
       "        0.99624657, 0.99627614, 0.99627938, 0.99549031, 0.99555851,\n",
       "        0.99654372, 0.99683295, 0.99629541, 0.99676994, 0.99598812,\n",
       "        0.99531408, 0.99645573, 0.99556903, 0.99561513, 0.99656888,\n",
       "        0.9962228 , 0.99630753, 0.99588481, 0.99631066, 0.99674267,\n",
       "        0.99613066, 0.99635786, 0.99576202, 0.99576227, 0.99646842,\n",
       "        0.99481107, 0.99595928, 0.99560482, 0.99615485, 0.99540712,\n",
       "        0.99703131, 0.99650021, 0.99648663, 0.99639105, 0.99532109,\n",
       "        0.99612927, 0.99608494, 0.99371522, 0.99443135, 0.99578675,\n",
       "        0.99597866, 0.9959038 , 0.99653872, 0.99643508, 0.99629   ,\n",
       "        0.99632014, 0.99576067, 0.99616178, 0.99542032, 0.99563498,\n",
       "        0.99646176, 0.95659357, 0.4931228 , 0.9453514 , 0.96379827,\n",
       "        0.57224198, 0.96990959, 0.98164122, 0.51859843, 0.98586762,\n",
       "        0.98370258, 0.67774578, 0.98623107, 0.95635666, 0.35715754,\n",
       "        0.95397928, 0.97254886, 0.61496928, 0.97706798, 0.97677764,\n",
       "        0.56371212, 0.97939964, 0.98544361, 0.71244917, 0.98406038,\n",
       "        0.96275123, 0.50089197, 0.97232443, 0.97144619, 0.60726501,\n",
       "        0.96751983, 0.97900001, 0.64304889, 0.97652784, 0.98345171,\n",
       "        0.53078598, 0.98493683, 0.95903048, 0.42931958, 0.96666925,\n",
       "        0.97961293, 0.48237954, 0.97260848, 0.98214689, 0.46312617,\n",
       "        0.97440992, 0.9819817 , 0.48999448, 0.97867383, 0.98791877,\n",
       "        0.50580864, 0.99046465, 0.99155206, 0.65096682, 0.99309336,\n",
       "        0.99282606, 0.82002725, 0.99323175, 0.99368817, 0.73860125,\n",
       "        0.99352972, 0.99040762, 0.71227791, 0.98819019, 0.99267528,\n",
       "        0.80085916, 0.99292422, 0.99358261, 0.60757741, 0.99310751,\n",
       "        0.99353252, 0.76952261, 0.99365779, 0.99017202, 0.61988012,\n",
       "        0.97504143, 0.99192298, 0.6937608 , 0.99241476, 0.9936364 ,\n",
       "        0.62197054, 0.99339201, 0.9931232 , 0.64874411, 0.99353338,\n",
       "        0.98622686, 0.61386563, 0.99082673, 0.99309386, 0.67786621,\n",
       "        0.9933838 , 0.9934791 , 0.82112613, 0.99294806, 0.99390245,\n",
       "        0.73886895, 0.99349838, 0.97704002, 0.96856495, 0.98136032,\n",
       "        0.99353887, 0.99410919, 0.9939609 , 0.99435051, 0.99429889,\n",
       "        0.99432021, 0.99476514, 0.9945632 , 0.99486729, 0.99218847,\n",
       "        0.96933387, 0.95971073, 0.99372824, 0.99406707, 0.9935225 ,\n",
       "        0.99446172, 0.99415625, 0.99420668, 0.99455205, 0.99453728,\n",
       "        0.99471069, 0.98685971, 0.9688322 , 0.99102776, 0.99320098,\n",
       "        0.99420872, 0.99392187, 0.99430728, 0.99431272, 0.99432675,\n",
       "        0.99447252, 0.9945553 , 0.99451734, 0.9912835 , 0.99181461,\n",
       "        0.97438275, 0.99315683, 0.99324198, 0.99373016, 0.99430166,\n",
       "        0.99429971, 0.99430783, 0.99454292, 0.99441307, 0.99442521,\n",
       "        0.98197677, 0.99421928, 0.99415687, 0.99518266, 0.99481846,\n",
       "        0.99515386, 0.99538443, 0.99493543, 0.99559064, 0.99537681,\n",
       "        0.99578629, 0.99555393, 0.99361498, 0.99307201, 0.99424528,\n",
       "        0.99534948, 0.99462443, 0.99438123, 0.99519056, 0.99556377,\n",
       "        0.99514368, 0.99533231, 0.99571924, 0.99565892, 0.99379931,\n",
       "        0.99369411, 0.99435833, 0.99432541, 0.99483086, 0.99487651,\n",
       "        0.99530109, 0.99537772, 0.99490357, 0.99557842, 0.99598939,\n",
       "        0.99541914, 0.99215162, 0.99358475, 0.99413964, 0.99418134,\n",
       "        0.9947541 , 0.99471071, 0.99485403, 0.99506347, 0.99522836,\n",
       "        0.99467216, 0.99522906, 0.99487253]),\n",
       " 'split3_test_roc_auc_ovo': array([0.99673697, 0.99694605, 0.99707453, 0.99722863, 0.99695441,\n",
       "        0.99670252, 0.99570527, 0.99635832, 0.99587735, 0.99659774,\n",
       "        0.99543229, 0.99604053, 0.99709198, 0.99629029, 0.99720296,\n",
       "        0.99730834, 0.99709137, 0.99715763, 0.99681866, 0.99613691,\n",
       "        0.99668017, 0.99610232, 0.99587788, 0.99643114, 0.99736952,\n",
       "        0.99718067, 0.99659528, 0.99678844, 0.99735574, 0.99661217,\n",
       "        0.99689118, 0.99593803, 0.99685273, 0.99511395, 0.99592145,\n",
       "        0.99620454, 0.99707617, 0.99645957, 0.99629122, 0.99696878,\n",
       "        0.99703359, 0.99701065, 0.9960535 , 0.99710525, 0.99722543,\n",
       "        0.99675931, 0.99655143, 0.99651064, 0.99705401, 0.99708517,\n",
       "        0.9969381 , 0.99667126, 0.99698596, 0.99654873, 0.99614543,\n",
       "        0.99613612, 0.99613143, 0.9958424 , 0.99452397, 0.99535839,\n",
       "        0.9972749 , 0.99694451, 0.99732634, 0.99641663, 0.99656431,\n",
       "        0.99698404, 0.99657558, 0.99643465, 0.99616574, 0.99481138,\n",
       "        0.99643788, 0.99606912, 0.99700084, 0.99742013, 0.99729563,\n",
       "        0.99692262, 0.99674931, 0.99666227, 0.99634969, 0.9953907 ,\n",
       "        0.99566407, 0.99668004, 0.99610435, 0.99355827, 0.99723491,\n",
       "        0.99657461, 0.99706614, 0.99698424, 0.99719077, 0.9971491 ,\n",
       "        0.99710137, 0.99719521, 0.99581568, 0.99625102, 0.9964116 ,\n",
       "        0.99432188, 0.95035207, 0.52727323, 0.93213671, 0.97047118,\n",
       "        0.70085648, 0.97941209, 0.98365105, 0.46211107, 0.98451729,\n",
       "        0.98811953, 0.65572476, 0.98746774, 0.92557395, 0.46135022,\n",
       "        0.96271969, 0.98144396, 0.57868817, 0.96229644, 0.9856509 ,\n",
       "        0.40682073, 0.97446589, 0.98479561, 0.48301775, 0.98543025,\n",
       "        0.92190722, 0.51718096, 0.9385918 , 0.96001421, 0.58508648,\n",
       "        0.96817863, 0.98493701, 0.54818945, 0.98342717, 0.98432739,\n",
       "        0.55844706, 0.98686712, 0.97042171, 0.48301112, 0.94917204,\n",
       "        0.97063946, 0.50440651, 0.97764744, 0.97989689, 0.49072886,\n",
       "        0.98357737, 0.98757288, 0.33017394, 0.98843571, 0.99059533,\n",
       "        0.41625995, 0.98318469, 0.99270911, 0.53710548, 0.99321811,\n",
       "        0.99486274, 0.74953373, 0.99446271, 0.99495802, 0.71327383,\n",
       "        0.99498806, 0.99078675, 0.71938244, 0.96850678, 0.99435582,\n",
       "        0.69014471, 0.99277031, 0.99350829, 0.55123253, 0.99473545,\n",
       "        0.99481732, 0.66506694, 0.99485619, 0.99109705, 0.83127171,\n",
       "        0.96914757, 0.9932332 , 0.76284311, 0.99424115, 0.99485938,\n",
       "        0.61837931, 0.9945184 , 0.99504499, 0.80185403, 0.99497007,\n",
       "        0.99009104, 0.61786094, 0.99177375, 0.98985649, 0.80919731,\n",
       "        0.99266076, 0.99447034, 0.77057852, 0.9948785 , 0.99449649,\n",
       "        0.62421273, 0.99519024, 0.96867022, 0.99224668, 0.96927754,\n",
       "        0.99494245, 0.99549501, 0.99442422, 0.9958062 , 0.99559173,\n",
       "        0.99513334, 0.99595364, 0.995979  , 0.99604466, 0.96877991,\n",
       "        0.96692557, 0.96944159, 0.9956996 , 0.99557181, 0.99526621,\n",
       "        0.9960012 , 0.99566662, 0.99551279, 0.99589211, 0.99627343,\n",
       "        0.99585286, 0.98948735, 0.96912713, 0.98352286, 0.9954803 ,\n",
       "        0.99447147, 0.99524305, 0.99563185, 0.99560607, 0.99588723,\n",
       "        0.99603034, 0.99589323, 0.99598663, 0.97081617, 0.99327734,\n",
       "        0.96816434, 0.99461344, 0.99510289, 0.99510034, 0.9951939 ,\n",
       "        0.99552278, 0.99549711, 0.99565164, 0.99571616, 0.9958641 ,\n",
       "        0.99493343, 0.99395392, 0.99504322, 0.99613254, 0.99611681,\n",
       "        0.99635986, 0.99637908, 0.99634615, 0.99607062, 0.99636349,\n",
       "        0.99642003, 0.99658091, 0.99484747, 0.99238776, 0.99193529,\n",
       "        0.99625537, 0.99607762, 0.99580662, 0.99611047, 0.99613745,\n",
       "        0.99637561, 0.99629846, 0.99656175, 0.99628485, 0.99465447,\n",
       "        0.99470234, 0.99457663, 0.99600926, 0.99611359, 0.99592085,\n",
       "        0.99625265, 0.99604179, 0.99618891, 0.99658083, 0.99622002,\n",
       "        0.99635438, 0.9941422 , 0.99451234, 0.99462628, 0.9962446 ,\n",
       "        0.99597707, 0.99618268, 0.99623416, 0.99635654, 0.99608672,\n",
       "        0.99629636, 0.99638038, 0.99646594]),\n",
       " 'split4_test_roc_auc_ovo': array([0.99651936, 0.99646354, 0.99659348, 0.99612686, 0.99618038,\n",
       "        0.99613106, 0.99532558, 0.99594558, 0.99668997, 0.99389169,\n",
       "        0.99509521, 0.99614525, 0.99643132, 0.99636882, 0.99585663,\n",
       "        0.99668166, 0.99586247, 0.99633812, 0.99568079, 0.99576361,\n",
       "        0.99587126, 0.99456445, 0.99492083, 0.99549715, 0.99658973,\n",
       "        0.99669229, 0.99643336, 0.99676476, 0.99640831, 0.99683661,\n",
       "        0.99624916, 0.99562941, 0.99622457, 0.99601832, 0.99602728,\n",
       "        0.99547509, 0.99659804, 0.99654757, 0.99618621, 0.99642414,\n",
       "        0.99630843, 0.99639285, 0.99669608, 0.99703791, 0.9963921 ,\n",
       "        0.99651305, 0.99699118, 0.99621429, 0.99638866, 0.99659774,\n",
       "        0.99647995, 0.9971035 , 0.99634301, 0.99574265, 0.99610343,\n",
       "        0.99561136, 0.99381343, 0.99453627, 0.99438722, 0.99420544,\n",
       "        0.99653367, 0.9960977 , 0.99570743, 0.99668268, 0.99568222,\n",
       "        0.99695538, 0.99466998, 0.99582159, 0.99607902, 0.99551115,\n",
       "        0.99614288, 0.99548007, 0.9960022 , 0.99703692, 0.99615292,\n",
       "        0.99619293, 0.99671976, 0.99675481, 0.99684186, 0.9966634 ,\n",
       "        0.99573256, 0.99345145, 0.99468642, 0.9957341 , 0.99592287,\n",
       "        0.99578265, 0.99667866, 0.99659355, 0.99627011, 0.99594737,\n",
       "        0.9961847 , 0.99549294, 0.99664434, 0.99506728, 0.99578922,\n",
       "        0.99522793, 0.94909051, 0.61495334, 0.96760937, 0.97968261,\n",
       "        0.25858863, 0.97992252, 0.98236577, 0.58009821, 0.98174007,\n",
       "        0.98855877, 0.55647589, 0.98805148, 0.96149728, 0.45447509,\n",
       "        0.96630879, 0.96534339, 0.34939227, 0.97709387, 0.98229905,\n",
       "        0.67032682, 0.98624632, 0.99003938, 0.63293888, 0.98552256,\n",
       "        0.96079271, 0.66435994, 0.92817358, 0.96809385, 0.51928068,\n",
       "        0.9862376 , 0.97776362, 0.72355794, 0.98707419, 0.98599228,\n",
       "        0.45650746, 0.98598005, 0.97657115, 0.4936356 , 0.95213809,\n",
       "        0.9781163 , 0.5469128 , 0.9778734 , 0.9846864 , 0.53798751,\n",
       "        0.98836786, 0.98802236, 0.72047424, 0.98651716, 0.9911849 ,\n",
       "        0.42943216, 0.98417242, 0.99451474, 0.61034526, 0.99451191,\n",
       "        0.99518276, 0.85672975, 0.99532459, 0.99556061, 0.69657528,\n",
       "        0.99554517, 0.9916901 , 0.53045031, 0.96524626, 0.99480921,\n",
       "        0.53676596, 0.99504988, 0.99463122, 0.50540598, 0.99505498,\n",
       "        0.99536529, 0.63036905, 0.99537698, 0.97116734, 0.42226398,\n",
       "        0.99269586, 0.99476504, 0.55072093, 0.99480758, 0.99515078,\n",
       "        0.83817109, 0.99491792, 0.99576177, 0.62181126, 0.99552145,\n",
       "        0.98447087, 0.52837735, 0.96941505, 0.99357814, 0.69430919,\n",
       "        0.99492926, 0.99524365, 0.73467881, 0.99451353, 0.99533616,\n",
       "        0.76949352, 0.99562005, 0.97849834, 0.99264623, 0.97772873,\n",
       "        0.99550313, 0.99513008, 0.99533652, 0.99580207, 0.99609166,\n",
       "        0.99594564, 0.99600405, 0.99609732, 0.99560595, 0.9749899 ,\n",
       "        0.97707792, 0.9698931 , 0.99513764, 0.99511231, 0.99559497,\n",
       "        0.99610729, 0.99565148, 0.99545738, 0.99613153, 0.99602598,\n",
       "        0.99597548, 0.96891772, 0.98449874, 0.9823063 , 0.99539314,\n",
       "        0.99547038, 0.99495873, 0.99573388, 0.99600978, 0.99558751,\n",
       "        0.99601003, 0.99606929, 0.99574208, 0.98543402, 0.97045686,\n",
       "        0.96916397, 0.99537231, 0.99509764, 0.99449095, 0.99533556,\n",
       "        0.99540383, 0.99586382, 0.99591073, 0.99576769, 0.99576557,\n",
       "        0.9953239 , 0.99427942, 0.99480551, 0.99593777, 0.99538898,\n",
       "        0.99572397, 0.99604692, 0.99594708, 0.99588201, 0.99608455,\n",
       "        0.99618897, 0.99612901, 0.99459448, 0.99524962, 0.99533796,\n",
       "        0.99640785, 0.9960545 , 0.9957229 , 0.99636552, 0.99596695,\n",
       "        0.99608187, 0.99610078, 0.99631868, 0.99617193, 0.99510039,\n",
       "        0.99488322, 0.99482433, 0.99573666, 0.99601306, 0.99653254,\n",
       "        0.99615461, 0.99583807, 0.99634927, 0.99582351, 0.9961528 ,\n",
       "        0.99621469, 0.99427271, 0.99464724, 0.9948793 , 0.99569822,\n",
       "        0.99587721, 0.99584147, 0.99591965, 0.99610352, 0.99643403,\n",
       "        0.99647378, 0.99606449, 0.99597743]),\n",
       " 'mean_test_roc_auc_ovo': array([0.99600032, 0.99613678, 0.99616123, 0.99591776, 0.99598156,\n",
       "        0.99566349, 0.99505802, 0.99555581, 0.99526279, 0.99483759,\n",
       "        0.9944816 , 0.99514092, 0.99587274, 0.99580059, 0.99582498,\n",
       "        0.99559473, 0.99590765, 0.99587264, 0.99559518, 0.99542663,\n",
       "        0.99598279, 0.99458698, 0.99489832, 0.99478116, 0.99602558,\n",
       "        0.9958434 , 0.9961107 , 0.99566969, 0.99616831, 0.99551408,\n",
       "        0.99534206, 0.99522385, 0.99544525, 0.9952296 , 0.99495792,\n",
       "        0.99519662, 0.99584212, 0.99576199, 0.99564059, 0.99542326,\n",
       "        0.99573574, 0.99556837, 0.99562735, 0.99607591, 0.99566601,\n",
       "        0.99548051, 0.99550478, 0.99561227, 0.99590124, 0.99607468,\n",
       "        0.99597106, 0.99604759, 0.9955126 , 0.99568972, 0.9954165 ,\n",
       "        0.99484965, 0.99472098, 0.99459234, 0.99420984, 0.99499852,\n",
       "        0.99607024, 0.99614674, 0.99591131, 0.99545527, 0.99600089,\n",
       "        0.9958446 , 0.99521111, 0.99547375, 0.99527253, 0.99460531,\n",
       "        0.99523699, 0.99450942, 0.9959051 , 0.99633151, 0.99590734,\n",
       "        0.99630854, 0.9961203 , 0.99597517, 0.99578013, 0.99528002,\n",
       "        0.99566483, 0.99451586, 0.99462202, 0.99447509, 0.99601377,\n",
       "        0.99566151, 0.99582812, 0.99595052, 0.99615477, 0.99572128,\n",
       "        0.99592555, 0.99558927, 0.99547964, 0.99518556, 0.99529552,\n",
       "        0.99535072, 0.95113194, 0.54169527, 0.95215164, 0.97190127,\n",
       "        0.55184826, 0.97617754, 0.98126633, 0.52462927, 0.98231611,\n",
       "        0.98630088, 0.58597293, 0.98578953, 0.94185877, 0.45207032,\n",
       "        0.95800243, 0.96960097, 0.53501028, 0.97229987, 0.97980983,\n",
       "        0.55084345, 0.98166241, 0.98589599, 0.59584152, 0.98473078,\n",
       "        0.95082112, 0.52782498, 0.95029284, 0.97040804, 0.58132502,\n",
       "        0.97652473, 0.98014067, 0.59564965, 0.98270984, 0.9847265 ,\n",
       "        0.52596597, 0.98503154, 0.96355037, 0.50652964, 0.95567989,\n",
       "        0.97230226, 0.50503649, 0.97263485, 0.98049637, 0.50685884,\n",
       "        0.98184587, 0.98508866, 0.46453191, 0.98373621, 0.98462656,\n",
       "        0.5073577 , 0.98516916, 0.9926022 , 0.65533321, 0.99272488,\n",
       "        0.9936314 , 0.79133103, 0.99380218, 0.99403439, 0.69443201,\n",
       "        0.99414127, 0.98971318, 0.63744571, 0.98009787, 0.99361543,\n",
       "        0.59127251, 0.99211052, 0.99373112, 0.57196736, 0.99350982,\n",
       "        0.99410468, 0.69360589, 0.99402878, 0.98498164, 0.62704955,\n",
       "        0.97543385, 0.99263708, 0.60039903, 0.99307783, 0.99395581,\n",
       "        0.66554154, 0.99374054, 0.99417439, 0.68332638, 0.99412651,\n",
       "        0.98754977, 0.60207771, 0.98139454, 0.99229009, 0.71396703,\n",
       "        0.99307011, 0.99375163, 0.74767802, 0.99362357, 0.99393964,\n",
       "        0.70846536, 0.99409638, 0.97732359, 0.97977396, 0.97663538,\n",
       "        0.99341325, 0.9941483 , 0.99381591, 0.99471467, 0.99463763,\n",
       "        0.99477191, 0.99504658, 0.99509287, 0.99512376, 0.97922704,\n",
       "        0.96953964, 0.97466073, 0.99434575, 0.99401942, 0.9938639 ,\n",
       "        0.99499824, 0.99459871, 0.99451284, 0.99504697, 0.99514613,\n",
       "        0.99486712, 0.9836696 , 0.97723695, 0.98098672, 0.99404369,\n",
       "        0.99425832, 0.99424466, 0.99469237, 0.99461486, 0.99472434,\n",
       "        0.99503499, 0.99500428, 0.99494446, 0.97986516, 0.98263339,\n",
       "        0.9762156 , 0.99331241, 0.99354138, 0.99398669, 0.99454263,\n",
       "        0.99456149, 0.99475688, 0.99488983, 0.99488474, 0.99492148,\n",
       "        0.9912558 , 0.99382725, 0.99316291, 0.99534364, 0.99522926,\n",
       "        0.99503842, 0.99534786, 0.99522972, 0.99538277, 0.99537303,\n",
       "        0.99563293, 0.99544745, 0.99315553, 0.99349599, 0.99303353,\n",
       "        0.99539452, 0.99524627, 0.99503788, 0.99529971, 0.99533965,\n",
       "        0.99547352, 0.99537674, 0.99557711, 0.99551951, 0.99391055,\n",
       "        0.99422828, 0.99369764, 0.9949862 , 0.99504229, 0.99520481,\n",
       "        0.99530884, 0.99532448, 0.9953411 , 0.9955131 , 0.99552445,\n",
       "        0.99543816, 0.99316894, 0.99367269, 0.99339257, 0.99494784,\n",
       "        0.9948624 , 0.9950771 , 0.99510872, 0.99523102, 0.9953859 ,\n",
       "        0.99538432, 0.99524849, 0.9953217 ]),\n",
       " 'std_test_roc_auc_ovo': array([0.00073551, 0.00055072, 0.00057892, 0.00073387, 0.0011665 ,\n",
       "        0.00099467, 0.00066907, 0.00076274, 0.00129195, 0.00113866,\n",
       "        0.00099565, 0.00106527, 0.00079174, 0.00061541, 0.00086459,\n",
       "        0.00139188, 0.00069852, 0.00081898, 0.00090841, 0.00055556,\n",
       "        0.00043234, 0.00113342, 0.00137276, 0.00135479, 0.0008494 ,\n",
       "        0.00093806, 0.00040119, 0.00102463, 0.00083504, 0.00118069,\n",
       "        0.00109953, 0.00087389, 0.00118232, 0.00057674, 0.00090117,\n",
       "        0.00108094, 0.00090005, 0.00065474, 0.00049438, 0.00147776,\n",
       "        0.00093244, 0.00111253, 0.00099301, 0.00108632, 0.00136686,\n",
       "        0.00136249, 0.00137634, 0.00096911, 0.00075801, 0.00065255,\n",
       "        0.00090567, 0.00102159, 0.00131612, 0.00140039, 0.00081313,\n",
       "        0.00124712, 0.00138417, 0.00114161, 0.00121166, 0.00088014,\n",
       "        0.00103397, 0.00047923, 0.00074297, 0.00133554, 0.00057426,\n",
       "        0.00111354, 0.00104725, 0.00072408, 0.00092199, 0.00142534,\n",
       "        0.00091307, 0.00170627, 0.00061826, 0.00081655, 0.00078899,\n",
       "        0.00058088, 0.00075511, 0.0008604 , 0.00102136, 0.00078843,\n",
       "        0.00032824, 0.00156958, 0.00080355, 0.0007819 , 0.00085272,\n",
       "        0.00065444, 0.00114974, 0.0010272 , 0.00079758, 0.0009942 ,\n",
       "        0.00086059, 0.00090993, 0.00093294, 0.00067519, 0.00095413,\n",
       "        0.00075171, 0.00830146, 0.05199578, 0.01272818, 0.0052568 ,\n",
       "        0.15525718, 0.0046596 , 0.0017439 , 0.03796219, 0.00346637,\n",
       "        0.00181601, 0.07784019, 0.00214349, 0.0150324 , 0.0548272 ,\n",
       "        0.00641838, 0.00711169, 0.09956128, 0.00559674, 0.00361535,\n",
       "        0.09066329, 0.00426602, 0.00222634, 0.07703851, 0.00084543,\n",
       "        0.01734925, 0.09482071, 0.01539946, 0.00604628, 0.05788985,\n",
       "        0.00742922, 0.00343382, 0.08359563, 0.00366306, 0.00165397,\n",
       "        0.09070736, 0.00167187, 0.01010874, 0.07705058, 0.01314616,\n",
       "        0.00600921, 0.04336281, 0.0046106 , 0.00262268, 0.04210864,\n",
       "        0.00454279, 0.00233574, 0.13811617, 0.00366112, 0.01067615,\n",
       "        0.1010249 , 0.00610715, 0.0010264 , 0.07918051, 0.00156399,\n",
       "        0.00119786, 0.0417045 , 0.00100913, 0.00128991, 0.04041499,\n",
       "        0.00110242, 0.00161559, 0.08890524, 0.01093501, 0.00095154,\n",
       "        0.13715566, 0.00213224, 0.0006057 , 0.07908657, 0.00143398,\n",
       "        0.00096166, 0.08340238, 0.00098961, 0.00744804, 0.15725614,\n",
       "        0.0089937 , 0.00133668, 0.10845928, 0.00130498, 0.00105475,\n",
       "        0.09457529, 0.00096594, 0.00118194, 0.08280418, 0.00113199,\n",
       "        0.00322508, 0.04123825, 0.01096497, 0.00134545, 0.08449488,\n",
       "        0.00126881, 0.00118699, 0.04555016, 0.00097839, 0.00111597,\n",
       "        0.07618809, 0.00116776, 0.00463   , 0.01046605, 0.00740371,\n",
       "        0.00169629, 0.00119107, 0.00116764, 0.00106712, 0.00137487,\n",
       "        0.00085327, 0.0009881 , 0.00092842, 0.00073639, 0.00779398,\n",
       "        0.00525177, 0.01081824, 0.00109728, 0.00137348, 0.00137066,\n",
       "        0.00101545, 0.00095202, 0.00103631, 0.00095324, 0.00099656,\n",
       "        0.00122019, 0.00746405, 0.00718914, 0.01158625, 0.00114744,\n",
       "        0.00088085, 0.0009005 , 0.00105381, 0.00132484, 0.00106763,\n",
       "        0.00093928, 0.00099621, 0.00096334, 0.00816988, 0.0086851 ,\n",
       "        0.00762107, 0.00173416, 0.00154465, 0.00097528, 0.00074169,\n",
       "        0.00083973, 0.00090822, 0.00087074, 0.00084969, 0.00089219,\n",
       "        0.00484597, 0.000532  , 0.00188003, 0.00079366, 0.00064436,\n",
       "        0.00105602, 0.00104698, 0.00097815, 0.00071378, 0.00097149,\n",
       "        0.00082418, 0.00102961, 0.00257999, 0.00095317, 0.00147856,\n",
       "        0.00095109, 0.00084368, 0.00082202, 0.00098202, 0.0008208 ,\n",
       "        0.00090016, 0.00082751, 0.00100764, 0.00083949, 0.00086906,\n",
       "        0.00057921, 0.00112603, 0.00083739, 0.00107717, 0.00103349,\n",
       "        0.00097355, 0.0007722 , 0.00092433, 0.00078622, 0.00090137,\n",
       "        0.00102472, 0.00087886, 0.00139328, 0.00147038, 0.00100907,\n",
       "        0.00105845, 0.00102996, 0.00096427, 0.00099593, 0.0008248 ,\n",
       "        0.00095679, 0.00110952, 0.00095695]),\n",
       " 'rank_test_roc_auc_ovo': array([ 17,   7,   4,  24,  19,  46, 113,  57,  94, 134, 154, 108,  30,\n",
       "         37,  36,  53,  26,  31,  52,  72,  18, 148, 128, 135,  14,  33,\n",
       "          9,  43,   3,  60,  84, 102,  70, 100, 124, 105,  34,  39,  48,\n",
       "         73,  40,  56,  50,  10,  44,  64,  63,  51,  29,  11,  21,  13,\n",
       "         62,  42,  74, 133, 139, 147, 160, 121,  12,   6,  25,  68,  16,\n",
       "         32, 103,  66,  93, 145,  97, 153,  28,   1,  27,   2,   8,  20,\n",
       "         38,  92,  45, 151, 143, 155,  15,  47,  35,  22,   5,  41,  23,\n",
       "         54,  65, 106,  91,  81, 253, 278, 252, 245, 276, 239, 225, 282,\n",
       "        221, 207, 273, 209, 256, 288, 250, 247, 279, 244, 231, 277, 223,\n",
       "        208, 270, 214, 254, 280, 255, 246, 274, 237, 228, 271, 219, 215,\n",
       "        281, 212, 249, 285, 251, 243, 286, 242, 227, 284, 222, 211, 287,\n",
       "        217, 216, 283, 210, 201, 265, 199, 184, 257, 178, 168, 261, 163,\n",
       "        205, 266, 229, 186, 272, 203, 181, 275, 188, 165, 262, 169, 213,\n",
       "        267, 240, 200, 269, 196, 172, 264, 180, 161, 263, 164, 206, 268,\n",
       "        224, 202, 259, 197, 179, 258, 185, 173, 260, 166, 234, 232, 236,\n",
       "        190, 162, 177, 140, 142, 136, 115, 111, 109, 233, 248, 241, 156,\n",
       "        170, 175, 122, 146, 152, 114, 107, 131, 218, 235, 226, 167, 157,\n",
       "        158, 141, 144, 138, 119, 120, 126, 230, 220, 238, 192, 187, 171,\n",
       "        150, 149, 137, 129, 130, 127, 204, 176, 194,  83, 101, 117,  82,\n",
       "         99,  78,  80,  49,  69, 195, 189, 198,  75,  96, 118,  90,  86,\n",
       "         67,  79,  55,  59, 174, 159, 182, 123, 116, 104,  89,  87,  85,\n",
       "         61,  58,  71, 193, 183, 191, 125, 132, 112, 110,  98,  76,  77,\n",
       "         95,  88]),\n",
       " 'split0_test_neg_log_loss': array([-0.21248971, -0.20477009, -0.19870349, -0.2133103 , -0.19518295,\n",
       "        -0.21293852, -0.25662934, -0.2261523 , -0.26638746, -0.28499179,\n",
       "        -0.27168492, -0.27871909, -0.21540934, -0.21222521, -0.20878578,\n",
       "        -0.22809658, -0.20393925, -0.23626902, -0.2071036 , -0.23946769,\n",
       "        -0.23229358, -0.2886617 , -0.26147863, -0.29263492, -0.20461207,\n",
       "        -0.21170552, -0.20669906, -0.22536285, -0.21279801, -0.21146889,\n",
       "        -0.23714345, -0.22326234, -0.25240338, -0.23764579, -0.2469322 ,\n",
       "        -0.28261673, -0.19685381, -0.19600067, -0.20664958, -0.20195026,\n",
       "        -0.20096121, -0.2026435 , -0.20039254, -0.20428437, -0.21267973,\n",
       "        -0.20835824, -0.23199535, -0.22834716, -0.20693683, -0.20106025,\n",
       "        -0.20264043, -0.22459789, -0.22346742, -0.1966637 , -0.25469743,\n",
       "        -0.25478746, -0.2636943 , -0.3094899 , -0.27960628, -0.26232438,\n",
       "        -0.19389072, -0.19665234, -0.21049172, -0.24882583, -0.21340538,\n",
       "        -0.23834194, -0.2512826 , -0.25729456, -0.25940333, -0.31568748,\n",
       "        -0.2597395 , -0.35880085, -0.20156485, -0.21385569, -0.20362624,\n",
       "        -0.21172874, -0.20761136, -0.21754671, -0.25580093, -0.25055383,\n",
       "        -0.2266031 , -0.30268418, -0.27972703, -0.30544965, -0.1894442 ,\n",
       "        -0.20390012, -0.20102873, -0.22119939, -0.19388873, -0.21345403,\n",
       "        -0.24509339, -0.22616014, -0.23164034, -0.25547628, -0.25411683,\n",
       "        -0.21719823, -0.86126025, -1.98045435, -0.83152992, -0.67229004,\n",
       "        -1.93919996, -0.71509997, -0.57167885, -1.96898743, -0.61070077,\n",
       "        -0.5246522 , -1.95110506, -0.53354495, -1.04875323, -2.03332564,\n",
       "        -0.80822931, -0.69017762, -1.91482106, -0.67566572, -0.58006451,\n",
       "        -1.92494984, -0.56857591, -0.53998608, -1.98421048, -0.52997549,\n",
       "        -0.87349645, -1.99062735, -0.8133669 , -0.64515343, -1.94810596,\n",
       "        -0.62526639, -0.57258646, -1.92238295, -0.58797544, -0.52090234,\n",
       "        -1.93144727, -0.54971775, -0.8079007 , -1.95107828, -0.82896486,\n",
       "        -0.66278815, -1.98459662, -0.65815647, -0.5907035 , -1.9469739 ,\n",
       "        -0.59341494, -0.55253851, -1.93618625, -0.5466549 , -0.33594351,\n",
       "        -2.07192022, -0.314035  , -0.27853659, -1.73658564, -0.26883423,\n",
       "        -0.25610282, -1.50917948, -0.26027863, -0.24456227, -1.90437588,\n",
       "        -0.2437771 , -0.36923125, -1.91269441, -0.33156659, -0.25999972,\n",
       "        -2.19527011, -0.27683149, -0.25235515, -1.99451914, -0.26174286,\n",
       "        -0.24872666, -1.52177099, -0.24986794, -0.33217611, -1.65888308,\n",
       "        -0.36755309, -0.27741764, -1.84609161, -0.26616101, -0.25547872,\n",
       "        -1.89329085, -0.2545175 , -0.24139947, -1.66177673, -0.2423276 ,\n",
       "        -0.31163783, -2.03298808, -0.33954995, -0.2787079 , -1.65458381,\n",
       "        -0.27256891, -0.25099057, -1.7342049 , -0.2611352 , -0.25227922,\n",
       "        -1.45047415, -0.25395591, -0.34623675, -0.38173878, -0.31692371,\n",
       "        -0.25825735, -0.24111844, -0.22992405, -0.21641093, -0.21044848,\n",
       "        -0.21491662, -0.208325  , -0.20397356, -0.20531882, -0.36717376,\n",
       "        -0.37244272, -0.3365895 , -0.22058001, -0.23887348, -0.24621146,\n",
       "        -0.20907365, -0.21808742, -0.21603432, -0.20870746, -0.20768789,\n",
       "        -0.20737673, -0.34039276, -0.34705393, -0.36375969, -0.24012319,\n",
       "        -0.22449557, -0.22096466, -0.21251637, -0.21493862, -0.21079808,\n",
       "        -0.20734176, -0.21186985, -0.21027143, -0.33708042, -0.35987392,\n",
       "        -0.3170631 , -0.26406488, -0.24420828, -0.24275743, -0.22752384,\n",
       "        -0.23153367, -0.22150089, -0.22234293, -0.21931699, -0.21769584,\n",
       "        -0.24883168, -0.24394994, -0.24899212, -0.20530464, -0.20456263,\n",
       "        -0.2144253 , -0.20148418, -0.20237368, -0.20291169, -0.19947544,\n",
       "        -0.2013119 , -0.20248107, -0.2295817 , -0.23967964, -0.25390984,\n",
       "        -0.20954972, -0.20313179, -0.19964458, -0.20346756, -0.20436512,\n",
       "        -0.19585592, -0.20095034, -0.19968377, -0.19857732, -0.24177033,\n",
       "        -0.23544542, -0.24775065, -0.20568296, -0.2085576 , -0.20517862,\n",
       "        -0.2036838 , -0.19974642, -0.20430924, -0.20221074, -0.19989966,\n",
       "        -0.19728785, -0.23662936, -0.22194365, -0.24149557, -0.20573685,\n",
       "        -0.2151704 , -0.20577324, -0.20580417, -0.20631114, -0.20179073,\n",
       "        -0.19806864, -0.2000879 , -0.19952354]),\n",
       " 'split1_test_neg_log_loss': array([-0.20465109, -0.2179804 , -0.19383352, -0.21348508, -0.24089096,\n",
       "        -0.22318443, -0.21573124, -0.25279723, -0.24363668, -0.25388776,\n",
       "        -0.28927787, -0.27386108, -0.20192063, -0.19420468, -0.22180247,\n",
       "        -0.22995128, -0.21070349, -0.20518001, -0.22145738, -0.23927409,\n",
       "        -0.1960786 , -0.24861705, -0.29605372, -0.28985887, -0.21401745,\n",
       "        -0.19966845, -0.19280061, -0.21570402, -0.21035422, -0.2517033 ,\n",
       "        -0.22229014, -0.24032442, -0.241678  , -0.2218192 , -0.24834503,\n",
       "        -0.23492291, -0.20160488, -0.19616257, -0.20598156, -0.21475414,\n",
       "        -0.1992677 , -0.20366213, -0.20874329, -0.19752758, -0.22000493,\n",
       "        -0.21843001, -0.21447592, -0.2022166 , -0.21123039, -0.2136394 ,\n",
       "        -0.22144592, -0.20375976, -0.23669942, -0.25129557, -0.24053861,\n",
       "        -0.27812717, -0.30732795, -0.26144666, -0.37103189, -0.28368072,\n",
       "        -0.21407228, -0.20066857, -0.20185514, -0.22306063, -0.21204745,\n",
       "        -0.21323008, -0.27061651, -0.23209747, -0.23929253, -0.29249228,\n",
       "        -0.28221622, -0.2911568 , -0.20325287, -0.20212306, -0.20817817,\n",
       "        -0.20448518, -0.21851992, -0.21760049, -0.22924574, -0.25240376,\n",
       "        -0.21223381, -0.30481594, -0.29729618, -0.24473918, -0.20041495,\n",
       "        -0.21003226, -0.21407033, -0.1946665 , -0.20286298, -0.2185004 ,\n",
       "        -0.2009841 , -0.21018898, -0.22601707, -0.2348454 , -0.22204086,\n",
       "        -0.22010432, -0.84447401, -2.15265804, -0.90232133, -0.65221373,\n",
       "        -1.85124349, -0.667051  , -0.58174959, -1.98941425, -0.57048957,\n",
       "        -0.54343508, -1.93619545, -0.51210826, -0.94779535, -1.93632855,\n",
       "        -0.86960285, -0.66083415, -2.00472035, -0.64335595, -0.61648528,\n",
       "        -1.9500506 , -0.58425005, -0.53790776, -1.904843  , -0.51403519,\n",
       "        -0.89306052, -1.93864988, -0.85757068, -0.70446487, -1.96443553,\n",
       "        -0.64456693, -0.57378378, -1.98868586, -0.58487277, -0.55075545,\n",
       "        -1.91938295, -0.51592049, -0.83481501, -2.06452163, -0.88276074,\n",
       "        -0.6638125 , -1.98322535, -0.69916081, -0.60077889, -1.87643429,\n",
       "        -0.58992597, -0.53808786, -1.91023629, -0.53707065, -0.41948659,\n",
       "        -1.50727681, -0.37496769, -0.26337562, -1.64525831, -0.2905768 ,\n",
       "        -0.27157295, -1.54702843, -0.2557615 , -0.25146612, -1.56131758,\n",
       "        -0.25124431, -0.35140434, -2.10732159, -0.37310874, -0.27157799,\n",
       "        -2.11034279, -0.28546327, -0.25915745, -1.63368336, -0.25904251,\n",
       "        -0.24495106, -1.888421  , -0.24916518, -0.43672178, -2.03138381,\n",
       "        -0.38678573, -0.27700909, -2.19483539, -0.28265948, -0.26816097,\n",
       "        -2.14076037, -0.25875906, -0.24552493, -1.83585095, -0.24589358,\n",
       "        -0.35597999, -2.10900068, -0.39127872, -0.26773179, -2.09471744,\n",
       "        -0.2724788 , -0.26812797, -1.65294735, -0.26202471, -0.25731392,\n",
       "        -1.82565057, -0.25039779, -0.35289702, -0.35155549, -0.37012074,\n",
       "        -0.2403928 , -0.23206362, -0.23861293, -0.22364157, -0.22294237,\n",
       "        -0.22234288, -0.21964513, -0.21711333, -0.21614048, -0.34751931,\n",
       "        -0.40117154, -0.30934652, -0.23509656, -0.23427707, -0.2320431 ,\n",
       "        -0.21755332, -0.22428609, -0.22657004, -0.22085936, -0.2212486 ,\n",
       "        -0.22079583, -0.32234292, -0.38612868, -0.30592626, -0.22764392,\n",
       "        -0.25161622, -0.23516277, -0.22291169, -0.22636659, -0.22513256,\n",
       "        -0.2194479 , -0.21952708, -0.22030923, -0.37570889, -0.36390665,\n",
       "        -0.33925816, -0.24046597, -0.24638763, -0.24272673, -0.2278206 ,\n",
       "        -0.22807903, -0.23051168, -0.22144472, -0.22470342, -0.2217603 ,\n",
       "        -0.2478007 , -0.24154452, -0.24814245, -0.21943205, -0.21549943,\n",
       "        -0.22158076, -0.2145618 , -0.21586208, -0.21180935, -0.21322762,\n",
       "        -0.20964242, -0.21370527, -0.27628997, -0.2239177 , -0.23280008,\n",
       "        -0.21678072, -0.21457305, -0.21850092, -0.22011894, -0.21508052,\n",
       "        -0.21132061, -0.21070136, -0.21679043, -0.21433384, -0.24028859,\n",
       "        -0.22509586, -0.23666278, -0.21701165, -0.22523536, -0.2205509 ,\n",
       "        -0.21445933, -0.21702439, -0.2140928 , -0.21531797, -0.21163967,\n",
       "        -0.21623813, -0.24805663, -0.24570568, -0.25821574, -0.21658273,\n",
       "        -0.21755136, -0.2171062 , -0.21362727, -0.21941208, -0.21053997,\n",
       "        -0.21033074, -0.21558029, -0.21335226]),\n",
       " 'split2_test_neg_log_loss': array([-0.18788651, -0.19949063, -0.22011704, -0.22551916, -0.20041223,\n",
       "        -0.20688647, -0.25568204, -0.2094533 , -0.21943829, -0.26901302,\n",
       "        -0.28668235, -0.23079335, -0.22095929, -0.20190693, -0.19750135,\n",
       "        -0.20905462, -0.21708412, -0.22638655, -0.23528863, -0.23067933,\n",
       "        -0.23015399, -0.26444567, -0.20925431, -0.25190039, -0.20954304,\n",
       "        -0.21234414, -0.19440575, -0.20790963, -0.19846011, -0.22680199,\n",
       "        -0.23347333, -0.24135965, -0.22019448, -0.24516671, -0.27336717,\n",
       "        -0.21968772, -0.20897958, -0.20422101, -0.20529633, -0.19576583,\n",
       "        -0.19969476, -0.1988715 , -0.20471598, -0.19137629, -0.19400974,\n",
       "        -0.20177923, -0.20389362, -0.19747915, -0.21871666, -0.2062646 ,\n",
       "        -0.19497175, -0.18926513, -0.20367338, -0.18806118, -0.23750829,\n",
       "        -0.2442464 , -0.21001087, -0.25480986, -0.26673991, -0.23531275,\n",
       "        -0.19098473, -0.19911828, -0.22112869, -0.20857473, -0.19196661,\n",
       "        -0.21907326, -0.21645987, -0.23599821, -0.26973047, -0.21499567,\n",
       "        -0.29242802, -0.24945157, -0.2094666 , -0.20104563, -0.21685417,\n",
       "        -0.1792285 , -0.19656808, -0.20231294, -0.21591759, -0.27181858,\n",
       "        -0.21114968, -0.22902543, -0.28220239, -0.29367267, -0.19183093,\n",
       "        -0.19456912, -0.20123081, -0.19600916, -0.19995429, -0.19638406,\n",
       "        -0.20704922, -0.20552448, -0.21033889, -0.2350899 , -0.2256334 ,\n",
       "        -0.22376369, -0.81650424, -1.8844784 , -0.88846966, -0.66001791,\n",
       "        -1.94731733, -0.70410131, -0.57838594, -1.96809277, -0.57666816,\n",
       "        -0.55516016, -1.91596172, -0.52772788, -0.81521581, -1.99404637,\n",
       "        -0.76518733, -0.65179734, -1.94129682, -0.68321184, -0.59800349,\n",
       "        -2.06310628, -0.58659478, -0.53143616, -1.86507789, -0.53304916,\n",
       "        -0.84008931, -2.03361184, -0.83879516, -0.65093686, -1.92075059,\n",
       "        -0.6740543 , -0.5819813 , -2.00109941, -0.60009403, -0.56350655,\n",
       "        -1.93225712, -0.54666912, -0.83918011, -1.98027577, -0.81145565,\n",
       "        -0.68468684, -1.92735374, -0.67393172, -0.58353693, -2.05084672,\n",
       "        -0.61287914, -0.55907787, -1.93877148, -0.57293604, -0.39098023,\n",
       "        -2.12373285, -0.37404715, -0.30871044, -1.91728088, -0.27766828,\n",
       "        -0.26900609, -1.49438367, -0.26297572, -0.2548267 , -1.61350032,\n",
       "        -0.26160264, -0.36932674, -1.70730456, -0.33555136, -0.28956909,\n",
       "        -1.7008573 , -0.28249986, -0.26407813, -1.79565668, -0.26789692,\n",
       "        -0.25844219, -1.65016677, -0.25703791, -0.32707843, -1.90954679,\n",
       "        -0.37090166, -0.28117487, -1.61706095, -0.29131642, -0.25972138,\n",
       "        -1.8757564 , -0.26336363, -0.26204032, -1.73145892, -0.25537174,\n",
       "        -0.3906821 , -1.81421529, -0.32953825, -0.26761361, -1.76011426,\n",
       "        -0.26954313, -0.26434703, -1.62156201, -0.27426822, -0.2571178 ,\n",
       "        -1.71347949, -0.2605389 , -0.37176475, -0.37186175, -0.37081004,\n",
       "        -0.24395799, -0.23051704, -0.23836715, -0.22239861, -0.22543754,\n",
       "        -0.22496508, -0.21951551, -0.22021743, -0.21869385, -0.29738872,\n",
       "        -0.36882834, -0.37307082, -0.24099957, -0.23812689, -0.24470546,\n",
       "        -0.2237734 , -0.22901616, -0.22342806, -0.21985033, -0.22051169,\n",
       "        -0.22027743, -0.3331179 , -0.38987535, -0.30751512, -0.25072435,\n",
       "        -0.2332599 , -0.24673235, -0.2284844 , -0.22954083, -0.22674806,\n",
       "        -0.2209373 , -0.22458387, -0.22175698, -0.3154829 , -0.30722568,\n",
       "        -0.43613383, -0.25663587, -0.25450907, -0.24866801, -0.23819633,\n",
       "        -0.23306045, -0.23242638, -0.23159575, -0.23091808, -0.23223094,\n",
       "        -0.31231945, -0.23484061, -0.23777691, -0.21066586, -0.21264853,\n",
       "        -0.21254533, -0.2063624 , -0.21128385, -0.21174615, -0.21402067,\n",
       "        -0.2044015 , -0.20561932, -0.24257398, -0.24718608, -0.23582033,\n",
       "        -0.2124701 , -0.2165095 , -0.22303637, -0.21112118, -0.206349  ,\n",
       "        -0.21021378, -0.21219021, -0.20690371, -0.20390037, -0.24988408,\n",
       "        -0.24311811, -0.241333  , -0.22223464, -0.21078293, -0.21528109,\n",
       "        -0.20797142, -0.21393405, -0.21877606, -0.21257572, -0.20196182,\n",
       "        -0.21025731, -0.26904674, -0.24371732, -0.23738777, -0.22609131,\n",
       "        -0.21241389, -0.21285711, -0.21601678, -0.2117546 , -0.20742901,\n",
       "        -0.22051173, -0.2103092 , -0.21096205]),\n",
       " 'split3_test_neg_log_loss': array([-0.1799077 , -0.18310127, -0.17009122, -0.17121766, -0.17457099,\n",
       "        -0.18455016, -0.2291717 , -0.19205915, -0.19861766, -0.20510558,\n",
       "        -0.22268473, -0.2174749 , -0.1714992 , -0.18701946, -0.17790783,\n",
       "        -0.16543877, -0.17074785, -0.172956  , -0.18870853, -0.20004819,\n",
       "        -0.18481411, -0.21793014, -0.21265362, -0.21058006, -0.17417189,\n",
       "        -0.17354127, -0.18545915, -0.17715736, -0.1677367 , -0.18190481,\n",
       "        -0.17143362, -0.19705699, -0.17723768, -0.25087446, -0.2079492 ,\n",
       "        -0.20806818, -0.17024382, -0.18164736, -0.19073382, -0.17398795,\n",
       "        -0.17171447, -0.17850993, -0.18983294, -0.16669912, -0.17008915,\n",
       "        -0.1751841 , -0.18626506, -0.18089347, -0.17255001, -0.16906719,\n",
       "        -0.17284177, -0.19101368, -0.17616054, -0.18239762, -0.22080266,\n",
       "        -0.21260404, -0.20183784, -0.23727567, -0.26162759, -0.23144608,\n",
       "        -0.17089242, -0.17716213, -0.1737109 , -0.19485755, -0.19618605,\n",
       "        -0.16983346, -0.19466206, -0.21176667, -0.21033643, -0.27010514,\n",
       "        -0.22334595, -0.22508139, -0.17716562, -0.16693518, -0.17539894,\n",
       "        -0.17637594, -0.17396869, -0.1758587 , -0.19566075, -0.23866048,\n",
       "        -0.22790013, -0.22326516, -0.22322524, -0.3101889 , -0.16998968,\n",
       "        -0.18143471, -0.17011583, -0.18366816, -0.16940395, -0.16608452,\n",
       "        -0.17038599, -0.15658695, -0.1912889 , -0.20080658, -0.19982906,\n",
       "        -0.23397828, -0.86763182, -1.94076362, -0.81289056, -0.68425316,\n",
       "        -1.86194478, -0.69275258, -0.59026865, -1.99755929, -0.57706844,\n",
       "        -0.55594003, -1.95919641, -0.52159156, -0.92041763, -2.08492546,\n",
       "        -0.74724886, -0.63186151, -1.98150213, -0.70032425, -0.60700104,\n",
       "        -1.92722849, -0.58587929, -0.53301895, -1.96916016, -0.54728118,\n",
       "        -0.84545431, -1.9287408 , -0.90891846, -0.68571432, -2.00232402,\n",
       "        -0.63773112, -0.5830809 , -1.93176837, -0.59608448, -0.52966237,\n",
       "        -1.95497006, -0.53687113, -0.79561676, -2.00551357, -0.82869234,\n",
       "        -0.69711119, -1.92896936, -0.67562761, -0.59766569, -1.94065003,\n",
       "        -0.59881711, -0.5415383 , -2.01889637, -0.54039886, -0.35763491,\n",
       "        -2.41212842, -0.41305353, -0.27489128, -1.88914141, -0.27559174,\n",
       "        -0.24271101, -1.73974977, -0.25159881, -0.24052995, -1.60078974,\n",
       "        -0.2425765 , -0.37280374, -1.855393  , -0.40141065, -0.27529833,\n",
       "        -1.66142822, -0.2863157 , -0.24791742, -1.85873537, -0.24934858,\n",
       "        -0.23323658, -1.78398799, -0.23834296, -0.35106585, -1.72413913,\n",
       "        -0.3764793 , -0.26121491, -1.90256537, -0.27610472, -0.24224289,\n",
       "        -1.87757854, -0.2494651 , -0.24077123, -1.47957436, -0.23659707,\n",
       "        -0.35715264, -1.95867639, -0.34259012, -0.31197477, -1.71910001,\n",
       "        -0.26372035, -0.25165882, -1.77721522, -0.24331967, -0.24335927,\n",
       "        -1.90165865, -0.23790413, -0.36692757, -0.28265711, -0.3511136 ,\n",
       "        -0.219953  , -0.21393716, -0.23243406, -0.20370544, -0.20508643,\n",
       "        -0.21306513, -0.1977116 , -0.19759139, -0.19847434, -0.39088735,\n",
       "        -0.3932339 , -0.39540129, -0.21753851, -0.20891711, -0.21621149,\n",
       "        -0.20468556, -0.20671231, -0.20561064, -0.20113692, -0.19630174,\n",
       "        -0.19839274, -0.3102372 , -0.36204793, -0.33664949, -0.21754762,\n",
       "        -0.22569658, -0.21973494, -0.20765109, -0.20727351, -0.20600833,\n",
       "        -0.19685623, -0.19934576, -0.19799274, -0.35313138, -0.28736639,\n",
       "        -0.38194984, -0.24379919, -0.23284964, -0.23514534, -0.22037056,\n",
       "        -0.21448706, -0.21808874, -0.21387875, -0.21361719, -0.21028432,\n",
       "        -0.22225626, -0.23654314, -0.2225801 , -0.19054487, -0.19396827,\n",
       "        -0.19167208, -0.18394311, -0.18940071, -0.1903455 , -0.18786433,\n",
       "        -0.18943766, -0.18509575, -0.22228916, -0.23774582, -0.24657454,\n",
       "        -0.19580301, -0.19221202, -0.19859783, -0.18941181, -0.1891621 ,\n",
       "        -0.19084625, -0.18957999, -0.18407884, -0.19068196, -0.2294502 ,\n",
       "        -0.22923432, -0.22113664, -0.1929852 , -0.19359987, -0.19889543,\n",
       "        -0.19255719, -0.19462592, -0.19110529, -0.18438136, -0.18808095,\n",
       "        -0.18721043, -0.23845165, -0.23310805, -0.22706624, -0.19696252,\n",
       "        -0.19401328, -0.1947167 , -0.19324686, -0.18911434, -0.19588633,\n",
       "        -0.19069559, -0.19103078, -0.1892228 ]),\n",
       " 'split4_test_neg_log_loss': array([-0.17883577, -0.17748623, -0.17547042, -0.20442419, -0.18211722,\n",
       "        -0.18914247, -0.21624101, -0.20348394, -0.17983604, -0.28576586,\n",
       "        -0.24405895, -0.22191675, -0.17896818, -0.17872244, -0.18890604,\n",
       "        -0.18974334, -0.20500687, -0.18884097, -0.1956485 , -0.21105885,\n",
       "        -0.21968604, -0.25417394, -0.26951535, -0.23634688, -0.17739589,\n",
       "        -0.18128356, -0.18369976, -0.17418778, -0.18872887, -0.17673593,\n",
       "        -0.18985133, -0.20554541, -0.20520524, -0.22773046, -0.21047166,\n",
       "        -0.21927157, -0.18063799, -0.18100816, -0.1889027 , -0.17778732,\n",
       "        -0.17478626, -0.17516354, -0.17474432, -0.17223747, -0.18703198,\n",
       "        -0.1834602 , -0.17451974, -0.18534314, -0.18027744, -0.18127614,\n",
       "        -0.17842484, -0.16882153, -0.19461103, -0.20880529, -0.20308836,\n",
       "        -0.19933591, -0.26572937, -0.2578021 , -0.30910612, -0.32740213,\n",
       "        -0.18632419, -0.19279985, -0.19826865, -0.18097816, -0.20049683,\n",
       "        -0.16918074, -0.26364004, -0.20815754, -0.21758762, -0.27776524,\n",
       "        -0.22702901, -0.2772836 , -0.18873393, -0.16608244, -0.19766707,\n",
       "        -0.19822595, -0.17606188, -0.17342188, -0.1953116 , -0.18898886,\n",
       "        -0.22154877, -0.31958598, -0.27136265, -0.21923802, -0.18714476,\n",
       "        -0.18895031, -0.17755549, -0.18094067, -0.18365512, -0.19865392,\n",
       "        -0.1922924 , -0.20016365, -0.18181285, -0.23404686, -0.20637173,\n",
       "        -0.23179747, -0.87169412, -2.09256687, -0.89276511, -0.66778183,\n",
       "        -1.89775166, -0.64204428, -0.57862272, -1.88219611, -0.5968594 ,\n",
       "        -0.52632026, -1.89402349, -0.52899004, -0.83784704, -1.94921186,\n",
       "        -0.84621821, -0.66857327, -2.13490735, -0.65532148, -0.57830494,\n",
       "        -1.87520999, -0.59104164, -0.52603676, -1.89601857, -0.53645577,\n",
       "        -0.77161033, -1.9595337 , -0.86646102, -0.62740309, -2.07346742,\n",
       "        -0.67072105, -0.59989029, -1.916115  , -0.55921734, -0.56161879,\n",
       "        -1.95964506, -0.51581705, -0.81724533, -1.9278844 , -0.83804739,\n",
       "        -0.65454557, -1.89401166, -0.65261763, -0.59445684, -1.92517541,\n",
       "        -0.57111844, -0.54741485, -1.87796655, -0.53212773, -0.3669852 ,\n",
       "        -2.12941042, -0.41130677, -0.26754404, -1.92852616, -0.26057101,\n",
       "        -0.23975109, -1.52718433, -0.23685954, -0.23349698, -1.62819109,\n",
       "        -0.23641459, -0.36910386, -2.42195996, -0.3853584 , -0.26402288,\n",
       "        -1.85110508, -0.25353876, -0.2463017 , -2.18721095, -0.2443913 ,\n",
       "        -0.2303223 , -1.7486821 , -0.22788719, -0.36117287, -2.3769684 ,\n",
       "        -0.30915911, -0.2584472 , -2.0798431 , -0.25932488, -0.24520221,\n",
       "        -1.4445523 , -0.25146338, -0.23770609, -1.74488412, -0.23714392,\n",
       "        -0.46157482, -1.9467251 , -0.40596331, -0.27932362, -1.95675244,\n",
       "        -0.25876622, -0.24885669, -1.65330569, -0.24823034, -0.24009272,\n",
       "        -1.735723  , -0.23926037, -0.34160618, -0.29576452, -0.32784938,\n",
       "        -0.22277143, -0.2225145 , -0.22181714, -0.20545704, -0.2071128 ,\n",
       "        -0.20675972, -0.20107367, -0.19764448, -0.20267037, -0.35739454,\n",
       "        -0.34164963, -0.34547164, -0.22383619, -0.22062084, -0.21279698,\n",
       "        -0.20284947, -0.20943326, -0.20783798, -0.19883328, -0.20047117,\n",
       "        -0.19965506, -0.36252109, -0.32896118, -0.3320324 , -0.2196152 ,\n",
       "        -0.21817751, -0.22647804, -0.20699365, -0.203312  , -0.21055538,\n",
       "        -0.19900065, -0.20030352, -0.20227591, -0.35460519, -0.36474199,\n",
       "        -0.41516086, -0.22637855, -0.22908027, -0.23835899, -0.22061452,\n",
       "        -0.21835506, -0.21615721, -0.21119879, -0.21083609, -0.21003622,\n",
       "        -0.22777659, -0.22657835, -0.23729518, -0.19835829, -0.20280159,\n",
       "        -0.19397133, -0.19181774, -0.19221094, -0.19674139, -0.18730703,\n",
       "        -0.18790931, -0.18747712, -0.23274408, -0.21853496, -0.21871912,\n",
       "        -0.18941927, -0.19815241, -0.19819678, -0.1897883 , -0.19292621,\n",
       "        -0.18978099, -0.18907301, -0.18674431, -0.1867459 , -0.22991847,\n",
       "        -0.2240838 , -0.2225355 , -0.20029547, -0.19357896, -0.19631028,\n",
       "        -0.18809094, -0.19510934, -0.19154539, -0.19304217, -0.18831175,\n",
       "        -0.18997244, -0.23041215, -0.23555933, -0.23266574, -0.20315623,\n",
       "        -0.19801859, -0.19972896, -0.19739186, -0.19571667, -0.19139208,\n",
       "        -0.19166614, -0.19104087, -0.19236156]),\n",
       " 'mean_test_neg_log_loss': array([-0.19275416, -0.19656573, -0.19164314, -0.20559128, -0.19863487,\n",
       "        -0.20334041, -0.23469107, -0.21678918, -0.22158323, -0.2597528 ,\n",
       "        -0.26287776, -0.24455303, -0.19775133, -0.19481574, -0.19898069,\n",
       "        -0.20445692, -0.20149632, -0.20592651, -0.20964133, -0.22410563,\n",
       "        -0.21260527, -0.2547657 , -0.24979113, -0.25626422, -0.19594807,\n",
       "        -0.19570859, -0.19261287, -0.20006433, -0.19561558, -0.20972298,\n",
       "        -0.21083838, -0.22150976, -0.21934376, -0.23664732, -0.23741305,\n",
       "        -0.23291342, -0.19166402, -0.19180795, -0.1995128 , -0.1928491 ,\n",
       "        -0.18928488, -0.19177012, -0.19568581, -0.18642496, -0.1967631 ,\n",
       "        -0.19744236, -0.20222994, -0.1988559 , -0.19794227, -0.19426152,\n",
       "        -0.19406494, -0.1954916 , -0.20692236, -0.20544467, -0.23132707,\n",
       "        -0.23782019, -0.24972006, -0.26416484, -0.29762236, -0.26803321,\n",
       "        -0.19123287, -0.19328023, -0.20109102, -0.21125938, -0.20282047,\n",
       "        -0.2019319 , -0.23933222, -0.22906289, -0.23927008, -0.27420916,\n",
       "        -0.25695174, -0.28035484, -0.19603677, -0.1900084 , -0.20034492,\n",
       "        -0.19400886, -0.19454599, -0.19734814, -0.21838732, -0.2404851 ,\n",
       "        -0.2198871 , -0.27587534, -0.2707627 , -0.27465768, -0.1877649 ,\n",
       "        -0.19577731, -0.19280024, -0.19529677, -0.18995301, -0.19861538,\n",
       "        -0.20316102, -0.19972484, -0.20821961, -0.232053  , -0.22159838,\n",
       "        -0.2253684 , -0.85231289, -2.01018426, -0.86559532, -0.66731133,\n",
       "        -1.89949144, -0.68420983, -0.58014115, -1.96124997, -0.58635727,\n",
       "        -0.54110155, -1.93129643, -0.52479254, -0.91400581, -1.99956758,\n",
       "        -0.80729731, -0.66064878, -1.99544954, -0.67157585, -0.59597185,\n",
       "        -1.94810904, -0.58326833, -0.53367714, -1.92386202, -0.53215936,\n",
       "        -0.84474218, -1.97023271, -0.85702244, -0.66273451, -1.9818167 ,\n",
       "        -0.65046796, -0.58226455, -1.95201032, -0.58564881, -0.5452891 ,\n",
       "        -1.93954049, -0.53299911, -0.81895158, -1.98585473, -0.8379842 ,\n",
       "        -0.67258885, -1.94363134, -0.67189885, -0.59342837, -1.94801607,\n",
       "        -0.59323112, -0.54773148, -1.93641139, -0.54583764, -0.37420609,\n",
       "        -2.04889374, -0.37748203, -0.27861159, -1.82335848, -0.27464841,\n",
       "        -0.25582879, -1.56350514, -0.25349484, -0.2449764 , -1.66163492,\n",
       "        -0.24712303, -0.36637399, -2.0009347 , -0.36539915, -0.2720936 ,\n",
       "        -1.9038007 , -0.27692982, -0.25396197, -1.8939611 , -0.25648444,\n",
       "        -0.24313576, -1.71860577, -0.24446024, -0.36164301, -1.94018424,\n",
       "        -0.36217578, -0.27105274, -1.92807929, -0.2751133 , -0.25416123,\n",
       "        -1.84638769, -0.25551373, -0.24548841, -1.69070902, -0.24346678,\n",
       "        -0.37540548, -1.97232111, -0.36178407, -0.28107034, -1.83705359,\n",
       "        -0.26741548, -0.25679622, -1.68784703, -0.25779563, -0.25003259,\n",
       "        -1.72539717, -0.24841142, -0.35588646, -0.33671553, -0.34736349,\n",
       "        -0.23706651, -0.22803015, -0.23223106, -0.21432272, -0.21420552,\n",
       "        -0.21640989, -0.20925418, -0.20730804, -0.20825957, -0.35207273,\n",
       "        -0.37546523, -0.35197596, -0.22761017, -0.22816308, -0.2303937 ,\n",
       "        -0.21158708, -0.21750705, -0.21589621, -0.20987747, -0.20924422,\n",
       "        -0.20929956, -0.33372237, -0.36281341, -0.32917659, -0.23113086,\n",
       "        -0.23064916, -0.22981455, -0.21571144, -0.21628631, -0.21584848,\n",
       "        -0.20871677, -0.21112602, -0.21052126, -0.34720176, -0.33662293,\n",
       "        -0.37791316, -0.24626889, -0.24140698, -0.2415313 , -0.22690517,\n",
       "        -0.22510305, -0.22373698, -0.22009219, -0.21987835, -0.21840153,\n",
       "        -0.25179694, -0.23669131, -0.23895735, -0.20486114, -0.20589609,\n",
       "        -0.20683896, -0.19963385, -0.20222625, -0.20271082, -0.20037902,\n",
       "        -0.19854056, -0.19887571, -0.24069578, -0.23341284, -0.23756478,\n",
       "        -0.20480456, -0.20491575, -0.2075953 , -0.20278156, -0.20157659,\n",
       "        -0.19960351, -0.20049898, -0.19884021, -0.19884788, -0.23826233,\n",
       "        -0.2313955 , -0.23388372, -0.20764198, -0.20635094, -0.20724327,\n",
       "        -0.20135254, -0.20408802, -0.20396576, -0.20150559, -0.19797877,\n",
       "        -0.20019323, -0.24451931, -0.23600681, -0.23936621, -0.20970592,\n",
       "        -0.2074335 , -0.20603644, -0.20521739, -0.20446177, -0.20140762,\n",
       "        -0.20225457, -0.20160981, -0.20108444]),\n",
       " 'std_test_neg_log_loss': array([0.01351648, 0.0146954 , 0.01784069, 0.01844952, 0.02303282,\n",
       "        0.01451302, 0.0181783 , 0.02110292, 0.03088242, 0.02973266,\n",
       "        0.02573254, 0.02631059, 0.01954369, 0.01160624, 0.01526254,\n",
       "        0.02436736, 0.01607179, 0.0233187 , 0.01695774, 0.01586359,\n",
       "        0.01892725, 0.02296804, 0.03372921, 0.03147766, 0.01676172,\n",
       "        0.01579744, 0.00815217, 0.02069052, 0.01640252, 0.02799673,\n",
       "        0.02580167, 0.01791022, 0.02670318, 0.01072727, 0.02488686,\n",
       "        0.02627836, 0.01418437, 0.00906089, 0.00794824, 0.01519058,\n",
       "        0.01313992, 0.01234253, 0.01222291, 0.01454054, 0.01792929,\n",
       "        0.01593347, 0.0203049 , 0.01666234, 0.01814312, 0.01654795,\n",
       "        0.01743052, 0.01836111, 0.02130177, 0.02459291, 0.01776188,\n",
       "        0.02853325, 0.03908906, 0.02413619, 0.04023621, 0.03524585,\n",
       "        0.01390391, 0.00848605, 0.01579938, 0.02342515, 0.00853696,\n",
       "        0.02774928, 0.02908937, 0.01783551, 0.02298029, 0.03343785,\n",
       "        0.02803338, 0.04536531, 0.01159693, 0.01970839, 0.01395819,\n",
       "        0.01393492, 0.01740477, 0.01937551, 0.02268378, 0.02785826,\n",
       "        0.00702837, 0.04106005, 0.02520088, 0.03616252, 0.0099571 ,\n",
       "        0.01022704, 0.01635961, 0.01423498, 0.01219926, 0.01832396,\n",
       "        0.02437421, 0.02325093, 0.0192548 , 0.01758399, 0.01886627,\n",
       "        0.00651921, 0.0201696 , 0.09858066, 0.03618971, 0.01089189,\n",
       "        0.0389993 , 0.0264394 , 0.00603573, 0.04115288, 0.0150655 ,\n",
       "        0.01350806, 0.02374538, 0.00740239, 0.0835456 , 0.05475569,\n",
       "        0.04642575, 0.01919291, 0.07637071, 0.02019486, 0.0149113 ,\n",
       "        0.0624736 , 0.00768294, 0.00493014, 0.04535728, 0.01078124,\n",
       "        0.04130834, 0.03812185, 0.03168793, 0.0281641 , 0.05289233,\n",
       "        0.01896666, 0.00976732, 0.03558321, 0.01429829, 0.01713082,\n",
       "        0.01527835, 0.01461696, 0.01631189, 0.04727798, 0.02398497,\n",
       "        0.01578825, 0.03517922, 0.01625958, 0.00597174, 0.05705284,\n",
       "        0.01354383, 0.00752733, 0.04671259, 0.01434847, 0.02871399,\n",
       "        0.29593999, 0.03592517, 0.01596504, 0.11265722, 0.00995692,\n",
       "        0.01305499, 0.08986869, 0.00918012, 0.00762712, 0.12338907,\n",
       "        0.00863922, 0.00761271, 0.24648239, 0.02753242, 0.01026964,\n",
       "        0.21462587, 0.01215805, 0.00673985, 0.18695267, 0.00850416,\n",
       "        0.01030552, 0.12445438, 0.01021435, 0.03952702, 0.25528112,\n",
       "        0.02729859, 0.00931805, 0.19914639, 0.01140076, 0.00949689,\n",
       "        0.22457171, 0.00502269, 0.00864295, 0.11922802, 0.00687149,\n",
       "        0.04987055, 0.09818408, 0.03073856, 0.01626479, 0.1636132 ,\n",
       "        0.00538861, 0.00785567, 0.05820742, 0.01096752, 0.00709365,\n",
       "        0.15293801, 0.00867073, 0.01166196, 0.04020416, 0.02185899,\n",
       "        0.01417624, 0.00919556, 0.00619989, 0.00833965, 0.00836745,\n",
       "        0.00655644, 0.00910255, 0.0096095 , 0.00783098, 0.03089988,\n",
       "        0.02084811, 0.02975591, 0.00894904, 0.01165044, 0.01391782,\n",
       "        0.00792965, 0.00849075, 0.00826325, 0.00916297, 0.01017799,\n",
       "        0.00967833, 0.01763673, 0.02311221, 0.02130628, 0.01260191,\n",
       "        0.0115271 , 0.0100585 , 0.00855768, 0.01028274, 0.00843027,\n",
       "        0.01001463, 0.01008113, 0.00945577, 0.0200557 , 0.03276003,\n",
       "        0.0446996 , 0.0131116 , 0.00926817, 0.00457623, 0.00649591,\n",
       "        0.00737244, 0.00656899, 0.00716835, 0.00729982, 0.00823285,\n",
       "        0.03205422, 0.00603259, 0.00956487, 0.00993424, 0.00763474,\n",
       "        0.01185788, 0.01075948, 0.01032278, 0.00840233, 0.01165755,\n",
       "        0.00849904, 0.01093806, 0.01895273, 0.010576  , 0.01207676,\n",
       "        0.01041576, 0.00935936, 0.01086149, 0.01198482, 0.0094006 ,\n",
       "        0.0093497 , 0.00990711, 0.012265  , 0.00978321, 0.00772982,\n",
       "        0.0070942 , 0.01045731, 0.01070856, 0.01188988, 0.0093293 ,\n",
       "        0.0097392 , 0.00952146, 0.01132852, 0.01165071, 0.00891732,\n",
       "        0.01131533, 0.01350694, 0.00848405, 0.01058243, 0.01036164,\n",
       "        0.00954754, 0.00821037, 0.00885697, 0.01088244, 0.00707954,\n",
       "        0.01150474, 0.00996783, 0.00966737]),\n",
       " 'rank_test_neg_log_loss': array([ 12,  29,   7,  79,  38,  69, 145, 114, 123, 187, 188, 166,  33,\n",
       "         20,  43,  72,  57,  81,  97, 126, 106, 179, 173, 182,  27,  25,\n",
       "         11,  48,  23,  99, 102, 122, 118, 147, 150, 142,   8,  10,  44,\n",
       "         14,   3,   9,  24,   1,  30,  32,  63,  41,  34,  18,  17,  22,\n",
       "         85,  78, 138, 152, 172, 189, 204, 191,   6,  15,  54, 104,  67,\n",
       "         61, 156, 133, 155, 195, 185, 202,  28,   5,  50,  16,  19,  31,\n",
       "        116, 158, 120, 199, 192, 197,   2,  26,  13,  21,   4,  37,  68,\n",
       "         47,  91, 140, 124, 128, 253, 287, 255, 244, 267, 248, 233, 279,\n",
       "        237, 229, 271, 225, 256, 285, 249, 242, 284, 245, 240, 277, 235,\n",
       "        228, 269, 226, 252, 280, 254, 243, 282, 241, 234, 278, 236, 230,\n",
       "        273, 227, 250, 283, 251, 247, 275, 246, 239, 276, 238, 232, 272,\n",
       "        231, 220, 288, 223, 201, 263, 196, 181, 257, 176, 167, 258, 170,\n",
       "        219, 286, 218, 194, 268, 200, 177, 266, 183, 162, 261, 164, 214,\n",
       "        274, 216, 193, 270, 198, 178, 265, 180, 168, 260, 163, 221, 281,\n",
       "        215, 203, 264, 190, 184, 259, 186, 174, 262, 171, 213, 208, 210,\n",
       "        149, 131, 141, 108, 107, 113,  95,  87,  92, 212, 222, 211, 130,\n",
       "        132, 135, 105, 115, 111, 100,  94,  96, 206, 217, 205, 137, 136,\n",
       "        134, 109, 112, 110,  93, 103, 101, 209, 207, 224, 169, 160, 161,\n",
       "        129, 127, 125, 121, 119, 117, 175, 148, 154,  75,  80,  84,  46,\n",
       "         62,  65,  51,  36,  42, 159, 143, 151,  74,  76,  89,  66,  59,\n",
       "         45,  52,  39,  40, 153, 139, 144,  90,  83,  86,  55,  71,  70,\n",
       "         58,  35,  49, 165, 146, 157,  98,  88,  82,  77,  73,  56,  64,\n",
       "         60,  53])}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRIAL_4_MLP.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FOUR NEG LOG LOSS RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.1,\n",
       " 'classifier__hidden_layer_sizes': (15,),\n",
       " 'classifier__learning_rate': 'invscaling',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL FOUR NEG LOG LOSS RESULTS ---------------------------------------------\")\n",
    "TRIAL_4_MLP.cv_results_['rank_test_neg_log_loss']\n",
    "TRIAL_4_MLP.cv_results_['params'][ np.argmin(TRIAL_4_MLP.cv_results_['rank_test_neg_log_loss']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FOUR F1 RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'tanh',\n",
       " 'classifier__alpha': 0.0001,\n",
       " 'classifier__hidden_layer_sizes': (15,),\n",
       " 'classifier__learning_rate': 'constant',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL FOUR F1 RESULTS ---------------------------------------------\")\n",
    "TRIAL_4_MLP.cv_results_['rank_test_f1_micro']\n",
    "TRIAL_4_MLP.cv_results_['params'][ np.argmin(TRIAL_4_MLP.cv_results_['rank_test_f1_micro']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FOUR ROC AUC RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'tanh',\n",
       " 'classifier__alpha': 0.01,\n",
       " 'classifier__hidden_layer_sizes': (5,),\n",
       " 'classifier__learning_rate': 'invscaling',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL FOUR ROC AUC RESULTS ---------------------------------------------\")\n",
    "TRIAL_4_MLP.cv_results_['rank_test_roc_auc_ovo']\n",
    "TRIAL_4_MLP.cv_results_['params'][ np.argmin(TRIAL_4_MLP.cv_results_['rank_test_roc_auc_ovo']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 4 MPL using best NEG LOG LOSS hyperparameters :0.9259475218658892\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 4 MPL using best F1 hyperparameters :0.923731778425656\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 4 MPL using best ROC_AUC hyperparameters :0.9290962099125365\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best NEG LOG LOSS results to train on test set, \n",
    "p = TRIAL_4_MLP.cv_results_['params'][ np.argmin(TRIAL_4_MLP.cv_results_['rank_test_neg_log_loss']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train__neg_log_loss = pipe.predict(X_train)\n",
    "y_pred_test_neg_log_loss = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 4 MPL using best NEG LOG LOSS hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_neg_log_loss)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best F1 results to train on test set, LR_Trial 1\n",
    "p = TRIAL_4_MLP.cv_results_['params'][ np.argmin(TRIAL_4_MLP.cv_results_['rank_test_f1_micro']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train_roc_auc = pipe.predict(X_train)\n",
    "y_pred_test_f1 = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 4 MPL using best F1 hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_f1)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best ROC_AUC results to train on test set, LR_Trial 1\n",
    "p  = TRIAL_4_MLP.cv_results_['params'][ np.argmin(TRIAL_4_MLP.cv_results_['rank_test_roc_auc_ovo']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train_roc_auc = pipe.predict(X_train)\n",
    "y_pred_test_roc_auc = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 4 MPL using best ROC_AUC hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_roc_auc)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTI LAYERED PERCEPTRON TRIAL FIVE ON GAMMA PARTICLES DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    8.5s\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:   11.5s\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:   14.5s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:   16.0s\n",
      "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed:   17.9s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   21.1s\n",
      "[Parallel(n_jobs=-1)]: Done 189 tasks      | elapsed:   22.8s\n",
      "[Parallel(n_jobs=-1)]: Done 210 tasks      | elapsed:   24.7s\n",
      "[Parallel(n_jobs=-1)]: Done 233 tasks      | elapsed:   28.8s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:   31.2s\n",
      "[Parallel(n_jobs=-1)]: Done 281 tasks      | elapsed:   35.5s\n",
      "[Parallel(n_jobs=-1)]: Done 306 tasks      | elapsed:   39.4s\n",
      "[Parallel(n_jobs=-1)]: Done 333 tasks      | elapsed:   43.7s\n",
      "[Parallel(n_jobs=-1)]: Done 360 tasks      | elapsed:   47.7s\n",
      "[Parallel(n_jobs=-1)]: Done 389 tasks      | elapsed:   51.2s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:   56.4s\n",
      "[Parallel(n_jobs=-1)]: Done 449 tasks      | elapsed:   59.9s\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 513 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 546 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 581 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 616 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 653 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 690 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 729 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 809 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 850 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 893 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 936 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 981 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1026 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1073 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1120 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1169 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1269 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1320 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1373 tasks      | elapsed:  4.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FIVE RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1440 out of 1440 | elapsed:  5.1min finished\n"
     ]
    }
   ],
   "source": [
    "ITERATIONS = 1\n",
    "for i in range(1):\n",
    "    #For reach \"trial\", randomly select 5000 data points\n",
    "    random5000DataPoints = beanData.sample(n = 5000, replace = True)\n",
    "    #From that 5000, create x set and y set\n",
    "    xSet = random5000DataPoints.iloc[:,0:15].values\n",
    "    ySet = random5000DataPoints.iloc[:,16].values\n",
    "    ySet = encodeLabel.fit_transform(ySet)\n",
    "    # Run GridSearch to find best hyperparameters per 5000 data points \n",
    "    \n",
    "    pipe = Pipeline([('std', StandardScaler()),('classifier', MLPClassifier(max_iter = 250))])\n",
    "\n",
    "    search_space = [\n",
    "                {\n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                {\n",
    "                 \n",
    "                  \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['sgd'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                \n",
    "                {\n",
    "                \n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['adam'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                ]\n",
    "    \n",
    "    #'precision_micro','roc_auc_ovo','jaccard','neg_log_loss'] JACCARD DOES NOT WORK FOR NON BINARY?\n",
    "    runHyperParamSearch = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=5), \n",
    "                          scoring=['recall_micro','f1_micro','roc_auc_ovo','neg_log_loss'], \n",
    "                          refit=False, verbose=10, n_jobs = -1)\n",
    "    TRIAL_5_MLP = runHyperParamSearch.fit(xSet, ySet)\n",
    "    print(\"---------------------------------TRIAL FIVE RESULTS ---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([1.21684723, 1.25227909, 1.25347939, 1.50179291, 1.55343671,\n",
       "        1.5529356 , 2.02694464, 2.15235062, 2.17707143, 2.45360851,\n",
       "        2.33360786, 2.37213755, 1.44984679, 1.31272578, 1.29201016,\n",
       "        1.55793953, 1.64901829, 1.666434  , 2.13584142, 2.13053346,\n",
       "        2.39155626, 2.49014101, 2.43489208, 2.41147194, 1.40310531,\n",
       "        1.37548246, 1.3082252 , 1.49648886, 1.50589585, 1.58596368,\n",
       "        1.99551716, 2.00642529, 2.04445872, 2.32219677, 2.31148658,\n",
       "        2.23662348, 1.3788846 , 1.31863365, 1.27639723, 1.56564817,\n",
       "        1.57045193, 1.56905155, 2.0580708 , 2.23362141, 2.13207164,\n",
       "        2.45901585, 2.5189662 , 2.54558859, 1.6284996 , 1.54302692,\n",
       "        1.42102237, 1.78453407, 1.88722267, 1.84668922, 2.45741334,\n",
       "        2.84004278, 2.99197359, 3.21776671, 3.29363251, 3.28652601,\n",
       "        1.92955933, 1.67303839, 1.58736477, 1.98730874, 1.99331546,\n",
       "        1.99381485, 2.66579094, 2.55810022, 2.68080549, 2.93952756,\n",
       "        2.86826625, 2.95414004, 1.75961289, 1.55483727, 1.66643338,\n",
       "        1.94227057, 1.95898509, 2.06627693, 2.37974916, 2.54398761,\n",
       "        2.54769168, 2.97866035, 2.88097682, 2.85265164, 1.66573167,\n",
       "        1.53702183, 1.52460957, 2.02934604, 1.81496119, 1.80064902,\n",
       "        2.27495861, 2.37954764, 2.38685479, 2.64427304, 2.57631521,\n",
       "        2.59643221, 3.44936757, 0.34969931, 3.32495871, 3.52723336,\n",
       "        0.46690168, 3.62111416, 3.84320502, 1.02167835, 3.86712561,\n",
       "        4.1375577 , 1.55213475, 4.22543378, 3.43945737, 0.3114677 ,\n",
       "        3.26140451, 3.56896925, 0.9536202 , 3.47658939, 3.781252  ,\n",
       "        0.83912177, 3.83479786, 4.05188456, 2.07368336, 4.08441181,\n",
       "        3.45447097, 0.42786794, 3.62511735, 3.85021124, 0.99205332,\n",
       "        3.79256158, 4.28958945, 0.92979951, 4.4051887 , 4.44842577,\n",
       "        1.58716493, 4.33773031, 3.6487381 , 0.4391777 , 3.49450498,\n",
       "        3.67305865, 0.52535157, 3.6911747 , 4.05458694, 0.78957934,\n",
       "        4.13725772, 4.4481245 , 1.51940646, 4.35904899, 3.7704423 ,\n",
       "        3.25089583, 3.95299935, 4.23143902, 4.37486253, 4.13395529,\n",
       "        4.65790601, 4.53480029, 4.65340223, 4.67742257, 4.87289114,\n",
       "        4.83856106, 3.5854836 , 3.06503577, 3.5095181 , 3.88654232,\n",
       "        3.97782073, 3.97021418, 4.36425347, 4.26726999, 4.40568919,\n",
       "        4.42910919, 4.35074172, 4.31521015, 3.39061551, 3.41313539,\n",
       "        3.59288964, 3.92307353, 3.92017164, 3.87943606, 4.43071117,\n",
       "        4.32401867, 4.25636034, 4.72216091, 4.43741593, 4.63288407,\n",
       "        3.57517452, 3.26210537, 3.49660759, 3.84991078, 3.90125556,\n",
       "        3.96440978, 4.49226346, 4.41849966, 4.4516284 , 4.67802296,\n",
       "        4.47304664, 4.41419573, 3.75142589, 3.60650182, 3.4774909 ,\n",
       "        3.89925294, 3.96450949, 4.03036709, 4.3659554 , 4.47534904,\n",
       "        4.36215148, 4.68202615, 4.59585218, 4.64039059, 3.68957324,\n",
       "        3.77064252, 3.69277577, 3.95790324, 4.10813303, 3.87943678,\n",
       "        4.16898541, 4.21222234, 4.41519718, 4.59324946, 4.56452522,\n",
       "        4.59925513, 3.85011086, 3.78145199, 3.77104297, 4.0097486 ,\n",
       "        4.03777213, 4.17248859, 4.4223032 , 4.35204258, 4.44652376,\n",
       "        4.53750224, 4.65800576, 4.63008094, 3.77034245, 3.7966651 ,\n",
       "        3.86672473, 4.17649193, 4.17048659, 4.17408953, 4.41649818,\n",
       "        4.61106572, 4.53049626, 4.67712216, 4.69733982, 4.62637849,\n",
       "        3.88108635, 3.79136009, 3.93278179, 4.23193913, 4.26797056,\n",
       "        4.23994651, 4.56492581, 4.62968178, 4.7654984 , 5.19246597,\n",
       "        4.91272459, 4.8749917 , 3.85121164, 3.90525799, 3.93808661,\n",
       "        4.15967717, 4.26126451, 4.2522573 , 4.76820126, 4.70174336,\n",
       "        4.86578431, 5.16554193, 4.99039183, 5.06875944, 3.85901885,\n",
       "        3.87533245, 3.72170081, 4.15612731, 4.16838531, 4.22963753,\n",
       "        4.58204045, 4.44622359, 4.4522296 , 4.86618505, 4.86200047,\n",
       "        4.98828936, 3.85491505, 3.78715687, 3.88834376, 3.95219889,\n",
       "        3.98782983, 4.16378093, 4.59915547, 4.75468907, 4.83175507,\n",
       "        4.96697068, 4.65830555, 4.39087543]),\n",
       " 'std_fit_time': array([0.03021128, 0.03047583, 0.02739184, 0.03237971, 0.03871396,\n",
       "        0.03299929, 0.10065921, 0.04245666, 0.04853712, 0.07282074,\n",
       "        0.08954431, 0.07280814, 0.06998084, 0.02192838, 0.01333877,\n",
       "        0.05760615, 0.06733706, 0.03565276, 0.09171638, 0.05991435,\n",
       "        0.17594802, 0.0987576 , 0.14414788, 0.05071681, 0.05090763,\n",
       "        0.07532121, 0.02878524, 0.02775688, 0.02337661, 0.0439543 ,\n",
       "        0.07109897, 0.0327151 , 0.04121621, 0.03488073, 0.08769057,\n",
       "        0.05301313, 0.06869817, 0.03032082, 0.01601558, 0.01170421,\n",
       "        0.05059512, 0.02247303, 0.09974548, 0.10105734, 0.12564559,\n",
       "        0.12130853, 0.16701534, 0.0876165 , 0.12708637, 0.05042313,\n",
       "        0.06319288, 0.03439237, 0.04697262, 0.02622982, 0.17287285,\n",
       "        0.21622337, 0.23926123, 0.44587931, 0.33687068, 0.46281236,\n",
       "        0.2729254 , 0.15014238, 0.18153576, 0.11611124, 0.17981869,\n",
       "        0.11184239, 0.18950886, 0.08292866, 0.17527428, 0.1112897 ,\n",
       "        0.11995015, 0.09954355, 0.12809803, 0.16806585, 0.17859378,\n",
       "        0.11887577, 0.0719255 , 0.11861026, 0.08418332, 0.22888292,\n",
       "        0.2277787 , 0.16852541, 0.12234125, 0.04994523, 0.16980109,\n",
       "        0.06689867, 0.15027526, 0.17754487, 0.11477808, 0.07767544,\n",
       "        0.08651514, 0.07038995, 0.08287748, 0.08722401, 0.05686445,\n",
       "        0.19427469, 0.16861939, 0.12901658, 0.13296848, 0.10043172,\n",
       "        0.16212483, 0.14683071, 0.06448812, 0.456856  , 0.06104527,\n",
       "        0.10072325, 0.75453888, 0.05028327, 0.06289784, 0.07566932,\n",
       "        0.0996524 , 0.13557326, 0.20506804, 0.06233612, 0.10402613,\n",
       "        0.27932082, 0.03996527, 0.08459448, 0.94079387, 0.12064529,\n",
       "        0.12413215, 0.21512918, 0.36045224, 0.36358276, 0.30285074,\n",
       "        0.2299388 , 0.29481416, 0.49928871, 0.528922  , 0.29739024,\n",
       "        0.33118738, 0.26832398, 0.22872524, 0.11055084, 0.23922553,\n",
       "        0.09057274, 0.12171287, 0.02233533, 0.18706857, 0.32717147,\n",
       "        0.12914626, 0.13740576, 0.26125588, 0.1120258 , 0.17112816,\n",
       "        0.43059153, 0.24536212, 0.23445963, 0.25980636, 0.32798614,\n",
       "        0.36340881, 0.10595609, 0.13060351, 0.25316128, 0.47985433,\n",
       "        0.32327853, 0.04513394, 0.3958134 , 0.07984725, 0.2007508 ,\n",
       "        0.16855787, 0.1799685 , 0.15387331, 0.15690142, 0.18105285,\n",
       "        0.15940228, 0.07829483, 0.11438047, 0.06391398, 0.1024169 ,\n",
       "        0.14615295, 0.13642967, 0.19644975, 0.09706018, 0.09978047,\n",
       "        0.11508908, 0.0467438 , 0.18828441, 0.0791561 , 0.19342995,\n",
       "        0.13781199, 0.60329502, 0.06567165, 0.22997577, 0.30037356,\n",
       "        0.14695805, 0.34453632, 0.2833577 , 0.2026142 , 0.20080579,\n",
       "        0.14202275, 0.03509836, 0.11209058, 0.07285619, 0.07222411,\n",
       "        0.1435874 , 0.11380429, 0.11022065, 0.07781674, 0.23592322,\n",
       "        0.09461958, 0.14090759, 0.08839646, 0.12415869, 0.06143148,\n",
       "        0.12063557, 0.09740115, 0.06357674, 0.10866741, 0.07736961,\n",
       "        0.10932927, 0.06083651, 0.1407647 , 0.11022413, 0.03730307,\n",
       "        0.09244555, 0.11219957, 0.11193679, 0.09328075, 0.14036206,\n",
       "        0.09023219, 0.1858097 , 0.17700164, 0.1717498 , 0.21144825,\n",
       "        0.11039592, 0.3286953 , 0.21882497, 0.08638277, 0.16901138,\n",
       "        0.24348848, 0.22352828, 0.02928734, 0.03625276, 0.09344899,\n",
       "        0.16720794, 0.09499836, 0.04421765, 0.19028882, 0.09662052,\n",
       "        0.09852777, 0.10798647, 0.11217256, 0.14551009, 0.20592752,\n",
       "        0.12652016, 0.09200486, 0.15878734, 0.13889527, 0.19479529,\n",
       "        0.2643538 , 0.13656116, 0.06142334, 0.20684051, 0.15188393,\n",
       "        0.06554767, 0.2559315 , 0.10255538, 0.06370918, 0.25131265,\n",
       "        0.2657932 , 0.21535671, 0.04407378, 0.31888662, 0.07781727,\n",
       "        0.16354764, 0.08531674, 0.03880137, 0.16956674, 0.1734001 ,\n",
       "        0.1626014 , 0.07901707, 0.14060906, 0.25518166, 0.22500544,\n",
       "        0.18397658, 0.12170096, 0.11442699, 0.24256008, 0.09707655,\n",
       "        0.04760517, 0.12038728, 0.22780136, 0.11796276, 0.26414743,\n",
       "        0.1062747 , 0.10899719, 0.09490149]),\n",
       " 'mean_score_time': array([0.03733244, 0.03582988, 0.03673215, 0.04003406, 0.04073482,\n",
       "        0.0397357 , 0.04874172, 0.0470408 , 0.05074477, 0.0545486 ,\n",
       "        0.04333806, 0.0417366 , 0.0452374 , 0.04093552, 0.04333792,\n",
       "        0.04614024, 0.04854417, 0.04483891, 0.05164213, 0.05504894,\n",
       "        0.04884272, 0.04663906, 0.0632545 , 0.03813281, 0.03983455,\n",
       "        0.03823376, 0.03773236, 0.04033327, 0.04233594, 0.04073548,\n",
       "        0.05194345, 0.0483408 , 0.04694133, 0.04513917, 0.04343872,\n",
       "        0.04273691, 0.04243584, 0.03583069, 0.03843369, 0.04283533,\n",
       "        0.04934196, 0.04363708, 0.06135139, 0.05144405, 0.06071234,\n",
       "        0.05644865, 0.05444646, 0.0599515 , 0.04553819, 0.05024366,\n",
       "        0.04293656, 0.04043493, 0.04844184, 0.04934201, 0.05074463,\n",
       "        0.07746701, 0.06225243, 0.07526512, 0.05424676, 0.04854217,\n",
       "        0.05234461, 0.05154467, 0.04343719, 0.04423819, 0.04623818,\n",
       "        0.06015182, 0.07036352, 0.05935206, 0.06345391, 0.05774894,\n",
       "        0.0543489 , 0.04894247, 0.04613876, 0.05454645, 0.04824128,\n",
       "        0.04674053, 0.04103622, 0.05494967, 0.05654683, 0.06295533,\n",
       "        0.0578495 , 0.0540473 , 0.05054431, 0.04613953, 0.05044341,\n",
       "        0.05304542, 0.04894242, 0.04634042, 0.05424662, 0.03953433,\n",
       "        0.04693942, 0.04453807, 0.04754071, 0.04353843, 0.04603972,\n",
       "        0.04103556, 0.03542881, 0.03533168, 0.03683214, 0.03813276,\n",
       "        0.04103513, 0.03883348, 0.03773236, 0.03663168, 0.0419364 ,\n",
       "        0.04243693, 0.03843322, 0.04363766, 0.04073544, 0.03753247,\n",
       "        0.0356307 , 0.03863325, 0.03653097, 0.03633142, 0.03683157,\n",
       "        0.03763242, 0.03823309, 0.04083533, 0.0418364 , 0.03803339,\n",
       "        0.04133544, 0.03603077, 0.0414361 , 0.0476409 , 0.0373323 ,\n",
       "        0.04433832, 0.04173565, 0.04383779, 0.04673996, 0.04463816,\n",
       "        0.04203639, 0.03973422, 0.04473848, 0.03583121, 0.04103584,\n",
       "        0.04083519, 0.03793273, 0.0416357 , 0.05925117, 0.03723154,\n",
       "        0.04894195, 0.05414686, 0.04003458, 0.03813276, 0.03833299,\n",
       "        0.05044355, 0.04824171, 0.04343748, 0.04443822, 0.04273696,\n",
       "        0.04673991, 0.0481411 , 0.04704061, 0.05584817, 0.05895057,\n",
       "        0.04103527, 0.03923378, 0.04343734, 0.04213634, 0.04443836,\n",
       "        0.05104394, 0.04894238, 0.04453816, 0.04383788, 0.04694037,\n",
       "        0.03813267, 0.03953481, 0.03913398, 0.03893366, 0.04013453,\n",
       "        0.03803287, 0.05314617, 0.04213643, 0.04303708, 0.04724112,\n",
       "        0.04103537, 0.05274496, 0.04323678, 0.04173594, 0.03913364,\n",
       "        0.04143548, 0.03873301, 0.03993392, 0.04473867, 0.05054317,\n",
       "        0.04353757, 0.04944243, 0.04233618, 0.05624809, 0.04263673,\n",
       "        0.04183612, 0.03723207, 0.04093494, 0.03633122, 0.04173565,\n",
       "        0.03623128, 0.03973432, 0.03783264, 0.04894218, 0.04393806,\n",
       "        0.04303627, 0.04784102, 0.04163585, 0.04363751, 0.03743196,\n",
       "        0.03833294, 0.04023404, 0.03783255, 0.04173579, 0.04063501,\n",
       "        0.04413815, 0.0398344 , 0.04263625, 0.05024362, 0.038133  ,\n",
       "        0.04724073, 0.04523873, 0.05224557, 0.04113526, 0.04123502,\n",
       "        0.04463863, 0.0408349 , 0.04583993, 0.03943372, 0.0470407 ,\n",
       "        0.04493842, 0.04223647, 0.04864182, 0.04203615, 0.04603982,\n",
       "        0.05634861, 0.04834189, 0.03783259, 0.0406352 , 0.04724069,\n",
       "        0.04023457, 0.04333735, 0.04764128, 0.03833275, 0.04834127,\n",
       "        0.04198742, 0.04293661, 0.04003448, 0.03993444, 0.04303694,\n",
       "        0.03893352, 0.040135  , 0.04984298, 0.04483876, 0.0458395 ,\n",
       "        0.04573932, 0.03813319, 0.04353738, 0.04353795, 0.04373732,\n",
       "        0.04243665, 0.04964328, 0.04343748, 0.04493842, 0.04844203,\n",
       "        0.0450387 , 0.05134439, 0.04313712, 0.04493833, 0.03993425,\n",
       "        0.04123559, 0.03853283, 0.03968072, 0.04483867, 0.04213662,\n",
       "        0.04113569, 0.0396338 , 0.0415349 , 0.04884229, 0.0429183 ,\n",
       "        0.04023495, 0.04273667, 0.04233637, 0.0373323 , 0.03813281,\n",
       "        0.04093494, 0.04143596, 0.04273667, 0.04804134, 0.03933401,\n",
       "        0.03923469, 0.03513026, 0.03162775]),\n",
       " 'std_score_time': array([0.00204141, 0.00092849, 0.00169241, 0.00325844, 0.0013274 ,\n",
       "        0.0022071 , 0.00351984, 0.00603904, 0.00585763, 0.01156842,\n",
       "        0.00572873, 0.00337344, 0.006436  , 0.00623019, 0.00671133,\n",
       "        0.00470876, 0.01193523, 0.01031341, 0.0056214 , 0.0112004 ,\n",
       "        0.00443781, 0.0041803 , 0.0173056 , 0.00165637, 0.0041334 ,\n",
       "        0.00325299, 0.00271567, 0.0013659 , 0.00392278, 0.00199267,\n",
       "        0.00967457, 0.00237995, 0.00115976, 0.00222451, 0.00508745,\n",
       "        0.00818871, 0.00686457, 0.00067847, 0.00220238, 0.00459339,\n",
       "        0.01065719, 0.00220537, 0.01498478, 0.00739026, 0.0114851 ,\n",
       "        0.00837425, 0.00491682, 0.01413794, 0.00803197, 0.00898714,\n",
       "        0.00622842, 0.00283724, 0.00828084, 0.00595327, 0.01325854,\n",
       "        0.01698326, 0.01260285, 0.02095758, 0.01232973, 0.00699836,\n",
       "        0.01284727, 0.0078807 , 0.00433242, 0.00454907, 0.00594293,\n",
       "        0.01213691, 0.0285563 , 0.01701616, 0.01417315, 0.0078489 ,\n",
       "        0.00854283, 0.00757224, 0.00634848, 0.00827112, 0.00881489,\n",
       "        0.00622377, 0.00281446, 0.00516297, 0.01217902, 0.00968504,\n",
       "        0.00701026, 0.01333451, 0.00624394, 0.0043792 , 0.01039823,\n",
       "        0.0124011 , 0.0060675 , 0.00990525, 0.01502056, 0.00209975,\n",
       "        0.00462391, 0.00253128, 0.00630743, 0.00301753, 0.00543566,\n",
       "        0.00238996, 0.00096734, 0.00231724, 0.00186154, 0.00328026,\n",
       "        0.00673655, 0.0012509 , 0.00120961, 0.00102015, 0.00747208,\n",
       "        0.00605806, 0.00453596, 0.00643562, 0.00811006, 0.00444209,\n",
       "        0.00111462, 0.00384241, 0.00230411, 0.00112324, 0.000813  ,\n",
       "        0.00159507, 0.0029953 , 0.00444913, 0.0113929 , 0.00264797,\n",
       "        0.00487884, 0.00137922, 0.00519477, 0.00825562, 0.00404847,\n",
       "        0.01271322, 0.0035042 , 0.00659061, 0.01045214, 0.00709391,\n",
       "        0.0053521 , 0.00524539, 0.01302902, 0.00169258, 0.00552724,\n",
       "        0.00229517, 0.00208502, 0.00278393, 0.02829178, 0.00163232,\n",
       "        0.00799093, 0.01431032, 0.0022822 , 0.00135765, 0.00120922,\n",
       "        0.01200775, 0.00602677, 0.00345855, 0.0048561 , 0.00344622,\n",
       "        0.00717937, 0.01078652, 0.00906909, 0.01634389, 0.02639926,\n",
       "        0.00487288, 0.00334364, 0.00768453, 0.00578758, 0.00269306,\n",
       "        0.00860979, 0.00606647, 0.00685419, 0.00772449, 0.00437829,\n",
       "        0.00159547, 0.00216978, 0.00215585, 0.00417961, 0.00505838,\n",
       "        0.00167518, 0.02360448, 0.00652001, 0.00532468, 0.01041845,\n",
       "        0.00424664, 0.01343387, 0.00408538, 0.00556008, 0.00251974,\n",
       "        0.00630942, 0.00421784, 0.00609986, 0.00985483, 0.00517176,\n",
       "        0.00373161, 0.01978393, 0.00595146, 0.0138203 , 0.00300957,\n",
       "        0.00436989, 0.0018348 , 0.00622946, 0.00229533, 0.00799162,\n",
       "        0.00163296, 0.00248431, 0.00301349, 0.01545167, 0.00599173,\n",
       "        0.00490353, 0.01143996, 0.00326495, 0.00828582, 0.00198727,\n",
       "        0.00172233, 0.00635072, 0.00132806, 0.0068145 , 0.00273006,\n",
       "        0.01647112, 0.00379233, 0.00490733, 0.00620687, 0.00241886,\n",
       "        0.00874598, 0.01385283, 0.01435334, 0.00180127, 0.00600199,\n",
       "        0.00883577, 0.00343204, 0.01327994, 0.00404585, 0.00753627,\n",
       "        0.01584836, 0.00561375, 0.01925848, 0.00854525, 0.01095929,\n",
       "        0.02212493, 0.01249482, 0.00250496, 0.00248017, 0.00937659,\n",
       "        0.00358861, 0.00312666, 0.00463452, 0.00136505, 0.01205384,\n",
       "        0.0056141 , 0.00476192, 0.00353873, 0.00193553, 0.00592957,\n",
       "        0.00360028, 0.00239807, 0.01327646, 0.00802289, 0.00549655,\n",
       "        0.00927375, 0.00233447, 0.00640092, 0.00663158, 0.00785903,\n",
       "        0.00977816, 0.01064116, 0.00979817, 0.005882  , 0.00678389,\n",
       "        0.00707753, 0.00572001, 0.00422734, 0.00712891, 0.00458013,\n",
       "        0.00688785, 0.00158172, 0.00146623, 0.00999655, 0.00527153,\n",
       "        0.00588176, 0.00388189, 0.00386403, 0.00970681, 0.0048876 ,\n",
       "        0.00296207, 0.00401143, 0.00498999, 0.00367157, 0.00248016,\n",
       "        0.00748615, 0.00385513, 0.00199181, 0.00966238, 0.00273379,\n",
       "        0.00287641, 0.01177218, 0.00139409]),\n",
       " 'param_classifier': masked_array(data=[MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__activation': masked_array(data=['logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__alpha': masked_array(data=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__hidden_layer_sizes': masked_array(data=[(5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__learning_rate': masked_array(data=['constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'}],\n",
       " 'split0_test_recall_micro': array([0.918, 0.921, 0.924, 0.924, 0.919, 0.921, 0.916, 0.926, 0.926,\n",
       "        0.913, 0.917, 0.914, 0.926, 0.918, 0.926, 0.926, 0.924, 0.924,\n",
       "        0.927, 0.917, 0.922, 0.914, 0.931, 0.914, 0.917, 0.918, 0.916,\n",
       "        0.917, 0.922, 0.921, 0.929, 0.92 , 0.924, 0.927, 0.912, 0.923,\n",
       "        0.922, 0.921, 0.925, 0.919, 0.918, 0.919, 0.923, 0.922, 0.92 ,\n",
       "        0.921, 0.926, 0.92 , 0.925, 0.922, 0.924, 0.919, 0.922, 0.922,\n",
       "        0.918, 0.915, 0.928, 0.921, 0.918, 0.911, 0.922, 0.926, 0.92 ,\n",
       "        0.92 , 0.921, 0.918, 0.92 , 0.92 , 0.921, 0.929, 0.923, 0.926,\n",
       "        0.922, 0.918, 0.926, 0.916, 0.918, 0.923, 0.921, 0.92 , 0.908,\n",
       "        0.924, 0.907, 0.924, 0.92 , 0.92 , 0.92 , 0.923, 0.924, 0.921,\n",
       "        0.921, 0.926, 0.919, 0.92 , 0.926, 0.924, 0.746, 0.145, 0.745,\n",
       "        0.83 , 0.   , 0.825, 0.853, 0.196, 0.865, 0.85 , 0.234, 0.872,\n",
       "        0.771, 0.148, 0.716, 0.822, 0.132, 0.824, 0.832, 0.157, 0.825,\n",
       "        0.871, 0.116, 0.866, 0.772, 0.148, 0.778, 0.834, 0.116, 0.833,\n",
       "        0.836, 0.261, 0.822, 0.885, 0.057, 0.869, 0.766, 0.122, 0.784,\n",
       "        0.831, 0.225, 0.833, 0.843, 0.26 , 0.825, 0.849, 0.115, 0.848,\n",
       "        0.892, 0.459, 0.869, 0.901, 0.339, 0.889, 0.892, 0.33 , 0.903,\n",
       "        0.908, 0.424, 0.904, 0.905, 0.284, 0.874, 0.899, 0.176, 0.905,\n",
       "        0.91 , 0.294, 0.903, 0.905, 0.26 , 0.908, 0.843, 0.333, 0.894,\n",
       "        0.906, 0.423, 0.904, 0.899, 0.285, 0.901, 0.9  , 0.212, 0.901,\n",
       "        0.892, 0.329, 0.904, 0.907, 0.181, 0.905, 0.902, 0.089, 0.905,\n",
       "        0.903, 0.389, 0.907, 0.856, 0.858, 0.902, 0.912, 0.916, 0.917,\n",
       "        0.918, 0.919, 0.919, 0.915, 0.919, 0.921, 0.869, 0.902, 0.861,\n",
       "        0.918, 0.916, 0.911, 0.917, 0.912, 0.917, 0.919, 0.918, 0.915,\n",
       "        0.877, 0.863, 0.871, 0.916, 0.91 , 0.912, 0.915, 0.917, 0.918,\n",
       "        0.917, 0.917, 0.922, 0.898, 0.874, 0.869, 0.917, 0.913, 0.913,\n",
       "        0.916, 0.914, 0.917, 0.915, 0.913, 0.913, 0.909, 0.911, 0.912,\n",
       "        0.914, 0.914, 0.915, 0.926, 0.921, 0.917, 0.923, 0.922, 0.923,\n",
       "        0.914, 0.912, 0.91 , 0.921, 0.913, 0.921, 0.92 , 0.916, 0.916,\n",
       "        0.919, 0.922, 0.923, 0.911, 0.92 , 0.912, 0.92 , 0.918, 0.916,\n",
       "        0.922, 0.921, 0.917, 0.918, 0.921, 0.919, 0.91 , 0.914, 0.905,\n",
       "        0.917, 0.92 , 0.919, 0.92 , 0.919, 0.922, 0.921, 0.919, 0.92 ]),\n",
       " 'split1_test_recall_micro': array([0.907, 0.911, 0.901, 0.918, 0.917, 0.917, 0.919, 0.919, 0.91 ,\n",
       "        0.914, 0.912, 0.907, 0.912, 0.916, 0.905, 0.914, 0.916, 0.916,\n",
       "        0.915, 0.914, 0.912, 0.918, 0.917, 0.923, 0.914, 0.912, 0.901,\n",
       "        0.911, 0.908, 0.913, 0.918, 0.913, 0.921, 0.911, 0.912, 0.918,\n",
       "        0.909, 0.913, 0.914, 0.916, 0.915, 0.916, 0.917, 0.918, 0.916,\n",
       "        0.913, 0.926, 0.912, 0.907, 0.908, 0.911, 0.911, 0.91 , 0.923,\n",
       "        0.915, 0.92 , 0.926, 0.926, 0.92 , 0.911, 0.911, 0.907, 0.908,\n",
       "        0.917, 0.916, 0.918, 0.918, 0.919, 0.915, 0.923, 0.927, 0.916,\n",
       "        0.911, 0.908, 0.907, 0.923, 0.912, 0.915, 0.915, 0.917, 0.929,\n",
       "        0.902, 0.922, 0.907, 0.91 , 0.912, 0.908, 0.915, 0.912, 0.919,\n",
       "        0.916, 0.915, 0.918, 0.915, 0.916, 0.922, 0.754, 0.238, 0.733,\n",
       "        0.791, 0.256, 0.794, 0.821, 0.117, 0.828, 0.857, 0.259, 0.854,\n",
       "        0.694, 0.12 , 0.714, 0.803, 0.139, 0.8  , 0.826, 0.194, 0.815,\n",
       "        0.854, 0.242, 0.863, 0.77 , 0.049, 0.747, 0.781, 0.048, 0.79 ,\n",
       "        0.82 , 0.196, 0.829, 0.864, 0.197, 0.836, 0.767, 0.233, 0.715,\n",
       "        0.791, 0.259, 0.795, 0.85 , 0.323, 0.818, 0.863, 0.196, 0.839,\n",
       "        0.845, 0.26 , 0.893, 0.889, 0.308, 0.9  , 0.891, 0.313, 0.896,\n",
       "        0.895, 0.37 , 0.9  , 0.791, 0.108, 0.887, 0.89 , 0.199, 0.891,\n",
       "        0.895, 0.312, 0.895, 0.897, 0.495, 0.897, 0.848, 0.16 , 0.827,\n",
       "        0.893, 0.241, 0.892, 0.898, 0.335, 0.893, 0.891, 0.366, 0.891,\n",
       "        0.89 , 0.286, 0.874, 0.885, 0.17 , 0.892, 0.892, 0.309, 0.892,\n",
       "        0.894, 0.362, 0.893, 0.849, 0.842, 0.846, 0.896, 0.899, 0.901,\n",
       "        0.9  , 0.899, 0.897, 0.899, 0.9  , 0.901, 0.889, 0.849, 0.886,\n",
       "        0.904, 0.899, 0.895, 0.896, 0.899, 0.899, 0.9  , 0.894, 0.9  ,\n",
       "        0.854, 0.828, 0.856, 0.897, 0.898, 0.898, 0.9  , 0.899, 0.898,\n",
       "        0.9  , 0.899, 0.902, 0.85 , 0.858, 0.853, 0.893, 0.895, 0.898,\n",
       "        0.893, 0.898, 0.892, 0.893, 0.898, 0.898, 0.899, 0.901, 0.893,\n",
       "        0.898, 0.901, 0.903, 0.904, 0.908, 0.902, 0.907, 0.903, 0.903,\n",
       "        0.899, 0.892, 0.896, 0.902, 0.894, 0.898, 0.904, 0.908, 0.903,\n",
       "        0.906, 0.903, 0.905, 0.897, 0.898, 0.896, 0.9  , 0.904, 0.899,\n",
       "        0.907, 0.901, 0.903, 0.904, 0.902, 0.901, 0.899, 0.892, 0.895,\n",
       "        0.901, 0.9  , 0.902, 0.903, 0.903, 0.902, 0.901, 0.906, 0.901]),\n",
       " 'split2_test_recall_micro': array([0.923, 0.922, 0.924, 0.92 , 0.916, 0.924, 0.914, 0.928, 0.912,\n",
       "        0.916, 0.924, 0.917, 0.917, 0.915, 0.919, 0.917, 0.921, 0.92 ,\n",
       "        0.919, 0.923, 0.922, 0.917, 0.913, 0.917, 0.921, 0.92 , 0.916,\n",
       "        0.92 , 0.922, 0.924, 0.913, 0.92 , 0.924, 0.917, 0.919, 0.925,\n",
       "        0.919, 0.917, 0.926, 0.921, 0.923, 0.914, 0.923, 0.927, 0.919,\n",
       "        0.922, 0.92 , 0.928, 0.918, 0.923, 0.921, 0.916, 0.919, 0.924,\n",
       "        0.918, 0.91 , 0.917, 0.916, 0.923, 0.915, 0.927, 0.919, 0.923,\n",
       "        0.917, 0.917, 0.913, 0.916, 0.911, 0.914, 0.915, 0.909, 0.914,\n",
       "        0.924, 0.917, 0.916, 0.918, 0.924, 0.926, 0.928, 0.922, 0.919,\n",
       "        0.913, 0.91 , 0.914, 0.919, 0.918, 0.923, 0.927, 0.924, 0.927,\n",
       "        0.924, 0.925, 0.925, 0.922, 0.921, 0.919, 0.586, 0.259, 0.747,\n",
       "        0.8  , 0.259, 0.783, 0.823, 0.198, 0.857, 0.856, 0.271, 0.869,\n",
       "        0.727, 0.196, 0.747, 0.804, 0.196, 0.786, 0.839, 0.189, 0.812,\n",
       "        0.835, 0.229, 0.869, 0.705, 0.196, 0.712, 0.799, 0.117, 0.775,\n",
       "        0.827, 0.391, 0.826, 0.838, 0.085, 0.828, 0.735, 0.232, 0.737,\n",
       "        0.793, 0.259, 0.789, 0.81 , 0.097, 0.819, 0.871, 0.255, 0.834,\n",
       "        0.861, 0.179, 0.868, 0.915, 0.066, 0.914, 0.912, 0.409, 0.91 ,\n",
       "        0.917, 0.142, 0.915, 0.853, 0.194, 0.866, 0.913, 0.295, 0.914,\n",
       "        0.92 , 0.223, 0.92 , 0.919, 0.158, 0.911, 0.91 , 0.102, 0.874,\n",
       "        0.907, 0.28 , 0.911, 0.922, 0.257, 0.916, 0.913, 0.437, 0.913,\n",
       "        0.911, 0.059, 0.864, 0.913, 0.228, 0.914, 0.918, 0.499, 0.918,\n",
       "        0.913, 0.42 , 0.919, 0.88 , 0.908, 0.876, 0.921, 0.913, 0.922,\n",
       "        0.915, 0.918, 0.917, 0.916, 0.917, 0.919, 0.875, 0.914, 0.87 ,\n",
       "        0.921, 0.914, 0.915, 0.916, 0.919, 0.919, 0.916, 0.916, 0.918,\n",
       "        0.862, 0.869, 0.852, 0.916, 0.921, 0.922, 0.918, 0.913, 0.92 ,\n",
       "        0.92 , 0.914, 0.915, 0.871, 0.875, 0.872, 0.912, 0.924, 0.924,\n",
       "        0.917, 0.916, 0.918, 0.92 , 0.921, 0.921, 0.919, 0.914, 0.914,\n",
       "        0.92 , 0.917, 0.921, 0.915, 0.92 , 0.92 , 0.917, 0.919, 0.917,\n",
       "        0.92 , 0.908, 0.908, 0.918, 0.917, 0.914, 0.918, 0.915, 0.916,\n",
       "        0.916, 0.92 , 0.917, 0.912, 0.912, 0.911, 0.917, 0.917, 0.917,\n",
       "        0.921, 0.918, 0.915, 0.916, 0.917, 0.913, 0.918, 0.913, 0.912,\n",
       "        0.916, 0.916, 0.917, 0.918, 0.919, 0.914, 0.915, 0.914, 0.919]),\n",
       " 'split3_test_recall_micro': array([0.936, 0.927, 0.934, 0.926, 0.929, 0.933, 0.934, 0.931, 0.924,\n",
       "        0.932, 0.932, 0.93 , 0.937, 0.925, 0.929, 0.936, 0.938, 0.927,\n",
       "        0.935, 0.933, 0.93 , 0.922, 0.922, 0.924, 0.934, 0.933, 0.933,\n",
       "        0.936, 0.927, 0.934, 0.93 , 0.937, 0.928, 0.929, 0.922, 0.929,\n",
       "        0.935, 0.93 , 0.935, 0.93 , 0.932, 0.929, 0.937, 0.929, 0.929,\n",
       "        0.933, 0.935, 0.929, 0.93 , 0.935, 0.927, 0.924, 0.928, 0.928,\n",
       "        0.918, 0.932, 0.936, 0.935, 0.928, 0.933, 0.936, 0.925, 0.928,\n",
       "        0.931, 0.93 , 0.928, 0.929, 0.927, 0.925, 0.922, 0.926, 0.924,\n",
       "        0.936, 0.935, 0.932, 0.933, 0.932, 0.938, 0.933, 0.929, 0.92 ,\n",
       "        0.923, 0.929, 0.932, 0.927, 0.938, 0.934, 0.927, 0.937, 0.936,\n",
       "        0.932, 0.932, 0.936, 0.939, 0.929, 0.931, 0.739, 0.297, 0.7  ,\n",
       "        0.799, 0.142, 0.786, 0.833, 0.148, 0.852, 0.851, 0.096, 0.85 ,\n",
       "        0.721, 0.117, 0.775, 0.814, 0.15 , 0.807, 0.834, 0.258, 0.841,\n",
       "        0.875, 0.248, 0.872, 0.756, 0.145, 0.774, 0.819, 0.13 , 0.823,\n",
       "        0.845, 0.096, 0.821, 0.851, 0.263, 0.852, 0.74 , 0.113, 0.717,\n",
       "        0.831, 0.117, 0.79 , 0.841, 0.114, 0.823, 0.889, 0.38 , 0.862,\n",
       "        0.865, 0.235, 0.848, 0.91 , 0.324, 0.914, 0.92 , 0.44 , 0.918,\n",
       "        0.917, 0.168, 0.923, 0.884, 0.194, 0.874, 0.911, 0.329, 0.916,\n",
       "        0.917, 0.185, 0.922, 0.919, 0.298, 0.923, 0.869, 0.19 , 0.874,\n",
       "        0.915, 0.102, 0.924, 0.93 , 0.2  , 0.92 , 0.918, 0.279, 0.923,\n",
       "        0.88 , 0.13 , 0.896, 0.915, 0.182, 0.918, 0.924, 0.282, 0.922,\n",
       "        0.922, 0.268, 0.926, 0.876, 0.887, 0.853, 0.925, 0.925, 0.928,\n",
       "        0.931, 0.929, 0.929, 0.933, 0.93 , 0.929, 0.882, 0.878, 0.876,\n",
       "        0.924, 0.93 , 0.924, 0.928, 0.926, 0.93 , 0.93 , 0.932, 0.931,\n",
       "        0.909, 0.922, 0.908, 0.927, 0.926, 0.928, 0.926, 0.927, 0.924,\n",
       "        0.932, 0.928, 0.931, 0.873, 0.879, 0.87 , 0.922, 0.927, 0.922,\n",
       "        0.926, 0.928, 0.928, 0.932, 0.931, 0.929, 0.926, 0.923, 0.928,\n",
       "        0.933, 0.933, 0.927, 0.931, 0.927, 0.932, 0.934, 0.927, 0.931,\n",
       "        0.924, 0.917, 0.928, 0.931, 0.928, 0.933, 0.934, 0.93 , 0.929,\n",
       "        0.932, 0.93 , 0.932, 0.917, 0.93 , 0.924, 0.926, 0.932, 0.928,\n",
       "        0.931, 0.931, 0.932, 0.932, 0.932, 0.93 , 0.927, 0.927, 0.925,\n",
       "        0.928, 0.93 , 0.927, 0.931, 0.936, 0.93 , 0.929, 0.93 , 0.934]),\n",
       " 'split4_test_recall_micro': array([0.927, 0.927, 0.924, 0.924, 0.928, 0.935, 0.926, 0.926, 0.928,\n",
       "        0.926, 0.932, 0.929, 0.93 , 0.926, 0.924, 0.922, 0.928, 0.933,\n",
       "        0.928, 0.926, 0.933, 0.933, 0.928, 0.927, 0.927, 0.931, 0.927,\n",
       "        0.925, 0.916, 0.929, 0.926, 0.922, 0.931, 0.921, 0.934, 0.93 ,\n",
       "        0.931, 0.929, 0.929, 0.924, 0.931, 0.931, 0.922, 0.93 , 0.933,\n",
       "        0.931, 0.929, 0.924, 0.925, 0.93 , 0.925, 0.919, 0.93 , 0.928,\n",
       "        0.924, 0.929, 0.923, 0.922, 0.926, 0.928, 0.922, 0.925, 0.93 ,\n",
       "        0.929, 0.921, 0.926, 0.932, 0.93 , 0.924, 0.922, 0.927, 0.92 ,\n",
       "        0.924, 0.928, 0.924, 0.931, 0.924, 0.928, 0.927, 0.921, 0.928,\n",
       "        0.932, 0.923, 0.934, 0.927, 0.93 , 0.928, 0.928, 0.933, 0.926,\n",
       "        0.929, 0.932, 0.933, 0.928, 0.931, 0.928, 0.711, 0.111, 0.645,\n",
       "        0.788, 0.196, 0.786, 0.831, 0.148, 0.827, 0.875, 0.25 , 0.889,\n",
       "        0.741, 0.147, 0.73 , 0.801, 0.273, 0.779, 0.833, 0.183, 0.837,\n",
       "        0.838, 0.038, 0.88 , 0.727, 0.196, 0.71 , 0.778, 0.038, 0.803,\n",
       "        0.845, 0.143, 0.814, 0.879, 0.06 , 0.858, 0.741, 0.111, 0.711,\n",
       "        0.807, 0.143, 0.781, 0.834, 0.319, 0.828, 0.845, 0.175, 0.877,\n",
       "        0.89 , 0.03 , 0.917, 0.925, 0.145, 0.923, 0.922, 0.239, 0.919,\n",
       "        0.922, 0.206, 0.924, 0.869, 0.24 , 0.909, 0.919, 0.26 , 0.917,\n",
       "        0.924, 0.233, 0.92 , 0.921, 0.36 , 0.928, 0.874, 0.079, 0.873,\n",
       "        0.921, 0.237, 0.92 , 0.921, 0.153, 0.922, 0.917, 0.195, 0.919,\n",
       "        0.866, 0.334, 0.871, 0.919, 0.139, 0.918, 0.925, 0.307, 0.925,\n",
       "        0.924, 0.327, 0.93 , 0.877, 0.878, 0.915, 0.926, 0.929, 0.92 ,\n",
       "        0.926, 0.927, 0.922, 0.921, 0.921, 0.922, 0.873, 0.873, 0.864,\n",
       "        0.927, 0.926, 0.924, 0.923, 0.925, 0.926, 0.922, 0.921, 0.926,\n",
       "        0.858, 0.871, 0.876, 0.924, 0.923, 0.929, 0.922, 0.923, 0.922,\n",
       "        0.922, 0.924, 0.926, 0.877, 0.88 , 0.879, 0.922, 0.925, 0.921,\n",
       "        0.928, 0.923, 0.922, 0.922, 0.925, 0.921, 0.928, 0.922, 0.92 ,\n",
       "        0.928, 0.926, 0.926, 0.927, 0.925, 0.928, 0.932, 0.929, 0.927,\n",
       "        0.921, 0.927, 0.913, 0.923, 0.925, 0.929, 0.926, 0.929, 0.923,\n",
       "        0.93 , 0.928, 0.926, 0.921, 0.919, 0.925, 0.923, 0.925, 0.925,\n",
       "        0.925, 0.926, 0.924, 0.926, 0.926, 0.926, 0.923, 0.924, 0.924,\n",
       "        0.926, 0.929, 0.925, 0.925, 0.927, 0.926, 0.927, 0.928, 0.925]),\n",
       " 'mean_test_recall_micro': array([0.9222, 0.9216, 0.9214, 0.9224, 0.9218, 0.926 , 0.9218, 0.926 ,\n",
       "        0.92  , 0.9202, 0.9234, 0.9194, 0.9244, 0.92  , 0.9206, 0.923 ,\n",
       "        0.9254, 0.924 , 0.9248, 0.9226, 0.9238, 0.9208, 0.9222, 0.921 ,\n",
       "        0.9226, 0.9228, 0.9186, 0.9218, 0.919 , 0.9242, 0.9232, 0.9224,\n",
       "        0.9256, 0.921 , 0.9198, 0.925 , 0.9232, 0.922 , 0.9258, 0.922 ,\n",
       "        0.9238, 0.9218, 0.9244, 0.9252, 0.9234, 0.924 , 0.9272, 0.9226,\n",
       "        0.921 , 0.9236, 0.9216, 0.9178, 0.9218, 0.925 , 0.9186, 0.9212,\n",
       "        0.926 , 0.924 , 0.923 , 0.9196, 0.9236, 0.9204, 0.9218, 0.9228,\n",
       "        0.921 , 0.9206, 0.923 , 0.9214, 0.9198, 0.9222, 0.9224, 0.92  ,\n",
       "        0.9234, 0.9212, 0.921 , 0.9242, 0.922 , 0.926 , 0.9248, 0.9218,\n",
       "        0.9208, 0.9188, 0.9182, 0.9222, 0.9206, 0.9236, 0.9226, 0.924 ,\n",
       "        0.926 , 0.9258, 0.9244, 0.926 , 0.9262, 0.9248, 0.9246, 0.9248,\n",
       "        0.7072, 0.21  , 0.714 , 0.8016, 0.1706, 0.7948, 0.8322, 0.1614,\n",
       "        0.8458, 0.8578, 0.222 , 0.8668, 0.7308, 0.1456, 0.7364, 0.8088,\n",
       "        0.178 , 0.7992, 0.8328, 0.1962, 0.826 , 0.8546, 0.1746, 0.87  ,\n",
       "        0.746 , 0.1468, 0.7442, 0.8022, 0.0898, 0.8048, 0.8346, 0.2174,\n",
       "        0.8224, 0.8634, 0.1324, 0.8486, 0.7498, 0.1622, 0.7328, 0.8106,\n",
       "        0.2006, 0.7976, 0.8356, 0.2226, 0.8226, 0.8634, 0.2242, 0.852 ,\n",
       "        0.8706, 0.2326, 0.879 , 0.908 , 0.2364, 0.908 , 0.9074, 0.3462,\n",
       "        0.9092, 0.9118, 0.262 , 0.9132, 0.8604, 0.204 , 0.882 , 0.9064,\n",
       "        0.2518, 0.9086, 0.9132, 0.2494, 0.912 , 0.9122, 0.3142, 0.9134,\n",
       "        0.8688, 0.1728, 0.8684, 0.9084, 0.2566, 0.9102, 0.914 , 0.246 ,\n",
       "        0.9104, 0.9078, 0.2978, 0.9094, 0.8878, 0.2276, 0.8818, 0.9078,\n",
       "        0.18  , 0.9094, 0.9122, 0.2972, 0.9124, 0.9112, 0.3532, 0.915 ,\n",
       "        0.8676, 0.8746, 0.8784, 0.916 , 0.9164, 0.9176, 0.918 , 0.9184,\n",
       "        0.9168, 0.9168, 0.9174, 0.9184, 0.8776, 0.8832, 0.8714, 0.9188,\n",
       "        0.917 , 0.9138, 0.916 , 0.9162, 0.9182, 0.9174, 0.9162, 0.918 ,\n",
       "        0.872 , 0.8706, 0.8726, 0.916 , 0.9156, 0.9178, 0.9162, 0.9158,\n",
       "        0.9164, 0.9182, 0.9164, 0.9192, 0.8738, 0.8732, 0.8686, 0.9132,\n",
       "        0.9168, 0.9156, 0.916 , 0.9158, 0.9154, 0.9164, 0.9176, 0.9164,\n",
       "        0.9162, 0.9142, 0.9134, 0.9186, 0.9182, 0.9184, 0.9206, 0.9202,\n",
       "        0.9198, 0.9226, 0.92  , 0.9202, 0.9156, 0.9112, 0.911 , 0.919 ,\n",
       "        0.9154, 0.919 , 0.9204, 0.9196, 0.9174, 0.9206, 0.9206, 0.9206,\n",
       "        0.9116, 0.9158, 0.9136, 0.9172, 0.9192, 0.917 , 0.9212, 0.9194,\n",
       "        0.9182, 0.9192, 0.9196, 0.9178, 0.9154, 0.914 , 0.9122, 0.9176,\n",
       "        0.919 , 0.918 , 0.9194, 0.9208, 0.9188, 0.9186, 0.9194, 0.9198]),\n",
       " 'std_test_recall_micro': array([0.00962081, 0.0058515 , 0.01091055, 0.00293939, 0.00556417,\n",
       "        0.0069282 , 0.00733212, 0.00394968, 0.00748331, 0.007494  ,\n",
       "        0.00798999, 0.00886792, 0.00895768, 0.00460435, 0.00845222,\n",
       "        0.00769415, 0.00741889, 0.00583095, 0.00705408, 0.00671118,\n",
       "        0.00733212, 0.00661513, 0.00667533, 0.00477493, 0.00717217,\n",
       "        0.00798499, 0.0109654 , 0.00842378, 0.00651153, 0.00713863,\n",
       "        0.00661513, 0.00791454, 0.00349857, 0.00657267, 0.00810925,\n",
       "        0.0043359 , 0.00917388, 0.00663325, 0.00685274, 0.00477493,\n",
       "        0.00679412, 0.00691086, 0.00668132, 0.00453431, 0.00646838,\n",
       "        0.00726636, 0.00487442, 0.00618385, 0.00797496, 0.00913455,\n",
       "        0.00564269, 0.00426146, 0.00711056, 0.00252982, 0.00293939,\n",
       "        0.0082801 , 0.00622896, 0.0063561 , 0.00368782, 0.00915642,\n",
       "        0.00811419, 0.00714423, 0.00775629, 0.00601332, 0.00493964,\n",
       "        0.00557136, 0.00632456, 0.00665132, 0.00453431, 0.00444522,\n",
       "        0.00685857, 0.0045607 , 0.00793977, 0.00936803, 0.00867179,\n",
       "        0.00679412, 0.00669328, 0.00745654, 0.00620967, 0.00396989,\n",
       "        0.00757364, 0.01034215, 0.00832827, 0.01036147, 0.00628013,\n",
       "        0.00924338, 0.00870862, 0.00481664, 0.0086487 , 0.0059127 ,\n",
       "        0.00567803, 0.00622896, 0.00724983, 0.00823165, 0.0054626 ,\n",
       "        0.00426146, 0.06230698, 0.07039886, 0.0383875 , 0.01492113,\n",
       "        0.0955774 , 0.01553577, 0.01135606, 0.0312    , 0.01551   ,\n",
       "        0.00901998, 0.06414671, 0.01393413, 0.02523807, 0.02835913,\n",
       "        0.02263272, 0.00798499, 0.05247857, 0.01586695, 0.00416653,\n",
       "        0.03342694, 0.01152389, 0.01640244, 0.08364592, 0.00583095,\n",
       "        0.02605379, 0.05368575, 0.02913692, 0.02162776, 0.03865954,\n",
       "        0.02115089, 0.00989141, 0.10273188, 0.00508331, 0.01737354,\n",
       "        0.08294962, 0.014827  , 0.01378985, 0.05752008, 0.02713227,\n",
       "        0.01754537, 0.05953688, 0.01826034, 0.01377824, 0.09832721,\n",
       "        0.00372022, 0.01586947, 0.08984743, 0.01570987, 0.01796218,\n",
       "        0.13851585, 0.0237571 , 0.01226377, 0.11019728, 0.01201666,\n",
       "        0.01341044, 0.0715525 , 0.00879545, 0.0095373 , 0.1133843 ,\n",
       "        0.00974474, 0.03870711, 0.05850128, 0.01508642, 0.01046136,\n",
       "        0.05731806, 0.00976934, 0.01018627, 0.04694933, 0.01093618,\n",
       "        0.0095163 , 0.11170031, 0.01103812, 0.02376047, 0.08936979,\n",
       "        0.0221504 , 0.00945727, 0.10272799, 0.01146124, 0.0130384 ,\n",
       "        0.06376206, 0.01139474, 0.01057166, 0.09199   , 0.0118254 ,\n",
       "        0.01481081, 0.11221872, 0.01541947, 0.01203993, 0.0286007 ,\n",
       "        0.00991161, 0.0130292 , 0.1300283 , 0.01227355, 0.01137365,\n",
       "        0.05247247, 0.01349074, 0.01259524, 0.02288755, 0.02682238,\n",
       "        0.01115347, 0.01046136, 0.00904655, 0.01063955, 0.0106132 ,\n",
       "        0.01070327, 0.01096175, 0.00976934, 0.00932952, 0.00708802,\n",
       "        0.02281578, 0.00893532, 0.00798499, 0.0108074 , 0.01068457,\n",
       "        0.01089954, 0.00994786, 0.01068457, 0.00987117, 0.0124    ,\n",
       "        0.01063955, 0.02006988, 0.03005728, 0.01983532, 0.01044988,\n",
       "        0.01032666, 0.0116    , 0.00890842, 0.00968297, 0.00941488,\n",
       "        0.0104    , 0.01001199, 0.01006777, 0.01530229, 0.00793473,\n",
       "        0.00854634, 0.01075918, 0.01193985, 0.00956243, 0.01244186,\n",
       "        0.01020588, 0.01232234, 0.01293986, 0.01141227, 0.01049952,\n",
       "        0.01087014, 0.00803492, 0.01162067, 0.0121918 , 0.01090688,\n",
       "        0.0088    , 0.00985089, 0.00661513, 0.0104    , 0.00993177,\n",
       "        0.00920869, 0.00976524, 0.00891291, 0.01151347, 0.01027619,\n",
       "        0.0095289 , 0.01197664, 0.0123774 , 0.00991161, 0.00854634,\n",
       "        0.00868562, 0.00954149, 0.00954149, 0.00917824, 0.0081388 ,\n",
       "        0.01059056, 0.01055651, 0.00910824, 0.00932523, 0.0100995 ,\n",
       "        0.00790949, 0.0102098 , 0.0096623 , 0.0095163 , 0.01013114,\n",
       "        0.01022546, 0.00997196, 0.01228007, 0.01140877, 0.00956243,\n",
       "        0.01088118, 0.00880909, 0.00935094, 0.01088853, 0.00992774,\n",
       "        0.01007174, 0.00889044, 0.0107963 ]),\n",
       " 'rank_test_recall_micro': array([ 53,  67,  69,  50,  61,   3,  61,   4,  94,  92,  35, 105,  21,\n",
       "         94,  82,  40,  12,  26,  16,  45,  30,  79,  53,  74,  49,  44,\n",
       "        120,  61, 112,  24,  38,  50,  11,  74,  98,  14,  38,  57,   9,\n",
       "         57,  31,  61,  21,  13,  35,  26,   1,  45,  74,  32,  67, 134,\n",
       "         61,  14, 119,  72,   4,  26,  40, 102,  32,  89,  61,  43,  74,\n",
       "         84,  40,  69,  98,  53,  50,  94,  35,  71,  74,  24,  57,   4,\n",
       "         16,  60,  79, 116, 126,  56,  84,  34,  45,  26,   4,   9,  21,\n",
       "          4,   2,  16,  20,  16, 256, 274, 255, 245, 282, 248, 237, 284,\n",
       "        233, 229, 272, 225, 254, 286, 252, 242, 279, 246, 236, 277, 238,\n",
       "        230, 280, 220, 250, 285, 251, 244, 288, 243, 235, 273, 240, 226,\n",
       "        287, 232, 249, 283, 253, 241, 276, 247, 234, 271, 239, 226, 270,\n",
       "        231, 218, 268, 209, 200, 267, 199, 203, 258, 196, 187, 262, 179,\n",
       "        228, 275, 207, 204, 264, 197, 179, 265, 186, 184, 259, 177, 221,\n",
       "        281, 223, 198, 263, 193, 173, 266, 192, 201, 260, 194, 205, 269,\n",
       "        208, 201, 278, 194, 184, 261, 182, 190, 257, 171, 224, 212, 210,\n",
       "        158, 150, 137, 132, 123, 148, 146, 140, 124, 211, 206, 217, 116,\n",
       "        144, 175, 158, 154, 126, 140, 154, 132, 216, 218, 215, 158, 165,\n",
       "        134, 154, 162, 150, 126, 150, 109, 213, 214, 222, 181, 146, 165,\n",
       "        158, 162, 168, 150, 137, 149, 154, 172, 177, 120, 126, 124,  84,\n",
       "         92,  98,  45,  94,  91, 165, 189, 191, 112, 168, 112,  89, 102,\n",
       "        140,  84,  82,  84, 188, 162, 176, 143, 109, 144,  72, 105, 126,\n",
       "        109, 102, 134, 168, 173, 183, 137, 112, 131, 105,  79, 116, 120,\n",
       "        105,  98]),\n",
       " 'split0_test_f1_micro': array([0.918, 0.921, 0.924, 0.924, 0.919, 0.921, 0.916, 0.926, 0.926,\n",
       "        0.913, 0.917, 0.914, 0.926, 0.918, 0.926, 0.926, 0.924, 0.924,\n",
       "        0.927, 0.917, 0.922, 0.914, 0.931, 0.914, 0.917, 0.918, 0.916,\n",
       "        0.917, 0.922, 0.921, 0.929, 0.92 , 0.924, 0.927, 0.912, 0.923,\n",
       "        0.922, 0.921, 0.925, 0.919, 0.918, 0.919, 0.923, 0.922, 0.92 ,\n",
       "        0.921, 0.926, 0.92 , 0.925, 0.922, 0.924, 0.919, 0.922, 0.922,\n",
       "        0.918, 0.915, 0.928, 0.921, 0.918, 0.911, 0.922, 0.926, 0.92 ,\n",
       "        0.92 , 0.921, 0.918, 0.92 , 0.92 , 0.921, 0.929, 0.923, 0.926,\n",
       "        0.922, 0.918, 0.926, 0.916, 0.918, 0.923, 0.921, 0.92 , 0.908,\n",
       "        0.924, 0.907, 0.924, 0.92 , 0.92 , 0.92 , 0.923, 0.924, 0.921,\n",
       "        0.921, 0.926, 0.919, 0.92 , 0.926, 0.924, 0.746, 0.145, 0.745,\n",
       "        0.83 , 0.   , 0.825, 0.853, 0.196, 0.865, 0.85 , 0.234, 0.872,\n",
       "        0.771, 0.148, 0.716, 0.822, 0.132, 0.824, 0.832, 0.157, 0.825,\n",
       "        0.871, 0.116, 0.866, 0.772, 0.148, 0.778, 0.834, 0.116, 0.833,\n",
       "        0.836, 0.261, 0.822, 0.885, 0.057, 0.869, 0.766, 0.122, 0.784,\n",
       "        0.831, 0.225, 0.833, 0.843, 0.26 , 0.825, 0.849, 0.115, 0.848,\n",
       "        0.892, 0.459, 0.869, 0.901, 0.339, 0.889, 0.892, 0.33 , 0.903,\n",
       "        0.908, 0.424, 0.904, 0.905, 0.284, 0.874, 0.899, 0.176, 0.905,\n",
       "        0.91 , 0.294, 0.903, 0.905, 0.26 , 0.908, 0.843, 0.333, 0.894,\n",
       "        0.906, 0.423, 0.904, 0.899, 0.285, 0.901, 0.9  , 0.212, 0.901,\n",
       "        0.892, 0.329, 0.904, 0.907, 0.181, 0.905, 0.902, 0.089, 0.905,\n",
       "        0.903, 0.389, 0.907, 0.856, 0.858, 0.902, 0.912, 0.916, 0.917,\n",
       "        0.918, 0.919, 0.919, 0.915, 0.919, 0.921, 0.869, 0.902, 0.861,\n",
       "        0.918, 0.916, 0.911, 0.917, 0.912, 0.917, 0.919, 0.918, 0.915,\n",
       "        0.877, 0.863, 0.871, 0.916, 0.91 , 0.912, 0.915, 0.917, 0.918,\n",
       "        0.917, 0.917, 0.922, 0.898, 0.874, 0.869, 0.917, 0.913, 0.913,\n",
       "        0.916, 0.914, 0.917, 0.915, 0.913, 0.913, 0.909, 0.911, 0.912,\n",
       "        0.914, 0.914, 0.915, 0.926, 0.921, 0.917, 0.923, 0.922, 0.923,\n",
       "        0.914, 0.912, 0.91 , 0.921, 0.913, 0.921, 0.92 , 0.916, 0.916,\n",
       "        0.919, 0.922, 0.923, 0.911, 0.92 , 0.912, 0.92 , 0.918, 0.916,\n",
       "        0.922, 0.921, 0.917, 0.918, 0.921, 0.919, 0.91 , 0.914, 0.905,\n",
       "        0.917, 0.92 , 0.919, 0.92 , 0.919, 0.922, 0.921, 0.919, 0.92 ]),\n",
       " 'split1_test_f1_micro': array([0.907, 0.911, 0.901, 0.918, 0.917, 0.917, 0.919, 0.919, 0.91 ,\n",
       "        0.914, 0.912, 0.907, 0.912, 0.916, 0.905, 0.914, 0.916, 0.916,\n",
       "        0.915, 0.914, 0.912, 0.918, 0.917, 0.923, 0.914, 0.912, 0.901,\n",
       "        0.911, 0.908, 0.913, 0.918, 0.913, 0.921, 0.911, 0.912, 0.918,\n",
       "        0.909, 0.913, 0.914, 0.916, 0.915, 0.916, 0.917, 0.918, 0.916,\n",
       "        0.913, 0.926, 0.912, 0.907, 0.908, 0.911, 0.911, 0.91 , 0.923,\n",
       "        0.915, 0.92 , 0.926, 0.926, 0.92 , 0.911, 0.911, 0.907, 0.908,\n",
       "        0.917, 0.916, 0.918, 0.918, 0.919, 0.915, 0.923, 0.927, 0.916,\n",
       "        0.911, 0.908, 0.907, 0.923, 0.912, 0.915, 0.915, 0.917, 0.929,\n",
       "        0.902, 0.922, 0.907, 0.91 , 0.912, 0.908, 0.915, 0.912, 0.919,\n",
       "        0.916, 0.915, 0.918, 0.915, 0.916, 0.922, 0.754, 0.238, 0.733,\n",
       "        0.791, 0.256, 0.794, 0.821, 0.117, 0.828, 0.857, 0.259, 0.854,\n",
       "        0.694, 0.12 , 0.714, 0.803, 0.139, 0.8  , 0.826, 0.194, 0.815,\n",
       "        0.854, 0.242, 0.863, 0.77 , 0.049, 0.747, 0.781, 0.048, 0.79 ,\n",
       "        0.82 , 0.196, 0.829, 0.864, 0.197, 0.836, 0.767, 0.233, 0.715,\n",
       "        0.791, 0.259, 0.795, 0.85 , 0.323, 0.818, 0.863, 0.196, 0.839,\n",
       "        0.845, 0.26 , 0.893, 0.889, 0.308, 0.9  , 0.891, 0.313, 0.896,\n",
       "        0.895, 0.37 , 0.9  , 0.791, 0.108, 0.887, 0.89 , 0.199, 0.891,\n",
       "        0.895, 0.312, 0.895, 0.897, 0.495, 0.897, 0.848, 0.16 , 0.827,\n",
       "        0.893, 0.241, 0.892, 0.898, 0.335, 0.893, 0.891, 0.366, 0.891,\n",
       "        0.89 , 0.286, 0.874, 0.885, 0.17 , 0.892, 0.892, 0.309, 0.892,\n",
       "        0.894, 0.362, 0.893, 0.849, 0.842, 0.846, 0.896, 0.899, 0.901,\n",
       "        0.9  , 0.899, 0.897, 0.899, 0.9  , 0.901, 0.889, 0.849, 0.886,\n",
       "        0.904, 0.899, 0.895, 0.896, 0.899, 0.899, 0.9  , 0.894, 0.9  ,\n",
       "        0.854, 0.828, 0.856, 0.897, 0.898, 0.898, 0.9  , 0.899, 0.898,\n",
       "        0.9  , 0.899, 0.902, 0.85 , 0.858, 0.853, 0.893, 0.895, 0.898,\n",
       "        0.893, 0.898, 0.892, 0.893, 0.898, 0.898, 0.899, 0.901, 0.893,\n",
       "        0.898, 0.901, 0.903, 0.904, 0.908, 0.902, 0.907, 0.903, 0.903,\n",
       "        0.899, 0.892, 0.896, 0.902, 0.894, 0.898, 0.904, 0.908, 0.903,\n",
       "        0.906, 0.903, 0.905, 0.897, 0.898, 0.896, 0.9  , 0.904, 0.899,\n",
       "        0.907, 0.901, 0.903, 0.904, 0.902, 0.901, 0.899, 0.892, 0.895,\n",
       "        0.901, 0.9  , 0.902, 0.903, 0.903, 0.902, 0.901, 0.906, 0.901]),\n",
       " 'split2_test_f1_micro': array([0.923, 0.922, 0.924, 0.92 , 0.916, 0.924, 0.914, 0.928, 0.912,\n",
       "        0.916, 0.924, 0.917, 0.917, 0.915, 0.919, 0.917, 0.921, 0.92 ,\n",
       "        0.919, 0.923, 0.922, 0.917, 0.913, 0.917, 0.921, 0.92 , 0.916,\n",
       "        0.92 , 0.922, 0.924, 0.913, 0.92 , 0.924, 0.917, 0.919, 0.925,\n",
       "        0.919, 0.917, 0.926, 0.921, 0.923, 0.914, 0.923, 0.927, 0.919,\n",
       "        0.922, 0.92 , 0.928, 0.918, 0.923, 0.921, 0.916, 0.919, 0.924,\n",
       "        0.918, 0.91 , 0.917, 0.916, 0.923, 0.915, 0.927, 0.919, 0.923,\n",
       "        0.917, 0.917, 0.913, 0.916, 0.911, 0.914, 0.915, 0.909, 0.914,\n",
       "        0.924, 0.917, 0.916, 0.918, 0.924, 0.926, 0.928, 0.922, 0.919,\n",
       "        0.913, 0.91 , 0.914, 0.919, 0.918, 0.923, 0.927, 0.924, 0.927,\n",
       "        0.924, 0.925, 0.925, 0.922, 0.921, 0.919, 0.586, 0.259, 0.747,\n",
       "        0.8  , 0.259, 0.783, 0.823, 0.198, 0.857, 0.856, 0.271, 0.869,\n",
       "        0.727, 0.196, 0.747, 0.804, 0.196, 0.786, 0.839, 0.189, 0.812,\n",
       "        0.835, 0.229, 0.869, 0.705, 0.196, 0.712, 0.799, 0.117, 0.775,\n",
       "        0.827, 0.391, 0.826, 0.838, 0.085, 0.828, 0.735, 0.232, 0.737,\n",
       "        0.793, 0.259, 0.789, 0.81 , 0.097, 0.819, 0.871, 0.255, 0.834,\n",
       "        0.861, 0.179, 0.868, 0.915, 0.066, 0.914, 0.912, 0.409, 0.91 ,\n",
       "        0.917, 0.142, 0.915, 0.853, 0.194, 0.866, 0.913, 0.295, 0.914,\n",
       "        0.92 , 0.223, 0.92 , 0.919, 0.158, 0.911, 0.91 , 0.102, 0.874,\n",
       "        0.907, 0.28 , 0.911, 0.922, 0.257, 0.916, 0.913, 0.437, 0.913,\n",
       "        0.911, 0.059, 0.864, 0.913, 0.228, 0.914, 0.918, 0.499, 0.918,\n",
       "        0.913, 0.42 , 0.919, 0.88 , 0.908, 0.876, 0.921, 0.913, 0.922,\n",
       "        0.915, 0.918, 0.917, 0.916, 0.917, 0.919, 0.875, 0.914, 0.87 ,\n",
       "        0.921, 0.914, 0.915, 0.916, 0.919, 0.919, 0.916, 0.916, 0.918,\n",
       "        0.862, 0.869, 0.852, 0.916, 0.921, 0.922, 0.918, 0.913, 0.92 ,\n",
       "        0.92 , 0.914, 0.915, 0.871, 0.875, 0.872, 0.912, 0.924, 0.924,\n",
       "        0.917, 0.916, 0.918, 0.92 , 0.921, 0.921, 0.919, 0.914, 0.914,\n",
       "        0.92 , 0.917, 0.921, 0.915, 0.92 , 0.92 , 0.917, 0.919, 0.917,\n",
       "        0.92 , 0.908, 0.908, 0.918, 0.917, 0.914, 0.918, 0.915, 0.916,\n",
       "        0.916, 0.92 , 0.917, 0.912, 0.912, 0.911, 0.917, 0.917, 0.917,\n",
       "        0.921, 0.918, 0.915, 0.916, 0.917, 0.913, 0.918, 0.913, 0.912,\n",
       "        0.916, 0.916, 0.917, 0.918, 0.919, 0.914, 0.915, 0.914, 0.919]),\n",
       " 'split3_test_f1_micro': array([0.936, 0.927, 0.934, 0.926, 0.929, 0.933, 0.934, 0.931, 0.924,\n",
       "        0.932, 0.932, 0.93 , 0.937, 0.925, 0.929, 0.936, 0.938, 0.927,\n",
       "        0.935, 0.933, 0.93 , 0.922, 0.922, 0.924, 0.934, 0.933, 0.933,\n",
       "        0.936, 0.927, 0.934, 0.93 , 0.937, 0.928, 0.929, 0.922, 0.929,\n",
       "        0.935, 0.93 , 0.935, 0.93 , 0.932, 0.929, 0.937, 0.929, 0.929,\n",
       "        0.933, 0.935, 0.929, 0.93 , 0.935, 0.927, 0.924, 0.928, 0.928,\n",
       "        0.918, 0.932, 0.936, 0.935, 0.928, 0.933, 0.936, 0.925, 0.928,\n",
       "        0.931, 0.93 , 0.928, 0.929, 0.927, 0.925, 0.922, 0.926, 0.924,\n",
       "        0.936, 0.935, 0.932, 0.933, 0.932, 0.938, 0.933, 0.929, 0.92 ,\n",
       "        0.923, 0.929, 0.932, 0.927, 0.938, 0.934, 0.927, 0.937, 0.936,\n",
       "        0.932, 0.932, 0.936, 0.939, 0.929, 0.931, 0.739, 0.297, 0.7  ,\n",
       "        0.799, 0.142, 0.786, 0.833, 0.148, 0.852, 0.851, 0.096, 0.85 ,\n",
       "        0.721, 0.117, 0.775, 0.814, 0.15 , 0.807, 0.834, 0.258, 0.841,\n",
       "        0.875, 0.248, 0.872, 0.756, 0.145, 0.774, 0.819, 0.13 , 0.823,\n",
       "        0.845, 0.096, 0.821, 0.851, 0.263, 0.852, 0.74 , 0.113, 0.717,\n",
       "        0.831, 0.117, 0.79 , 0.841, 0.114, 0.823, 0.889, 0.38 , 0.862,\n",
       "        0.865, 0.235, 0.848, 0.91 , 0.324, 0.914, 0.92 , 0.44 , 0.918,\n",
       "        0.917, 0.168, 0.923, 0.884, 0.194, 0.874, 0.911, 0.329, 0.916,\n",
       "        0.917, 0.185, 0.922, 0.919, 0.298, 0.923, 0.869, 0.19 , 0.874,\n",
       "        0.915, 0.102, 0.924, 0.93 , 0.2  , 0.92 , 0.918, 0.279, 0.923,\n",
       "        0.88 , 0.13 , 0.896, 0.915, 0.182, 0.918, 0.924, 0.282, 0.922,\n",
       "        0.922, 0.268, 0.926, 0.876, 0.887, 0.853, 0.925, 0.925, 0.928,\n",
       "        0.931, 0.929, 0.929, 0.933, 0.93 , 0.929, 0.882, 0.878, 0.876,\n",
       "        0.924, 0.93 , 0.924, 0.928, 0.926, 0.93 , 0.93 , 0.932, 0.931,\n",
       "        0.909, 0.922, 0.908, 0.927, 0.926, 0.928, 0.926, 0.927, 0.924,\n",
       "        0.932, 0.928, 0.931, 0.873, 0.879, 0.87 , 0.922, 0.927, 0.922,\n",
       "        0.926, 0.928, 0.928, 0.932, 0.931, 0.929, 0.926, 0.923, 0.928,\n",
       "        0.933, 0.933, 0.927, 0.931, 0.927, 0.932, 0.934, 0.927, 0.931,\n",
       "        0.924, 0.917, 0.928, 0.931, 0.928, 0.933, 0.934, 0.93 , 0.929,\n",
       "        0.932, 0.93 , 0.932, 0.917, 0.93 , 0.924, 0.926, 0.932, 0.928,\n",
       "        0.931, 0.931, 0.932, 0.932, 0.932, 0.93 , 0.927, 0.927, 0.925,\n",
       "        0.928, 0.93 , 0.927, 0.931, 0.936, 0.93 , 0.929, 0.93 , 0.934]),\n",
       " 'split4_test_f1_micro': array([0.927, 0.927, 0.924, 0.924, 0.928, 0.935, 0.926, 0.926, 0.928,\n",
       "        0.926, 0.932, 0.929, 0.93 , 0.926, 0.924, 0.922, 0.928, 0.933,\n",
       "        0.928, 0.926, 0.933, 0.933, 0.928, 0.927, 0.927, 0.931, 0.927,\n",
       "        0.925, 0.916, 0.929, 0.926, 0.922, 0.931, 0.921, 0.934, 0.93 ,\n",
       "        0.931, 0.929, 0.929, 0.924, 0.931, 0.931, 0.922, 0.93 , 0.933,\n",
       "        0.931, 0.929, 0.924, 0.925, 0.93 , 0.925, 0.919, 0.93 , 0.928,\n",
       "        0.924, 0.929, 0.923, 0.922, 0.926, 0.928, 0.922, 0.925, 0.93 ,\n",
       "        0.929, 0.921, 0.926, 0.932, 0.93 , 0.924, 0.922, 0.927, 0.92 ,\n",
       "        0.924, 0.928, 0.924, 0.931, 0.924, 0.928, 0.927, 0.921, 0.928,\n",
       "        0.932, 0.923, 0.934, 0.927, 0.93 , 0.928, 0.928, 0.933, 0.926,\n",
       "        0.929, 0.932, 0.933, 0.928, 0.931, 0.928, 0.711, 0.111, 0.645,\n",
       "        0.788, 0.196, 0.786, 0.831, 0.148, 0.827, 0.875, 0.25 , 0.889,\n",
       "        0.741, 0.147, 0.73 , 0.801, 0.273, 0.779, 0.833, 0.183, 0.837,\n",
       "        0.838, 0.038, 0.88 , 0.727, 0.196, 0.71 , 0.778, 0.038, 0.803,\n",
       "        0.845, 0.143, 0.814, 0.879, 0.06 , 0.858, 0.741, 0.111, 0.711,\n",
       "        0.807, 0.143, 0.781, 0.834, 0.319, 0.828, 0.845, 0.175, 0.877,\n",
       "        0.89 , 0.03 , 0.917, 0.925, 0.145, 0.923, 0.922, 0.239, 0.919,\n",
       "        0.922, 0.206, 0.924, 0.869, 0.24 , 0.909, 0.919, 0.26 , 0.917,\n",
       "        0.924, 0.233, 0.92 , 0.921, 0.36 , 0.928, 0.874, 0.079, 0.873,\n",
       "        0.921, 0.237, 0.92 , 0.921, 0.153, 0.922, 0.917, 0.195, 0.919,\n",
       "        0.866, 0.334, 0.871, 0.919, 0.139, 0.918, 0.925, 0.307, 0.925,\n",
       "        0.924, 0.327, 0.93 , 0.877, 0.878, 0.915, 0.926, 0.929, 0.92 ,\n",
       "        0.926, 0.927, 0.922, 0.921, 0.921, 0.922, 0.873, 0.873, 0.864,\n",
       "        0.927, 0.926, 0.924, 0.923, 0.925, 0.926, 0.922, 0.921, 0.926,\n",
       "        0.858, 0.871, 0.876, 0.924, 0.923, 0.929, 0.922, 0.923, 0.922,\n",
       "        0.922, 0.924, 0.926, 0.877, 0.88 , 0.879, 0.922, 0.925, 0.921,\n",
       "        0.928, 0.923, 0.922, 0.922, 0.925, 0.921, 0.928, 0.922, 0.92 ,\n",
       "        0.928, 0.926, 0.926, 0.927, 0.925, 0.928, 0.932, 0.929, 0.927,\n",
       "        0.921, 0.927, 0.913, 0.923, 0.925, 0.929, 0.926, 0.929, 0.923,\n",
       "        0.93 , 0.928, 0.926, 0.921, 0.919, 0.925, 0.923, 0.925, 0.925,\n",
       "        0.925, 0.926, 0.924, 0.926, 0.926, 0.926, 0.923, 0.924, 0.924,\n",
       "        0.926, 0.929, 0.925, 0.925, 0.927, 0.926, 0.927, 0.928, 0.925]),\n",
       " 'mean_test_f1_micro': array([0.9222, 0.9216, 0.9214, 0.9224, 0.9218, 0.926 , 0.9218, 0.926 ,\n",
       "        0.92  , 0.9202, 0.9234, 0.9194, 0.9244, 0.92  , 0.9206, 0.923 ,\n",
       "        0.9254, 0.924 , 0.9248, 0.9226, 0.9238, 0.9208, 0.9222, 0.921 ,\n",
       "        0.9226, 0.9228, 0.9186, 0.9218, 0.919 , 0.9242, 0.9232, 0.9224,\n",
       "        0.9256, 0.921 , 0.9198, 0.925 , 0.9232, 0.922 , 0.9258, 0.922 ,\n",
       "        0.9238, 0.9218, 0.9244, 0.9252, 0.9234, 0.924 , 0.9272, 0.9226,\n",
       "        0.921 , 0.9236, 0.9216, 0.9178, 0.9218, 0.925 , 0.9186, 0.9212,\n",
       "        0.926 , 0.924 , 0.923 , 0.9196, 0.9236, 0.9204, 0.9218, 0.9228,\n",
       "        0.921 , 0.9206, 0.923 , 0.9214, 0.9198, 0.9222, 0.9224, 0.92  ,\n",
       "        0.9234, 0.9212, 0.921 , 0.9242, 0.922 , 0.926 , 0.9248, 0.9218,\n",
       "        0.9208, 0.9188, 0.9182, 0.9222, 0.9206, 0.9236, 0.9226, 0.924 ,\n",
       "        0.926 , 0.9258, 0.9244, 0.926 , 0.9262, 0.9248, 0.9246, 0.9248,\n",
       "        0.7072, 0.21  , 0.714 , 0.8016, 0.1706, 0.7948, 0.8322, 0.1614,\n",
       "        0.8458, 0.8578, 0.222 , 0.8668, 0.7308, 0.1456, 0.7364, 0.8088,\n",
       "        0.178 , 0.7992, 0.8328, 0.1962, 0.826 , 0.8546, 0.1746, 0.87  ,\n",
       "        0.746 , 0.1468, 0.7442, 0.8022, 0.0898, 0.8048, 0.8346, 0.2174,\n",
       "        0.8224, 0.8634, 0.1324, 0.8486, 0.7498, 0.1622, 0.7328, 0.8106,\n",
       "        0.2006, 0.7976, 0.8356, 0.2226, 0.8226, 0.8634, 0.2242, 0.852 ,\n",
       "        0.8706, 0.2326, 0.879 , 0.908 , 0.2364, 0.908 , 0.9074, 0.3462,\n",
       "        0.9092, 0.9118, 0.262 , 0.9132, 0.8604, 0.204 , 0.882 , 0.9064,\n",
       "        0.2518, 0.9086, 0.9132, 0.2494, 0.912 , 0.9122, 0.3142, 0.9134,\n",
       "        0.8688, 0.1728, 0.8684, 0.9084, 0.2566, 0.9102, 0.914 , 0.246 ,\n",
       "        0.9104, 0.9078, 0.2978, 0.9094, 0.8878, 0.2276, 0.8818, 0.9078,\n",
       "        0.18  , 0.9094, 0.9122, 0.2972, 0.9124, 0.9112, 0.3532, 0.915 ,\n",
       "        0.8676, 0.8746, 0.8784, 0.916 , 0.9164, 0.9176, 0.918 , 0.9184,\n",
       "        0.9168, 0.9168, 0.9174, 0.9184, 0.8776, 0.8832, 0.8714, 0.9188,\n",
       "        0.917 , 0.9138, 0.916 , 0.9162, 0.9182, 0.9174, 0.9162, 0.918 ,\n",
       "        0.872 , 0.8706, 0.8726, 0.916 , 0.9156, 0.9178, 0.9162, 0.9158,\n",
       "        0.9164, 0.9182, 0.9164, 0.9192, 0.8738, 0.8732, 0.8686, 0.9132,\n",
       "        0.9168, 0.9156, 0.916 , 0.9158, 0.9154, 0.9164, 0.9176, 0.9164,\n",
       "        0.9162, 0.9142, 0.9134, 0.9186, 0.9182, 0.9184, 0.9206, 0.9202,\n",
       "        0.9198, 0.9226, 0.92  , 0.9202, 0.9156, 0.9112, 0.911 , 0.919 ,\n",
       "        0.9154, 0.919 , 0.9204, 0.9196, 0.9174, 0.9206, 0.9206, 0.9206,\n",
       "        0.9116, 0.9158, 0.9136, 0.9172, 0.9192, 0.917 , 0.9212, 0.9194,\n",
       "        0.9182, 0.9192, 0.9196, 0.9178, 0.9154, 0.914 , 0.9122, 0.9176,\n",
       "        0.919 , 0.918 , 0.9194, 0.9208, 0.9188, 0.9186, 0.9194, 0.9198]),\n",
       " 'std_test_f1_micro': array([0.00962081, 0.0058515 , 0.01091055, 0.00293939, 0.00556417,\n",
       "        0.0069282 , 0.00733212, 0.00394968, 0.00748331, 0.007494  ,\n",
       "        0.00798999, 0.00886792, 0.00895768, 0.00460435, 0.00845222,\n",
       "        0.00769415, 0.00741889, 0.00583095, 0.00705408, 0.00671118,\n",
       "        0.00733212, 0.00661513, 0.00667533, 0.00477493, 0.00717217,\n",
       "        0.00798499, 0.0109654 , 0.00842378, 0.00651153, 0.00713863,\n",
       "        0.00661513, 0.00791454, 0.00349857, 0.00657267, 0.00810925,\n",
       "        0.0043359 , 0.00917388, 0.00663325, 0.00685274, 0.00477493,\n",
       "        0.00679412, 0.00691086, 0.00668132, 0.00453431, 0.00646838,\n",
       "        0.00726636, 0.00487442, 0.00618385, 0.00797496, 0.00913455,\n",
       "        0.00564269, 0.00426146, 0.00711056, 0.00252982, 0.00293939,\n",
       "        0.0082801 , 0.00622896, 0.0063561 , 0.00368782, 0.00915642,\n",
       "        0.00811419, 0.00714423, 0.00775629, 0.00601332, 0.00493964,\n",
       "        0.00557136, 0.00632456, 0.00665132, 0.00453431, 0.00444522,\n",
       "        0.00685857, 0.0045607 , 0.00793977, 0.00936803, 0.00867179,\n",
       "        0.00679412, 0.00669328, 0.00745654, 0.00620967, 0.00396989,\n",
       "        0.00757364, 0.01034215, 0.00832827, 0.01036147, 0.00628013,\n",
       "        0.00924338, 0.00870862, 0.00481664, 0.0086487 , 0.0059127 ,\n",
       "        0.00567803, 0.00622896, 0.00724983, 0.00823165, 0.0054626 ,\n",
       "        0.00426146, 0.06230698, 0.07039886, 0.0383875 , 0.01492113,\n",
       "        0.0955774 , 0.01553577, 0.01135606, 0.0312    , 0.01551   ,\n",
       "        0.00901998, 0.06414671, 0.01393413, 0.02523807, 0.02835913,\n",
       "        0.02263272, 0.00798499, 0.05247857, 0.01586695, 0.00416653,\n",
       "        0.03342694, 0.01152389, 0.01640244, 0.08364592, 0.00583095,\n",
       "        0.02605379, 0.05368575, 0.02913692, 0.02162776, 0.03865954,\n",
       "        0.02115089, 0.00989141, 0.10273188, 0.00508331, 0.01737354,\n",
       "        0.08294962, 0.014827  , 0.01378985, 0.05752008, 0.02713227,\n",
       "        0.01754537, 0.05953688, 0.01826034, 0.01377824, 0.09832721,\n",
       "        0.00372022, 0.01586947, 0.08984743, 0.01570987, 0.01796218,\n",
       "        0.13851585, 0.0237571 , 0.01226377, 0.11019728, 0.01201666,\n",
       "        0.01341044, 0.0715525 , 0.00879545, 0.0095373 , 0.1133843 ,\n",
       "        0.00974474, 0.03870711, 0.05850128, 0.01508642, 0.01046136,\n",
       "        0.05731806, 0.00976934, 0.01018627, 0.04694933, 0.01093618,\n",
       "        0.0095163 , 0.11170031, 0.01103812, 0.02376047, 0.08936979,\n",
       "        0.0221504 , 0.00945727, 0.10272799, 0.01146124, 0.0130384 ,\n",
       "        0.06376206, 0.01139474, 0.01057166, 0.09199   , 0.0118254 ,\n",
       "        0.01481081, 0.11221872, 0.01541947, 0.01203993, 0.0286007 ,\n",
       "        0.00991161, 0.0130292 , 0.1300283 , 0.01227355, 0.01137365,\n",
       "        0.05247247, 0.01349074, 0.01259524, 0.02288755, 0.02682238,\n",
       "        0.01115347, 0.01046136, 0.00904655, 0.01063955, 0.0106132 ,\n",
       "        0.01070327, 0.01096175, 0.00976934, 0.00932952, 0.00708802,\n",
       "        0.02281578, 0.00893532, 0.00798499, 0.0108074 , 0.01068457,\n",
       "        0.01089954, 0.00994786, 0.01068457, 0.00987117, 0.0124    ,\n",
       "        0.01063955, 0.02006988, 0.03005728, 0.01983532, 0.01044988,\n",
       "        0.01032666, 0.0116    , 0.00890842, 0.00968297, 0.00941488,\n",
       "        0.0104    , 0.01001199, 0.01006777, 0.01530229, 0.00793473,\n",
       "        0.00854634, 0.01075918, 0.01193985, 0.00956243, 0.01244186,\n",
       "        0.01020588, 0.01232234, 0.01293986, 0.01141227, 0.01049952,\n",
       "        0.01087014, 0.00803492, 0.01162067, 0.0121918 , 0.01090688,\n",
       "        0.0088    , 0.00985089, 0.00661513, 0.0104    , 0.00993177,\n",
       "        0.00920869, 0.00976524, 0.00891291, 0.01151347, 0.01027619,\n",
       "        0.0095289 , 0.01197664, 0.0123774 , 0.00991161, 0.00854634,\n",
       "        0.00868562, 0.00954149, 0.00954149, 0.00917824, 0.0081388 ,\n",
       "        0.01059056, 0.01055651, 0.00910824, 0.00932523, 0.0100995 ,\n",
       "        0.00790949, 0.0102098 , 0.0096623 , 0.0095163 , 0.01013114,\n",
       "        0.01022546, 0.00997196, 0.01228007, 0.01140877, 0.00956243,\n",
       "        0.01088118, 0.00880909, 0.00935094, 0.01088853, 0.00992774,\n",
       "        0.01007174, 0.00889044, 0.0107963 ]),\n",
       " 'rank_test_f1_micro': array([ 53,  67,  69,  50,  61,   3,  61,   4,  94,  92,  35, 105,  21,\n",
       "         94,  82,  40,  12,  26,  16,  45,  30,  79,  53,  74,  49,  44,\n",
       "        120,  61, 112,  24,  38,  50,  11,  74,  98,  14,  39,  57,   9,\n",
       "         57,  31,  61,  21,  13,  35,  26,   1,  45,  74,  32,  68, 134,\n",
       "         61,  14, 119,  72,   4,  26,  40, 102,  32,  89,  61,  43,  74,\n",
       "         84,  40,  69,  98,  53,  50,  94,  35,  71,  74,  24,  57,   4,\n",
       "         16,  60,  79, 116, 126,  56,  84,  34,  45,  26,   4,  10,  21,\n",
       "          4,   2,  16,  20,  16, 256, 274, 255, 245, 282, 248, 237, 284,\n",
       "        233, 229, 272, 225, 254, 286, 252, 242, 279, 246, 236, 277, 238,\n",
       "        230, 280, 220, 250, 285, 251, 244, 288, 243, 235, 273, 240, 226,\n",
       "        287, 232, 249, 283, 253, 241, 276, 247, 234, 271, 239, 226, 270,\n",
       "        231, 218, 268, 209, 200, 267, 199, 203, 258, 196, 187, 262, 179,\n",
       "        228, 275, 207, 204, 264, 197, 179, 265, 186, 184, 259, 177, 221,\n",
       "        281, 223, 198, 263, 193, 173, 266, 192, 201, 260, 194, 205, 269,\n",
       "        208, 202, 278, 194, 184, 261, 182, 189, 257, 171, 224, 212, 210,\n",
       "        158, 149, 137, 132, 123, 148, 146, 140, 124, 211, 206, 217, 118,\n",
       "        144, 175, 158, 154, 126, 140, 154, 132, 216, 218, 215, 158, 165,\n",
       "        134, 154, 162, 151, 126, 151, 109, 213, 214, 222, 181, 146, 165,\n",
       "        158, 162, 168, 151, 137, 149, 154, 172, 177, 120, 126, 124,  84,\n",
       "         92,  98,  45,  94,  91, 165, 189, 191, 112, 168, 112,  89, 102,\n",
       "        140,  84,  82,  84, 188, 162, 176, 143, 109, 144,  72, 105, 126,\n",
       "        109, 102, 134, 168, 173, 183, 137, 112, 131, 105,  79, 116, 120,\n",
       "        105,  98]),\n",
       " 'split0_test_roc_auc_ovo': array([0.99550303, 0.99463372, 0.9937685 , 0.99404313, 0.99453152,\n",
       "        0.99481876, 0.99246208, 0.99355304, 0.99352636, 0.99314256,\n",
       "        0.9942421 , 0.99252552, 0.99488829, 0.99419982, 0.99534771,\n",
       "        0.99415336, 0.99404867, 0.99472324, 0.99434887, 0.99362131,\n",
       "        0.99425417, 0.99353066, 0.99345139, 0.99393279, 0.99480314,\n",
       "        0.99424648, 0.99503584, 0.99489278, 0.99558111, 0.99475009,\n",
       "        0.99454152, 0.99366785, 0.99458654, 0.99385562, 0.9927892 ,\n",
       "        0.9949361 , 0.99511786, 0.99482112, 0.99441111, 0.99500311,\n",
       "        0.99446406, 0.99453517, 0.99454048, 0.99434692, 0.9945242 ,\n",
       "        0.99485315, 0.99336126, 0.99362137, 0.99524599, 0.99592105,\n",
       "        0.99558659, 0.99384   , 0.99469666, 0.99462155, 0.99245781,\n",
       "        0.99406482, 0.99522891, 0.99390158, 0.9937443 , 0.99179789,\n",
       "        0.99321717, 0.99515923, 0.99509682, 0.99414289, 0.9941086 ,\n",
       "        0.99467476, 0.99374567, 0.99330961, 0.99447573, 0.99301026,\n",
       "        0.9933044 , 0.99334275, 0.99493108, 0.99411319, 0.9960232 ,\n",
       "        0.99475862, 0.99525127, 0.99391536, 0.99347777, 0.99427941,\n",
       "        0.99349112, 0.99322149, 0.9928718 , 0.99406432, 0.99514667,\n",
       "        0.99457775, 0.99497048, 0.99517729, 0.99382103, 0.99422968,\n",
       "        0.99509009, 0.99376002, 0.9934723 , 0.99208407, 0.99176066,\n",
       "        0.99334682, 0.94005113, 0.56158207, 0.95337161, 0.97702026,\n",
       "        0.37012361, 0.96412663, 0.9855782 , 0.44267142, 0.98049024,\n",
       "        0.98459528, 0.53873937, 0.98241462, 0.96042268, 0.51229334,\n",
       "        0.93977222, 0.97884825, 0.42135191, 0.96517363, 0.97883099,\n",
       "        0.45119223, 0.97721825, 0.98080042, 0.49425943, 0.98498934,\n",
       "        0.94940965, 0.37012787, 0.95676868, 0.97293132, 0.58744275,\n",
       "        0.98246503, 0.98464452, 0.46467427, 0.97824653, 0.98565388,\n",
       "        0.58958209, 0.97984383, 0.92485574, 0.50741996, 0.93856148,\n",
       "        0.9723316 , 0.82698584, 0.96730958, 0.98229909, 0.52779118,\n",
       "        0.97337592, 0.98311872, 0.44404626, 0.98510337, 0.99123898,\n",
       "        0.77951458, 0.98168277, 0.99234369, 0.63478265, 0.99193696,\n",
       "        0.992821  , 0.64438419, 0.99352692, 0.99369776, 0.77709638,\n",
       "        0.99330911, 0.99064266, 0.4831066 , 0.98596627, 0.99324555,\n",
       "        0.58908317, 0.99282179, 0.9937464 , 0.6921058 , 0.99315218,\n",
       "        0.99341962, 0.72254455, 0.99373341, 0.96904747, 0.36593629,\n",
       "        0.9918531 , 0.99180575, 0.81507523, 0.99309045, 0.99233742,\n",
       "        0.63392021, 0.99362545, 0.99364143, 0.57944364, 0.99379173,\n",
       "        0.98878435, 0.73271998, 0.98934705, 0.99315696, 0.60290581,\n",
       "        0.99288123, 0.99345134, 0.42812141, 0.99307914, 0.99370034,\n",
       "        0.69178779, 0.99349514, 0.96241899, 0.97438111, 0.99083292,\n",
       "        0.99432609, 0.99456317, 0.99453637, 0.99491973, 0.99497406,\n",
       "        0.99464758, 0.99496027, 0.99499638, 0.99492748, 0.97374351,\n",
       "        0.98727699, 0.98714163, 0.99340049, 0.99398527, 0.99336066,\n",
       "        0.9947766 , 0.99451423, 0.9949517 , 0.99503472, 0.99521465,\n",
       "        0.9950003 , 0.9686244 , 0.98413941, 0.97044108, 0.99403877,\n",
       "        0.99352373, 0.99463029, 0.99521131, 0.99472842, 0.99451206,\n",
       "        0.99513118, 0.99508802, 0.99518288, 0.99031862, 0.9685745 ,\n",
       "        0.97606774, 0.99418533, 0.99393819, 0.99424815, 0.99459936,\n",
       "        0.99461788, 0.99481558, 0.99445095, 0.99468356, 0.99480358,\n",
       "        0.99431215, 0.99463242, 0.99417947, 0.99506737, 0.99508321,\n",
       "        0.9950602 , 0.9954431 , 0.99517865, 0.99499279, 0.99493793,\n",
       "        0.99556704, 0.99512445, 0.99447131, 0.99427382, 0.99282681,\n",
       "        0.9950512 , 0.99476428, 0.99519119, 0.99550122, 0.99489199,\n",
       "        0.99518793, 0.99549959, 0.99520136, 0.99486816, 0.9942778 ,\n",
       "        0.99397017, 0.99290077, 0.99490191, 0.99470996, 0.99452491,\n",
       "        0.99511694, 0.99525001, 0.99503292, 0.99520007, 0.99531887,\n",
       "        0.99530891, 0.99409669, 0.99303487, 0.99285469, 0.99504148,\n",
       "        0.99477882, 0.9950334 , 0.99499743, 0.99545035, 0.99503693,\n",
       "        0.99507327, 0.9951464 , 0.99537974]),\n",
       " 'split1_test_roc_auc_ovo': array([0.99490335, 0.99484322, 0.99203882, 0.99498723, 0.99487416,\n",
       "        0.99454469, 0.99474791, 0.99434822, 0.99428878, 0.99436014,\n",
       "        0.9944177 , 0.99359349, 0.99504771, 0.99559695, 0.99502042,\n",
       "        0.99535652, 0.99464712, 0.99524501, 0.99474798, 0.99380447,\n",
       "        0.99460806, 0.99404567, 0.99354594, 0.99477741, 0.99498488,\n",
       "        0.994694  , 0.99289599, 0.99481142, 0.99430585, 0.99494531,\n",
       "        0.99397316, 0.99386555, 0.99456902, 0.99341569, 0.99399951,\n",
       "        0.99450566, 0.99489464, 0.99509768, 0.99501251, 0.99531438,\n",
       "        0.99527055, 0.99575241, 0.99537257, 0.99569676, 0.99519999,\n",
       "        0.99459009, 0.99499638, 0.99547377, 0.99478294, 0.99437507,\n",
       "        0.99507788, 0.99519694, 0.99395712, 0.99410955, 0.99289316,\n",
       "        0.9928432 , 0.99472853, 0.99321943, 0.99301827, 0.99214692,\n",
       "        0.99525166, 0.99513381, 0.99458617, 0.9950174 , 0.99515358,\n",
       "        0.99442789, 0.99475544, 0.9931691 , 0.99436002, 0.9937504 ,\n",
       "        0.99358737, 0.99438804, 0.99482828, 0.99466125, 0.99529711,\n",
       "        0.9949682 , 0.99508458, 0.99459677, 0.99501166, 0.99517716,\n",
       "        0.99563166, 0.99401416, 0.99402587, 0.99355065, 0.99395862,\n",
       "        0.99484043, 0.99487044, 0.99510052, 0.99548048, 0.99546228,\n",
       "        0.9955163 , 0.9936324 , 0.9950468 , 0.99426751, 0.99467839,\n",
       "        0.99492766, 0.95649103, 0.45612917, 0.94880328, 0.97665657,\n",
       "        0.46966823, 0.95773204, 0.98176215, 0.64477064, 0.97701756,\n",
       "        0.98419695, 0.29578833, 0.9815381 , 0.95555382, 0.45952426,\n",
       "        0.95713411, 0.95976628, 0.39457866, 0.9644374 , 0.97817435,\n",
       "        0.67369659, 0.97908507, 0.98397238, 0.64147647, 0.98417578,\n",
       "        0.9547494 , 0.44717072, 0.94706527, 0.95750594, 0.544136  ,\n",
       "        0.95832597, 0.97916565, 0.61389469, 0.98094826, 0.98525051,\n",
       "        0.62843945, 0.97989563, 0.95924705, 0.61297698, 0.924311  ,\n",
       "        0.97885003, 0.68934345, 0.96901581, 0.97933831, 0.46840052,\n",
       "        0.98220326, 0.98269112, 0.60405809, 0.98057537, 0.95826735,\n",
       "        0.4966557 , 0.98847723, 0.99044582, 0.79117315, 0.99186088,\n",
       "        0.99202618, 0.61454991, 0.99225234, 0.99241945, 0.74828558,\n",
       "        0.99299906, 0.9533521 , 0.64071316, 0.99025378, 0.99148591,\n",
       "        0.59854451, 0.99171631, 0.99152146, 0.59325448, 0.9925626 ,\n",
       "        0.99229778, 0.64713997, 0.99276468, 0.9793103 , 0.46990133,\n",
       "        0.98164519, 0.99128096, 0.60246374, 0.99013016, 0.99230577,\n",
       "        0.82041846, 0.99249083, 0.9923689 , 0.64316557, 0.99216025,\n",
       "        0.99050292, 0.52625179, 0.98427347, 0.99064688, 0.71162068,\n",
       "        0.99107228, 0.99164012, 0.80326618, 0.99240292, 0.99216676,\n",
       "        0.64638921, 0.99230338, 0.97705537, 0.96818844, 0.964432  ,\n",
       "        0.99329681, 0.99330826, 0.99354862, 0.99364761, 0.99320969,\n",
       "        0.99358427, 0.99399362, 0.99397301, 0.99402414, 0.98818987,\n",
       "        0.98173026, 0.98691078, 0.99263498, 0.99287426, 0.99308712,\n",
       "        0.99368038, 0.99356393, 0.99381508, 0.99387156, 0.99396965,\n",
       "        0.99399006, 0.96983731, 0.97354474, 0.96729911, 0.99255505,\n",
       "        0.99265266, 0.9927617 , 0.99394105, 0.99351161, 0.99391542,\n",
       "        0.99418601, 0.99413504, 0.99415334, 0.98504996, 0.96637751,\n",
       "        0.96384465, 0.99227592, 0.99245761, 0.9926018 , 0.99343535,\n",
       "        0.99338295, 0.99348045, 0.99352361, 0.99354751, 0.99369985,\n",
       "        0.99162561, 0.99269488, 0.99182904, 0.99419146, 0.99412611,\n",
       "        0.99425633, 0.99460338, 0.9944744 , 0.99467711, 0.99477107,\n",
       "        0.99468979, 0.99471959, 0.99229043, 0.99248742, 0.99221175,\n",
       "        0.99391103, 0.99400075, 0.99388022, 0.99465684, 0.99445757,\n",
       "        0.99440583, 0.99487452, 0.99485904, 0.99492025, 0.99222276,\n",
       "        0.99262793, 0.98973969, 0.99425503, 0.99373059, 0.99416539,\n",
       "        0.99462554, 0.9944875 , 0.99446294, 0.99478207, 0.99473727,\n",
       "        0.99439664, 0.99277219, 0.99295772, 0.99297265, 0.99414047,\n",
       "        0.99375034, 0.99363051, 0.99443672, 0.99437597, 0.99439775,\n",
       "        0.99438568, 0.99480919, 0.99440803]),\n",
       " 'split2_test_roc_auc_ovo': array([0.99588897, 0.99447617, 0.99565687, 0.99311488, 0.9941156 ,\n",
       "        0.99507525, 0.99478082, 0.99518227, 0.99318989, 0.99470014,\n",
       "        0.99503353, 0.99344798, 0.99493847, 0.99497097, 0.99487432,\n",
       "        0.99540136, 0.99618663, 0.99496971, 0.99433104, 0.99439016,\n",
       "        0.99448933, 0.99447777, 0.99367366, 0.99484516, 0.99549293,\n",
       "        0.9936929 , 0.99442454, 0.99383725, 0.99475282, 0.99546363,\n",
       "        0.9936729 , 0.99597961, 0.99566974, 0.99291421, 0.99426567,\n",
       "        0.99565703, 0.99464585, 0.99593758, 0.99388025, 0.99560731,\n",
       "        0.99555795, 0.99453515, 0.99523768, 0.99499522, 0.99591458,\n",
       "        0.99397792, 0.99499985, 0.99530328, 0.9941559 , 0.99561384,\n",
       "        0.99349788, 0.9942083 , 0.99564278, 0.9953583 , 0.99466454,\n",
       "        0.99427655, 0.99346202, 0.99094311, 0.99481169, 0.99283962,\n",
       "        0.99557954, 0.99455849, 0.99468459, 0.99587852, 0.99420101,\n",
       "        0.99382456, 0.99523787, 0.99419075, 0.99450828, 0.99371955,\n",
       "        0.99188849, 0.99085   , 0.99410595, 0.99455417, 0.99566148,\n",
       "        0.99569989, 0.9959233 , 0.99450478, 0.99514348, 0.99489313,\n",
       "        0.99499288, 0.99435177, 0.9941335 , 0.99041884, 0.99509558,\n",
       "        0.99535635, 0.99532266, 0.99590893, 0.99533744, 0.99611937,\n",
       "        0.99516027, 0.99441921, 0.99592192, 0.99493421, 0.99373258,\n",
       "        0.9951808 , 0.90223816, 0.51756977, 0.94250292, 0.9689119 ,\n",
       "        0.55183612, 0.95264345, 0.97048436, 0.4346142 , 0.98489527,\n",
       "        0.97976948, 0.48645635, 0.98562578, 0.9542697 , 0.51350033,\n",
       "        0.95266647, 0.97268467, 0.45443819, 0.97269009, 0.9779868 ,\n",
       "        0.51270309, 0.9737216 , 0.98245989, 0.59113067, 0.98592007,\n",
       "        0.92247709, 0.6085051 , 0.92949376, 0.9564844 , 0.64172438,\n",
       "        0.96224394, 0.97041948, 0.60972808, 0.97534198, 0.98325479,\n",
       "        0.63131094, 0.98028841, 0.93609936, 0.52467275, 0.95644704,\n",
       "        0.97574732, 0.59007671, 0.9753338 , 0.97782647, 0.67946525,\n",
       "        0.9818514 , 0.9836348 , 0.59299003, 0.98216085, 0.98270037,\n",
       "        0.70897929, 0.97865595, 0.99277163, 0.54088141, 0.9931068 ,\n",
       "        0.99317535, 0.6970916 , 0.99294594, 0.99307011, 0.43345418,\n",
       "        0.99305206, 0.97296475, 0.65551004, 0.98140519, 0.99257794,\n",
       "        0.48480574, 0.99242109, 0.99288792, 0.77215335, 0.99329069,\n",
       "        0.99291082, 0.69471271, 0.99289248, 0.9909431 , 0.4231702 ,\n",
       "        0.98748047, 0.99213787, 0.67362525, 0.99162073, 0.99286616,\n",
       "        0.61883078, 0.99273458, 0.99330147, 0.82192257, 0.99272894,\n",
       "        0.99179637, 0.53205076, 0.97160723, 0.99252026, 0.47046331,\n",
       "        0.99330725, 0.99337265, 0.84908952, 0.99316687, 0.99333073,\n",
       "        0.72749066, 0.99295583, 0.97620777, 0.99124159, 0.98052394,\n",
       "        0.99380251, 0.99355908, 0.99359235, 0.99407405, 0.99408864,\n",
       "        0.99392296, 0.99435259, 0.99452271, 0.993898  , 0.96820944,\n",
       "        0.99206441, 0.97189985, 0.99437328, 0.99395096, 0.9937773 ,\n",
       "        0.99404174, 0.99438475, 0.99414395, 0.99418702, 0.99389634,\n",
       "        0.99373792, 0.98080498, 0.97947199, 0.96218083, 0.99319997,\n",
       "        0.99361338, 0.99407818, 0.99396124, 0.99369396, 0.99440332,\n",
       "        0.99427033, 0.99414848, 0.99438246, 0.97285119, 0.97072202,\n",
       "        0.97370167, 0.99325905, 0.99410016, 0.99405571, 0.99404463,\n",
       "        0.99382811, 0.99375397, 0.99422135, 0.9941612 , 0.99428371,\n",
       "        0.99342907, 0.99263879, 0.99271685, 0.99444433, 0.99538214,\n",
       "        0.99571721, 0.99550829, 0.99537137, 0.9955617 , 0.99525457,\n",
       "        0.99523899, 0.99481324, 0.99353867, 0.99310198, 0.99359861,\n",
       "        0.99529361, 0.99460006, 0.99404905, 0.99463558, 0.99499298,\n",
       "        0.99471256, 0.99539605, 0.99546754, 0.99519584, 0.99239198,\n",
       "        0.99153257, 0.99384152, 0.99482951, 0.99512723, 0.99469865,\n",
       "        0.99546337, 0.99437907, 0.99454707, 0.99502099, 0.9950061 ,\n",
       "        0.99551488, 0.99270781, 0.99281286, 0.99280307, 0.99388832,\n",
       "        0.9945617 , 0.99484667, 0.99541347, 0.99494216, 0.99479059,\n",
       "        0.99442104, 0.99470702, 0.99539362]),\n",
       " 'split3_test_roc_auc_ovo': array([0.99688501, 0.99676873, 0.99691061, 0.9960695 , 0.99631073,\n",
       "        0.99631113, 0.99615577, 0.99605418, 0.99531096, 0.99467614,\n",
       "        0.99509452, 0.9960015 , 0.9968669 , 0.99654631, 0.9964427 ,\n",
       "        0.99688985, 0.99600163, 0.99621597, 0.99618252, 0.99629634,\n",
       "        0.9953883 , 0.99427899, 0.99500612, 0.99487837, 0.99696602,\n",
       "        0.99726647, 0.99736146, 0.99656152, 0.99688449, 0.99685823,\n",
       "        0.99560395, 0.9963218 , 0.99628916, 0.99607922, 0.99503521,\n",
       "        0.99567137, 0.99718671, 0.99551499, 0.99609038, 0.99657009,\n",
       "        0.99687392, 0.99684434, 0.99708649, 0.99675794, 0.99635318,\n",
       "        0.99647564, 0.99634243, 0.99670337, 0.99658982, 0.99617468,\n",
       "        0.99625046, 0.99582134, 0.99579449, 0.99524984, 0.99484672,\n",
       "        0.99640684, 0.99461164, 0.99417076, 0.99440581, 0.99532335,\n",
       "        0.99652327, 0.9960387 , 0.99611496, 0.9963826 , 0.99590373,\n",
       "        0.99640464, 0.99503909, 0.99463107, 0.99605444, 0.99533748,\n",
       "        0.99523308, 0.99407472, 0.99695167, 0.99708348, 0.99677954,\n",
       "        0.99649031, 0.99515674, 0.99665894, 0.99585688, 0.99675579,\n",
       "        0.99590614, 0.9952499 , 0.99512039, 0.99554429, 0.99672453,\n",
       "        0.99691104, 0.99650427, 0.99691621, 0.99666062, 0.9960613 ,\n",
       "        0.99651473, 0.99713866, 0.99539028, 0.99578636, 0.99573045,\n",
       "        0.99568702, 0.95786566, 0.71924421, 0.94410181, 0.95848266,\n",
       "        0.39756222, 0.96141707, 0.98178718, 0.44391692, 0.98254033,\n",
       "        0.98245821, 0.62676692, 0.98318907, 0.92734136, 0.5967738 ,\n",
       "        0.94709171, 0.95566123, 0.37437534, 0.97018821, 0.97816681,\n",
       "        0.3218641 , 0.97242542, 0.98727051, 0.58481028, 0.98023862,\n",
       "        0.91692851, 0.48957066, 0.95764853, 0.96918057, 0.4478255 ,\n",
       "        0.97494724, 0.98098499, 0.57167929, 0.97167929, 0.97987169,\n",
       "        0.49593987, 0.98533196, 0.93064344, 0.4764587 , 0.95523518,\n",
       "        0.96477516, 0.65649573, 0.95543774, 0.97812561, 0.53625224,\n",
       "        0.97913628, 0.98767011, 0.81865765, 0.98528566, 0.98169267,\n",
       "        0.70997323, 0.96355023, 0.99322012, 0.71388594, 0.99352393,\n",
       "        0.99414214, 0.62241962, 0.99492974, 0.99435341, 0.61317522,\n",
       "        0.99541519, 0.98570668, 0.48945959, 0.97076946, 0.99325596,\n",
       "        0.71060802, 0.9927128 , 0.99430849, 0.46482396, 0.99426668,\n",
       "        0.99512127, 0.64755635, 0.9947954 , 0.96982916, 0.66789251,\n",
       "        0.98670145, 0.99356269, 0.5137464 , 0.99429039, 0.99454162,\n",
       "        0.62131625, 0.99447758, 0.99446739, 0.43810751, 0.99490911,\n",
       "        0.97050931, 0.52711967, 0.9880981 , 0.99259774, 0.53334594,\n",
       "        0.99315652, 0.9949655 , 0.7649064 , 0.99417484, 0.99460384,\n",
       "        0.75279624, 0.99485542, 0.97839143, 0.97072574, 0.96579231,\n",
       "        0.99531544, 0.99482414, 0.9946817 , 0.99563219, 0.99509771,\n",
       "        0.99561715, 0.99613799, 0.99631943, 0.99602182, 0.97015799,\n",
       "        0.97974656, 0.97826128, 0.99503905, 0.99394753, 0.99480352,\n",
       "        0.99560585, 0.99530086, 0.99550688, 0.99596961, 0.995818  ,\n",
       "        0.99606569, 0.99200081, 0.99316208, 0.9880388 , 0.99486469,\n",
       "        0.99529395, 0.99562309, 0.99488497, 0.99561991, 0.99524165,\n",
       "        0.99596804, 0.99629014, 0.99596114, 0.96855633, 0.97108822,\n",
       "        0.9750553 , 0.99406362, 0.99504807, 0.99460817, 0.99521376,\n",
       "        0.99537848, 0.99562767, 0.99576851, 0.99587883, 0.99581046,\n",
       "        0.9956131 , 0.99437994, 0.99551786, 0.99641183, 0.99647757,\n",
       "        0.99600096, 0.99635234, 0.99598518, 0.99667226, 0.99628453,\n",
       "        0.99646385, 0.99666264, 0.99514268, 0.99446135, 0.99574418,\n",
       "        0.99641562, 0.99605529, 0.99679219, 0.99644844, 0.99645688,\n",
       "        0.99671636, 0.99678259, 0.9965731 , 0.99662365, 0.99443051,\n",
       "        0.99448819, 0.99463317, 0.99603743, 0.99590209, 0.99581285,\n",
       "        0.99651662, 0.99648726, 0.9966242 , 0.99648059, 0.99652932,\n",
       "        0.99656972, 0.99329407, 0.99549448, 0.9946305 , 0.99580212,\n",
       "        0.99572534, 0.99582348, 0.99663284, 0.99666575, 0.99637736,\n",
       "        0.99678294, 0.9966464 , 0.99658196]),\n",
       " 'split4_test_roc_auc_ovo': array([0.99665168, 0.99625387, 0.99594687, 0.99606631, 0.9958338 ,\n",
       "        0.99556747, 0.99569444, 0.99469249, 0.99563917, 0.99542249,\n",
       "        0.9949296 , 0.99491651, 0.99583843, 0.99631786, 0.99575464,\n",
       "        0.99609645, 0.99571479, 0.99591334, 0.99528488, 0.99563979,\n",
       "        0.99609729, 0.99525583, 0.99618838, 0.99546759, 0.9961278 ,\n",
       "        0.99663763, 0.99627403, 0.9961395 , 0.99626004, 0.99592798,\n",
       "        0.99552589, 0.99511997, 0.99573372, 0.99487679, 0.99560561,\n",
       "        0.99521533, 0.9963933 , 0.99596955, 0.99587513, 0.99594701,\n",
       "        0.99612187, 0.99592935, 0.99591725, 0.99575561, 0.99608294,\n",
       "        0.99557942, 0.9957641 , 0.99548524, 0.99601501, 0.99595829,\n",
       "        0.99655336, 0.9962176 , 0.99593946, 0.99559224, 0.99507058,\n",
       "        0.99519539, 0.99582646, 0.99504107, 0.99517688, 0.99477613,\n",
       "        0.99574393, 0.99634658, 0.99632023, 0.9964083 , 0.9957884 ,\n",
       "        0.99596179, 0.99496972, 0.99542934, 0.995099  , 0.99538636,\n",
       "        0.99338229, 0.99480329, 0.99610695, 0.99644528, 0.99616189,\n",
       "        0.99687791, 0.99590977, 0.99682648, 0.99619153, 0.99516674,\n",
       "        0.99463883, 0.99541117, 0.99629582, 0.99565533, 0.99666137,\n",
       "        0.99665442, 0.99670685, 0.99648282, 0.99629668, 0.99656677,\n",
       "        0.99604596, 0.99625824, 0.99629929, 0.99504349, 0.99459757,\n",
       "        0.99507882, 0.93371921, 0.56398236, 0.93407228, 0.96599487,\n",
       "        0.7011145 , 0.9592443 , 0.9791395 , 0.48307043, 0.97947045,\n",
       "        0.98511572, 0.38846027, 0.9889877 , 0.92134702, 0.43581082,\n",
       "        0.9231671 , 0.97619823, 0.55631481, 0.9591118 , 0.98312946,\n",
       "        0.62087511, 0.97605767, 0.98341014, 0.50467532, 0.98814655,\n",
       "        0.93931835, 0.55383289, 0.95336562, 0.97770353, 0.48502372,\n",
       "        0.98127691, 0.98652094, 0.46579562, 0.97203444, 0.98895281,\n",
       "        0.56090372, 0.98353741, 0.96588588, 0.55844387, 0.9497831 ,\n",
       "        0.96485963, 0.76632903, 0.959105  , 0.9720371 , 0.67137758,\n",
       "        0.98225434, 0.98491575, 0.42018362, 0.98697676, 0.99165185,\n",
       "        0.28752244, 0.9904511 , 0.99443833, 0.53364633, 0.99388275,\n",
       "        0.99510122, 0.62977661, 0.99498305, 0.99524266, 0.73294753,\n",
       "        0.99537815, 0.98079284, 0.68247863, 0.99382149, 0.99342478,\n",
       "        0.68611711, 0.99346036, 0.99563644, 0.59337658, 0.99521367,\n",
       "        0.99545838, 0.84725459, 0.99575999, 0.984195  , 0.55281392,\n",
       "        0.9735574 , 0.99388991, 0.73093657, 0.99431825, 0.99479622,\n",
       "        0.64466172, 0.99508987, 0.99488455, 0.71611754, 0.99498637,\n",
       "        0.98646128, 0.63136504, 0.9822084 , 0.99483612, 0.42076129,\n",
       "        0.99410429, 0.99488125, 0.68907577, 0.9952929 , 0.99519448,\n",
       "        0.65277411, 0.99535785, 0.97514475, 0.9742395 , 0.9921507 ,\n",
       "        0.99566414, 0.99572393, 0.99530032, 0.99557504, 0.99575578,\n",
       "        0.99582202, 0.99578135, 0.99568475, 0.99588296, 0.97189626,\n",
       "        0.97073757, 0.97541783, 0.99546568, 0.99540874, 0.99509776,\n",
       "        0.99558768, 0.99595085, 0.99586637, 0.99577509, 0.99591766,\n",
       "        0.99591948, 0.96915647, 0.98598   , 0.977773  , 0.995696  ,\n",
       "        0.99532932, 0.99578085, 0.99563076, 0.9956025 , 0.99572965,\n",
       "        0.99584108, 0.99588564, 0.9957745 , 0.96847837, 0.98744295,\n",
       "        0.97506642, 0.99494539, 0.99533287, 0.99541699, 0.99557495,\n",
       "        0.99565994, 0.995548  , 0.99573749, 0.99571919, 0.99567598,\n",
       "        0.99445385, 0.99550005, 0.99552491, 0.99590879, 0.99574951,\n",
       "        0.99611436, 0.99609721, 0.99609484, 0.995907  , 0.99622496,\n",
       "        0.99604452, 0.99603772, 0.9954808 , 0.99494572, 0.99102007,\n",
       "        0.99594559, 0.99633777, 0.99551953, 0.99588414, 0.99633092,\n",
       "        0.9959758 , 0.99656443, 0.99629608, 0.99625541, 0.99508727,\n",
       "        0.99539735, 0.99504067, 0.99629136, 0.99580242, 0.99621444,\n",
       "        0.995974  , 0.99598554, 0.9958544 , 0.99603188, 0.99619011,\n",
       "        0.99609951, 0.99507855, 0.99499965, 0.99540622, 0.99556443,\n",
       "        0.99574863, 0.9957137 , 0.99596732, 0.99601015, 0.99599856,\n",
       "        0.99585855, 0.99588327, 0.99596918]),\n",
       " 'mean_test_roc_auc_ovo': array([0.99596641, 0.99539514, 0.99486433, 0.99485621, 0.99513316,\n",
       "        0.99526346, 0.9947682 , 0.99476604, 0.99439103, 0.99446029,\n",
       "        0.99474349, 0.994097  , 0.99551596, 0.99552638, 0.99548796,\n",
       "        0.99557951, 0.99531977, 0.99541345, 0.99497906, 0.99475042,\n",
       "        0.99496743, 0.99431779, 0.9943731 , 0.99478026, 0.99567495,\n",
       "        0.9953075 , 0.99519837, 0.9952485 , 0.99555686, 0.99558905,\n",
       "        0.99466348, 0.99499095, 0.99536964, 0.99422831, 0.99433904,\n",
       "        0.9951971 , 0.99564767, 0.99546818, 0.99505387, 0.99568838,\n",
       "        0.99565767, 0.99551929, 0.99563089, 0.99551049, 0.99561498,\n",
       "        0.99509524, 0.9950928 , 0.99531741, 0.99535793, 0.99560859,\n",
       "        0.99539323, 0.99505684, 0.9952061 , 0.9949863 , 0.99398656,\n",
       "        0.99455736, 0.99477151, 0.99345519, 0.99423139, 0.99337678,\n",
       "        0.99526311, 0.99544736, 0.99536055, 0.99556594, 0.99503106,\n",
       "        0.99505873, 0.99474956, 0.99414597, 0.9948995 , 0.99424081,\n",
       "        0.99347913, 0.99349176, 0.99538479, 0.99537147, 0.99598464,\n",
       "        0.99575899, 0.99546513, 0.99530046, 0.99513627, 0.99525444,\n",
       "        0.99493213, 0.9944497 , 0.99448947, 0.99384669, 0.99551736,\n",
       "        0.995668  , 0.99567494, 0.99591715, 0.99551925, 0.99568788,\n",
       "        0.99566547, 0.99504171, 0.99522612, 0.99442313, 0.99409993,\n",
       "        0.99484423, 0.93807304, 0.56370151, 0.94457038, 0.96941325,\n",
       "        0.49806094, 0.9590327 , 0.97975028, 0.48980872, 0.98088277,\n",
       "        0.98322713, 0.46724225, 0.98435105, 0.94378692, 0.50358051,\n",
       "        0.94396632, 0.96863173, 0.44021178, 0.96632023, 0.97925768,\n",
       "        0.51606622, 0.9757016 , 0.98358267, 0.56327043, 0.98469407,\n",
       "        0.9365766 , 0.49384145, 0.94886837, 0.96676115, 0.54123047,\n",
       "        0.97185182, 0.98034712, 0.54515439, 0.9756501 , 0.98459673,\n",
       "        0.58123521, 0.98177945, 0.94334629, 0.53599445, 0.94486756,\n",
       "        0.97131275, 0.70584615, 0.96524039, 0.97792531, 0.57665735,\n",
       "        0.97976424, 0.9844061 , 0.57598713, 0.9840204 , 0.98111024,\n",
       "        0.59652905, 0.98056346, 0.99264392, 0.6428739 , 0.99286226,\n",
       "        0.99345318, 0.64164439, 0.9937276 , 0.99375668, 0.66099178,\n",
       "        0.99403071, 0.97669181, 0.5902536 , 0.98444324, 0.99279803,\n",
       "        0.61383171, 0.99262647, 0.99362014, 0.62314283, 0.99369716,\n",
       "        0.99384157, 0.71184164, 0.99398919, 0.97866501, 0.49594285,\n",
       "        0.98424752, 0.99253544, 0.66716944, 0.99269   , 0.99336944,\n",
       "        0.66782948, 0.99368366, 0.99373275, 0.63975137, 0.99371528,\n",
       "        0.98561085, 0.58990145, 0.98310685, 0.99275159, 0.54781941,\n",
       "        0.99290432, 0.99366217, 0.70689186, 0.99362334, 0.99379923,\n",
       "        0.6942476 , 0.99379352, 0.97384366, 0.97575528, 0.97874637,\n",
       "        0.994481  , 0.99439572, 0.99433187, 0.99476972, 0.99462518,\n",
       "        0.9947188 , 0.99504516, 0.99509926, 0.99495088, 0.97443941,\n",
       "        0.98231116, 0.97992627, 0.9941827 , 0.99403335, 0.99402527,\n",
       "        0.99473845, 0.99474292, 0.9948568 , 0.9949676 , 0.99496326,\n",
       "        0.99494269, 0.97608479, 0.98325964, 0.97314657, 0.9940709 ,\n",
       "        0.99408261, 0.99457482, 0.99472587, 0.99463128, 0.99476042,\n",
       "        0.99507933, 0.99510946, 0.99509086, 0.97705089, 0.97284104,\n",
       "        0.97274716, 0.99374586, 0.99417538, 0.99418617, 0.99457361,\n",
       "        0.99457347, 0.99464513, 0.99474038, 0.99479806, 0.99485472,\n",
       "        0.99388676, 0.99396922, 0.99395362, 0.99520476, 0.99536371,\n",
       "        0.99542981, 0.99560086, 0.99542089, 0.99556217, 0.99549461,\n",
       "        0.99560084, 0.99547153, 0.99418478, 0.99385406, 0.99308028,\n",
       "        0.99532341, 0.99515163, 0.99508643, 0.99542524, 0.99542607,\n",
       "        0.9953997 , 0.99582344, 0.99567942, 0.99557266, 0.99368207,\n",
       "        0.99360324, 0.99323116, 0.99526305, 0.99505446, 0.99508325,\n",
       "        0.99553929, 0.99531787, 0.99530431, 0.99550312, 0.99555634,\n",
       "        0.99557793, 0.99358986, 0.99385992, 0.99373343, 0.99488737,\n",
       "        0.99491297, 0.99500955, 0.99548956, 0.99548888, 0.99532024,\n",
       "        0.9953043 , 0.99543846, 0.99554651]),\n",
       " 'std_test_roc_auc_ovo': array([0.00072996, 0.00093306, 0.00174193, 0.00115299, 0.00081756,\n",
       "        0.00062287, 0.00127295, 0.00083473, 0.00095968, 0.00074517,\n",
       "        0.00034627, 0.00122008, 0.00075839, 0.00086486, 0.00056524,\n",
       "        0.00090549, 0.00082953, 0.0005649 , 0.0006946 , 0.00104693,\n",
       "        0.0006812 , 0.0005658 , 0.00106948, 0.00049081, 0.00079263,\n",
       "        0.00139398, 0.00153377, 0.00098256, 0.00094543, 0.00075626,\n",
       "        0.00078752, 0.0010754 , 0.00068152, 0.00112894, 0.00096037,\n",
       "        0.0004434 , 0.00097734, 0.00045384, 0.00084155, 0.00054043,\n",
       "        0.00080985, 0.0008849 , 0.00084984, 0.00080874, 0.00066569,\n",
       "        0.00086048, 0.00100283, 0.00098491, 0.00086456, 0.00064218,\n",
       "        0.00107795, 0.00091128, 0.00076081, 0.00054346, 0.00108694,\n",
       "        0.00119058, 0.00078285, 0.00138509, 0.00077072, 0.00141713,\n",
       "        0.00110502, 0.00065264, 0.00072336, 0.0008718 , 0.00076027,\n",
       "        0.00096908, 0.00052509, 0.0008412 , 0.00063204, 0.00095304,\n",
       "        0.00106378, 0.00140459, 0.00101268, 0.00116959, 0.00049838,\n",
       "        0.00082661, 0.00037237, 0.00120175, 0.00093798, 0.0008184 ,\n",
       "        0.00084903, 0.00080902, 0.0011506 , 0.00189938, 0.00104986,\n",
       "        0.0009475 , 0.0007772 , 0.00071168, 0.00098241, 0.00080936,\n",
       "        0.00054306, 0.00140667, 0.00097681, 0.0012649 , 0.00133027,\n",
       "        0.00079092, 0.020189  , 0.08706095, 0.00648202, 0.00695337,\n",
       "        0.11949613, 0.00385207, 0.00506738, 0.07928811, 0.0026809 ,\n",
       "        0.00194555, 0.11535353, 0.00268835, 0.01611895, 0.05548511,\n",
       "        0.0119084 , 0.0092183 , 0.06396064, 0.00474041, 0.00195711,\n",
       "        0.12471299, 0.00238867, 0.00213463, 0.05577141, 0.00259343,\n",
       "        0.01474652, 0.08276252, 0.01037649, 0.00842533, 0.06946471,\n",
       "        0.00986219, 0.00560258, 0.06689308, 0.00357064, 0.00298822,\n",
       "        0.04998831, 0.0022463 , 0.01622743, 0.04673216, 0.01206709,\n",
       "        0.00569029, 0.08299803, 0.00712948, 0.00334208, 0.08400167,\n",
       "        0.00339737, 0.00179482, 0.14258274, 0.00231689, 0.01215222,\n",
       "        0.18136048, 0.00953661, 0.00130307, 0.09943236, 0.00082437,\n",
       "        0.00106807, 0.0294158 , 0.00108157, 0.00098258, 0.12675796,\n",
       "        0.00112028, 0.01304872, 0.08596504, 0.00800295, 0.00071734,\n",
       "        0.08010583, 0.00056772, 0.0013781 , 0.1037345 , 0.00093526,\n",
       "        0.0012393 , 0.07357109, 0.00114484, 0.00839426, 0.10552415,\n",
       "        0.00625091, 0.00101529, 0.10364314, 0.00161672, 0.00108254,\n",
       "        0.07685614, 0.00099345, 0.00088552, 0.12909459, 0.0011347 ,\n",
       "        0.00775943, 0.08179963, 0.00629702, 0.00134287, 0.10217884,\n",
       "        0.00100228, 0.0012167 , 0.14893236, 0.00100851, 0.0010477 ,\n",
       "        0.04135111, 0.0011477 , 0.00581038, 0.00808165, 0.01184592,\n",
       "        0.0008925 , 0.00087844, 0.00067267, 0.00079473, 0.00088485,\n",
       "        0.00088879, 0.00081595, 0.00082983, 0.0008926 , 0.00711576,\n",
       "        0.00722008, 0.00613786, 0.00104144, 0.00080643, 0.00079237,\n",
       "        0.00078485, 0.00081743, 0.00078028, 0.00083321, 0.00087524,\n",
       "        0.00095682, 0.00914647, 0.0065568 , 0.00899936, 0.00112502,\n",
       "        0.0010582 , 0.00110368, 0.00067533, 0.00090137, 0.00064408,\n",
       "        0.0007517 , 0.00087976, 0.00072305, 0.0089811 , 0.00749345,\n",
       "        0.00451439, 0.00090892, 0.00101134, 0.00091914, 0.00077312,\n",
       "        0.00087216, 0.00088997, 0.00088147, 0.00089433, 0.00080626,\n",
       "        0.00132703, 0.00112658, 0.00148393, 0.00084576, 0.00077462,\n",
       "        0.00069153, 0.00060655, 0.00058815, 0.00070125, 0.00064006,\n",
       "        0.00061705, 0.00075611, 0.00115632, 0.00091324, 0.00157658,\n",
       "        0.00085438, 0.00089468, 0.001062  , 0.00070389, 0.00081143,\n",
       "        0.00084502, 0.00072897, 0.00065198, 0.0007259 , 0.00115616,\n",
       "        0.00136934, 0.00189272, 0.00077351, 0.00079429, 0.00078918,\n",
       "        0.00065776, 0.00082325, 0.00082458, 0.00064486, 0.0006897 ,\n",
       "        0.00073858, 0.00089523, 0.00114558, 0.00107884, 0.00075827,\n",
       "        0.00075516, 0.00078577, 0.0007605 , 0.00079946, 0.00074692,\n",
       "        0.00091334, 0.0007312 , 0.00072085]),\n",
       " 'rank_test_roc_auc_ovo': array([  2,  52, 110, 112,  81,  69, 119, 120, 144, 140, 124, 158,  34,\n",
       "         30,  40,  21,  62,  50, 100, 122, 102, 148, 145, 116,   9,  65,\n",
       "         77,  73,  26,  20, 130,  98,  56, 151, 146,  78,  14,  42,  93,\n",
       "          6,  13,  31,  15,  35,  16,  84,  85,  64,  59,  17,  53,  91,\n",
       "         75,  99, 165, 137, 117, 191, 150, 193,  70,  44,  58,  24,  96,\n",
       "         90, 123, 156, 108, 149, 190, 189,  54,  55,   1,   5,  43,  68,\n",
       "         80,  72, 106, 141, 138, 171,  33,  11,  10,   3,  32,   7,  12,\n",
       "         95,  74, 142, 157, 114, 255, 275, 251, 243, 283, 248, 225, 286,\n",
       "        220, 215, 287, 210, 253, 282, 252, 244, 288, 246, 226, 281, 234,\n",
       "        213, 276, 206, 256, 285, 249, 245, 279, 241, 222, 278, 235, 207,\n",
       "        272, 218, 254, 280, 250, 242, 259, 247, 229, 273, 224, 209, 274,\n",
       "        212, 219, 269, 221, 202, 264, 198, 192, 265, 179, 175, 263, 162,\n",
       "        231, 270, 208, 199, 268, 203, 186, 267, 181, 172, 257, 164, 228,\n",
       "        284, 211, 204, 262, 201, 194, 261, 182, 178, 266, 180, 205, 271,\n",
       "        216, 200, 277, 197, 184, 258, 185, 173, 260, 174, 237, 233, 227,\n",
       "        139, 143, 147, 118, 133, 129,  94,  83, 104, 236, 217, 223, 154,\n",
       "        161, 163, 127, 125, 111, 101, 103, 105, 232, 214, 238, 160, 159,\n",
       "        134, 128, 132, 121,  89,  82,  86, 230, 239, 240, 176, 155, 152,\n",
       "        135, 136, 131, 126, 115, 113, 168, 166, 167,  76,  57,  46,  18,\n",
       "         49,  25,  37,  19,  41, 153, 170, 196,  60,  79,  87,  48,  47,\n",
       "         51,   4,   8,  23, 183, 187, 195,  71,  92,  88,  29,  63,  66,\n",
       "         36,  27,  22, 188, 169, 177, 109, 107,  97,  38,  39,  61,  67,\n",
       "         45,  28]),\n",
       " 'split0_test_neg_log_loss': array([-0.214976  , -0.22395857, -0.23006384, -0.23748728, -0.22152701,\n",
       "        -0.22380794, -0.29094092, -0.27029463, -0.26543689, -0.29152994,\n",
       "        -0.27945414, -0.32832424, -0.21844352, -0.22420129, -0.22104977,\n",
       "        -0.21782365, -0.23486232, -0.23516536, -0.26078624, -0.26298726,\n",
       "        -0.24487056, -0.28602206, -0.26912952, -0.28702621, -0.22397521,\n",
       "        -0.22117432, -0.22358598, -0.22315814, -0.20920153, -0.22153106,\n",
       "        -0.23205671, -0.25477793, -0.24082934, -0.26865547, -0.29059703,\n",
       "        -0.24513259, -0.22044244, -0.21938299, -0.22240634, -0.21790903,\n",
       "        -0.2217932 , -0.22525131, -0.22046922, -0.22413398, -0.22674781,\n",
       "        -0.21832591, -0.24253228, -0.23878265, -0.22224055, -0.20787048,\n",
       "        -0.22124254, -0.24735558, -0.2307862 , -0.2285826 , -0.32013295,\n",
       "        -0.24941079, -0.23173915, -0.31440854, -0.29512352, -0.43171022,\n",
       "        -0.23656231, -0.22045771, -0.22089273, -0.24166464, -0.24171839,\n",
       "        -0.23657531, -0.25953608, -0.27809372, -0.25549046, -0.31059126,\n",
       "        -0.30085045, -0.2908645 , -0.22548509, -0.2310436 , -0.20509313,\n",
       "        -0.23311099, -0.2239039 , -0.23811612, -0.29826285, -0.27554196,\n",
       "        -0.29452307, -0.33790086, -0.33853937, -0.28252676, -0.2197274 ,\n",
       "        -0.22628918, -0.21998077, -0.22241194, -0.23153067, -0.22650479,\n",
       "        -0.21686179, -0.25017264, -0.2704315 , -0.30192907, -0.31612571,\n",
       "        -0.25349503, -0.92873634, -1.94799076, -0.82997576, -0.64723841,\n",
       "        -1.98795287, -0.64909913, -0.56551946, -1.91210508, -0.58726727,\n",
       "        -0.54405424, -1.88996995, -0.53180791, -0.8239008 , -1.96092123,\n",
       "        -1.01214053, -0.66192217, -2.09362309, -0.68691263, -0.60193561,\n",
       "        -1.9278139 , -0.61409409, -0.55716822, -2.0180352 , -0.54258096,\n",
       "        -0.78832896, -2.15163975, -0.78867996, -0.65620028, -1.99209002,\n",
       "        -0.64328539, -0.59638191, -1.89028741, -0.59000875, -0.54227129,\n",
       "        -1.94106169, -0.54144335, -0.8444867 , -2.01164719, -0.83117854,\n",
       "        -0.66472983, -1.83613293, -0.67953086, -0.61434146, -1.87880065,\n",
       "        -0.61144938, -0.5540169 , -1.97862026, -0.53436904, -0.33499277,\n",
       "        -1.63015285, -0.40392558, -0.29324985, -1.90420162, -0.31270407,\n",
       "        -0.28232248, -1.84556812, -0.27247378, -0.26187796, -1.63324491,\n",
       "        -0.26313977, -0.34552921, -2.03410911, -0.39693022, -0.28738597,\n",
       "        -1.81289038, -0.30273013, -0.27035306, -1.58533383, -0.27867467,\n",
       "        -0.27133785, -1.67486958, -0.26560551, -0.41009359, -2.07935027,\n",
       "        -0.3511188 , -0.30743148, -1.53653344, -0.29420775, -0.29004203,\n",
       "        -1.73915294, -0.27397806, -0.26699101, -1.85902642, -0.26433894,\n",
       "        -0.36652085, -1.74547697, -0.36079862, -0.29529375, -1.87873359,\n",
       "        -0.30096714, -0.27627785, -2.22515524, -0.27739206, -0.26602173,\n",
       "        -1.62970423, -0.26913546, -0.43456188, -0.40230075, -0.3102263 ,\n",
       "        -0.24621483, -0.24351677, -0.24283661, -0.23294246, -0.22988053,\n",
       "        -0.23124927, -0.22746508, -0.22478449, -0.23058508, -0.35940317,\n",
       "        -0.33320966, -0.36766389, -0.25345065, -0.25137356, -0.26095341,\n",
       "        -0.23443198, -0.23628628, -0.22750914, -0.22741489, -0.22635888,\n",
       "        -0.22802307, -0.36208049, -0.35183877, -0.35576509, -0.247318  ,\n",
       "        -0.25721848, -0.24439483, -0.23096657, -0.23230113, -0.23676116,\n",
       "        -0.22651375, -0.22762456, -0.22818168, -0.32306445, -0.37524423,\n",
       "        -0.40526511, -0.24711231, -0.2581008 , -0.25441384, -0.24227325,\n",
       "        -0.24726926, -0.23906948, -0.23783232, -0.23668807, -0.23778309,\n",
       "        -0.24761076, -0.23942059, -0.24811969, -0.22791663, -0.22836281,\n",
       "        -0.22903759, -0.22114408, -0.22020152, -0.22369702, -0.22353698,\n",
       "        -0.21740712, -0.22342627, -0.23783063, -0.24314426, -0.26130796,\n",
       "        -0.22377986, -0.22896735, -0.22401774, -0.21681477, -0.22740573,\n",
       "        -0.22366802, -0.21751589, -0.22085511, -0.22603864, -0.2522639 ,\n",
       "        -0.2387602 , -0.25917431, -0.23105033, -0.22610245, -0.23562281,\n",
       "        -0.2212805 , -0.22245552, -0.22384474, -0.22056618, -0.21854351,\n",
       "        -0.22255517, -0.24947834, -0.2506387 , -0.26399868, -0.2262132 ,\n",
       "        -0.22604029, -0.22537484, -0.22532465, -0.21933868, -0.22091686,\n",
       "        -0.22195822, -0.22124347, -0.22067829]),\n",
       " 'split1_test_neg_log_loss': array([-0.23891898, -0.24308193, -0.27607176, -0.24453579, -0.24060242,\n",
       "        -0.2539189 , -0.2723177 , -0.27180624, -0.28516755, -0.31443886,\n",
       "        -0.31025794, -0.32011856, -0.24041388, -0.22282612, -0.23737182,\n",
       "        -0.2637184 , -0.25193315, -0.2387946 , -0.29825541, -0.30324524,\n",
       "        -0.27707202, -0.33064359, -0.351217  , -0.27563929, -0.23071312,\n",
       "        -0.24236527, -0.27630807, -0.26865976, -0.26051985, -0.24397017,\n",
       "        -0.2938119 , -0.27567918, -0.25860158, -0.28509519, -0.27566539,\n",
       "        -0.27217722, -0.23074473, -0.22812051, -0.23675555, -0.22760292,\n",
       "        -0.23337324, -0.2261385 , -0.22642312, -0.2220203 , -0.22984453,\n",
       "        -0.25026969, -0.2325763 , -0.22775262, -0.24084974, -0.24786668,\n",
       "        -0.24007219, -0.25557304, -0.28967321, -0.30984369, -0.41018703,\n",
       "        -0.34357438, -0.29961471, -0.39404766, -0.37413623, -0.43365046,\n",
       "        -0.23718335, -0.24156958, -0.24764046, -0.26350727, -0.24939331,\n",
       "        -0.28122518, -0.28128334, -0.33141768, -0.29018313, -0.36138303,\n",
       "        -0.38090827, -0.36154378, -0.23479332, -0.23953831, -0.23242463,\n",
       "        -0.26564774, -0.27833907, -0.26552758, -0.26626736, -0.27141985,\n",
       "        -0.24736288, -0.31765775, -0.31797768, -0.33083718, -0.26090553,\n",
       "        -0.24059563, -0.23247859, -0.2406786 , -0.23375593, -0.23515829,\n",
       "        -0.24056463, -0.308408  , -0.26651639, -0.28570259, -0.29221601,\n",
       "        -0.25276983, -0.78091848, -1.92868062, -0.91784577, -0.65990734,\n",
       "        -1.95106555, -0.68045542, -0.59698704, -2.02419489, -0.62016192,\n",
       "        -0.56051278, -1.94049772, -0.55922155, -0.88648271, -1.95162111,\n",
       "        -0.83587847, -0.68219221, -1.96657057, -0.72115563, -0.59720464,\n",
       "        -1.95232667, -0.608996  , -0.55742604, -1.95259711, -0.55500565,\n",
       "        -0.88751347, -1.959845  , -0.79370962, -0.72841293, -1.94251548,\n",
       "        -0.67722319, -0.61505139, -1.89341354, -0.61143576, -0.54025102,\n",
       "        -1.91271623, -0.56422657, -0.8716565 , -1.95269267, -0.82490642,\n",
       "        -0.68511772, -1.90465186, -0.68178996, -0.58735148, -1.91804707,\n",
       "        -0.62420693, -0.5644331 , -1.97553875, -0.58560952, -0.42943067,\n",
       "        -1.91086925, -0.37507737, -0.33565586, -1.74288111, -0.31025361,\n",
       "        -0.30297103, -1.74843294, -0.29879636, -0.29434935, -1.6264417 ,\n",
       "        -0.28768753, -0.5160814 , -2.11368223, -0.3533177 , -0.31038556,\n",
       "        -1.91244113, -0.31205781, -0.30926278, -1.86265762, -0.29153406,\n",
       "        -0.29240924, -1.59785404, -0.28369433, -0.43712032, -2.004011  ,\n",
       "        -0.47951913, -0.32863391, -1.89221757, -0.33309222, -0.29180531,\n",
       "        -1.63353846, -0.29339243, -0.29135711, -1.80822522, -0.29892934,\n",
       "        -0.35812223, -1.90895003, -0.43645753, -0.33261866, -1.92106291,\n",
       "        -0.32233515, -0.31149994, -1.65329316, -0.2987311 , -0.2978991 ,\n",
       "        -1.6997352 , -0.29534194, -0.43613783, -0.43461757, -0.42103764,\n",
       "        -0.27795729, -0.2769458 , -0.26981918, -0.26447493, -0.26685873,\n",
       "        -0.26526649, -0.25874533, -0.26080334, -0.2573862 , -0.35458982,\n",
       "        -0.41877234, -0.36826258, -0.28065476, -0.28031461, -0.28607809,\n",
       "        -0.26411102, -0.265436  , -0.26468608, -0.26305843, -0.26611346,\n",
       "        -0.25816101, -0.41400772, -0.4158028 , -0.4097271 , -0.29076688,\n",
       "        -0.28205682, -0.28002055, -0.26320798, -0.26642241, -0.26009646,\n",
       "        -0.25681567, -0.25285822, -0.2593699 , -0.39805181, -0.40589855,\n",
       "        -0.41107824, -0.29885192, -0.29206267, -0.29112608, -0.27552331,\n",
       "        -0.27171267, -0.2780906 , -0.27656542, -0.27013839, -0.26591577,\n",
       "        -0.29496872, -0.28112056, -0.29831184, -0.25936312, -0.25473119,\n",
       "        -0.25233191, -0.24744508, -0.24815111, -0.24051992, -0.2442867 ,\n",
       "        -0.24592162, -0.23944261, -0.27514668, -0.28649262, -0.28736773,\n",
       "        -0.25418319, -0.261415  , -0.26212507, -0.24378208, -0.24235362,\n",
       "        -0.24871601, -0.2354944 , -0.24157056, -0.23835056, -0.29556898,\n",
       "        -0.2916363 , -0.32746108, -0.25751103, -0.2572926 , -0.25222915,\n",
       "        -0.24516276, -0.24840236, -0.25082559, -0.23995646, -0.2412165 ,\n",
       "        -0.24450101, -0.28151814, -0.2793933 , -0.28567954, -0.24723128,\n",
       "        -0.26044407, -0.26118745, -0.24576441, -0.2448964 , -0.24999032,\n",
       "        -0.24729766, -0.24168165, -0.24917773]),\n",
       " 'split2_test_neg_log_loss': array([-0.2086971 , -0.23061922, -0.21392153, -0.26026961, -0.25576829,\n",
       "        -0.22480417, -0.23992505, -0.23560755, -0.31136404, -0.26463795,\n",
       "        -0.2653538 , -0.31353157, -0.22779179, -0.23338132, -0.2346774 ,\n",
       "        -0.22403549, -0.20494913, -0.23363338, -0.27357603, -0.23949445,\n",
       "        -0.2611264 , -0.27393985, -0.29696123, -0.24768121, -0.21645456,\n",
       "        -0.24006173, -0.22969658, -0.24644643, -0.23041432, -0.22200092,\n",
       "        -0.25908486, -0.22191574, -0.22969504, -0.30488612, -0.26370938,\n",
       "        -0.22993516, -0.22399359, -0.20717903, -0.23078417, -0.21168413,\n",
       "        -0.209948  , -0.22681908, -0.21801645, -0.219848  , -0.20760094,\n",
       "        -0.24228946, -0.23300514, -0.21604754, -0.25125112, -0.2155049 ,\n",
       "        -0.23937116, -0.25790781, -0.22390088, -0.2298839 , -0.27169129,\n",
       "        -0.2822874 , -0.29969184, -0.38991921, -0.27715003, -0.35871526,\n",
       "        -0.21445463, -0.22735801, -0.21880151, -0.22160379, -0.25131043,\n",
       "        -0.25026815, -0.2504212 , -0.26649477, -0.25323671, -0.32819611,\n",
       "        -0.42703783, -0.44995278, -0.22562273, -0.24309233, -0.21762544,\n",
       "        -0.21805496, -0.21520615, -0.23630664, -0.23419346, -0.25810743,\n",
       "        -0.25752486, -0.2950682 , -0.2853104 , -0.44591303, -0.21989926,\n",
       "        -0.22472692, -0.22349717, -0.20557095, -0.214423  , -0.19963643,\n",
       "        -0.23657372, -0.24751095, -0.21462819, -0.25491125, -0.25925909,\n",
       "        -0.25314855, -1.1118196 , -1.85818765, -0.88139433, -0.64569772,\n",
       "        -1.86609227, -0.67319373, -0.59474107, -2.00302721, -0.59691169,\n",
       "        -0.55859983, -1.93474461, -0.55351068, -0.81902785, -1.919158  ,\n",
       "        -0.88890531, -0.66981394, -1.99411425, -0.68548821, -0.57836125,\n",
       "        -1.95821593, -0.62344767, -0.54139508, -1.97914248, -0.53701403,\n",
       "        -0.853523  , -1.95475012, -0.80950808, -0.70635396, -1.93387019,\n",
       "        -0.69640281, -0.61842182, -1.89197221, -0.59805828, -0.54546018,\n",
       "        -1.9598328 , -0.57369134, -0.82435087, -1.9510576 , -0.86686527,\n",
       "        -0.71144655, -1.9082185 , -0.7216855 , -0.61970896, -1.89944124,\n",
       "        -0.5992068 , -0.53958871, -1.95517079, -0.569428  , -0.41507508,\n",
       "        -1.84714975, -0.42122638, -0.28740719, -2.04363957, -0.27856167,\n",
       "        -0.27372644, -1.60804317, -0.28232117, -0.26258386, -2.22217398,\n",
       "        -0.26664924, -0.42638699, -1.85127574, -0.38834161, -0.2843577 ,\n",
       "        -2.00981436, -0.28994952, -0.27044288, -1.79988583, -0.26556096,\n",
       "        -0.27076999, -1.85984176, -0.2726888 , -0.34632496, -2.28880329,\n",
       "        -0.41045235, -0.29405374, -1.68665586, -0.29930615, -0.26399879,\n",
       "        -1.92788427, -0.2802951 , -0.26939175, -1.53111147, -0.27166661,\n",
       "        -0.33040386, -2.05744386, -0.46115906, -0.29741245, -2.02375152,\n",
       "        -0.29471157, -0.26988226, -1.54840573, -0.26856277, -0.26400593,\n",
       "        -1.60426655, -0.26843386, -0.36181051, -0.29988078, -0.36150275,\n",
       "        -0.23782179, -0.24997285, -0.24480396, -0.23714968, -0.23562112,\n",
       "        -0.23555962, -0.23024774, -0.22898623, -0.23522474, -0.39063999,\n",
       "        -0.30022557, -0.36449521, -0.24058401, -0.24421579, -0.25358725,\n",
       "        -0.23576217, -0.22743198, -0.23601409, -0.2303088 , -0.23822196,\n",
       "        -0.24004147, -0.36048465, -0.38412211, -0.44816378, -0.25369714,\n",
       "        -0.24277961, -0.24429609, -0.23370294, -0.24534168, -0.23314393,\n",
       "        -0.23174138, -0.2342859 , -0.23399191, -0.37655478, -0.38159525,\n",
       "        -0.36918575, -0.26793922, -0.24704202, -0.24751658, -0.24270757,\n",
       "        -0.24760086, -0.2447254 , -0.23772813, -0.23897138, -0.23663358,\n",
       "        -0.24886465, -0.27758367, -0.25835244, -0.22701202, -0.22323543,\n",
       "        -0.21369185, -0.22300024, -0.22545756, -0.21452852, -0.21988601,\n",
       "        -0.22019451, -0.22979396, -0.24715999, -0.25610836, -0.25560675,\n",
       "        -0.22104027, -0.22970665, -0.22772485, -0.23463691, -0.22540371,\n",
       "        -0.22382889, -0.22236035, -0.21580047, -0.21956821, -0.26310585,\n",
       "        -0.27281752, -0.25508804, -0.22065164, -0.22032308, -0.22320612,\n",
       "        -0.21539106, -0.22617433, -0.22804609, -0.2229981 , -0.22333207,\n",
       "        -0.21587097, -0.26332439, -0.26368694, -0.26711217, -0.23885606,\n",
       "        -0.23188046, -0.22426312, -0.2224225 , -0.21977327, -0.22497894,\n",
       "        -0.22976714, -0.2260622 , -0.22120755]),\n",
       " 'split3_test_neg_log_loss': array([-0.18679639, -0.19429336, -0.18637201, -0.19652744, -0.19698991,\n",
       "        -0.19334453, -0.20435455, -0.20522947, -0.20971702, -0.25154804,\n",
       "        -0.23524973, -0.20954789, -0.18815693, -0.20115281, -0.20454991,\n",
       "        -0.18460388, -0.1972693 , -0.20292689, -0.21493028, -0.20470307,\n",
       "        -0.21986713, -0.24093439, -0.27009413, -0.2417858 , -0.18626949,\n",
       "        -0.18245311, -0.17402473, -0.18995649, -0.19340558, -0.18485204,\n",
       "        -0.20715778, -0.20685711, -0.20912203, -0.20938775, -0.23271169,\n",
       "        -0.22057404, -0.17938159, -0.20121021, -0.1994099 , -0.18208889,\n",
       "        -0.18538029, -0.18901508, -0.18026266, -0.18315203, -0.19325387,\n",
       "        -0.18903825, -0.19063413, -0.18864856, -0.1975261 , -0.19672641,\n",
       "        -0.20087659, -0.20132035, -0.20127073, -0.21965809, -0.24718012,\n",
       "        -0.21140727, -0.23783419, -0.25886371, -0.27908271, -0.23362761,\n",
       "        -0.19423902, -0.20617714, -0.19751655, -0.19640841, -0.20706162,\n",
       "        -0.19248015, -0.22888169, -0.23966687, -0.22719169, -0.2642662 ,\n",
       "        -0.26381315, -0.29968482, -0.18511592, -0.18388283, -0.18666322,\n",
       "        -0.19145256, -0.21361597, -0.19131252, -0.21391415, -0.19674471,\n",
       "        -0.2240711 , -0.24966864, -0.27917628, -0.22036381, -0.19456982,\n",
       "        -0.18426531, -0.18983904, -0.19000962, -0.17745263, -0.1859783 ,\n",
       "        -0.19672603, -0.18316037, -0.20711294, -0.21381832, -0.22172423,\n",
       "        -0.21542818, -0.78292985, -1.92881983, -0.89055376, -0.67609034,\n",
       "        -1.93913646, -0.6910063 , -0.58761562, -1.99578115, -0.59325845,\n",
       "        -0.55388311, -1.97896073, -0.56155217, -0.96207255, -1.94291286,\n",
       "        -0.79673242, -0.70942642, -2.09198654, -0.67411779, -0.59106689,\n",
       "        -1.91522405, -0.63272965, -0.53291621, -1.92864312, -0.54326222,\n",
       "        -0.78466533, -1.96664762, -0.81242871, -0.67908619, -1.97098076,\n",
       "        -0.66021646, -0.5937626 , -1.99466399, -0.64001463, -0.57047055,\n",
       "        -1.90256517, -0.55499561, -0.81162552, -1.97782245, -0.92147084,\n",
       "        -0.65819846, -1.91171497, -0.69435626, -0.59907388, -1.95567225,\n",
       "        -0.60651103, -0.53997908, -1.84120762, -0.55970682, -0.38535087,\n",
       "        -1.90435148, -0.45183281, -0.28510019, -1.94410903, -0.28216415,\n",
       "        -0.25714422, -1.70318186, -0.26643564, -0.25629915, -1.83847457,\n",
       "        -0.24252731, -0.42699465, -2.1095303 , -0.40584128, -0.27937021,\n",
       "        -1.74605015, -0.2878243 , -0.25447092, -2.00250164, -0.26324792,\n",
       "        -0.24941569, -1.82445576, -0.2466105 , -0.39712202, -1.95360605,\n",
       "        -0.44518427, -0.28016319, -1.98243557, -0.27576838, -0.25154037,\n",
       "        -1.87443334, -0.26173311, -0.25173729, -1.96974833, -0.24609824,\n",
       "        -0.40277315, -2.00432998, -0.37628704, -0.29172158, -1.89867626,\n",
       "        -0.28365409, -0.24776781, -1.7577495 , -0.2677973 , -0.24829593,\n",
       "        -1.66650524, -0.24794741, -0.35605834, -0.34436397, -0.41359342,\n",
       "        -0.22808667, -0.22801804, -0.23091675, -0.21073313, -0.21997028,\n",
       "        -0.20865704, -0.20148839, -0.20014227, -0.20594131, -0.35489335,\n",
       "        -0.36064128, -0.35756144, -0.22687636, -0.23435154, -0.23156443,\n",
       "        -0.21627671, -0.21643559, -0.21361466, -0.2084048 , -0.20598334,\n",
       "        -0.20234874, -0.29480721, -0.28068982, -0.35716125, -0.23266359,\n",
       "        -0.22576477, -0.21857567, -0.22043724, -0.2101585 , -0.22040451,\n",
       "        -0.20519514, -0.20407667, -0.2052723 , -0.38077439, -0.37568189,\n",
       "        -0.39295425, -0.24943726, -0.2329235 , -0.24322353, -0.2275615 ,\n",
       "        -0.22296656, -0.22183498, -0.21815837, -0.21544556, -0.2145522 ,\n",
       "        -0.21667317, -0.22471428, -0.22402389, -0.19818063, -0.19595678,\n",
       "        -0.20683643, -0.19348077, -0.20452122, -0.19265877, -0.19682577,\n",
       "        -0.19632937, -0.19158194, -0.2218073 , -0.24221469, -0.21544782,\n",
       "        -0.19728514, -0.2050298 , -0.19164509, -0.19608756, -0.19781093,\n",
       "        -0.19274204, -0.19288576, -0.19326651, -0.19116967, -0.24093055,\n",
       "        -0.23049633, -0.23913367, -0.20619096, -0.20790178, -0.20753378,\n",
       "        -0.19642388, -0.19617229, -0.1927417 , -0.19639411, -0.19535001,\n",
       "        -0.19345324, -0.24955846, -0.22136698, -0.23857606, -0.20921915,\n",
       "        -0.20790093, -0.2093127 , -0.19515851, -0.19341567, -0.19867816,\n",
       "        -0.1898649 , -0.19375921, -0.19380104]),\n",
       " 'split4_test_neg_log_loss': array([-0.19245029, -0.18759004, -0.20423495, -0.2084461 , -0.20151563,\n",
       "        -0.2085565 , -0.22662572, -0.24703286, -0.21213669, -0.24527816,\n",
       "        -0.26033978, -0.25462694, -0.20517247, -0.19417965, -0.19890665,\n",
       "        -0.20247939, -0.20845668, -0.20003844, -0.22832897, -0.22534089,\n",
       "        -0.1958133 , -0.26804961, -0.21014016, -0.23663374, -0.18699361,\n",
       "        -0.18620124, -0.18434564, -0.2021629 , -0.20094895, -0.19754689,\n",
       "        -0.21904549, -0.22956413, -0.20858283, -0.23949393, -0.21156677,\n",
       "        -0.22366881, -0.18743854, -0.19357741, -0.19005641, -0.19587661,\n",
       "        -0.18487132, -0.19495569, -0.20253918, -0.20286636, -0.18868204,\n",
       "        -0.1992054 , -0.20546628, -0.20872123, -0.19678664, -0.1855416 ,\n",
       "        -0.19127284, -0.19214101, -0.20458445, -0.21152685, -0.24342498,\n",
       "        -0.23500108, -0.22253978, -0.26244486, -0.27281282, -0.25735542,\n",
       "        -0.20749552, -0.18817285, -0.18808877, -0.2011474 , -0.20557849,\n",
       "        -0.20184331, -0.22332749, -0.23392719, -0.25671006, -0.23980768,\n",
       "        -0.29838706, -0.26762728, -0.19614208, -0.19259573, -0.19102658,\n",
       "        -0.18797115, -0.20589236, -0.18821695, -0.19984439, -0.2381209 ,\n",
       "        -0.26301047, -0.23935881, -0.23021291, -0.2462587 , -0.1865335 ,\n",
       "        -0.18413959, -0.18785879, -0.19226736, -0.19004125, -0.19117173,\n",
       "        -0.19997234, -0.19977923, -0.19881848, -0.23971402, -0.22647299,\n",
       "        -0.23038918, -0.89262873, -2.07948254, -1.06832162, -0.64828122,\n",
       "        -1.88532058, -0.6830308 , -0.61510575, -1.95982842, -0.58898947,\n",
       "        -0.54373761, -1.97692003, -0.53521029, -0.8227308 , -1.95906733,\n",
       "        -0.91091147, -0.64271189, -2.00635404, -0.70088279, -0.59354315,\n",
       "        -1.9600295 , -0.59663334, -0.57021982, -2.06119154, -0.5279602 ,\n",
       "        -0.86006231, -1.92382591, -0.79015741, -0.67862253, -2.05663212,\n",
       "        -0.6939079 , -0.56570206, -1.93525261, -0.62680767, -0.5301302 ,\n",
       "        -1.96871873, -0.5390203 , -0.86593484, -2.02688827, -0.85496526,\n",
       "        -0.66701104, -1.92615234, -0.74695442, -0.5967962 , -1.86581438,\n",
       "        -0.5955312 , -0.57370326, -1.93853427, -0.54204456, -0.3742406 ,\n",
       "        -2.49343192, -0.3359412 , -0.25866978, -2.19889398, -0.27685985,\n",
       "        -0.25118796, -1.7948478 , -0.25362244, -0.2402496 , -1.78096888,\n",
       "        -0.2405409 , -0.37849876, -1.83285785, -0.3404217 , -0.27573137,\n",
       "        -1.80602206, -0.28038665, -0.24632217, -1.88896713, -0.24869158,\n",
       "        -0.2442727 , -1.45805971, -0.23747288, -0.4143175 , -2.19085924,\n",
       "        -0.40029237, -0.28891071, -1.74093138, -0.28200526, -0.25915335,\n",
       "        -1.78680629, -0.25056253, -0.24609648, -1.96333583, -0.25189188,\n",
       "        -0.39898932, -1.84743009, -0.37841663, -0.273074  , -2.08704072,\n",
       "        -0.26804954, -0.24874931, -1.72080271, -0.24961957, -0.24291363,\n",
       "        -1.63636503, -0.23680642, -0.35219885, -0.34961508, -0.29925713,\n",
       "        -0.21907579, -0.22165328, -0.22651341, -0.20983367, -0.20712029,\n",
       "        -0.20831984, -0.20286084, -0.20545318, -0.20210484, -0.36487614,\n",
       "        -0.37710353, -0.39363572, -0.22011477, -0.22310342, -0.219847  ,\n",
       "        -0.21038139, -0.20404732, -0.20685627, -0.20327619, -0.20218732,\n",
       "        -0.20283871, -0.40811227, -0.35858088, -0.34834609, -0.21896556,\n",
       "        -0.22662075, -0.21703873, -0.20755472, -0.20749853, -0.21091564,\n",
       "        -0.20259266, -0.20288629, -0.20149447, -0.38932779, -0.349898  ,\n",
       "        -0.34431589, -0.23616224, -0.23529466, -0.23368358, -0.21868887,\n",
       "        -0.22027676, -0.22149638, -0.21572599, -0.21467457, -0.21616875,\n",
       "        -0.22899884, -0.22431498, -0.22324266, -0.19945415, -0.20256993,\n",
       "        -0.1965018 , -0.19691601, -0.19596214, -0.19663922, -0.19110779,\n",
       "        -0.19295394, -0.19373154, -0.21783433, -0.21888797, -0.27747943,\n",
       "        -0.20116559, -0.1981293 , -0.20156385, -0.19442969, -0.19124686,\n",
       "        -0.19933607, -0.1895964 , -0.19209326, -0.19294874, -0.23562739,\n",
       "        -0.22404783, -0.22426399, -0.19854171, -0.20064049, -0.19448001,\n",
       "        -0.20027538, -0.19909691, -0.19551993, -0.19549229, -0.19582659,\n",
       "        -0.19748414, -0.22430038, -0.22392291, -0.22892541, -0.20598265,\n",
       "        -0.19922404, -0.20351209, -0.19847928, -0.19740134, -0.19724658,\n",
       "        -0.19679055, -0.19743119, -0.1966466 ]),\n",
       " 'mean_test_neg_log_loss': array([-0.20836775, -0.21590862, -0.22213282, -0.22945324, -0.22328065,\n",
       "        -0.22088641, -0.24683279, -0.24599415, -0.25676444, -0.27348659,\n",
       "        -0.27013108, -0.28522984, -0.21599572, -0.21514824, -0.21931111,\n",
       "        -0.21853216, -0.21949412, -0.22211173, -0.25517539, -0.24715418,\n",
       "        -0.23974988, -0.2799179 , -0.27950841, -0.25775325, -0.2088812 ,\n",
       "        -0.21445113, -0.2175922 , -0.22607674, -0.21889805, -0.21398022,\n",
       "        -0.24223135, -0.23775882, -0.22936617, -0.26150369, -0.25485005,\n",
       "        -0.23829756, -0.20840018, -0.20989403, -0.21588248, -0.20703231,\n",
       "        -0.20707321, -0.21243593, -0.20954213, -0.21040413, -0.20922584,\n",
       "        -0.21982574, -0.22084283, -0.21599052, -0.22173083, -0.21070201,\n",
       "        -0.21856706, -0.23085956, -0.23004309, -0.23989902, -0.29852327,\n",
       "        -0.26433618, -0.25828393, -0.3239368 , -0.29966106, -0.34301179,\n",
       "        -0.21798697, -0.21674706, -0.214588  , -0.2248663 , -0.23101245,\n",
       "        -0.23247842, -0.24868996, -0.26992005, -0.25656241, -0.30084886,\n",
       "        -0.33419935, -0.33393463, -0.21343183, -0.21803056, -0.2065666 ,\n",
       "        -0.21924748, -0.22739149, -0.22389596, -0.24249644, -0.24798697,\n",
       "        -0.25729848, -0.28793085, -0.29024333, -0.3051799 , -0.2163271 ,\n",
       "        -0.21200333, -0.21073087, -0.21018769, -0.2094407 , -0.20768991,\n",
       "        -0.2181397 , -0.23780624, -0.2315015 , -0.25921505, -0.2631596 ,\n",
       "        -0.24104615, -0.8994066 , -1.94863228, -0.91761825, -0.65544301,\n",
       "        -1.92591355, -0.67535708, -0.59199379, -1.97898735, -0.59731776,\n",
       "        -0.55215751, -1.94421861, -0.54826052, -0.86284294, -1.9467361 ,\n",
       "        -0.88891364, -0.67321333, -2.0305297 , -0.69371141, -0.59242231,\n",
       "        -1.94272201, -0.61518015, -0.55182508, -1.98792189, -0.54116461,\n",
       "        -0.83481861, -1.99134168, -0.79889676, -0.68973518, -1.97921772,\n",
       "        -0.67420715, -0.59786396, -1.92111795, -0.61326502, -0.54571665,\n",
       "        -1.93697892, -0.55467543, -0.84361089, -1.98402164, -0.85987727,\n",
       "        -0.67730072, -1.89737412, -0.7048634 , -0.6034544 , -1.90355512,\n",
       "        -0.60738107, -0.55434421, -1.93781434, -0.55823159, -0.387818  ,\n",
       "        -1.95719105, -0.39760067, -0.29201657, -1.96674506, -0.29210867,\n",
       "        -0.27347043, -1.74001478, -0.27472988, -0.26307198, -1.82026081,\n",
       "        -0.26010895, -0.4186982 , -1.98829105, -0.3769705 , -0.28744616,\n",
       "        -1.85744362, -0.29458968, -0.27017036, -1.82786921, -0.26954184,\n",
       "        -0.26564109, -1.68301617, -0.2612144 , -0.40099568, -2.10332597,\n",
       "        -0.41731338, -0.2998386 , -1.76775476, -0.29687595, -0.27130797,\n",
       "        -1.79236306, -0.27199224, -0.26511473, -1.82628945, -0.266585  ,\n",
       "        -0.37136188, -1.91272619, -0.40262378, -0.29802409, -1.961853  ,\n",
       "        -0.2939435 , -0.27083543, -1.78108127, -0.27242056, -0.26382726,\n",
       "        -1.64731525, -0.26353302, -0.38815348, -0.36615563, -0.36112345,\n",
       "        -0.24183127, -0.24402135, -0.24297798, -0.23102677, -0.23189019,\n",
       "        -0.22981045, -0.22416148, -0.2240339 , -0.22624843, -0.36488049,\n",
       "        -0.35799048, -0.37032377, -0.24433611, -0.24667178, -0.25040603,\n",
       "        -0.23219266, -0.22992743, -0.22973605, -0.22649262, -0.22777299,\n",
       "        -0.2262826 , -0.36789847, -0.35820688, -0.38383266, -0.24868223,\n",
       "        -0.24688809, -0.24086517, -0.23117389, -0.23234445, -0.23226434,\n",
       "        -0.22457172, -0.22434633, -0.22566205, -0.37355464, -0.37766358,\n",
       "        -0.38455985, -0.25990059, -0.25308473, -0.25399272, -0.2413509 ,\n",
       "        -0.24196522, -0.24104337, -0.23720205, -0.23518359, -0.23421068,\n",
       "        -0.24742323, -0.24943082, -0.2504101 , -0.22238531, -0.22097123,\n",
       "        -0.21967992, -0.21639723, -0.21885871, -0.21360869, -0.21512865,\n",
       "        -0.21456131, -0.21559526, -0.23995579, -0.24936958, -0.25944194,\n",
       "        -0.21949081, -0.22464962, -0.22141532, -0.2171502 , -0.21684417,\n",
       "        -0.21765821, -0.21157056, -0.21271718, -0.21361516, -0.25749933,\n",
       "        -0.25155164, -0.26102422, -0.22278914, -0.22245208, -0.22261438,\n",
       "        -0.21570672, -0.21846028, -0.21819561, -0.21508143, -0.21485374,\n",
       "        -0.2147729 , -0.25363594, -0.24780176, -0.25685837, -0.22550047,\n",
       "        -0.22509796, -0.22473004, -0.21742987, -0.21496507, -0.21836217,\n",
       "        -0.21713569, -0.21603554, -0.21630225]),\n",
       " 'std_test_neg_log_loss': array([0.01841654, 0.02139511, 0.0304595 , 0.02352482, 0.02246546,\n",
       "        0.02012765, 0.03116232, 0.0246069 , 0.04017004, 0.02591511,\n",
       "        0.02462716, 0.04591216, 0.01807499, 0.01489232, 0.01548916,\n",
       "        0.02638714, 0.02057084, 0.01695141, 0.03022037, 0.0338629 ,\n",
       "        0.02898919, 0.02934847, 0.04572751, 0.01989601, 0.0187199 ,\n",
       "        0.02569916, 0.03642377, 0.02868205, 0.0242118 , 0.02068453,\n",
       "        0.031035  , 0.02449765, 0.01912064, 0.03373497, 0.0288207 ,\n",
       "        0.01893464, 0.02082747, 0.0124198 , 0.01810325, 0.01618466,\n",
       "        0.01939144, 0.0168105 , 0.01662836, 0.01557001, 0.01680442,\n",
       "        0.02369291, 0.01951939, 0.0170778 , 0.0221148 , 0.02116914,\n",
       "        0.01980084, 0.02823534, 0.03184343, 0.03559577, 0.06217342,\n",
       "        0.04579459, 0.03412714, 0.05895043, 0.03799522, 0.08441717,\n",
       "        0.01673354, 0.01827964, 0.02070561, 0.02513321, 0.02042063,\n",
       "        0.03239715, 0.0210646 , 0.03484606, 0.0200261 , 0.04375269,\n",
       "        0.06024881, 0.06579852, 0.01924026, 0.02479118, 0.01691593,\n",
       "        0.02862645, 0.02610854, 0.02974593, 0.03572124, 0.02875382,\n",
       "        0.02289595, 0.03809166, 0.03701936, 0.07955218, 0.02597151,\n",
       "        0.02336356, 0.01833632, 0.01912451, 0.02237676, 0.01958374,\n",
       "        0.01807164, 0.04396349, 0.03062443, 0.03157769, 0.03668159,\n",
       "        0.01554824, 0.12135215, 0.07223993, 0.08054492, 0.01148756,\n",
       "        0.04445834, 0.0143104 , 0.01604183, 0.03935472, 0.0119069 ,\n",
       "        0.00708307, 0.03262795, 0.01237233, 0.05558883, 0.01518097,\n",
       "        0.07345594, 0.02218735, 0.05245767, 0.0161392 , 0.00792692,\n",
       "        0.01794575, 0.01232675, 0.01315003, 0.04714689, 0.00882682,\n",
       "        0.04108646, 0.08147961, 0.01003374, 0.02503451, 0.04388618,\n",
       "        0.02020771, 0.01882253, 0.04043845, 0.01829967, 0.01339778,\n",
       "        0.02576575, 0.01321441, 0.02315035, 0.03068316, 0.03438984,\n",
       "        0.01926574, 0.03147982, 0.02584244, 0.01187908, 0.03156036,\n",
       "        0.01007627, 0.01342198, 0.05044379, 0.01848528, 0.0330318 ,\n",
       "        0.28689896, 0.03961942, 0.02484753, 0.15122502, 0.01592703,\n",
       "        0.01850277, 0.08123079, 0.01520678, 0.01758789, 0.21723286,\n",
       "        0.01734683, 0.05758669, 0.12284451, 0.02552052, 0.01215084,\n",
       "        0.09303681, 0.01131751, 0.02164963, 0.13787397, 0.01454139,\n",
       "        0.01729257, 0.14778147, 0.01693059, 0.03022954, 0.12240773,\n",
       "        0.04327783, 0.01689276, 0.15642165, 0.01995651, 0.01651074,\n",
       "        0.10313969, 0.01480089, 0.0158185 , 0.15993138, 0.0185074 ,\n",
       "        0.02693038, 0.11098162, 0.03898732, 0.01932274, 0.08008416,\n",
       "        0.01806863, 0.02325529, 0.23317259, 0.01595683, 0.01920594,\n",
       "        0.03286138, 0.0201218 , 0.03866011, 0.04718968, 0.05051913,\n",
       "        0.02023416, 0.01937076, 0.01510205, 0.02010432, 0.01998194,\n",
       "        0.02098364, 0.02103228, 0.02141557, 0.02031863, 0.0134104 ,\n",
       "        0.04004408, 0.01226067, 0.02148354, 0.01932478, 0.02316343,\n",
       "        0.01879142, 0.02078139, 0.02024481, 0.02106487, 0.02328342,\n",
       "        0.02159173, 0.0428354 , 0.04480731, 0.03890363, 0.02423966,\n",
       "        0.02105895, 0.02289932, 0.01847032, 0.02208969, 0.01668559,\n",
       "        0.01976552, 0.01894129, 0.0210322 , 0.02630319, 0.01785149,\n",
       "        0.02473191, 0.02199253, 0.02146586, 0.01974559, 0.01936058,\n",
       "        0.01884751, 0.02069298, 0.02179005, 0.02024183, 0.0186323 ,\n",
       "        0.02663723, 0.0250549 , 0.02755796, 0.02249918, 0.02080302,\n",
       "        0.01945125, 0.01967122, 0.01807323, 0.0176278 , 0.01926407,\n",
       "        0.01909232, 0.01942311, 0.02055987, 0.02210252, 0.02473408,\n",
       "        0.02026457, 0.02228979, 0.02443036, 0.01987466, 0.01925114,\n",
       "        0.01997648, 0.01764144, 0.01850365, 0.01861463, 0.02126522,\n",
       "        0.02616266, 0.03544863, 0.02069716, 0.01959171, 0.02032521,\n",
       "        0.01737112, 0.01920256, 0.02170598, 0.0169969 , 0.01745233,\n",
       "        0.01843344, 0.01879239, 0.02248293, 0.02050013, 0.01610673,\n",
       "        0.02126704, 0.02008817, 0.01868282, 0.01849861, 0.01941122,\n",
       "        0.02121403, 0.01804293, 0.02008312]),\n",
       " 'rank_test_neg_log_loss': array([  5,  36,  72,  96,  77,  67, 133, 131, 152, 182, 175, 186,  38,\n",
       "         32,  61,  56,  63,  71, 150, 135, 117, 185, 184, 156,   7,  24,\n",
       "         48,  89,  59,  23, 126, 114,  95, 164, 149, 116,   6,  11,  35,\n",
       "          2,   3,  18,  10,  13,   8,  65,  66,  37,  70,  14,  57, 101,\n",
       "        100, 118, 196, 169, 157, 201, 197, 204,  50,  43,  26,  85, 102,\n",
       "        110, 140, 174, 151, 199, 203, 202,  20,  51,   1,  60,  93,  78,\n",
       "        127, 138, 154, 188, 189, 200,  41,  17,  15,  12,   9,   4,  52,\n",
       "        115, 105, 158, 166, 122, 255, 277, 256, 241, 271, 244, 233, 281,\n",
       "        235, 229, 275, 227, 253, 276, 254, 242, 287, 247, 234, 274, 240,\n",
       "        228, 284, 225, 250, 286, 249, 246, 282, 243, 236, 270, 239, 226,\n",
       "        272, 231, 251, 283, 252, 245, 267, 248, 237, 268, 238, 230, 273,\n",
       "        232, 218, 278, 220, 190, 280, 191, 181, 259, 183, 165, 263, 161,\n",
       "        224, 285, 214, 187, 266, 193, 176, 265, 173, 171, 258, 163, 221,\n",
       "        288, 223, 198, 260, 194, 178, 262, 179, 170, 264, 172, 212, 269,\n",
       "        222, 195, 279, 192, 177, 261, 180, 168, 257, 167, 219, 209, 207,\n",
       "        124, 129, 128, 103, 106,  98,  80,  79,  90, 208, 205, 211, 130,\n",
       "        132, 143, 107,  99,  97,  92,  94,  91, 210, 206, 216, 139, 134,\n",
       "        120, 104, 109, 108,  82,  81,  88, 213, 215, 217, 160, 146, 148,\n",
       "        123, 125, 121, 113, 112, 111, 136, 142, 144,  73,  68,  64,  42,\n",
       "         58,  21,  31,  25,  33, 119, 141, 159,  62,  83,  69,  46,  44,\n",
       "         49,  16,  19,  22, 155, 145, 162,  76,  74,  75,  34,  55,  53,\n",
       "         30,  28,  27, 147, 137, 153,  87,  86,  84,  47,  29,  54,  45,\n",
       "         39,  40])}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRIAL_5_MLP.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FIVE NEG LOG LOSS RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'tanh',\n",
       " 'classifier__alpha': 0.01,\n",
       " 'classifier__hidden_layer_sizes': (5,),\n",
       " 'classifier__learning_rate': 'adaptive',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL FIVE NEG LOG LOSS RESULTS ---------------------------------------------\")\n",
    "TRIAL_5_MLP.cv_results_['rank_test_neg_log_loss']\n",
    "TRIAL_5_MLP.cv_results_['params'][ np.argmin(TRIAL_5_MLP.cv_results_['rank_test_neg_log_loss']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FIVE F1 RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.1,\n",
       " 'classifier__hidden_layer_sizes': (20,),\n",
       " 'classifier__learning_rate': 'invscaling',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL FIVE F1 RESULTS ---------------------------------------------\")\n",
    "TRIAL_5_MLP.cv_results_['rank_test_f1_micro']\n",
    "TRIAL_5_MLP.cv_results_['params'][ np.argmin(TRIAL_5_MLP.cv_results_['rank_test_f1_micro']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FIVE ROC AUC RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'tanh',\n",
       " 'classifier__alpha': 0.01,\n",
       " 'classifier__hidden_layer_sizes': (5,),\n",
       " 'classifier__learning_rate': 'adaptive',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL FIVE ROC AUC RESULTS ---------------------------------------------\")\n",
    "TRIAL_5_MLP.cv_results_['rank_test_roc_auc_ovo']\n",
    "TRIAL_5_MLP.cv_results_['params'][ np.argmin(TRIAL_5_MLP.cv_results_['rank_test_roc_auc_ovo']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 5 MLP using best NEG LOG LOSS hyperparameters :0.9276967930029154\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 5 MLP using best F1 hyperparameters :0.9259475218658892\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 5 MLP using best ROC_AUC hyperparameters :0.928862973760933\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best NEG LOG LOSS results to train on test set, \n",
    "p = TRIAL_5_MLP.cv_results_['params'][ np.argmin(TRIAL_5_MLP.cv_results_['rank_test_neg_log_loss']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train__neg_log_loss = pipe.predict(X_train)\n",
    "y_pred_test_neg_log_loss = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 5 MLP using best NEG LOG LOSS hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_neg_log_loss)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best F1 results to train on test set, LR_Trial 1\n",
    "p = TRIAL_5_MLP.cv_results_['params'][ np.argmin(TRIAL_5_MLP.cv_results_['rank_test_f1_micro']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train_roc_auc = pipe.predict(X_train)\n",
    "y_pred_test_f1 = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 5 MLP using best F1 hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_f1)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best ROC_AUC results to train on test set, LR_Trial 1\n",
    "p  = TRIAL_5_MLP.cv_results_['params'][ np.argmin(TRIAL_5_MLP.cv_results_['rank_test_roc_auc_ovo']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train_roc_auc = pipe.predict(X_train)\n",
    "y_pred_test_roc_auc = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 5 MLP using best ROC_AUC hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_roc_auc)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
