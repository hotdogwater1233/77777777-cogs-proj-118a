{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries;\n",
    "import numpy as np\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# use seaborn plotting defaults\n",
    "import seaborn as sns; sns.set_style('white')\n",
    "\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from numpy import genfromtxt\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import load_digits, make_blobs\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, plot_confusion_matrix\n",
    "from sklearn.model_selection import KFold, train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, StandardScaler \n",
    "from sklearn.preprocessing import RobustScaler, Normalizer, QuantileTransformer, PowerTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import seaborn as sns\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fLength</th>\n",
       "      <th>fWidth</th>\n",
       "      <th>fSize</th>\n",
       "      <th>fConc</th>\n",
       "      <th>fConc1</th>\n",
       "      <th>fAsym</th>\n",
       "      <th>fM3Long</th>\n",
       "      <th>fM3Trans</th>\n",
       "      <th>fAlpha</th>\n",
       "      <th>fDist</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.7967</td>\n",
       "      <td>16.0021</td>\n",
       "      <td>2.6449</td>\n",
       "      <td>0.3918</td>\n",
       "      <td>0.1982</td>\n",
       "      <td>27.7004</td>\n",
       "      <td>22.0110</td>\n",
       "      <td>-8.2027</td>\n",
       "      <td>40.0920</td>\n",
       "      <td>81.8828</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31.6036</td>\n",
       "      <td>11.7235</td>\n",
       "      <td>2.5185</td>\n",
       "      <td>0.5303</td>\n",
       "      <td>0.3773</td>\n",
       "      <td>26.2722</td>\n",
       "      <td>23.8238</td>\n",
       "      <td>-9.9574</td>\n",
       "      <td>6.3609</td>\n",
       "      <td>205.2610</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>162.0520</td>\n",
       "      <td>136.0310</td>\n",
       "      <td>4.0612</td>\n",
       "      <td>0.0374</td>\n",
       "      <td>0.0187</td>\n",
       "      <td>116.7410</td>\n",
       "      <td>-64.8580</td>\n",
       "      <td>-45.2160</td>\n",
       "      <td>76.9600</td>\n",
       "      <td>256.7880</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.8172</td>\n",
       "      <td>9.5728</td>\n",
       "      <td>2.3385</td>\n",
       "      <td>0.6147</td>\n",
       "      <td>0.3922</td>\n",
       "      <td>27.2107</td>\n",
       "      <td>-6.4633</td>\n",
       "      <td>-7.1513</td>\n",
       "      <td>10.4490</td>\n",
       "      <td>116.7370</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75.1362</td>\n",
       "      <td>30.9205</td>\n",
       "      <td>3.1611</td>\n",
       "      <td>0.3168</td>\n",
       "      <td>0.1832</td>\n",
       "      <td>-5.5277</td>\n",
       "      <td>28.5525</td>\n",
       "      <td>21.8393</td>\n",
       "      <td>4.6480</td>\n",
       "      <td>356.4620</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fLength    fWidth   fSize   fConc  fConc1     fAsym  fM3Long  fM3Trans  \\\n",
       "0   28.7967   16.0021  2.6449  0.3918  0.1982   27.7004  22.0110   -8.2027   \n",
       "1   31.6036   11.7235  2.5185  0.5303  0.3773   26.2722  23.8238   -9.9574   \n",
       "2  162.0520  136.0310  4.0612  0.0374  0.0187  116.7410 -64.8580  -45.2160   \n",
       "3   23.8172    9.5728  2.3385  0.6147  0.3922   27.2107  -6.4633   -7.1513   \n",
       "4   75.1362   30.9205  3.1611  0.3168  0.1832   -5.5277  28.5525   21.8393   \n",
       "\n",
       "    fAlpha     fDist class  \n",
       "0  40.0920   81.8828     g  \n",
       "1   6.3609  205.2610     g  \n",
       "2  76.9600  256.7880     g  \n",
       "3  10.4490  116.7370     g  \n",
       "4   4.6480  356.4620     g  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gammaData = pd.read_csv('telescope_data.csv')\n",
    "del gammaData['Unnamed: 0']\n",
    "gammaData.head()\n",
    "\n",
    "X = gammaData.iloc[:,0:9].values\n",
    "y = gammaData.iloc[:,10].values\n",
    "gammaData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encodeLabel = LabelEncoder()\n",
    "y = encodeLabel.fit_transform(y)\n",
    "#To be used for validation purposes with best model for each trial 5000 train and 45000 test\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.75, random_state=1738)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTI LAYERED PERCEPTRON TRIAL ONE ON GAMMA PARTICLES DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:    6.3s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    6.6s\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:    9.8s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:   10.6s\n",
      "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed:   11.6s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   13.4s\n",
      "[Parallel(n_jobs=-1)]: Done 189 tasks      | elapsed:   14.3s\n",
      "[Parallel(n_jobs=-1)]: Done 210 tasks      | elapsed:   15.2s\n",
      "[Parallel(n_jobs=-1)]: Done 233 tasks      | elapsed:   17.6s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:   18.6s\n",
      "[Parallel(n_jobs=-1)]: Done 281 tasks      | elapsed:   20.6s\n",
      "[Parallel(n_jobs=-1)]: Done 306 tasks      | elapsed:   22.6s\n",
      "[Parallel(n_jobs=-1)]: Done 333 tasks      | elapsed:   24.7s\n",
      "[Parallel(n_jobs=-1)]: Done 360 tasks      | elapsed:   27.2s\n",
      "[Parallel(n_jobs=-1)]: Done 389 tasks      | elapsed:   28.7s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:   32.0s\n",
      "[Parallel(n_jobs=-1)]: Done 449 tasks      | elapsed:   33.7s\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:   36.8s\n",
      "[Parallel(n_jobs=-1)]: Done 513 tasks      | elapsed:   40.9s\n",
      "[Parallel(n_jobs=-1)]: Done 546 tasks      | elapsed:   44.8s\n",
      "[Parallel(n_jobs=-1)]: Done 581 tasks      | elapsed:   48.3s\n",
      "[Parallel(n_jobs=-1)]: Done 616 tasks      | elapsed:   52.7s\n",
      "[Parallel(n_jobs=-1)]: Done 653 tasks      | elapsed:   57.0s\n",
      "[Parallel(n_jobs=-1)]: Done 690 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 729 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 809 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 850 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 893 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 936 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 981 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1026 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1073 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1120 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1169 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1269 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1320 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1373 tasks      | elapsed:  2.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL ONE RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1440 out of 1440 | elapsed:  3.1min finished\n"
     ]
    }
   ],
   "source": [
    "ITERATIONS = 1\n",
    "for i in range(1):\n",
    "    #For reach \"trial\", randomly select 5000 data points\n",
    "    random5000DataPoints = gammaData.sample(n = 5000, replace = True)\n",
    "    #From that 5000, create x set and y set\n",
    "    xSet = random5000DataPoints.iloc[:,0:9].values\n",
    "    ySet = random5000DataPoints.iloc[:,10].values\n",
    "    ySet = encodeLabel.fit_transform(ySet)\n",
    "    # Run GridSearch to find best hyperparameters per 5000 data points \n",
    "    \n",
    "    pipe = Pipeline([('std', StandardScaler()),('classifier', MLPClassifier(max_iter = 250))])\n",
    "\n",
    "    search_space = [\n",
    "                {\n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                {\n",
    "                 \n",
    "                  \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['sgd'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                \n",
    "                {\n",
    "                \n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['adam'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                ]\n",
    "    \n",
    "    runHyperParamSearch = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=5), \n",
    "                          scoring=['recall_micro', 'f1_micro','precision_micro','roc_auc_ovo','jaccard','neg_log_loss'], \n",
    "                          refit=False, verbose=10, n_jobs = -1)\n",
    "    TRIAL_1_MLP = runHyperParamSearch.fit(xSet, ySet)\n",
    "    print(\"---------------------------------TRIAL ONE RESULTS ---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.52405186, 0.54356666, 0.55397615, 0.78017063, 0.83071465,\n",
       "        0.82300725, 1.21004143, 1.21884818, 1.26288633, 1.48397665,\n",
       "        1.48677931, 1.43433266, 0.68869247, 0.57228956, 0.53876314,\n",
       "        0.73953705, 0.81039915, 0.83832216, 1.11956377, 1.1763123 ,\n",
       "        1.18111854, 1.42102203, 1.39659982, 1.35306139, 0.60522094,\n",
       "        0.54887137, 0.55227556, 0.73263068, 0.73373079, 0.80479193,\n",
       "        1.14568663, 1.17851372, 1.19402761, 1.43953724, 1.44213905,\n",
       "        1.36587195, 0.57619495, 0.54136381, 0.54076524, 0.7513464 ,\n",
       "        0.75465169, 0.79968781, 1.12827148, 1.14968772, 1.1918242 ,\n",
       "        1.47196603, 1.46515913, 1.41161304, 0.7027041 , 0.6368464 ,\n",
       "        0.62954016, 0.87565274, 0.92659774, 0.93110118, 1.35876837,\n",
       "        1.41171484, 1.42112241, 1.78653836, 1.75020576, 1.81576152,\n",
       "        0.73142781, 0.65025783, 0.64265289, 0.94921598, 0.93290286,\n",
       "        0.98234615, 1.39109497, 1.45184999, 1.50089087, 2.05186453,\n",
       "        1.98000326, 1.92115135, 0.77206526, 0.72542272, 0.63774705,\n",
       "        0.92679706, 0.93040075, 1.00166163, 1.41381679, 1.45394983,\n",
       "        1.42742743, 1.85909719, 1.90754118, 1.82796974, 0.83261576,\n",
       "        0.73483205, 0.64665651, 0.97794151, 0.9752398 , 1.02828422,\n",
       "        1.45194883, 1.48337541, 1.57605553, 1.92175221, 1.84068141,\n",
       "        1.77102232, 2.44019876, 0.20827885, 2.37123933, 2.5501935 ,\n",
       "        0.24501061, 2.61534963, 2.78569603, 0.49042196, 2.78159208,\n",
       "        2.75006542, 0.19156442, 2.74015632, 2.40436764, 0.16524234,\n",
       "        2.31348991, 2.39526062, 0.15573425, 2.5252717 , 2.65248208,\n",
       "        0.3383913 , 2.71193309, 2.85675654, 0.19386678, 2.78629632,\n",
       "        2.2922709 , 0.22819638, 2.33680897, 2.48053308, 0.55737967,\n",
       "        2.51166024, 2.77408633, 0.3868329 , 2.66198964, 2.72384276,\n",
       "        0.37932639, 2.7046258 , 2.21020131, 0.12861071, 2.20499601,\n",
       "        2.36373353, 0.33769054, 2.41808057, 2.6815062 , 0.27663774,\n",
       "        2.79410324, 2.94203053, 0.54176669, 2.82282791, 2.17476959,\n",
       "        0.46560001, 2.38575068, 2.54238672, 0.42726798, 2.5631042 ,\n",
       "        2.76197538, 1.22135019, 2.82082572, 2.92881851, 0.40004349,\n",
       "        2.89839234, 2.3693378 , 0.63054204, 2.33931174, 2.51366224,\n",
       "        0.74484043, 2.61615019, 2.77448635, 0.49082217, 2.78599563,\n",
       "        2.82012496, 0.98354592, 2.97695994, 2.52216926, 0.96883345,\n",
       "        2.68801188, 2.67189803, 0.62083402, 2.52187047, 2.70802894,\n",
       "        0.4789124 , 2.94673395, 3.22417264, 0.42206278, 3.52182889,\n",
       "        2.48443656, 0.35740752, 2.47242618, 2.63796878, 0.42476521,\n",
       "        2.56570654, 2.72894673, 0.62533851, 2.74065685, 2.89619064,\n",
       "        0.73172979, 2.92671652, 2.53327885, 2.55329576, 2.52006731,\n",
       "        2.63586707, 2.63176322, 2.68991356, 2.93982787, 2.93112044,\n",
       "        3.06393557, 3.17212777, 3.11017466, 3.14530463, 2.71423402,\n",
       "        2.63896933, 2.65698495, 2.6577857 , 2.70432601, 2.75256743,\n",
       "        3.00390239, 2.97746053, 3.00428305, 3.16622267, 3.35278339,\n",
       "        3.3030417 , 2.68701053, 2.67710242, 2.63056221, 2.73254967,\n",
       "        2.6995213 , 2.81321974, 2.98967118, 3.06923981, 3.20805893,\n",
       "        3.2026546 , 3.12288599, 3.10857382, 2.7236412 , 2.61905193,\n",
       "        2.68731103, 2.81401997, 2.72764544, 2.80401177, 3.04952292,\n",
       "        3.0894568 , 3.06293449, 3.16181941, 3.36129069, 3.15951753,\n",
       "        2.78459449, 2.77428594, 2.83864112, 2.92932   , 2.95624261,\n",
       "        2.96394906, 3.14650593, 3.27731867, 3.24378991, 3.28152246,\n",
       "        3.27892003, 3.34577723, 2.61294737, 2.62976141, 2.61374788,\n",
       "        2.78289351, 2.78579621, 2.95263948, 3.08845601, 3.20895934,\n",
       "        3.33576913, 3.32415891, 3.20405593, 3.2308784 , 2.59583254,\n",
       "        2.5860239 , 2.60373902, 2.74506092, 2.76337724, 2.7604744 ,\n",
       "        3.14660664, 3.12889061, 3.11467876, 3.27051334, 3.29773555,\n",
       "        3.32465887, 2.68951287, 2.75897279, 2.7428587 , 2.89649124,\n",
       "        2.94012837, 2.95474148, 3.28602581, 3.27711864, 3.38431015,\n",
       "        3.30294032, 3.0271029 , 2.80140858]),\n",
       " 'std_fit_time': array([0.00426309, 0.01694802, 0.0300938 , 0.03237876, 0.03263166,\n",
       "        0.04300706, 0.04865587, 0.06060099, 0.03592848, 0.06038826,\n",
       "        0.06206966, 0.0204755 , 0.08166304, 0.02861979, 0.01589193,\n",
       "        0.00747972, 0.041718  , 0.05644409, 0.02393735, 0.02376417,\n",
       "        0.03256114, 0.02900134, 0.02434638, 0.04135603, 0.08601882,\n",
       "        0.0366702 , 0.00828595, 0.01165108, 0.01537989, 0.01878094,\n",
       "        0.03436007, 0.02296661, 0.05647538, 0.03170048, 0.03909059,\n",
       "        0.04654766, 0.06759128, 0.05285393, 0.03706646, 0.02719934,\n",
       "        0.01848282, 0.02626591, 0.04891572, 0.06361979, 0.0211676 ,\n",
       "        0.07317438, 0.04095086, 0.06040512, 0.04761155, 0.0166161 ,\n",
       "        0.02185661, 0.02106671, 0.0431662 , 0.04116037, 0.05807375,\n",
       "        0.03546575, 0.03664858, 0.12303089, 0.08027642, 0.14264051,\n",
       "        0.03997054, 0.0062959 , 0.01698697, 0.01830064, 0.04075175,\n",
       "        0.04714099, 0.03749506, 0.05192874, 0.02419652, 0.13966699,\n",
       "        0.19921978, 0.11574777, 0.02883989, 0.0346871 , 0.01490046,\n",
       "        0.02546442, 0.02459811, 0.02324251, 0.0681317 , 0.08296005,\n",
       "        0.0423373 , 0.08206431, 0.15575925, 0.13172222, 0.09822659,\n",
       "        0.04651448, 0.00972549, 0.02745204, 0.08083568, 0.06920991,\n",
       "        0.05555443, 0.1133523 , 0.07891921, 0.10324453, 0.08650127,\n",
       "        0.08738245, 0.09163204, 0.05897282, 0.06341361, 0.13285249,\n",
       "        0.13672159, 0.15348221, 0.14231397, 0.33426027, 0.06816361,\n",
       "        0.01839072, 0.03002826, 0.09369372, 0.06923769, 0.03370873,\n",
       "        0.07017378, 0.07980382, 0.03446725, 0.02639997, 0.04540182,\n",
       "        0.26162021, 0.07858361, 0.06901681, 0.04549325, 0.08352575,\n",
       "        0.02148868, 0.08822569, 0.08261366, 0.08112335, 0.69710848,\n",
       "        0.02380891, 0.12441634, 0.31320801, 0.0677282 , 0.04749304,\n",
       "        0.27517003, 0.03695148, 0.01348932, 0.00772027, 0.01490699,\n",
       "        0.0167785 , 0.24069378, 0.05785924, 0.08063015, 0.2049773 ,\n",
       "        0.0894037 , 0.12229702, 0.71554127, 0.04589265, 0.47728168,\n",
       "        0.43318415, 0.04091155, 0.04362038, 0.16978692, 0.08264887,\n",
       "        0.05404718, 0.71251743, 0.07171322, 0.04445262, 0.20426293,\n",
       "        0.03824429, 0.0730471 , 0.33536797, 0.04234681, 0.06472413,\n",
       "        0.28743648, 0.08837306, 0.05823714, 0.21734463, 0.05336708,\n",
       "        0.02788575, 0.55174334, 0.19375978, 0.1009331 , 0.80231205,\n",
       "        0.10543426, 0.10483236, 0.29179577, 0.04661186, 0.05302328,\n",
       "        0.14969864, 0.15751582, 0.20170434, 0.1551234 , 0.11716088,\n",
       "        0.11657884, 0.18475041, 0.10830926, 0.10656738, 0.09026482,\n",
       "        0.01981867, 0.0379028 , 0.56436394, 0.02733745, 0.03712458,\n",
       "        0.35675152, 0.03552199, 0.02089663, 0.02473227, 0.01502425,\n",
       "        0.025651  , 0.05267239, 0.06478009, 0.02361453, 0.07369576,\n",
       "        0.035377  , 0.07839478, 0.11524931, 0.13860405, 0.12045174,\n",
       "        0.09247436, 0.0451951 , 0.04054921, 0.06918274, 0.05916126,\n",
       "        0.0946653 , 0.0633777 , 0.1039168 , 0.11852061, 0.17466803,\n",
       "        0.22250202, 0.12546131, 0.05378024, 0.06320524, 0.10542916,\n",
       "        0.05736818, 0.09568446, 0.06893064, 0.07374917, 0.13269356,\n",
       "        0.02595962, 0.07841577, 0.03382435, 0.12794235, 0.04604977,\n",
       "        0.16066602, 0.14896571, 0.03968844, 0.03950121, 0.03024728,\n",
       "        0.07066901, 0.07288192, 0.03824374, 0.20716616, 0.09519838,\n",
       "        0.18295171, 0.06116813, 0.04486773, 0.05888663, 0.14886566,\n",
       "        0.1408791 , 0.05335101, 0.09188649, 0.05665181, 0.04470651,\n",
       "        0.08829903, 0.0749802 , 0.01638063, 0.05298978, 0.05006961,\n",
       "        0.10648394, 0.09478255, 0.10407929, 0.07723386, 0.22665923,\n",
       "        0.16261428, 0.1020663 , 0.02294273, 0.07722415, 0.04912824,\n",
       "        0.05703517, 0.07888256, 0.04557645, 0.05509361, 0.06010911,\n",
       "        0.08081854, 0.05912439, 0.08385838, 0.09277879, 0.07017977,\n",
       "        0.10478127, 0.11800084, 0.10117664, 0.1618477 , 0.12122042,\n",
       "        0.09119984, 0.13691205, 0.05225589, 0.08560142, 0.14674955,\n",
       "        0.12216604, 0.05301094, 0.14125009]),\n",
       " 'mean_score_time': array([0.01461163, 0.01261158, 0.01431227, 0.01191282, 0.01251245,\n",
       "        0.01151114, 0.01751456, 0.01360979, 0.01481309, 0.01491218,\n",
       "        0.0127099 , 0.01181068, 0.01441112, 0.01130981, 0.01030908,\n",
       "        0.01381207, 0.01381021, 0.01431384, 0.01381273, 0.01411438,\n",
       "        0.01411119, 0.01301455, 0.01230979, 0.01060858, 0.01080828,\n",
       "        0.01070895, 0.01030793, 0.01080909, 0.01271081, 0.01251111,\n",
       "        0.01591258, 0.01391206, 0.01521282, 0.01391115, 0.01211081,\n",
       "        0.01070976, 0.01160941, 0.01000957, 0.01000957, 0.01221075,\n",
       "        0.01341119, 0.01431198, 0.0145123 , 0.01391344, 0.01431398,\n",
       "        0.01341033, 0.01281147, 0.01541338, 0.01421237, 0.01251097,\n",
       "        0.01120834, 0.01151018, 0.01271091, 0.01381216, 0.0135129 ,\n",
       "        0.01371222, 0.01251025, 0.01401224, 0.01271081, 0.01150975,\n",
       "        0.0111104 , 0.01030993, 0.0104084 , 0.01671562, 0.01411285,\n",
       "        0.01251044, 0.01301284, 0.0208168 , 0.0173141 , 0.01421299,\n",
       "        0.01270981, 0.01381216, 0.01861506, 0.01040821, 0.0129117 ,\n",
       "        0.01211061, 0.01661401, 0.0148119 , 0.01451359, 0.01261125,\n",
       "        0.01521392, 0.02211962, 0.01781397, 0.01311083, 0.01341138,\n",
       "        0.01190991, 0.01040826, 0.0173142 , 0.01501145, 0.01281176,\n",
       "        0.01301231, 0.0175149 , 0.01461329, 0.01631455, 0.01161041,\n",
       "        0.01251078, 0.01040916, 0.0093082 , 0.01050916, 0.01000838,\n",
       "        0.01251097, 0.01070933, 0.00960779, 0.00970807, 0.01070924,\n",
       "        0.0110095 , 0.00940847, 0.00980844, 0.00930767, 0.00880771,\n",
       "        0.01010857, 0.01681366, 0.00870714, 0.0101088 , 0.01070857,\n",
       "        0.00970864, 0.0105082 , 0.01130986, 0.009308  , 0.01010904,\n",
       "        0.0093082 , 0.00940804, 0.00950832, 0.01561346, 0.0095078 ,\n",
       "        0.00940766, 0.00940762, 0.00970812, 0.0096086 , 0.00970812,\n",
       "        0.00980859, 0.00990872, 0.00950794, 0.00890789, 0.00970883,\n",
       "        0.00940752, 0.0093082 , 0.01261015, 0.01180997, 0.00970855,\n",
       "        0.01050887, 0.010709  , 0.01030865, 0.01351185, 0.00990844,\n",
       "        0.01030893, 0.00980868, 0.01301088, 0.00900731, 0.01100907,\n",
       "        0.0097084 , 0.00980868, 0.01130958, 0.01080947, 0.00970826,\n",
       "        0.01060901, 0.01000819, 0.00990872, 0.0102087 , 0.01130929,\n",
       "        0.00920815, 0.01261072, 0.00990863, 0.00930815, 0.00970836,\n",
       "        0.01050949, 0.01000843, 0.01701493, 0.01341157, 0.00950789,\n",
       "        0.01241093, 0.00950823, 0.01050897, 0.00950632, 0.0108098 ,\n",
       "        0.00970864, 0.01561399, 0.01251097, 0.01010861, 0.01341152,\n",
       "        0.00940795, 0.0107089 , 0.01100965, 0.01040893, 0.00930781,\n",
       "        0.01120944, 0.01000881, 0.00960803, 0.01090984, 0.01030889,\n",
       "        0.00990839, 0.00990882, 0.00910754, 0.00980883, 0.01020899,\n",
       "        0.00960855, 0.01070933, 0.00990834, 0.01020927, 0.01151018,\n",
       "        0.01130972, 0.01120982, 0.01160965, 0.01191015, 0.01020875,\n",
       "        0.00950851, 0.01070933, 0.00940804, 0.0097084 , 0.01000891,\n",
       "        0.01099091, 0.00970869, 0.01301117, 0.01411252, 0.01200995,\n",
       "        0.00990739, 0.00920787, 0.00990849, 0.00920758, 0.01030931,\n",
       "        0.01171031, 0.02311954, 0.01311154, 0.0124105 , 0.01471272,\n",
       "        0.01050878, 0.01070895, 0.01030822, 0.01331162, 0.00960855,\n",
       "        0.01020894, 0.0097084 , 0.01171017, 0.01040859, 0.01040893,\n",
       "        0.01030889, 0.02071767, 0.01100945, 0.0115098 , 0.01030836,\n",
       "        0.01000838, 0.01271105, 0.01010857, 0.01170979, 0.01020861,\n",
       "        0.01601391, 0.01060934, 0.0099081 , 0.01291089, 0.01160932,\n",
       "        0.01231065, 0.01201029, 0.01010852, 0.00940866, 0.00970831,\n",
       "        0.01050887, 0.01000834, 0.00960855, 0.010109  , 0.00960832,\n",
       "        0.01020818, 0.01020918, 0.01010857, 0.01141   , 0.00930805,\n",
       "        0.00920792, 0.00980868, 0.00990825, 0.01160965, 0.00950766,\n",
       "        0.00990815, 0.01070976, 0.01060934, 0.01030817, 0.010109  ,\n",
       "        0.01231079, 0.00960822, 0.00960822, 0.01000862, 0.01170983,\n",
       "        0.01030893, 0.01361165, 0.01060915, 0.0119103 , 0.01531339,\n",
       "        0.01090927, 0.00760636, 0.00600543]),\n",
       " 'std_score_time': array([3.09145871e-03, 9.70538032e-04, 3.71173430e-03, 1.02079169e-03,\n",
       "        1.18506727e-03, 7.06044464e-04, 8.60984482e-03, 1.82773195e-03,\n",
       "        2.87528214e-03, 3.75126822e-03, 1.60040894e-03, 2.15978230e-03,\n",
       "        7.59975058e-03, 2.20735753e-03, 5.12134198e-04, 1.94082013e-03,\n",
       "        1.43649491e-03, 3.01039758e-03, 1.16695830e-03, 1.15584057e-03,\n",
       "        4.91593316e-04, 1.44864146e-03, 6.80033569e-04, 2.00677959e-04,\n",
       "        1.63140293e-03, 5.09930660e-04, 8.13778662e-04, 6.79407626e-04,\n",
       "        1.32744548e-03, 1.37897984e-03, 6.32586965e-03, 1.46485192e-03,\n",
       "        2.46696488e-03, 1.32118225e-03, 1.06946382e-03, 6.79435560e-04,\n",
       "        1.39462142e-03, 4.47821875e-04, 5.49860927e-04, 8.13003979e-04,\n",
       "        2.92483356e-03, 3.67101187e-03, 1.30705851e-03, 1.46124975e-03,\n",
       "        1.25140995e-03, 1.39331003e-03, 1.60113621e-03, 5.43936150e-03,\n",
       "        4.84701438e-03, 4.77949190e-03, 1.50409862e-03, 8.94336081e-04,\n",
       "        1.03099003e-03, 2.01741850e-03, 6.33317181e-04, 1.99081564e-03,\n",
       "        1.89877503e-03, 4.49818985e-04, 1.03142472e-03, 3.16282266e-04,\n",
       "        1.06803267e-03, 1.16675214e-03, 7.35845178e-04, 5.96822118e-03,\n",
       "        3.99553486e-03, 1.30552046e-03, 7.07494355e-04, 6.56768607e-03,\n",
       "        3.64467278e-03, 1.86093967e-03, 5.09962960e-04, 2.29611372e-03,\n",
       "        1.11966181e-02, 1.11464144e-03, 3.44296735e-03, 8.01635035e-04,\n",
       "        5.15633393e-03, 2.50357900e-03, 8.95349154e-04, 1.24149268e-03,\n",
       "        1.75087673e-03, 1.38221263e-02, 7.88519656e-03, 6.63693972e-04,\n",
       "        3.40157552e-03, 2.15594871e-03, 1.06885505e-03, 8.97330340e-03,\n",
       "        5.33426422e-03, 1.53769195e-03, 8.39367141e-04, 4.51133752e-03,\n",
       "        1.11426079e-03, 7.43349253e-03, 3.75209934e-04, 3.34916898e-03,\n",
       "        8.00556030e-04, 5.10070918e-04, 1.84535272e-03, 1.26604483e-03,\n",
       "        4.82111983e-03, 1.03060490e-03, 1.99914682e-04, 9.27819109e-04,\n",
       "        1.91522970e-03, 3.01932638e-03, 3.74418992e-04, 6.00862550e-04,\n",
       "        2.44736568e-04, 2.45223420e-04, 8.00538338e-04, 1.36218075e-02,\n",
       "        2.45009428e-04, 5.83789234e-04, 1.50467370e-03, 6.78626880e-04,\n",
       "        1.22568788e-03, 1.07794228e-03, 2.45495970e-04, 8.00836155e-04,\n",
       "        6.78936272e-04, 9.70218193e-04, 4.47607866e-04, 1.19830109e-02,\n",
       "        7.07190783e-04, 3.74011249e-04, 3.74572315e-04, 7.48946350e-04,\n",
       "        3.74457327e-04, 3.99971150e-04, 9.28400225e-04, 8.00585894e-04,\n",
       "        3.16733367e-04, 3.74381023e-04, 7.49424338e-04, 5.84132759e-04,\n",
       "        8.72001155e-04, 4.57982244e-03, 2.62137168e-03, 9.28307672e-04,\n",
       "        1.81811227e-03, 6.77656728e-04, 9.80669131e-04, 4.05266533e-03,\n",
       "        9.70188882e-04, 2.04093524e-03, 9.28112305e-04, 7.50987141e-03,\n",
       "        3.16958849e-04, 1.64409351e-03, 2.45184409e-04, 2.45301395e-04,\n",
       "        1.20910899e-03, 1.53762329e-03, 3.99900034e-04, 8.00597726e-04,\n",
       "        1.00073828e-03, 8.00764601e-04, 8.72230949e-04, 1.66289504e-03,\n",
       "        5.10285885e-04, 5.56705484e-03, 2.00607518e-04, 2.45612771e-04,\n",
       "        2.45223512e-04, 1.04996038e-03, 3.16582262e-04, 5.41694216e-03,\n",
       "        6.34083529e-03, 1.04984673e-03, 5.56774240e-03, 3.16506877e-04,\n",
       "        1.48453069e-03, 6.30044210e-04, 1.40134274e-03, 5.10360751e-04,\n",
       "        4.09528718e-03, 3.14908553e-03, 7.35890407e-04, 6.81323131e-03,\n",
       "        3.74406373e-04, 1.43632564e-03, 1.00102432e-03, 2.06116064e-03,\n",
       "        4.00400290e-04, 3.67222558e-03, 3.16657429e-04, 2.00510847e-04,\n",
       "        1.71654310e-03, 1.40132578e-03, 2.00367541e-04, 6.63744249e-04,\n",
       "        3.74431878e-04, 1.20918391e-03, 1.36488016e-03, 5.83347374e-04,\n",
       "        2.92847098e-03, 1.20074759e-03, 5.10005471e-04, 2.66660844e-03,\n",
       "        2.01709847e-03, 1.53766691e-03, 1.62664158e-03, 3.86896011e-03,\n",
       "        1.77869567e-03, 4.48194350e-04, 2.48397583e-03, 3.74686775e-04,\n",
       "        6.78416068e-04, 9.49971672e-04, 1.07632209e-03, 5.10622900e-04,\n",
       "        7.02784795e-03, 3.78973185e-03, 5.02433623e-03, 1.06886074e-03,\n",
       "        7.48321878e-04, 1.39411764e-03, 5.10370276e-04, 8.12973019e-04,\n",
       "        2.46353706e-03, 1.42607619e-02, 4.53607506e-03, 2.22447524e-03,\n",
       "        4.43778102e-03, 8.37482840e-04, 1.36510741e-03, 6.78858869e-04,\n",
       "        3.54734190e-03, 1.15888398e-03, 1.20949578e-03, 5.10501269e-04,\n",
       "        3.06177414e-03, 1.35775978e-03, 1.39432300e-03, 1.43632549e-03,\n",
       "        2.11737093e-02, 2.26028706e-03, 1.04984692e-03, 1.03007672e-03,\n",
       "        5.48205424e-04, 6.18291497e-03, 1.49802005e-03, 1.16738983e-03,\n",
       "        1.12340570e-03, 8.59223819e-03, 9.70365789e-04, 3.73948414e-04,\n",
       "        5.30964558e-03, 1.56340820e-03, 2.65946491e-03, 2.21592197e-03,\n",
       "        1.24213009e-03, 6.64075181e-04, 7.49405227e-04, 1.30522736e-03,\n",
       "        8.37568303e-04, 5.84165696e-04, 2.00486942e-04, 4.89969738e-04,\n",
       "        4.00638637e-04, 2.45690761e-04, 3.73947502e-04, 1.85644549e-03,\n",
       "        5.10108535e-04, 3.99971150e-04, 6.78668955e-04, 5.83895412e-04,\n",
       "        2.74814695e-03, 8.12024420e-07, 3.74406646e-04, 1.07778760e-03,\n",
       "        1.71554198e-03, 4.00138168e-04, 3.74788619e-04, 3.14286567e-03,\n",
       "        3.74304395e-04, 9.70365801e-04, 8.95135415e-04, 2.78808960e-03,\n",
       "        5.10108290e-04, 4.95829323e-03, 5.83641806e-04, 3.07514402e-03,\n",
       "        8.87087188e-03, 1.71613434e-03, 1.35721142e-03, 3.16733195e-04]),\n",
       " 'param_classifier': masked_array(data=[MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__activation': masked_array(data=['logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__alpha': masked_array(data=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__hidden_layer_sizes': masked_array(data=[(5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__learning_rate': masked_array(data=['constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'}],\n",
       " 'split0_test_recall_micro': array([0.852, 0.847, 0.848, 0.847, 0.855, 0.854, 0.859, 0.849, 0.853,\n",
       "        0.856, 0.863, 0.857, 0.847, 0.863, 0.848, 0.857, 0.853, 0.85 ,\n",
       "        0.865, 0.843, 0.856, 0.851, 0.864, 0.85 , 0.857, 0.849, 0.856,\n",
       "        0.852, 0.848, 0.855, 0.848, 0.86 , 0.87 , 0.851, 0.841, 0.864,\n",
       "        0.857, 0.861, 0.861, 0.85 , 0.856, 0.856, 0.862, 0.858, 0.851,\n",
       "        0.861, 0.852, 0.854, 0.854, 0.858, 0.853, 0.872, 0.856, 0.857,\n",
       "        0.854, 0.849, 0.851, 0.86 , 0.857, 0.853, 0.857, 0.862, 0.86 ,\n",
       "        0.865, 0.852, 0.856, 0.867, 0.859, 0.852, 0.853, 0.859, 0.855,\n",
       "        0.86 , 0.859, 0.852, 0.85 , 0.862, 0.861, 0.855, 0.859, 0.852,\n",
       "        0.841, 0.845, 0.855, 0.855, 0.857, 0.852, 0.856, 0.851, 0.86 ,\n",
       "        0.855, 0.852, 0.85 , 0.851, 0.849, 0.861, 0.785, 0.349, 0.782,\n",
       "        0.789, 0.698, 0.788, 0.789, 0.465, 0.787, 0.79 , 0.651, 0.786,\n",
       "        0.792, 0.669, 0.784, 0.786, 0.651, 0.788, 0.789, 0.469, 0.788,\n",
       "        0.787, 0.651, 0.789, 0.789, 0.349, 0.791, 0.787, 0.317, 0.784,\n",
       "        0.785, 0.346, 0.786, 0.789, 0.597, 0.787, 0.787, 0.686, 0.784,\n",
       "        0.786, 0.349, 0.789, 0.786, 0.349, 0.79 , 0.785, 0.651, 0.789,\n",
       "        0.789, 0.65 , 0.791, 0.807, 0.349, 0.797, 0.815, 0.344, 0.812,\n",
       "        0.812, 0.715, 0.801, 0.818, 0.63 , 0.798, 0.796, 0.666, 0.803,\n",
       "        0.799, 0.548, 0.804, 0.81 , 0.447, 0.802, 0.815, 0.325, 0.791,\n",
       "        0.809, 0.352, 0.816, 0.804, 0.577, 0.795, 0.795, 0.522, 0.8  ,\n",
       "        0.782, 0.616, 0.799, 0.813, 0.543, 0.808, 0.798, 0.624, 0.803,\n",
       "        0.802, 0.48 , 0.798, 0.828, 0.822, 0.803, 0.836, 0.831, 0.839,\n",
       "        0.841, 0.843, 0.839, 0.835, 0.843, 0.835, 0.823, 0.821, 0.807,\n",
       "        0.837, 0.831, 0.841, 0.841, 0.84 , 0.837, 0.84 , 0.84 , 0.84 ,\n",
       "        0.805, 0.82 , 0.798, 0.837, 0.839, 0.84 , 0.837, 0.838, 0.832,\n",
       "        0.842, 0.838, 0.838, 0.821, 0.825, 0.807, 0.833, 0.828, 0.837,\n",
       "        0.839, 0.838, 0.838, 0.84 , 0.836, 0.837, 0.85 , 0.851, 0.847,\n",
       "        0.858, 0.85 , 0.857, 0.861, 0.861, 0.862, 0.858, 0.858, 0.867,\n",
       "        0.85 , 0.848, 0.848, 0.861, 0.863, 0.867, 0.859, 0.865, 0.862,\n",
       "        0.859, 0.87 , 0.863, 0.853, 0.852, 0.851, 0.855, 0.858, 0.855,\n",
       "        0.858, 0.856, 0.854, 0.863, 0.859, 0.865, 0.849, 0.835, 0.837,\n",
       "        0.854, 0.86 , 0.851, 0.858, 0.859, 0.864, 0.857, 0.861, 0.86 ]),\n",
       " 'split1_test_recall_micro': array([0.839, 0.837, 0.838, 0.85 , 0.851, 0.854, 0.848, 0.842, 0.849,\n",
       "        0.86 , 0.844, 0.85 , 0.84 , 0.85 , 0.839, 0.857, 0.843, 0.841,\n",
       "        0.842, 0.844, 0.866, 0.844, 0.848, 0.859, 0.839, 0.844, 0.842,\n",
       "        0.848, 0.837, 0.855, 0.85 , 0.846, 0.833, 0.86 , 0.84 , 0.835,\n",
       "        0.851, 0.856, 0.842, 0.858, 0.857, 0.851, 0.851, 0.849, 0.852,\n",
       "        0.866, 0.848, 0.849, 0.838, 0.845, 0.841, 0.844, 0.845, 0.837,\n",
       "        0.843, 0.854, 0.832, 0.856, 0.838, 0.839, 0.839, 0.843, 0.84 ,\n",
       "        0.848, 0.852, 0.85 , 0.844, 0.868, 0.852, 0.856, 0.848, 0.854,\n",
       "        0.836, 0.833, 0.846, 0.849, 0.848, 0.862, 0.843, 0.847, 0.838,\n",
       "        0.832, 0.838, 0.85 , 0.839, 0.845, 0.847, 0.851, 0.849, 0.856,\n",
       "        0.846, 0.859, 0.857, 0.849, 0.849, 0.841, 0.775, 0.651, 0.776,\n",
       "        0.775, 0.349, 0.778, 0.778, 0.349, 0.778, 0.779, 0.651, 0.782,\n",
       "        0.772, 0.4  , 0.773, 0.773, 0.651, 0.785, 0.782, 0.651, 0.787,\n",
       "        0.782, 0.651, 0.784, 0.771, 0.349, 0.776, 0.783, 0.652, 0.781,\n",
       "        0.776, 0.651, 0.778, 0.785, 0.344, 0.786, 0.767, 0.654, 0.781,\n",
       "        0.776, 0.5  , 0.778, 0.785, 0.674, 0.782, 0.783, 0.349, 0.784,\n",
       "        0.783, 0.554, 0.817, 0.817, 0.624, 0.816, 0.81 , 0.496, 0.792,\n",
       "        0.808, 0.593, 0.817, 0.791, 0.528, 0.785, 0.815, 0.313, 0.811,\n",
       "        0.809, 0.51 , 0.817, 0.795, 0.604, 0.805, 0.807, 0.324, 0.791,\n",
       "        0.791, 0.316, 0.811, 0.803, 0.718, 0.814, 0.801, 0.675, 0.799,\n",
       "        0.798, 0.486, 0.798, 0.806, 0.581, 0.806, 0.803, 0.623, 0.806,\n",
       "        0.805, 0.598, 0.803, 0.823, 0.828, 0.81 , 0.836, 0.829, 0.837,\n",
       "        0.836, 0.838, 0.836, 0.838, 0.838, 0.841, 0.814, 0.82 , 0.805,\n",
       "        0.832, 0.835, 0.828, 0.838, 0.84 , 0.835, 0.838, 0.837, 0.835,\n",
       "        0.806, 0.821, 0.826, 0.83 , 0.832, 0.832, 0.835, 0.838, 0.835,\n",
       "        0.842, 0.839, 0.834, 0.823, 0.821, 0.814, 0.832, 0.828, 0.824,\n",
       "        0.838, 0.836, 0.83 , 0.833, 0.838, 0.824, 0.835, 0.84 , 0.844,\n",
       "        0.841, 0.851, 0.844, 0.856, 0.851, 0.845, 0.844, 0.842, 0.849,\n",
       "        0.844, 0.827, 0.844, 0.841, 0.848, 0.853, 0.846, 0.851, 0.849,\n",
       "        0.855, 0.851, 0.844, 0.83 , 0.832, 0.841, 0.851, 0.838, 0.838,\n",
       "        0.847, 0.848, 0.838, 0.846, 0.847, 0.848, 0.839, 0.836, 0.835,\n",
       "        0.838, 0.842, 0.847, 0.839, 0.842, 0.84 , 0.836, 0.847, 0.84 ]),\n",
       " 'split2_test_recall_micro': array([0.849, 0.847, 0.847, 0.863, 0.845, 0.87 , 0.862, 0.87 , 0.872,\n",
       "        0.862, 0.869, 0.857, 0.849, 0.847, 0.843, 0.849, 0.851, 0.849,\n",
       "        0.861, 0.853, 0.855, 0.859, 0.861, 0.858, 0.848, 0.855, 0.842,\n",
       "        0.86 , 0.863, 0.867, 0.861, 0.857, 0.867, 0.869, 0.854, 0.871,\n",
       "        0.847, 0.851, 0.845, 0.865, 0.864, 0.865, 0.87 , 0.852, 0.857,\n",
       "        0.862, 0.861, 0.862, 0.842, 0.846, 0.841, 0.874, 0.862, 0.863,\n",
       "        0.863, 0.849, 0.862, 0.86 , 0.855, 0.861, 0.842, 0.851, 0.848,\n",
       "        0.838, 0.848, 0.847, 0.859, 0.858, 0.846, 0.861, 0.849, 0.863,\n",
       "        0.846, 0.851, 0.845, 0.864, 0.842, 0.858, 0.862, 0.854, 0.852,\n",
       "        0.863, 0.851, 0.885, 0.859, 0.852, 0.845, 0.863, 0.856, 0.848,\n",
       "        0.85 , 0.864, 0.866, 0.866, 0.854, 0.862, 0.773, 0.659, 0.773,\n",
       "        0.763, 0.606, 0.778, 0.777, 0.331, 0.771, 0.775, 0.651, 0.781,\n",
       "        0.775, 0.385, 0.762, 0.765, 0.421, 0.77 , 0.779, 0.348, 0.775,\n",
       "        0.776, 0.528, 0.778, 0.774, 0.352, 0.763, 0.78 , 0.349, 0.773,\n",
       "        0.776, 0.651, 0.776, 0.775, 0.651, 0.777, 0.773, 0.651, 0.758,\n",
       "        0.779, 0.651, 0.767, 0.773, 0.651, 0.773, 0.775, 0.651, 0.778,\n",
       "        0.792, 0.446, 0.79 , 0.789, 0.678, 0.814, 0.798, 0.307, 0.79 ,\n",
       "        0.814, 0.664, 0.798, 0.794, 0.377, 0.794, 0.8  , 0.482, 0.818,\n",
       "        0.804, 0.723, 0.799, 0.808, 0.468, 0.798, 0.808, 0.669, 0.789,\n",
       "        0.813, 0.469, 0.798, 0.805, 0.481, 0.8  , 0.795, 0.536, 0.8  ,\n",
       "        0.802, 0.62 , 0.801, 0.802, 0.618, 0.8  , 0.806, 0.311, 0.806,\n",
       "        0.803, 0.533, 0.8  , 0.803, 0.819, 0.814, 0.815, 0.827, 0.82 ,\n",
       "        0.825, 0.824, 0.822, 0.824, 0.83 , 0.822, 0.813, 0.819, 0.809,\n",
       "        0.823, 0.824, 0.83 , 0.827, 0.82 , 0.83 , 0.823, 0.825, 0.825,\n",
       "        0.815, 0.817, 0.813, 0.817, 0.814, 0.824, 0.83 , 0.825, 0.826,\n",
       "        0.822, 0.831, 0.823, 0.799, 0.81 , 0.798, 0.819, 0.815, 0.815,\n",
       "        0.818, 0.821, 0.817, 0.816, 0.82 , 0.817, 0.834, 0.837, 0.824,\n",
       "        0.84 , 0.85 , 0.85 , 0.854, 0.851, 0.849, 0.858, 0.844, 0.85 ,\n",
       "        0.834, 0.836, 0.831, 0.845, 0.846, 0.842, 0.849, 0.847, 0.846,\n",
       "        0.854, 0.851, 0.848, 0.837, 0.834, 0.834, 0.839, 0.848, 0.848,\n",
       "        0.847, 0.85 , 0.852, 0.851, 0.848, 0.848, 0.838, 0.832, 0.835,\n",
       "        0.843, 0.845, 0.84 , 0.848, 0.844, 0.842, 0.844, 0.851, 0.849]),\n",
       " 'split3_test_recall_micro': array([0.852, 0.854, 0.857, 0.86 , 0.852, 0.859, 0.858, 0.867, 0.863,\n",
       "        0.871, 0.878, 0.879, 0.859, 0.866, 0.864, 0.873, 0.873, 0.869,\n",
       "        0.87 , 0.865, 0.859, 0.869, 0.862, 0.861, 0.847, 0.858, 0.869,\n",
       "        0.874, 0.877, 0.872, 0.853, 0.873, 0.874, 0.861, 0.86 , 0.856,\n",
       "        0.852, 0.862, 0.854, 0.872, 0.871, 0.872, 0.876, 0.861, 0.867,\n",
       "        0.876, 0.884, 0.871, 0.858, 0.856, 0.864, 0.863, 0.871, 0.854,\n",
       "        0.853, 0.871, 0.859, 0.872, 0.869, 0.866, 0.85 , 0.86 , 0.861,\n",
       "        0.866, 0.872, 0.86 , 0.857, 0.857, 0.854, 0.864, 0.873, 0.862,\n",
       "        0.861, 0.85 , 0.856, 0.863, 0.851, 0.859, 0.852, 0.859, 0.86 ,\n",
       "        0.863, 0.864, 0.834, 0.855, 0.851, 0.862, 0.879, 0.869, 0.869,\n",
       "        0.868, 0.86 , 0.865, 0.864, 0.869, 0.866, 0.796, 0.349, 0.79 ,\n",
       "        0.797, 0.484, 0.799, 0.799, 0.651, 0.786, 0.796, 0.653, 0.792,\n",
       "        0.791, 0.46 , 0.791, 0.783, 0.655, 0.795, 0.795, 0.655, 0.797,\n",
       "        0.795, 0.652, 0.794, 0.79 , 0.655, 0.789, 0.802, 0.376, 0.79 ,\n",
       "        0.792, 0.349, 0.801, 0.798, 0.651, 0.793, 0.793, 0.651, 0.792,\n",
       "        0.791, 0.349, 0.794, 0.794, 0.672, 0.795, 0.794, 0.651, 0.796,\n",
       "        0.817, 0.405, 0.807, 0.809, 0.718, 0.831, 0.83 , 0.533, 0.82 ,\n",
       "        0.845, 0.631, 0.838, 0.824, 0.422, 0.82 , 0.802, 0.5  , 0.804,\n",
       "        0.83 , 0.585, 0.841, 0.82 , 0.665, 0.826, 0.81 , 0.553, 0.802,\n",
       "        0.838, 0.618, 0.824, 0.827, 0.502, 0.823, 0.826, 0.676, 0.824,\n",
       "        0.811, 0.665, 0.834, 0.836, 0.548, 0.807, 0.833, 0.538, 0.832,\n",
       "        0.837, 0.69 , 0.821, 0.846, 0.853, 0.842, 0.849, 0.848, 0.844,\n",
       "        0.846, 0.852, 0.851, 0.849, 0.852, 0.851, 0.843, 0.844, 0.835,\n",
       "        0.84 , 0.848, 0.848, 0.849, 0.849, 0.853, 0.849, 0.852, 0.849,\n",
       "        0.81 , 0.842, 0.835, 0.846, 0.846, 0.846, 0.849, 0.849, 0.848,\n",
       "        0.85 , 0.849, 0.851, 0.824, 0.807, 0.815, 0.814, 0.843, 0.844,\n",
       "        0.848, 0.847, 0.849, 0.845, 0.848, 0.846, 0.849, 0.865, 0.842,\n",
       "        0.858, 0.86 , 0.85 , 0.865, 0.869, 0.869, 0.874, 0.869, 0.866,\n",
       "        0.856, 0.851, 0.854, 0.856, 0.86 , 0.858, 0.868, 0.86 , 0.864,\n",
       "        0.859, 0.862, 0.868, 0.85 , 0.847, 0.868, 0.858, 0.849, 0.869,\n",
       "        0.865, 0.861, 0.873, 0.866, 0.872, 0.862, 0.85 , 0.851, 0.855,\n",
       "        0.853, 0.856, 0.856, 0.867, 0.869, 0.874, 0.866, 0.866, 0.862]),\n",
       " 'split4_test_recall_micro': array([0.839, 0.849, 0.84 , 0.848, 0.851, 0.855, 0.851, 0.843, 0.853,\n",
       "        0.851, 0.853, 0.836, 0.838, 0.827, 0.839, 0.841, 0.831, 0.854,\n",
       "        0.836, 0.848, 0.834, 0.843, 0.828, 0.827, 0.829, 0.825, 0.834,\n",
       "        0.839, 0.861, 0.839, 0.843, 0.856, 0.849, 0.846, 0.84 , 0.833,\n",
       "        0.825, 0.832, 0.83 , 0.839, 0.839, 0.848, 0.846, 0.851, 0.839,\n",
       "        0.843, 0.845, 0.844, 0.834, 0.845, 0.842, 0.85 , 0.837, 0.84 ,\n",
       "        0.841, 0.831, 0.834, 0.846, 0.826, 0.843, 0.845, 0.841, 0.839,\n",
       "        0.833, 0.844, 0.841, 0.83 , 0.826, 0.837, 0.857, 0.84 , 0.843,\n",
       "        0.838, 0.841, 0.835, 0.84 , 0.858, 0.843, 0.843, 0.84 , 0.846,\n",
       "        0.841, 0.856, 0.834, 0.832, 0.841, 0.844, 0.846, 0.845, 0.844,\n",
       "        0.847, 0.829, 0.837, 0.847, 0.834, 0.844, 0.767, 0.605, 0.782,\n",
       "        0.777, 0.651, 0.781, 0.773, 0.485, 0.77 , 0.778, 0.651, 0.78 ,\n",
       "        0.768, 0.318, 0.775, 0.781, 0.651, 0.774, 0.775, 0.651, 0.784,\n",
       "        0.778, 0.664, 0.779, 0.774, 0.651, 0.777, 0.777, 0.676, 0.779,\n",
       "        0.78 , 0.651, 0.782, 0.779, 0.615, 0.777, 0.757, 0.651, 0.777,\n",
       "        0.778, 0.391, 0.774, 0.775, 0.486, 0.777, 0.776, 0.651, 0.777,\n",
       "        0.796, 0.651, 0.783, 0.785, 0.486, 0.785, 0.788, 0.61 , 0.796,\n",
       "        0.792, 0.683, 0.795, 0.794, 0.527, 0.792, 0.787, 0.46 , 0.794,\n",
       "        0.794, 0.659, 0.792, 0.801, 0.556, 0.788, 0.787, 0.324, 0.8  ,\n",
       "        0.791, 0.498, 0.796, 0.791, 0.657, 0.79 , 0.796, 0.691, 0.789,\n",
       "        0.787, 0.666, 0.784, 0.781, 0.481, 0.797, 0.792, 0.652, 0.787,\n",
       "        0.785, 0.535, 0.789, 0.796, 0.79 , 0.791, 0.815, 0.814, 0.808,\n",
       "        0.817, 0.817, 0.821, 0.817, 0.821, 0.817, 0.786, 0.803, 0.813,\n",
       "        0.821, 0.803, 0.811, 0.818, 0.815, 0.817, 0.818, 0.818, 0.821,\n",
       "        0.793, 0.816, 0.785, 0.814, 0.817, 0.811, 0.822, 0.814, 0.821,\n",
       "        0.82 , 0.824, 0.818, 0.814, 0.798, 0.79 , 0.81 , 0.799, 0.808,\n",
       "        0.811, 0.813, 0.814, 0.818, 0.816, 0.811, 0.821, 0.818, 0.812,\n",
       "        0.843, 0.826, 0.834, 0.838, 0.837, 0.837, 0.833, 0.842, 0.846,\n",
       "        0.827, 0.827, 0.826, 0.833, 0.832, 0.835, 0.841, 0.845, 0.846,\n",
       "        0.842, 0.84 , 0.841, 0.826, 0.821, 0.821, 0.839, 0.833, 0.836,\n",
       "        0.832, 0.843, 0.841, 0.837, 0.843, 0.841, 0.822, 0.827, 0.824,\n",
       "        0.829, 0.835, 0.838, 0.834, 0.844, 0.837, 0.84 , 0.841, 0.847]),\n",
       " 'mean_test_recall_micro': array([0.8462, 0.8468, 0.846 , 0.8536, 0.8508, 0.8584, 0.8556, 0.8542,\n",
       "        0.858 , 0.86  , 0.8614, 0.8558, 0.8466, 0.8506, 0.8466, 0.8554,\n",
       "        0.8502, 0.8526, 0.8548, 0.8506, 0.854 , 0.8532, 0.8526, 0.851 ,\n",
       "        0.844 , 0.8462, 0.8486, 0.8546, 0.8572, 0.8576, 0.851 , 0.8584,\n",
       "        0.8586, 0.8574, 0.847 , 0.8518, 0.8464, 0.8524, 0.8464, 0.8568,\n",
       "        0.8574, 0.8584, 0.861 , 0.8542, 0.8532, 0.8616, 0.858 , 0.856 ,\n",
       "        0.8452, 0.85  , 0.8482, 0.8606, 0.8542, 0.8502, 0.8508, 0.8508,\n",
       "        0.8476, 0.8588, 0.849 , 0.8524, 0.8466, 0.8514, 0.8496, 0.85  ,\n",
       "        0.8536, 0.8508, 0.8514, 0.8536, 0.8482, 0.8582, 0.8538, 0.8554,\n",
       "        0.8482, 0.8468, 0.8468, 0.8532, 0.8522, 0.8566, 0.851 , 0.8518,\n",
       "        0.8496, 0.848 , 0.8508, 0.8516, 0.848 , 0.8492, 0.85  , 0.859 ,\n",
       "        0.854 , 0.8554, 0.8532, 0.8528, 0.855 , 0.8554, 0.851 , 0.8548,\n",
       "        0.7792, 0.5226, 0.7806, 0.7802, 0.5576, 0.7848, 0.7832, 0.4562,\n",
       "        0.7784, 0.7836, 0.6514, 0.7842, 0.7796, 0.4464, 0.777 , 0.7776,\n",
       "        0.6058, 0.7824, 0.784 , 0.5548, 0.7862, 0.7836, 0.6292, 0.7848,\n",
       "        0.7796, 0.4712, 0.7792, 0.7858, 0.474 , 0.7814, 0.7818, 0.5296,\n",
       "        0.7846, 0.7852, 0.5716, 0.784 , 0.7754, 0.6586, 0.7784, 0.782 ,\n",
       "        0.448 , 0.7804, 0.7826, 0.5664, 0.7834, 0.7826, 0.5906, 0.7848,\n",
       "        0.7954, 0.5412, 0.7976, 0.8014, 0.571 , 0.8086, 0.8082, 0.458 ,\n",
       "        0.802 , 0.8142, 0.6572, 0.8098, 0.8042, 0.4968, 0.7978, 0.8   ,\n",
       "        0.4842, 0.806 , 0.8072, 0.605 , 0.8106, 0.8068, 0.548 , 0.8038,\n",
       "        0.8054, 0.439 , 0.7946, 0.8084, 0.4506, 0.809 , 0.806 , 0.587 ,\n",
       "        0.8044, 0.8026, 0.62  , 0.8024, 0.796 , 0.6106, 0.8032, 0.8076,\n",
       "        0.5542, 0.8036, 0.8064, 0.5496, 0.8068, 0.8064, 0.5672, 0.8022,\n",
       "        0.8192, 0.8224, 0.812 , 0.8302, 0.8298, 0.8296, 0.833 , 0.8348,\n",
       "        0.8338, 0.8326, 0.8368, 0.8332, 0.8158, 0.8214, 0.8138, 0.8306,\n",
       "        0.8282, 0.8316, 0.8346, 0.8328, 0.8344, 0.8336, 0.8344, 0.834 ,\n",
       "        0.8058, 0.8232, 0.8114, 0.8288, 0.8296, 0.8306, 0.8346, 0.8328,\n",
       "        0.8324, 0.8352, 0.8362, 0.8328, 0.8162, 0.8122, 0.8048, 0.8216,\n",
       "        0.8226, 0.8256, 0.8308, 0.831 , 0.8296, 0.8304, 0.8316, 0.827 ,\n",
       "        0.8378, 0.8422, 0.8338, 0.848 , 0.8474, 0.847 , 0.8548, 0.8538,\n",
       "        0.8524, 0.8534, 0.851 , 0.8556, 0.8422, 0.8378, 0.8406, 0.8472,\n",
       "        0.8498, 0.851 , 0.8526, 0.8536, 0.8534, 0.8538, 0.8548, 0.8528,\n",
       "        0.8392, 0.8372, 0.843 , 0.8484, 0.8452, 0.8492, 0.8498, 0.8516,\n",
       "        0.8516, 0.8526, 0.8538, 0.8528, 0.8396, 0.8362, 0.8372, 0.8434,\n",
       "        0.8476, 0.8464, 0.8492, 0.8516, 0.8514, 0.8486, 0.8532, 0.8516]),\n",
       " 'std_test_recall_micro': array([0.00597997, 0.00552811, 0.00672309, 0.0065909 , 0.00324962,\n",
       "        0.00608605, 0.00523832, 0.01195659, 0.00839047, 0.00666333,\n",
       "        0.01190966, 0.01390539, 0.0074458 , 0.01386506, 0.00930806,\n",
       "        0.0106132 , 0.01377534, 0.00922171, 0.01334766, 0.00801499,\n",
       "        0.01071448, 0.00976524, 0.01352923, 0.01256981, 0.00942338,\n",
       "        0.01165161, 0.01241934, 0.0118254 , 0.0136587 , 0.01144727,\n",
       "        0.00596657, 0.00868562, 0.0153961 , 0.00806474, 0.00839047,\n",
       "        0.01530229, 0.01116423, 0.01092886, 0.01059434, 0.01151347,\n",
       "        0.01066958, 0.00891291, 0.01124278, 0.00453431, 0.00908625,\n",
       "        0.01070701, 0.01407125, 0.00957079, 0.00926067, 0.00576194,\n",
       "        0.00910824, 0.01185917, 0.01205653, 0.01002796, 0.00800999,\n",
       "        0.01278124, 0.01246756, 0.00835225, 0.01516575, 0.0102684 ,\n",
       "        0.0063435 , 0.00854634, 0.0094361 , 0.01354991, 0.00966644,\n",
       "        0.00667533, 0.01300154, 0.01434713, 0.00620967, 0.00386782,\n",
       "        0.01133843, 0.00717217, 0.01059056, 0.00895321, 0.00713863,\n",
       "        0.00910824, 0.00711056, 0.0069455 , 0.00729383, 0.00735935,\n",
       "        0.00731027, 0.01268069, 0.00893085, 0.01870401, 0.01054514,\n",
       "        0.0056    , 0.00660303, 0.0114717 , 0.00829458, 0.00884534,\n",
       "        0.00803492, 0.01251239, 0.01071448, 0.00796492, 0.01122497,\n",
       "        0.01022546, 0.01020588, 0.14293719, 0.0058515 , 0.01177115,\n",
       "        0.126261  , 0.00798499, 0.0095163 , 0.11490936, 0.00717217,\n",
       "        0.00801499, 0.0008    , 0.0044    , 0.00997196, 0.12012094,\n",
       "        0.00989949, 0.00763151, 0.09241299, 0.00917824, 0.00715542,\n",
       "        0.12544066, 0.00708237, 0.00682935, 0.05083857, 0.00604649,\n",
       "        0.00816333, 0.14844851, 0.01012719, 0.00874986, 0.15643913,\n",
       "        0.00560714, 0.00607947, 0.14868705, 0.00889044, 0.00800999,\n",
       "        0.11570583, 0.00619677, 0.01310878, 0.01374918, 0.01132431,\n",
       "        0.00562139, 0.11555432, 0.00985089, 0.00770973, 0.12931605,\n",
       "        0.00811419, 0.00688767, 0.1208    , 0.00708237, 0.01160345,\n",
       "        0.10165707, 0.01248359, 0.01228983, 0.13592351, 0.01598249,\n",
       "        0.0144    , 0.11486514, 0.01186592, 0.01723253, 0.04209703,\n",
       "        0.01601749, 0.013891  , 0.08896606, 0.01187266, 0.00909945,\n",
       "        0.11242847, 0.00807465, 0.01244829, 0.07682968, 0.01725804,\n",
       "        0.00847113, 0.08184131, 0.0124964 , 0.00960417, 0.14514958,\n",
       "        0.00531413, 0.01733897, 0.10811771, 0.01065833, 0.0116619 ,\n",
       "        0.09013545, 0.01227355, 0.01190966, 0.07464851, 0.01156892,\n",
       "        0.01041153, 0.06583495, 0.0165336 , 0.01776063, 0.04541101,\n",
       "        0.00431741, 0.01412232, 0.12527825, 0.01444161, 0.01689497,\n",
       "        0.07188713, 0.01049571, 0.01794882, 0.02014547, 0.01691153,\n",
       "        0.01328759, 0.01087014, 0.01348481, 0.01060189, 0.01270276,\n",
       "        0.0112321 , 0.0111463 , 0.01064707, 0.0124    , 0.01838913,\n",
       "        0.01309351, 0.0109252 , 0.00749933, 0.0148243 , 0.01262696,\n",
       "        0.0108922 , 0.01301384, 0.01162067, 0.01142979, 0.01187603,\n",
       "        0.01011929, 0.00730479, 0.00957914, 0.0181615 , 0.01202331,\n",
       "        0.01237093, 0.01228983, 0.00886792, 0.01208967, 0.00917824,\n",
       "        0.0119733 , 0.00837616, 0.01161723, 0.00928224, 0.00974474,\n",
       "        0.00957914, 0.00935094, 0.0147594 , 0.01336563, 0.01393413,\n",
       "        0.01228007, 0.01303227, 0.01160345, 0.01189285, 0.01285302,\n",
       "        0.01075918, 0.01558717, 0.01354105, 0.00822192, 0.01134196,\n",
       "        0.00769415, 0.00923905, 0.01077775, 0.0115862 , 0.013937  ,\n",
       "        0.0108074 , 0.00900222, 0.01051475, 0.01014692, 0.01049952,\n",
       "        0.01012719, 0.01107068, 0.01136662, 0.0096871 , 0.00768375,\n",
       "        0.00793977, 0.00624179, 0.0103034 , 0.01072194, 0.01068457,\n",
       "        0.01108873, 0.0158619 , 0.00798999, 0.00879545, 0.01205653,\n",
       "        0.0112321 , 0.00628013, 0.01233856, 0.0107443 , 0.01053376,\n",
       "        0.00915205, 0.01009158, 0.00803492, 0.010008  , 0.00939361,\n",
       "        0.00917824, 0.00671118, 0.01208967, 0.01063203, 0.0148    ,\n",
       "        0.0112    , 0.00913017, 0.00826075]),\n",
       " 'rank_test_recall_micro': array([126, 117, 128,  44,  83,   9,  23,  35,  13,   5,   2,  22, 120,\n",
       "         87, 120,  25,  90,  58,  30,  87,  38,  50,  58,  76, 131, 126,\n",
       "        103,  34,  18,  15,  76,   9,   8,  16, 115,  67, 124,  62, 123,\n",
       "         19,  16,   9,   3,  35,  50,   1,  13,  21, 129,  91, 105,   4,\n",
       "         35,  89,  82,  83, 111,   7, 101,  62, 120,  73,  96,  91,  44,\n",
       "         83,  73,  44, 105,  12,  40,  25, 105, 117, 117,  50,  65,  20,\n",
       "         76,  66,  96, 108,  83,  68, 108,  99,  91,   6,  38,  25,  50,\n",
       "         56,  29,  28,  76,  30, 250, 278, 245, 247, 271, 228, 238, 284,\n",
       "        253, 235, 259, 232, 249, 287, 255, 254, 263, 241, 233, 272, 225,\n",
       "        235, 260, 230, 248, 282, 250, 226, 281, 244, 243, 277, 231, 227,\n",
       "        267, 233, 256, 257, 252, 242, 286, 246, 239, 270, 237, 239, 265,\n",
       "        228, 223, 276, 221, 218, 268, 195, 197, 283, 217, 187, 258, 193,\n",
       "        210, 279, 220, 219, 280, 205, 199, 264, 192, 200, 275, 211, 207,\n",
       "        288, 224, 196, 285, 194, 204, 266, 209, 214, 261, 215, 222, 262,\n",
       "        213, 198, 273, 212, 202, 274, 200, 202, 269, 216, 184, 181, 190,\n",
       "        170, 171, 172, 157, 147, 153, 161, 143, 156, 186, 183, 188, 168,\n",
       "        176, 163, 149, 158, 150, 155, 150, 152, 206, 179, 191, 175, 172,\n",
       "        167, 148, 158, 162, 146, 144, 158, 185, 189, 208, 182, 180, 178,\n",
       "        166, 165, 172, 169, 164, 177, 139, 135, 153, 108, 113, 115,  30,\n",
       "         43,  64,  48,  76,  24, 134, 139, 136, 114,  94,  76,  58,  44,\n",
       "         49,  40,  30,  56, 138, 141, 133, 104, 129,  98,  94,  68,  68,\n",
       "         58,  40,  55, 137, 145, 141, 132, 111, 124,  99,  68,  73, 102,\n",
       "         50,  68]),\n",
       " 'split0_test_f1_micro': array([0.852, 0.847, 0.848, 0.847, 0.855, 0.854, 0.859, 0.849, 0.853,\n",
       "        0.856, 0.863, 0.857, 0.847, 0.863, 0.848, 0.857, 0.853, 0.85 ,\n",
       "        0.865, 0.843, 0.856, 0.851, 0.864, 0.85 , 0.857, 0.849, 0.856,\n",
       "        0.852, 0.848, 0.855, 0.848, 0.86 , 0.87 , 0.851, 0.841, 0.864,\n",
       "        0.857, 0.861, 0.861, 0.85 , 0.856, 0.856, 0.862, 0.858, 0.851,\n",
       "        0.861, 0.852, 0.854, 0.854, 0.858, 0.853, 0.872, 0.856, 0.857,\n",
       "        0.854, 0.849, 0.851, 0.86 , 0.857, 0.853, 0.857, 0.862, 0.86 ,\n",
       "        0.865, 0.852, 0.856, 0.867, 0.859, 0.852, 0.853, 0.859, 0.855,\n",
       "        0.86 , 0.859, 0.852, 0.85 , 0.862, 0.861, 0.855, 0.859, 0.852,\n",
       "        0.841, 0.845, 0.855, 0.855, 0.857, 0.852, 0.856, 0.851, 0.86 ,\n",
       "        0.855, 0.852, 0.85 , 0.851, 0.849, 0.861, 0.785, 0.349, 0.782,\n",
       "        0.789, 0.698, 0.788, 0.789, 0.465, 0.787, 0.79 , 0.651, 0.786,\n",
       "        0.792, 0.669, 0.784, 0.786, 0.651, 0.788, 0.789, 0.469, 0.788,\n",
       "        0.787, 0.651, 0.789, 0.789, 0.349, 0.791, 0.787, 0.317, 0.784,\n",
       "        0.785, 0.346, 0.786, 0.789, 0.597, 0.787, 0.787, 0.686, 0.784,\n",
       "        0.786, 0.349, 0.789, 0.786, 0.349, 0.79 , 0.785, 0.651, 0.789,\n",
       "        0.789, 0.65 , 0.791, 0.807, 0.349, 0.797, 0.815, 0.344, 0.812,\n",
       "        0.812, 0.715, 0.801, 0.818, 0.63 , 0.798, 0.796, 0.666, 0.803,\n",
       "        0.799, 0.548, 0.804, 0.81 , 0.447, 0.802, 0.815, 0.325, 0.791,\n",
       "        0.809, 0.352, 0.816, 0.804, 0.577, 0.795, 0.795, 0.522, 0.8  ,\n",
       "        0.782, 0.616, 0.799, 0.813, 0.543, 0.808, 0.798, 0.624, 0.803,\n",
       "        0.802, 0.48 , 0.798, 0.828, 0.822, 0.803, 0.836, 0.831, 0.839,\n",
       "        0.841, 0.843, 0.839, 0.835, 0.843, 0.835, 0.823, 0.821, 0.807,\n",
       "        0.837, 0.831, 0.841, 0.841, 0.84 , 0.837, 0.84 , 0.84 , 0.84 ,\n",
       "        0.805, 0.82 , 0.798, 0.837, 0.839, 0.84 , 0.837, 0.838, 0.832,\n",
       "        0.842, 0.838, 0.838, 0.821, 0.825, 0.807, 0.833, 0.828, 0.837,\n",
       "        0.839, 0.838, 0.838, 0.84 , 0.836, 0.837, 0.85 , 0.851, 0.847,\n",
       "        0.858, 0.85 , 0.857, 0.861, 0.861, 0.862, 0.858, 0.858, 0.867,\n",
       "        0.85 , 0.848, 0.848, 0.861, 0.863, 0.867, 0.859, 0.865, 0.862,\n",
       "        0.859, 0.87 , 0.863, 0.853, 0.852, 0.851, 0.855, 0.858, 0.855,\n",
       "        0.858, 0.856, 0.854, 0.863, 0.859, 0.865, 0.849, 0.835, 0.837,\n",
       "        0.854, 0.86 , 0.851, 0.858, 0.859, 0.864, 0.857, 0.861, 0.86 ]),\n",
       " 'split1_test_f1_micro': array([0.839, 0.837, 0.838, 0.85 , 0.851, 0.854, 0.848, 0.842, 0.849,\n",
       "        0.86 , 0.844, 0.85 , 0.84 , 0.85 , 0.839, 0.857, 0.843, 0.841,\n",
       "        0.842, 0.844, 0.866, 0.844, 0.848, 0.859, 0.839, 0.844, 0.842,\n",
       "        0.848, 0.837, 0.855, 0.85 , 0.846, 0.833, 0.86 , 0.84 , 0.835,\n",
       "        0.851, 0.856, 0.842, 0.858, 0.857, 0.851, 0.851, 0.849, 0.852,\n",
       "        0.866, 0.848, 0.849, 0.838, 0.845, 0.841, 0.844, 0.845, 0.837,\n",
       "        0.843, 0.854, 0.832, 0.856, 0.838, 0.839, 0.839, 0.843, 0.84 ,\n",
       "        0.848, 0.852, 0.85 , 0.844, 0.868, 0.852, 0.856, 0.848, 0.854,\n",
       "        0.836, 0.833, 0.846, 0.849, 0.848, 0.862, 0.843, 0.847, 0.838,\n",
       "        0.832, 0.838, 0.85 , 0.839, 0.845, 0.847, 0.851, 0.849, 0.856,\n",
       "        0.846, 0.859, 0.857, 0.849, 0.849, 0.841, 0.775, 0.651, 0.776,\n",
       "        0.775, 0.349, 0.778, 0.778, 0.349, 0.778, 0.779, 0.651, 0.782,\n",
       "        0.772, 0.4  , 0.773, 0.773, 0.651, 0.785, 0.782, 0.651, 0.787,\n",
       "        0.782, 0.651, 0.784, 0.771, 0.349, 0.776, 0.783, 0.652, 0.781,\n",
       "        0.776, 0.651, 0.778, 0.785, 0.344, 0.786, 0.767, 0.654, 0.781,\n",
       "        0.776, 0.5  , 0.778, 0.785, 0.674, 0.782, 0.783, 0.349, 0.784,\n",
       "        0.783, 0.554, 0.817, 0.817, 0.624, 0.816, 0.81 , 0.496, 0.792,\n",
       "        0.808, 0.593, 0.817, 0.791, 0.528, 0.785, 0.815, 0.313, 0.811,\n",
       "        0.809, 0.51 , 0.817, 0.795, 0.604, 0.805, 0.807, 0.324, 0.791,\n",
       "        0.791, 0.316, 0.811, 0.803, 0.718, 0.814, 0.801, 0.675, 0.799,\n",
       "        0.798, 0.486, 0.798, 0.806, 0.581, 0.806, 0.803, 0.623, 0.806,\n",
       "        0.805, 0.598, 0.803, 0.823, 0.828, 0.81 , 0.836, 0.829, 0.837,\n",
       "        0.836, 0.838, 0.836, 0.838, 0.838, 0.841, 0.814, 0.82 , 0.805,\n",
       "        0.832, 0.835, 0.828, 0.838, 0.84 , 0.835, 0.838, 0.837, 0.835,\n",
       "        0.806, 0.821, 0.826, 0.83 , 0.832, 0.832, 0.835, 0.838, 0.835,\n",
       "        0.842, 0.839, 0.834, 0.823, 0.821, 0.814, 0.832, 0.828, 0.824,\n",
       "        0.838, 0.836, 0.83 , 0.833, 0.838, 0.824, 0.835, 0.84 , 0.844,\n",
       "        0.841, 0.851, 0.844, 0.856, 0.851, 0.845, 0.844, 0.842, 0.849,\n",
       "        0.844, 0.827, 0.844, 0.841, 0.848, 0.853, 0.846, 0.851, 0.849,\n",
       "        0.855, 0.851, 0.844, 0.83 , 0.832, 0.841, 0.851, 0.838, 0.838,\n",
       "        0.847, 0.848, 0.838, 0.846, 0.847, 0.848, 0.839, 0.836, 0.835,\n",
       "        0.838, 0.842, 0.847, 0.839, 0.842, 0.84 , 0.836, 0.847, 0.84 ]),\n",
       " 'split2_test_f1_micro': array([0.849, 0.847, 0.847, 0.863, 0.845, 0.87 , 0.862, 0.87 , 0.872,\n",
       "        0.862, 0.869, 0.857, 0.849, 0.847, 0.843, 0.849, 0.851, 0.849,\n",
       "        0.861, 0.853, 0.855, 0.859, 0.861, 0.858, 0.848, 0.855, 0.842,\n",
       "        0.86 , 0.863, 0.867, 0.861, 0.857, 0.867, 0.869, 0.854, 0.871,\n",
       "        0.847, 0.851, 0.845, 0.865, 0.864, 0.865, 0.87 , 0.852, 0.857,\n",
       "        0.862, 0.861, 0.862, 0.842, 0.846, 0.841, 0.874, 0.862, 0.863,\n",
       "        0.863, 0.849, 0.862, 0.86 , 0.855, 0.861, 0.842, 0.851, 0.848,\n",
       "        0.838, 0.848, 0.847, 0.859, 0.858, 0.846, 0.861, 0.849, 0.863,\n",
       "        0.846, 0.851, 0.845, 0.864, 0.842, 0.858, 0.862, 0.854, 0.852,\n",
       "        0.863, 0.851, 0.885, 0.859, 0.852, 0.845, 0.863, 0.856, 0.848,\n",
       "        0.85 , 0.864, 0.866, 0.866, 0.854, 0.862, 0.773, 0.659, 0.773,\n",
       "        0.763, 0.606, 0.778, 0.777, 0.331, 0.771, 0.775, 0.651, 0.781,\n",
       "        0.775, 0.385, 0.762, 0.765, 0.421, 0.77 , 0.779, 0.348, 0.775,\n",
       "        0.776, 0.528, 0.778, 0.774, 0.352, 0.763, 0.78 , 0.349, 0.773,\n",
       "        0.776, 0.651, 0.776, 0.775, 0.651, 0.777, 0.773, 0.651, 0.758,\n",
       "        0.779, 0.651, 0.767, 0.773, 0.651, 0.773, 0.775, 0.651, 0.778,\n",
       "        0.792, 0.446, 0.79 , 0.789, 0.678, 0.814, 0.798, 0.307, 0.79 ,\n",
       "        0.814, 0.664, 0.798, 0.794, 0.377, 0.794, 0.8  , 0.482, 0.818,\n",
       "        0.804, 0.723, 0.799, 0.808, 0.468, 0.798, 0.808, 0.669, 0.789,\n",
       "        0.813, 0.469, 0.798, 0.805, 0.481, 0.8  , 0.795, 0.536, 0.8  ,\n",
       "        0.802, 0.62 , 0.801, 0.802, 0.618, 0.8  , 0.806, 0.311, 0.806,\n",
       "        0.803, 0.533, 0.8  , 0.803, 0.819, 0.814, 0.815, 0.827, 0.82 ,\n",
       "        0.825, 0.824, 0.822, 0.824, 0.83 , 0.822, 0.813, 0.819, 0.809,\n",
       "        0.823, 0.824, 0.83 , 0.827, 0.82 , 0.83 , 0.823, 0.825, 0.825,\n",
       "        0.815, 0.817, 0.813, 0.817, 0.814, 0.824, 0.83 , 0.825, 0.826,\n",
       "        0.822, 0.831, 0.823, 0.799, 0.81 , 0.798, 0.819, 0.815, 0.815,\n",
       "        0.818, 0.821, 0.817, 0.816, 0.82 , 0.817, 0.834, 0.837, 0.824,\n",
       "        0.84 , 0.85 , 0.85 , 0.854, 0.851, 0.849, 0.858, 0.844, 0.85 ,\n",
       "        0.834, 0.836, 0.831, 0.845, 0.846, 0.842, 0.849, 0.847, 0.846,\n",
       "        0.854, 0.851, 0.848, 0.837, 0.834, 0.834, 0.839, 0.848, 0.848,\n",
       "        0.847, 0.85 , 0.852, 0.851, 0.848, 0.848, 0.838, 0.832, 0.835,\n",
       "        0.843, 0.845, 0.84 , 0.848, 0.844, 0.842, 0.844, 0.851, 0.849]),\n",
       " 'split3_test_f1_micro': array([0.852, 0.854, 0.857, 0.86 , 0.852, 0.859, 0.858, 0.867, 0.863,\n",
       "        0.871, 0.878, 0.879, 0.859, 0.866, 0.864, 0.873, 0.873, 0.869,\n",
       "        0.87 , 0.865, 0.859, 0.869, 0.862, 0.861, 0.847, 0.858, 0.869,\n",
       "        0.874, 0.877, 0.872, 0.853, 0.873, 0.874, 0.861, 0.86 , 0.856,\n",
       "        0.852, 0.862, 0.854, 0.872, 0.871, 0.872, 0.876, 0.861, 0.867,\n",
       "        0.876, 0.884, 0.871, 0.858, 0.856, 0.864, 0.863, 0.871, 0.854,\n",
       "        0.853, 0.871, 0.859, 0.872, 0.869, 0.866, 0.85 , 0.86 , 0.861,\n",
       "        0.866, 0.872, 0.86 , 0.857, 0.857, 0.854, 0.864, 0.873, 0.862,\n",
       "        0.861, 0.85 , 0.856, 0.863, 0.851, 0.859, 0.852, 0.859, 0.86 ,\n",
       "        0.863, 0.864, 0.834, 0.855, 0.851, 0.862, 0.879, 0.869, 0.869,\n",
       "        0.868, 0.86 , 0.865, 0.864, 0.869, 0.866, 0.796, 0.349, 0.79 ,\n",
       "        0.797, 0.484, 0.799, 0.799, 0.651, 0.786, 0.796, 0.653, 0.792,\n",
       "        0.791, 0.46 , 0.791, 0.783, 0.655, 0.795, 0.795, 0.655, 0.797,\n",
       "        0.795, 0.652, 0.794, 0.79 , 0.655, 0.789, 0.802, 0.376, 0.79 ,\n",
       "        0.792, 0.349, 0.801, 0.798, 0.651, 0.793, 0.793, 0.651, 0.792,\n",
       "        0.791, 0.349, 0.794, 0.794, 0.672, 0.795, 0.794, 0.651, 0.796,\n",
       "        0.817, 0.405, 0.807, 0.809, 0.718, 0.831, 0.83 , 0.533, 0.82 ,\n",
       "        0.845, 0.631, 0.838, 0.824, 0.422, 0.82 , 0.802, 0.5  , 0.804,\n",
       "        0.83 , 0.585, 0.841, 0.82 , 0.665, 0.826, 0.81 , 0.553, 0.802,\n",
       "        0.838, 0.618, 0.824, 0.827, 0.502, 0.823, 0.826, 0.676, 0.824,\n",
       "        0.811, 0.665, 0.834, 0.836, 0.548, 0.807, 0.833, 0.538, 0.832,\n",
       "        0.837, 0.69 , 0.821, 0.846, 0.853, 0.842, 0.849, 0.848, 0.844,\n",
       "        0.846, 0.852, 0.851, 0.849, 0.852, 0.851, 0.843, 0.844, 0.835,\n",
       "        0.84 , 0.848, 0.848, 0.849, 0.849, 0.853, 0.849, 0.852, 0.849,\n",
       "        0.81 , 0.842, 0.835, 0.846, 0.846, 0.846, 0.849, 0.849, 0.848,\n",
       "        0.85 , 0.849, 0.851, 0.824, 0.807, 0.815, 0.814, 0.843, 0.844,\n",
       "        0.848, 0.847, 0.849, 0.845, 0.848, 0.846, 0.849, 0.865, 0.842,\n",
       "        0.858, 0.86 , 0.85 , 0.865, 0.869, 0.869, 0.874, 0.869, 0.866,\n",
       "        0.856, 0.851, 0.854, 0.856, 0.86 , 0.858, 0.868, 0.86 , 0.864,\n",
       "        0.859, 0.862, 0.868, 0.85 , 0.847, 0.868, 0.858, 0.849, 0.869,\n",
       "        0.865, 0.861, 0.873, 0.866, 0.872, 0.862, 0.85 , 0.851, 0.855,\n",
       "        0.853, 0.856, 0.856, 0.867, 0.869, 0.874, 0.866, 0.866, 0.862]),\n",
       " 'split4_test_f1_micro': array([0.839, 0.849, 0.84 , 0.848, 0.851, 0.855, 0.851, 0.843, 0.853,\n",
       "        0.851, 0.853, 0.836, 0.838, 0.827, 0.839, 0.841, 0.831, 0.854,\n",
       "        0.836, 0.848, 0.834, 0.843, 0.828, 0.827, 0.829, 0.825, 0.834,\n",
       "        0.839, 0.861, 0.839, 0.843, 0.856, 0.849, 0.846, 0.84 , 0.833,\n",
       "        0.825, 0.832, 0.83 , 0.839, 0.839, 0.848, 0.846, 0.851, 0.839,\n",
       "        0.843, 0.845, 0.844, 0.834, 0.845, 0.842, 0.85 , 0.837, 0.84 ,\n",
       "        0.841, 0.831, 0.834, 0.846, 0.826, 0.843, 0.845, 0.841, 0.839,\n",
       "        0.833, 0.844, 0.841, 0.83 , 0.826, 0.837, 0.857, 0.84 , 0.843,\n",
       "        0.838, 0.841, 0.835, 0.84 , 0.858, 0.843, 0.843, 0.84 , 0.846,\n",
       "        0.841, 0.856, 0.834, 0.832, 0.841, 0.844, 0.846, 0.845, 0.844,\n",
       "        0.847, 0.829, 0.837, 0.847, 0.834, 0.844, 0.767, 0.605, 0.782,\n",
       "        0.777, 0.651, 0.781, 0.773, 0.485, 0.77 , 0.778, 0.651, 0.78 ,\n",
       "        0.768, 0.318, 0.775, 0.781, 0.651, 0.774, 0.775, 0.651, 0.784,\n",
       "        0.778, 0.664, 0.779, 0.774, 0.651, 0.777, 0.777, 0.676, 0.779,\n",
       "        0.78 , 0.651, 0.782, 0.779, 0.615, 0.777, 0.757, 0.651, 0.777,\n",
       "        0.778, 0.391, 0.774, 0.775, 0.486, 0.777, 0.776, 0.651, 0.777,\n",
       "        0.796, 0.651, 0.783, 0.785, 0.486, 0.785, 0.788, 0.61 , 0.796,\n",
       "        0.792, 0.683, 0.795, 0.794, 0.527, 0.792, 0.787, 0.46 , 0.794,\n",
       "        0.794, 0.659, 0.792, 0.801, 0.556, 0.788, 0.787, 0.324, 0.8  ,\n",
       "        0.791, 0.498, 0.796, 0.791, 0.657, 0.79 , 0.796, 0.691, 0.789,\n",
       "        0.787, 0.666, 0.784, 0.781, 0.481, 0.797, 0.792, 0.652, 0.787,\n",
       "        0.785, 0.535, 0.789, 0.796, 0.79 , 0.791, 0.815, 0.814, 0.808,\n",
       "        0.817, 0.817, 0.821, 0.817, 0.821, 0.817, 0.786, 0.803, 0.813,\n",
       "        0.821, 0.803, 0.811, 0.818, 0.815, 0.817, 0.818, 0.818, 0.821,\n",
       "        0.793, 0.816, 0.785, 0.814, 0.817, 0.811, 0.822, 0.814, 0.821,\n",
       "        0.82 , 0.824, 0.818, 0.814, 0.798, 0.79 , 0.81 , 0.799, 0.808,\n",
       "        0.811, 0.813, 0.814, 0.818, 0.816, 0.811, 0.821, 0.818, 0.812,\n",
       "        0.843, 0.826, 0.834, 0.838, 0.837, 0.837, 0.833, 0.842, 0.846,\n",
       "        0.827, 0.827, 0.826, 0.833, 0.832, 0.835, 0.841, 0.845, 0.846,\n",
       "        0.842, 0.84 , 0.841, 0.826, 0.821, 0.821, 0.839, 0.833, 0.836,\n",
       "        0.832, 0.843, 0.841, 0.837, 0.843, 0.841, 0.822, 0.827, 0.824,\n",
       "        0.829, 0.835, 0.838, 0.834, 0.844, 0.837, 0.84 , 0.841, 0.847]),\n",
       " 'mean_test_f1_micro': array([0.8462, 0.8468, 0.846 , 0.8536, 0.8508, 0.8584, 0.8556, 0.8542,\n",
       "        0.858 , 0.86  , 0.8614, 0.8558, 0.8466, 0.8506, 0.8466, 0.8554,\n",
       "        0.8502, 0.8526, 0.8548, 0.8506, 0.854 , 0.8532, 0.8526, 0.851 ,\n",
       "        0.844 , 0.8462, 0.8486, 0.8546, 0.8572, 0.8576, 0.851 , 0.8584,\n",
       "        0.8586, 0.8574, 0.847 , 0.8518, 0.8464, 0.8524, 0.8464, 0.8568,\n",
       "        0.8574, 0.8584, 0.861 , 0.8542, 0.8532, 0.8616, 0.858 , 0.856 ,\n",
       "        0.8452, 0.85  , 0.8482, 0.8606, 0.8542, 0.8502, 0.8508, 0.8508,\n",
       "        0.8476, 0.8588, 0.849 , 0.8524, 0.8466, 0.8514, 0.8496, 0.85  ,\n",
       "        0.8536, 0.8508, 0.8514, 0.8536, 0.8482, 0.8582, 0.8538, 0.8554,\n",
       "        0.8482, 0.8468, 0.8468, 0.8532, 0.8522, 0.8566, 0.851 , 0.8518,\n",
       "        0.8496, 0.848 , 0.8508, 0.8516, 0.848 , 0.8492, 0.85  , 0.859 ,\n",
       "        0.854 , 0.8554, 0.8532, 0.8528, 0.855 , 0.8554, 0.851 , 0.8548,\n",
       "        0.7792, 0.5226, 0.7806, 0.7802, 0.5576, 0.7848, 0.7832, 0.4562,\n",
       "        0.7784, 0.7836, 0.6514, 0.7842, 0.7796, 0.4464, 0.777 , 0.7776,\n",
       "        0.6058, 0.7824, 0.784 , 0.5548, 0.7862, 0.7836, 0.6292, 0.7848,\n",
       "        0.7796, 0.4712, 0.7792, 0.7858, 0.474 , 0.7814, 0.7818, 0.5296,\n",
       "        0.7846, 0.7852, 0.5716, 0.784 , 0.7754, 0.6586, 0.7784, 0.782 ,\n",
       "        0.448 , 0.7804, 0.7826, 0.5664, 0.7834, 0.7826, 0.5906, 0.7848,\n",
       "        0.7954, 0.5412, 0.7976, 0.8014, 0.571 , 0.8086, 0.8082, 0.458 ,\n",
       "        0.802 , 0.8142, 0.6572, 0.8098, 0.8042, 0.4968, 0.7978, 0.8   ,\n",
       "        0.4842, 0.806 , 0.8072, 0.605 , 0.8106, 0.8068, 0.548 , 0.8038,\n",
       "        0.8054, 0.439 , 0.7946, 0.8084, 0.4506, 0.809 , 0.806 , 0.587 ,\n",
       "        0.8044, 0.8026, 0.62  , 0.8024, 0.796 , 0.6106, 0.8032, 0.8076,\n",
       "        0.5542, 0.8036, 0.8064, 0.5496, 0.8068, 0.8064, 0.5672, 0.8022,\n",
       "        0.8192, 0.8224, 0.812 , 0.8302, 0.8298, 0.8296, 0.833 , 0.8348,\n",
       "        0.8338, 0.8326, 0.8368, 0.8332, 0.8158, 0.8214, 0.8138, 0.8306,\n",
       "        0.8282, 0.8316, 0.8346, 0.8328, 0.8344, 0.8336, 0.8344, 0.834 ,\n",
       "        0.8058, 0.8232, 0.8114, 0.8288, 0.8296, 0.8306, 0.8346, 0.8328,\n",
       "        0.8324, 0.8352, 0.8362, 0.8328, 0.8162, 0.8122, 0.8048, 0.8216,\n",
       "        0.8226, 0.8256, 0.8308, 0.831 , 0.8296, 0.8304, 0.8316, 0.827 ,\n",
       "        0.8378, 0.8422, 0.8338, 0.848 , 0.8474, 0.847 , 0.8548, 0.8538,\n",
       "        0.8524, 0.8534, 0.851 , 0.8556, 0.8422, 0.8378, 0.8406, 0.8472,\n",
       "        0.8498, 0.851 , 0.8526, 0.8536, 0.8534, 0.8538, 0.8548, 0.8528,\n",
       "        0.8392, 0.8372, 0.843 , 0.8484, 0.8452, 0.8492, 0.8498, 0.8516,\n",
       "        0.8516, 0.8526, 0.8538, 0.8528, 0.8396, 0.8362, 0.8372, 0.8434,\n",
       "        0.8476, 0.8464, 0.8492, 0.8516, 0.8514, 0.8486, 0.8532, 0.8516]),\n",
       " 'std_test_f1_micro': array([0.00597997, 0.00552811, 0.00672309, 0.0065909 , 0.00324962,\n",
       "        0.00608605, 0.00523832, 0.01195659, 0.00839047, 0.00666333,\n",
       "        0.01190966, 0.01390539, 0.0074458 , 0.01386506, 0.00930806,\n",
       "        0.0106132 , 0.01377534, 0.00922171, 0.01334766, 0.00801499,\n",
       "        0.01071448, 0.00976524, 0.01352923, 0.01256981, 0.00942338,\n",
       "        0.01165161, 0.01241934, 0.0118254 , 0.0136587 , 0.01144727,\n",
       "        0.00596657, 0.00868562, 0.0153961 , 0.00806474, 0.00839047,\n",
       "        0.01530229, 0.01116423, 0.01092886, 0.01059434, 0.01151347,\n",
       "        0.01066958, 0.00891291, 0.01124278, 0.00453431, 0.00908625,\n",
       "        0.01070701, 0.01407125, 0.00957079, 0.00926067, 0.00576194,\n",
       "        0.00910824, 0.01185917, 0.01205653, 0.01002796, 0.00800999,\n",
       "        0.01278124, 0.01246756, 0.00835225, 0.01516575, 0.0102684 ,\n",
       "        0.0063435 , 0.00854634, 0.0094361 , 0.01354991, 0.00966644,\n",
       "        0.00667533, 0.01300154, 0.01434713, 0.00620967, 0.00386782,\n",
       "        0.01133843, 0.00717217, 0.01059056, 0.00895321, 0.00713863,\n",
       "        0.00910824, 0.00711056, 0.0069455 , 0.00729383, 0.00735935,\n",
       "        0.00731027, 0.01268069, 0.00893085, 0.01870401, 0.01054514,\n",
       "        0.0056    , 0.00660303, 0.0114717 , 0.00829458, 0.00884534,\n",
       "        0.00803492, 0.01251239, 0.01071448, 0.00796492, 0.01122497,\n",
       "        0.01022546, 0.01020588, 0.14293719, 0.0058515 , 0.01177115,\n",
       "        0.126261  , 0.00798499, 0.0095163 , 0.11490936, 0.00717217,\n",
       "        0.00801499, 0.0008    , 0.0044    , 0.00997196, 0.12012094,\n",
       "        0.00989949, 0.00763151, 0.09241299, 0.00917824, 0.00715542,\n",
       "        0.12544066, 0.00708237, 0.00682935, 0.05083857, 0.00604649,\n",
       "        0.00816333, 0.14844851, 0.01012719, 0.00874986, 0.15643913,\n",
       "        0.00560714, 0.00607947, 0.14868705, 0.00889044, 0.00800999,\n",
       "        0.11570583, 0.00619677, 0.01310878, 0.01374918, 0.01132431,\n",
       "        0.00562139, 0.11555432, 0.00985089, 0.00770973, 0.12931605,\n",
       "        0.00811419, 0.00688767, 0.1208    , 0.00708237, 0.01160345,\n",
       "        0.10165707, 0.01248359, 0.01228983, 0.13592351, 0.01598249,\n",
       "        0.0144    , 0.11486514, 0.01186592, 0.01723253, 0.04209703,\n",
       "        0.01601749, 0.013891  , 0.08896606, 0.01187266, 0.00909945,\n",
       "        0.11242847, 0.00807465, 0.01244829, 0.07682968, 0.01725804,\n",
       "        0.00847113, 0.08184131, 0.0124964 , 0.00960417, 0.14514958,\n",
       "        0.00531413, 0.01733897, 0.10811771, 0.01065833, 0.0116619 ,\n",
       "        0.09013545, 0.01227355, 0.01190966, 0.07464851, 0.01156892,\n",
       "        0.01041153, 0.06583495, 0.0165336 , 0.01776063, 0.04541101,\n",
       "        0.00431741, 0.01412232, 0.12527825, 0.01444161, 0.01689497,\n",
       "        0.07188713, 0.01049571, 0.01794882, 0.02014547, 0.01691153,\n",
       "        0.01328759, 0.01087014, 0.01348481, 0.01060189, 0.01270276,\n",
       "        0.0112321 , 0.0111463 , 0.01064707, 0.0124    , 0.01838913,\n",
       "        0.01309351, 0.0109252 , 0.00749933, 0.0148243 , 0.01262696,\n",
       "        0.0108922 , 0.01301384, 0.01162067, 0.01142979, 0.01187603,\n",
       "        0.01011929, 0.00730479, 0.00957914, 0.0181615 , 0.01202331,\n",
       "        0.01237093, 0.01228983, 0.00886792, 0.01208967, 0.00917824,\n",
       "        0.0119733 , 0.00837616, 0.01161723, 0.00928224, 0.00974474,\n",
       "        0.00957914, 0.00935094, 0.0147594 , 0.01336563, 0.01393413,\n",
       "        0.01228007, 0.01303227, 0.01160345, 0.01189285, 0.01285302,\n",
       "        0.01075918, 0.01558717, 0.01354105, 0.00822192, 0.01134196,\n",
       "        0.00769415, 0.00923905, 0.01077775, 0.0115862 , 0.013937  ,\n",
       "        0.0108074 , 0.00900222, 0.01051475, 0.01014692, 0.01049952,\n",
       "        0.01012719, 0.01107068, 0.01136662, 0.0096871 , 0.00768375,\n",
       "        0.00793977, 0.00624179, 0.0103034 , 0.01072194, 0.01068457,\n",
       "        0.01108873, 0.0158619 , 0.00798999, 0.00879545, 0.01205653,\n",
       "        0.0112321 , 0.00628013, 0.01233856, 0.0107443 , 0.01053376,\n",
       "        0.00915205, 0.01009158, 0.00803492, 0.010008  , 0.00939361,\n",
       "        0.00917824, 0.00671118, 0.01208967, 0.01063203, 0.0148    ,\n",
       "        0.0112    , 0.00913017, 0.00826075]),\n",
       " 'rank_test_f1_micro': array([126, 117, 128,  44,  83,   9,  23,  35,  13,   5,   2,  22, 120,\n",
       "         87, 120,  27,  89,  58,  30,  87,  38,  50,  58,  76, 131, 126,\n",
       "        103,  34,  18,  15,  76,   9,   8,  16, 116,  67, 124,  63, 123,\n",
       "         19,  16,   9,   3,  35,  50,   1,  13,  21, 129,  93, 105,   4,\n",
       "         35,  89,  82,  83, 111,   7, 101,  62, 120,  73,  96,  91,  44,\n",
       "         83,  73,  44, 105,  12,  40,  25, 105, 117, 117,  50,  65,  20,\n",
       "         81,  66,  96, 108,  83,  68, 108,  99,  91,   6,  38,  25,  50,\n",
       "         56,  29,  27,  76,  30, 250, 278, 245, 247, 271, 228, 238, 284,\n",
       "        253, 235, 259, 232, 249, 287, 255, 254, 263, 241, 233, 272, 225,\n",
       "        235, 260, 228, 248, 282, 250, 226, 281, 244, 243, 277, 231, 227,\n",
       "        267, 233, 256, 257, 252, 242, 286, 246, 239, 270, 237, 239, 265,\n",
       "        228, 223, 276, 221, 218, 268, 195, 197, 283, 217, 187, 258, 193,\n",
       "        210, 279, 220, 219, 280, 204, 199, 264, 192, 200, 275, 211, 207,\n",
       "        288, 224, 196, 285, 194, 204, 266, 209, 214, 261, 215, 222, 262,\n",
       "        213, 198, 273, 212, 202, 274, 200, 202, 269, 216, 184, 181, 190,\n",
       "        170, 171, 172, 157, 147, 153, 161, 143, 156, 186, 183, 188, 167,\n",
       "        176, 163, 149, 158, 150, 155, 151, 152, 206, 179, 191, 175, 172,\n",
       "        167, 148, 158, 162, 146, 144, 158, 185, 189, 208, 182, 180, 178,\n",
       "        166, 165, 172, 169, 164, 177, 139, 135, 153, 108, 113, 115,  30,\n",
       "         43,  63,  48,  76,  24, 134, 139, 136, 114,  94,  76,  58,  44,\n",
       "         49,  40,  30,  56, 138, 141, 133, 104, 129,  98,  94,  68,  68,\n",
       "         58,  40,  55, 137, 145, 141, 132, 111, 124,  99,  68,  73, 102,\n",
       "         50,  72]),\n",
       " 'split0_test_precision_micro': array([0.852, 0.847, 0.848, 0.847, 0.855, 0.854, 0.859, 0.849, 0.853,\n",
       "        0.856, 0.863, 0.857, 0.847, 0.863, 0.848, 0.857, 0.853, 0.85 ,\n",
       "        0.865, 0.843, 0.856, 0.851, 0.864, 0.85 , 0.857, 0.849, 0.856,\n",
       "        0.852, 0.848, 0.855, 0.848, 0.86 , 0.87 , 0.851, 0.841, 0.864,\n",
       "        0.857, 0.861, 0.861, 0.85 , 0.856, 0.856, 0.862, 0.858, 0.851,\n",
       "        0.861, 0.852, 0.854, 0.854, 0.858, 0.853, 0.872, 0.856, 0.857,\n",
       "        0.854, 0.849, 0.851, 0.86 , 0.857, 0.853, 0.857, 0.862, 0.86 ,\n",
       "        0.865, 0.852, 0.856, 0.867, 0.859, 0.852, 0.853, 0.859, 0.855,\n",
       "        0.86 , 0.859, 0.852, 0.85 , 0.862, 0.861, 0.855, 0.859, 0.852,\n",
       "        0.841, 0.845, 0.855, 0.855, 0.857, 0.852, 0.856, 0.851, 0.86 ,\n",
       "        0.855, 0.852, 0.85 , 0.851, 0.849, 0.861, 0.785, 0.349, 0.782,\n",
       "        0.789, 0.698, 0.788, 0.789, 0.465, 0.787, 0.79 , 0.651, 0.786,\n",
       "        0.792, 0.669, 0.784, 0.786, 0.651, 0.788, 0.789, 0.469, 0.788,\n",
       "        0.787, 0.651, 0.789, 0.789, 0.349, 0.791, 0.787, 0.317, 0.784,\n",
       "        0.785, 0.346, 0.786, 0.789, 0.597, 0.787, 0.787, 0.686, 0.784,\n",
       "        0.786, 0.349, 0.789, 0.786, 0.349, 0.79 , 0.785, 0.651, 0.789,\n",
       "        0.789, 0.65 , 0.791, 0.807, 0.349, 0.797, 0.815, 0.344, 0.812,\n",
       "        0.812, 0.715, 0.801, 0.818, 0.63 , 0.798, 0.796, 0.666, 0.803,\n",
       "        0.799, 0.548, 0.804, 0.81 , 0.447, 0.802, 0.815, 0.325, 0.791,\n",
       "        0.809, 0.352, 0.816, 0.804, 0.577, 0.795, 0.795, 0.522, 0.8  ,\n",
       "        0.782, 0.616, 0.799, 0.813, 0.543, 0.808, 0.798, 0.624, 0.803,\n",
       "        0.802, 0.48 , 0.798, 0.828, 0.822, 0.803, 0.836, 0.831, 0.839,\n",
       "        0.841, 0.843, 0.839, 0.835, 0.843, 0.835, 0.823, 0.821, 0.807,\n",
       "        0.837, 0.831, 0.841, 0.841, 0.84 , 0.837, 0.84 , 0.84 , 0.84 ,\n",
       "        0.805, 0.82 , 0.798, 0.837, 0.839, 0.84 , 0.837, 0.838, 0.832,\n",
       "        0.842, 0.838, 0.838, 0.821, 0.825, 0.807, 0.833, 0.828, 0.837,\n",
       "        0.839, 0.838, 0.838, 0.84 , 0.836, 0.837, 0.85 , 0.851, 0.847,\n",
       "        0.858, 0.85 , 0.857, 0.861, 0.861, 0.862, 0.858, 0.858, 0.867,\n",
       "        0.85 , 0.848, 0.848, 0.861, 0.863, 0.867, 0.859, 0.865, 0.862,\n",
       "        0.859, 0.87 , 0.863, 0.853, 0.852, 0.851, 0.855, 0.858, 0.855,\n",
       "        0.858, 0.856, 0.854, 0.863, 0.859, 0.865, 0.849, 0.835, 0.837,\n",
       "        0.854, 0.86 , 0.851, 0.858, 0.859, 0.864, 0.857, 0.861, 0.86 ]),\n",
       " 'split1_test_precision_micro': array([0.839, 0.837, 0.838, 0.85 , 0.851, 0.854, 0.848, 0.842, 0.849,\n",
       "        0.86 , 0.844, 0.85 , 0.84 , 0.85 , 0.839, 0.857, 0.843, 0.841,\n",
       "        0.842, 0.844, 0.866, 0.844, 0.848, 0.859, 0.839, 0.844, 0.842,\n",
       "        0.848, 0.837, 0.855, 0.85 , 0.846, 0.833, 0.86 , 0.84 , 0.835,\n",
       "        0.851, 0.856, 0.842, 0.858, 0.857, 0.851, 0.851, 0.849, 0.852,\n",
       "        0.866, 0.848, 0.849, 0.838, 0.845, 0.841, 0.844, 0.845, 0.837,\n",
       "        0.843, 0.854, 0.832, 0.856, 0.838, 0.839, 0.839, 0.843, 0.84 ,\n",
       "        0.848, 0.852, 0.85 , 0.844, 0.868, 0.852, 0.856, 0.848, 0.854,\n",
       "        0.836, 0.833, 0.846, 0.849, 0.848, 0.862, 0.843, 0.847, 0.838,\n",
       "        0.832, 0.838, 0.85 , 0.839, 0.845, 0.847, 0.851, 0.849, 0.856,\n",
       "        0.846, 0.859, 0.857, 0.849, 0.849, 0.841, 0.775, 0.651, 0.776,\n",
       "        0.775, 0.349, 0.778, 0.778, 0.349, 0.778, 0.779, 0.651, 0.782,\n",
       "        0.772, 0.4  , 0.773, 0.773, 0.651, 0.785, 0.782, 0.651, 0.787,\n",
       "        0.782, 0.651, 0.784, 0.771, 0.349, 0.776, 0.783, 0.652, 0.781,\n",
       "        0.776, 0.651, 0.778, 0.785, 0.344, 0.786, 0.767, 0.654, 0.781,\n",
       "        0.776, 0.5  , 0.778, 0.785, 0.674, 0.782, 0.783, 0.349, 0.784,\n",
       "        0.783, 0.554, 0.817, 0.817, 0.624, 0.816, 0.81 , 0.496, 0.792,\n",
       "        0.808, 0.593, 0.817, 0.791, 0.528, 0.785, 0.815, 0.313, 0.811,\n",
       "        0.809, 0.51 , 0.817, 0.795, 0.604, 0.805, 0.807, 0.324, 0.791,\n",
       "        0.791, 0.316, 0.811, 0.803, 0.718, 0.814, 0.801, 0.675, 0.799,\n",
       "        0.798, 0.486, 0.798, 0.806, 0.581, 0.806, 0.803, 0.623, 0.806,\n",
       "        0.805, 0.598, 0.803, 0.823, 0.828, 0.81 , 0.836, 0.829, 0.837,\n",
       "        0.836, 0.838, 0.836, 0.838, 0.838, 0.841, 0.814, 0.82 , 0.805,\n",
       "        0.832, 0.835, 0.828, 0.838, 0.84 , 0.835, 0.838, 0.837, 0.835,\n",
       "        0.806, 0.821, 0.826, 0.83 , 0.832, 0.832, 0.835, 0.838, 0.835,\n",
       "        0.842, 0.839, 0.834, 0.823, 0.821, 0.814, 0.832, 0.828, 0.824,\n",
       "        0.838, 0.836, 0.83 , 0.833, 0.838, 0.824, 0.835, 0.84 , 0.844,\n",
       "        0.841, 0.851, 0.844, 0.856, 0.851, 0.845, 0.844, 0.842, 0.849,\n",
       "        0.844, 0.827, 0.844, 0.841, 0.848, 0.853, 0.846, 0.851, 0.849,\n",
       "        0.855, 0.851, 0.844, 0.83 , 0.832, 0.841, 0.851, 0.838, 0.838,\n",
       "        0.847, 0.848, 0.838, 0.846, 0.847, 0.848, 0.839, 0.836, 0.835,\n",
       "        0.838, 0.842, 0.847, 0.839, 0.842, 0.84 , 0.836, 0.847, 0.84 ]),\n",
       " 'split2_test_precision_micro': array([0.849, 0.847, 0.847, 0.863, 0.845, 0.87 , 0.862, 0.87 , 0.872,\n",
       "        0.862, 0.869, 0.857, 0.849, 0.847, 0.843, 0.849, 0.851, 0.849,\n",
       "        0.861, 0.853, 0.855, 0.859, 0.861, 0.858, 0.848, 0.855, 0.842,\n",
       "        0.86 , 0.863, 0.867, 0.861, 0.857, 0.867, 0.869, 0.854, 0.871,\n",
       "        0.847, 0.851, 0.845, 0.865, 0.864, 0.865, 0.87 , 0.852, 0.857,\n",
       "        0.862, 0.861, 0.862, 0.842, 0.846, 0.841, 0.874, 0.862, 0.863,\n",
       "        0.863, 0.849, 0.862, 0.86 , 0.855, 0.861, 0.842, 0.851, 0.848,\n",
       "        0.838, 0.848, 0.847, 0.859, 0.858, 0.846, 0.861, 0.849, 0.863,\n",
       "        0.846, 0.851, 0.845, 0.864, 0.842, 0.858, 0.862, 0.854, 0.852,\n",
       "        0.863, 0.851, 0.885, 0.859, 0.852, 0.845, 0.863, 0.856, 0.848,\n",
       "        0.85 , 0.864, 0.866, 0.866, 0.854, 0.862, 0.773, 0.659, 0.773,\n",
       "        0.763, 0.606, 0.778, 0.777, 0.331, 0.771, 0.775, 0.651, 0.781,\n",
       "        0.775, 0.385, 0.762, 0.765, 0.421, 0.77 , 0.779, 0.348, 0.775,\n",
       "        0.776, 0.528, 0.778, 0.774, 0.352, 0.763, 0.78 , 0.349, 0.773,\n",
       "        0.776, 0.651, 0.776, 0.775, 0.651, 0.777, 0.773, 0.651, 0.758,\n",
       "        0.779, 0.651, 0.767, 0.773, 0.651, 0.773, 0.775, 0.651, 0.778,\n",
       "        0.792, 0.446, 0.79 , 0.789, 0.678, 0.814, 0.798, 0.307, 0.79 ,\n",
       "        0.814, 0.664, 0.798, 0.794, 0.377, 0.794, 0.8  , 0.482, 0.818,\n",
       "        0.804, 0.723, 0.799, 0.808, 0.468, 0.798, 0.808, 0.669, 0.789,\n",
       "        0.813, 0.469, 0.798, 0.805, 0.481, 0.8  , 0.795, 0.536, 0.8  ,\n",
       "        0.802, 0.62 , 0.801, 0.802, 0.618, 0.8  , 0.806, 0.311, 0.806,\n",
       "        0.803, 0.533, 0.8  , 0.803, 0.819, 0.814, 0.815, 0.827, 0.82 ,\n",
       "        0.825, 0.824, 0.822, 0.824, 0.83 , 0.822, 0.813, 0.819, 0.809,\n",
       "        0.823, 0.824, 0.83 , 0.827, 0.82 , 0.83 , 0.823, 0.825, 0.825,\n",
       "        0.815, 0.817, 0.813, 0.817, 0.814, 0.824, 0.83 , 0.825, 0.826,\n",
       "        0.822, 0.831, 0.823, 0.799, 0.81 , 0.798, 0.819, 0.815, 0.815,\n",
       "        0.818, 0.821, 0.817, 0.816, 0.82 , 0.817, 0.834, 0.837, 0.824,\n",
       "        0.84 , 0.85 , 0.85 , 0.854, 0.851, 0.849, 0.858, 0.844, 0.85 ,\n",
       "        0.834, 0.836, 0.831, 0.845, 0.846, 0.842, 0.849, 0.847, 0.846,\n",
       "        0.854, 0.851, 0.848, 0.837, 0.834, 0.834, 0.839, 0.848, 0.848,\n",
       "        0.847, 0.85 , 0.852, 0.851, 0.848, 0.848, 0.838, 0.832, 0.835,\n",
       "        0.843, 0.845, 0.84 , 0.848, 0.844, 0.842, 0.844, 0.851, 0.849]),\n",
       " 'split3_test_precision_micro': array([0.852, 0.854, 0.857, 0.86 , 0.852, 0.859, 0.858, 0.867, 0.863,\n",
       "        0.871, 0.878, 0.879, 0.859, 0.866, 0.864, 0.873, 0.873, 0.869,\n",
       "        0.87 , 0.865, 0.859, 0.869, 0.862, 0.861, 0.847, 0.858, 0.869,\n",
       "        0.874, 0.877, 0.872, 0.853, 0.873, 0.874, 0.861, 0.86 , 0.856,\n",
       "        0.852, 0.862, 0.854, 0.872, 0.871, 0.872, 0.876, 0.861, 0.867,\n",
       "        0.876, 0.884, 0.871, 0.858, 0.856, 0.864, 0.863, 0.871, 0.854,\n",
       "        0.853, 0.871, 0.859, 0.872, 0.869, 0.866, 0.85 , 0.86 , 0.861,\n",
       "        0.866, 0.872, 0.86 , 0.857, 0.857, 0.854, 0.864, 0.873, 0.862,\n",
       "        0.861, 0.85 , 0.856, 0.863, 0.851, 0.859, 0.852, 0.859, 0.86 ,\n",
       "        0.863, 0.864, 0.834, 0.855, 0.851, 0.862, 0.879, 0.869, 0.869,\n",
       "        0.868, 0.86 , 0.865, 0.864, 0.869, 0.866, 0.796, 0.349, 0.79 ,\n",
       "        0.797, 0.484, 0.799, 0.799, 0.651, 0.786, 0.796, 0.653, 0.792,\n",
       "        0.791, 0.46 , 0.791, 0.783, 0.655, 0.795, 0.795, 0.655, 0.797,\n",
       "        0.795, 0.652, 0.794, 0.79 , 0.655, 0.789, 0.802, 0.376, 0.79 ,\n",
       "        0.792, 0.349, 0.801, 0.798, 0.651, 0.793, 0.793, 0.651, 0.792,\n",
       "        0.791, 0.349, 0.794, 0.794, 0.672, 0.795, 0.794, 0.651, 0.796,\n",
       "        0.817, 0.405, 0.807, 0.809, 0.718, 0.831, 0.83 , 0.533, 0.82 ,\n",
       "        0.845, 0.631, 0.838, 0.824, 0.422, 0.82 , 0.802, 0.5  , 0.804,\n",
       "        0.83 , 0.585, 0.841, 0.82 , 0.665, 0.826, 0.81 , 0.553, 0.802,\n",
       "        0.838, 0.618, 0.824, 0.827, 0.502, 0.823, 0.826, 0.676, 0.824,\n",
       "        0.811, 0.665, 0.834, 0.836, 0.548, 0.807, 0.833, 0.538, 0.832,\n",
       "        0.837, 0.69 , 0.821, 0.846, 0.853, 0.842, 0.849, 0.848, 0.844,\n",
       "        0.846, 0.852, 0.851, 0.849, 0.852, 0.851, 0.843, 0.844, 0.835,\n",
       "        0.84 , 0.848, 0.848, 0.849, 0.849, 0.853, 0.849, 0.852, 0.849,\n",
       "        0.81 , 0.842, 0.835, 0.846, 0.846, 0.846, 0.849, 0.849, 0.848,\n",
       "        0.85 , 0.849, 0.851, 0.824, 0.807, 0.815, 0.814, 0.843, 0.844,\n",
       "        0.848, 0.847, 0.849, 0.845, 0.848, 0.846, 0.849, 0.865, 0.842,\n",
       "        0.858, 0.86 , 0.85 , 0.865, 0.869, 0.869, 0.874, 0.869, 0.866,\n",
       "        0.856, 0.851, 0.854, 0.856, 0.86 , 0.858, 0.868, 0.86 , 0.864,\n",
       "        0.859, 0.862, 0.868, 0.85 , 0.847, 0.868, 0.858, 0.849, 0.869,\n",
       "        0.865, 0.861, 0.873, 0.866, 0.872, 0.862, 0.85 , 0.851, 0.855,\n",
       "        0.853, 0.856, 0.856, 0.867, 0.869, 0.874, 0.866, 0.866, 0.862]),\n",
       " 'split4_test_precision_micro': array([0.839, 0.849, 0.84 , 0.848, 0.851, 0.855, 0.851, 0.843, 0.853,\n",
       "        0.851, 0.853, 0.836, 0.838, 0.827, 0.839, 0.841, 0.831, 0.854,\n",
       "        0.836, 0.848, 0.834, 0.843, 0.828, 0.827, 0.829, 0.825, 0.834,\n",
       "        0.839, 0.861, 0.839, 0.843, 0.856, 0.849, 0.846, 0.84 , 0.833,\n",
       "        0.825, 0.832, 0.83 , 0.839, 0.839, 0.848, 0.846, 0.851, 0.839,\n",
       "        0.843, 0.845, 0.844, 0.834, 0.845, 0.842, 0.85 , 0.837, 0.84 ,\n",
       "        0.841, 0.831, 0.834, 0.846, 0.826, 0.843, 0.845, 0.841, 0.839,\n",
       "        0.833, 0.844, 0.841, 0.83 , 0.826, 0.837, 0.857, 0.84 , 0.843,\n",
       "        0.838, 0.841, 0.835, 0.84 , 0.858, 0.843, 0.843, 0.84 , 0.846,\n",
       "        0.841, 0.856, 0.834, 0.832, 0.841, 0.844, 0.846, 0.845, 0.844,\n",
       "        0.847, 0.829, 0.837, 0.847, 0.834, 0.844, 0.767, 0.605, 0.782,\n",
       "        0.777, 0.651, 0.781, 0.773, 0.485, 0.77 , 0.778, 0.651, 0.78 ,\n",
       "        0.768, 0.318, 0.775, 0.781, 0.651, 0.774, 0.775, 0.651, 0.784,\n",
       "        0.778, 0.664, 0.779, 0.774, 0.651, 0.777, 0.777, 0.676, 0.779,\n",
       "        0.78 , 0.651, 0.782, 0.779, 0.615, 0.777, 0.757, 0.651, 0.777,\n",
       "        0.778, 0.391, 0.774, 0.775, 0.486, 0.777, 0.776, 0.651, 0.777,\n",
       "        0.796, 0.651, 0.783, 0.785, 0.486, 0.785, 0.788, 0.61 , 0.796,\n",
       "        0.792, 0.683, 0.795, 0.794, 0.527, 0.792, 0.787, 0.46 , 0.794,\n",
       "        0.794, 0.659, 0.792, 0.801, 0.556, 0.788, 0.787, 0.324, 0.8  ,\n",
       "        0.791, 0.498, 0.796, 0.791, 0.657, 0.79 , 0.796, 0.691, 0.789,\n",
       "        0.787, 0.666, 0.784, 0.781, 0.481, 0.797, 0.792, 0.652, 0.787,\n",
       "        0.785, 0.535, 0.789, 0.796, 0.79 , 0.791, 0.815, 0.814, 0.808,\n",
       "        0.817, 0.817, 0.821, 0.817, 0.821, 0.817, 0.786, 0.803, 0.813,\n",
       "        0.821, 0.803, 0.811, 0.818, 0.815, 0.817, 0.818, 0.818, 0.821,\n",
       "        0.793, 0.816, 0.785, 0.814, 0.817, 0.811, 0.822, 0.814, 0.821,\n",
       "        0.82 , 0.824, 0.818, 0.814, 0.798, 0.79 , 0.81 , 0.799, 0.808,\n",
       "        0.811, 0.813, 0.814, 0.818, 0.816, 0.811, 0.821, 0.818, 0.812,\n",
       "        0.843, 0.826, 0.834, 0.838, 0.837, 0.837, 0.833, 0.842, 0.846,\n",
       "        0.827, 0.827, 0.826, 0.833, 0.832, 0.835, 0.841, 0.845, 0.846,\n",
       "        0.842, 0.84 , 0.841, 0.826, 0.821, 0.821, 0.839, 0.833, 0.836,\n",
       "        0.832, 0.843, 0.841, 0.837, 0.843, 0.841, 0.822, 0.827, 0.824,\n",
       "        0.829, 0.835, 0.838, 0.834, 0.844, 0.837, 0.84 , 0.841, 0.847]),\n",
       " 'mean_test_precision_micro': array([0.8462, 0.8468, 0.846 , 0.8536, 0.8508, 0.8584, 0.8556, 0.8542,\n",
       "        0.858 , 0.86  , 0.8614, 0.8558, 0.8466, 0.8506, 0.8466, 0.8554,\n",
       "        0.8502, 0.8526, 0.8548, 0.8506, 0.854 , 0.8532, 0.8526, 0.851 ,\n",
       "        0.844 , 0.8462, 0.8486, 0.8546, 0.8572, 0.8576, 0.851 , 0.8584,\n",
       "        0.8586, 0.8574, 0.847 , 0.8518, 0.8464, 0.8524, 0.8464, 0.8568,\n",
       "        0.8574, 0.8584, 0.861 , 0.8542, 0.8532, 0.8616, 0.858 , 0.856 ,\n",
       "        0.8452, 0.85  , 0.8482, 0.8606, 0.8542, 0.8502, 0.8508, 0.8508,\n",
       "        0.8476, 0.8588, 0.849 , 0.8524, 0.8466, 0.8514, 0.8496, 0.85  ,\n",
       "        0.8536, 0.8508, 0.8514, 0.8536, 0.8482, 0.8582, 0.8538, 0.8554,\n",
       "        0.8482, 0.8468, 0.8468, 0.8532, 0.8522, 0.8566, 0.851 , 0.8518,\n",
       "        0.8496, 0.848 , 0.8508, 0.8516, 0.848 , 0.8492, 0.85  , 0.859 ,\n",
       "        0.854 , 0.8554, 0.8532, 0.8528, 0.855 , 0.8554, 0.851 , 0.8548,\n",
       "        0.7792, 0.5226, 0.7806, 0.7802, 0.5576, 0.7848, 0.7832, 0.4562,\n",
       "        0.7784, 0.7836, 0.6514, 0.7842, 0.7796, 0.4464, 0.777 , 0.7776,\n",
       "        0.6058, 0.7824, 0.784 , 0.5548, 0.7862, 0.7836, 0.6292, 0.7848,\n",
       "        0.7796, 0.4712, 0.7792, 0.7858, 0.474 , 0.7814, 0.7818, 0.5296,\n",
       "        0.7846, 0.7852, 0.5716, 0.784 , 0.7754, 0.6586, 0.7784, 0.782 ,\n",
       "        0.448 , 0.7804, 0.7826, 0.5664, 0.7834, 0.7826, 0.5906, 0.7848,\n",
       "        0.7954, 0.5412, 0.7976, 0.8014, 0.571 , 0.8086, 0.8082, 0.458 ,\n",
       "        0.802 , 0.8142, 0.6572, 0.8098, 0.8042, 0.4968, 0.7978, 0.8   ,\n",
       "        0.4842, 0.806 , 0.8072, 0.605 , 0.8106, 0.8068, 0.548 , 0.8038,\n",
       "        0.8054, 0.439 , 0.7946, 0.8084, 0.4506, 0.809 , 0.806 , 0.587 ,\n",
       "        0.8044, 0.8026, 0.62  , 0.8024, 0.796 , 0.6106, 0.8032, 0.8076,\n",
       "        0.5542, 0.8036, 0.8064, 0.5496, 0.8068, 0.8064, 0.5672, 0.8022,\n",
       "        0.8192, 0.8224, 0.812 , 0.8302, 0.8298, 0.8296, 0.833 , 0.8348,\n",
       "        0.8338, 0.8326, 0.8368, 0.8332, 0.8158, 0.8214, 0.8138, 0.8306,\n",
       "        0.8282, 0.8316, 0.8346, 0.8328, 0.8344, 0.8336, 0.8344, 0.834 ,\n",
       "        0.8058, 0.8232, 0.8114, 0.8288, 0.8296, 0.8306, 0.8346, 0.8328,\n",
       "        0.8324, 0.8352, 0.8362, 0.8328, 0.8162, 0.8122, 0.8048, 0.8216,\n",
       "        0.8226, 0.8256, 0.8308, 0.831 , 0.8296, 0.8304, 0.8316, 0.827 ,\n",
       "        0.8378, 0.8422, 0.8338, 0.848 , 0.8474, 0.847 , 0.8548, 0.8538,\n",
       "        0.8524, 0.8534, 0.851 , 0.8556, 0.8422, 0.8378, 0.8406, 0.8472,\n",
       "        0.8498, 0.851 , 0.8526, 0.8536, 0.8534, 0.8538, 0.8548, 0.8528,\n",
       "        0.8392, 0.8372, 0.843 , 0.8484, 0.8452, 0.8492, 0.8498, 0.8516,\n",
       "        0.8516, 0.8526, 0.8538, 0.8528, 0.8396, 0.8362, 0.8372, 0.8434,\n",
       "        0.8476, 0.8464, 0.8492, 0.8516, 0.8514, 0.8486, 0.8532, 0.8516]),\n",
       " 'std_test_precision_micro': array([0.00597997, 0.00552811, 0.00672309, 0.0065909 , 0.00324962,\n",
       "        0.00608605, 0.00523832, 0.01195659, 0.00839047, 0.00666333,\n",
       "        0.01190966, 0.01390539, 0.0074458 , 0.01386506, 0.00930806,\n",
       "        0.0106132 , 0.01377534, 0.00922171, 0.01334766, 0.00801499,\n",
       "        0.01071448, 0.00976524, 0.01352923, 0.01256981, 0.00942338,\n",
       "        0.01165161, 0.01241934, 0.0118254 , 0.0136587 , 0.01144727,\n",
       "        0.00596657, 0.00868562, 0.0153961 , 0.00806474, 0.00839047,\n",
       "        0.01530229, 0.01116423, 0.01092886, 0.01059434, 0.01151347,\n",
       "        0.01066958, 0.00891291, 0.01124278, 0.00453431, 0.00908625,\n",
       "        0.01070701, 0.01407125, 0.00957079, 0.00926067, 0.00576194,\n",
       "        0.00910824, 0.01185917, 0.01205653, 0.01002796, 0.00800999,\n",
       "        0.01278124, 0.01246756, 0.00835225, 0.01516575, 0.0102684 ,\n",
       "        0.0063435 , 0.00854634, 0.0094361 , 0.01354991, 0.00966644,\n",
       "        0.00667533, 0.01300154, 0.01434713, 0.00620967, 0.00386782,\n",
       "        0.01133843, 0.00717217, 0.01059056, 0.00895321, 0.00713863,\n",
       "        0.00910824, 0.00711056, 0.0069455 , 0.00729383, 0.00735935,\n",
       "        0.00731027, 0.01268069, 0.00893085, 0.01870401, 0.01054514,\n",
       "        0.0056    , 0.00660303, 0.0114717 , 0.00829458, 0.00884534,\n",
       "        0.00803492, 0.01251239, 0.01071448, 0.00796492, 0.01122497,\n",
       "        0.01022546, 0.01020588, 0.14293719, 0.0058515 , 0.01177115,\n",
       "        0.126261  , 0.00798499, 0.0095163 , 0.11490936, 0.00717217,\n",
       "        0.00801499, 0.0008    , 0.0044    , 0.00997196, 0.12012094,\n",
       "        0.00989949, 0.00763151, 0.09241299, 0.00917824, 0.00715542,\n",
       "        0.12544066, 0.00708237, 0.00682935, 0.05083857, 0.00604649,\n",
       "        0.00816333, 0.14844851, 0.01012719, 0.00874986, 0.15643913,\n",
       "        0.00560714, 0.00607947, 0.14868705, 0.00889044, 0.00800999,\n",
       "        0.11570583, 0.00619677, 0.01310878, 0.01374918, 0.01132431,\n",
       "        0.00562139, 0.11555432, 0.00985089, 0.00770973, 0.12931605,\n",
       "        0.00811419, 0.00688767, 0.1208    , 0.00708237, 0.01160345,\n",
       "        0.10165707, 0.01248359, 0.01228983, 0.13592351, 0.01598249,\n",
       "        0.0144    , 0.11486514, 0.01186592, 0.01723253, 0.04209703,\n",
       "        0.01601749, 0.013891  , 0.08896606, 0.01187266, 0.00909945,\n",
       "        0.11242847, 0.00807465, 0.01244829, 0.07682968, 0.01725804,\n",
       "        0.00847113, 0.08184131, 0.0124964 , 0.00960417, 0.14514958,\n",
       "        0.00531413, 0.01733897, 0.10811771, 0.01065833, 0.0116619 ,\n",
       "        0.09013545, 0.01227355, 0.01190966, 0.07464851, 0.01156892,\n",
       "        0.01041153, 0.06583495, 0.0165336 , 0.01776063, 0.04541101,\n",
       "        0.00431741, 0.01412232, 0.12527825, 0.01444161, 0.01689497,\n",
       "        0.07188713, 0.01049571, 0.01794882, 0.02014547, 0.01691153,\n",
       "        0.01328759, 0.01087014, 0.01348481, 0.01060189, 0.01270276,\n",
       "        0.0112321 , 0.0111463 , 0.01064707, 0.0124    , 0.01838913,\n",
       "        0.01309351, 0.0109252 , 0.00749933, 0.0148243 , 0.01262696,\n",
       "        0.0108922 , 0.01301384, 0.01162067, 0.01142979, 0.01187603,\n",
       "        0.01011929, 0.00730479, 0.00957914, 0.0181615 , 0.01202331,\n",
       "        0.01237093, 0.01228983, 0.00886792, 0.01208967, 0.00917824,\n",
       "        0.0119733 , 0.00837616, 0.01161723, 0.00928224, 0.00974474,\n",
       "        0.00957914, 0.00935094, 0.0147594 , 0.01336563, 0.01393413,\n",
       "        0.01228007, 0.01303227, 0.01160345, 0.01189285, 0.01285302,\n",
       "        0.01075918, 0.01558717, 0.01354105, 0.00822192, 0.01134196,\n",
       "        0.00769415, 0.00923905, 0.01077775, 0.0115862 , 0.013937  ,\n",
       "        0.0108074 , 0.00900222, 0.01051475, 0.01014692, 0.01049952,\n",
       "        0.01012719, 0.01107068, 0.01136662, 0.0096871 , 0.00768375,\n",
       "        0.00793977, 0.00624179, 0.0103034 , 0.01072194, 0.01068457,\n",
       "        0.01108873, 0.0158619 , 0.00798999, 0.00879545, 0.01205653,\n",
       "        0.0112321 , 0.00628013, 0.01233856, 0.0107443 , 0.01053376,\n",
       "        0.00915205, 0.01009158, 0.00803492, 0.010008  , 0.00939361,\n",
       "        0.00917824, 0.00671118, 0.01208967, 0.01063203, 0.0148    ,\n",
       "        0.0112    , 0.00913017, 0.00826075]),\n",
       " 'rank_test_precision_micro': array([126, 117, 128,  44,  83,   9,  23,  35,  13,   5,   2,  22, 120,\n",
       "         87, 120,  25,  90,  58,  30,  87,  38,  50,  58,  76, 131, 126,\n",
       "        103,  34,  18,  15,  76,   9,   8,  16, 115,  67, 124,  62, 123,\n",
       "         19,  16,   9,   3,  35,  50,   1,  13,  21, 129,  91, 105,   4,\n",
       "         35,  89,  82,  83, 111,   7, 101,  62, 120,  73,  96,  91,  44,\n",
       "         83,  73,  44, 105,  12,  40,  25, 105, 117, 117,  50,  65,  20,\n",
       "         76,  66,  96, 108,  83,  68, 108,  99,  91,   6,  38,  25,  50,\n",
       "         56,  29,  28,  76,  30, 250, 278, 245, 247, 271, 228, 238, 284,\n",
       "        253, 235, 259, 232, 249, 287, 255, 254, 263, 241, 233, 272, 225,\n",
       "        235, 260, 230, 248, 282, 250, 226, 281, 244, 243, 277, 231, 227,\n",
       "        267, 233, 256, 257, 252, 242, 286, 246, 239, 270, 237, 239, 265,\n",
       "        228, 223, 276, 221, 218, 268, 195, 197, 283, 217, 187, 258, 193,\n",
       "        210, 279, 220, 219, 280, 205, 199, 264, 192, 200, 275, 211, 207,\n",
       "        288, 224, 196, 285, 194, 204, 266, 209, 214, 261, 215, 222, 262,\n",
       "        213, 198, 273, 212, 202, 274, 200, 202, 269, 216, 184, 181, 190,\n",
       "        170, 171, 172, 157, 147, 153, 161, 143, 156, 186, 183, 188, 168,\n",
       "        176, 163, 149, 158, 150, 155, 150, 152, 206, 179, 191, 175, 172,\n",
       "        167, 148, 158, 162, 146, 144, 158, 185, 189, 208, 182, 180, 178,\n",
       "        166, 165, 172, 169, 164, 177, 139, 135, 153, 108, 113, 115,  30,\n",
       "         43,  64,  48,  76,  24, 134, 139, 136, 114,  94,  76,  58,  44,\n",
       "         49,  40,  30,  56, 138, 141, 133, 104, 129,  98,  94,  68,  68,\n",
       "         58,  40,  55, 137, 145, 141, 132, 111, 124,  99,  68,  73, 102,\n",
       "         50,  68]),\n",
       " 'split0_test_roc_auc_ovo': array([0.91500843, 0.90644765, 0.91155331, 0.91082707, 0.91427779,\n",
       "        0.91177338, 0.90781209, 0.90595909, 0.90694061, 0.90436137,\n",
       "        0.91008763, 0.90452423, 0.90162369, 0.91204627, 0.91046616,\n",
       "        0.90962108, 0.90651367, 0.90911052, 0.91451107, 0.90158848,\n",
       "        0.910607  , 0.90982795, 0.90421613, 0.90391243, 0.90980594,\n",
       "        0.91217831, 0.91834031, 0.91206387, 0.90282528, 0.91454188,\n",
       "        0.89600306, 0.91267127, 0.91353835, 0.90487194, 0.89838864,\n",
       "        0.9158579 , 0.90988957, 0.91957711, 0.91016686, 0.91389927,\n",
       "        0.91784735, 0.91257004, 0.91018446, 0.91686583, 0.90892125,\n",
       "        0.90337105, 0.90706825, 0.90287809, 0.91855158, 0.92004806,\n",
       "        0.91718714, 0.92034736, 0.91378923, 0.91103834, 0.89709462,\n",
       "        0.9044538 , 0.9112144 , 0.91081827, 0.90601631, 0.89745994,\n",
       "        0.91959472, 0.91509206, 0.91974437, 0.91024608, 0.91475755,\n",
       "        0.90943622, 0.92205511, 0.92161937, 0.90182615, 0.90741156,\n",
       "        0.90620557, 0.89705941, 0.91378923, 0.91710791, 0.91959032,\n",
       "        0.91365279, 0.9056818 , 0.91437022, 0.90489395, 0.90610874,\n",
       "        0.90212545, 0.89338862, 0.90603832, 0.90646086, 0.91757446,\n",
       "        0.92039578, 0.91382885, 0.91040894, 0.9022927 , 0.91806742,\n",
       "        0.90554536, 0.90933939, 0.91304099, 0.90201541, 0.89645641,\n",
       "        0.91548378, 0.84060229, 0.45516045, 0.83643414, 0.84098962,\n",
       "        0.72085264, 0.8421736 , 0.84194473, 0.45646768, 0.83995528,\n",
       "        0.83964718, 0.54390204, 0.83690949, 0.83858204, 0.49299073,\n",
       "        0.83300103, 0.84047465, 0.57226924, 0.83833996, 0.84066831,\n",
       "        0.77448844, 0.84148258, 0.84335318, 0.27885246, 0.84174226,\n",
       "        0.84086638, 0.79154398, 0.8458708 , 0.83976602, 0.35146281,\n",
       "        0.83869207, 0.83981003, 0.43606266, 0.8412493 , 0.84267977,\n",
       "        0.48637538, 0.8390926 , 0.84417185, 0.6035678 , 0.83975282,\n",
       "        0.8418479 , 0.66078196, 0.84398259, 0.83831355, 0.67242814,\n",
       "        0.84202395, 0.84222202, 0.5474716 , 0.84435671, 0.86109534,\n",
       "        0.51119063, 0.86087527, 0.86907073, 0.40476411, 0.85812438,\n",
       "        0.87289117, 0.45301256, 0.87548801, 0.87187884, 0.67250736,\n",
       "        0.86810681, 0.87317726, 0.32088169, 0.86249059, 0.8643524 ,\n",
       "        0.6473532 , 0.87102056, 0.86387704, 0.7284363 , 0.87380226,\n",
       "        0.8782257 , 0.42337334, 0.8668436 , 0.87412797, 0.33710096,\n",
       "        0.85628458, 0.87218694, 0.38705716, 0.87640791, 0.86587969,\n",
       "        0.26559096, 0.8671429 , 0.86563761, 0.54726033, 0.86660153,\n",
       "        0.85414548, 0.36659932, 0.86201524, 0.87726619, 0.67437797,\n",
       "        0.87219134, 0.86173355, 0.72421534, 0.86523708, 0.86647829,\n",
       "        0.40493576, 0.86797917, 0.88280318, 0.87810686, 0.86935242,\n",
       "        0.8982874 , 0.8942557 , 0.89986752, 0.90118795, 0.90128038,\n",
       "        0.89787367, 0.90153566, 0.90294412, 0.90070379, 0.88086655,\n",
       "        0.87629787, 0.87077408, 0.90026805, 0.88936131, 0.8970506 ,\n",
       "        0.89663687, 0.9010603 , 0.8998279 , 0.90068178, 0.90143883,\n",
       "        0.89965625, 0.86858657, 0.88096338, 0.86941404, 0.89659726,\n",
       "        0.89936575, 0.89779885, 0.8942513 , 0.90094147, 0.88967821,\n",
       "        0.90190098, 0.90176893, 0.89727948, 0.88032078, 0.88646957,\n",
       "        0.87418959, 0.89165005, 0.88970462, 0.89524162, 0.89819938,\n",
       "        0.89743793, 0.89521961, 0.8985691 , 0.8992029 , 0.89816857,\n",
       "        0.90861315, 0.9109107 , 0.90740716, 0.90942302, 0.91034732,\n",
       "        0.9127593 , 0.91112637, 0.91323465, 0.91115278, 0.91271528,\n",
       "        0.91551899, 0.91300578, 0.90791333, 0.89342823, 0.90394324,\n",
       "        0.91264046, 0.91650932, 0.91591512, 0.91573026, 0.9105938 ,\n",
       "        0.91482357, 0.91514047, 0.91488519, 0.91633326, 0.90510962,\n",
       "        0.91217391, 0.90947583, 0.90924696, 0.90800136, 0.9155322 ,\n",
       "        0.91345032, 0.91005682, 0.91402691, 0.9136968 , 0.91356476,\n",
       "        0.91396529, 0.90996879, 0.89586662, 0.90556737, 0.91165454,\n",
       "        0.91403571, 0.90808938, 0.9139873 , 0.91405332, 0.91053658,\n",
       "        0.91334909, 0.9149292 , 0.91172496]),\n",
       " 'split1_test_roc_auc_ovo': array([0.89120991, 0.8914872 , 0.88818173, 0.90525927, 0.90668093,\n",
       "        0.90007879, 0.90034727, 0.89353826, 0.8979705 , 0.90765364,\n",
       "        0.89936135, 0.90264042, 0.89085339, 0.90200221, 0.89238069,\n",
       "        0.90169851, 0.90404447, 0.89412365, 0.88783841, 0.89356027,\n",
       "        0.89713863, 0.88728383, 0.89574338, 0.88878472, 0.89075216,\n",
       "        0.89839744, 0.89934815, 0.90131559, 0.89500834, 0.89247752,\n",
       "        0.89590623, 0.89813335, 0.88865708, 0.90052333, 0.89234988,\n",
       "        0.87922922, 0.89186572, 0.89634197, 0.89358668, 0.89865712,\n",
       "        0.9056598 , 0.90378479, 0.90634202, 0.89700659, 0.90551455,\n",
       "        0.90601631, 0.89714303, 0.89714303, 0.89842825, 0.89665007,\n",
       "        0.89414566, 0.8914872 , 0.90143002, 0.89623634, 0.89337981,\n",
       "        0.8954881 , 0.89379795, 0.89429091, 0.88751711, 0.8911703 ,\n",
       "        0.89907526, 0.89353386, 0.89157523, 0.89979709, 0.90507441,\n",
       "        0.89616592, 0.9044362 , 0.90464307, 0.90100309, 0.89664567,\n",
       "        0.89717824, 0.89275921, 0.89694057, 0.88049683, 0.89571257,\n",
       "        0.89947579, 0.8967293 , 0.89772402, 0.89133755, 0.90006118,\n",
       "        0.89679972, 0.89632437, 0.89620993, 0.8899643 , 0.89326978,\n",
       "        0.88980145, 0.90230591, 0.89898283, 0.89636398, 0.90376718,\n",
       "        0.88652239, 0.89547049, 0.9019802 , 0.88649598, 0.89781645,\n",
       "        0.89845906, 0.81933459, 0.3841038 , 0.82085308, 0.82580909,\n",
       "        0.40097007, 0.82248161, 0.82126242, 0.47182866, 0.82210309,\n",
       "        0.82172897, 0.4401912 , 0.82216031, 0.82751245, 0.43771319,\n",
       "        0.82446666, 0.81879762, 0.33150234, 0.82200626, 0.82101594,\n",
       "        0.36938103, 0.82216911, 0.8242818 , 0.37989604, 0.82701068,\n",
       "        0.81583986, 0.60658278, 0.82251242, 0.82492441, 0.68837891,\n",
       "        0.81927297, 0.82342792, 0.36513805, 0.82254323, 0.82693146,\n",
       "        0.3782323 , 0.8239825 , 0.80539527, 0.53931575, 0.82422458,\n",
       "        0.82180379, 0.41104934, 0.82724836, 0.82264887, 0.69932086,\n",
       "        0.82634607, 0.81930378, 0.50557881, 0.82373602, 0.8424641 ,\n",
       "        0.44585143, 0.86345891, 0.86171594, 0.51884471, 0.85681275,\n",
       "        0.85815519, 0.64874845, 0.8474245 , 0.85791751, 0.5792191 ,\n",
       "        0.8699246 , 0.85061114, 0.52937293, 0.84241128, 0.86082685,\n",
       "        0.38489166, 0.85631539, 0.85702402, 0.42284957, 0.86599413,\n",
       "        0.85275904, 0.60515231, 0.86197562, 0.85486732, 0.55071105,\n",
       "        0.8458664 , 0.84511375, 0.34685452, 0.86762706, 0.85613933,\n",
       "        0.74732723, 0.86549237, 0.85723969, 0.64568946, 0.85013138,\n",
       "        0.85014459, 0.48692556, 0.8486569 , 0.86139904, 0.55144169,\n",
       "        0.85491573, 0.86059358, 0.60690848, 0.86507863, 0.85434355,\n",
       "        0.66221242, 0.85781628, 0.87001263, 0.87842376, 0.86027227,\n",
       "        0.88400477, 0.87781196, 0.88324332, 0.88601622, 0.88435689,\n",
       "        0.8834766 , 0.88585777, 0.88734986, 0.88492027, 0.8649774 ,\n",
       "        0.86725294, 0.86039551, 0.88179966, 0.88450213, 0.88124067,\n",
       "        0.88526358, 0.88638154, 0.88431287, 0.88720461, 0.88681288,\n",
       "        0.88534281, 0.86269306, 0.87136387, 0.87995546, 0.87892552,\n",
       "        0.88172043, 0.88212536, 0.8834898 , 0.88598982, 0.88528119,\n",
       "        0.88579175, 0.88549245, 0.88525918, 0.87399152, 0.87048799,\n",
       "        0.86415873, 0.88281198, 0.87808485, 0.87506107, 0.88308047,\n",
       "        0.88187888, 0.87991584, 0.88135511, 0.88198452, 0.87693168,\n",
       "        0.88815532, 0.89079177, 0.88727503, 0.8918085 , 0.89798811,\n",
       "        0.89300569, 0.89356467, 0.89547049, 0.89314214, 0.89421608,\n",
       "        0.89351186, 0.89684814, 0.89007874, 0.88074331, 0.89433492,\n",
       "        0.89015357, 0.89228826, 0.89799691, 0.88948015, 0.8939564 ,\n",
       "        0.89953741, 0.89984111, 0.89595905, 0.89962984, 0.88799246,\n",
       "        0.88549686, 0.88808489, 0.89401362, 0.89234988, 0.89510077,\n",
       "        0.89275481, 0.89976188, 0.89855589, 0.90218267, 0.89473545,\n",
       "        0.89900044, 0.88700654, 0.8843921 , 0.88835778, 0.89290446,\n",
       "        0.8939652 , 0.89480147, 0.89356467, 0.89493792, 0.89792209,\n",
       "        0.89205498, 0.89347224, 0.89563334]),\n",
       " 'split2_test_roc_auc_ovo': array([0.91142567, 0.91827869, 0.91640808, 0.91594153, 0.91582269,\n",
       "        0.92522854, 0.91450667, 0.92518453, 0.92491164, 0.91978398,\n",
       "        0.91444505, 0.91115718, 0.90604712, 0.91844154, 0.91466072,\n",
       "        0.92064226, 0.9179926 , 0.91356476, 0.91965194, 0.90960348,\n",
       "        0.90965629, 0.91639048, 0.91539135, 0.91560702, 0.91466512,\n",
       "        0.91497762, 0.90384201, 0.92272413, 0.92158416, 0.92058504,\n",
       "        0.9207699 , 0.91609558, 0.91969595, 0.93172505, 0.91118799,\n",
       "        0.92096796, 0.90647406, 0.91268007, 0.91594593, 0.91882887,\n",
       "        0.92884652, 0.92104279, 0.92648295, 0.91731038, 0.92459914,\n",
       "        0.92485002, 0.91650491, 0.91406652, 0.92062465, 0.91188782,\n",
       "        0.91225313, 0.9253958 , 0.91472674, 0.91885968, 0.91918538,\n",
       "        0.90865277, 0.92165899, 0.91093711, 0.91033411, 0.92129367,\n",
       "        0.91538255, 0.90555856, 0.91419416, 0.9090621 , 0.91477515,\n",
       "        0.91299257, 0.92100317, 0.9068922 , 0.91179979, 0.9140005 ,\n",
       "        0.91743362, 0.91687023, 0.91896531, 0.91157532, 0.91487198,\n",
       "        0.9229398 , 0.91159732, 0.90819942, 0.91762288, 0.91326546,\n",
       "        0.9093746 , 0.91895651, 0.90283408, 0.9219979 , 0.91898292,\n",
       "        0.91426019, 0.91054098, 0.92365283, 0.91651812, 0.91561142,\n",
       "        0.91710351, 0.91733678, 0.92072148, 0.91692305, 0.91437022,\n",
       "        0.91447145, 0.83752129, 0.61513915, 0.83758291, 0.83514452,\n",
       "        0.58740135, 0.83681706, 0.83626688, 0.52582538, 0.8344667 ,\n",
       "        0.83705914, 0.73030251, 0.83611284, 0.83053623, 0.56592679,\n",
       "        0.83863485, 0.83081352, 0.44861553, 0.8363285 , 0.83608643,\n",
       "        0.29832878, 0.83734523, 0.83829594, 0.37719356, 0.8323012 ,\n",
       "        0.83561107, 0.55882288, 0.83434786, 0.83659699, 0.44785408,\n",
       "        0.83884172, 0.83677745, 0.33836857, 0.83750809, 0.83719559,\n",
       "        0.36859317, 0.83707675, 0.83789101, 0.57186871, 0.82817706,\n",
       "        0.83818591, 0.51991866, 0.83429945, 0.83890774, 0.37848758,\n",
       "        0.83583114, 0.83648696, 0.21896663, 0.83849401, 0.83973521,\n",
       "        0.37893213, 0.85009177, 0.85375376, 0.68148627, 0.8683797 ,\n",
       "        0.8671385 , 0.29810431, 0.86224851, 0.8803912 , 0.64106796,\n",
       "        0.86987619, 0.85941399, 0.59039432, 0.85630218, 0.87447568,\n",
       "        0.46045977, 0.87592815, 0.87398712, 0.7157954 , 0.86519307,\n",
       "        0.86818604, 0.62185133, 0.87011827, 0.87615703, 0.56844   ,\n",
       "        0.86318602, 0.87877147, 0.4916791 , 0.86572124, 0.87365701,\n",
       "        0.38290221, 0.87085771, 0.86635505, 0.41860219, 0.86381542,\n",
       "        0.86608656, 0.71309293, 0.86649589, 0.87126264, 0.55427621,\n",
       "        0.86871421, 0.87295279, 0.2728005 , 0.87512709, 0.87351177,\n",
       "        0.56061866, 0.86648709, 0.86680839, 0.89008754, 0.87982341,\n",
       "        0.88570372, 0.89476626, 0.89310252, 0.89417207, 0.89194935,\n",
       "        0.89275481, 0.89422049, 0.89694497, 0.88938332, 0.88702415,\n",
       "        0.88826095, 0.86840611, 0.89274161, 0.88937891, 0.89595905,\n",
       "        0.89417647, 0.893349  , 0.89539126, 0.89462102, 0.8945858 ,\n",
       "        0.89511838, 0.88517115, 0.88668084, 0.88590179, 0.88733225,\n",
       "        0.88584457, 0.8951888 , 0.89633317, 0.89462102, 0.89462982,\n",
       "        0.89385517, 0.8961175 , 0.89414566, 0.86642987, 0.87036915,\n",
       "        0.86646948, 0.88642996, 0.88429967, 0.88899599, 0.88756112,\n",
       "        0.89283844, 0.89015357, 0.89156642, 0.89126273, 0.88849423,\n",
       "        0.90674695, 0.9025964 , 0.90009639, 0.90857794, 0.91038693,\n",
       "        0.91422938, 0.91619241, 0.91545297, 0.91498202, 0.91736759,\n",
       "        0.91446265, 0.91712992, 0.90327862, 0.90604712, 0.90062016,\n",
       "        0.91043535, 0.91036932, 0.90974432, 0.9127549 , 0.91516248,\n",
       "        0.91542216, 0.91705069, 0.91557621, 0.91448906, 0.89905765,\n",
       "        0.89637718, 0.90573462, 0.9112188 , 0.91172056, 0.90797055,\n",
       "        0.9167822 , 0.91591952, 0.91486318, 0.91656213, 0.91401371,\n",
       "        0.91470473, 0.90406648, 0.90866597, 0.90554096, 0.91000401,\n",
       "        0.91217391, 0.91082707, 0.91315983, 0.91327427, 0.91479276,\n",
       "        0.91670298, 0.91382444, 0.91673379]),\n",
       " 'split3_test_roc_auc_ovo': array([0.91918979, 0.91086669, 0.91143007, 0.92268892, 0.91918538,\n",
       "        0.92597679, 0.92360882, 0.92842398, 0.91781654, 0.92005687,\n",
       "        0.91688784, 0.92775496, 0.91700668, 0.92421622, 0.91823908,\n",
       "        0.92349878, 0.92730162, 0.9241546 , 0.92574351, 0.91987641,\n",
       "        0.92206392, 0.92698471, 0.92150053, 0.9170859 , 0.91382004,\n",
       "        0.92240283, 0.91760087, 0.92665021, 0.9297312 , 0.92249966,\n",
       "        0.91656653, 0.92559826, 0.92904018, 0.91240278, 0.91845035,\n",
       "        0.91504804, 0.91451107, 0.9241722 , 0.91699347, 0.92453752,\n",
       "        0.91668537, 0.92621006, 0.92768014, 0.92163258, 0.92350318,\n",
       "        0.92327431, 0.92145652, 0.917997  , 0.92229719, 0.92186585,\n",
       "        0.92352959, 0.91149609, 0.92871007, 0.9177065 , 0.91736759,\n",
       "        0.92235881, 0.91396529, 0.91878926, 0.90890365, 0.91686583,\n",
       "        0.90949344, 0.91996884, 0.92066426, 0.91391687, 0.92034736,\n",
       "        0.91718273, 0.91792658, 0.91490279, 0.91448906, 0.91587551,\n",
       "        0.9226405 , 0.91037813, 0.91624523, 0.91218271, 0.91539135,\n",
       "        0.92044859, 0.91252602, 0.9177021 , 0.91040013, 0.91714312,\n",
       "        0.91686583, 0.91596354, 0.91407533, 0.90552335, 0.91254803,\n",
       "        0.91468712, 0.91876725, 0.92505249, 0.91968715, 0.92240283,\n",
       "        0.91239838, 0.92709915, 0.91889929, 0.92369245, 0.91913257,\n",
       "        0.91674259, 0.85308474, 0.31284029, 0.85606451, 0.85654866,\n",
       "        0.21555553, 0.85458123, 0.85164988, 0.34629994, 0.85691838,\n",
       "        0.8575918 , 0.3359918 , 0.85503897, 0.85624056, 0.68960691,\n",
       "        0.85000374, 0.84916747, 0.52569774, 0.85659268, 0.85137699,\n",
       "        0.32619422, 0.85445358, 0.8569712 , 0.51846619, 0.85456802,\n",
       "        0.8514034 , 0.5576565 , 0.85147382, 0.85547912, 0.61300446,\n",
       "        0.85332682, 0.85256097, 0.68151268, 0.85271062, 0.8560469 ,\n",
       "        0.73247241, 0.85609972, 0.85282946, 0.19000084, 0.85713405,\n",
       "        0.85489813, 0.37098755, 0.84780743, 0.85124054, 0.49864656,\n",
       "        0.85873177, 0.8538946 , 0.65540341, 0.85098086, 0.88605144,\n",
       "        0.4442009 , 0.8856465 , 0.87784717, 0.8141101 , 0.88820373,\n",
       "        0.88944934, 0.46124323, 0.88088416, 0.88883314, 0.58001136,\n",
       "        0.89104706, 0.88234543, 0.38762495, 0.88455495, 0.88322132,\n",
       "        0.66702758, 0.8723674 , 0.89121431, 0.39079837, 0.89133315,\n",
       "        0.88154437, 0.60120423, 0.88561569, 0.8671473 , 0.65703634,\n",
       "        0.8757697 , 0.89293087, 0.62028002, 0.88769757, 0.88722662,\n",
       "        0.76738454, 0.88551886, 0.88786042, 0.58553515, 0.88707257,\n",
       "        0.88583577, 0.62905647, 0.88649158, 0.89348545, 0.55158253,\n",
       "        0.88202413, 0.88949775, 0.52756394, 0.88919405, 0.89187893,\n",
       "        0.72793014, 0.88662362, 0.90061576, 0.90071259, 0.90091946,\n",
       "        0.90317739, 0.90333144, 0.90238513, 0.90206823, 0.90404887,\n",
       "        0.90400046, 0.90269764, 0.90230151, 0.90335345, 0.89978389,\n",
       "        0.89830061, 0.8957962 , 0.89986312, 0.90311137, 0.90233672,\n",
       "        0.90287369, 0.90471349, 0.9047443 , 0.90370116, 0.9032126 ,\n",
       "        0.90343267, 0.88689211, 0.9001228 , 0.89705941, 0.90216066,\n",
       "        0.90286049, 0.90286489, 0.90334024, 0.90269323, 0.90205943,\n",
       "        0.90275045, 0.9035251 , 0.90368796, 0.89198016, 0.88428646,\n",
       "        0.8884018 , 0.89070815, 0.8985779 , 0.89990713, 0.90014921,\n",
       "        0.90143002, 0.90157527, 0.90181735, 0.90142562, 0.89932174,\n",
       "        0.90854713, 0.91124081, 0.91240718, 0.91634646, 0.9204706 ,\n",
       "        0.91753485, 0.92187025, 0.92288258, 0.92005247, 0.91995123,\n",
       "        0.92222677, 0.92254807, 0.91412374, 0.90890805, 0.90943622,\n",
       "        0.91384205, 0.91660615, 0.91672939, 0.9201625 , 0.92074349,\n",
       "        0.92204631, 0.92098997, 0.9201757 , 0.92161057, 0.90675575,\n",
       "        0.90723111, 0.91263606, 0.91788696, 0.90881122, 0.92044419,\n",
       "        0.92133328, 0.91975317, 0.92075669, 0.92394333, 0.92265811,\n",
       "        0.91987201, 0.91069063, 0.9075348 , 0.90945823, 0.91730157,\n",
       "        0.91528572, 0.91573907, 0.91981479, 0.91658854, 0.92161057,\n",
       "        0.92045299, 0.91932623, 0.91988961]),\n",
       " 'split4_test_roc_auc_ovo': array([0.89707261, 0.8976536 , 0.89526802, 0.89885959, 0.88657961,\n",
       "        0.90701544, 0.90547053, 0.89779885, 0.90329623, 0.89139037,\n",
       "        0.9041193 , 0.88745549, 0.88795285, 0.89434373, 0.89899603,\n",
       "        0.88626711, 0.88019313, 0.90371436, 0.89074336, 0.89569496,\n",
       "        0.90108671, 0.88734986, 0.8819405 , 0.88158839, 0.87246423,\n",
       "        0.89154882, 0.89726187, 0.90255239, 0.90461666, 0.90018882,\n",
       "        0.90317299, 0.89593704, 0.89483669, 0.89931294, 0.90200221,\n",
       "        0.88733665, 0.87404874, 0.89195815, 0.88324773, 0.89867913,\n",
       "        0.90191418, 0.89650923, 0.90130678, 0.89687014, 0.90499078,\n",
       "        0.888415  , 0.89935695, 0.89578299, 0.89745113, 0.88566411,\n",
       "        0.89188773, 0.89062012, 0.89027681, 0.90066418, 0.88090617,\n",
       "        0.88639475, 0.88962539, 0.89313773, 0.8847134 , 0.89558933,\n",
       "        0.89315534, 0.89292206, 0.89294847, 0.89131114, 0.89099864,\n",
       "        0.89846786, 0.88948015, 0.89038684, 0.88755672, 0.89174688,\n",
       "        0.890246  , 0.89445376, 0.89371872, 0.89275481, 0.89500834,\n",
       "        0.88751271, 0.90522846, 0.90272404, 0.90261841, 0.89367911,\n",
       "        0.89417647, 0.88550566, 0.90345908, 0.89708581, 0.89746874,\n",
       "        0.89372753, 0.88366146, 0.89383756, 0.89556292, 0.8979617 ,\n",
       "        0.89214741, 0.8911703 , 0.89085779, 0.89151801, 0.89135075,\n",
       "        0.89308052, 0.81070339, 0.43937253, 0.81017962, 0.8033398 ,\n",
       "        0.65742807, 0.80847187, 0.80357308, 0.56333435, 0.80176849,\n",
       "        0.80531604, 0.42348778, 0.80411886, 0.8104305 , 0.41138825,\n",
       "        0.80451498, 0.810096  , 0.41905114, 0.79980546, 0.80256075,\n",
       "        0.65493686, 0.80643841, 0.80587943, 0.53293368, 0.8023891 ,\n",
       "        0.81275886, 0.50407352, 0.81207224, 0.81111713, 0.6029736 ,\n",
       "        0.80260036, 0.80851588, 0.50193443, 0.80429931, 0.80267519,\n",
       "        0.58869537, 0.80498594, 0.80120951, 0.4269385 , 0.80820338,\n",
       "        0.8063988 , 0.52469861, 0.80201938, 0.80428171, 0.32448206,\n",
       "        0.80652644, 0.80324297, 0.70156119, 0.80018838, 0.83953715,\n",
       "        0.57325076, 0.8375565 , 0.82820347, 0.30061312, 0.83639012,\n",
       "        0.83197109, 0.70532001, 0.84363928, 0.84272818, 0.5949454 ,\n",
       "        0.84981008, 0.83805386, 0.52949617, 0.83186546, 0.83468677,\n",
       "        0.45260763, 0.84676869, 0.85280745, 0.53409126, 0.84779863,\n",
       "        0.84663665, 0.64844035, 0.84456358, 0.83719999, 0.26834625,\n",
       "        0.839097  , 0.84472203, 0.5878547 , 0.83956355, 0.84399139,\n",
       "        0.62788128, 0.83624928, 0.84347642, 0.74234482, 0.84932592,\n",
       "        0.83580033, 0.65361643, 0.82253443, 0.83831355, 0.32344333,\n",
       "        0.84775461, 0.83795703, 0.60794282, 0.84149138, 0.83727041,\n",
       "        0.56968561, 0.84631534, 0.8470944 , 0.84496851, 0.84349843,\n",
       "        0.8649686 , 0.85979252, 0.86092368, 0.86477933, 0.86523708,\n",
       "        0.86727054, 0.8659105 , 0.8655892 , 0.86553638, 0.84116567,\n",
       "        0.8520196 , 0.86075203, 0.86562441, 0.85474408, 0.86261383,\n",
       "        0.86618339, 0.86598973, 0.86517546, 0.86692723, 0.86657512,\n",
       "        0.86628462, 0.84440953, 0.86372299, 0.84017975, 0.86085766,\n",
       "        0.86205045, 0.86242897, 0.86668075, 0.86352493, 0.86619659,\n",
       "        0.8668392 , 0.86687882, 0.86684801, 0.85924234, 0.85378457,\n",
       "        0.8424817 , 0.86219129, 0.85300992, 0.85862174, 0.86141224,\n",
       "        0.85900466, 0.86262704, 0.86201084, 0.86292633, 0.85857332,\n",
       "        0.87969137, 0.86948886, 0.87872746, 0.88751711, 0.8794713 ,\n",
       "        0.88415442, 0.88752151, 0.89029001, 0.88863067, 0.88823014,\n",
       "        0.89058491, 0.89223984, 0.87402673, 0.87690087, 0.87326969,\n",
       "        0.88969582, 0.88385952, 0.88291322, 0.88755672, 0.88881113,\n",
       "        0.89095022, 0.88802768, 0.89182171, 0.88917645, 0.87788679,\n",
       "        0.87271071, 0.86638145, 0.88479703, 0.87833133, 0.88488946,\n",
       "        0.88517995, 0.88572133, 0.88704616, 0.88713859, 0.89014916,\n",
       "        0.89053209, 0.87365701, 0.87829172, 0.87223095, 0.87845017,\n",
       "        0.88437449, 0.88674686, 0.88642116, 0.88729264, 0.8828736 ,\n",
       "        0.89157523, 0.88959899, 0.8862627 ]),\n",
       " 'mean_test_roc_auc_ovo': array([0.90678128, 0.90494676, 0.90456824, 0.91071528, 0.90850928,\n",
       "        0.91401459, 0.91034908, 0.91018094, 0.9101871 , 0.90864925,\n",
       "        0.90898023, 0.90670646, 0.90069675, 0.91020999, 0.90694853,\n",
       "        0.90834555, 0.9072091 , 0.90893358, 0.90769766, 0.90406472,\n",
       "        0.90811051, 0.90556737, 0.90375838, 0.90139569, 0.9003015 ,\n",
       "        0.907901  , 0.90727864, 0.91306124, 0.91075313, 0.91005858,\n",
       "        0.90648374, 0.9096871 , 0.90915365, 0.90976721, 0.90447581,\n",
       "        0.90368796, 0.89935783, 0.9089459 , 0.90398813, 0.91092038,\n",
       "        0.91419064, 0.91202338, 0.91439927, 0.9099371 , 0.91350578,\n",
       "        0.90918534, 0.90830593, 0.90557353, 0.91147056, 0.90722318,\n",
       "        0.90780065, 0.90786931, 0.90978657, 0.90890101, 0.90158671,\n",
       "        0.90346965, 0.9060524 , 0.90559465, 0.89949692, 0.90447581,\n",
       "        0.90734026, 0.90541508, 0.9078253 , 0.90486666, 0.90919062,\n",
       "        0.90684906, 0.91098024, 0.90768885, 0.90333496, 0.90513603,\n",
       "        0.90674079, 0.90230415, 0.90793181, 0.90282352, 0.90811491,\n",
       "        0.90880594, 0.90635258, 0.90814396, 0.90537458, 0.90605152,\n",
       "        0.90386841, 0.90202774, 0.90452335, 0.90420644, 0.90796879,\n",
       "        0.90657441, 0.90582089, 0.91038693, 0.90608497, 0.91156211,\n",
       "        0.90274341, 0.90808322, 0.90909995, 0.90412898, 0.90382528,\n",
       "        0.90764748, 0.83224926, 0.44132325, 0.83222285, 0.83236634,\n",
       "        0.51644153, 0.83290507, 0.8309394 , 0.4727512 , 0.83104239,\n",
       "        0.83226863, 0.49477507, 0.83086809, 0.83266036, 0.51952517,\n",
       "        0.83012425, 0.82986985, 0.4594272 , 0.83061457, 0.83034168,\n",
       "        0.48466587, 0.83237778, 0.83375631, 0.41746839, 0.83160225,\n",
       "        0.83129591, 0.60373593, 0.83325543, 0.83357673, 0.54073477,\n",
       "        0.83054679, 0.83221845, 0.46460328, 0.83166211, 0.83310578,\n",
       "        0.51087373, 0.8322475 , 0.82829942, 0.46633832, 0.83149838,\n",
       "        0.8326269 , 0.49748722, 0.83107144, 0.83107848, 0.51467304,\n",
       "        0.83389187, 0.83103007, 0.52579633, 0.8315512 , 0.85377665,\n",
       "        0.47068517, 0.85952579, 0.85811821, 0.54396366, 0.86158214,\n",
       "        0.86392106, 0.51328571, 0.86193689, 0.86834977, 0.61355024,\n",
       "        0.86975295, 0.86072034, 0.47155401, 0.85552489, 0.8635126 ,\n",
       "        0.52246797, 0.86448004, 0.86778199, 0.55839418, 0.86882425,\n",
       "        0.86547036, 0.58000431, 0.86582335, 0.86189992, 0.47632692,\n",
       "        0.85604074, 0.86674501, 0.4867451 , 0.86740347, 0.86537881,\n",
       "        0.55821725, 0.86505222, 0.86411384, 0.58788639, 0.86338936,\n",
       "        0.85840255, 0.56985814, 0.85723881, 0.86834537, 0.53102434,\n",
       "        0.86512   , 0.86454694, 0.54788621, 0.86722565, 0.86469659,\n",
       "        0.58507652, 0.8650443 , 0.87346687, 0.87845985, 0.8707732 ,\n",
       "        0.88722838, 0.88599158, 0.88790444, 0.88964476, 0.88937451,\n",
       "        0.88907522, 0.89004441, 0.89102593, 0.88877944, 0.87476353,\n",
       "        0.87642639, 0.87122479, 0.88805937, 0.88421956, 0.88784018,\n",
       "        0.8890268 , 0.89029881, 0.88989036, 0.89062716, 0.89052505,\n",
       "        0.88996695, 0.86955048, 0.88057078, 0.87450209, 0.88517467,\n",
       "        0.88636834, 0.88808137, 0.88881905, 0.88955409, 0.88756905,\n",
       "        0.89022751, 0.89075656, 0.88944406, 0.87439293, 0.87307955,\n",
       "        0.86714026, 0.88275829, 0.88073539, 0.88356551, 0.88608048,\n",
       "        0.88651799, 0.88589827, 0.88706376, 0.88736042, 0.88429791,\n",
       "        0.89835078, 0.89700571, 0.89718265, 0.90273461, 0.90373285,\n",
       "        0.90433673, 0.90605504, 0.90746614, 0.90559201, 0.90649607,\n",
       "        0.90726104, 0.90835435, 0.89788423, 0.89320552, 0.89632085,\n",
       "        0.90335345, 0.90392651, 0.90465979, 0.90513691, 0.90585346,\n",
       "        0.90855594, 0.90820998, 0.90768357, 0.90824784, 0.89536045,\n",
       "        0.89479795, 0.89646257, 0.90343267, 0.89984287, 0.90478743,\n",
       "        0.90590011, 0.90624255, 0.90704977, 0.9087047 , 0.90702424,\n",
       "        0.90761491, 0.89707789, 0.89495024, 0.89623106, 0.90206295,\n",
       "        0.90396701, 0.90324077, 0.90538955, 0.90522934, 0.90554712,\n",
       "        0.90682705, 0.90623022, 0.90604888]),\n",
       " 'std_test_roc_auc_ovo': array([0.01076989, 0.00948039, 0.01087262, 0.00825857, 0.01170465,\n",
       "        0.01016922, 0.00804212, 0.01418475, 0.0098213 , 0.01068983,\n",
       "        0.00647723, 0.01307473, 0.01053374, 0.01082969, 0.00974466,\n",
       "        0.01352107, 0.01587674, 0.00999733, 0.01547138, 0.0096666 ,\n",
       "        0.00863754, 0.01587552, 0.01407537, 0.0141834 , 0.01640428,\n",
       "        0.01128174, 0.0089883 , 0.01027057, 0.01285211, 0.01176599,\n",
       "        0.01037734, 0.01118653, 0.01517185, 0.01189788, 0.00927956,\n",
       "        0.01697853, 0.01475033, 0.01269842, 0.01333161, 0.01055538,\n",
       "        0.0095695 , 0.01087634, 0.01073773, 0.01074337, 0.00872231,\n",
       "        0.0135583 , 0.00944868, 0.00895172, 0.01111576, 0.01398019,\n",
       "        0.0126096 , 0.01443566, 0.01303064, 0.00905008, 0.01465583,\n",
       "        0.01216463, 0.01227065, 0.01012793, 0.01104991, 0.01217852,\n",
       "        0.00988947, 0.01097921, 0.0129062 , 0.00822069, 0.01033965,\n",
       "        0.00819262, 0.01246148, 0.01053918, 0.00951561, 0.00949156,\n",
       "        0.01209086, 0.00956663, 0.01046881, 0.01391468, 0.0105441 ,\n",
       "        0.01341091, 0.00565579, 0.00732227, 0.00871943, 0.00853263,\n",
       "        0.00832093, 0.01312198, 0.00577295, 0.01074146, 0.01059114,\n",
       "        0.0123474 , 0.0123078 , 0.01260951, 0.01013373, 0.00918661,\n",
       "        0.01168485, 0.01337116, 0.01122574, 0.01428876, 0.01087685,\n",
       "        0.00987237, 0.01524925, 0.100189  , 0.0156848 , 0.01762045,\n",
       "        0.18469567, 0.01597933, 0.01685813, 0.07386217, 0.01842609,\n",
       "        0.01764189, 0.13502595, 0.01696811, 0.0149417 , 0.10017163,\n",
       "        0.0152561 , 0.01413754, 0.08394879, 0.01892352, 0.01697572,\n",
       "        0.19293248, 0.01656491, 0.01742714, 0.09568794, 0.01735065,\n",
       "        0.01481241, 0.09935095, 0.01453541, 0.01487575, 0.12275672,\n",
       "        0.01767588, 0.01504207, 0.12254502, 0.01673921, 0.01788198,\n",
       "        0.13682095, 0.01703296, 0.0209958 , 0.15047196, 0.01632127,\n",
       "        0.01683443, 0.10137305, 0.01622251, 0.01618195, 0.15096849,\n",
       "        0.01727842, 0.01781826, 0.16894633, 0.01807982, 0.01801735,\n",
       "        0.06617725, 0.01594511, 0.01694741, 0.1849965 , 0.01687754,\n",
       "        0.01895549, 0.14688202, 0.01475223, 0.01638485, 0.037102  ,\n",
       "        0.01306837, 0.01575113, 0.10056796, 0.01802002, 0.01641627,\n",
       "        0.11326653, 0.01110317, 0.01373169, 0.14194421, 0.01410463,\n",
       "        0.01374579, 0.08006221, 0.01325147, 0.0144213 , 0.14786044,\n",
       "        0.0128915 , 0.01904139, 0.10731834, 0.01594284, 0.01475734,\n",
       "        0.20036573, 0.0160422 , 0.01445269, 0.107249  , 0.01374735,\n",
       "        0.01678572, 0.12587935, 0.02117676, 0.01827718, 0.11403534,\n",
       "        0.01228296, 0.01687465, 0.15117376, 0.01559398, 0.01832562,\n",
       "        0.10921801, 0.01326183, 0.01775389, 0.01871996, 0.01921928,\n",
       "        0.01330329, 0.01548715, 0.01503006, 0.01370795, 0.01393921,\n",
       "        0.01281018, 0.01348752, 0.0138907 , 0.0134914 , 0.02018996,\n",
       "        0.01611458, 0.01295264, 0.01305998, 0.01599275, 0.01442789,\n",
       "        0.01274482, 0.01370528, 0.01408253, 0.01312537, 0.01329627,\n",
       "        0.01329647, 0.01565093, 0.01255694, 0.01935167, 0.01451911,\n",
       "        0.01452006, 0.01453972, 0.01276982, 0.01427028, 0.0120523 ,\n",
       "        0.0132132 , 0.01349745, 0.01276309, 0.01129214, 0.01175795,\n",
       "        0.01496082, 0.01075871, 0.01540931, 0.01501591, 0.01388892,\n",
       "        0.01523198, 0.01362427, 0.01435655, 0.01399448, 0.01517719,\n",
       "        0.01209875, 0.0156399 , 0.01251856, 0.01109156, 0.01406986,\n",
       "        0.01325679, 0.01325222, 0.01243903, 0.01241681, 0.0128246 ,\n",
       "        0.01273706, 0.01176587, 0.01430741, 0.01290484, 0.01252116,\n",
       "        0.0110198 , 0.01340703, 0.0127712 , 0.01378563, 0.0123517 ,\n",
       "        0.01148663, 0.01238452, 0.0114828 , 0.01200731, 0.01093787,\n",
       "        0.01437608, 0.01727401, 0.01216028, 0.01270404, 0.01311851,\n",
       "        0.01425295, 0.01227883, 0.01240887, 0.01285698, 0.01242442,\n",
       "        0.01101691, 0.01449339, 0.01213518, 0.01404843, 0.01434375,\n",
       "        0.01249598, 0.01077533, 0.01297701, 0.01182585, 0.01371598,\n",
       "        0.0124625 , 0.01220011, 0.01294548]),\n",
       " 'rank_test_roc_auc_ovo': array([ 67,  95,  99,  12,  35,   3,  14,  17,  16,  33,  27,  69, 128,\n",
       "         15,  64,  37,  61,  29,  51, 106,  43,  87, 112, 127, 129,  47,\n",
       "         58,   5,  11,  18,  72,  22,  25,  21, 101, 114, 132,  28, 107,\n",
       "         10,   2,   6,   1,  19,   4,  24,  38,  86,   8,  60,  50,  48,\n",
       "         20,  30, 126, 115,  78,  84, 131, 102,  57,  89,  49,  96,  23,\n",
       "         65,   9,  52, 118,  94,  68, 123,  46, 120,  42,  31,  73,  41,\n",
       "         91,  79, 110, 125, 100, 104,  45,  70,  83,  13,  76,   7, 121,\n",
       "         44,  26, 105, 111,  54, 236, 287, 238, 234, 272, 230, 249, 281,\n",
       "        247, 235, 277, 250, 231, 271, 254, 255, 286, 251, 253, 279, 233,\n",
       "        226, 288, 241, 244, 258, 228, 227, 267, 252, 239, 285, 240, 229,\n",
       "        275, 237, 256, 284, 243, 232, 276, 246, 245, 273, 225, 248, 269,\n",
       "        242, 224, 283, 218, 220, 266, 216, 211, 274, 214, 194, 257, 191,\n",
       "        217, 282, 223, 212, 270, 209, 196, 263, 193, 202, 261, 201, 215,\n",
       "        280, 222, 200, 278, 197, 203, 264, 205, 210, 259, 213, 219, 262,\n",
       "        221, 195, 268, 204, 208, 265, 198, 207, 260, 206, 187, 182, 190,\n",
       "        168, 173, 164, 154, 157, 158, 151, 145, 161, 184, 183, 189, 163,\n",
       "        177, 165, 159, 149, 153, 147, 148, 152, 192, 181, 185, 175, 171,\n",
       "        162, 160, 155, 166, 150, 146, 156, 186, 188, 199, 179, 180, 178,\n",
       "        172, 170, 174, 169, 167, 176, 133, 137, 135, 122, 113, 103,  77,\n",
       "         56,  85,  71,  59,  36, 134, 144, 139, 117, 109,  98,  93,  82,\n",
       "         34,  40,  53,  39, 141, 143, 138, 116, 130,  97,  81,  74,  62,\n",
       "         32,  63,  55, 136, 142, 140, 124, 108, 119,  90,  92,  88,  66,\n",
       "         75,  80]),\n",
       " 'split0_test_jaccard': array([0.61458333, 0.60769231, 0.60209424, 0.60769231, 0.62337662,\n",
       "        0.63224181, 0.62796834, 0.61772152, 0.62210797, 0.63265306,\n",
       "        0.64415584, 0.62857143, 0.60769231, 0.64507772, 0.60416667,\n",
       "        0.63520408, 0.62307692, 0.61340206, 0.65025907, 0.5974359 ,\n",
       "        0.62982005, 0.62656642, 0.65306122, 0.61928934, 0.62467192,\n",
       "        0.61282051, 0.62597403, 0.6185567 , 0.61323155, 0.62724936,\n",
       "        0.6092545 , 0.63541667, 0.65608466, 0.61892583, 0.60545906,\n",
       "        0.65128205, 0.62565445, 0.63421053, 0.6351706 , 0.61038961,\n",
       "        0.62005277, 0.62204724, 0.64155844, 0.63212435, 0.62373737,\n",
       "        0.64082687, 0.6185567 , 0.62176166, 0.61879896, 0.62532982,\n",
       "        0.61518325, 0.66492147, 0.62402089, 0.63239075, 0.62564103,\n",
       "        0.62155388, 0.61989796, 0.64285714, 0.63520408, 0.62210797,\n",
       "        0.62857143, 0.640625  , 0.62962963, 0.64379947, 0.62148338,\n",
       "        0.62694301, 0.65364583, 0.63565891, 0.62148338, 0.61917098,\n",
       "        0.63846154, 0.62628866, 0.6373057 , 0.6328125 , 0.61458333,\n",
       "        0.62216625, 0.640625  , 0.63707572, 0.62239583, 0.63846154,\n",
       "        0.62051282, 0.59541985, 0.6125    , 0.63010204, 0.62140992,\n",
       "        0.62565445, 0.61458333, 0.63265306, 0.61597938, 0.63824289,\n",
       "        0.62915601, 0.62148338, 0.61734694, 0.62182741, 0.61082474,\n",
       "        0.63989637, 0.47432763, 0.349     , 0.455     , 0.49033816,\n",
       "        0.39235412, 0.48668281, 0.48536585, 0.22463768, 0.48175182,\n",
       "        0.48780488, 0.        , 0.47804878, 0.47869674, 0.05965909,\n",
       "        0.46666667, 0.4754902 , 0.        , 0.48292683, 0.48157248,\n",
       "        0.38470452, 0.48292683, 0.48550725, 0.        , 0.48410758,\n",
       "        0.48284314, 0.349     , 0.48899756, 0.47665848, 0.3121853 ,\n",
       "        0.47445255, 0.47815534, 0.34205231, 0.47804878, 0.48536585,\n",
       "        0.10840708, 0.48300971, 0.46482412, 0.18441558, 0.47188264,\n",
       "        0.47677262, 0.349     , 0.48536585, 0.48184019, 0.349     ,\n",
       "        0.48780488, 0.48067633, 0.        , 0.486618  , 0.49881235,\n",
       "        0.06914894, 0.51732102, 0.52345679, 0.26606539, 0.50366748,\n",
       "        0.54094293, 0.31236897, 0.53      , 0.53694581, 0.22554348,\n",
       "        0.50985222, 0.54271357, 0.05128205, 0.5       , 0.51658768,\n",
       "        0.29087049, 0.5195122 , 0.50614251, 0.39247312, 0.5207824 ,\n",
       "        0.52853598, 0.21225071, 0.50868486, 0.53282828, 0.19064748,\n",
       "        0.4877451 , 0.52839506, 0.25259516, 0.54114713, 0.51842752,\n",
       "        0.02981651, 0.50121655, 0.50242718, 0.27245053, 0.5157385 ,\n",
       "        0.50228311, 0.03030303, 0.52816901, 0.53015075, 0.3739726 ,\n",
       "        0.5270936 , 0.50611247, 0.4031746 , 0.52068127, 0.51111111,\n",
       "        0.15309446, 0.50970874, 0.56675063, 0.54822335, 0.51358025,\n",
       "        0.57731959, 0.57106599, 0.58717949, 0.59020619, 0.59431525,\n",
       "        0.58290155, 0.57474227, 0.59431525, 0.57474227, 0.55189873,\n",
       "        0.54452926, 0.5175    , 0.58205128, 0.56330749, 0.59020619,\n",
       "        0.59125964, 0.58868895, 0.57881137, 0.58762887, 0.58868895,\n",
       "        0.58762887, 0.51492537, 0.54545455, 0.50731707, 0.57989691,\n",
       "        0.58823529, 0.58974359, 0.57989691, 0.58247423, 0.56589147,\n",
       "        0.59278351, 0.58354756, 0.58139535, 0.55472637, 0.55470738,\n",
       "        0.5315534 , 0.56958763, 0.55670103, 0.57881137, 0.58611825,\n",
       "        0.58354756, 0.578125  , 0.58762887, 0.58056266, 0.58097686,\n",
       "        0.61636829, 0.61696658, 0.60969388, 0.63496144, 0.61139896,\n",
       "        0.62760417, 0.6351706 , 0.63802083, 0.63968668, 0.63212435,\n",
       "        0.62729659, 0.64907652, 0.609375  , 0.60519481, 0.6092545 ,\n",
       "        0.63707572, 0.64229765, 0.65274151, 0.63089005, 0.64935065,\n",
       "        0.63779528, 0.6328125 , 0.65789474, 0.63756614, 0.62113402,\n",
       "        0.61658031, 0.61597938, 0.62724936, 0.63116883, 0.62435233,\n",
       "        0.62827225, 0.62597403, 0.61780105, 0.64136126, 0.63185379,\n",
       "        0.64751958, 0.60677083, 0.57474227, 0.58629442, 0.62467866,\n",
       "        0.63541667, 0.61498708, 0.62631579, 0.63565891, 0.64490862,\n",
       "        0.63049096, 0.63989637, 0.63350785]),\n",
       " 'split1_test_jaccard': array([0.59950249, 0.59753086, 0.59296482, 0.63054187, 0.63390663,\n",
       "        0.63771712, 0.62376238, 0.61369193, 0.62990196, 0.65174129,\n",
       "        0.61097257, 0.63325183, 0.60099751, 0.62406015, 0.60148515,\n",
       "        0.64427861, 0.61330049, 0.60740741, 0.61463415, 0.6195122 ,\n",
       "        0.665     , 0.61764706, 0.62836186, 0.64303797, 0.6044226 ,\n",
       "        0.61      , 0.60891089, 0.62376238, 0.59753086, 0.63567839,\n",
       "        0.62779156, 0.62254902, 0.59068627, 0.65087282, 0.6125908 ,\n",
       "        0.60620525, 0.62935323, 0.64      , 0.61179361, 0.64676617,\n",
       "        0.64427861, 0.63027295, 0.63300493, 0.62623762, 0.6345679 ,\n",
       "        0.66246851, 0.62836186, 0.62807882, 0.60487805, 0.61055276,\n",
       "        0.60349127, 0.61290323, 0.61633663, 0.6004902 , 0.60945274,\n",
       "        0.63681592, 0.59808612, 0.6426799 , 0.60294118, 0.61390887,\n",
       "        0.59950249, 0.60552764, 0.59899749, 0.62      , 0.6372549 ,\n",
       "        0.62593516, 0.61858191, 0.66666667, 0.63546798, 0.64356436,\n",
       "        0.62836186, 0.64303178, 0.599022  , 0.59759036, 0.61691542,\n",
       "        0.62623762, 0.62561576, 0.65413534, 0.61425061, 0.62682927,\n",
       "        0.60391198, 0.59615385, 0.60487805, 0.62962963, 0.60246914,\n",
       "        0.61538462, 0.61654135, 0.6275    , 0.62531017, 0.6372796 ,\n",
       "        0.62068966, 0.64483627, 0.6460396 , 0.62899263, 0.62807882,\n",
       "        0.61594203, 0.47552448, 0.        , 0.47663551, 0.47306792,\n",
       "        0.349     , 0.47016706, 0.47641509, 0.34507042, 0.47641509,\n",
       "        0.46875   , 0.        , 0.48095238, 0.46478873, 0.26918392,\n",
       "        0.45563549, 0.46838407, 0.        , 0.48687351, 0.47721823,\n",
       "        0.        , 0.49043062, 0.48463357, 0.        , 0.48815166,\n",
       "        0.46244131, 0.349     , 0.46793349, 0.48578199, 0.00286533,\n",
       "        0.48104265, 0.46919431, 0.        , 0.47764706, 0.49052133,\n",
       "        0.344     , 0.492891  , 0.42751843, 0.01142857, 0.48591549,\n",
       "        0.47044917, 0.1638796 , 0.47142857, 0.48687351, 0.07122507,\n",
       "        0.48095238, 0.48210024, 0.349     , 0.48693587, 0.51345291,\n",
       "        0.19349005, 0.55147059, 0.55797101, 0.16258352, 0.55231144,\n",
       "        0.53995157, 0.35877863, 0.51288056, 0.54176611, 0.26929982,\n",
       "        0.55147059, 0.5162037 , 0.27384615, 0.52008929, 0.55528846,\n",
       "        0.28955533, 0.54676259, 0.54086538, 0.19141914, 0.5470297 ,\n",
       "        0.52102804, 0.29285714, 0.53012048, 0.53381643, 0.31509625,\n",
       "        0.50356295, 0.50591017, 0.24084351, 0.55529412, 0.52870813,\n",
       "        0.4244898 , 0.54854369, 0.52163462, 0.29956897, 0.5202864 ,\n",
       "        0.52132701, 0.28312413, 0.52470588, 0.53588517, 0.20643939,\n",
       "        0.52567237, 0.53427896, 0.32678571, 0.52798054, 0.53791469,\n",
       "        0.35782748, 0.53206651, 0.56403941, 0.5721393 , 0.53883495,\n",
       "        0.58897243, 0.57462687, 0.58838384, 0.59102244, 0.59296482,\n",
       "        0.58897243, 0.59398496, 0.59801489, 0.60050251, 0.5496368 ,\n",
       "        0.55445545, 0.53012048, 0.58208955, 0.58646617, 0.57530864,\n",
       "        0.59398496, 0.59899749, 0.58646617, 0.59701493, 0.5914787 ,\n",
       "        0.59057072, 0.53026634, 0.55583127, 0.56823821, 0.57816377,\n",
       "        0.58208955, 0.58104738, 0.58542714, 0.59398496, 0.5875    ,\n",
       "        0.60201511, 0.59649123, 0.58291457, 0.56723716, 0.55693069,\n",
       "        0.54411765, 0.57788945, 0.57107232, 0.56218905, 0.59296482,\n",
       "        0.58897243, 0.57393484, 0.58145363, 0.59296482, 0.56109726,\n",
       "        0.59158416, 0.60297767, 0.6080402 , 0.61124694, 0.62935323,\n",
       "        0.61670762, 0.63544304, 0.63027295, 0.61633663, 0.61290323,\n",
       "        0.60987654, 0.62155388, 0.6119403 , 0.57804878, 0.61386139,\n",
       "        0.61029412, 0.62282878, 0.63432836, 0.61306533, 0.62842893,\n",
       "        0.62531017, 0.63104326, 0.62842893, 0.61386139, 0.57605985,\n",
       "        0.5768262 , 0.60050251, 0.62842893, 0.6019656 , 0.5990099 ,\n",
       "        0.62407862, 0.62189055, 0.60294118, 0.62162162, 0.61940299,\n",
       "        0.62653563, 0.59850374, 0.59      , 0.59558824, 0.60294118,\n",
       "        0.6010101 , 0.62034739, 0.60148515, 0.60794045, 0.60493827,\n",
       "        0.6       , 0.6175    , 0.60687961]),\n",
       " 'split2_test_jaccard': array([0.61964736, 0.61068702, 0.61167513, 0.66089109, 0.61633663,\n",
       "        0.66666667, 0.64615385, 0.67005076, 0.67839196, 0.64795918,\n",
       "        0.66323907, 0.6460396 , 0.61577608, 0.61265823, 0.5994898 ,\n",
       "        0.61964736, 0.63118812, 0.62716049, 0.65594059, 0.63613861,\n",
       "        0.63840399, 0.6509901 , 0.65422886, 0.63682864, 0.61323155,\n",
       "        0.63476071, 0.605     , 0.65174129, 0.6566416 , 0.66243655,\n",
       "        0.65336658, 0.64427861, 0.66582915, 0.66751269, 0.63950617,\n",
       "        0.67091837, 0.61940299, 0.61794872, 0.6125    , 0.65561224,\n",
       "        0.66253102, 0.65736041, 0.67088608, 0.63546798, 0.64516129,\n",
       "        0.655     , 0.64987406, 0.65063291, 0.60301508, 0.60913706,\n",
       "        0.60447761, 0.68181818, 0.65151515, 0.65139949, 0.65835411,\n",
       "        0.61964736, 0.65151515, 0.64467005, 0.64285714, 0.64810127,\n",
       "        0.59898477, 0.62562814, 0.61518987, 0.60098522, 0.62282878,\n",
       "        0.62034739, 0.64213198, 0.64411028, 0.62254902, 0.64987406,\n",
       "        0.62623762, 0.65835411, 0.60913706, 0.61989796, 0.61055276,\n",
       "        0.65743073, 0.61083744, 0.63682864, 0.65586035, 0.63681592,\n",
       "        0.62626263, 0.65316456, 0.62935323, 0.70360825, 0.63471503,\n",
       "        0.62626263, 0.60759494, 0.65491184, 0.6372796 , 0.61809045,\n",
       "        0.62871287, 0.65306122, 0.65989848, 0.665     , 0.63316583,\n",
       "        0.65413534, 0.4569378 , 0.03399433, 0.46335697, 0.42195122,\n",
       "        0.19262295, 0.47016706, 0.47030879, 0.32763819, 0.4547619 ,\n",
       "        0.46300716, 0.        , 0.47607656, 0.45783133, 0.30272109,\n",
       "        0.4195122 , 0.42542787, 0.25290323, 0.45368171, 0.47002398,\n",
       "        0.348     , 0.46300716, 0.46539379, 0.06163022, 0.47016706,\n",
       "        0.45542169, 0.32359081, 0.43301435, 0.47743468, 0.349     ,\n",
       "        0.45823389, 0.46666667, 0.        , 0.46539379, 0.46428571,\n",
       "        0.        , 0.46650718, 0.45432692, 0.        , 0.39800995,\n",
       "        0.47002398, 0.        , 0.43990385, 0.45563549, 0.        ,\n",
       "        0.45563549, 0.46172249, 0.        , 0.47016706, 0.51173709,\n",
       "        0.1529052 , 0.48275862, 0.50469484, 0.15039578, 0.5496368 ,\n",
       "        0.51904762, 0.22309417, 0.49760766, 0.54411765, 0.26153846,\n",
       "        0.51789976, 0.51869159, 0.34143763, 0.50480769, 0.51807229,\n",
       "        0.21276596, 0.56038647, 0.52542373, 0.29873418, 0.51913876,\n",
       "        0.53623188, 0.35042735, 0.51904762, 0.53170732, 0.05698006,\n",
       "        0.50235849, 0.54830918, 0.25630252, 0.51904762, 0.53571429,\n",
       "        0.16019417, 0.52267303, 0.50956938, 0.14862385, 0.52038369,\n",
       "        0.52403846, 0.40345369, 0.52505967, 0.52631579, 0.21399177,\n",
       "        0.51690821, 0.53477218, 0.17583732, 0.53588517, 0.52870813,\n",
       "        0.28593272, 0.5215311 , 0.52415459, 0.55528256, 0.53960396,\n",
       "        0.54433498, 0.56965174, 0.55555556, 0.56575682, 0.56218905,\n",
       "        0.555     , 0.56218905, 0.575     , 0.55721393, 0.5461165 ,\n",
       "        0.55745721, 0.53640777, 0.55860349, 0.56      , 0.57816377,\n",
       "        0.56857855, 0.55223881, 0.575     , 0.55970149, 0.56467662,\n",
       "        0.56575682, 0.54878049, 0.54926108, 0.53940887, 0.55147059,\n",
       "        0.54299754, 0.5654321 , 0.575     , 0.56467662, 0.565     ,\n",
       "        0.55610973, 0.57960199, 0.56188119, 0.51213592, 0.53545232,\n",
       "        0.50851582, 0.54975124, 0.54433498, 0.54878049, 0.55172414,\n",
       "        0.55802469, 0.55147059, 0.54901961, 0.55445545, 0.55036855,\n",
       "        0.5839599 , 0.5925    , 0.56756757, 0.6       , 0.62216625,\n",
       "        0.61734694, 0.62849873, 0.61892583, 0.62060302, 0.63682864,\n",
       "        0.60406091, 0.62025316, 0.5839599 , 0.58690176, 0.58374384,\n",
       "        0.60659898, 0.61111111, 0.59898477, 0.61479592, 0.61167513,\n",
       "        0.61111111, 0.62944162, 0.62086514, 0.6112532 , 0.58734177,\n",
       "        0.57760814, 0.5839599 , 0.59240506, 0.61712846, 0.61518987,\n",
       "        0.61265823, 0.62406015, 0.6281407 , 0.62182741, 0.61323155,\n",
       "        0.61323155, 0.59296482, 0.58104738, 0.58955224, 0.60453401,\n",
       "        0.60459184, 0.59798995, 0.6142132 , 0.6       , 0.60201511,\n",
       "        0.60204082, 0.62278481, 0.61381074]),\n",
       " 'split3_test_jaccard': array([0.62436548, 0.635     , 0.6314433 , 0.65087282, 0.62531646,\n",
       "        0.63753213, 0.64231738, 0.65809769, 0.64415584, 0.66752577,\n",
       "        0.68146214, 0.68814433, 0.64122137, 0.65104167, 0.65482234,\n",
       "        0.67098446, 0.67098446, 0.65616798, 0.67088608, 0.65648855,\n",
       "        0.64925373, 0.66919192, 0.64795918, 0.65162907, 0.61167513,\n",
       "        0.6377551 , 0.65885417, 0.67357513, 0.67716535, 0.66925065,\n",
       "        0.62878788, 0.67098446, 0.6744186 , 0.64810127, 0.64556962,\n",
       "        0.64      , 0.62340967, 0.64341085, 0.63316583, 0.66579634,\n",
       "        0.66230366, 0.66315789, 0.67792208, 0.64631043, 0.65721649,\n",
       "        0.67875648, 0.69712794, 0.6640625 , 0.63589744, 0.63636364,\n",
       "        0.64948454, 0.64781491, 0.66923077, 0.63316583, 0.62878788,\n",
       "        0.66923077, 0.64925373, 0.67430025, 0.66496164, 0.66075949,\n",
       "        0.61439589, 0.65087282, 0.64082687, 0.65641026, 0.67095116,\n",
       "        0.64285714, 0.64160401, 0.63979849, 0.63591022, 0.65656566,\n",
       "        0.67848101, 0.64974619, 0.64358974, 0.61538462, 0.63451777,\n",
       "        0.64871795, 0.62468514, 0.64030612, 0.62626263, 0.64483627,\n",
       "        0.64912281, 0.6540404 , 0.65743073, 0.59610706, 0.63383838,\n",
       "        0.62086514, 0.6443299 , 0.68157895, 0.66323907, 0.66149871,\n",
       "        0.66834171, 0.6437659 , 0.6529563 , 0.65743073, 0.66919192,\n",
       "        0.65903308, 0.49127182, 0.349     , 0.48402948, 0.5012285 ,\n",
       "        0.03370787, 0.5037037 , 0.50614251, 0.        , 0.48184019,\n",
       "        0.50122249, 0.00857143, 0.49514563, 0.4877451 , 0.37427578,\n",
       "        0.48267327, 0.46683047, 0.01428571, 0.49754902, 0.49382716,\n",
       "        0.04958678, 0.50245098, 0.49754902, 0.00286533, 0.49385749,\n",
       "        0.47630923, 0.01146132, 0.47901235, 0.51231527, 0.33757962,\n",
       "        0.48529412, 0.49019608, 0.349     , 0.50864198, 0.50611247,\n",
       "        0.        , 0.49388753, 0.48762376, 0.        , 0.48768473,\n",
       "        0.4877451 , 0.349     , 0.49509804, 0.49385749, 0.12299465,\n",
       "        0.4963145 , 0.49633252, 0.        , 0.49753695, 0.54926108,\n",
       "        0.32309443, 0.51629073, 0.53300733, 0.49097473, 0.59079903,\n",
       "        0.58024691, 0.13678373, 0.56416465, 0.60858586, 0.25303644,\n",
       "        0.59296482, 0.56327543, 0.18130312, 0.55223881, 0.52058111,\n",
       "        0.36305732, 0.53882353, 0.56852792, 0.10944206, 0.61029412,\n",
       "        0.55665025, 0.22988506, 0.57142857, 0.53545232, 0.3783032 ,\n",
       "        0.50746269, 0.5990099 , 0.28330206, 0.56327543, 0.57283951,\n",
       "        0.39489672, 0.55970149, 0.56930693, 0.14058355, 0.56218905,\n",
       "        0.5275    , 0.32186235, 0.59903382, 0.59305211, 0.29153605,\n",
       "        0.52227723, 0.58145363, 0.26315789, 0.58104738, 0.59653465,\n",
       "        0.44937833, 0.55802469, 0.61209068, 0.6259542 , 0.59898477,\n",
       "        0.62060302, 0.6142132 , 0.60406091, 0.61403509, 0.62531646,\n",
       "        0.62182741, 0.61772152, 0.62626263, 0.62278481, 0.5974359 ,\n",
       "        0.6       , 0.58121827, 0.59183673, 0.61518987, 0.61712846,\n",
       "        0.61479592, 0.61675127, 0.6259542 , 0.61868687, 0.62531646,\n",
       "        0.61675127, 0.52736318, 0.59278351, 0.57908163, 0.60613811,\n",
       "        0.60814249, 0.61012658, 0.61772152, 0.61772152, 0.6142132 ,\n",
       "        0.62121212, 0.61964736, 0.62468514, 0.55443038, 0.5175    ,\n",
       "        0.53865337, 0.5361596 , 0.5974359 , 0.6       , 0.6112532 ,\n",
       "        0.61167513, 0.61577608, 0.60759494, 0.61323155, 0.60714286,\n",
       "        0.61964736, 0.65648855, 0.60598504, 0.63402062, 0.64285714,\n",
       "        0.61928934, 0.6484375 , 0.66062176, 0.66410256, 0.66754617,\n",
       "        0.66149871, 0.65374677, 0.62790698, 0.62562814, 0.62944162,\n",
       "        0.63451777, 0.64285714, 0.63682864, 0.65535248, 0.63824289,\n",
       "        0.6503856 , 0.63846154, 0.64705882, 0.65625   , 0.62216625,\n",
       "        0.61654135, 0.66153846, 0.63959391, 0.62060302, 0.65974026,\n",
       "        0.65473146, 0.63896104, 0.66402116, 0.65463918, 0.66666667,\n",
       "        0.64705882, 0.61832061, 0.62182741, 0.62915601, 0.62210797,\n",
       "        0.63076923, 0.63265306, 0.65274151, 0.65885417, 0.671875  ,\n",
       "        0.65552699, 0.65374677, 0.64524422]),\n",
       " 'split4_test_jaccard': array([0.61016949, 0.6225    , 0.60591133, 0.62376238, 0.62935323,\n",
       "        0.63010204, 0.63390663, 0.62168675, 0.64320388, 0.63922518,\n",
       "        0.63703704, 0.60386473, 0.60487805, 0.57178218, 0.60827251,\n",
       "        0.61219512, 0.58980583, 0.63950617, 0.60671463, 0.62745098,\n",
       "        0.60663507, 0.61800487, 0.59241706, 0.5861244 , 0.58595642,\n",
       "        0.57729469, 0.60096154, 0.60827251, 0.64631043, 0.60539216,\n",
       "        0.61985472, 0.64444444, 0.62716049, 0.62068966, 0.61165049,\n",
       "        0.59855769, 0.57420925, 0.59322034, 0.58435208, 0.60731707,\n",
       "        0.61111111, 0.62094763, 0.62254902, 0.62842893, 0.60344828,\n",
       "        0.62168675, 0.62287105, 0.6195122 , 0.59903382, 0.60957179,\n",
       "        0.60987654, 0.62593516, 0.60340633, 0.6107056 , 0.60837438,\n",
       "        0.60047281, 0.60663507, 0.62068966, 0.58865248, 0.61707317,\n",
       "        0.61346633, 0.60545906, 0.60731707, 0.59661836, 0.61097257,\n",
       "        0.61870504, 0.5952381 , 0.58472554, 0.60340633, 0.6460396 ,\n",
       "        0.6097561 , 0.61800487, 0.60294118, 0.61219512, 0.59951456,\n",
       "        0.60880196, 0.65024631, 0.61800487, 0.62168675, 0.61352657,\n",
       "        0.62711864, 0.61029412, 0.64963504, 0.59708738, 0.59124088,\n",
       "        0.60545906, 0.61290323, 0.61209068, 0.6182266 , 0.61576355,\n",
       "        0.62591687, 0.58894231, 0.60628019, 0.625     , 0.6047619 ,\n",
       "        0.61764706, 0.42326733, 0.0940367 , 0.46699267, 0.45476773,\n",
       "        0.        , 0.46715328, 0.45169082, 0.32414698, 0.43765281,\n",
       "        0.46246973, 0.        , 0.46472019, 0.42997543, 0.31663327,\n",
       "        0.45520581, 0.47228916, 0.        , 0.45012165, 0.45520581,\n",
       "        0.        , 0.47188264, 0.45985401, 0.06145251, 0.46359223,\n",
       "        0.44197531, 0.        , 0.45343137, 0.45742092, 0.23222749,\n",
       "        0.46489104, 0.46731235, 0.        , 0.46568627, 0.4622871 ,\n",
       "        0.28438662, 0.46004843, 0.38324873, 0.        , 0.45742092,\n",
       "        0.45853659, 0.32182628, 0.45542169, 0.45255474, 0.06884058,\n",
       "        0.46004843, 0.45631068, 0.        , 0.45873786, 0.50843373,\n",
       "        0.        , 0.50229358, 0.48809524, 0.1013986 , 0.48931116,\n",
       "        0.49282297, 0.39721793, 0.50485437, 0.50239234, 0.09428571,\n",
       "        0.51421801, 0.5       , 0.26323988, 0.51058824, 0.49526066,\n",
       "        0.24581006, 0.49878345, 0.50717703, 0.24390244, 0.5       ,\n",
       "        0.52392344, 0.33333333, 0.49160671, 0.49764151, 0.14321926,\n",
       "        0.53051643, 0.50356295, 0.33773087, 0.5177305 , 0.49638554,\n",
       "        0.32480315, 0.5       , 0.51543943, 0.41477273, 0.4940048 ,\n",
       "        0.51480638, 0.2594235 , 0.49056604, 0.4822695 , 0.12331081,\n",
       "        0.51781473, 0.5       , 0.29268293, 0.49285714, 0.48564593,\n",
       "        0.26540284, 0.49521531, 0.50122249, 0.5       , 0.49759615,\n",
       "        0.54433498, 0.54299754, 0.52941176, 0.55036855, 0.54814815,\n",
       "        0.5591133 , 0.55147059, 0.56019656, 0.54926108, 0.48433735,\n",
       "        0.52184466, 0.5461165 , 0.56019656, 0.51477833, 0.53676471,\n",
       "        0.54950495, 0.54878049, 0.54814815, 0.55501222, 0.55172414,\n",
       "        0.55802469, 0.50240385, 0.55012225, 0.48192771, 0.54299754,\n",
       "        0.5436409 , 0.53562654, 0.56157635, 0.54299754, 0.5591133 ,\n",
       "        0.55445545, 0.5654321 , 0.55172414, 0.54744526, 0.51325301,\n",
       "        0.49880668, 0.53316953, 0.50855746, 0.52941176, 0.53562654,\n",
       "        0.5382716 , 0.54411765, 0.54950495, 0.54455446, 0.53333333,\n",
       "        0.55361596, 0.55172414, 0.53580247, 0.60651629, 0.56608479,\n",
       "        0.585     , 0.59600998, 0.5955335 , 0.5995086 , 0.59168704,\n",
       "        0.605     , 0.61403509, 0.56641604, 0.56857855, 0.56930693,\n",
       "        0.58968059, 0.58415842, 0.58852868, 0.60545906, 0.61346633,\n",
       "        0.61012658, 0.60696517, 0.60199005, 0.60447761, 0.57142857,\n",
       "        0.55583127, 0.55361596, 0.59445844, 0.58354115, 0.5920398 ,\n",
       "        0.58823529, 0.6075    , 0.60545906, 0.59653465, 0.60651629,\n",
       "        0.60349127, 0.555     , 0.57178218, 0.56327543, 0.57568238,\n",
       "        0.5875    , 0.59801489, 0.59012346, 0.6070529 , 0.6004902 ,\n",
       "        0.60493827, 0.60447761, 0.61557789]),\n",
       " 'mean_test_jaccard': array([0.61365363, 0.61468204, 0.60881776, 0.63475209, 0.62565792,\n",
       "        0.64085196, 0.63482171, 0.63624973, 0.64355232, 0.6478209 ,\n",
       "        0.64737333, 0.63997439, 0.61411306, 0.62092399, 0.61364729,\n",
       "        0.63646192, 0.62567116, 0.62872882, 0.6396869 , 0.62740525,\n",
       "        0.63782257, 0.63648007, 0.63520564, 0.62738189, 0.60799152,\n",
       "        0.6145262 , 0.61994012, 0.6351816 , 0.63817596, 0.64000142,\n",
       "        0.62781105, 0.64353464, 0.64283583, 0.64122045, 0.62295523,\n",
       "        0.63339267, 0.61440592, 0.62575809, 0.61539642, 0.63717629,\n",
       "        0.64005543, 0.63875723, 0.64918411, 0.63371386, 0.63282627,\n",
       "        0.65174772, 0.64335832, 0.63680962, 0.61232467, 0.61819101,\n",
       "        0.61650264, 0.64667859, 0.63290195, 0.62563037, 0.62612203,\n",
       "        0.62954415, 0.62507761, 0.6450394 , 0.6269233 , 0.63239015,\n",
       "        0.61098418, 0.62562253, 0.61839219, 0.62356266, 0.63269816,\n",
       "        0.62695755, 0.63024037, 0.63419198, 0.62376339, 0.64304293,\n",
       "        0.63625963, 0.63908512, 0.61839914, 0.61557611, 0.61521677,\n",
       "        0.6326709 , 0.63040193, 0.63727014, 0.62809123, 0.63209391,\n",
       "        0.62538578, 0.62181455, 0.63075941, 0.63130687, 0.61673467,\n",
       "        0.61872518, 0.61919055, 0.64174691, 0.63200697, 0.63417504,\n",
       "        0.63456342, 0.63041782, 0.6365043 , 0.63965015, 0.62920464,\n",
       "        0.63733078, 0.46426581, 0.16520621, 0.46920293, 0.46827071,\n",
       "        0.19353699, 0.47957479, 0.47798461, 0.24429866, 0.46648437,\n",
       "        0.47665085, 0.00171429, 0.47898871, 0.46380747, 0.26449463,\n",
       "        0.45593869, 0.46168435, 0.05343779, 0.47423054, 0.47556953,\n",
       "        0.15645826, 0.48213965, 0.47858753, 0.02518961, 0.47997521,\n",
       "        0.46379813, 0.20661043, 0.46447782, 0.48192227, 0.24677155,\n",
       "        0.47278285, 0.47430495, 0.13821046, 0.47908358, 0.48171449,\n",
       "        0.14735874, 0.47926877, 0.44350839, 0.03916883, 0.46018275,\n",
       "        0.47270549, 0.23674118, 0.4694436 , 0.47415229, 0.12241206,\n",
       "        0.47615113, 0.47542845, 0.0698    , 0.47999915, 0.51633943,\n",
       "        0.14772772, 0.51402691, 0.52144504, 0.2342836 , 0.53714518,\n",
       "        0.5346024 , 0.28564869, 0.52190145, 0.54676155, 0.22074078,\n",
       "        0.53728108, 0.52817686, 0.22222177, 0.5175448 , 0.52115804,\n",
       "        0.28041183, 0.53285365, 0.52962731, 0.24719419, 0.53944899,\n",
       "        0.53327392, 0.28375072, 0.52417765, 0.52628917, 0.21684925,\n",
       "        0.50632913, 0.53703745, 0.27415482, 0.53929896, 0.530415  ,\n",
       "        0.26684007, 0.52642695, 0.52367551, 0.25519993, 0.52252049,\n",
       "        0.51799099, 0.25963334, 0.53350688, 0.53353466, 0.24185013,\n",
       "        0.52195323, 0.53132345, 0.29232769, 0.5316903 , 0.5319829 ,\n",
       "        0.30232717, 0.52330927, 0.55365156, 0.56031988, 0.53772002,\n",
       "        0.575113  , 0.57451107, 0.57291831, 0.58227782, 0.58458675,\n",
       "        0.58156294, 0.58002168, 0.59075786, 0.58090092, 0.54588506,\n",
       "        0.55565732, 0.54227261, 0.57495552, 0.56794837, 0.57951435,\n",
       "        0.5836248 , 0.5810914 , 0.58287598, 0.58360888, 0.58437697,\n",
       "        0.58374647, 0.52474785, 0.55869053, 0.5351947 , 0.57173338,\n",
       "        0.57302116, 0.57639524, 0.58392438, 0.58037097, 0.57834359,\n",
       "        0.58531518, 0.58894405, 0.58052008, 0.54719502, 0.53556868,\n",
       "        0.52432938, 0.55331149, 0.55562034, 0.56383854, 0.57553739,\n",
       "        0.57609828, 0.57268483, 0.5750404 , 0.57715379, 0.56658377,\n",
       "        0.59303513, 0.60413139, 0.58541783, 0.61734906, 0.61437208,\n",
       "        0.61318961, 0.62871197, 0.62867498, 0.6280475 , 0.62821789,\n",
       "        0.62154655, 0.63173308, 0.59991964, 0.59287041, 0.60112166,\n",
       "        0.61563344, 0.62065062, 0.62228239, 0.62391257, 0.62823279,\n",
       "        0.62694575, 0.62774482, 0.63124754, 0.62468167, 0.59562609,\n",
       "        0.58867745, 0.60311924, 0.61642714, 0.61088141, 0.61806643,\n",
       "        0.62159517, 0.62367715, 0.62367263, 0.62719682, 0.62753426,\n",
       "        0.62756737, 0.594312  , 0.58787985, 0.59277327, 0.60598884,\n",
       "        0.61185757, 0.61279847, 0.61697582, 0.62190128, 0.62484544,\n",
       "        0.61859941, 0.62768111, 0.62300406]),\n",
       " 'std_test_jaccard': array([0.00853144, 0.01290674, 0.01284561, 0.01904656, 0.00589752,\n",
       "        0.01324344, 0.00842279, 0.02316926, 0.01928978, 0.01189639,\n",
       "        0.02389969, 0.0276993 , 0.01439552, 0.02822606, 0.02079667,\n",
       "        0.02062084, 0.02656997, 0.01766936, 0.02475542, 0.01940399,\n",
       "        0.019526  , 0.02037107, 0.02332834, 0.02319461, 0.01278707,\n",
       "        0.02171911, 0.02124029, 0.02399655, 0.02898817, 0.02340067,\n",
       "        0.01457181, 0.01588189, 0.03055156, 0.01871021, 0.01628913,\n",
       "        0.02729284, 0.02035565, 0.01846937, 0.01839506, 0.02391676,\n",
       "        0.02124077, 0.01794357, 0.02156938, 0.00704417, 0.01841599,\n",
       "        0.01937995, 0.02895987, 0.01752525, 0.01353779, 0.01091618,\n",
       "        0.01701381, 0.02505973, 0.02403959, 0.01800728, 0.01810574,\n",
       "        0.02295578, 0.0218125 , 0.01708551, 0.02760135, 0.01862015,\n",
       "        0.01098114, 0.01829219, 0.01509081, 0.02339625, 0.02087694,\n",
       "        0.0085515 , 0.02087792, 0.02695734, 0.01187961, 0.01271731,\n",
       "        0.02303365, 0.01489064, 0.01839692, 0.01140812, 0.01135013,\n",
       "        0.01784387, 0.01368699, 0.01153287, 0.01441864, 0.0109332 ,\n",
       "        0.01450578, 0.02649227, 0.02036112, 0.03909413, 0.01726921,\n",
       "        0.00770139, 0.01291681, 0.02418707, 0.01729089, 0.01655576,\n",
       "        0.01715626, 0.02323286, 0.02092548, 0.01791437, 0.02258853,\n",
       "        0.01791587, 0.02320301, 0.15305885, 0.0101573 , 0.02801438,\n",
       "        0.15918527, 0.01388548, 0.01788907, 0.12926296, 0.01752821,\n",
       "        0.01534833, 0.00342857, 0.00977606, 0.01987956, 0.1078992 ,\n",
       "        0.02077184, 0.01837914, 0.09988607, 0.01888234, 0.01278694,\n",
       "        0.17272202, 0.01380702, 0.01392117, 0.02969957, 0.01132475,\n",
       "        0.01461151, 0.16431983, 0.01967936, 0.01781457, 0.12859043,\n",
       "        0.01001496, 0.00895223, 0.16928681, 0.01577169, 0.01653687,\n",
       "        0.14310196, 0.01375171, 0.03578959, 0.07275814, 0.03294998,\n",
       "        0.00954747, 0.13696624, 0.01992395, 0.01684372, 0.11984753,\n",
       "        0.01578361, 0.01457653, 0.1396    , 0.01377108, 0.01722235,\n",
       "        0.11033819, 0.02250947, 0.02393599, 0.13909487, 0.03653393,\n",
       "        0.02877398, 0.09442828, 0.02372312, 0.03440687, 0.06492979,\n",
       "        0.03150208, 0.02222831, 0.09945321, 0.01859353, 0.01931995,\n",
       "        0.05058955, 0.02156816, 0.0232928 , 0.09577022, 0.03845431,\n",
       "        0.01276748, 0.05477389, 0.02681483, 0.01437656, 0.11609127,\n",
       "        0.01382096, 0.03501936, 0.03469511, 0.01849025, 0.02503013,\n",
       "        0.14977662, 0.02426679, 0.02368417, 0.10220083, 0.0221011 ,\n",
       "        0.00888821, 0.12466033, 0.03553871, 0.03532572, 0.08486061,\n",
       "        0.00407283, 0.02881117, 0.07469324, 0.02862409, 0.0368662 ,\n",
       "        0.09859707, 0.02125679, 0.03824527, 0.04063927, 0.03449874,\n",
       "        0.02884742, 0.0228248 , 0.02685781, 0.02208618, 0.02703025,\n",
       "        0.02403121, 0.02356816, 0.02239737, 0.02734434, 0.03604415,\n",
       "        0.02544833, 0.02157449, 0.01320178, 0.03312533, 0.02598531,\n",
       "        0.02248473, 0.02655728, 0.02511253, 0.02373048, 0.02529577,\n",
       "        0.02066977, 0.01556019, 0.01736679, 0.0364698 , 0.02248558,\n",
       "        0.02573648, 0.0249796 , 0.01865371, 0.02540832, 0.02036359,\n",
       "        0.02618601, 0.01826943, 0.02503464, 0.01865456, 0.01814996,\n",
       "        0.01760918, 0.0177851 , 0.02943042, 0.02427552, 0.02775333,\n",
       "        0.02547336, 0.0251163 , 0.02275319, 0.0250645 , 0.02549834,\n",
       "        0.02404572, 0.03402691, 0.02934084, 0.01444792, 0.02621864,\n",
       "        0.01462395, 0.01758172, 0.02146578, 0.02210191, 0.02531861,\n",
       "        0.02165624, 0.01633414, 0.02188331, 0.02034876, 0.0216547 ,\n",
       "        0.01788893, 0.02185933, 0.02435795, 0.01776773, 0.01441117,\n",
       "        0.01551409, 0.01082665, 0.01965609, 0.01932964, 0.02187246,\n",
       "        0.02406998, 0.03578461, 0.01927458, 0.01656561, 0.02376986,\n",
       "        0.02164031, 0.01003904, 0.02210869, 0.01976763, 0.02127178,\n",
       "        0.01768754, 0.02143107, 0.01808505, 0.02123253, 0.01754436,\n",
       "        0.01831083, 0.01337032, 0.02161047, 0.02213519, 0.02872284,\n",
       "        0.02150259, 0.01728966, 0.01417475]),\n",
       " 'rank_test_jaccard': array([122, 117, 130,  36,  79,  14,  35,  32,   7,   3,   4,  17, 121,\n",
       "         98, 123,  30,  78,  58,  18,  70,  23,  29,  33,  71, 131, 118,\n",
       "        100,  34,  22,  16,  65,   8,  11,  13,  92,  41, 119,  77, 115,\n",
       "         26,  15,  21,   2,  40,  43,   1,   9,  27, 126, 106, 111,   5,\n",
       "         42,  80,  76,  56,  83,   6,  75,  46, 128,  81, 105,  90,  44,\n",
       "         73,  55,  38,  87,  10,  31,  20, 104, 114, 116,  45,  54,  25,\n",
       "         63,  47,  82,  95,  52,  50, 110, 102, 101,  12,  48,  39,  37,\n",
       "         53,  28,  19,  57,  24, 250, 278, 246, 247, 277, 230, 235, 269,\n",
       "        248, 236, 288, 233, 251, 264, 255, 253, 285, 241, 238, 279, 225,\n",
       "        234, 287, 229, 252, 276, 249, 226, 268, 243, 240, 282, 232, 227,\n",
       "        281, 231, 256, 286, 254, 244, 271, 245, 242, 283, 237, 239, 284,\n",
       "        228, 222, 280, 223, 218, 272, 193, 197, 259, 217, 186, 274, 192,\n",
       "        207, 273, 221, 219, 261, 201, 206, 267, 189, 200, 260, 212, 209,\n",
       "        275, 224, 194, 262, 190, 205, 263, 208, 213, 266, 215, 220, 265,\n",
       "        199, 198, 270, 216, 204, 258, 203, 202, 257, 214, 183, 179, 191,\n",
       "        168, 171, 173, 155, 148, 156, 161, 142, 158, 187, 181, 188, 170,\n",
       "        176, 162, 152, 157, 154, 153, 149, 151, 210, 180, 196, 175, 172,\n",
       "        165, 150, 160, 163, 147, 143, 159, 185, 195, 211, 184, 182, 178,\n",
       "        167, 166, 174, 169, 164, 177, 139, 133, 146, 108, 120, 124,  59,\n",
       "         60,  64,  62,  97,  49, 136, 140, 135, 113,  99,  93,  86,  61,\n",
       "         74,  66,  51,  85, 137, 144, 134, 112, 129, 107,  96,  88,  89,\n",
       "         72,  69,  68, 138, 145, 141, 132, 127, 125, 109,  94,  84, 103,\n",
       "         67,  91]),\n",
       " 'split0_test_neg_log_loss': array([-0.3381284 , -0.3545935 , -0.33990056, -0.34273284, -0.33557114,\n",
       "        -0.34427126, -0.3486316 , -0.35352328, -0.35174919, -0.35610315,\n",
       "        -0.34194637, -0.36955702, -0.3617278 , -0.34584233, -0.34470706,\n",
       "        -0.34286413, -0.35324921, -0.34664599, -0.3311234 , -0.37631289,\n",
       "        -0.34462175, -0.35977821, -0.35928482, -0.36457628, -0.3414686 ,\n",
       "        -0.3407195 , -0.32949463, -0.34089353, -0.35523188, -0.33115391,\n",
       "        -0.377248  , -0.33653078, -0.33128788, -0.35858217, -0.38094852,\n",
       "        -0.33722146, -0.34430772, -0.33215039, -0.3402445 , -0.33867036,\n",
       "        -0.32811435, -0.34041395, -0.33478735, -0.32899257, -0.3482479 ,\n",
       "        -0.35087585, -0.35561209, -0.36738483, -0.33006099, -0.33040337,\n",
       "        -0.33200195, -0.32641525, -0.3397147 , -0.345354  , -0.37101114,\n",
       "        -0.36641935, -0.34184115, -0.34802766, -0.37010096, -0.37644276,\n",
       "        -0.33301651, -0.33466429, -0.33133747, -0.34292697, -0.33369747,\n",
       "        -0.34444782, -0.32361107, -0.32544142, -0.36348637, -0.35981559,\n",
       "        -0.35817671, -0.3788948 , -0.33992305, -0.33349363, -0.33247718,\n",
       "        -0.33918897, -0.34566306, -0.33185268, -0.35646099, -0.36108644,\n",
       "        -0.35745957, -0.4089835 , -0.37146716, -0.37075871, -0.33123265,\n",
       "        -0.33084031, -0.33749465, -0.34521085, -0.35906816, -0.32693244,\n",
       "        -0.35433553, -0.34696622, -0.34122162, -0.37762791, -0.37771572,\n",
       "        -0.33452721, -0.47143865, -0.74086869, -0.49810263, -0.465058  ,\n",
       "        -0.67887508, -0.46169145, -0.46172365, -0.69351138, -0.4642719 ,\n",
       "        -0.46257723, -0.6425867 , -0.46456789, -0.48079618, -0.68358666,\n",
       "        -0.48339434, -0.47061774, -0.66146576, -0.4692919 , -0.46303165,\n",
       "        -0.6864746 , -0.46039502, -0.45836183, -0.68174015, -0.45920708,\n",
       "        -0.4674022 , -0.72800144, -0.46725016, -0.47020724, -0.73215298,\n",
       "        -0.4678512 , -0.46363125, -0.72673257, -0.46225312, -0.45942212,\n",
       "        -0.68456241, -0.46133749, -0.49919399, -0.67279324, -0.47920986,\n",
       "        -0.46274953, -0.7115976 , -0.46178329, -0.46444883, -0.73233763,\n",
       "        -0.46199531, -0.45848598, -0.65038062, -0.45723614, -0.43164364,\n",
       "        -0.67794574, -0.42981   , -0.41867548, -0.80648941, -0.43131824,\n",
       "        -0.41275505, -0.84087623, -0.41013576, -0.41264705, -0.59105096,\n",
       "        -0.41754409, -0.41325521, -0.72851032, -0.42836417, -0.42539078,\n",
       "        -0.63262762, -0.41649325, -0.42676693, -0.69250747, -0.41198501,\n",
       "        -0.40612804, -0.77613695, -0.41934884, -0.41105936, -0.89865735,\n",
       "        -0.44403693, -0.41399956, -0.77836898, -0.40865097, -0.42255976,\n",
       "        -0.81909757, -0.42085354, -0.4219343 , -0.68957036, -0.41930528,\n",
       "        -0.44301384, -0.78883937, -0.42540131, -0.40752464, -0.69402984,\n",
       "        -0.41436947, -0.42768738, -0.63711984, -0.42212141, -0.42176168,\n",
       "        -0.83144495, -0.41800198, -0.40188036, -0.40550987, -0.41573112,\n",
       "        -0.37739722, -0.38255049, -0.37445142, -0.3716774 , -0.37144382,\n",
       "        -0.3771312 , -0.37118251, -0.36731854, -0.37267698, -0.4032428 ,\n",
       "        -0.40652736, -0.41568481, -0.37406749, -0.3902899 , -0.37959399,\n",
       "        -0.37918556, -0.3714823 , -0.37523573, -0.37232495, -0.37020344,\n",
       "        -0.37462591, -0.41640609, -0.40199497, -0.41802953, -0.37969839,\n",
       "        -0.37472459, -0.37754531, -0.38365462, -0.37193372, -0.38921894,\n",
       "        -0.36998451, -0.37073264, -0.37879609, -0.40607262, -0.39707374,\n",
       "        -0.41209966, -0.38845691, -0.39103392, -0.38300542, -0.37767917,\n",
       "        -0.3786714 , -0.38344346, -0.37732562, -0.37637283, -0.37797189,\n",
       "        -0.35476541, -0.3501223 , -0.35682076, -0.349507  , -0.34965349,\n",
       "        -0.34498377, -0.34344938, -0.34081203, -0.34458964, -0.34147254,\n",
       "        -0.33532796, -0.33654688, -0.35544461, -0.37965443, -0.36361857,\n",
       "        -0.34299683, -0.33642381, -0.33469026, -0.33442239, -0.3437944 ,\n",
       "        -0.33871417, -0.33426546, -0.33545383, -0.33404348, -0.35797273,\n",
       "        -0.34805727, -0.35403841, -0.34893766, -0.35018445, -0.33863619,\n",
       "        -0.34104438, -0.3474761 , -0.34006806, -0.33552595, -0.33837958,\n",
       "        -0.33865324, -0.35037342, -0.38170096, -0.36130638, -0.34735404,\n",
       "        -0.34276289, -0.3512835 , -0.33974464, -0.34032607, -0.3443911 ,\n",
       "        -0.3405755 , -0.33776121, -0.34285135]),\n",
       " 'split1_test_neg_log_loss': array([-0.38026003, -0.38146311, -0.38399712, -0.35928864, -0.36886961,\n",
       "        -0.37107189, -0.38157599, -0.40275302, -0.38530783, -0.38322176,\n",
       "        -0.41373439, -0.39363662, -0.38558966, -0.37639534, -0.38242134,\n",
       "        -0.38022331, -0.37670506, -0.38958537, -0.40696807, -0.39350492,\n",
       "        -0.40362698, -0.42756442, -0.4137343 , -0.42560592, -0.38643864,\n",
       "        -0.37028576, -0.36994099, -0.37923406, -0.3863431 , -0.37832346,\n",
       "        -0.39976728, -0.39085274, -0.4094053 , -0.39349101, -0.42432517,\n",
       "        -0.45415913, -0.38157626, -0.3680773 , -0.37961168, -0.37471224,\n",
       "        -0.3658935 , -0.36942842, -0.37200114, -0.39628039, -0.377904  ,\n",
       "        -0.38112117, -0.39274323, -0.39326787, -0.37807872, -0.37544235,\n",
       "        -0.37145259, -0.39940244, -0.36517829, -0.39090352, -0.40844048,\n",
       "        -0.4152716 , -0.4004306 , -0.40409966, -0.42810362, -0.4241166 ,\n",
       "        -0.35758701, -0.38518237, -0.38747945, -0.38091422, -0.36776513,\n",
       "        -0.38487973, -0.38649632, -0.37049282, -0.37741752, -0.41753287,\n",
       "        -0.40957029, -0.41653632, -0.37943739, -0.4023889 , -0.36827955,\n",
       "        -0.37582891, -0.38967285, -0.3809171 , -0.39817236, -0.40101416,\n",
       "        -0.40353208, -0.4222313 , -0.39568513, -0.42553088, -0.38593392,\n",
       "        -0.38940347, -0.37102348, -0.37670577, -0.39103606, -0.37537854,\n",
       "        -0.42075661, -0.39399035, -0.38704983, -0.43209955, -0.4026153 ,\n",
       "        -0.3923156 , -0.48544699, -0.65954923, -0.48585957, -0.48037606,\n",
       "        -0.76898935, -0.48309446, -0.48037757, -0.75533083, -0.48063938,\n",
       "        -0.48011074, -0.65284106, -0.47967696, -0.48890312, -0.71968137,\n",
       "        -0.49097686, -0.48268522, -0.67892159, -0.48116187, -0.48338307,\n",
       "        -0.65939917, -0.48199545, -0.47741225, -0.65735146, -0.47467274,\n",
       "        -0.49967204, -0.80296482, -0.48899729, -0.48081407, -0.62990499,\n",
       "        -0.48770781, -0.47914398, -0.65946825, -0.47971247, -0.47406185,\n",
       "        -0.74773145, -0.47639911, -0.5195183 , -0.64279705, -0.48280498,\n",
       "        -0.48369731, -0.69615727, -0.47875138, -0.4814136 , -0.64372437,\n",
       "        -0.47615425, -0.48169665, -0.77933497, -0.47912419, -0.4586649 ,\n",
       "        -0.7184816 , -0.43072513, -0.43088047, -0.6864357 , -0.43548653,\n",
       "        -0.43650416, -0.72711485, -0.45075674, -0.43824295, -0.6663648 ,\n",
       "        -0.42036019, -0.44430602, -0.84247054, -0.45510088, -0.43383014,\n",
       "        -0.85362752, -0.43888084, -0.43970611, -0.78061074, -0.42815111,\n",
       "        -0.44460492, -0.71416609, -0.43492364, -0.43809921, -1.08881946,\n",
       "        -0.45113844, -0.4548675 , -0.80709767, -0.4227754 , -0.43803057,\n",
       "        -0.59621674, -0.42627826, -0.43774665, -0.61759747, -0.44707607,\n",
       "        -0.44632284, -0.84029451, -0.44639599, -0.43159032, -0.69998497,\n",
       "        -0.44053635, -0.43458342, -0.6650101 , -0.42832709, -0.4414723 ,\n",
       "        -0.65988212, -0.43802129, -0.42143015, -0.40839122, -0.43244833,\n",
       "        -0.39949803, -0.40980762, -0.40008693, -0.39338426, -0.40014276,\n",
       "        -0.40013612, -0.39478898, -0.39256551, -0.39699373, -0.42563558,\n",
       "        -0.42315156, -0.43198316, -0.40370183, -0.39907379, -0.40430583,\n",
       "        -0.39757557, -0.39258483, -0.39855861, -0.39396518, -0.39376703,\n",
       "        -0.3952475 , -0.42924893, -0.41678214, -0.40492433, -0.40794444,\n",
       "        -0.40195411, -0.40330707, -0.39972773, -0.39471389, -0.39792449,\n",
       "        -0.39548118, -0.39513736, -0.39680414, -0.41571218, -0.41937256,\n",
       "        -0.42720683, -0.40079846, -0.41000959, -0.4131623 , -0.40096404,\n",
       "        -0.40266567, -0.40733594, -0.40424761, -0.40290283, -0.41139869,\n",
       "        -0.39455366, -0.38831025, -0.38887325, -0.38255969, -0.38002442,\n",
       "        -0.37888927, -0.37995659, -0.37586513, -0.37756865, -0.37767352,\n",
       "        -0.38053404, -0.37194594, -0.38937783, -0.40412715, -0.37999513,\n",
       "        -0.38605786, -0.3784889 , -0.37160767, -0.38903989, -0.3794181 ,\n",
       "        -0.3705899 , -0.36749322, -0.37668538, -0.36872009, -0.39530332,\n",
       "        -0.40311987, -0.3956334 , -0.3759777 , -0.38402689, -0.37889522,\n",
       "        -0.37882104, -0.37027679, -0.37338314, -0.36531688, -0.37631745,\n",
       "        -0.36771696, -0.39021292, -0.39691501, -0.38730829, -0.38153206,\n",
       "        -0.37886519, -0.38096305, -0.37799115, -0.37674862, -0.3726633 ,\n",
       "        -0.38289636, -0.380964  , -0.3752112 ]),\n",
       " 'split2_test_neg_log_loss': array([-0.36142836, -0.34641285, -0.34051153, -0.3526945 , -0.3426687 ,\n",
       "        -0.32640211, -0.36050909, -0.33407417, -0.33034303, -0.34617857,\n",
       "        -0.36525771, -0.37171364, -0.36199196, -0.33800426, -0.35122907,\n",
       "        -0.3363057 , -0.35319785, -0.352824  , -0.34697563, -0.37728731,\n",
       "        -0.36683089, -0.35937696, -0.36411382, -0.36749304, -0.34480376,\n",
       "        -0.34767262, -0.36838153, -0.34769744, -0.33965768, -0.33339216,\n",
       "        -0.34046369, -0.35787165, -0.3433955 , -0.33081892, -0.36671793,\n",
       "        -0.34952342, -0.35708062, -0.35048529, -0.34830373, -0.3399576 ,\n",
       "        -0.32242153, -0.33621529, -0.32414882, -0.35704055, -0.33740524,\n",
       "        -0.34775586, -0.36337503, -0.37111912, -0.33757225, -0.35550647,\n",
       "        -0.36720982, -0.32700087, -0.35981553, -0.35201239, -0.37785807,\n",
       "        -0.37598135, -0.36299158, -0.36627372, -0.39291904, -0.35803924,\n",
       "        -0.34936422, -0.36420261, -0.35355284, -0.37637751, -0.3480443 ,\n",
       "        -0.35291448, -0.33688222, -0.38142607, -0.36557062, -0.36554386,\n",
       "        -0.36431689, -0.39067242, -0.34646189, -0.35698513, -0.3511436 ,\n",
       "        -0.33812111, -0.35512612, -0.37771808, -0.3536839 , -0.35827655,\n",
       "        -0.37602364, -0.38561611, -0.39781375, -0.35396352, -0.33495985,\n",
       "        -0.34589469, -0.35427767, -0.32687985, -0.3523813 , -0.34861424,\n",
       "        -0.3623605 , -0.36406724, -0.33960315, -0.37039785, -0.37425746,\n",
       "        -0.35893178, -0.48342499, -0.63978158, -0.48148181, -0.48988179,\n",
       "        -0.67439267, -0.47471747, -0.47217497, -0.72376906, -0.47650789,\n",
       "        -0.47443055, -0.63681859, -0.47177919, -0.48532646, -0.70595141,\n",
       "        -0.49108211, -0.48917147, -0.70040288, -0.48061571, -0.47201717,\n",
       "        -0.7610811 , -0.47291158, -0.47262369, -0.69039648, -0.47523155,\n",
       "        -0.48530699, -0.71359716, -0.48811193, -0.47222349, -0.88230877,\n",
       "        -0.47815663, -0.47262972, -0.66640806, -0.47267194, -0.47391951,\n",
       "        -0.66638534, -0.47203202, -0.48556024, -0.64828123, -0.51155492,\n",
       "        -0.47255112, -0.65562308, -0.48681522, -0.47148911, -0.65843958,\n",
       "        -0.47684263, -0.47288689, -0.6649062 , -0.47059123, -0.46326398,\n",
       "        -0.76568473, -0.45325256, -0.44712295, -0.60643633, -0.42513553,\n",
       "        -0.42677423, -0.92504784, -0.43988395, -0.40855956, -0.61972146,\n",
       "        -0.42654867, -0.43811072, -0.8384453 , -0.44295392, -0.41856659,\n",
       "        -0.81006002, -0.41407786, -0.42320668, -0.56650044, -0.4295652 ,\n",
       "        -0.42473681, -0.71975594, -0.42347725, -0.41968925, -0.64867402,\n",
       "        -0.43893309, -0.41054244, -0.72052641, -0.43153234, -0.41782674,\n",
       "        -0.75068575, -0.42481262, -0.4296848 , -0.7362598 , -0.43459297,\n",
       "        -0.42855153, -0.6582396 , -0.43297986, -0.42374293, -0.6624958 ,\n",
       "        -0.42398708, -0.41994772, -0.9133933 , -0.41681709, -0.4187953 ,\n",
       "        -0.70951118, -0.42810605, -0.4248417 , -0.39284868, -0.4094983 ,\n",
       "        -0.40122973, -0.38456829, -0.38756478, -0.38664296, -0.39133138,\n",
       "        -0.38865476, -0.3864873 , -0.38082537, -0.39582922, -0.39921181,\n",
       "        -0.39428451, -0.42202254, -0.38951201, -0.39472907, -0.38163063,\n",
       "        -0.38637694, -0.38859761, -0.38343292, -0.3860034 , -0.38654264,\n",
       "        -0.38575861, -0.40108084, -0.39931396, -0.40182074, -0.39800697,\n",
       "        -0.40028775, -0.38386023, -0.38211737, -0.38441426, -0.38648311,\n",
       "        -0.38831171, -0.38188831, -0.38653588, -0.42751991, -0.42060015,\n",
       "        -0.42941176, -0.40278706, -0.40411748, -0.39604201, -0.39797327,\n",
       "        -0.38899151, -0.39528748, -0.39129647, -0.39250592, -0.39723844,\n",
       "        -0.36288672, -0.37037476, -0.37595836, -0.35822721, -0.35737139,\n",
       "        -0.34796959, -0.34605374, -0.34545976, -0.34684612, -0.34370301,\n",
       "        -0.34913591, -0.34253224, -0.3691354 , -0.36271326, -0.3734585 ,\n",
       "        -0.35450679, -0.35852432, -0.35721001, -0.34969716, -0.34786057,\n",
       "        -0.34659276, -0.34560708, -0.34351137, -0.34905464, -0.3759955 ,\n",
       "        -0.38706432, -0.36644714, -0.35435997, -0.35316906, -0.36164842,\n",
       "        -0.34239531, -0.34477811, -0.34505618, -0.34778197, -0.3499887 ,\n",
       "        -0.34924526, -0.3678723 , -0.36088974, -0.36490739, -0.35927113,\n",
       "        -0.35420941, -0.35729815, -0.35070499, -0.35353481, -0.34974051,\n",
       "        -0.34594754, -0.35029656, -0.34790185]),\n",
       " 'split3_test_neg_log_loss': array([-0.33037392, -0.34243537, -0.33894753, -0.32367617, -0.33052087,\n",
       "        -0.3140163 , -0.32216313, -0.31199367, -0.33874987, -0.33824329,\n",
       "        -0.34307059, -0.31542736, -0.33260439, -0.31834147, -0.32840509,\n",
       "        -0.32080171, -0.3015321 , -0.31305154, -0.32085813, -0.33057714,\n",
       "        -0.33070425, -0.32882105, -0.34537506, -0.34359848, -0.34041215,\n",
       "        -0.32072904, -0.32926459, -0.30914457, -0.30346721, -0.32446761,\n",
       "        -0.33534257, -0.31471665, -0.31919731, -0.35972571, -0.33924056,\n",
       "        -0.35665954, -0.33667103, -0.31710025, -0.33471083, -0.31550964,\n",
       "        -0.33580397, -0.31079075, -0.31586815, -0.3228348 , -0.31831636,\n",
       "        -0.31935329, -0.33054172, -0.34258945, -0.32106635, -0.32655781,\n",
       "        -0.31901779, -0.34664428, -0.30718912, -0.32919782, -0.34277414,\n",
       "        -0.3301136 , -0.35318707, -0.34632649, -0.37306506, -0.35026775,\n",
       "        -0.34853102, -0.32910895, -0.32499876, -0.33573202, -0.32405067,\n",
       "        -0.33872782, -0.33189893, -0.33722787, -0.35043111, -0.35043036,\n",
       "        -0.34968941, -0.37402161, -0.33367833, -0.33907399, -0.33175055,\n",
       "        -0.32356715, -0.33900513, -0.33607981, -0.36756357, -0.3389021 ,\n",
       "        -0.34002535, -0.34767567, -0.35101283, -0.37292519, -0.34041729,\n",
       "        -0.33526606, -0.32800223, -0.30568159, -0.32630425, -0.32048634,\n",
       "        -0.35318739, -0.3111053 , -0.32983202, -0.33790798, -0.33757659,\n",
       "        -0.34814438, -0.46427584, -0.7859823 , -0.4618814 , -0.44939448,\n",
       "        -0.71714443, -0.45510514, -0.45594565, -0.67580841, -0.44739633,\n",
       "        -0.44544835, -0.68217741, -0.44997296, -0.46157388, -0.69718919,\n",
       "        -0.47080812, -0.46922367, -0.64621646, -0.45354368, -0.45365166,\n",
       "        -0.68257487, -0.45228045, -0.44855282, -0.65386168, -0.4515023 ,\n",
       "        -0.47184758, -0.65306708, -0.46705244, -0.45096485, -0.6981903 ,\n",
       "        -0.45911623, -0.45480532, -0.74289926, -0.45299992, -0.44800784,\n",
       "        -0.63326078, -0.44856733, -0.46653809, -0.66720823, -0.46033942,\n",
       "        -0.45346689, -0.80723643, -0.4567681 , -0.4547917 , -0.66938838,\n",
       "        -0.45028233, -0.45094087, -0.64155801, -0.45462457, -0.40477609,\n",
       "        -1.01899166, -0.40664472, -0.41675932, -0.58127944, -0.39834859,\n",
       "        -0.39635213, -0.72650003, -0.40972665, -0.39748534, -0.66921712,\n",
       "        -0.39351835, -0.4079453 , -0.85758731, -0.40213992, -0.40870263,\n",
       "        -0.70797878, -0.42143961, -0.39466262, -0.71594178, -0.39306823,\n",
       "        -0.40647574, -0.64506432, -0.4023257 , -0.43686376, -0.72717635,\n",
       "        -0.41937711, -0.39145051, -0.64611126, -0.39898342, -0.40036085,\n",
       "        -0.68821681, -0.40469277, -0.39926637, -0.64967892, -0.39949423,\n",
       "        -0.40569483, -0.64822959, -0.4067868 , -0.39048004, -0.69343396,\n",
       "        -0.41036082, -0.39556922, -0.70848988, -0.39676903, -0.39423812,\n",
       "        -0.63544335, -0.40206828, -0.38144346, -0.37957871, -0.37487237,\n",
       "        -0.36645201, -0.36887035, -0.37198715, -0.36820126, -0.36766781,\n",
       "        -0.36787308, -0.36892512, -0.36756044, -0.36624394, -0.3796092 ,\n",
       "        -0.38337456, -0.38814669, -0.37897935, -0.36940779, -0.36969243,\n",
       "        -0.37156602, -0.36638938, -0.36607498, -0.36381188, -0.36575512,\n",
       "        -0.36837669, -0.39993293, -0.37871841, -0.38690641, -0.37270585,\n",
       "        -0.37095604, -0.37017399, -0.36797721, -0.36644885, -0.36727259,\n",
       "        -0.3692018 , -0.36462626, -0.3644968 , -0.39359663, -0.40600758,\n",
       "        -0.39730519, -0.39503495, -0.38277619, -0.37947509, -0.37873434,\n",
       "        -0.37349057, -0.37275768, -0.37021609, -0.37223611, -0.38029802,\n",
       "        -0.35278402, -0.35040926, -0.3444442 , -0.33590027, -0.33071399,\n",
       "        -0.3347916 , -0.32532417, -0.32139719, -0.32680528, -0.32891623,\n",
       "        -0.32474649, -0.32240551, -0.34318058, -0.35314401, -0.34954535,\n",
       "        -0.34124464, -0.33877644, -0.33387804, -0.32881731, -0.32810611,\n",
       "        -0.3251837 , -0.32507773, -0.32840469, -0.32650193, -0.35769616,\n",
       "        -0.35555039, -0.34497399, -0.33226757, -0.35506861, -0.32894009,\n",
       "        -0.32320226, -0.3299736 , -0.32853316, -0.31939795, -0.32050945,\n",
       "        -0.32659534, -0.35233549, -0.35785156, -0.35042505, -0.33790837,\n",
       "        -0.33791624, -0.33804272, -0.33175929, -0.33605646, -0.32868455,\n",
       "        -0.32839965, -0.33008755, -0.32894419]),\n",
       " 'split4_test_neg_log_loss': array([-0.37448802, -0.37574868, -0.3830302 , -0.37088553, -0.38859099,\n",
       "        -0.36716955, -0.37463028, -0.38414252, -0.39185099, -0.40731691,\n",
       "        -0.38983044, -0.42088411, -0.3962266 , -0.37304805, -0.36988583,\n",
       "        -0.40292914, -0.41393452, -0.3679199 , -0.41469319, -0.38537376,\n",
       "        -0.38217257, -0.41518756, -0.44155583, -0.43298372, -0.42275019,\n",
       "        -0.38462956, -0.37250349, -0.36647895, -0.36151517, -0.36803064,\n",
       "        -0.37884722, -0.38851934, -0.39025284, -0.39169882, -0.3820297 ,\n",
       "        -0.4066972 , -0.41437218, -0.38301249, -0.39905012, -0.36682979,\n",
       "        -0.3634142 , -0.38431501, -0.37318195, -0.37826073, -0.36908263,\n",
       "        -0.41161987, -0.37801761, -0.38455711, -0.37236929, -0.3992665 ,\n",
       "        -0.38550899, -0.38704441, -0.38855164, -0.37492298, -0.42619914,\n",
       "        -0.42260011, -0.41173876, -0.41994268, -0.42540465, -0.41388226,\n",
       "        -0.37556578, -0.37461239, -0.38735988, -0.39251751, -0.39762632,\n",
       "        -0.37499017, -0.41342761, -0.39698115, -0.40915411, -0.40582814,\n",
       "        -0.41093056, -0.41416819, -0.37896381, -0.38604588, -0.37837993,\n",
       "        -0.39276339, -0.36514941, -0.36846163, -0.38026563, -0.40516089,\n",
       "        -0.39370557, -0.42849543, -0.39935407, -0.41165909, -0.37012281,\n",
       "        -0.37825664, -0.39567239, -0.38065451, -0.39078103, -0.37538189,\n",
       "        -0.39233754, -0.39991951, -0.4145857 , -0.4250917 , -0.41459166,\n",
       "        -0.41461423, -0.50272463, -0.67967039, -0.49316697, -0.49276412,\n",
       "        -0.63572786, -0.48692265, -0.49236722, -0.69614374, -0.49506781,\n",
       "        -0.489796  , -0.65916964, -0.48892277, -0.50714727, -0.74957691,\n",
       "        -0.49501528, -0.48625557, -0.65432113, -0.49805049, -0.49467176,\n",
       "        -0.64882029, -0.4893248 , -0.48870927, -0.6723194 , -0.49053919,\n",
       "        -0.49795327, -0.65361918, -0.48986237, -0.48938001, -0.6800131 ,\n",
       "        -0.4934851 , -0.48782465, -0.6575353 , -0.48939925, -0.49053348,\n",
       "        -0.68515356, -0.48971771, -0.5218768 , -0.65536309, -0.49499478,\n",
       "        -0.49058018, -0.7142693 , -0.49411169, -0.49081883, -0.69890576,\n",
       "        -0.49032532, -0.49191776, -0.640099  , -0.49354496, -0.45240395,\n",
       "        -0.64238633, -0.45665967, -0.46750293, -0.79049572, -0.46017132,\n",
       "        -0.46339086, -0.6562598 , -0.44708138, -0.45093496, -0.62313629,\n",
       "        -0.44096733, -0.45519828, -0.73871051, -0.46358883, -0.45677134,\n",
       "        -0.78672843, -0.4436558 , -0.43869252, -0.65366985, -0.44391296,\n",
       "        -0.44563784, -0.67056039, -0.44970445, -0.45956944, -0.95754503,\n",
       "        -0.45584719, -0.44906319, -0.73608125, -0.45522693, -0.4482817 ,\n",
       "        -0.6539283 , -0.46018895, -0.44905995, -0.61031152, -0.4476662 ,\n",
       "        -0.46048783, -0.6186192 , -0.47469201, -0.45687296, -0.798953  ,\n",
       "        -0.44254693, -0.45753988, -0.63406562, -0.45152776, -0.45710501,\n",
       "        -0.68063559, -0.44691202, -0.44410235, -0.44853069, -0.44953566,\n",
       "        -0.42361185, -0.4302674 , -0.42893588, -0.42459927, -0.42390611,\n",
       "        -0.42004468, -0.42293326, -0.42338107, -0.42298064, -0.45245274,\n",
       "        -0.43873159, -0.43266889, -0.42281186, -0.43680553, -0.42648123,\n",
       "        -0.4223271 , -0.42299262, -0.42417537, -0.42152445, -0.42133121,\n",
       "        -0.42334713, -0.44941947, -0.42656715, -0.45404958, -0.4277423 ,\n",
       "        -0.42675232, -0.42755405, -0.42245928, -0.42632033, -0.42351627,\n",
       "        -0.42216991, -0.42161419, -0.42215824, -0.43220706, -0.43796515,\n",
       "        -0.45128059, -0.42782132, -0.43734451, -0.43141704, -0.427348  ,\n",
       "        -0.43019529, -0.42693964, -0.4275153 , -0.42622486, -0.43146416,\n",
       "        -0.39964412, -0.41479434, -0.40258237, -0.38791493, -0.40026803,\n",
       "        -0.39183801, -0.38421302, -0.38041798, -0.38537569, -0.38624441,\n",
       "        -0.38135899, -0.3781672 , -0.41184442, -0.40398772, -0.41069838,\n",
       "        -0.38387264, -0.3920358 , -0.39668569, -0.3839568 , -0.38408552,\n",
       "        -0.38099308, -0.38519391, -0.37960089, -0.38416981, -0.40221013,\n",
       "        -0.41261603, -0.41781541, -0.38889089, -0.40102779, -0.39077967,\n",
       "        -0.39362565, -0.38987202, -0.38797481, -0.38668095, -0.38364965,\n",
       "        -0.38059005, -0.4105509 , -0.40327907, -0.41279656, -0.40188565,\n",
       "        -0.39343241, -0.38890661, -0.38986573, -0.38610855, -0.39479364,\n",
       "        -0.3810678 , -0.3831522 , -0.38870418]),\n",
       " 'mean_test_neg_log_loss': array([-0.35693575, -0.3601307 , -0.35727739, -0.34985554, -0.35324426,\n",
       "        -0.34458622, -0.35750202, -0.35729733, -0.35960018, -0.36621273,\n",
       "        -0.3707679 , -0.37424375, -0.36762808, -0.35032629, -0.35532968,\n",
       "        -0.3566248 , -0.35972375, -0.35400536, -0.36412368, -0.3726112 ,\n",
       "        -0.36559129, -0.37814564, -0.38481277, -0.38685149, -0.36717467,\n",
       "        -0.35280729, -0.35391704, -0.34868971, -0.34924301, -0.34707355,\n",
       "        -0.36633375, -0.35769823, -0.35870776, -0.36686332, -0.37865237,\n",
       "        -0.38085215, -0.36680156, -0.35016515, -0.36038417, -0.34713592,\n",
       "        -0.34312951, -0.34823269, -0.34399748, -0.35668181, -0.35019123,\n",
       "        -0.36214521, -0.36405793, -0.37178368, -0.34782952, -0.3574353 ,\n",
       "        -0.35503823, -0.35730145, -0.35208986, -0.35847814, -0.3852566 ,\n",
       "        -0.3820772 , -0.37403783, -0.37693404, -0.39791866, -0.38454972,\n",
       "        -0.35281291, -0.35755412, -0.35694568, -0.36569364, -0.35423678,\n",
       "        -0.359192  , -0.35846323, -0.36231387, -0.37321194, -0.37983016,\n",
       "        -0.37853677, -0.39485867, -0.35569289, -0.36359751, -0.35240617,\n",
       "        -0.3538939 , -0.35892332, -0.35900586, -0.37122929, -0.37288803,\n",
       "        -0.37414925, -0.3986004 , -0.38306659, -0.38696748, -0.35253331,\n",
       "        -0.35593223, -0.35729408, -0.34702651, -0.36391416, -0.34935869,\n",
       "        -0.37659551, -0.36320972, -0.36245846, -0.388625  , -0.38135135,\n",
       "        -0.36970664, -0.48146222, -0.70117044, -0.48409848, -0.47549489,\n",
       "        -0.69502588, -0.47230623, -0.47251781, -0.70891268, -0.47277666,\n",
       "        -0.47047257, -0.65471868, -0.47098395, -0.48474938, -0.71119711,\n",
       "        -0.48625534, -0.47959073, -0.66826556, -0.47653273, -0.47335106,\n",
       "        -0.68767001, -0.47138146, -0.46913197, -0.67113383, -0.47023057,\n",
       "        -0.48443642, -0.71024994, -0.48025484, -0.47271793, -0.72451403,\n",
       "        -0.47726339, -0.47160698, -0.69060869, -0.47140734, -0.46918896,\n",
       "        -0.68341871, -0.46961073, -0.49853748, -0.65728857, -0.48578079,\n",
       "        -0.472609  , -0.71697673, -0.47564594, -0.47259241, -0.68055914,\n",
       "        -0.47111997, -0.47118563, -0.67525576, -0.47102422, -0.44215051,\n",
       "        -0.76469801, -0.43541842, -0.43618823, -0.69422732, -0.43009204,\n",
       "        -0.42715528, -0.77515975, -0.4315169 , -0.42157397, -0.63389813,\n",
       "        -0.41978773, -0.43176311, -0.8011448 , -0.43842954, -0.4286523 ,\n",
       "        -0.75820447, -0.42690947, -0.42460697, -0.68184606, -0.4213365 ,\n",
       "        -0.42551667, -0.70513674, -0.42595598, -0.4330562 , -0.86417444,\n",
       "        -0.44186655, -0.42398464, -0.73763711, -0.42343381, -0.42541193,\n",
       "        -0.70162903, -0.42736523, -0.42753841, -0.66068361, -0.42962695,\n",
       "        -0.43681418, -0.71084445, -0.43725119, -0.42204218, -0.70977951,\n",
       "        -0.42636013, -0.42706552, -0.71161575, -0.42311248, -0.42667448,\n",
       "        -0.70338344, -0.42662192, -0.41473961, -0.40697183, -0.41641716,\n",
       "        -0.39363777, -0.39521283, -0.39260523, -0.38890103, -0.39089838,\n",
       "        -0.39076797, -0.38886343, -0.38633018, -0.3909449 , -0.41203043,\n",
       "        -0.40921392, -0.41810122, -0.39381451, -0.39806122, -0.39234082,\n",
       "        -0.39140624, -0.38840935, -0.38949552, -0.38752597, -0.38751989,\n",
       "        -0.38947117, -0.41921765, -0.40467533, -0.41314612, -0.39721959,\n",
       "        -0.39493496, -0.39248813, -0.39118724, -0.38876621, -0.39288308,\n",
       "        -0.38902982, -0.38679975, -0.38975823, -0.41502168, -0.41620384,\n",
       "        -0.4234608 , -0.40297974, -0.40505634, -0.40062037, -0.39653976,\n",
       "        -0.39480289, -0.39715284, -0.39412022, -0.39404851, -0.39967424,\n",
       "        -0.37292679, -0.37480218, -0.37373579, -0.36282182, -0.36360626,\n",
       "        -0.35969445, -0.35579938, -0.35279042, -0.35623707, -0.35560194,\n",
       "        -0.35422068, -0.35031955, -0.37379657, -0.38072531, -0.37546318,\n",
       "        -0.36173575, -0.36084985, -0.35881433, -0.35718671, -0.35665294,\n",
       "        -0.35241472, -0.35152748, -0.35273123, -0.35249799, -0.37783557,\n",
       "        -0.38128158, -0.37578167, -0.36008676, -0.36869536, -0.35977992,\n",
       "        -0.35581773, -0.35647532, -0.35500307, -0.35094074, -0.35376896,\n",
       "        -0.35256017, -0.37426901, -0.38012727, -0.37534873, -0.36559025,\n",
       "        -0.36143723, -0.36329881, -0.35801316, -0.3585549 , -0.35805462,\n",
       "        -0.35577737, -0.3564523 , -0.35672255]),\n",
       " 'std_test_neg_log_loss': array([0.01965469, 0.01569049, 0.02142981, 0.0159712 , 0.02206389,\n",
       "        0.02225659, 0.0210214 , 0.03284163, 0.02471175, 0.02555278,\n",
       "        0.02770445, 0.03475382, 0.02206233, 0.02186706, 0.01897592,\n",
       "        0.03028695, 0.0366004 , 0.02526786, 0.03910984, 0.0219174 ,\n",
       "        0.02600006, 0.03724745, 0.03658329, 0.03569831, 0.0326718 ,\n",
       "        0.02244837, 0.02007806, 0.02397915, 0.02738683, 0.0217596 ,\n",
       "        0.02458952, 0.02947768, 0.03496491, 0.02342886, 0.02756231,\n",
       "        0.04361717, 0.02824739, 0.02373906, 0.02480087, 0.02131534,\n",
       "        0.01809755, 0.02591773, 0.02410806, 0.0280932 , 0.02148557,\n",
       "        0.03153737, 0.02103737, 0.01730217, 0.02304089, 0.0274298 ,\n",
       "        0.02519599, 0.03047227, 0.02731181, 0.02187795, 0.02924454,\n",
       "        0.03384233, 0.02724459, 0.0299124 , 0.02483197, 0.02956253,\n",
       "        0.01387523, 0.02205257, 0.02662792, 0.02227712, 0.02621862,\n",
       "        0.01780462, 0.03519378, 0.02691794, 0.01990713, 0.02670737,\n",
       "        0.026311  , 0.01760302, 0.01961569, 0.0266836 , 0.01871905,\n",
       "        0.02598734, 0.0177273 , 0.02089287, 0.0164132 , 0.02584737,\n",
       "        0.02320474, 0.02940021, 0.01899462, 0.02700364, 0.0216071 ,\n",
       "        0.02356332, 0.02416824, 0.02874122, 0.02461066, 0.02320062,\n",
       "        0.02624537, 0.03247926, 0.03274092, 0.03534187, 0.02659641,\n",
       "        0.02948382, 0.01317319, 0.05428107, 0.0125052 , 0.01624312,\n",
       "        0.04508361, 0.01220156, 0.0130157 , 0.02782623, 0.01605697,\n",
       "        0.01530035, 0.0157792 , 0.01327124, 0.01464229, 0.02249919,\n",
       "        0.00859219, 0.0081704 , 0.01935565, 0.01471081, 0.01450206,\n",
       "        0.03930679, 0.01358317, 0.01416703, 0.01395156, 0.01363414,\n",
       "        0.01314709, 0.05549647, 0.01071349, 0.01282901, 0.08553558,\n",
       "        0.01257725, 0.01154793, 0.03657454, 0.01278214, 0.01446197,\n",
       "        0.03727305, 0.01391047, 0.02086969, 0.01125734, 0.01702057,\n",
       "        0.01348738, 0.04976103, 0.01431032, 0.01260489, 0.0315863 ,\n",
       "        0.0137434 , 0.01492869, 0.05278239, 0.01436691, 0.0215911 ,\n",
       "        0.13363255, 0.0181695 , 0.01903959, 0.09208211, 0.01983724,\n",
       "        0.02261311, 0.09549699, 0.01796886, 0.01984565, 0.02984691,\n",
       "        0.01542548, 0.0182025 , 0.05560343, 0.02167783, 0.01629625,\n",
       "        0.0786134 , 0.0120567 , 0.01630753, 0.0708931 , 0.0173804 ,\n",
       "        0.0173684 , 0.04503443, 0.01582781, 0.01676554, 0.15848119,\n",
       "        0.0126532 , 0.0241725 , 0.05505639, 0.01945855, 0.01658604,\n",
       "        0.0771653 , 0.01811925, 0.01674937, 0.04703418, 0.01827055,\n",
       "        0.01857944, 0.08721687, 0.02268121, 0.02242396, 0.04647327,\n",
       "        0.01317849, 0.02014086, 0.10436446, 0.0177156 , 0.02136973,\n",
       "        0.06850253, 0.01562703, 0.02136811, 0.02315659, 0.02501257,\n",
       "        0.01996621, 0.02195176, 0.02076518, 0.0201231 , 0.02046116,\n",
       "        0.01821808, 0.01955333, 0.02076389, 0.02015236, 0.0249469 ,\n",
       "        0.01981579, 0.01626908, 0.01770392, 0.02188453, 0.02048838,\n",
       "        0.01767729, 0.01992424, 0.02036394, 0.01996551, 0.01978258,\n",
       "        0.01928776, 0.01855546, 0.0163433 , 0.02272147, 0.0197747 ,\n",
       "        0.02036523, 0.02069936, 0.01859349, 0.02119445, 0.01829804,\n",
       "        0.01947448, 0.0202756 , 0.01932924, 0.01407933, 0.01394882,\n",
       "        0.01809187, 0.01338605, 0.01876686, 0.01941188, 0.01813026,\n",
       "        0.02031087, 0.0188547 , 0.02039233, 0.01951567, 0.02000212,\n",
       "        0.02008934, 0.02451756, 0.02102533, 0.01970996, 0.0242037 ,\n",
       "        0.02179761, 0.02265897, 0.02226423, 0.02188231, 0.02226866,\n",
       "        0.02315339, 0.02132001, 0.0244388 , 0.02085711, 0.02038679,\n",
       "        0.01951869, 0.0217852 , 0.02368237, 0.02494151, 0.02157947,\n",
       "        0.02054356, 0.02200432, 0.02131232, 0.02142187, 0.01845403,\n",
       "        0.02552704, 0.02709222, 0.02006592, 0.02024772, 0.02335818,\n",
       "        0.02615635, 0.02109963, 0.02212517, 0.02335199, 0.02348876,\n",
       "        0.01946261, 0.02308064, 0.01836628, 0.02223857, 0.02328207,\n",
       "        0.02136181, 0.0188999 , 0.02231645, 0.01976809, 0.02315852,\n",
       "        0.02214636, 0.0218909 , 0.02193858]),\n",
       " 'rank_test_neg_log_loss': array([ 52,  78,  55,  12,  29,   3,  60,  57,  73,  97, 105, 116, 102,\n",
       "         16,  38,  48,  75,  33,  93, 108,  95, 125, 137, 141, 101,  27,\n",
       "         32,   9,  10,   5,  98,  62,  68, 100, 127, 131,  99,  13,  79,\n",
       "          6,   1,   8,   2,  50,  14,  83,  92, 107,   7,  59,  37,  58,\n",
       "         19,  66, 138, 134, 114, 123, 174, 136,  28,  61,  53,  96,  35,\n",
       "         72,  65,  84, 111, 128, 126, 168,  40,  89,  20,  31,  70,  71,\n",
       "        106, 109, 115, 176, 135, 142,  23,  44,  56,   4,  91,  11, 122,\n",
       "         87,  85, 146, 133, 104, 250, 271, 251, 244, 270, 237, 238, 275,\n",
       "        242, 229, 258, 230, 253, 279, 255, 248, 261, 246, 243, 267, 234,\n",
       "        225, 262, 228, 252, 277, 249, 241, 282, 247, 236, 268, 235, 226,\n",
       "        266, 227, 256, 259, 254, 240, 281, 245, 239, 264, 232, 233, 263,\n",
       "        231, 224, 285, 218, 219, 269, 214, 209, 286, 215, 194, 257, 192,\n",
       "        216, 287, 222, 212, 284, 207, 200, 265, 193, 202, 274, 203, 217,\n",
       "        288, 223, 199, 283, 197, 201, 272, 210, 211, 260, 213, 220, 278,\n",
       "        221, 195, 276, 204, 208, 280, 196, 206, 273, 205, 186, 182, 189,\n",
       "        163, 170, 161, 149, 155, 154, 148, 139, 156, 184, 183, 190, 164,\n",
       "        175, 159, 158, 145, 152, 144, 143, 151, 191, 180, 185, 173, 169,\n",
       "        160, 157, 147, 162, 150, 140, 153, 187, 188, 198, 179, 181, 178,\n",
       "        171, 167, 172, 166, 165, 177, 110, 118, 112,  86,  90,  74,  42,\n",
       "         26,  45,  39,  34,  15, 113, 130, 120,  82,  80,  69,  54,  49,\n",
       "         21,  18,  25,  22, 124, 132, 121,  77, 103,  76,  43,  47,  36,\n",
       "         17,  30,  24, 117, 129, 119,  94,  81,  88,  63,  67,  64,  41,\n",
       "         46,  51])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRIAL_1_MLP.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL ONE NEG LOG LOSS RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.1,\n",
       " 'classifier__hidden_layer_sizes': (10,),\n",
       " 'classifier__learning_rate': 'invscaling',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL ONE NEG LOG LOSS RESULTS ---------------------------------------------\")\n",
    "TRIAL_1_MLP.cv_results_['rank_test_neg_log_loss']\n",
    "TRIAL_1_MLP.cv_results_['params'][ np.argmin(TRIAL_1_MLP.cv_results_['rank_test_neg_log_loss']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL ONE F1 RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.1,\n",
       " 'classifier__hidden_layer_sizes': (20,),\n",
       " 'classifier__learning_rate': 'constant',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL ONE F1 RESULTS ---------------------------------------------\")\n",
    "TRIAL_1_MLP.cv_results_['rank_test_f1_micro']\n",
    "TRIAL_1_MLP.cv_results_['params'][ np.argmin(TRIAL_1_MLP.cv_results_['rank_test_f1_micro']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL ONE ROC AUC RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.1,\n",
       " 'classifier__hidden_layer_sizes': (15,),\n",
       " 'classifier__learning_rate': 'constant',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL ONE ROC AUC RESULTS ---------------------------------------------\")\n",
    "TRIAL_1_MLP.cv_results_['rank_test_roc_auc_ovo']\n",
    "TRIAL_1_MLP.cv_results_['params'][ np.argmin(TRIAL_1_MLP.cv_results_['rank_test_roc_auc_ovo']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL ONE JACCARD SCORE RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.1,\n",
       " 'classifier__hidden_layer_sizes': (20,),\n",
       " 'classifier__learning_rate': 'constant',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL ONE JACCARD SCORE RESULTS ---------------------------------------------\")\n",
    "TRIAL_1_MLP.cv_results_['rank_test_jaccard']\n",
    "TRIAL_1_MLP.cv_results_['params'][ np.argmin(TRIAL_1_MLP.cv_results_['rank_test_jaccard']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 1 Multilayered Perceptrons using best NEG LOG LOSS hyperparameters :0.788503329828251\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 1 Multilayered Perceptrons using best F1 hyperparameters :0.8119172800560813\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 1 Multilayered Perceptrons using best ROC_AUC hyperparameters :0.8072905713284262\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 1 Multilayered Perceptrons using best JACCARD hyperparameters :0.8023133543638276\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best NEG LOG LOSS results to train on test set, \n",
    "bestMPLT1_1 = MLPClassifier(activation = 'logistic', alpha = .1, hidden_layer_sizes = (10,), \n",
    "                          learning_rate = 'invscaling',solver = 'lbfgs')\n",
    "bestMPLT1_1.fit(X_train,y_train)\n",
    "y_pred1_1 = bestMPLT1_1.predict(X_test)\n",
    "print('Accuracy of Trial 1 Multilayered Perceptrons using best NEG LOG LOSS hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred1_1)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best F1 results to train on test set, \n",
    "bestMPLT1_2 = MLPClassifier(activation = 'logistic', alpha = 0.1, hidden_layer_sizes = (20,), \n",
    "                          learning_rate = 'constant',solver = 'lbfgs')\n",
    "bestMPLT1_2.fit(X_train,y_train)\n",
    "y_pred1_2 = bestMPLT1_2.predict(X_test)\n",
    "print('Accuracy of Trial 1 Multilayered Perceptrons using best F1 hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred1_2)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best ROC_AUC results to train on test set,\n",
    "bestMPLT1_3 = MLPClassifier(activation = 'logistic', alpha = 0.1, hidden_layer_sizes = (15,), \n",
    "                          learning_rate = 'constant',solver = 'lbfgs')\n",
    "bestMPLT1_3.fit(X_train,y_train)\n",
    "y_pred1_3 = bestMPLT1_3.predict(X_test)\n",
    "print('Accuracy of Trial 1 Multilayered Perceptrons using best ROC_AUC hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred1_3)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best JACCARD results to train on test set, \n",
    "bestMPLT1_4 = MLPClassifier(activation = 'logistic', alpha = 0.1, hidden_layer_sizes = (20,), \n",
    "                          learning_rate = 'constant',solver = 'lbfgs')\n",
    "bestMPLT1_4.fit(X_train,y_train)\n",
    "y_pred1_4 = bestMPLT1_4.predict(X_test)\n",
    "print('Accuracy of Trial 1 Multilayered Perceptrons using best JACCARD hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred1_4)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTI LAYERED PERCEPTRON TRIAL TWO ON GAMMA PARTICLES DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:    6.4s\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:    8.0s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed:    9.5s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   11.6s\n",
      "[Parallel(n_jobs=-1)]: Done 189 tasks      | elapsed:   12.5s\n",
      "[Parallel(n_jobs=-1)]: Done 210 tasks      | elapsed:   13.6s\n",
      "[Parallel(n_jobs=-1)]: Done 233 tasks      | elapsed:   16.0s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:   17.0s\n",
      "[Parallel(n_jobs=-1)]: Done 281 tasks      | elapsed:   19.0s\n",
      "[Parallel(n_jobs=-1)]: Done 306 tasks      | elapsed:   21.1s\n",
      "[Parallel(n_jobs=-1)]: Done 333 tasks      | elapsed:   23.1s\n",
      "[Parallel(n_jobs=-1)]: Done 360 tasks      | elapsed:   25.4s\n",
      "[Parallel(n_jobs=-1)]: Done 389 tasks      | elapsed:   27.1s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:   30.0s\n",
      "[Parallel(n_jobs=-1)]: Done 449 tasks      | elapsed:   31.6s\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:   34.7s\n",
      "[Parallel(n_jobs=-1)]: Done 513 tasks      | elapsed:   38.7s\n",
      "[Parallel(n_jobs=-1)]: Done 546 tasks      | elapsed:   42.3s\n",
      "[Parallel(n_jobs=-1)]: Done 581 tasks      | elapsed:   45.8s\n",
      "[Parallel(n_jobs=-1)]: Done 616 tasks      | elapsed:   50.0s\n",
      "[Parallel(n_jobs=-1)]: Done 653 tasks      | elapsed:   54.2s\n",
      "[Parallel(n_jobs=-1)]: Done 690 tasks      | elapsed:   58.3s\n",
      "[Parallel(n_jobs=-1)]: Done 729 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 809 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 850 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 893 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 936 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 981 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1026 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1073 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1120 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1169 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1269 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1320 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1373 tasks      | elapsed:  2.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL TWO RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1440 out of 1440 | elapsed:  3.0min finished\n"
     ]
    }
   ],
   "source": [
    "ITERATIONS = 1\n",
    "for i in range(1):\n",
    "    #For reach \"trial\", randomly select 5000 data points\n",
    "    random5000DataPoints = gammaData.sample(n = 5000, replace = True)\n",
    "    #From that 5000, create x set and y set\n",
    "    xSet = random5000DataPoints.iloc[:,0:9].values\n",
    "    ySet = random5000DataPoints.iloc[:,10].values\n",
    "    ySet = encodeLabel.fit_transform(ySet)\n",
    "    # Run GridSearch to find best hyperparameters per 5000 data points \n",
    "    pipe = Pipeline([('std', StandardScaler()),('classifier', MLPClassifier(max_iter = 250))])\n",
    "\n",
    "    search_space = [\n",
    "                {\n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                {\n",
    "                 \n",
    "                  \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['sgd'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                \n",
    "                {\n",
    "                \n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['adam'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                ]\n",
    "    \n",
    "    runHyperParamSearch = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=5), \n",
    "                          scoring=['recall_micro', 'f1_micro','precision_micro','roc_auc_ovo','jaccard','neg_log_loss'], \n",
    "                          refit=False, verbose=10, n_jobs = -1)\n",
    "    TRIAL_2_MLP = runHyperParamSearch.fit(xSet, ySet)\n",
    "    print(\"---------------------------------TRIAL TWO RESULTS ---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.60372047, 0.61973314, 0.64665713, 0.86314282, 0.86154079,\n",
       "        0.84712787, 1.32123752, 1.37198071, 1.38318815, 1.6846499 ,\n",
       "        1.67133536, 1.70576577, 0.78037214, 0.64835653, 0.62603803,\n",
       "        0.81640253, 0.86514492, 0.89527183, 1.2960146 , 1.33704762,\n",
       "        1.39429984, 1.72908626, 1.64541659, 1.6113852 , 0.7340323 ,\n",
       "        0.64625301, 0.635147  , 0.83321681, 0.84282789, 0.88946447,\n",
       "        1.31703296, 1.34225426, 1.39540272, 1.71807661, 1.71697674,\n",
       "        1.6976573 , 0.70710254, 0.59480958, 0.52955518, 0.83291712,\n",
       "        0.85473747, 0.89607186, 1.31513071, 1.37117882, 1.3621726 ,\n",
       "        1.69325595, 1.63960876, 1.62359419, 0.80389209, 0.70030193,\n",
       "        0.68718972, 0.95992565, 0.95772414, 1.04449883, 1.46856575,\n",
       "        1.584864  , 1.58296037, 1.99191241, 1.9182467 , 1.91094184,\n",
       "        0.78086815, 0.71841793, 0.70410571, 0.97033377, 0.97464013,\n",
       "        1.04259763, 1.48187418, 1.54813166, 1.59336958, 1.97659926,\n",
       "        2.05476809, 1.96078668, 0.76275544, 0.70540648, 0.70560713,\n",
       "        0.9680336 , 0.98774891, 1.01797614, 1.48748069, 1.56824865,\n",
       "        1.59587317, 2.03945322, 1.93996739, 1.92795749, 0.76916151,\n",
       "        0.71691718, 0.67688279, 0.97363839, 0.98795071, 1.00096226,\n",
       "        1.50999942, 1.58456402, 1.59667377, 1.91114202, 1.95027566,\n",
       "        1.79294047, 2.03124552, 0.19606709, 2.28436413, 1.66993656,\n",
       "        0.1541327 , 2.28356428, 1.62539802, 0.19686971, 2.48673911,\n",
       "        1.98270516, 0.32017579, 2.51115928, 1.96999402, 0.17445045,\n",
       "        2.27615685, 1.68825188, 0.16794457, 2.31859369, 1.66893511,\n",
       "        0.16914539, 2.300879  , 1.74269919, 0.49392471, 2.45300941,\n",
       "        1.91764951, 0.17495041, 2.62435665, 1.88151855, 0.21618609,\n",
       "        2.44340119, 1.82887244, 0.20968037, 2.59563231, 1.73098898,\n",
       "        0.22489319, 2.5213686 , 2.09349995, 0.21618547, 2.49844885,\n",
       "        1.82326794, 0.20147333, 2.44259996, 1.83798132, 0.20317464,\n",
       "        2.5198669 , 1.8029501 , 0.22249045, 2.54638977, 1.09083824,\n",
       "        1.12326603, 1.73959627, 1.0329885 , 1.22455311, 1.60748196,\n",
       "        0.86584463, 1.61668963, 1.52401061, 0.88245916, 1.90013428,\n",
       "        1.67714229, 1.14648595, 1.22935729, 1.6812458 , 0.87465243,\n",
       "        0.85003138, 1.5170043 , 0.93660541, 1.6004766 , 1.65932693,\n",
       "        0.8389215 , 1.09293985, 1.48737926, 1.1041492 , 1.34765887,\n",
       "        1.57675581, 0.93960757, 1.37738452, 1.48197556, 0.96813245,\n",
       "        1.29751496, 1.58646426, 0.93920746, 1.33234563, 1.51950693,\n",
       "        1.00646515, 0.50973821, 1.41701856, 0.93830671, 1.63320451,\n",
       "        1.3990037 , 0.8612412 , 1.44194012, 1.50859723, 0.83671942,\n",
       "        2.05656829, 1.60387955, 1.84999051, 1.63000154, 1.92435503,\n",
       "        1.37508211, 1.36207204, 1.42302394, 1.41771936, 1.52651348,\n",
       "        1.35886922, 1.49038124, 1.21964912, 1.23396101, 1.818363  ,\n",
       "        1.94577265, 1.82937303, 1.47947259, 1.42572632, 1.44304056,\n",
       "        1.42402482, 1.47636967, 1.65021968, 1.52971582, 1.43303204,\n",
       "        1.4841764 , 1.99071221, 1.61458869, 1.83677964, 1.48998132,\n",
       "        1.39189725, 1.45014715, 1.37518291, 1.18001494, 1.33765049,\n",
       "        1.64291329, 1.37468219, 1.04519858, 1.82807202, 1.60798254,\n",
       "        1.50899739, 1.2606843 , 1.39339905, 1.32523971, 1.35976982,\n",
       "        1.33584828, 1.35136237, 1.34725885, 1.4513483 , 1.28890834,\n",
       "        1.98500705, 1.80415111, 2.11501913, 2.57031059, 2.67640147,\n",
       "        2.84474664, 3.23708425, 3.26771026, 3.38711329, 3.43935781,\n",
       "        3.51322098, 3.47448773, 1.86780648, 2.16356077, 2.13463573,\n",
       "        2.64777699, 2.35892911, 2.96765237, 3.26360664, 3.26931176,\n",
       "        3.24338965, 3.43095021, 3.42864804, 3.41884017, 1.88962483,\n",
       "        1.95998521, 2.06777749, 2.49824862, 2.77788863, 2.64967885,\n",
       "        3.18634028, 3.24549141, 3.28452439, 3.39431887, 3.42664661,\n",
       "        3.5132215 , 1.95327997, 1.88372025, 1.84178405, 2.55790014,\n",
       "        2.51586404, 2.23572268, 3.13619695, 3.02450089, 2.88838468,\n",
       "        3.30534239, 3.1459053 , 2.86836629]),\n",
       " 'std_fit_time': array([0.00727699, 0.02940285, 0.03037874, 0.03789969, 0.01379842,\n",
       "        0.03129325, 0.03796912, 0.01655053, 0.02812038, 0.06908695,\n",
       "        0.03695639, 0.02723807, 0.06306915, 0.03314801, 0.00723095,\n",
       "        0.01461824, 0.0359448 , 0.02818492, 0.03844234, 0.04908693,\n",
       "        0.05454336, 0.07149811, 0.05734306, 0.03433776, 0.07375418,\n",
       "        0.01944165, 0.01829265, 0.01314212, 0.02779626, 0.04413467,\n",
       "        0.05799261, 0.03022244, 0.0754616 , 0.02597993, 0.03218092,\n",
       "        0.05825569, 0.06258224, 0.05336273, 0.10280818, 0.02223282,\n",
       "        0.01728261, 0.03247594, 0.04261702, 0.04337276, 0.03384208,\n",
       "        0.02838475, 0.02346126, 0.02651464, 0.09927403, 0.03161293,\n",
       "        0.01809656, 0.02157885, 0.03753851, 0.02451526, 0.04365087,\n",
       "        0.04263111, 0.0398175 , 0.15775047, 0.07684749, 0.14196033,\n",
       "        0.04257401, 0.024591  , 0.02358985, 0.01934207, 0.03199914,\n",
       "        0.05508632, 0.05793508, 0.06466967, 0.02150361, 0.12450312,\n",
       "        0.16604592, 0.10157777, 0.06698645, 0.02439039, 0.02990131,\n",
       "        0.01413469, 0.02679353, 0.0339662 , 0.05066425, 0.04449479,\n",
       "        0.02635566, 0.15496994, 0.03628466, 0.12782443, 0.04719032,\n",
       "        0.01704606, 0.01673165, 0.02917726, 0.02868388, 0.02349912,\n",
       "        0.07022069, 0.10356802, 0.03094438, 0.05910004, 0.14400784,\n",
       "        0.11396682, 0.1653933 , 0.06143818, 0.05551789, 0.08455204,\n",
       "        0.00348152, 0.11841709, 0.09156937, 0.02690937, 0.19597073,\n",
       "        0.11410673, 0.10724289, 0.14341962, 0.10921079, 0.01440557,\n",
       "        0.16545196, 0.08561315, 0.0130445 , 0.1434546 , 0.077232  ,\n",
       "        0.00746961, 0.10522739, 0.1007691 , 0.56619131, 0.06246198,\n",
       "        0.10650874, 0.02452715, 0.11325537, 0.11726889, 0.07882354,\n",
       "        0.17934236, 0.18189488, 0.02884839, 0.10191952, 0.04000056,\n",
       "        0.01432523, 0.19043371, 0.16559302, 0.11541462, 0.06858428,\n",
       "        0.12034988, 0.04531629, 0.14984963, 0.10832776, 0.02410883,\n",
       "        0.09710183, 0.13008172, 0.02681386, 0.08223886, 0.25032742,\n",
       "        0.73719194, 0.22207948, 0.17793048, 0.80842511, 0.16280216,\n",
       "        0.14754411, 0.77472239, 0.13039449, 0.19377653, 0.66450743,\n",
       "        0.22067409, 0.27212676, 0.66768794, 0.193116  , 0.1134226 ,\n",
       "        0.66092168, 0.10054413, 0.14417065, 0.39972319, 0.15061797,\n",
       "        0.20926612, 0.42262122, 0.07974713, 0.16028555, 0.77782135,\n",
       "        0.09556881, 0.12195989, 0.93654042, 0.12512943, 0.09982181,\n",
       "        0.36804304, 0.19093744, 0.18108524, 0.36397534, 0.06502865,\n",
       "        0.07948619, 0.28638013, 0.14797518, 0.07204715, 0.87326775,\n",
       "        0.09762407, 0.09551599, 0.85305215, 0.11352572, 0.11214522,\n",
       "        0.88378679, 0.15043101, 0.37595715, 0.37359147, 0.38713586,\n",
       "        0.27875722, 0.20186889, 0.14716863, 0.18526986, 0.18428088,\n",
       "        0.21368008, 0.31803028, 0.18134091, 0.23933581, 0.09349924,\n",
       "        0.28201015, 0.23810791, 0.16676454, 0.30797287, 0.0964553 ,\n",
       "        0.2549295 , 0.33370951, 0.24669793, 0.19777576, 0.36423849,\n",
       "        0.12489513, 0.36812957, 0.31347146, 0.25598519, 0.2373528 ,\n",
       "        0.16323564, 0.21031354, 0.22492624, 0.12136028, 0.36180372,\n",
       "        0.22485067, 0.32199597, 0.30884384, 0.46867321, 0.09578559,\n",
       "        0.25771472, 0.09857765, 0.24739061, 0.18040702, 0.14096632,\n",
       "        0.20132456, 0.15580795, 0.38916383, 0.13854824, 0.19353733,\n",
       "        0.41525042, 0.29890178, 0.40547316, 0.47881681, 0.28435044,\n",
       "        0.29045991, 0.09768041, 0.04926969, 0.05214453, 0.07731385,\n",
       "        0.0261979 , 0.09381835, 0.44514182, 0.38570502, 0.37932034,\n",
       "        0.41641212, 0.73056755, 0.06250461, 0.10386295, 0.03524288,\n",
       "        0.04519974, 0.05725308, 0.07726703, 0.10339789, 0.23490073,\n",
       "        0.08333235, 0.35400382, 0.47796403, 0.20136708, 0.4789082 ,\n",
       "        0.04696739, 0.07174322, 0.04206765, 0.04825218, 0.04861429,\n",
       "        0.10180823, 0.29145236, 0.4584285 , 0.2922839 , 0.26987689,\n",
       "        0.39950469, 0.47912992, 0.13125802, 0.4694209 , 0.7074632 ,\n",
       "        0.03639894, 0.04876433, 0.16396837]),\n",
       " 'mean_score_time': array([0.01020861, 0.01110997, 0.01321077, 0.01871619, 0.01221066,\n",
       "        0.01231222, 0.01711493, 0.01521282, 0.01551466, 0.01481233,\n",
       "        0.01231203, 0.01231041, 0.01150784, 0.01020951, 0.01181016,\n",
       "        0.01241107, 0.01391168, 0.01331058, 0.01551309, 0.01621566,\n",
       "        0.01611309, 0.0139122 , 0.01551123, 0.01181002, 0.01121011,\n",
       "        0.01080928, 0.01120963, 0.0111083 , 0.01320977, 0.01301136,\n",
       "        0.01521378, 0.01611528, 0.01731534, 0.0142118 , 0.01200933,\n",
       "        0.01261096, 0.01131177, 0.01091061, 0.01090975, 0.01341219,\n",
       "        0.01200881, 0.01301007, 0.01431184, 0.01491394, 0.01591311,\n",
       "        0.01671429, 0.01351213, 0.01321192, 0.01290941, 0.0115098 ,\n",
       "        0.01111012, 0.01581302, 0.01401024, 0.01441278, 0.01601052,\n",
       "        0.01561365, 0.01681547, 0.01491175, 0.01411233, 0.01250992,\n",
       "        0.01201057, 0.01070962, 0.01281185, 0.01321211, 0.01280947,\n",
       "        0.0170146 , 0.01411309, 0.01501169, 0.01501188, 0.02081881,\n",
       "        0.0143115 , 0.01271019, 0.01171036, 0.01090913, 0.01131024,\n",
       "        0.01201029, 0.01291184, 0.01381073, 0.01471043, 0.01751695,\n",
       "        0.01401267, 0.0144125 , 0.01461329, 0.01241035, 0.011409  ,\n",
       "        0.0105092 , 0.01180954, 0.01170912, 0.01290984, 0.01301169,\n",
       "        0.01501169, 0.01641307, 0.01411219, 0.0135119 , 0.0123105 ,\n",
       "        0.01151071, 0.00900812, 0.00960903, 0.00930824, 0.0098083 ,\n",
       "        0.00950799, 0.01040926, 0.0110096 , 0.00990877, 0.01000824,\n",
       "        0.00982003, 0.00980783, 0.00970836, 0.01090946, 0.01060929,\n",
       "        0.01090975, 0.01201081, 0.00910773, 0.00960851, 0.00980816,\n",
       "        0.00940833, 0.01050887, 0.00990829, 0.01070948, 0.0133111 ,\n",
       "        0.01010847, 0.00950823, 0.01131001, 0.01160984, 0.01070886,\n",
       "        0.00970855, 0.01050925, 0.01100936, 0.01160975, 0.01050925,\n",
       "        0.01171026, 0.01090903, 0.00910835, 0.01040936, 0.00900888,\n",
       "        0.01020851, 0.00970802, 0.00990882, 0.01020823, 0.00960808,\n",
       "        0.01000938, 0.01110997, 0.0100091 , 0.01231074, 0.00970807,\n",
       "        0.0097084 , 0.0103086 , 0.01130929, 0.01090894, 0.01020885,\n",
       "        0.01010866, 0.01551375, 0.01030869, 0.01080933, 0.01120963,\n",
       "        0.01070948, 0.01000838, 0.01000876, 0.00960813, 0.00970821,\n",
       "        0.00980844, 0.0119103 , 0.0100091 , 0.01030874, 0.01191001,\n",
       "        0.01070957, 0.01070962, 0.01080947, 0.00980878, 0.00950837,\n",
       "        0.01010861, 0.00940824, 0.01231012, 0.00980787, 0.01040921,\n",
       "        0.01040955, 0.01010861, 0.01110997, 0.0106092 , 0.01000881,\n",
       "        0.00930805, 0.00910773, 0.00970845, 0.01100926, 0.00990891,\n",
       "        0.00960851, 0.01010876, 0.01000814, 0.0097084 , 0.01171012,\n",
       "        0.01020899, 0.01501236, 0.00930834, 0.00940795, 0.01000881,\n",
       "        0.00930829, 0.01030865, 0.00940824, 0.00990887, 0.01010885,\n",
       "        0.010709  , 0.01191034, 0.01090922, 0.00990872, 0.00920815,\n",
       "        0.00920811, 0.00980816, 0.00970864, 0.01030893, 0.01080914,\n",
       "        0.01040926, 0.01070933, 0.01010928, 0.00990853, 0.01010919,\n",
       "        0.01030884, 0.00980806, 0.00940809, 0.00900774, 0.00960832,\n",
       "        0.00960855, 0.01020875, 0.0098084 , 0.0098084 , 0.00980854,\n",
       "        0.00980868, 0.01050897, 0.01020904, 0.00920787, 0.00930796,\n",
       "        0.00920782, 0.00930777, 0.01020856, 0.01030898, 0.00970788,\n",
       "        0.00960851, 0.01000867, 0.01020827, 0.01110959, 0.01000867,\n",
       "        0.00980873, 0.01020865, 0.01060925, 0.01080904, 0.00980878,\n",
       "        0.01020875, 0.0119102 , 0.01100936, 0.0119101 , 0.01090937,\n",
       "        0.01271086, 0.01160994, 0.0096086 , 0.01100903, 0.01010895,\n",
       "        0.01000872, 0.01030869, 0.0102088 , 0.01271114, 0.01040931,\n",
       "        0.01201034, 0.0113101 , 0.01241078, 0.01110983, 0.01000843,\n",
       "        0.00920792, 0.00950875, 0.0097086 , 0.01120954, 0.00970817,\n",
       "        0.01080937, 0.01020861, 0.01080928, 0.01140966, 0.01531339,\n",
       "        0.01100965, 0.00960851, 0.00920792, 0.00970831, 0.01020832,\n",
       "        0.01010885, 0.01030917, 0.01100969, 0.01050944, 0.01110892,\n",
       "        0.00980854, 0.00800643, 0.0059051 ]),\n",
       " 'std_score_time': array([6.00139829e-04, 4.89561156e-04, 3.69783432e-03, 7.27629961e-03,\n",
       "        1.03143834e-03, 6.79555311e-04, 1.98756511e-03, 2.25248513e-03,\n",
       "        2.32547915e-03, 1.56703912e-03, 9.29080299e-04, 1.07920905e-03,\n",
       "        1.44844233e-03, 6.79625471e-04, 1.60079927e-03, 1.59583936e-03,\n",
       "        2.10919855e-03, 6.01142737e-04, 1.45148412e-03, 3.23802582e-03,\n",
       "        2.71225275e-03, 1.59526125e-03, 2.28224915e-03, 1.36464939e-03,\n",
       "        8.12973215e-04, 4.01187322e-04, 5.09698833e-04, 9.69291648e-04,\n",
       "        1.75215981e-03, 5.48685862e-04, 6.77807162e-04, 2.61701863e-03,\n",
       "        3.29522936e-03, 1.53859175e-03, 4.47767723e-04, 1.49764126e-03,\n",
       "        1.47232243e-03, 8.60996543e-04, 9.17159372e-04, 2.49989143e-03,\n",
       "        5.48903626e-04, 1.00317093e-03, 1.28942843e-03, 1.24187666e-03,\n",
       "        1.49888541e-03, 4.92990855e-03, 1.14113857e-03, 1.75029022e-03,\n",
       "        3.05576012e-03, 5.47727271e-04, 1.02109111e-03, 5.06934873e-03,\n",
       "        1.81930690e-03, 2.37569059e-03, 3.83701502e-03, 1.20210022e-03,\n",
       "        1.69293601e-03, 2.65491218e-03, 1.24262678e-03, 8.95562535e-04,\n",
       "        9.49092720e-04, 7.49163632e-04, 3.61539613e-03, 2.46500020e-03,\n",
       "        1.07831206e-03, 5.38119038e-03, 1.23943334e-03, 6.32938646e-04,\n",
       "        1.38031162e-03, 9.42879539e-03, 2.13640253e-03, 1.03231021e-03,\n",
       "        8.72400829e-04, 1.11534832e-03, 5.10186359e-04, 3.18774874e-04,\n",
       "        1.39315945e-03, 1.02986826e-03, 2.42443626e-03, 2.45201831e-03,\n",
       "        8.38937959e-04, 1.02068423e-03, 2.59823865e-03, 4.89727394e-04,\n",
       "        6.66379807e-04, 4.47928436e-04, 2.85804377e-03, 8.72028481e-04,\n",
       "        1.96272547e-03, 7.72879622e-04, 3.01996778e-03, 4.09481048e-03,\n",
       "        4.90039620e-04, 7.75187321e-04, 6.00417835e-04, 1.26598871e-03,\n",
       "        3.81469727e-07, 5.83961452e-04, 2.45496155e-04, 8.71951969e-04,\n",
       "        4.47341460e-04, 8.60531026e-04, 1.37970602e-03, 4.89706877e-04,\n",
       "        5.48249328e-04, 4.21054485e-04, 2.45281716e-04, 2.45223651e-04,\n",
       "        3.07525423e-03, 2.01152338e-03, 2.35557780e-03, 2.68505079e-03,\n",
       "        2.00200102e-04, 4.90125975e-04, 2.45554359e-04, 2.00320567e-04,\n",
       "        6.32862250e-04, 5.83903905e-04, 1.69273688e-03, 4.53816098e-03,\n",
       "        8.00788489e-04, 5.48379433e-04, 3.12653539e-03, 3.36971993e-03,\n",
       "        6.78472730e-04, 4.00948563e-04, 7.08471806e-04, 1.26610122e-03,\n",
       "        2.41840330e-03, 1.04939226e-03, 3.44735715e-03, 1.82914591e-03,\n",
       "        2.00486318e-04, 1.59548324e-03, 2.41733396e-06, 8.72438713e-04,\n",
       "        7.48813110e-04, 5.83421119e-04, 4.01151587e-04, 4.90797102e-04,\n",
       "        7.74694625e-04, 8.60525068e-04, 4.47448130e-04, 2.33886834e-03,\n",
       "        3.99995543e-04, 7.48869911e-04, 1.03065556e-03, 1.43657496e-03,\n",
       "        1.11485968e-03, 1.16697690e-03, 7.35462253e-04, 7.08380334e-03,\n",
       "        4.00639659e-04, 9.28086691e-04, 1.47069696e-03, 1.03108631e-03,\n",
       "        3.16581896e-04, 8.37653844e-04, 4.90252272e-04, 4.00877467e-04,\n",
       "        2.44912291e-04, 4.80408673e-03, 3.16205148e-04, 2.45673057e-04,\n",
       "        3.80358698e-03, 6.78823767e-04, 2.44912616e-04, 1.28998205e-03,\n",
       "        6.00457445e-04, 1.00088130e-03, 1.11458146e-03, 2.00152418e-04,\n",
       "        5.60517313e-03, 4.00806335e-04, 8.60552758e-04, 1.06868444e-03,\n",
       "        3.75273043e-04, 4.90923425e-04, 9.70429697e-04, 3.37174788e-07,\n",
       "        2.45048346e-04, 1.99961797e-04, 4.00340858e-04, 2.51229635e-03,\n",
       "        1.06866656e-03, 2.00271947e-04, 4.90067180e-04, 4.47607815e-04,\n",
       "        2.45963311e-04, 1.69273684e-03, 4.00114216e-04, 8.55694173e-03,\n",
       "        4.00007037e-04, 3.74534277e-04, 1.55058930e-03, 4.00758056e-04,\n",
       "        9.28215083e-04, 3.74801650e-04, 8.00871925e-04, 7.35046894e-04,\n",
       "        9.28143219e-04, 2.22484218e-03, 2.31267819e-03, 1.99580328e-04,\n",
       "        2.45320618e-04, 2.45165193e-04, 6.78795785e-04, 5.10407728e-04,\n",
       "        2.11383161e-03, 1.94096273e-03, 3.74584789e-04, 1.36513884e-03,\n",
       "        9.70769085e-04, 3.74508906e-04, 3.74355496e-04, 1.36496780e-03,\n",
       "        1.40135658e-03, 5.83535453e-04, 4.10190833e-07, 9.70282193e-04,\n",
       "        4.90504874e-04, 1.40147220e-03, 4.00292926e-04, 2.44873064e-04,\n",
       "        4.00603125e-04, 4.00495714e-04, 4.47714477e-04, 6.78640892e-04,\n",
       "        2.45457901e-04, 2.44775734e-04, 4.00138168e-04, 2.44912431e-04,\n",
       "        1.03071578e-03, 8.13184548e-04, 4.00686534e-04, 2.00271777e-04,\n",
       "        5.47987739e-04, 5.10258018e-04, 1.82929202e-03, 3.16431114e-04,\n",
       "        5.10248656e-04, 1.50452435e-03, 2.26927452e-03, 6.79041723e-04,\n",
       "        4.00543298e-04, 6.79182393e-04, 9.17083503e-04, 8.37283370e-04,\n",
       "        2.33424594e-03, 3.74304486e-04, 1.80709583e-03, 1.32013009e-03,\n",
       "        5.83952695e-04, 2.28244396e-03, 5.83723514e-04, 1.05000592e-03,\n",
       "        9.80289426e-04, 4.00567406e-04, 2.31726312e-03, 2.00129493e-04,\n",
       "        1.95116228e-03, 8.12832249e-04, 2.22452031e-03, 1.02111878e-03,\n",
       "        8.37397309e-04, 4.00209469e-04, 4.47287834e-04, 5.10613193e-04,\n",
       "        2.35988939e-03, 5.10781478e-04, 1.07782284e-03, 4.00662858e-04,\n",
       "        6.78619955e-04, 1.39371359e-03, 5.89217149e-03, 7.07999960e-04,\n",
       "        8.00490555e-04, 2.45125950e-04, 6.79006530e-04, 7.48564167e-04,\n",
       "        8.61162437e-04, 1.12329109e-03, 7.07696242e-04, 3.16280390e-04,\n",
       "        7.35416697e-04, 6.79013501e-04, 1.26581837e-03, 2.00486318e-04]),\n",
       " 'param_classifier': masked_array(data=[MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__activation': masked_array(data=['logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__alpha': masked_array(data=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__hidden_layer_sizes': masked_array(data=[(5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__learning_rate': masked_array(data=['constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'}],\n",
       " 'split0_test_recall_micro': array([0.763, 0.767, 0.76 , 0.762, 0.754, 0.755, 0.731, 0.753, 0.715,\n",
       "        0.715, 0.749, 0.741, 0.766, 0.765, 0.76 , 0.741, 0.748, 0.748,\n",
       "        0.743, 0.755, 0.722, 0.728, 0.755, 0.733, 0.765, 0.78 , 0.775,\n",
       "        0.761, 0.761, 0.772, 0.746, 0.749, 0.741, 0.747, 0.762, 0.743,\n",
       "        0.775, 0.773, 0.766, 0.763, 0.765, 0.779, 0.744, 0.755, 0.757,\n",
       "        0.745, 0.747, 0.748, 0.765, 0.77 , 0.782, 0.743, 0.762, 0.759,\n",
       "        0.734, 0.717, 0.742, 0.721, 0.714, 0.725, 0.775, 0.774, 0.76 ,\n",
       "        0.749, 0.757, 0.758, 0.749, 0.744, 0.727, 0.733, 0.746, 0.722,\n",
       "        0.768, 0.758, 0.763, 0.736, 0.748, 0.762, 0.753, 0.754, 0.745,\n",
       "        0.721, 0.734, 0.746, 0.771, 0.77 , 0.78 , 0.766, 0.754, 0.741,\n",
       "        0.748, 0.752, 0.741, 0.726, 0.751, 0.752, 0.77 , 0.758, 0.774,\n",
       "        0.777, 0.505, 0.771, 0.766, 0.495, 0.766, 0.766, 0.496, 0.769,\n",
       "        0.773, 0.48 , 0.767, 0.775, 0.646, 0.775, 0.775, 0.519, 0.774,\n",
       "        0.775, 0.498, 0.771, 0.769, 0.495, 0.771, 0.776, 0.495, 0.768,\n",
       "        0.767, 0.519, 0.77 , 0.772, 0.505, 0.772, 0.77 , 0.495, 0.771,\n",
       "        0.772, 0.495, 0.77 , 0.772, 0.328, 0.77 , 0.776, 0.495, 0.771,\n",
       "        0.772, 0.602, 0.769, 0.776, 0.661, 0.776, 0.76 , 0.495, 0.773,\n",
       "        0.766, 0.581, 0.766, 0.774, 0.574, 0.773, 0.769, 0.519, 0.766,\n",
       "        0.767, 0.548, 0.77 , 0.768, 0.722, 0.773, 0.763, 0.487, 0.765,\n",
       "        0.764, 0.592, 0.774, 0.776, 0.614, 0.771, 0.767, 0.649, 0.763,\n",
       "        0.77 , 0.519, 0.77 , 0.771, 0.519, 0.777, 0.779, 0.692, 0.768,\n",
       "        0.77 , 0.318, 0.771, 0.775, 0.776, 0.774, 0.772, 0.774, 0.776,\n",
       "        0.773, 0.774, 0.773, 0.776, 0.771, 0.771, 0.772, 0.772, 0.776,\n",
       "        0.775, 0.774, 0.775, 0.773, 0.772, 0.774, 0.771, 0.775, 0.776,\n",
       "        0.779, 0.773, 0.773, 0.774, 0.772, 0.776, 0.772, 0.77 , 0.771,\n",
       "        0.773, 0.774, 0.772, 0.774, 0.774, 0.773, 0.773, 0.772, 0.775,\n",
       "        0.773, 0.775, 0.775, 0.774, 0.773, 0.772, 0.778, 0.781, 0.782,\n",
       "        0.774, 0.777, 0.78 , 0.775, 0.778, 0.771, 0.777, 0.775, 0.782,\n",
       "        0.78 , 0.772, 0.773, 0.782, 0.78 , 0.78 , 0.771, 0.774, 0.771,\n",
       "        0.769, 0.773, 0.766, 0.774, 0.774, 0.775, 0.774, 0.773, 0.776,\n",
       "        0.778, 0.781, 0.772, 0.772, 0.775, 0.773, 0.775, 0.776, 0.767,\n",
       "        0.778, 0.774, 0.781, 0.774, 0.783, 0.777, 0.777, 0.78 , 0.781]),\n",
       " 'split1_test_recall_micro': array([0.76 , 0.778, 0.77 , 0.772, 0.769, 0.745, 0.74 , 0.731, 0.746,\n",
       "        0.716, 0.724, 0.733, 0.771, 0.76 , 0.769, 0.763, 0.753, 0.762,\n",
       "        0.751, 0.747, 0.747, 0.737, 0.732, 0.724, 0.771, 0.768, 0.774,\n",
       "        0.755, 0.772, 0.736, 0.739, 0.748, 0.753, 0.733, 0.735, 0.727,\n",
       "        0.766, 0.768, 0.766, 0.768, 0.752, 0.75 , 0.735, 0.752, 0.753,\n",
       "        0.738, 0.744, 0.753, 0.767, 0.769, 0.766, 0.75 , 0.747, 0.762,\n",
       "        0.758, 0.751, 0.731, 0.713, 0.728, 0.746, 0.765, 0.769, 0.764,\n",
       "        0.75 , 0.749, 0.768, 0.76 , 0.744, 0.732, 0.735, 0.717, 0.736,\n",
       "        0.781, 0.764, 0.776, 0.763, 0.758, 0.762, 0.757, 0.727, 0.745,\n",
       "        0.751, 0.717, 0.731, 0.777, 0.769, 0.767, 0.76 , 0.772, 0.743,\n",
       "        0.761, 0.739, 0.751, 0.745, 0.728, 0.732, 0.78 , 0.496, 0.78 ,\n",
       "        0.775, 0.587, 0.778, 0.783, 0.506, 0.775, 0.777, 0.494, 0.778,\n",
       "        0.778, 0.525, 0.785, 0.781, 0.508, 0.775, 0.783, 0.616, 0.782,\n",
       "        0.785, 0.494, 0.781, 0.783, 0.488, 0.781, 0.784, 0.506, 0.782,\n",
       "        0.778, 0.494, 0.782, 0.782, 0.535, 0.787, 0.775, 0.506, 0.787,\n",
       "        0.784, 0.493, 0.784, 0.782, 0.568, 0.783, 0.781, 0.618, 0.786,\n",
       "        0.785, 0.6  , 0.774, 0.783, 0.495, 0.78 , 0.776, 0.647, 0.786,\n",
       "        0.78 , 0.662, 0.786, 0.78 , 0.581, 0.781, 0.774, 0.632, 0.783,\n",
       "        0.776, 0.533, 0.781, 0.784, 0.727, 0.785, 0.784, 0.572, 0.777,\n",
       "        0.783, 0.467, 0.779, 0.781, 0.657, 0.782, 0.786, 0.655, 0.777,\n",
       "        0.776, 0.648, 0.777, 0.779, 0.509, 0.781, 0.777, 0.547, 0.783,\n",
       "        0.783, 0.59 , 0.784, 0.785, 0.784, 0.787, 0.786, 0.784, 0.786,\n",
       "        0.786, 0.786, 0.785, 0.784, 0.784, 0.787, 0.784, 0.786, 0.785,\n",
       "        0.784, 0.784, 0.787, 0.784, 0.786, 0.786, 0.786, 0.785, 0.786,\n",
       "        0.789, 0.787, 0.786, 0.784, 0.785, 0.785, 0.785, 0.787, 0.788,\n",
       "        0.786, 0.784, 0.785, 0.786, 0.782, 0.782, 0.784, 0.785, 0.787,\n",
       "        0.786, 0.785, 0.786, 0.785, 0.785, 0.785, 0.777, 0.784, 0.775,\n",
       "        0.775, 0.778, 0.77 , 0.78 , 0.775, 0.774, 0.76 , 0.777, 0.771,\n",
       "        0.775, 0.778, 0.783, 0.782, 0.779, 0.775, 0.771, 0.778, 0.777,\n",
       "        0.767, 0.768, 0.762, 0.784, 0.779, 0.784, 0.775, 0.778, 0.782,\n",
       "        0.776, 0.773, 0.777, 0.774, 0.771, 0.769, 0.778, 0.778, 0.779,\n",
       "        0.78 , 0.779, 0.777, 0.776, 0.787, 0.777, 0.774, 0.773, 0.766]),\n",
       " 'split2_test_recall_micro': array([0.784, 0.795, 0.786, 0.768, 0.769, 0.786, 0.765, 0.753, 0.767,\n",
       "        0.747, 0.742, 0.749, 0.786, 0.779, 0.787, 0.755, 0.758, 0.772,\n",
       "        0.752, 0.758, 0.753, 0.749, 0.733, 0.761, 0.793, 0.787, 0.771,\n",
       "        0.752, 0.758, 0.777, 0.782, 0.755, 0.741, 0.764, 0.734, 0.749,\n",
       "        0.803, 0.789, 0.789, 0.772, 0.788, 0.77 , 0.753, 0.759, 0.769,\n",
       "        0.765, 0.768, 0.759, 0.77 , 0.793, 0.792, 0.759, 0.755, 0.775,\n",
       "        0.753, 0.769, 0.762, 0.741, 0.736, 0.767, 0.778, 0.773, 0.773,\n",
       "        0.738, 0.769, 0.766, 0.775, 0.761, 0.749, 0.734, 0.752, 0.756,\n",
       "        0.792, 0.78 , 0.784, 0.761, 0.774, 0.757, 0.766, 0.749, 0.76 ,\n",
       "        0.737, 0.752, 0.738, 0.791, 0.771, 0.783, 0.781, 0.772, 0.784,\n",
       "        0.767, 0.764, 0.761, 0.743, 0.735, 0.763, 0.786, 0.477, 0.785,\n",
       "        0.791, 0.502, 0.791, 0.788, 0.586, 0.789, 0.786, 0.506, 0.79 ,\n",
       "        0.783, 0.494, 0.786, 0.786, 0.511, 0.785, 0.784, 0.401, 0.794,\n",
       "        0.789, 0.506, 0.791, 0.788, 0.358, 0.788, 0.791, 0.506, 0.788,\n",
       "        0.792, 0.494, 0.786, 0.789, 0.505, 0.786, 0.788, 0.526, 0.791,\n",
       "        0.789, 0.494, 0.791, 0.795, 0.494, 0.791, 0.791, 0.579, 0.791,\n",
       "        0.78 , 0.585, 0.803, 0.79 , 0.625, 0.791, 0.789, 0.616, 0.786,\n",
       "        0.784, 0.694, 0.787, 0.78 , 0.469, 0.785, 0.782, 0.656, 0.787,\n",
       "        0.782, 0.638, 0.789, 0.791, 0.665, 0.79 , 0.783, 0.469, 0.788,\n",
       "        0.786, 0.685, 0.797, 0.772, 0.619, 0.781, 0.795, 0.719, 0.796,\n",
       "        0.781, 0.765, 0.788, 0.797, 0.48 , 0.794, 0.783, 0.677, 0.789,\n",
       "        0.791, 0.641, 0.782, 0.794, 0.795, 0.796, 0.796, 0.796, 0.796,\n",
       "        0.796, 0.797, 0.797, 0.797, 0.796, 0.798, 0.793, 0.796, 0.798,\n",
       "        0.796, 0.793, 0.793, 0.792, 0.796, 0.797, 0.797, 0.793, 0.797,\n",
       "        0.796, 0.792, 0.793, 0.795, 0.795, 0.793, 0.795, 0.796, 0.798,\n",
       "        0.795, 0.797, 0.792, 0.796, 0.795, 0.791, 0.796, 0.795, 0.795,\n",
       "        0.795, 0.798, 0.794, 0.796, 0.794, 0.794, 0.795, 0.794, 0.791,\n",
       "        0.793, 0.797, 0.791, 0.793, 0.788, 0.784, 0.78 , 0.785, 0.79 ,\n",
       "        0.796, 0.795, 0.791, 0.795, 0.797, 0.792, 0.786, 0.786, 0.792,\n",
       "        0.787, 0.784, 0.782, 0.794, 0.794, 0.79 , 0.793, 0.788, 0.796,\n",
       "        0.786, 0.79 , 0.796, 0.788, 0.783, 0.785, 0.792, 0.793, 0.797,\n",
       "        0.789, 0.798, 0.787, 0.786, 0.793, 0.793, 0.796, 0.79 , 0.786]),\n",
       " 'split3_test_recall_micro': array([0.732, 0.757, 0.749, 0.745, 0.746, 0.757, 0.74 , 0.751, 0.728,\n",
       "        0.734, 0.748, 0.728, 0.753, 0.743, 0.752, 0.732, 0.755, 0.743,\n",
       "        0.731, 0.743, 0.748, 0.729, 0.734, 0.74 , 0.749, 0.746, 0.748,\n",
       "        0.749, 0.748, 0.753, 0.748, 0.74 , 0.728, 0.739, 0.736, 0.733,\n",
       "        0.758, 0.753, 0.752, 0.755, 0.764, 0.759, 0.758, 0.744, 0.76 ,\n",
       "        0.745, 0.753, 0.745, 0.757, 0.746, 0.756, 0.745, 0.765, 0.753,\n",
       "        0.745, 0.734, 0.765, 0.742, 0.746, 0.748, 0.76 , 0.751, 0.765,\n",
       "        0.75 , 0.74 , 0.752, 0.744, 0.746, 0.755, 0.719, 0.728, 0.719,\n",
       "        0.767, 0.764, 0.751, 0.749, 0.746, 0.764, 0.734, 0.746, 0.762,\n",
       "        0.745, 0.736, 0.743, 0.776, 0.764, 0.753, 0.751, 0.755, 0.738,\n",
       "        0.76 , 0.738, 0.768, 0.731, 0.739, 0.75 , 0.754, 0.494, 0.76 ,\n",
       "        0.767, 0.532, 0.766, 0.761, 0.516, 0.757, 0.759, 0.493, 0.767,\n",
       "        0.762, 0.527, 0.764, 0.764, 0.601, 0.763, 0.762, 0.505, 0.763,\n",
       "        0.761, 0.583, 0.763, 0.755, 0.494, 0.764, 0.76 , 0.653, 0.759,\n",
       "        0.759, 0.506, 0.766, 0.763, 0.574, 0.758, 0.76 , 0.516, 0.759,\n",
       "        0.759, 0.5  , 0.757, 0.763, 0.506, 0.768, 0.762, 0.484, 0.761,\n",
       "        0.76 , 0.666, 0.756, 0.764, 0.711, 0.761, 0.763, 0.672, 0.759,\n",
       "        0.756, 0.639, 0.752, 0.748, 0.585, 0.752, 0.754, 0.605, 0.759,\n",
       "        0.755, 0.661, 0.754, 0.757, 0.653, 0.753, 0.747, 0.315, 0.755,\n",
       "        0.755, 0.399, 0.748, 0.76 , 0.609, 0.747, 0.754, 0.686, 0.748,\n",
       "        0.758, 0.586, 0.753, 0.758, 0.556, 0.75 , 0.759, 0.564, 0.761,\n",
       "        0.762, 0.433, 0.754, 0.75 , 0.752, 0.752, 0.752, 0.752, 0.75 ,\n",
       "        0.749, 0.749, 0.75 , 0.753, 0.751, 0.751, 0.751, 0.746, 0.749,\n",
       "        0.752, 0.753, 0.751, 0.749, 0.752, 0.75 , 0.752, 0.751, 0.753,\n",
       "        0.757, 0.75 , 0.756, 0.752, 0.751, 0.753, 0.752, 0.752, 0.749,\n",
       "        0.752, 0.751, 0.751, 0.752, 0.754, 0.754, 0.754, 0.752, 0.753,\n",
       "        0.752, 0.749, 0.751, 0.751, 0.748, 0.754, 0.758, 0.754, 0.755,\n",
       "        0.761, 0.749, 0.753, 0.753, 0.763, 0.756, 0.754, 0.754, 0.759,\n",
       "        0.755, 0.751, 0.752, 0.752, 0.747, 0.757, 0.756, 0.758, 0.748,\n",
       "        0.757, 0.756, 0.757, 0.759, 0.754, 0.748, 0.75 , 0.754, 0.758,\n",
       "        0.756, 0.762, 0.754, 0.759, 0.76 , 0.757, 0.755, 0.749, 0.751,\n",
       "        0.758, 0.757, 0.754, 0.758, 0.755, 0.763, 0.756, 0.759, 0.754]),\n",
       " 'split4_test_recall_micro': array([0.744, 0.768, 0.764, 0.75 , 0.764, 0.78 , 0.752, 0.738, 0.735,\n",
       "        0.73 , 0.744, 0.731, 0.775, 0.768, 0.775, 0.741, 0.744, 0.759,\n",
       "        0.74 , 0.726, 0.725, 0.728, 0.724, 0.729, 0.774, 0.769, 0.77 ,\n",
       "        0.749, 0.771, 0.752, 0.736, 0.728, 0.732, 0.73 , 0.723, 0.721,\n",
       "        0.787, 0.777, 0.775, 0.747, 0.746, 0.771, 0.762, 0.739, 0.758,\n",
       "        0.756, 0.729, 0.736, 0.763, 0.752, 0.774, 0.726, 0.754, 0.762,\n",
       "        0.729, 0.732, 0.743, 0.725, 0.712, 0.743, 0.762, 0.769, 0.781,\n",
       "        0.761, 0.735, 0.734, 0.743, 0.747, 0.719, 0.747, 0.74 , 0.7  ,\n",
       "        0.77 , 0.763, 0.769, 0.744, 0.754, 0.761, 0.744, 0.731, 0.736,\n",
       "        0.735, 0.707, 0.734, 0.777, 0.761, 0.773, 0.744, 0.753, 0.757,\n",
       "        0.723, 0.735, 0.749, 0.736, 0.731, 0.736, 0.784, 0.506, 0.783,\n",
       "        0.777, 0.386, 0.776, 0.789, 0.472, 0.784, 0.78 , 0.492, 0.781,\n",
       "        0.787, 0.667, 0.785, 0.778, 0.494, 0.774, 0.779, 0.495, 0.776,\n",
       "        0.785, 0.494, 0.781, 0.779, 0.506, 0.788, 0.778, 0.494, 0.783,\n",
       "        0.784, 0.621, 0.778, 0.784, 0.565, 0.777, 0.787, 0.612, 0.776,\n",
       "        0.777, 0.494, 0.776, 0.788, 0.494, 0.779, 0.781, 0.529, 0.782,\n",
       "        0.78 , 0.54 , 0.774, 0.776, 0.484, 0.782, 0.777, 0.484, 0.79 ,\n",
       "        0.781, 0.577, 0.779, 0.784, 0.532, 0.778, 0.785, 0.694, 0.786,\n",
       "        0.78 , 0.606, 0.782, 0.785, 0.666, 0.783, 0.777, 0.706, 0.766,\n",
       "        0.786, 0.605, 0.783, 0.779, 0.587, 0.781, 0.785, 0.52 , 0.783,\n",
       "        0.788, 0.529, 0.771, 0.777, 0.539, 0.788, 0.772, 0.441, 0.784,\n",
       "        0.782, 0.589, 0.773, 0.782, 0.784, 0.79 , 0.785, 0.785, 0.786,\n",
       "        0.787, 0.784, 0.781, 0.786, 0.785, 0.786, 0.784, 0.784, 0.783,\n",
       "        0.788, 0.784, 0.784, 0.785, 0.784, 0.783, 0.784, 0.783, 0.785,\n",
       "        0.785, 0.782, 0.786, 0.783, 0.782, 0.785, 0.787, 0.787, 0.784,\n",
       "        0.783, 0.785, 0.783, 0.785, 0.784, 0.786, 0.784, 0.784, 0.785,\n",
       "        0.785, 0.785, 0.784, 0.784, 0.785, 0.785, 0.784, 0.773, 0.78 ,\n",
       "        0.779, 0.78 , 0.787, 0.772, 0.777, 0.773, 0.769, 0.775, 0.781,\n",
       "        0.786, 0.789, 0.778, 0.779, 0.783, 0.781, 0.776, 0.768, 0.778,\n",
       "        0.76 , 0.775, 0.774, 0.786, 0.786, 0.784, 0.78 , 0.789, 0.782,\n",
       "        0.772, 0.766, 0.775, 0.772, 0.77 , 0.774, 0.787, 0.784, 0.791,\n",
       "        0.789, 0.784, 0.781, 0.779, 0.777, 0.775, 0.779, 0.785, 0.779]),\n",
       " 'mean_test_recall_micro': array([0.7566, 0.773 , 0.7658, 0.7594, 0.7604, 0.7646, 0.7456, 0.7452,\n",
       "        0.7382, 0.7284, 0.7414, 0.7364, 0.7702, 0.763 , 0.7686, 0.7464,\n",
       "        0.7516, 0.7568, 0.7434, 0.7458, 0.739 , 0.7342, 0.7356, 0.7374,\n",
       "        0.7704, 0.77  , 0.7676, 0.7532, 0.762 , 0.758 , 0.7502, 0.744 ,\n",
       "        0.739 , 0.7426, 0.738 , 0.7346, 0.7778, 0.772 , 0.7696, 0.761 ,\n",
       "        0.763 , 0.7658, 0.7504, 0.7498, 0.7594, 0.7498, 0.7482, 0.7482,\n",
       "        0.7644, 0.766 , 0.774 , 0.7446, 0.7566, 0.7622, 0.7438, 0.7406,\n",
       "        0.7486, 0.7284, 0.7272, 0.7458, 0.768 , 0.7672, 0.7686, 0.7496,\n",
       "        0.75  , 0.7556, 0.7542, 0.7484, 0.7364, 0.7336, 0.7366, 0.7266,\n",
       "        0.7756, 0.7658, 0.7686, 0.7506, 0.756 , 0.7612, 0.7508, 0.7414,\n",
       "        0.7496, 0.7378, 0.7292, 0.7384, 0.7784, 0.767 , 0.7712, 0.7604,\n",
       "        0.7612, 0.7526, 0.7518, 0.7456, 0.754 , 0.7362, 0.7368, 0.7466,\n",
       "        0.7748, 0.5462, 0.7764, 0.7774, 0.5024, 0.7764, 0.7774, 0.515 ,\n",
       "        0.7742, 0.7736, 0.4962, 0.777 , 0.7766, 0.5386, 0.7774, 0.7768,\n",
       "        0.552 , 0.7744, 0.7766, 0.5072, 0.7778, 0.779 , 0.515 , 0.7774,\n",
       "        0.7748, 0.4682, 0.7784, 0.7778, 0.5308, 0.776 , 0.776 , 0.5268,\n",
       "        0.7764, 0.778 , 0.5368, 0.776 , 0.776 , 0.531 , 0.7768, 0.7762,\n",
       "        0.4952, 0.7756, 0.78  , 0.478 , 0.7782, 0.7782, 0.541 , 0.7782,\n",
       "        0.7754, 0.5986, 0.7752, 0.7778, 0.5952, 0.778 , 0.773 , 0.5828,\n",
       "        0.7788, 0.7734, 0.6306, 0.774 , 0.7732, 0.5482, 0.7738, 0.7728,\n",
       "        0.6212, 0.7762, 0.772 , 0.5972, 0.7752, 0.777 , 0.6866, 0.7768,\n",
       "        0.7708, 0.5098, 0.7702, 0.7748, 0.5496, 0.7762, 0.7736, 0.6172,\n",
       "        0.7724, 0.7774, 0.6458, 0.7734, 0.7746, 0.6094, 0.7718, 0.7764,\n",
       "        0.5206, 0.778 , 0.774 , 0.5842, 0.777 , 0.7776, 0.5142, 0.7728,\n",
       "        0.7772, 0.7782, 0.7798, 0.7782, 0.7782, 0.7788, 0.7782, 0.778 ,\n",
       "        0.7772, 0.7792, 0.7774, 0.7786, 0.7768, 0.7768, 0.7782, 0.779 ,\n",
       "        0.7776, 0.778 , 0.7766, 0.778 , 0.778 , 0.778 , 0.7774, 0.7794,\n",
       "        0.7812, 0.7768, 0.7788, 0.7776, 0.777 , 0.7784, 0.7782, 0.7784,\n",
       "        0.778 , 0.7778, 0.7782, 0.7766, 0.7786, 0.7778, 0.7772, 0.7782,\n",
       "        0.7776, 0.779 , 0.7782, 0.7784, 0.778 , 0.778 , 0.777 , 0.778 ,\n",
       "        0.7784, 0.7772, 0.7766, 0.7764, 0.7762, 0.7762, 0.7746, 0.7762,\n",
       "        0.7716, 0.768 , 0.7732, 0.7766, 0.7784, 0.777 , 0.7754, 0.778 ,\n",
       "        0.7772, 0.777 , 0.772 , 0.7728, 0.7732, 0.768 , 0.7712, 0.7682,\n",
       "        0.7794, 0.7774, 0.7762, 0.7744, 0.7764, 0.7788, 0.7736, 0.7744,\n",
       "        0.7748, 0.773 , 0.7718, 0.7716, 0.7774, 0.776 , 0.777 , 0.7788,\n",
       "        0.7784, 0.776 , 0.7746, 0.779 , 0.777 , 0.7764, 0.7774, 0.7732]),\n",
       " 'std_test_recall_micro': array([0.01770424, 0.01285302, 0.01220492, 0.01034601, 0.00904655,\n",
       "        0.01567929, 0.01177455, 0.00904212, 0.01756588, 0.0119432 ,\n",
       "        0.00906863, 0.00763151, 0.01083328, 0.01178134, 0.01207642,\n",
       "        0.01109234, 0.005004  , 0.0103034 , 0.00770973, 0.01126765,\n",
       "        0.01285302, 0.00813388, 0.01032666, 0.01290891, 0.01422111,\n",
       "        0.01392839, 0.00997196, 0.00448999, 0.00887694, 0.01484587,\n",
       "        0.01649727, 0.00931665, 0.0086487 , 0.01217539, 0.0128841 ,\n",
       "        0.01022937, 0.01586695, 0.01176435, 0.01217539, 0.0090111 ,\n",
       "        0.01442221, 0.01014692, 0.00976934, 0.00730479, 0.00531413,\n",
       "        0.0095373 , 0.01267123, 0.00773046, 0.00436348, 0.01643168,\n",
       "        0.01245793, 0.0108185 , 0.0063435 , 0.00719444, 0.01097998,\n",
       "        0.01782807, 0.01290891, 0.01137717, 0.01293677, 0.01337759,\n",
       "        0.00718331, 0.00835225, 0.00749933, 0.00728286, 0.0121326 ,\n",
       "        0.01222457, 0.01202331, 0.00640625, 0.01352923, 0.00889044,\n",
       "        0.01261111, 0.01865047, 0.00960417, 0.00744043, 0.01125344,\n",
       "        0.0102098 , 0.00995992, 0.00231517, 0.01097998, 0.01051856,\n",
       "        0.00989141, 0.01016661, 0.01568949, 0.00553534, 0.00668132,\n",
       "        0.00384708, 0.01066583, 0.01275304, 0.00884081, 0.01700118,\n",
       "        0.01566397, 0.0108922 , 0.00946573, 0.00713863, 0.00800999,\n",
       "        0.0112712 , 0.01177115, 0.10630974, 0.00900222, 0.00773563,\n",
       "        0.06571027, 0.00840476, 0.01163787, 0.03839792, 0.01165161,\n",
       "        0.00976934, 0.00507543, 0.0083666 , 0.00868562, 0.06667713,\n",
       "        0.00976934, 0.00735935, 0.06036224, 0.00697424, 0.00796492,\n",
       "        0.06842339, 0.01016661, 0.01011929, 0.03428119, 0.00958332,\n",
       "        0.01170299, 0.05540541, 0.0095205 , 0.01032279, 0.06131688,\n",
       "        0.01078888, 0.01178134, 0.0479975 , 0.00741889, 0.00931665,\n",
       "        0.02899931, 0.01060189, 0.01056409, 0.04178995, 0.01146124,\n",
       "        0.01038075, 0.00248193, 0.01170641, 0.01136662, 0.07986989,\n",
       "        0.00847113, 0.00945304, 0.05075825, 0.01083328, 0.00875443,\n",
       "        0.04043563, 0.01538051, 0.00863481, 0.09059007, 0.00981835,\n",
       "        0.01048809, 0.07829534, 0.01144378, 0.01068831, 0.04562718,\n",
       "        0.01331165, 0.01299846, 0.04388804, 0.01158275, 0.01097998,\n",
       "        0.0588809 , 0.01147868, 0.00993982, 0.04970875, 0.01222129,\n",
       "        0.01256981, 0.03132156, 0.01312098, 0.01406272, 0.12844828,\n",
       "        0.0113031 , 0.01285924, 0.10266762, 0.01604244, 0.0074458 ,\n",
       "        0.02270154, 0.01332066, 0.01481351, 0.06766801, 0.01654811,\n",
       "        0.01019019, 0.09042699, 0.01137365, 0.01264278, 0.02598923,\n",
       "        0.01516575, 0.00829458, 0.09222885, 0.01063955, 0.01028786,\n",
       "        0.12042159, 0.01064707, 0.01490503, 0.01442775, 0.0156512 ,\n",
       "        0.01515784, 0.01483779, 0.01572768, 0.01633891, 0.01623576,\n",
       "        0.0156512 , 0.01471598, 0.0153961 , 0.01625546, 0.01452446,\n",
       "        0.01718604, 0.01624069, 0.01509967, 0.01369087, 0.01469694,\n",
       "        0.01508111, 0.01507315, 0.01581139, 0.0154013 , 0.01438888,\n",
       "        0.01478648, 0.01330263, 0.0147973 , 0.01310572, 0.01443052,\n",
       "        0.01492649, 0.01379275, 0.01503862, 0.01565375, 0.01688787,\n",
       "        0.01468877, 0.01543243, 0.01431922, 0.01501466, 0.0136587 ,\n",
       "        0.01301384, 0.01411949, 0.01473228, 0.01447757, 0.01485126,\n",
       "        0.01641463, 0.01479189, 0.0151921 , 0.01596246, 0.01389964,\n",
       "        0.01204326, 0.01340746, 0.01197664, 0.0102684 , 0.01543243,\n",
       "        0.0136147 , 0.01297074, 0.00798499, 0.00900222, 0.00985901,\n",
       "        0.01028397, 0.01066958, 0.01363231, 0.01529706, 0.01312402,\n",
       "        0.01412799, 0.01642437, 0.01143678, 0.00969536, 0.00943186,\n",
       "        0.01435827, 0.010469  , 0.00919565, 0.00886341, 0.01202664,\n",
       "        0.01349963, 0.01489161, 0.01395134, 0.01272164, 0.01230285,\n",
       "        0.00991161, 0.01013114, 0.01337759, 0.00920869, 0.00746726,\n",
       "        0.00902441, 0.01275304, 0.01473771, 0.01658915, 0.01133843,\n",
       "        0.01336563, 0.01145426, 0.00924338, 0.01308434, 0.00954987,\n",
       "        0.01275304, 0.01078147, 0.01165161]),\n",
       " 'rank_test_recall_micro': array([198, 147, 180, 194, 192, 183, 225, 227, 239, 253, 233, 245, 165,\n",
       "        186, 171, 222, 207, 197, 231, 223, 237, 250, 248, 242, 164, 167,\n",
       "        176, 204, 188, 196, 211, 229, 236, 232, 240, 249,  54, 154, 168,\n",
       "        191, 185, 180, 210, 214, 195, 213, 219, 220, 184, 179, 134, 228,\n",
       "        199, 187, 230, 235, 217, 253, 255, 223, 173, 177, 169, 215, 212,\n",
       "        201, 202, 218, 245, 251, 244, 256, 117, 180, 169, 209, 200, 189,\n",
       "        208, 233, 215, 241, 252, 238,  18, 178, 161, 193, 189, 205, 206,\n",
       "        225, 203, 247, 243, 221, 123, 271,  99,  62, 284,  99,  62, 280,\n",
       "        133, 138, 285,  79,  92, 273,  61,  89, 268, 131,  92, 283,  51,\n",
       "          7, 279,  62, 125, 288,  18,  54, 276, 111, 111, 277,  99,  45,\n",
       "        274, 111, 111, 275,  85, 104, 286, 117,   2, 287,  27,  27, 272,\n",
       "         27, 120, 263, 121,  51, 265,  42, 147, 267,  12, 141, 259, 134,\n",
       "        146, 270, 137, 150, 260, 104, 154, 264, 121,  79, 257,  89, 163,\n",
       "        282, 165, 124, 269, 104, 140, 261, 153,  62, 258, 141, 127, 262,\n",
       "        157,  99, 278,  38, 134, 266,  76,  58, 281, 150,  71,  27,   3,\n",
       "         27,  27,  12,  26,  38,  71,   6,  62,  16,  85,  85,  27,   7,\n",
       "         58,  45,  91,  45,  42,  45,  62,   4,   1,  85,  11,  57,  82,\n",
       "         18,  27,  25,  38,  54,  27,  92,  16,  51,  71,  27,  58,   7,\n",
       "         27,  18,  45,  38,  76,  42,  18,  71,  92,  99, 104, 104, 127,\n",
       "        104, 159, 173, 143,  92,  24,  79, 119,  45,  71,  76, 154, 152,\n",
       "        143, 173, 161, 172,   4,  62, 104, 131,  97,  12, 138, 130, 125,\n",
       "        147, 157, 159,  70, 111,  82,  12,  18, 111, 127,  10,  82,  97,\n",
       "         62, 143]),\n",
       " 'split0_test_f1_micro': array([0.763, 0.767, 0.76 , 0.762, 0.754, 0.755, 0.731, 0.753, 0.715,\n",
       "        0.715, 0.749, 0.741, 0.766, 0.765, 0.76 , 0.741, 0.748, 0.748,\n",
       "        0.743, 0.755, 0.722, 0.728, 0.755, 0.733, 0.765, 0.78 , 0.775,\n",
       "        0.761, 0.761, 0.772, 0.746, 0.749, 0.741, 0.747, 0.762, 0.743,\n",
       "        0.775, 0.773, 0.766, 0.763, 0.765, 0.779, 0.744, 0.755, 0.757,\n",
       "        0.745, 0.747, 0.748, 0.765, 0.77 , 0.782, 0.743, 0.762, 0.759,\n",
       "        0.734, 0.717, 0.742, 0.721, 0.714, 0.725, 0.775, 0.774, 0.76 ,\n",
       "        0.749, 0.757, 0.758, 0.749, 0.744, 0.727, 0.733, 0.746, 0.722,\n",
       "        0.768, 0.758, 0.763, 0.736, 0.748, 0.762, 0.753, 0.754, 0.745,\n",
       "        0.721, 0.734, 0.746, 0.771, 0.77 , 0.78 , 0.766, 0.754, 0.741,\n",
       "        0.748, 0.752, 0.741, 0.726, 0.751, 0.752, 0.77 , 0.758, 0.774,\n",
       "        0.777, 0.505, 0.771, 0.766, 0.495, 0.766, 0.766, 0.496, 0.769,\n",
       "        0.773, 0.48 , 0.767, 0.775, 0.646, 0.775, 0.775, 0.519, 0.774,\n",
       "        0.775, 0.498, 0.771, 0.769, 0.495, 0.771, 0.776, 0.495, 0.768,\n",
       "        0.767, 0.519, 0.77 , 0.772, 0.505, 0.772, 0.77 , 0.495, 0.771,\n",
       "        0.772, 0.495, 0.77 , 0.772, 0.328, 0.77 , 0.776, 0.495, 0.771,\n",
       "        0.772, 0.602, 0.769, 0.776, 0.661, 0.776, 0.76 , 0.495, 0.773,\n",
       "        0.766, 0.581, 0.766, 0.774, 0.574, 0.773, 0.769, 0.519, 0.766,\n",
       "        0.767, 0.548, 0.77 , 0.768, 0.722, 0.773, 0.763, 0.487, 0.765,\n",
       "        0.764, 0.592, 0.774, 0.776, 0.614, 0.771, 0.767, 0.649, 0.763,\n",
       "        0.77 , 0.519, 0.77 , 0.771, 0.519, 0.777, 0.779, 0.692, 0.768,\n",
       "        0.77 , 0.318, 0.771, 0.775, 0.776, 0.774, 0.772, 0.774, 0.776,\n",
       "        0.773, 0.774, 0.773, 0.776, 0.771, 0.771, 0.772, 0.772, 0.776,\n",
       "        0.775, 0.774, 0.775, 0.773, 0.772, 0.774, 0.771, 0.775, 0.776,\n",
       "        0.779, 0.773, 0.773, 0.774, 0.772, 0.776, 0.772, 0.77 , 0.771,\n",
       "        0.773, 0.774, 0.772, 0.774, 0.774, 0.773, 0.773, 0.772, 0.775,\n",
       "        0.773, 0.775, 0.775, 0.774, 0.773, 0.772, 0.778, 0.781, 0.782,\n",
       "        0.774, 0.777, 0.78 , 0.775, 0.778, 0.771, 0.777, 0.775, 0.782,\n",
       "        0.78 , 0.772, 0.773, 0.782, 0.78 , 0.78 , 0.771, 0.774, 0.771,\n",
       "        0.769, 0.773, 0.766, 0.774, 0.774, 0.775, 0.774, 0.773, 0.776,\n",
       "        0.778, 0.781, 0.772, 0.772, 0.775, 0.773, 0.775, 0.776, 0.767,\n",
       "        0.778, 0.774, 0.781, 0.774, 0.783, 0.777, 0.777, 0.78 , 0.781]),\n",
       " 'split1_test_f1_micro': array([0.76 , 0.778, 0.77 , 0.772, 0.769, 0.745, 0.74 , 0.731, 0.746,\n",
       "        0.716, 0.724, 0.733, 0.771, 0.76 , 0.769, 0.763, 0.753, 0.762,\n",
       "        0.751, 0.747, 0.747, 0.737, 0.732, 0.724, 0.771, 0.768, 0.774,\n",
       "        0.755, 0.772, 0.736, 0.739, 0.748, 0.753, 0.733, 0.735, 0.727,\n",
       "        0.766, 0.768, 0.766, 0.768, 0.752, 0.75 , 0.735, 0.752, 0.753,\n",
       "        0.738, 0.744, 0.753, 0.767, 0.769, 0.766, 0.75 , 0.747, 0.762,\n",
       "        0.758, 0.751, 0.731, 0.713, 0.728, 0.746, 0.765, 0.769, 0.764,\n",
       "        0.75 , 0.749, 0.768, 0.76 , 0.744, 0.732, 0.735, 0.717, 0.736,\n",
       "        0.781, 0.764, 0.776, 0.763, 0.758, 0.762, 0.757, 0.727, 0.745,\n",
       "        0.751, 0.717, 0.731, 0.777, 0.769, 0.767, 0.76 , 0.772, 0.743,\n",
       "        0.761, 0.739, 0.751, 0.745, 0.728, 0.732, 0.78 , 0.496, 0.78 ,\n",
       "        0.775, 0.587, 0.778, 0.783, 0.506, 0.775, 0.777, 0.494, 0.778,\n",
       "        0.778, 0.525, 0.785, 0.781, 0.508, 0.775, 0.783, 0.616, 0.782,\n",
       "        0.785, 0.494, 0.781, 0.783, 0.488, 0.781, 0.784, 0.506, 0.782,\n",
       "        0.778, 0.494, 0.782, 0.782, 0.535, 0.787, 0.775, 0.506, 0.787,\n",
       "        0.784, 0.493, 0.784, 0.782, 0.568, 0.783, 0.781, 0.618, 0.786,\n",
       "        0.785, 0.6  , 0.774, 0.783, 0.495, 0.78 , 0.776, 0.647, 0.786,\n",
       "        0.78 , 0.662, 0.786, 0.78 , 0.581, 0.781, 0.774, 0.632, 0.783,\n",
       "        0.776, 0.533, 0.781, 0.784, 0.727, 0.785, 0.784, 0.572, 0.777,\n",
       "        0.783, 0.467, 0.779, 0.781, 0.657, 0.782, 0.786, 0.655, 0.777,\n",
       "        0.776, 0.648, 0.777, 0.779, 0.509, 0.781, 0.777, 0.547, 0.783,\n",
       "        0.783, 0.59 , 0.784, 0.785, 0.784, 0.787, 0.786, 0.784, 0.786,\n",
       "        0.786, 0.786, 0.785, 0.784, 0.784, 0.787, 0.784, 0.786, 0.785,\n",
       "        0.784, 0.784, 0.787, 0.784, 0.786, 0.786, 0.786, 0.785, 0.786,\n",
       "        0.789, 0.787, 0.786, 0.784, 0.785, 0.785, 0.785, 0.787, 0.788,\n",
       "        0.786, 0.784, 0.785, 0.786, 0.782, 0.782, 0.784, 0.785, 0.787,\n",
       "        0.786, 0.785, 0.786, 0.785, 0.785, 0.785, 0.777, 0.784, 0.775,\n",
       "        0.775, 0.778, 0.77 , 0.78 , 0.775, 0.774, 0.76 , 0.777, 0.771,\n",
       "        0.775, 0.778, 0.783, 0.782, 0.779, 0.775, 0.771, 0.778, 0.777,\n",
       "        0.767, 0.768, 0.762, 0.784, 0.779, 0.784, 0.775, 0.778, 0.782,\n",
       "        0.776, 0.773, 0.777, 0.774, 0.771, 0.769, 0.778, 0.778, 0.779,\n",
       "        0.78 , 0.779, 0.777, 0.776, 0.787, 0.777, 0.774, 0.773, 0.766]),\n",
       " 'split2_test_f1_micro': array([0.784, 0.795, 0.786, 0.768, 0.769, 0.786, 0.765, 0.753, 0.767,\n",
       "        0.747, 0.742, 0.749, 0.786, 0.779, 0.787, 0.755, 0.758, 0.772,\n",
       "        0.752, 0.758, 0.753, 0.749, 0.733, 0.761, 0.793, 0.787, 0.771,\n",
       "        0.752, 0.758, 0.777, 0.782, 0.755, 0.741, 0.764, 0.734, 0.749,\n",
       "        0.803, 0.789, 0.789, 0.772, 0.788, 0.77 , 0.753, 0.759, 0.769,\n",
       "        0.765, 0.768, 0.759, 0.77 , 0.793, 0.792, 0.759, 0.755, 0.775,\n",
       "        0.753, 0.769, 0.762, 0.741, 0.736, 0.767, 0.778, 0.773, 0.773,\n",
       "        0.738, 0.769, 0.766, 0.775, 0.761, 0.749, 0.734, 0.752, 0.756,\n",
       "        0.792, 0.78 , 0.784, 0.761, 0.774, 0.757, 0.766, 0.749, 0.76 ,\n",
       "        0.737, 0.752, 0.738, 0.791, 0.771, 0.783, 0.781, 0.772, 0.784,\n",
       "        0.767, 0.764, 0.761, 0.743, 0.735, 0.763, 0.786, 0.477, 0.785,\n",
       "        0.791, 0.502, 0.791, 0.788, 0.586, 0.789, 0.786, 0.506, 0.79 ,\n",
       "        0.783, 0.494, 0.786, 0.786, 0.511, 0.785, 0.784, 0.401, 0.794,\n",
       "        0.789, 0.506, 0.791, 0.788, 0.358, 0.788, 0.791, 0.506, 0.788,\n",
       "        0.792, 0.494, 0.786, 0.789, 0.505, 0.786, 0.788, 0.526, 0.791,\n",
       "        0.789, 0.494, 0.791, 0.795, 0.494, 0.791, 0.791, 0.579, 0.791,\n",
       "        0.78 , 0.585, 0.803, 0.79 , 0.625, 0.791, 0.789, 0.616, 0.786,\n",
       "        0.784, 0.694, 0.787, 0.78 , 0.469, 0.785, 0.782, 0.656, 0.787,\n",
       "        0.782, 0.638, 0.789, 0.791, 0.665, 0.79 , 0.783, 0.469, 0.788,\n",
       "        0.786, 0.685, 0.797, 0.772, 0.619, 0.781, 0.795, 0.719, 0.796,\n",
       "        0.781, 0.765, 0.788, 0.797, 0.48 , 0.794, 0.783, 0.677, 0.789,\n",
       "        0.791, 0.641, 0.782, 0.794, 0.795, 0.796, 0.796, 0.796, 0.796,\n",
       "        0.796, 0.797, 0.797, 0.797, 0.796, 0.798, 0.793, 0.796, 0.798,\n",
       "        0.796, 0.793, 0.793, 0.792, 0.796, 0.797, 0.797, 0.793, 0.797,\n",
       "        0.796, 0.792, 0.793, 0.795, 0.795, 0.793, 0.795, 0.796, 0.798,\n",
       "        0.795, 0.797, 0.792, 0.796, 0.795, 0.791, 0.796, 0.795, 0.795,\n",
       "        0.795, 0.798, 0.794, 0.796, 0.794, 0.794, 0.795, 0.794, 0.791,\n",
       "        0.793, 0.797, 0.791, 0.793, 0.788, 0.784, 0.78 , 0.785, 0.79 ,\n",
       "        0.796, 0.795, 0.791, 0.795, 0.797, 0.792, 0.786, 0.786, 0.792,\n",
       "        0.787, 0.784, 0.782, 0.794, 0.794, 0.79 , 0.793, 0.788, 0.796,\n",
       "        0.786, 0.79 , 0.796, 0.788, 0.783, 0.785, 0.792, 0.793, 0.797,\n",
       "        0.789, 0.798, 0.787, 0.786, 0.793, 0.793, 0.796, 0.79 , 0.786]),\n",
       " 'split3_test_f1_micro': array([0.732, 0.757, 0.749, 0.745, 0.746, 0.757, 0.74 , 0.751, 0.728,\n",
       "        0.734, 0.748, 0.728, 0.753, 0.743, 0.752, 0.732, 0.755, 0.743,\n",
       "        0.731, 0.743, 0.748, 0.729, 0.734, 0.74 , 0.749, 0.746, 0.748,\n",
       "        0.749, 0.748, 0.753, 0.748, 0.74 , 0.728, 0.739, 0.736, 0.733,\n",
       "        0.758, 0.753, 0.752, 0.755, 0.764, 0.759, 0.758, 0.744, 0.76 ,\n",
       "        0.745, 0.753, 0.745, 0.757, 0.746, 0.756, 0.745, 0.765, 0.753,\n",
       "        0.745, 0.734, 0.765, 0.742, 0.746, 0.748, 0.76 , 0.751, 0.765,\n",
       "        0.75 , 0.74 , 0.752, 0.744, 0.746, 0.755, 0.719, 0.728, 0.719,\n",
       "        0.767, 0.764, 0.751, 0.749, 0.746, 0.764, 0.734, 0.746, 0.762,\n",
       "        0.745, 0.736, 0.743, 0.776, 0.764, 0.753, 0.751, 0.755, 0.738,\n",
       "        0.76 , 0.738, 0.768, 0.731, 0.739, 0.75 , 0.754, 0.494, 0.76 ,\n",
       "        0.767, 0.532, 0.766, 0.761, 0.516, 0.757, 0.759, 0.493, 0.767,\n",
       "        0.762, 0.527, 0.764, 0.764, 0.601, 0.763, 0.762, 0.505, 0.763,\n",
       "        0.761, 0.583, 0.763, 0.755, 0.494, 0.764, 0.76 , 0.653, 0.759,\n",
       "        0.759, 0.506, 0.766, 0.763, 0.574, 0.758, 0.76 , 0.516, 0.759,\n",
       "        0.759, 0.5  , 0.757, 0.763, 0.506, 0.768, 0.762, 0.484, 0.761,\n",
       "        0.76 , 0.666, 0.756, 0.764, 0.711, 0.761, 0.763, 0.672, 0.759,\n",
       "        0.756, 0.639, 0.752, 0.748, 0.585, 0.752, 0.754, 0.605, 0.759,\n",
       "        0.755, 0.661, 0.754, 0.757, 0.653, 0.753, 0.747, 0.315, 0.755,\n",
       "        0.755, 0.399, 0.748, 0.76 , 0.609, 0.747, 0.754, 0.686, 0.748,\n",
       "        0.758, 0.586, 0.753, 0.758, 0.556, 0.75 , 0.759, 0.564, 0.761,\n",
       "        0.762, 0.433, 0.754, 0.75 , 0.752, 0.752, 0.752, 0.752, 0.75 ,\n",
       "        0.749, 0.749, 0.75 , 0.753, 0.751, 0.751, 0.751, 0.746, 0.749,\n",
       "        0.752, 0.753, 0.751, 0.749, 0.752, 0.75 , 0.752, 0.751, 0.753,\n",
       "        0.757, 0.75 , 0.756, 0.752, 0.751, 0.753, 0.752, 0.752, 0.749,\n",
       "        0.752, 0.751, 0.751, 0.752, 0.754, 0.754, 0.754, 0.752, 0.753,\n",
       "        0.752, 0.749, 0.751, 0.751, 0.748, 0.754, 0.758, 0.754, 0.755,\n",
       "        0.761, 0.749, 0.753, 0.753, 0.763, 0.756, 0.754, 0.754, 0.759,\n",
       "        0.755, 0.751, 0.752, 0.752, 0.747, 0.757, 0.756, 0.758, 0.748,\n",
       "        0.757, 0.756, 0.757, 0.759, 0.754, 0.748, 0.75 , 0.754, 0.758,\n",
       "        0.756, 0.762, 0.754, 0.759, 0.76 , 0.757, 0.755, 0.749, 0.751,\n",
       "        0.758, 0.757, 0.754, 0.758, 0.755, 0.763, 0.756, 0.759, 0.754]),\n",
       " 'split4_test_f1_micro': array([0.744, 0.768, 0.764, 0.75 , 0.764, 0.78 , 0.752, 0.738, 0.735,\n",
       "        0.73 , 0.744, 0.731, 0.775, 0.768, 0.775, 0.741, 0.744, 0.759,\n",
       "        0.74 , 0.726, 0.725, 0.728, 0.724, 0.729, 0.774, 0.769, 0.77 ,\n",
       "        0.749, 0.771, 0.752, 0.736, 0.728, 0.732, 0.73 , 0.723, 0.721,\n",
       "        0.787, 0.777, 0.775, 0.747, 0.746, 0.771, 0.762, 0.739, 0.758,\n",
       "        0.756, 0.729, 0.736, 0.763, 0.752, 0.774, 0.726, 0.754, 0.762,\n",
       "        0.729, 0.732, 0.743, 0.725, 0.712, 0.743, 0.762, 0.769, 0.781,\n",
       "        0.761, 0.735, 0.734, 0.743, 0.747, 0.719, 0.747, 0.74 , 0.7  ,\n",
       "        0.77 , 0.763, 0.769, 0.744, 0.754, 0.761, 0.744, 0.731, 0.736,\n",
       "        0.735, 0.707, 0.734, 0.777, 0.761, 0.773, 0.744, 0.753, 0.757,\n",
       "        0.723, 0.735, 0.749, 0.736, 0.731, 0.736, 0.784, 0.506, 0.783,\n",
       "        0.777, 0.386, 0.776, 0.789, 0.472, 0.784, 0.78 , 0.492, 0.781,\n",
       "        0.787, 0.667, 0.785, 0.778, 0.494, 0.774, 0.779, 0.495, 0.776,\n",
       "        0.785, 0.494, 0.781, 0.779, 0.506, 0.788, 0.778, 0.494, 0.783,\n",
       "        0.784, 0.621, 0.778, 0.784, 0.565, 0.777, 0.787, 0.612, 0.776,\n",
       "        0.777, 0.494, 0.776, 0.788, 0.494, 0.779, 0.781, 0.529, 0.782,\n",
       "        0.78 , 0.54 , 0.774, 0.776, 0.484, 0.782, 0.777, 0.484, 0.79 ,\n",
       "        0.781, 0.577, 0.779, 0.784, 0.532, 0.778, 0.785, 0.694, 0.786,\n",
       "        0.78 , 0.606, 0.782, 0.785, 0.666, 0.783, 0.777, 0.706, 0.766,\n",
       "        0.786, 0.605, 0.783, 0.779, 0.587, 0.781, 0.785, 0.52 , 0.783,\n",
       "        0.788, 0.529, 0.771, 0.777, 0.539, 0.788, 0.772, 0.441, 0.784,\n",
       "        0.782, 0.589, 0.773, 0.782, 0.784, 0.79 , 0.785, 0.785, 0.786,\n",
       "        0.787, 0.784, 0.781, 0.786, 0.785, 0.786, 0.784, 0.784, 0.783,\n",
       "        0.788, 0.784, 0.784, 0.785, 0.784, 0.783, 0.784, 0.783, 0.785,\n",
       "        0.785, 0.782, 0.786, 0.783, 0.782, 0.785, 0.787, 0.787, 0.784,\n",
       "        0.783, 0.785, 0.783, 0.785, 0.784, 0.786, 0.784, 0.784, 0.785,\n",
       "        0.785, 0.785, 0.784, 0.784, 0.785, 0.785, 0.784, 0.773, 0.78 ,\n",
       "        0.779, 0.78 , 0.787, 0.772, 0.777, 0.773, 0.769, 0.775, 0.781,\n",
       "        0.786, 0.789, 0.778, 0.779, 0.783, 0.781, 0.776, 0.768, 0.778,\n",
       "        0.76 , 0.775, 0.774, 0.786, 0.786, 0.784, 0.78 , 0.789, 0.782,\n",
       "        0.772, 0.766, 0.775, 0.772, 0.77 , 0.774, 0.787, 0.784, 0.791,\n",
       "        0.789, 0.784, 0.781, 0.779, 0.777, 0.775, 0.779, 0.785, 0.779]),\n",
       " 'mean_test_f1_micro': array([0.7566, 0.773 , 0.7658, 0.7594, 0.7604, 0.7646, 0.7456, 0.7452,\n",
       "        0.7382, 0.7284, 0.7414, 0.7364, 0.7702, 0.763 , 0.7686, 0.7464,\n",
       "        0.7516, 0.7568, 0.7434, 0.7458, 0.739 , 0.7342, 0.7356, 0.7374,\n",
       "        0.7704, 0.77  , 0.7676, 0.7532, 0.762 , 0.758 , 0.7502, 0.744 ,\n",
       "        0.739 , 0.7426, 0.738 , 0.7346, 0.7778, 0.772 , 0.7696, 0.761 ,\n",
       "        0.763 , 0.7658, 0.7504, 0.7498, 0.7594, 0.7498, 0.7482, 0.7482,\n",
       "        0.7644, 0.766 , 0.774 , 0.7446, 0.7566, 0.7622, 0.7438, 0.7406,\n",
       "        0.7486, 0.7284, 0.7272, 0.7458, 0.768 , 0.7672, 0.7686, 0.7496,\n",
       "        0.75  , 0.7556, 0.7542, 0.7484, 0.7364, 0.7336, 0.7366, 0.7266,\n",
       "        0.7756, 0.7658, 0.7686, 0.7506, 0.756 , 0.7612, 0.7508, 0.7414,\n",
       "        0.7496, 0.7378, 0.7292, 0.7384, 0.7784, 0.767 , 0.7712, 0.7604,\n",
       "        0.7612, 0.7526, 0.7518, 0.7456, 0.754 , 0.7362, 0.7368, 0.7466,\n",
       "        0.7748, 0.5462, 0.7764, 0.7774, 0.5024, 0.7764, 0.7774, 0.515 ,\n",
       "        0.7742, 0.7736, 0.4962, 0.777 , 0.7766, 0.5386, 0.7774, 0.7768,\n",
       "        0.552 , 0.7744, 0.7766, 0.5072, 0.7778, 0.779 , 0.515 , 0.7774,\n",
       "        0.7748, 0.4682, 0.7784, 0.7778, 0.5308, 0.776 , 0.776 , 0.5268,\n",
       "        0.7764, 0.778 , 0.5368, 0.776 , 0.776 , 0.531 , 0.7768, 0.7762,\n",
       "        0.4952, 0.7756, 0.78  , 0.478 , 0.7782, 0.7782, 0.541 , 0.7782,\n",
       "        0.7754, 0.5986, 0.7752, 0.7778, 0.5952, 0.778 , 0.773 , 0.5828,\n",
       "        0.7788, 0.7734, 0.6306, 0.774 , 0.7732, 0.5482, 0.7738, 0.7728,\n",
       "        0.6212, 0.7762, 0.772 , 0.5972, 0.7752, 0.777 , 0.6866, 0.7768,\n",
       "        0.7708, 0.5098, 0.7702, 0.7748, 0.5496, 0.7762, 0.7736, 0.6172,\n",
       "        0.7724, 0.7774, 0.6458, 0.7734, 0.7746, 0.6094, 0.7718, 0.7764,\n",
       "        0.5206, 0.778 , 0.774 , 0.5842, 0.777 , 0.7776, 0.5142, 0.7728,\n",
       "        0.7772, 0.7782, 0.7798, 0.7782, 0.7782, 0.7788, 0.7782, 0.778 ,\n",
       "        0.7772, 0.7792, 0.7774, 0.7786, 0.7768, 0.7768, 0.7782, 0.779 ,\n",
       "        0.7776, 0.778 , 0.7766, 0.778 , 0.778 , 0.778 , 0.7774, 0.7794,\n",
       "        0.7812, 0.7768, 0.7788, 0.7776, 0.777 , 0.7784, 0.7782, 0.7784,\n",
       "        0.778 , 0.7778, 0.7782, 0.7766, 0.7786, 0.7778, 0.7772, 0.7782,\n",
       "        0.7776, 0.779 , 0.7782, 0.7784, 0.778 , 0.778 , 0.777 , 0.778 ,\n",
       "        0.7784, 0.7772, 0.7766, 0.7764, 0.7762, 0.7762, 0.7746, 0.7762,\n",
       "        0.7716, 0.768 , 0.7732, 0.7766, 0.7784, 0.777 , 0.7754, 0.778 ,\n",
       "        0.7772, 0.777 , 0.772 , 0.7728, 0.7732, 0.768 , 0.7712, 0.7682,\n",
       "        0.7794, 0.7774, 0.7762, 0.7744, 0.7764, 0.7788, 0.7736, 0.7744,\n",
       "        0.7748, 0.773 , 0.7718, 0.7716, 0.7774, 0.776 , 0.777 , 0.7788,\n",
       "        0.7784, 0.776 , 0.7746, 0.779 , 0.777 , 0.7764, 0.7774, 0.7732]),\n",
       " 'std_test_f1_micro': array([0.01770424, 0.01285302, 0.01220492, 0.01034601, 0.00904655,\n",
       "        0.01567929, 0.01177455, 0.00904212, 0.01756588, 0.0119432 ,\n",
       "        0.00906863, 0.00763151, 0.01083328, 0.01178134, 0.01207642,\n",
       "        0.01109234, 0.005004  , 0.0103034 , 0.00770973, 0.01126765,\n",
       "        0.01285302, 0.00813388, 0.01032666, 0.01290891, 0.01422111,\n",
       "        0.01392839, 0.00997196, 0.00448999, 0.00887694, 0.01484587,\n",
       "        0.01649727, 0.00931665, 0.0086487 , 0.01217539, 0.0128841 ,\n",
       "        0.01022937, 0.01586695, 0.01176435, 0.01217539, 0.0090111 ,\n",
       "        0.01442221, 0.01014692, 0.00976934, 0.00730479, 0.00531413,\n",
       "        0.0095373 , 0.01267123, 0.00773046, 0.00436348, 0.01643168,\n",
       "        0.01245793, 0.0108185 , 0.0063435 , 0.00719444, 0.01097998,\n",
       "        0.01782807, 0.01290891, 0.01137717, 0.01293677, 0.01337759,\n",
       "        0.00718331, 0.00835225, 0.00749933, 0.00728286, 0.0121326 ,\n",
       "        0.01222457, 0.01202331, 0.00640625, 0.01352923, 0.00889044,\n",
       "        0.01261111, 0.01865047, 0.00960417, 0.00744043, 0.01125344,\n",
       "        0.0102098 , 0.00995992, 0.00231517, 0.01097998, 0.01051856,\n",
       "        0.00989141, 0.01016661, 0.01568949, 0.00553534, 0.00668132,\n",
       "        0.00384708, 0.01066583, 0.01275304, 0.00884081, 0.01700118,\n",
       "        0.01566397, 0.0108922 , 0.00946573, 0.00713863, 0.00800999,\n",
       "        0.0112712 , 0.01177115, 0.10630974, 0.00900222, 0.00773563,\n",
       "        0.06571027, 0.00840476, 0.01163787, 0.03839792, 0.01165161,\n",
       "        0.00976934, 0.00507543, 0.0083666 , 0.00868562, 0.06667713,\n",
       "        0.00976934, 0.00735935, 0.06036224, 0.00697424, 0.00796492,\n",
       "        0.06842339, 0.01016661, 0.01011929, 0.03428119, 0.00958332,\n",
       "        0.01170299, 0.05540541, 0.0095205 , 0.01032279, 0.06131688,\n",
       "        0.01078888, 0.01178134, 0.0479975 , 0.00741889, 0.00931665,\n",
       "        0.02899931, 0.01060189, 0.01056409, 0.04178995, 0.01146124,\n",
       "        0.01038075, 0.00248193, 0.01170641, 0.01136662, 0.07986989,\n",
       "        0.00847113, 0.00945304, 0.05075825, 0.01083328, 0.00875443,\n",
       "        0.04043563, 0.01538051, 0.00863481, 0.09059007, 0.00981835,\n",
       "        0.01048809, 0.07829534, 0.01144378, 0.01068831, 0.04562718,\n",
       "        0.01331165, 0.01299846, 0.04388804, 0.01158275, 0.01097998,\n",
       "        0.0588809 , 0.01147868, 0.00993982, 0.04970875, 0.01222129,\n",
       "        0.01256981, 0.03132156, 0.01312098, 0.01406272, 0.12844828,\n",
       "        0.0113031 , 0.01285924, 0.10266762, 0.01604244, 0.0074458 ,\n",
       "        0.02270154, 0.01332066, 0.01481351, 0.06766801, 0.01654811,\n",
       "        0.01019019, 0.09042699, 0.01137365, 0.01264278, 0.02598923,\n",
       "        0.01516575, 0.00829458, 0.09222885, 0.01063955, 0.01028786,\n",
       "        0.12042159, 0.01064707, 0.01490503, 0.01442775, 0.0156512 ,\n",
       "        0.01515784, 0.01483779, 0.01572768, 0.01633891, 0.01623576,\n",
       "        0.0156512 , 0.01471598, 0.0153961 , 0.01625546, 0.01452446,\n",
       "        0.01718604, 0.01624069, 0.01509967, 0.01369087, 0.01469694,\n",
       "        0.01508111, 0.01507315, 0.01581139, 0.0154013 , 0.01438888,\n",
       "        0.01478648, 0.01330263, 0.0147973 , 0.01310572, 0.01443052,\n",
       "        0.01492649, 0.01379275, 0.01503862, 0.01565375, 0.01688787,\n",
       "        0.01468877, 0.01543243, 0.01431922, 0.01501466, 0.0136587 ,\n",
       "        0.01301384, 0.01411949, 0.01473228, 0.01447757, 0.01485126,\n",
       "        0.01641463, 0.01479189, 0.0151921 , 0.01596246, 0.01389964,\n",
       "        0.01204326, 0.01340746, 0.01197664, 0.0102684 , 0.01543243,\n",
       "        0.0136147 , 0.01297074, 0.00798499, 0.00900222, 0.00985901,\n",
       "        0.01028397, 0.01066958, 0.01363231, 0.01529706, 0.01312402,\n",
       "        0.01412799, 0.01642437, 0.01143678, 0.00969536, 0.00943186,\n",
       "        0.01435827, 0.010469  , 0.00919565, 0.00886341, 0.01202664,\n",
       "        0.01349963, 0.01489161, 0.01395134, 0.01272164, 0.01230285,\n",
       "        0.00991161, 0.01013114, 0.01337759, 0.00920869, 0.00746726,\n",
       "        0.00902441, 0.01275304, 0.01473771, 0.01658915, 0.01133843,\n",
       "        0.01336563, 0.01145426, 0.00924338, 0.01308434, 0.00954987,\n",
       "        0.01275304, 0.01078147, 0.01165161]),\n",
       " 'rank_test_f1_micro': array([198, 147, 180, 194, 192, 183, 225, 227, 239, 253, 233, 245, 165,\n",
       "        186, 170, 222, 207, 197, 231, 223, 237, 250, 248, 242, 164, 167,\n",
       "        176, 204, 188, 196, 211, 229, 236, 232, 240, 249,  54, 154, 168,\n",
       "        191, 185, 180, 210, 214, 195, 213, 219, 220, 184, 179, 134, 228,\n",
       "        199, 187, 230, 235, 217, 253, 255, 223, 173, 177, 170, 215, 212,\n",
       "        201, 202, 218, 246, 251, 244, 256, 117, 180, 169, 209, 200, 189,\n",
       "        208, 233, 215, 241, 252, 238,  18, 178, 162, 192, 189, 205, 206,\n",
       "        225, 203, 247, 243, 221, 123, 271,  98,  63, 284,  98,  63, 280,\n",
       "        133, 138, 285,  77,  93, 273,  61,  89, 268, 131,  93, 283,  51,\n",
       "          7, 279,  63, 125, 288,  18,  54, 276, 111, 111, 277,  98,  42,\n",
       "        274, 111, 111, 275,  85, 105, 286, 117,   2, 287,  29,  26, 272,\n",
       "         29, 120, 263, 121,  51, 265,  42, 147, 267,  12, 141, 259, 134,\n",
       "        143, 270, 137, 150, 260, 105, 154, 264, 121,  77, 257,  89, 163,\n",
       "        282, 165, 123, 269, 105, 138, 261, 153,  63, 258, 141, 127, 262,\n",
       "        157,  98, 278,  38, 134, 266,  77,  60, 281, 151,  71,  26,   3,\n",
       "         29,  29,  12,  29,  38,  71,   6,  63,  16,  85,  85,  29,   7,\n",
       "         57,  42,  91,  42,  42,  42,  63,   4,   1,  85,  15,  58,  81,\n",
       "         18,  29,  25,  38,  54,  26,  93,  16,  51,  71,  29,  58,   7,\n",
       "         29,  18,  42,  38,  81,  42,  18,  71,  93,  98, 105, 105, 127,\n",
       "        105, 160, 173, 143,  91,  24,  77, 119,  50,  71,  76, 154, 151,\n",
       "        143, 173, 161, 172,   4,  63, 104, 131,  98,  12, 138, 130, 125,\n",
       "        147, 158, 159,  70, 111,  81,  11,  18, 111, 127,  10,  81,  97,\n",
       "         61, 143]),\n",
       " 'split0_test_precision_micro': array([0.763, 0.767, 0.76 , 0.762, 0.754, 0.755, 0.731, 0.753, 0.715,\n",
       "        0.715, 0.749, 0.741, 0.766, 0.765, 0.76 , 0.741, 0.748, 0.748,\n",
       "        0.743, 0.755, 0.722, 0.728, 0.755, 0.733, 0.765, 0.78 , 0.775,\n",
       "        0.761, 0.761, 0.772, 0.746, 0.749, 0.741, 0.747, 0.762, 0.743,\n",
       "        0.775, 0.773, 0.766, 0.763, 0.765, 0.779, 0.744, 0.755, 0.757,\n",
       "        0.745, 0.747, 0.748, 0.765, 0.77 , 0.782, 0.743, 0.762, 0.759,\n",
       "        0.734, 0.717, 0.742, 0.721, 0.714, 0.725, 0.775, 0.774, 0.76 ,\n",
       "        0.749, 0.757, 0.758, 0.749, 0.744, 0.727, 0.733, 0.746, 0.722,\n",
       "        0.768, 0.758, 0.763, 0.736, 0.748, 0.762, 0.753, 0.754, 0.745,\n",
       "        0.721, 0.734, 0.746, 0.771, 0.77 , 0.78 , 0.766, 0.754, 0.741,\n",
       "        0.748, 0.752, 0.741, 0.726, 0.751, 0.752, 0.77 , 0.758, 0.774,\n",
       "        0.777, 0.505, 0.771, 0.766, 0.495, 0.766, 0.766, 0.496, 0.769,\n",
       "        0.773, 0.48 , 0.767, 0.775, 0.646, 0.775, 0.775, 0.519, 0.774,\n",
       "        0.775, 0.498, 0.771, 0.769, 0.495, 0.771, 0.776, 0.495, 0.768,\n",
       "        0.767, 0.519, 0.77 , 0.772, 0.505, 0.772, 0.77 , 0.495, 0.771,\n",
       "        0.772, 0.495, 0.77 , 0.772, 0.328, 0.77 , 0.776, 0.495, 0.771,\n",
       "        0.772, 0.602, 0.769, 0.776, 0.661, 0.776, 0.76 , 0.495, 0.773,\n",
       "        0.766, 0.581, 0.766, 0.774, 0.574, 0.773, 0.769, 0.519, 0.766,\n",
       "        0.767, 0.548, 0.77 , 0.768, 0.722, 0.773, 0.763, 0.487, 0.765,\n",
       "        0.764, 0.592, 0.774, 0.776, 0.614, 0.771, 0.767, 0.649, 0.763,\n",
       "        0.77 , 0.519, 0.77 , 0.771, 0.519, 0.777, 0.779, 0.692, 0.768,\n",
       "        0.77 , 0.318, 0.771, 0.775, 0.776, 0.774, 0.772, 0.774, 0.776,\n",
       "        0.773, 0.774, 0.773, 0.776, 0.771, 0.771, 0.772, 0.772, 0.776,\n",
       "        0.775, 0.774, 0.775, 0.773, 0.772, 0.774, 0.771, 0.775, 0.776,\n",
       "        0.779, 0.773, 0.773, 0.774, 0.772, 0.776, 0.772, 0.77 , 0.771,\n",
       "        0.773, 0.774, 0.772, 0.774, 0.774, 0.773, 0.773, 0.772, 0.775,\n",
       "        0.773, 0.775, 0.775, 0.774, 0.773, 0.772, 0.778, 0.781, 0.782,\n",
       "        0.774, 0.777, 0.78 , 0.775, 0.778, 0.771, 0.777, 0.775, 0.782,\n",
       "        0.78 , 0.772, 0.773, 0.782, 0.78 , 0.78 , 0.771, 0.774, 0.771,\n",
       "        0.769, 0.773, 0.766, 0.774, 0.774, 0.775, 0.774, 0.773, 0.776,\n",
       "        0.778, 0.781, 0.772, 0.772, 0.775, 0.773, 0.775, 0.776, 0.767,\n",
       "        0.778, 0.774, 0.781, 0.774, 0.783, 0.777, 0.777, 0.78 , 0.781]),\n",
       " 'split1_test_precision_micro': array([0.76 , 0.778, 0.77 , 0.772, 0.769, 0.745, 0.74 , 0.731, 0.746,\n",
       "        0.716, 0.724, 0.733, 0.771, 0.76 , 0.769, 0.763, 0.753, 0.762,\n",
       "        0.751, 0.747, 0.747, 0.737, 0.732, 0.724, 0.771, 0.768, 0.774,\n",
       "        0.755, 0.772, 0.736, 0.739, 0.748, 0.753, 0.733, 0.735, 0.727,\n",
       "        0.766, 0.768, 0.766, 0.768, 0.752, 0.75 , 0.735, 0.752, 0.753,\n",
       "        0.738, 0.744, 0.753, 0.767, 0.769, 0.766, 0.75 , 0.747, 0.762,\n",
       "        0.758, 0.751, 0.731, 0.713, 0.728, 0.746, 0.765, 0.769, 0.764,\n",
       "        0.75 , 0.749, 0.768, 0.76 , 0.744, 0.732, 0.735, 0.717, 0.736,\n",
       "        0.781, 0.764, 0.776, 0.763, 0.758, 0.762, 0.757, 0.727, 0.745,\n",
       "        0.751, 0.717, 0.731, 0.777, 0.769, 0.767, 0.76 , 0.772, 0.743,\n",
       "        0.761, 0.739, 0.751, 0.745, 0.728, 0.732, 0.78 , 0.496, 0.78 ,\n",
       "        0.775, 0.587, 0.778, 0.783, 0.506, 0.775, 0.777, 0.494, 0.778,\n",
       "        0.778, 0.525, 0.785, 0.781, 0.508, 0.775, 0.783, 0.616, 0.782,\n",
       "        0.785, 0.494, 0.781, 0.783, 0.488, 0.781, 0.784, 0.506, 0.782,\n",
       "        0.778, 0.494, 0.782, 0.782, 0.535, 0.787, 0.775, 0.506, 0.787,\n",
       "        0.784, 0.493, 0.784, 0.782, 0.568, 0.783, 0.781, 0.618, 0.786,\n",
       "        0.785, 0.6  , 0.774, 0.783, 0.495, 0.78 , 0.776, 0.647, 0.786,\n",
       "        0.78 , 0.662, 0.786, 0.78 , 0.581, 0.781, 0.774, 0.632, 0.783,\n",
       "        0.776, 0.533, 0.781, 0.784, 0.727, 0.785, 0.784, 0.572, 0.777,\n",
       "        0.783, 0.467, 0.779, 0.781, 0.657, 0.782, 0.786, 0.655, 0.777,\n",
       "        0.776, 0.648, 0.777, 0.779, 0.509, 0.781, 0.777, 0.547, 0.783,\n",
       "        0.783, 0.59 , 0.784, 0.785, 0.784, 0.787, 0.786, 0.784, 0.786,\n",
       "        0.786, 0.786, 0.785, 0.784, 0.784, 0.787, 0.784, 0.786, 0.785,\n",
       "        0.784, 0.784, 0.787, 0.784, 0.786, 0.786, 0.786, 0.785, 0.786,\n",
       "        0.789, 0.787, 0.786, 0.784, 0.785, 0.785, 0.785, 0.787, 0.788,\n",
       "        0.786, 0.784, 0.785, 0.786, 0.782, 0.782, 0.784, 0.785, 0.787,\n",
       "        0.786, 0.785, 0.786, 0.785, 0.785, 0.785, 0.777, 0.784, 0.775,\n",
       "        0.775, 0.778, 0.77 , 0.78 , 0.775, 0.774, 0.76 , 0.777, 0.771,\n",
       "        0.775, 0.778, 0.783, 0.782, 0.779, 0.775, 0.771, 0.778, 0.777,\n",
       "        0.767, 0.768, 0.762, 0.784, 0.779, 0.784, 0.775, 0.778, 0.782,\n",
       "        0.776, 0.773, 0.777, 0.774, 0.771, 0.769, 0.778, 0.778, 0.779,\n",
       "        0.78 , 0.779, 0.777, 0.776, 0.787, 0.777, 0.774, 0.773, 0.766]),\n",
       " 'split2_test_precision_micro': array([0.784, 0.795, 0.786, 0.768, 0.769, 0.786, 0.765, 0.753, 0.767,\n",
       "        0.747, 0.742, 0.749, 0.786, 0.779, 0.787, 0.755, 0.758, 0.772,\n",
       "        0.752, 0.758, 0.753, 0.749, 0.733, 0.761, 0.793, 0.787, 0.771,\n",
       "        0.752, 0.758, 0.777, 0.782, 0.755, 0.741, 0.764, 0.734, 0.749,\n",
       "        0.803, 0.789, 0.789, 0.772, 0.788, 0.77 , 0.753, 0.759, 0.769,\n",
       "        0.765, 0.768, 0.759, 0.77 , 0.793, 0.792, 0.759, 0.755, 0.775,\n",
       "        0.753, 0.769, 0.762, 0.741, 0.736, 0.767, 0.778, 0.773, 0.773,\n",
       "        0.738, 0.769, 0.766, 0.775, 0.761, 0.749, 0.734, 0.752, 0.756,\n",
       "        0.792, 0.78 , 0.784, 0.761, 0.774, 0.757, 0.766, 0.749, 0.76 ,\n",
       "        0.737, 0.752, 0.738, 0.791, 0.771, 0.783, 0.781, 0.772, 0.784,\n",
       "        0.767, 0.764, 0.761, 0.743, 0.735, 0.763, 0.786, 0.477, 0.785,\n",
       "        0.791, 0.502, 0.791, 0.788, 0.586, 0.789, 0.786, 0.506, 0.79 ,\n",
       "        0.783, 0.494, 0.786, 0.786, 0.511, 0.785, 0.784, 0.401, 0.794,\n",
       "        0.789, 0.506, 0.791, 0.788, 0.358, 0.788, 0.791, 0.506, 0.788,\n",
       "        0.792, 0.494, 0.786, 0.789, 0.505, 0.786, 0.788, 0.526, 0.791,\n",
       "        0.789, 0.494, 0.791, 0.795, 0.494, 0.791, 0.791, 0.579, 0.791,\n",
       "        0.78 , 0.585, 0.803, 0.79 , 0.625, 0.791, 0.789, 0.616, 0.786,\n",
       "        0.784, 0.694, 0.787, 0.78 , 0.469, 0.785, 0.782, 0.656, 0.787,\n",
       "        0.782, 0.638, 0.789, 0.791, 0.665, 0.79 , 0.783, 0.469, 0.788,\n",
       "        0.786, 0.685, 0.797, 0.772, 0.619, 0.781, 0.795, 0.719, 0.796,\n",
       "        0.781, 0.765, 0.788, 0.797, 0.48 , 0.794, 0.783, 0.677, 0.789,\n",
       "        0.791, 0.641, 0.782, 0.794, 0.795, 0.796, 0.796, 0.796, 0.796,\n",
       "        0.796, 0.797, 0.797, 0.797, 0.796, 0.798, 0.793, 0.796, 0.798,\n",
       "        0.796, 0.793, 0.793, 0.792, 0.796, 0.797, 0.797, 0.793, 0.797,\n",
       "        0.796, 0.792, 0.793, 0.795, 0.795, 0.793, 0.795, 0.796, 0.798,\n",
       "        0.795, 0.797, 0.792, 0.796, 0.795, 0.791, 0.796, 0.795, 0.795,\n",
       "        0.795, 0.798, 0.794, 0.796, 0.794, 0.794, 0.795, 0.794, 0.791,\n",
       "        0.793, 0.797, 0.791, 0.793, 0.788, 0.784, 0.78 , 0.785, 0.79 ,\n",
       "        0.796, 0.795, 0.791, 0.795, 0.797, 0.792, 0.786, 0.786, 0.792,\n",
       "        0.787, 0.784, 0.782, 0.794, 0.794, 0.79 , 0.793, 0.788, 0.796,\n",
       "        0.786, 0.79 , 0.796, 0.788, 0.783, 0.785, 0.792, 0.793, 0.797,\n",
       "        0.789, 0.798, 0.787, 0.786, 0.793, 0.793, 0.796, 0.79 , 0.786]),\n",
       " 'split3_test_precision_micro': array([0.732, 0.757, 0.749, 0.745, 0.746, 0.757, 0.74 , 0.751, 0.728,\n",
       "        0.734, 0.748, 0.728, 0.753, 0.743, 0.752, 0.732, 0.755, 0.743,\n",
       "        0.731, 0.743, 0.748, 0.729, 0.734, 0.74 , 0.749, 0.746, 0.748,\n",
       "        0.749, 0.748, 0.753, 0.748, 0.74 , 0.728, 0.739, 0.736, 0.733,\n",
       "        0.758, 0.753, 0.752, 0.755, 0.764, 0.759, 0.758, 0.744, 0.76 ,\n",
       "        0.745, 0.753, 0.745, 0.757, 0.746, 0.756, 0.745, 0.765, 0.753,\n",
       "        0.745, 0.734, 0.765, 0.742, 0.746, 0.748, 0.76 , 0.751, 0.765,\n",
       "        0.75 , 0.74 , 0.752, 0.744, 0.746, 0.755, 0.719, 0.728, 0.719,\n",
       "        0.767, 0.764, 0.751, 0.749, 0.746, 0.764, 0.734, 0.746, 0.762,\n",
       "        0.745, 0.736, 0.743, 0.776, 0.764, 0.753, 0.751, 0.755, 0.738,\n",
       "        0.76 , 0.738, 0.768, 0.731, 0.739, 0.75 , 0.754, 0.494, 0.76 ,\n",
       "        0.767, 0.532, 0.766, 0.761, 0.516, 0.757, 0.759, 0.493, 0.767,\n",
       "        0.762, 0.527, 0.764, 0.764, 0.601, 0.763, 0.762, 0.505, 0.763,\n",
       "        0.761, 0.583, 0.763, 0.755, 0.494, 0.764, 0.76 , 0.653, 0.759,\n",
       "        0.759, 0.506, 0.766, 0.763, 0.574, 0.758, 0.76 , 0.516, 0.759,\n",
       "        0.759, 0.5  , 0.757, 0.763, 0.506, 0.768, 0.762, 0.484, 0.761,\n",
       "        0.76 , 0.666, 0.756, 0.764, 0.711, 0.761, 0.763, 0.672, 0.759,\n",
       "        0.756, 0.639, 0.752, 0.748, 0.585, 0.752, 0.754, 0.605, 0.759,\n",
       "        0.755, 0.661, 0.754, 0.757, 0.653, 0.753, 0.747, 0.315, 0.755,\n",
       "        0.755, 0.399, 0.748, 0.76 , 0.609, 0.747, 0.754, 0.686, 0.748,\n",
       "        0.758, 0.586, 0.753, 0.758, 0.556, 0.75 , 0.759, 0.564, 0.761,\n",
       "        0.762, 0.433, 0.754, 0.75 , 0.752, 0.752, 0.752, 0.752, 0.75 ,\n",
       "        0.749, 0.749, 0.75 , 0.753, 0.751, 0.751, 0.751, 0.746, 0.749,\n",
       "        0.752, 0.753, 0.751, 0.749, 0.752, 0.75 , 0.752, 0.751, 0.753,\n",
       "        0.757, 0.75 , 0.756, 0.752, 0.751, 0.753, 0.752, 0.752, 0.749,\n",
       "        0.752, 0.751, 0.751, 0.752, 0.754, 0.754, 0.754, 0.752, 0.753,\n",
       "        0.752, 0.749, 0.751, 0.751, 0.748, 0.754, 0.758, 0.754, 0.755,\n",
       "        0.761, 0.749, 0.753, 0.753, 0.763, 0.756, 0.754, 0.754, 0.759,\n",
       "        0.755, 0.751, 0.752, 0.752, 0.747, 0.757, 0.756, 0.758, 0.748,\n",
       "        0.757, 0.756, 0.757, 0.759, 0.754, 0.748, 0.75 , 0.754, 0.758,\n",
       "        0.756, 0.762, 0.754, 0.759, 0.76 , 0.757, 0.755, 0.749, 0.751,\n",
       "        0.758, 0.757, 0.754, 0.758, 0.755, 0.763, 0.756, 0.759, 0.754]),\n",
       " 'split4_test_precision_micro': array([0.744, 0.768, 0.764, 0.75 , 0.764, 0.78 , 0.752, 0.738, 0.735,\n",
       "        0.73 , 0.744, 0.731, 0.775, 0.768, 0.775, 0.741, 0.744, 0.759,\n",
       "        0.74 , 0.726, 0.725, 0.728, 0.724, 0.729, 0.774, 0.769, 0.77 ,\n",
       "        0.749, 0.771, 0.752, 0.736, 0.728, 0.732, 0.73 , 0.723, 0.721,\n",
       "        0.787, 0.777, 0.775, 0.747, 0.746, 0.771, 0.762, 0.739, 0.758,\n",
       "        0.756, 0.729, 0.736, 0.763, 0.752, 0.774, 0.726, 0.754, 0.762,\n",
       "        0.729, 0.732, 0.743, 0.725, 0.712, 0.743, 0.762, 0.769, 0.781,\n",
       "        0.761, 0.735, 0.734, 0.743, 0.747, 0.719, 0.747, 0.74 , 0.7  ,\n",
       "        0.77 , 0.763, 0.769, 0.744, 0.754, 0.761, 0.744, 0.731, 0.736,\n",
       "        0.735, 0.707, 0.734, 0.777, 0.761, 0.773, 0.744, 0.753, 0.757,\n",
       "        0.723, 0.735, 0.749, 0.736, 0.731, 0.736, 0.784, 0.506, 0.783,\n",
       "        0.777, 0.386, 0.776, 0.789, 0.472, 0.784, 0.78 , 0.492, 0.781,\n",
       "        0.787, 0.667, 0.785, 0.778, 0.494, 0.774, 0.779, 0.495, 0.776,\n",
       "        0.785, 0.494, 0.781, 0.779, 0.506, 0.788, 0.778, 0.494, 0.783,\n",
       "        0.784, 0.621, 0.778, 0.784, 0.565, 0.777, 0.787, 0.612, 0.776,\n",
       "        0.777, 0.494, 0.776, 0.788, 0.494, 0.779, 0.781, 0.529, 0.782,\n",
       "        0.78 , 0.54 , 0.774, 0.776, 0.484, 0.782, 0.777, 0.484, 0.79 ,\n",
       "        0.781, 0.577, 0.779, 0.784, 0.532, 0.778, 0.785, 0.694, 0.786,\n",
       "        0.78 , 0.606, 0.782, 0.785, 0.666, 0.783, 0.777, 0.706, 0.766,\n",
       "        0.786, 0.605, 0.783, 0.779, 0.587, 0.781, 0.785, 0.52 , 0.783,\n",
       "        0.788, 0.529, 0.771, 0.777, 0.539, 0.788, 0.772, 0.441, 0.784,\n",
       "        0.782, 0.589, 0.773, 0.782, 0.784, 0.79 , 0.785, 0.785, 0.786,\n",
       "        0.787, 0.784, 0.781, 0.786, 0.785, 0.786, 0.784, 0.784, 0.783,\n",
       "        0.788, 0.784, 0.784, 0.785, 0.784, 0.783, 0.784, 0.783, 0.785,\n",
       "        0.785, 0.782, 0.786, 0.783, 0.782, 0.785, 0.787, 0.787, 0.784,\n",
       "        0.783, 0.785, 0.783, 0.785, 0.784, 0.786, 0.784, 0.784, 0.785,\n",
       "        0.785, 0.785, 0.784, 0.784, 0.785, 0.785, 0.784, 0.773, 0.78 ,\n",
       "        0.779, 0.78 , 0.787, 0.772, 0.777, 0.773, 0.769, 0.775, 0.781,\n",
       "        0.786, 0.789, 0.778, 0.779, 0.783, 0.781, 0.776, 0.768, 0.778,\n",
       "        0.76 , 0.775, 0.774, 0.786, 0.786, 0.784, 0.78 , 0.789, 0.782,\n",
       "        0.772, 0.766, 0.775, 0.772, 0.77 , 0.774, 0.787, 0.784, 0.791,\n",
       "        0.789, 0.784, 0.781, 0.779, 0.777, 0.775, 0.779, 0.785, 0.779]),\n",
       " 'mean_test_precision_micro': array([0.7566, 0.773 , 0.7658, 0.7594, 0.7604, 0.7646, 0.7456, 0.7452,\n",
       "        0.7382, 0.7284, 0.7414, 0.7364, 0.7702, 0.763 , 0.7686, 0.7464,\n",
       "        0.7516, 0.7568, 0.7434, 0.7458, 0.739 , 0.7342, 0.7356, 0.7374,\n",
       "        0.7704, 0.77  , 0.7676, 0.7532, 0.762 , 0.758 , 0.7502, 0.744 ,\n",
       "        0.739 , 0.7426, 0.738 , 0.7346, 0.7778, 0.772 , 0.7696, 0.761 ,\n",
       "        0.763 , 0.7658, 0.7504, 0.7498, 0.7594, 0.7498, 0.7482, 0.7482,\n",
       "        0.7644, 0.766 , 0.774 , 0.7446, 0.7566, 0.7622, 0.7438, 0.7406,\n",
       "        0.7486, 0.7284, 0.7272, 0.7458, 0.768 , 0.7672, 0.7686, 0.7496,\n",
       "        0.75  , 0.7556, 0.7542, 0.7484, 0.7364, 0.7336, 0.7366, 0.7266,\n",
       "        0.7756, 0.7658, 0.7686, 0.7506, 0.756 , 0.7612, 0.7508, 0.7414,\n",
       "        0.7496, 0.7378, 0.7292, 0.7384, 0.7784, 0.767 , 0.7712, 0.7604,\n",
       "        0.7612, 0.7526, 0.7518, 0.7456, 0.754 , 0.7362, 0.7368, 0.7466,\n",
       "        0.7748, 0.5462, 0.7764, 0.7774, 0.5024, 0.7764, 0.7774, 0.515 ,\n",
       "        0.7742, 0.7736, 0.4962, 0.777 , 0.7766, 0.5386, 0.7774, 0.7768,\n",
       "        0.552 , 0.7744, 0.7766, 0.5072, 0.7778, 0.779 , 0.515 , 0.7774,\n",
       "        0.7748, 0.4682, 0.7784, 0.7778, 0.5308, 0.776 , 0.776 , 0.5268,\n",
       "        0.7764, 0.778 , 0.5368, 0.776 , 0.776 , 0.531 , 0.7768, 0.7762,\n",
       "        0.4952, 0.7756, 0.78  , 0.478 , 0.7782, 0.7782, 0.541 , 0.7782,\n",
       "        0.7754, 0.5986, 0.7752, 0.7778, 0.5952, 0.778 , 0.773 , 0.5828,\n",
       "        0.7788, 0.7734, 0.6306, 0.774 , 0.7732, 0.5482, 0.7738, 0.7728,\n",
       "        0.6212, 0.7762, 0.772 , 0.5972, 0.7752, 0.777 , 0.6866, 0.7768,\n",
       "        0.7708, 0.5098, 0.7702, 0.7748, 0.5496, 0.7762, 0.7736, 0.6172,\n",
       "        0.7724, 0.7774, 0.6458, 0.7734, 0.7746, 0.6094, 0.7718, 0.7764,\n",
       "        0.5206, 0.778 , 0.774 , 0.5842, 0.777 , 0.7776, 0.5142, 0.7728,\n",
       "        0.7772, 0.7782, 0.7798, 0.7782, 0.7782, 0.7788, 0.7782, 0.778 ,\n",
       "        0.7772, 0.7792, 0.7774, 0.7786, 0.7768, 0.7768, 0.7782, 0.779 ,\n",
       "        0.7776, 0.778 , 0.7766, 0.778 , 0.778 , 0.778 , 0.7774, 0.7794,\n",
       "        0.7812, 0.7768, 0.7788, 0.7776, 0.777 , 0.7784, 0.7782, 0.7784,\n",
       "        0.778 , 0.7778, 0.7782, 0.7766, 0.7786, 0.7778, 0.7772, 0.7782,\n",
       "        0.7776, 0.779 , 0.7782, 0.7784, 0.778 , 0.778 , 0.777 , 0.778 ,\n",
       "        0.7784, 0.7772, 0.7766, 0.7764, 0.7762, 0.7762, 0.7746, 0.7762,\n",
       "        0.7716, 0.768 , 0.7732, 0.7766, 0.7784, 0.777 , 0.7754, 0.778 ,\n",
       "        0.7772, 0.777 , 0.772 , 0.7728, 0.7732, 0.768 , 0.7712, 0.7682,\n",
       "        0.7794, 0.7774, 0.7762, 0.7744, 0.7764, 0.7788, 0.7736, 0.7744,\n",
       "        0.7748, 0.773 , 0.7718, 0.7716, 0.7774, 0.776 , 0.777 , 0.7788,\n",
       "        0.7784, 0.776 , 0.7746, 0.779 , 0.777 , 0.7764, 0.7774, 0.7732]),\n",
       " 'std_test_precision_micro': array([0.01770424, 0.01285302, 0.01220492, 0.01034601, 0.00904655,\n",
       "        0.01567929, 0.01177455, 0.00904212, 0.01756588, 0.0119432 ,\n",
       "        0.00906863, 0.00763151, 0.01083328, 0.01178134, 0.01207642,\n",
       "        0.01109234, 0.005004  , 0.0103034 , 0.00770973, 0.01126765,\n",
       "        0.01285302, 0.00813388, 0.01032666, 0.01290891, 0.01422111,\n",
       "        0.01392839, 0.00997196, 0.00448999, 0.00887694, 0.01484587,\n",
       "        0.01649727, 0.00931665, 0.0086487 , 0.01217539, 0.0128841 ,\n",
       "        0.01022937, 0.01586695, 0.01176435, 0.01217539, 0.0090111 ,\n",
       "        0.01442221, 0.01014692, 0.00976934, 0.00730479, 0.00531413,\n",
       "        0.0095373 , 0.01267123, 0.00773046, 0.00436348, 0.01643168,\n",
       "        0.01245793, 0.0108185 , 0.0063435 , 0.00719444, 0.01097998,\n",
       "        0.01782807, 0.01290891, 0.01137717, 0.01293677, 0.01337759,\n",
       "        0.00718331, 0.00835225, 0.00749933, 0.00728286, 0.0121326 ,\n",
       "        0.01222457, 0.01202331, 0.00640625, 0.01352923, 0.00889044,\n",
       "        0.01261111, 0.01865047, 0.00960417, 0.00744043, 0.01125344,\n",
       "        0.0102098 , 0.00995992, 0.00231517, 0.01097998, 0.01051856,\n",
       "        0.00989141, 0.01016661, 0.01568949, 0.00553534, 0.00668132,\n",
       "        0.00384708, 0.01066583, 0.01275304, 0.00884081, 0.01700118,\n",
       "        0.01566397, 0.0108922 , 0.00946573, 0.00713863, 0.00800999,\n",
       "        0.0112712 , 0.01177115, 0.10630974, 0.00900222, 0.00773563,\n",
       "        0.06571027, 0.00840476, 0.01163787, 0.03839792, 0.01165161,\n",
       "        0.00976934, 0.00507543, 0.0083666 , 0.00868562, 0.06667713,\n",
       "        0.00976934, 0.00735935, 0.06036224, 0.00697424, 0.00796492,\n",
       "        0.06842339, 0.01016661, 0.01011929, 0.03428119, 0.00958332,\n",
       "        0.01170299, 0.05540541, 0.0095205 , 0.01032279, 0.06131688,\n",
       "        0.01078888, 0.01178134, 0.0479975 , 0.00741889, 0.00931665,\n",
       "        0.02899931, 0.01060189, 0.01056409, 0.04178995, 0.01146124,\n",
       "        0.01038075, 0.00248193, 0.01170641, 0.01136662, 0.07986989,\n",
       "        0.00847113, 0.00945304, 0.05075825, 0.01083328, 0.00875443,\n",
       "        0.04043563, 0.01538051, 0.00863481, 0.09059007, 0.00981835,\n",
       "        0.01048809, 0.07829534, 0.01144378, 0.01068831, 0.04562718,\n",
       "        0.01331165, 0.01299846, 0.04388804, 0.01158275, 0.01097998,\n",
       "        0.0588809 , 0.01147868, 0.00993982, 0.04970875, 0.01222129,\n",
       "        0.01256981, 0.03132156, 0.01312098, 0.01406272, 0.12844828,\n",
       "        0.0113031 , 0.01285924, 0.10266762, 0.01604244, 0.0074458 ,\n",
       "        0.02270154, 0.01332066, 0.01481351, 0.06766801, 0.01654811,\n",
       "        0.01019019, 0.09042699, 0.01137365, 0.01264278, 0.02598923,\n",
       "        0.01516575, 0.00829458, 0.09222885, 0.01063955, 0.01028786,\n",
       "        0.12042159, 0.01064707, 0.01490503, 0.01442775, 0.0156512 ,\n",
       "        0.01515784, 0.01483779, 0.01572768, 0.01633891, 0.01623576,\n",
       "        0.0156512 , 0.01471598, 0.0153961 , 0.01625546, 0.01452446,\n",
       "        0.01718604, 0.01624069, 0.01509967, 0.01369087, 0.01469694,\n",
       "        0.01508111, 0.01507315, 0.01581139, 0.0154013 , 0.01438888,\n",
       "        0.01478648, 0.01330263, 0.0147973 , 0.01310572, 0.01443052,\n",
       "        0.01492649, 0.01379275, 0.01503862, 0.01565375, 0.01688787,\n",
       "        0.01468877, 0.01543243, 0.01431922, 0.01501466, 0.0136587 ,\n",
       "        0.01301384, 0.01411949, 0.01473228, 0.01447757, 0.01485126,\n",
       "        0.01641463, 0.01479189, 0.0151921 , 0.01596246, 0.01389964,\n",
       "        0.01204326, 0.01340746, 0.01197664, 0.0102684 , 0.01543243,\n",
       "        0.0136147 , 0.01297074, 0.00798499, 0.00900222, 0.00985901,\n",
       "        0.01028397, 0.01066958, 0.01363231, 0.01529706, 0.01312402,\n",
       "        0.01412799, 0.01642437, 0.01143678, 0.00969536, 0.00943186,\n",
       "        0.01435827, 0.010469  , 0.00919565, 0.00886341, 0.01202664,\n",
       "        0.01349963, 0.01489161, 0.01395134, 0.01272164, 0.01230285,\n",
       "        0.00991161, 0.01013114, 0.01337759, 0.00920869, 0.00746726,\n",
       "        0.00902441, 0.01275304, 0.01473771, 0.01658915, 0.01133843,\n",
       "        0.01336563, 0.01145426, 0.00924338, 0.01308434, 0.00954987,\n",
       "        0.01275304, 0.01078147, 0.01165161]),\n",
       " 'rank_test_precision_micro': array([198, 147, 180, 194, 192, 183, 225, 227, 239, 253, 233, 245, 165,\n",
       "        186, 171, 222, 207, 197, 231, 223, 237, 250, 248, 242, 164, 167,\n",
       "        176, 204, 188, 196, 211, 229, 236, 232, 240, 249,  54, 154, 168,\n",
       "        191, 185, 180, 210, 214, 195, 213, 219, 220, 184, 179, 134, 228,\n",
       "        199, 187, 230, 235, 217, 253, 255, 223, 173, 177, 169, 215, 212,\n",
       "        201, 202, 218, 245, 251, 244, 256, 117, 180, 169, 209, 200, 189,\n",
       "        208, 233, 215, 241, 252, 238,  18, 178, 161, 193, 189, 205, 206,\n",
       "        225, 203, 247, 243, 221, 123, 271,  99,  62, 284,  99,  62, 280,\n",
       "        133, 138, 285,  79,  92, 273,  61,  89, 268, 131,  92, 283,  51,\n",
       "          7, 279,  62, 125, 288,  18,  54, 276, 111, 111, 277,  99,  45,\n",
       "        274, 111, 111, 275,  85, 104, 286, 117,   2, 287,  27,  27, 272,\n",
       "         27, 120, 263, 121,  51, 265,  42, 147, 267,  12, 141, 259, 134,\n",
       "        146, 270, 137, 150, 260, 104, 154, 264, 121,  79, 257,  89, 163,\n",
       "        282, 165, 124, 269, 104, 140, 261, 153,  62, 258, 141, 127, 262,\n",
       "        157,  99, 278,  38, 134, 266,  76,  58, 281, 150,  71,  27,   3,\n",
       "         27,  27,  12,  26,  38,  71,   6,  62,  16,  85,  85,  27,   7,\n",
       "         58,  45,  91,  45,  42,  45,  62,   4,   1,  85,  11,  57,  82,\n",
       "         18,  27,  25,  38,  54,  27,  92,  16,  51,  71,  27,  58,   7,\n",
       "         27,  18,  45,  38,  76,  42,  18,  71,  92,  99, 104, 104, 127,\n",
       "        104, 159, 173, 143,  92,  24,  79, 119,  45,  71,  76, 154, 152,\n",
       "        143, 173, 161, 172,   4,  62, 104, 131,  97,  12, 138, 130, 125,\n",
       "        147, 157, 159,  70, 111,  82,  12,  18, 111, 127,  10,  82,  97,\n",
       "         62, 143]),\n",
       " 'split0_test_roc_auc_ovo': array([0.84638464, 0.85372937, 0.84906491, 0.83807981, 0.8290029 ,\n",
       "        0.8340394 , 0.8229503 , 0.82554655, 0.79535154, 0.78819882,\n",
       "        0.82093009, 0.8150135 , 0.85535754, 0.85759376, 0.84840084,\n",
       "        0.82790679, 0.82262226, 0.83071507, 0.81641364, 0.82993099,\n",
       "        0.80660466, 0.80748875, 0.81514551, 0.82140614, 0.84350835,\n",
       "        0.85253725, 0.84578458, 0.84086409, 0.84386039, 0.84214421,\n",
       "        0.81926993, 0.82257426, 0.81749775, 0.82185819, 0.83235124,\n",
       "        0.83329933, 0.85354535, 0.84570857, 0.85083708, 0.84442844,\n",
       "        0.84456046, 0.85120512, 0.82611461, 0.830047  , 0.82982298,\n",
       "        0.8270347 , 0.82492249, 0.82858286, 0.85317732, 0.85114911,\n",
       "        0.85632563, 0.82817082, 0.83170717, 0.8360196 , 0.81822182,\n",
       "        0.79510551, 0.81521352, 0.79648365, 0.79216722, 0.78669467,\n",
       "        0.85472947, 0.85677768, 0.85327333, 0.82947495, 0.83190719,\n",
       "        0.83107111, 0.82912691, 0.83075108, 0.79935994, 0.81150915,\n",
       "        0.81708571, 0.79479948, 0.85365337, 0.84993699, 0.85441344,\n",
       "        0.82571057, 0.8390239 , 0.84455246, 0.82244224, 0.81515752,\n",
       "        0.81971397, 0.7889509 , 0.81930993, 0.81240124, 0.85619762,\n",
       "        0.8560136 , 0.85751775, 0.84653665, 0.82919892, 0.82584658,\n",
       "        0.83211121, 0.83027103, 0.81919792, 0.80544054, 0.81882588,\n",
       "        0.81989399, 0.86279828, 0.84171217, 0.86184618, 0.8619542 ,\n",
       "        0.22543054, 0.86309831, 0.86162216, 0.25674967, 0.86420242,\n",
       "        0.86315432, 0.70957096, 0.86189019, 0.86338234, 0.20174017,\n",
       "        0.86208221, 0.86126213, 0.8159856 , 0.86188219, 0.86346635,\n",
       "        0.73112111, 0.86093009, 0.86279028, 0.61843384, 0.86312231,\n",
       "        0.86219022, 0.45788979, 0.86164616, 0.86087009, 0.77152915,\n",
       "        0.86363436, 0.86141414, 0.8059966 , 0.86405441, 0.86273827,\n",
       "        0.69676968, 0.8629623 , 0.86293429, 0.23470747, 0.86143414,\n",
       "        0.85865387, 0.51383538, 0.86316632, 0.86084608, 0.25731373,\n",
       "        0.86317832, 0.86290229, 0.34279028, 0.86305431, 0.85587759,\n",
       "        0.63566757, 0.85992599, 0.85962996, 0.73334933, 0.860034  ,\n",
       "        0.86049805, 0.60583258, 0.86128613, 0.85993399, 0.61530153,\n",
       "        0.85650565, 0.86073007, 0.6150095 , 0.85134113, 0.85479748,\n",
       "        0.52234423, 0.86217822, 0.85860586, 0.60865287, 0.85835384,\n",
       "        0.86155816, 0.78818682, 0.86335834, 0.85411341, 0.43635964,\n",
       "        0.85612961, 0.85344934, 0.66807481, 0.85874987, 0.85834183,\n",
       "        0.66613861, 0.8570217 , 0.85759776, 0.6960496 , 0.85746975,\n",
       "        0.85476548, 0.50585459, 0.85828183, 0.86167417, 0.49122512,\n",
       "        0.85924592, 0.8620142 , 0.76716872, 0.86010601, 0.85965397,\n",
       "        0.16266427, 0.86093009, 0.8630103 , 0.86311031, 0.86361836,\n",
       "        0.86333433, 0.86333833, 0.86357436, 0.86344634, 0.86376238,\n",
       "        0.86340234, 0.86363436, 0.8639824 , 0.86364636, 0.86283028,\n",
       "        0.86156216, 0.86281428, 0.86335034, 0.86383038, 0.86342234,\n",
       "        0.86334633, 0.86315832, 0.86353835, 0.86381038, 0.8639864 ,\n",
       "        0.86360236, 0.86279428, 0.86326233, 0.86370637, 0.86345835,\n",
       "        0.86315032, 0.86318632, 0.86332633, 0.86345035, 0.86331433,\n",
       "        0.86329033, 0.86336234, 0.86357436, 0.8640224 , 0.86378638,\n",
       "        0.8630183 , 0.86364636, 0.86390639, 0.86352635, 0.8639864 ,\n",
       "        0.86372237, 0.86387039, 0.86389839, 0.86393839, 0.86391039,\n",
       "        0.85830183, 0.86278628, 0.86325033, 0.86281428, 0.85641364,\n",
       "        0.86129813, 0.8559976 , 0.8589779 , 0.85550955, 0.85488149,\n",
       "        0.85278928, 0.85677768, 0.86130213, 0.860042  , 0.86057806,\n",
       "        0.85606161, 0.86009401, 0.86075008, 0.85725773, 0.85462146,\n",
       "        0.85562156, 0.8560456 , 0.85675768, 0.85066907, 0.85968997,\n",
       "        0.8619542 , 0.86026603, 0.85660166, 0.85588959, 0.85592159,\n",
       "        0.85388539, 0.85386939, 0.85210121, 0.85741374, 0.8549975 ,\n",
       "        0.85830183, 0.86060206, 0.86137414, 0.86219822, 0.86123012,\n",
       "        0.86041804, 0.8630223 , 0.85625363, 0.86170617, 0.86216622,\n",
       "        0.85784578, 0.85706571, 0.8580418 ]),\n",
       " 'split1_test_roc_auc_ovo': array([0.847198  , 0.85081052, 0.85102655, 0.84679394, 0.83351603,\n",
       "        0.83301995, 0.8097566 , 0.80959658, 0.82526684, 0.79151798,\n",
       "        0.80658015, 0.80049527, 0.84985038, 0.84959434, 0.85620729,\n",
       "        0.8389008 , 0.83227985, 0.8367885 , 0.82252244, 0.81574147,\n",
       "        0.82402666, 0.81116881, 0.81056072, 0.8187459 , 0.847238  ,\n",
       "        0.84400554, 0.8499664 , 0.83374006, 0.83847674, 0.82939943,\n",
       "        0.8138972 , 0.81933798, 0.82880335, 0.81342513, 0.81620953,\n",
       "        0.8020955 , 0.84389352, 0.85005441, 0.85522715, 0.8410011 ,\n",
       "        0.84324143, 0.83704453, 0.82368661, 0.83079964, 0.83449617,\n",
       "        0.81057272, 0.81861388, 0.82866333, 0.85053448, 0.8527948 ,\n",
       "        0.84194124, 0.83134771, 0.82406867, 0.83790466, 0.82547887,\n",
       "        0.82174233, 0.80608808, 0.80258757, 0.80750428, 0.81480533,\n",
       "        0.84806212, 0.84689795, 0.85167464, 0.8298955 , 0.81715167,\n",
       "        0.84334144, 0.83080364, 0.8118129 , 0.80284361, 0.82039414,\n",
       "        0.80218752, 0.81380519, 0.85215471, 0.84946232, 0.84840217,\n",
       "        0.84220528, 0.83921685, 0.8396089 , 0.81973804, 0.82912339,\n",
       "        0.82557488, 0.81907795, 0.80126338, 0.79104591, 0.84826615,\n",
       "        0.85437503, 0.84630587, 0.83736058, 0.83992495, 0.8277952 ,\n",
       "        0.82931142, 0.81590949, 0.82616297, 0.81904594, 0.82204237,\n",
       "        0.80829639, 0.86092397, 0.51355795, 0.85886768, 0.85837161,\n",
       "        0.68938327, 0.85824359, 0.86022387, 0.63170297, 0.85886368,\n",
       "        0.8597398 , 0.76083356, 0.85829559, 0.85720744, 0.57059817,\n",
       "        0.86064793, 0.86062393, 0.81290506, 0.85741947, 0.86019187,\n",
       "        0.76054952, 0.861132  , 0.86019587, 0.750024  , 0.85962379,\n",
       "        0.86016786, 0.33128771, 0.85993183, 0.85980381, 0.65504233,\n",
       "        0.86132003, 0.8597398 , 0.38134691, 0.86046391, 0.85943576,\n",
       "        0.79633467, 0.8604279 , 0.86101599, 0.78107648, 0.85975981,\n",
       "        0.86106399, 0.30942456, 0.85995183, 0.8603879 , 0.63341121,\n",
       "        0.86124802, 0.85953577, 0.68534269, 0.86057592, 0.86097198,\n",
       "        0.64569298, 0.854147  , 0.85859564, 0.49779968, 0.85808756,\n",
       "        0.8562513 , 0.70035285, 0.86021987, 0.85753549, 0.73740219,\n",
       "        0.86262822, 0.85219472, 0.64729321, 0.86295227, 0.86404042,\n",
       "        0.81186891, 0.8590557 , 0.8569354 , 0.53422493, 0.86054792,\n",
       "        0.85989983, 0.80740027, 0.86222816, 0.85822358, 0.60303884,\n",
       "        0.85547919, 0.86329631, 0.41477973, 0.85844362, 0.8569594 ,\n",
       "        0.78735738, 0.85793154, 0.85897169, 0.74817174, 0.8583436 ,\n",
       "        0.85541518, 0.72042774, 0.85720344, 0.85588725, 0.50709302,\n",
       "        0.861104  , 0.85957178, 0.61499256, 0.85749148, 0.8604399 ,\n",
       "        0.72660863, 0.85671537, 0.8604039 , 0.86089997, 0.86065193,\n",
       "        0.86097198, 0.86094398, 0.8618041 , 0.86165208, 0.86156807,\n",
       "        0.8618361 , 0.86115201, 0.86194812, 0.86215215, 0.86062393,\n",
       "        0.86055592, 0.86138804, 0.86112   , 0.86139204, 0.86202413,\n",
       "        0.86148005, 0.86131203, 0.86143605, 0.86134803, 0.86131603,\n",
       "        0.86198013, 0.86137204, 0.86191612, 0.86238818, 0.8618041 ,\n",
       "        0.861096  , 0.86147205, 0.861084  , 0.86212815, 0.86153206,\n",
       "        0.861084  , 0.86143205, 0.86140004, 0.86192412, 0.86158407,\n",
       "        0.86152406, 0.86201613, 0.8618241 , 0.86205614, 0.86218015,\n",
       "        0.86198013, 0.86237618, 0.86171209, 0.8618201 , 0.8617881 ,\n",
       "        0.85629931, 0.85953177, 0.85473908, 0.85715943, 0.85736346,\n",
       "        0.85158263, 0.85473908, 0.8534829 , 0.8548831 , 0.85193068,\n",
       "        0.85273479, 0.85429502, 0.85465907, 0.85911171, 0.85593926,\n",
       "        0.85799955, 0.85801155, 0.85596726, 0.85570322, 0.85358692,\n",
       "        0.85819958, 0.85182266, 0.85199469, 0.85169464, 0.85837961,\n",
       "        0.85923173, 0.8569474 , 0.85165464, 0.85453505, 0.8590197 ,\n",
       "        0.8548431 , 0.85244275, 0.85547519, 0.85144661, 0.85096654,\n",
       "        0.8548391 , 0.85701141, 0.86011186, 0.85814357, 0.8576315 ,\n",
       "        0.85388296, 0.8548951 , 0.85641932, 0.85719144, 0.85440703,\n",
       "        0.85573523, 0.8555312 , 0.85374294]),\n",
       " 'split2_test_roc_auc_ovo': array([0.86620873, 0.86801299, 0.86578067, 0.84577379, 0.83733458,\n",
       "        0.85939175, 0.84345746, 0.82873534, 0.83690851, 0.83238786,\n",
       "        0.81973004, 0.81744171, 0.86618473, 0.86563665, 0.86560065,\n",
       "        0.84229729, 0.84342545, 0.85720344, 0.83147573, 0.83801667,\n",
       "        0.83719256, 0.83786065, 0.81372518, 0.84229329, 0.86434447,\n",
       "        0.86732489, 0.86752892, 0.84141716, 0.85061449, 0.85265078,\n",
       "        0.85669136, 0.83502424, 0.83182778, 0.83653246, 0.81785777,\n",
       "        0.83307196, 0.86864508, 0.87007729, 0.86768895, 0.85595126,\n",
       "        0.85936375, 0.85685139, 0.8409811 , 0.83882879, 0.84496968,\n",
       "        0.84944232, 0.84819814, 0.8368325 , 0.8638604 , 0.87145749,\n",
       "        0.86158007, 0.84436159, 0.8375246 , 0.85439903, 0.8305356 ,\n",
       "        0.83904482, 0.82857732, 0.82835528, 0.82921541, 0.84764606,\n",
       "        0.86142004, 0.86278824, 0.86581668, 0.83288394, 0.8492943 ,\n",
       "        0.84496968, 0.84042502, 0.82982349, 0.82366261, 0.82158631,\n",
       "        0.83463219, 0.83082364, 0.86627274, 0.86029188, 0.86377238,\n",
       "        0.85457506, 0.84571378, 0.84586981, 0.84060505, 0.83003553,\n",
       "        0.82836728, 0.8145613 , 0.83616441, 0.81638956, 0.87303372,\n",
       "        0.86212815, 0.86890512, 0.84686995, 0.85124258, 0.85729545,\n",
       "        0.84287737, 0.84872622, 0.83640444, 0.83190779, 0.82832728,\n",
       "        0.82186235, 0.87127746, 0.49830776, 0.87048935, 0.87130947,\n",
       "        0.3028036 , 0.87191756, 0.87170553, 0.75879727, 0.8722136 ,\n",
       "        0.87186555, 0.28731337, 0.87165752, 0.87134547, 0.75200029,\n",
       "        0.87236962, 0.87233362, 0.72376822, 0.8728937 , 0.87128547,\n",
       "        0.30938455, 0.87488598, 0.87263766, 0.54465843, 0.87260966,\n",
       "        0.87235762, 0.31078075, 0.87200557, 0.87232161, 0.68453857,\n",
       "        0.87115345, 0.87349378, 0.59779408, 0.87142949, 0.87248964,\n",
       "        0.54842697, 0.8708214 , 0.87017731, 0.5326047 , 0.87079339,\n",
       "        0.87104143, 0.62795042, 0.87255365, 0.87309373, 0.55392377,\n",
       "        0.87244963, 0.87296571, 0.60039046, 0.87298971, 0.86966523,\n",
       "        0.63676769, 0.87334576, 0.86978525, 0.67026052, 0.86960122,\n",
       "        0.8722456 , 0.63689571, 0.87322174, 0.87066938, 0.784721  ,\n",
       "        0.87324175, 0.86463251, 0.42250084, 0.87077739, 0.8701293 ,\n",
       "        0.71751932, 0.87026932, 0.86850106, 0.74114673, 0.86987726,\n",
       "        0.87093341, 0.72176793, 0.87136548, 0.86420845, 0.45925413,\n",
       "        0.87113744, 0.86791298, 0.73376566, 0.87260165, 0.86650078,\n",
       "        0.66270743, 0.87031333, 0.87265366, 0.80364372, 0.87067738,\n",
       "        0.86421245, 0.84739002, 0.86794898, 0.87177754, 0.46920757,\n",
       "        0.8708334 , 0.87071338, 0.7340537 , 0.87022931, 0.87304572,\n",
       "        0.71377078, 0.86756893, 0.87482197, 0.87412587, 0.87457794,\n",
       "        0.87388584, 0.87522603, 0.87454193, 0.87472596, 0.87485798,\n",
       "        0.87391384, 0.87419788, 0.87454193, 0.87470596, 0.87404186,\n",
       "        0.87480997, 0.87536205, 0.87443392, 0.87370581, 0.87436191,\n",
       "        0.87402986, 0.87453393, 0.87493399, 0.87476597, 0.87461794,\n",
       "        0.87447792, 0.8757261 , 0.87475396, 0.87486598, 0.87451393,\n",
       "        0.87388184, 0.87420989, 0.87419788, 0.87413388, 0.87448593,\n",
       "        0.87490599, 0.87473396, 0.87374182, 0.87543806, 0.87517002,\n",
       "        0.87450593, 0.8743179 , 0.87426989, 0.87424189, 0.87391384,\n",
       "        0.87418588, 0.87412187, 0.87472996, 0.875002  , 0.87396985,\n",
       "        0.87226161, 0.87484998, 0.87326975, 0.87300971, 0.87559409,\n",
       "        0.87226961, 0.87213359, 0.87247364, 0.87130547, 0.86828903,\n",
       "        0.87163352, 0.87034933, 0.87613816, 0.87280968, 0.87370981,\n",
       "        0.87480197, 0.87480597, 0.87105343, 0.87253364, 0.87474596,\n",
       "        0.87367381, 0.86891712, 0.86754093, 0.86835704, 0.87462995,\n",
       "        0.87536605, 0.87540206, 0.87509401, 0.87092541, 0.87627418,\n",
       "        0.87555808, 0.87424189, 0.87238162, 0.87178954, 0.87035333,\n",
       "        0.87118945, 0.87495399, 0.87443792, 0.87484998, 0.87469396,\n",
       "        0.87447792, 0.8736338 , 0.87542206, 0.87585412, 0.87414588,\n",
       "        0.87416988, 0.87628619, 0.87283369]),\n",
       " 'split3_test_roc_auc_ovo': array([0.84065705, 0.84248532, 0.85185467, 0.8298635 , 0.83432814,\n",
       "        0.84680194, 0.8243147 , 0.82586692, 0.8187179 , 0.819478  ,\n",
       "        0.81392521, 0.81789778, 0.85116257, 0.84267334, 0.84892224,\n",
       "        0.82603495, 0.833356  , 0.8222144 , 0.81118881, 0.82811525,\n",
       "        0.83035557, 0.82021011, 0.81202893, 0.82028612, 0.83881279,\n",
       "        0.84457762, 0.84895025, 0.82453473, 0.840293  , 0.84063705,\n",
       "        0.81846986, 0.8305596 , 0.812493  , 0.81164488, 0.819438  ,\n",
       "        0.8069642 , 0.85051447, 0.84866621, 0.84592581, 0.8361324 ,\n",
       "        0.8382047 , 0.83896081, 0.83060761, 0.82864332, 0.83607239,\n",
       "        0.82025812, 0.83089165, 0.82675905, 0.85272679, 0.84809012,\n",
       "        0.85184667, 0.83565633, 0.84700197, 0.84202525, 0.81587749,\n",
       "        0.80233954, 0.83952089, 0.82367061, 0.8187419 , 0.82434671,\n",
       "        0.85179066, 0.85319086, 0.85412699, 0.84038502, 0.82558688,\n",
       "        0.84207726, 0.82067818, 0.81810581, 0.83095966, 0.80159143,\n",
       "        0.80159543, 0.80103535, 0.85248276, 0.84969836, 0.85144261,\n",
       "        0.83309997, 0.8312837 , 0.84706198, 0.82526684, 0.83759261,\n",
       "        0.84084108, 0.81815381, 0.81164088, 0.81270103, 0.8604239 ,\n",
       "        0.86009185, 0.84925029, 0.83566834, 0.8368085 , 0.82795923,\n",
       "        0.82812725, 0.81844986, 0.84143317, 0.81630155, 0.81119281,\n",
       "        0.82725112, 0.85440303, 0.44658431, 0.85527916, 0.85670337,\n",
       "        0.76354195, 0.8576475 , 0.85400698, 0.76561025, 0.85641132,\n",
       "        0.85645933, 0.58426013, 0.85708342, 0.8562593 , 0.54815893,\n",
       "        0.85774752, 0.85826759, 0.80810837, 0.85567922, 0.85669936,\n",
       "        0.39674513, 0.85439503, 0.85791554, 0.68479461, 0.85818758,\n",
       "        0.85665936, 0.71772735, 0.8576515 , 0.85607127, 0.78140852,\n",
       "        0.85634731, 0.85675537, 0.75428462, 0.85648333, 0.85644733,\n",
       "        0.59795811, 0.85735546, 0.85599126, 0.57787121, 0.85671537,\n",
       "        0.85801555, 0.72148789, 0.85616329, 0.85715943, 0.58881679,\n",
       "        0.85784353, 0.85737946, 0.31166088, 0.85758349, 0.85009841,\n",
       "        0.72841689, 0.85465507, 0.85402698, 0.77139508, 0.8562673 ,\n",
       "        0.85592325, 0.76168568, 0.85586324, 0.85259477, 0.6896113 ,\n",
       "        0.85584724, 0.85505513, 0.66287945, 0.85011042, 0.85575923,\n",
       "        0.62286169, 0.85675937, 0.85096654, 0.72237602, 0.85717543,\n",
       "        0.8534829 , 0.71105839, 0.85449505, 0.85244675, 0.26642637,\n",
       "        0.85568722, 0.8576155 , 0.34533773, 0.85087853, 0.85302284,\n",
       "        0.70224512, 0.85237474, 0.8555392 , 0.73359364, 0.85533917,\n",
       "        0.85612328, 0.71345474, 0.85099854, 0.85443904, 0.68013394,\n",
       "        0.85065849, 0.85548319, 0.61471252, 0.85565521, 0.85295483,\n",
       "        0.40858284, 0.85482309, 0.85705542, 0.85757149, 0.85827959,\n",
       "        0.85789154, 0.85825159, 0.85790354, 0.85772351, 0.85783553,\n",
       "        0.85784753, 0.85786753, 0.85797955, 0.85793954, 0.85808756,\n",
       "        0.8576635 , 0.85718343, 0.8576155 , 0.8576395 , 0.85788754,\n",
       "        0.85839961, 0.85804356, 0.85798755, 0.85780752, 0.85803156,\n",
       "        0.85790754, 0.8576275 , 0.85789954, 0.85771151, 0.85736346,\n",
       "        0.85781953, 0.85760349, 0.85746347, 0.85776352, 0.85776352,\n",
       "        0.85785953, 0.85778752, 0.85795955, 0.85728345, 0.85793954,\n",
       "        0.8576195 , 0.85785953, 0.85795155, 0.85795155, 0.85795555,\n",
       "        0.85818358, 0.85803956, 0.85778752, 0.85791554, 0.85798755,\n",
       "        0.85430302, 0.85706742, 0.8583116 , 0.85481909, 0.85309485,\n",
       "        0.85796355, 0.85411099, 0.85536317, 0.85856363, 0.85182266,\n",
       "        0.85543918, 0.85587925, 0.85608728, 0.85638732, 0.85895969,\n",
       "        0.85650334, 0.8576475 , 0.8548831 , 0.85404698, 0.85749148,\n",
       "        0.85171465, 0.85819558, 0.85792754, 0.85789554, 0.85939975,\n",
       "        0.85701141, 0.85535517, 0.85738746, 0.85593926, 0.8583516 ,\n",
       "        0.85640332, 0.85505513, 0.8603959 , 0.85865165, 0.85882767,\n",
       "        0.85413099, 0.85937975, 0.85877966, 0.85809557, 0.85912771,\n",
       "        0.86057992, 0.85678338, 0.8583196 , 0.85989983, 0.8618081 ,\n",
       "        0.85506713, 0.85650334, 0.8583076 ]),\n",
       " 'split4_test_roc_auc_ovo': array([0.8396129 , 0.84545775, 0.84416556, 0.82761118, 0.83657247,\n",
       "        0.83945288, 0.82891136, 0.80391176, 0.7951705 , 0.80479189,\n",
       "        0.81586548, 0.79462643, 0.83976893, 0.84642188, 0.84590581,\n",
       "        0.81672961, 0.82610696, 0.82858332, 0.80084332, 0.80501992,\n",
       "        0.80457586, 0.80155542, 0.79822694, 0.79268215, 0.84537773,\n",
       "        0.84395353, 0.8416412 , 0.82454673, 0.84371349, 0.83147173,\n",
       "        0.81930998, 0.79712679, 0.8076603 , 0.79565857, 0.8076403 ,\n",
       "        0.7958386 , 0.85343489, 0.84983038, 0.83846474, 0.82191836,\n",
       "        0.82687507, 0.82942344, 0.82375862, 0.81998208, 0.82762318,\n",
       "        0.81196492, 0.81000464, 0.80605607, 0.84953033, 0.8388728 ,\n",
       "        0.84968235, 0.79509449, 0.83428814, 0.82955146, 0.80424581,\n",
       "        0.78913364, 0.80931454, 0.79741883, 0.80089133, 0.80615209,\n",
       "        0.84384951, 0.85177466, 0.85538318, 0.82551087, 0.8222064 ,\n",
       "        0.81307308, 0.81202893, 0.81567346, 0.78660527, 0.81728169,\n",
       "        0.79969116, 0.78230465, 0.84882223, 0.84767006, 0.84876222,\n",
       "        0.82029012, 0.82342657, 0.8375046 , 0.80659615, 0.8110808 ,\n",
       "        0.80613208, 0.80355971, 0.78145253, 0.79849498, 0.85106255,\n",
       "        0.84477365, 0.85272679, 0.82913139, 0.82853931, 0.8249948 ,\n",
       "        0.80855243, 0.81090877, 0.81045671, 0.79676273, 0.80059529,\n",
       "        0.80782033, 0.86101599, 0.53640524, 0.85960378, 0.85891168,\n",
       "        0.32053816, 0.85674737, 0.86087197, 0.18088605, 0.86003985,\n",
       "        0.85815557, 0.42096862, 0.85843561, 0.85803556, 0.736078  ,\n",
       "        0.85943976, 0.85733146, 0.68368645, 0.85820758, 0.85792354,\n",
       "        0.46576707, 0.85866765, 0.86017186, 0.66265942, 0.85958378,\n",
       "        0.85951177, 0.48035717, 0.85811157, 0.8590037 , 0.40860284,\n",
       "        0.85977981, 0.85827159, 0.74743163, 0.85979981, 0.85939975,\n",
       "        0.80047527, 0.85918772, 0.86035189, 0.76740651, 0.8596958 ,\n",
       "        0.85729145, 0.33493223, 0.85853563, 0.85977181, 0.65560641,\n",
       "        0.85949177, 0.85877166, 0.53483702, 0.8597398 , 0.85617129,\n",
       "        0.51505817, 0.85397897, 0.8527828 , 0.4708358 , 0.85599126,\n",
       "        0.85472708, 0.48513786, 0.85771151, 0.85811157, 0.64077627,\n",
       "        0.85844362, 0.8548911 , 0.53132451, 0.85507113, 0.8569634 ,\n",
       "        0.76725048, 0.85743947, 0.85746748, 0.65821878, 0.85389496,\n",
       "        0.85865565, 0.73553792, 0.8507265 , 0.85527516, 0.81425725,\n",
       "        0.85021043, 0.85748348, 0.70004881, 0.85609928, 0.8527908 ,\n",
       "        0.65643453, 0.85729145, 0.85777152, 0.68689491, 0.85962779,\n",
       "        0.85879967, 0.60406699, 0.85581524, 0.85288282, 0.60397897,\n",
       "        0.85519115, 0.85226273, 0.40517435, 0.85400298, 0.85771551,\n",
       "        0.61562865, 0.85569122, 0.85756749, 0.85717143, 0.85799955,\n",
       "        0.85841561, 0.85785553, 0.85770751, 0.85786753, 0.85896369,\n",
       "        0.85855563, 0.85855563, 0.8583596 , 0.85839961, 0.85740347,\n",
       "        0.85781953, 0.85727945, 0.85811557, 0.8583396 , 0.8583156 ,\n",
       "        0.85737546, 0.85875966, 0.85846362, 0.85888768, 0.85805956,\n",
       "        0.85861964, 0.85784353, 0.85779952, 0.85751148, 0.85863164,\n",
       "        0.85841961, 0.85877166, 0.85845962, 0.85809557, 0.85829559,\n",
       "        0.85896769, 0.85883567, 0.85827559, 0.85816758, 0.85852363,\n",
       "        0.85812757, 0.85855563, 0.85883967, 0.85856363, 0.85878767,\n",
       "        0.85853163, 0.85884767, 0.85864765, 0.85909971, 0.85918372,\n",
       "        0.85559521, 0.85405498, 0.85423101, 0.85251076, 0.85615929,\n",
       "        0.85449105, 0.85089853, 0.85672737, 0.85283881, 0.85007441,\n",
       "        0.84855019, 0.85200269, 0.85669936, 0.85528316, 0.85618729,\n",
       "        0.85201869, 0.85319086, 0.85200669, 0.85103055, 0.84629787,\n",
       "        0.85085452, 0.84505369, 0.84921029, 0.85107455, 0.85637132,\n",
       "        0.85339489, 0.85608728, 0.85252676, 0.85648733, 0.8520627 ,\n",
       "        0.85238274, 0.85126658, 0.85038246, 0.84961835, 0.85117057,\n",
       "        0.85183066, 0.8555632 , 0.85631931, 0.85827959, 0.85687939,\n",
       "        0.85282681, 0.85427102, 0.85313485, 0.85199469, 0.85447904,\n",
       "        0.8513986 , 0.85534317, 0.85413099]),\n",
       " 'mean_test_roc_auc_ovo': array([0.84801227, 0.85209919, 0.85237847, 0.83762444, 0.83415082,\n",
       "        0.84254119, 0.82587808, 0.81873143, 0.81428306, 0.80727491,\n",
       "        0.81540619, 0.80909494, 0.85246483, 0.852384  , 0.85300737,\n",
       "        0.83037389, 0.83155811, 0.83510094, 0.81648879, 0.82336486,\n",
       "        0.82055106, 0.81565675, 0.80993746, 0.81908272, 0.84785627,\n",
       "        0.85047977, 0.85077427, 0.83302056, 0.84339162, 0.83926064,\n",
       "        0.82552767, 0.82092457, 0.81965644, 0.81582385, 0.81869937,\n",
       "        0.81425392, 0.85400667, 0.85286737, 0.85162875, 0.83988631,\n",
       "        0.84244908, 0.84269706, 0.82902971, 0.82966017, 0.83459688,\n",
       "        0.82385456, 0.82652616, 0.82537876, 0.85396586, 0.85247287,\n",
       "        0.85227519, 0.82692619, 0.83491811, 0.83998   , 0.81887192,\n",
       "        0.80947317, 0.81974287, 0.80970319, 0.80970403, 0.81592897,\n",
       "        0.85197036, 0.85428588, 0.85605496, 0.83163006, 0.82922929,\n",
       "        0.83490651, 0.82661254, 0.82123335, 0.80868622, 0.81447254,\n",
       "        0.8110384 , 0.80455366, 0.85467716, 0.85141192, 0.85335856,\n",
       "        0.8351762 , 0.83573296, 0.84291955, 0.82292966, 0.82459797,\n",
       "        0.82412586, 0.80886073, 0.80996623, 0.80620654, 0.85779679,\n",
       "        0.85547646, 0.85494117, 0.83911338, 0.83714285, 0.83277825,\n",
       "        0.82819594, 0.82485307, 0.82673104, 0.81389171, 0.81619673,\n",
       "        0.81702484, 0.86208375, 0.56731349, 0.86121723, 0.86145006,\n",
       "        0.46033951, 0.86153087, 0.8616861 , 0.51874924, 0.86234617,\n",
       "        0.86187491, 0.55258933, 0.86147247, 0.86124602, 0.56171511,\n",
       "        0.86245741, 0.86196374, 0.76889074, 0.86121643, 0.86191332,\n",
       "        0.53271348, 0.86200215, 0.86274224, 0.65211406, 0.86262542,\n",
       "        0.86217737, 0.45960855, 0.86186933, 0.8616141 , 0.66022428,\n",
       "        0.86244699, 0.86193494, 0.65737077, 0.86244619, 0.86210215,\n",
       "        0.68799294, 0.86215096, 0.86209415, 0.57873327, 0.8616797 ,\n",
       "        0.86121326, 0.5015261 , 0.86207414, 0.86225179, 0.53781438,\n",
       "        0.86284225, 0.86231098, 0.49500426, 0.86278865, 0.8585569 ,\n",
       "        0.63232066, 0.85921056, 0.85896413, 0.62872808, 0.85999627,\n",
       "        0.85992906, 0.63798094, 0.8616605 , 0.85976904, 0.69356246,\n",
       "        0.86133329, 0.85750071, 0.5758015 , 0.85805047, 0.86033797,\n",
       "        0.68836893, 0.86114042, 0.85849527, 0.65292386, 0.85996988,\n",
       "        0.86090599, 0.75279027, 0.86043471, 0.85685347, 0.51586724,\n",
       "        0.85772878, 0.85995152, 0.57240135, 0.85935459, 0.85752313,\n",
       "        0.69497661, 0.85898655, 0.86050677, 0.73367072, 0.86029154,\n",
       "        0.85786321, 0.67823881, 0.85804961, 0.85933216, 0.55032772,\n",
       "        0.85940659, 0.86000906, 0.62722037, 0.859497  , 0.86076198,\n",
       "        0.52545103, 0.85914574, 0.86257182, 0.86257582, 0.86302548,\n",
       "        0.86289986, 0.86312309, 0.86310629, 0.86308309, 0.86339753,\n",
       "        0.86311109, 0.86308148, 0.86336232, 0.86336873, 0.86259742,\n",
       "        0.86248222, 0.86280545, 0.86292706, 0.86298147, 0.8632023 ,\n",
       "        0.86292626, 0.8631615 , 0.86327191, 0.86332392, 0.8632023 ,\n",
       "        0.86331752, 0.86307269, 0.86312629, 0.86323671, 0.8631543 ,\n",
       "        0.86287346, 0.86304868, 0.86290626, 0.86311429, 0.86307829,\n",
       "        0.86322151, 0.86323031, 0.86299027, 0.86336712, 0.86340073,\n",
       "        0.86295907, 0.86327911, 0.86335832, 0.86326791, 0.86336472,\n",
       "        0.86332072, 0.86345113, 0.86335512, 0.86355515, 0.86336792,\n",
       "        0.85935219, 0.86165809, 0.86076035, 0.86006266, 0.85972506,\n",
       "        0.85952099, 0.85757596, 0.859405  , 0.85862011, 0.85539965,\n",
       "        0.85622939, 0.85786079, 0.8609772 , 0.86072678, 0.86107482,\n",
       "        0.85947703, 0.86074998, 0.85893211, 0.85811442, 0.85734874,\n",
       "        0.85801282, 0.85600693, 0.85668622, 0.85593817, 0.86169412,\n",
       "        0.86139166, 0.86081159, 0.85865291, 0.85875533, 0.86032595,\n",
       "        0.85861453, 0.85737515, 0.85814727, 0.85778398, 0.85726312,\n",
       "        0.85805841, 0.86150208, 0.86220458, 0.86231339, 0.86191254,\n",
       "        0.86043713, 0.86052112, 0.85990989, 0.86132925, 0.86140125,\n",
       "        0.85884332, 0.86014592, 0.8594114 ]),\n",
       " 'std_test_roc_auc_ovo': array([0.00958198, 0.00887809, 0.00721322, 0.00788919, 0.0029304 ,\n",
       "        0.00974235, 0.01085498, 0.01000507, 0.01658864, 0.01672076,\n",
       "        0.00508945, 0.00964863, 0.00855945, 0.00825106, 0.00717176,\n",
       "        0.0092299 , 0.00712954, 0.01199288, 0.01033334, 0.01161903,\n",
       "        0.01292174, 0.01264477, 0.0060559 , 0.01576503, 0.00870718,\n",
       "        0.0090259 , 0.00886576, 0.00743447, 0.00415333, 0.00833566,\n",
       "        0.01571035, 0.01313931, 0.00929182, 0.01337995, 0.00795147,\n",
       "        0.0158552 , 0.00811535, 0.00874318, 0.00977057, 0.01110761,\n",
       "        0.01050482, 0.00994818, 0.00648315, 0.00600352, 0.00602005,\n",
       "        0.01411065, 0.01285977, 0.01027146, 0.00512849, 0.0106437 ,\n",
       "        0.00658307, 0.01682001, 0.00749974, 0.00825637, 0.00897781,\n",
       "        0.01841755, 0.01252945, 0.01355937, 0.01305695, 0.02014113,\n",
       "        0.00596932, 0.00530257, 0.00502761, 0.00496627, 0.01111965,\n",
       "        0.0119603 , 0.00961842, 0.00766572, 0.01630388, 0.00732485,\n",
       "        0.01333795, 0.01660267, 0.00601605, 0.00451149, 0.00563779,\n",
       "        0.01217561, 0.00766587, 0.00370977, 0.01090953, 0.009907  ,\n",
       "        0.0113378 , 0.01138202, 0.01824952, 0.009732  , 0.00869259,\n",
       "        0.00602659, 0.00791819, 0.00678012, 0.00829194, 0.01231078,\n",
       "        0.01111747, 0.01352563, 0.01123968, 0.0120148 , 0.00955697,\n",
       "        0.00770869, 0.0054132 , 0.14034026, 0.00509416, 0.0052137 ,\n",
       "        0.22087868, 0.00564108, 0.00569496, 0.25064607, 0.00554042,\n",
       "        0.00546206, 0.17703518, 0.00533832, 0.00562223, 0.19821983,\n",
       "        0.00515667, 0.00538407, 0.05475221, 0.00617952, 0.00522142,\n",
       "        0.18117388, 0.00688312, 0.00518285, 0.06847868, 0.00525143,\n",
       "        0.00538943, 0.14537863, 0.00526228, 0.00558603, 0.13491166,\n",
       "        0.00495585, 0.00598239, 0.15448718, 0.00509459, 0.00556227,\n",
       "        0.10203153, 0.00470279, 0.00463652, 0.19850577, 0.00480388,\n",
       "        0.00507487, 0.16073229, 0.00570923, 0.0055692 , 0.14460642,\n",
       "        0.0051219 , 0.00562888, 0.1453998 , 0.00539291, 0.00653672,\n",
       "        0.06810987, 0.00740375, 0.00600433, 0.12255337, 0.00501687,\n",
       "        0.00645994, 0.09336897, 0.00608409, 0.00596742, 0.06190084,\n",
       "        0.00640689, 0.00452185, 0.08911812, 0.00778381, 0.00587585,\n",
       "        0.10409581, 0.00493255, 0.00566215, 0.07574202, 0.00540029,\n",
       "        0.00569479, 0.03804404, 0.00722184, 0.00413258, 0.18351171,\n",
       "        0.0070442 , 0.00506752, 0.1599301 , 0.0071989 , 0.00498465,\n",
       "        0.04887834, 0.00599765, 0.00617291, 0.04173687, 0.00537753,\n",
       "        0.00345849, 0.1156287 , 0.00554035, 0.00689603, 0.07958762,\n",
       "        0.00674949, 0.00631322, 0.12697189, 0.00573619, 0.00667015,\n",
       "        0.21416495, 0.00470446, 0.00648741, 0.00617655, 0.00611928,\n",
       "        0.00582733, 0.00636796, 0.00614661, 0.00622356, 0.00608961,\n",
       "        0.00577697, 0.00592067, 0.00602278, 0.00607055, 0.00603722,\n",
       "        0.00634809, 0.00666065, 0.00611908, 0.00580245, 0.00596825,\n",
       "        0.00594743, 0.00597197, 0.00617235, 0.00608559, 0.00612657,\n",
       "        0.00596259, 0.00663435, 0.00620289, 0.00631643, 0.00608186,\n",
       "        0.00582751, 0.00591642, 0.00600657, 0.0059386 , 0.00606138,\n",
       "        0.00613101, 0.0060749 , 0.00576198, 0.00651561, 0.00625421,\n",
       "        0.00611959, 0.00592061, 0.00585437, 0.00587124, 0.00571458,\n",
       "        0.00581955, 0.00575627, 0.00609164, 0.00609662, 0.00568589,\n",
       "        0.00658309, 0.00719307, 0.00703661, 0.00732328, 0.00806279,\n",
       "        0.00716141, 0.00747071, 0.00677568, 0.00660281, 0.00662709,\n",
       "        0.00801159, 0.00645128, 0.00790051, 0.0062855 , 0.00655173,\n",
       "        0.00791418, 0.00737924, 0.00668398, 0.00749916, 0.00944876,\n",
       "        0.00826956, 0.00786062, 0.00628125, 0.00674515, 0.00657168,\n",
       "        0.00752862, 0.00748603, 0.00851615, 0.00611896, 0.00833823,\n",
       "        0.00857201, 0.00852998, 0.00789507, 0.00779597, 0.00715123,\n",
       "        0.00688559, 0.00695276, 0.00634177, 0.00645954, 0.00656118,\n",
       "        0.00772061, 0.007251  , 0.00793234, 0.00796759, 0.00721127,\n",
       "        0.00794031, 0.00809477, 0.00697519]),\n",
       " 'rank_test_roc_auc_ovo': array([183, 177, 175, 194, 202, 188, 216, 232, 242, 254, 240, 251, 173,\n",
       "        174, 170, 207, 206, 198, 235, 223, 227, 239, 247, 230, 184, 182,\n",
       "        181, 203, 185, 192, 217, 226, 229, 238, 233, 243, 167, 171, 179,\n",
       "        191, 189, 187, 210, 208, 201, 222, 215, 218, 168, 172, 176, 212,\n",
       "        199, 190, 231, 250, 228, 249, 248, 237, 178, 166, 159, 205, 209,\n",
       "        200, 214, 225, 253, 241, 245, 256, 165, 180, 169, 197, 196, 186,\n",
       "        224, 220, 221, 252, 246, 255, 147, 162, 164, 193, 195, 204, 211,\n",
       "        219, 213, 244, 236, 234,  65, 276,  89,  83, 287,  80,  75, 283,\n",
       "         56,  72, 278,  82,  88, 277,  53,  68, 257,  90,  70, 281,  67,\n",
       "         47, 268,  48,  61, 288,  73,  79, 265,  54,  69, 266,  55,  63,\n",
       "        263,  62,  64, 273,  76,  91, 285,  66,  59, 280,  44,  58, 286,\n",
       "         46, 137, 270, 127, 130, 271, 111, 114, 269,  77, 116, 261,  86,\n",
       "        152, 274, 142, 105, 262,  92, 138, 267, 112,  95, 258, 104, 156,\n",
       "        284, 149, 113, 275, 124, 151, 260, 129, 102, 259, 107, 145, 264,\n",
       "        143, 126, 279, 122, 110, 272, 119,  97, 282, 128,  51,  50,  35,\n",
       "         42,  26,  29,  30,   4,  28,  31,   9,   5,  49,  52,  45,  39,\n",
       "         37,  21,  40,  23,  16,  12,  22,  14,  33,  25,  18,  24,  43,\n",
       "         34,  41,  27,  32,  20,  19,  36,   7,   3,  38,  15,  10,  17,\n",
       "          8,  13,   2,  11,   1,   6, 125,  78,  98, 109, 117, 118, 150,\n",
       "        123, 135, 163, 158, 146,  94, 100,  93, 120,  99, 131, 140, 154,\n",
       "        144, 160, 157, 161,  74,  85,  96, 134, 133, 106, 136, 153, 139,\n",
       "        148, 155, 141,  81,  60,  57,  71, 103, 101, 115,  87,  84, 132,\n",
       "        108, 121]),\n",
       " 'split0_test_jaccard': array([0.61897106, 0.62957075, 0.616     , 0.61980831, 0.60952381,\n",
       "        0.608     , 0.58615385, 0.60606061, 0.55813953, 0.56355283,\n",
       "        0.61443932, 0.59404389, 0.62619808, 0.62579618, 0.61661342,\n",
       "        0.59083728, 0.60189573, 0.60377358, 0.59780908, 0.61234177,\n",
       "        0.56965944, 0.57232704, 0.60610932, 0.57619048, 0.62698413,\n",
       "        0.64573269, 0.63942308, 0.62063492, 0.6200318 , 0.63225806,\n",
       "        0.60620155, 0.59904153, 0.5856    , 0.58995138, 0.62222222,\n",
       "        0.60031104, 0.63533225, 0.63327948, 0.625     , 0.62200957,\n",
       "        0.6263911 , 0.64065041, 0.59300477, 0.61356467, 0.6136725 ,\n",
       "        0.60218409, 0.60094637, 0.60377358, 0.62519936, 0.62962963,\n",
       "        0.65064103, 0.59335443, 0.62341772, 0.61562998, 0.58176101,\n",
       "        0.5619195 , 0.59875583, 0.56474259, 0.56666667, 0.57298137,\n",
       "        0.64057508, 0.64012739, 0.61965135, 0.60472441, 0.61489699,\n",
       "        0.60967742, 0.60596546, 0.59365079, 0.57410296, 0.58084772,\n",
       "        0.5955414 , 0.57230769, 0.63057325, 0.6128    , 0.62380952,\n",
       "        0.59196291, 0.60625   , 0.6192    , 0.6066879 , 0.60450161,\n",
       "        0.59968603, 0.56677019, 0.5945122 , 0.59873618, 0.63123994,\n",
       "        0.63949843, 0.64573269, 0.62619808, 0.6076555 , 0.59148265,\n",
       "        0.6       , 0.61006289, 0.58954041, 0.571875  , 0.60910518,\n",
       "        0.6032    , 0.62962963, 0.60586319, 0.63723917, 0.6414791 ,\n",
       "        0.505     , 0.6336    , 0.62798092, 0.        , 0.6256    ,\n",
       "        0.62798092, 0.0019802 , 0.6304    , 0.6368    , 0.00191939,\n",
       "        0.62779553, 0.63768116, 0.32954545, 0.63826367, 0.63826367,\n",
       "        0.51217039, 0.63548387, 0.6388443 , 0.01181102, 0.63242376,\n",
       "        0.62980769, 0.        , 0.6318328 , 0.63812601, 0.        ,\n",
       "        0.6288    , 0.62660256, 0.04752475, 0.63258786, 0.63461538,\n",
       "        0.505     , 0.63344051, 0.632     , 0.        , 0.6341853 ,\n",
       "        0.63461538, 0.        , 0.632     , 0.63344051, 0.11462451,\n",
       "        0.632     , 0.63929147, 0.        , 0.63123994, 0.63285024,\n",
       "        0.47424042, 0.62801932, 0.63812601, 0.48558422, 0.63929147,\n",
       "        0.61722488, 0.00980392, 0.6368    , 0.625     , 0.3503876 ,\n",
       "        0.62857143, 0.63897764, 0.29934211, 0.63387097, 0.6327504 ,\n",
       "        0.22042139, 0.62258065, 0.62838915, 0.46698113, 0.63081862,\n",
       "        0.62820513, 0.53433836, 0.63795853, 0.62559242, 0.        ,\n",
       "        0.62816456, 0.62300319, 0.25137615, 0.63192182, 0.64044944,\n",
       "        0.46013986, 0.6341853 , 0.62600321, 0.4761194 , 0.62200957,\n",
       "        0.63375796, 0.4239521 , 0.62962963, 0.63301282, 0.42669845,\n",
       "        0.64376997, 0.64583333, 0.5       , 0.62939297, 0.63081862,\n",
       "        0.29618163, 0.63593005, 0.63826367, 0.6433121 , 0.63955343,\n",
       "        0.63578275, 0.63782051, 0.63929147, 0.6368    , 0.6384    ,\n",
       "        0.63504823, 0.63929147, 0.6318328 , 0.6336    , 0.63285024,\n",
       "        0.63225806, 0.63929147, 0.63709677, 0.63723917, 0.63826367,\n",
       "        0.6368    , 0.63578275, 0.63782051, 0.6336    , 0.63826367,\n",
       "        0.63929147, 0.64412238, 0.63387097, 0.6391097 , 0.63548387,\n",
       "        0.63285024, 0.63987138, 0.63402889, 0.63081862, 0.6336    ,\n",
       "        0.6368    , 0.63723917, 0.63402889, 0.63897764, 0.63723917,\n",
       "        0.63387097, 0.63563403, 0.63402889, 0.63826367, 0.63621795,\n",
       "        0.64      , 0.63768116, 0.6384    , 0.63621795, 0.6352    ,\n",
       "        0.64193548, 0.64790997, 0.64951768, 0.63665595, 0.6414791 ,\n",
       "        0.648     , 0.63414634, 0.64423077, 0.63123994, 0.6432    ,\n",
       "        0.64057508, 0.64724919, 0.648     , 0.63285024, 0.63621795,\n",
       "        0.65008026, 0.64573269, 0.64686998, 0.63476874, 0.64126984,\n",
       "        0.6341853 , 0.6327504 , 0.63795853, 0.6256    , 0.63489499,\n",
       "        0.64069952, 0.6388443 , 0.63723917, 0.63621795, 0.64044944,\n",
       "        0.64365971, 0.64620355, 0.63809524, 0.63344051, 0.6365105 ,\n",
       "        0.63446055, 0.63709677, 0.64102564, 0.62419355, 0.64423077,\n",
       "        0.63489499, 0.647343  , 0.63782051, 0.6511254 , 0.6432    ,\n",
       "        0.64205457, 0.64630225, 0.64790997]),\n",
       " 'split1_test_jaccard': array([0.61414791, 0.63784666, 0.62903226, 0.62684124, 0.625     ,\n",
       "        0.59651899, 0.58795563, 0.57770801, 0.59229535, 0.55968992,\n",
       "        0.56535433, 0.58411215, 0.62944984, 0.61538462, 0.62439024,\n",
       "        0.62019231, 0.60416667, 0.61858974, 0.59577922, 0.5984127 ,\n",
       "        0.59584665, 0.58712716, 0.57861635, 0.566719  , 0.62764228,\n",
       "        0.62337662, 0.63311688, 0.60610932, 0.63106796, 0.58161648,\n",
       "        0.58832808, 0.60063391, 0.6048    , 0.58215962, 0.58135861,\n",
       "        0.57276995, 0.62074554, 0.6202946 , 0.62258065, 0.62398703,\n",
       "        0.60634921, 0.60127592, 0.58398744, 0.60128617, 0.60793651,\n",
       "        0.58146965, 0.59365079, 0.60289389, 0.62236629, 0.62621359,\n",
       "        0.62258065, 0.60127592, 0.61136713, 0.61550889, 0.61217949,\n",
       "        0.60663507, 0.57436709, 0.54874214, 0.57165354, 0.58699187,\n",
       "        0.6197411 , 0.62741935, 0.6187399 , 0.60567823, 0.60472441,\n",
       "        0.62520194, 0.61352657, 0.59557662, 0.58059468, 0.5846395 ,\n",
       "        0.55850234, 0.57894737, 0.64098361, 0.6211878 , 0.63278689,\n",
       "        0.61712439, 0.60967742, 0.61550889, 0.60358891, 0.57410296,\n",
       "        0.59330144, 0.60971787, 0.56595092, 0.57369255, 0.63621533,\n",
       "        0.625     , 0.62358643, 0.61352657, 0.62987013, 0.59011164,\n",
       "        0.6176    , 0.5947205 , 0.60223642, 0.58804523, 0.56893819,\n",
       "        0.57728707, 0.64516129, 0.03632887, 0.64227642, 0.63414634,\n",
       "        0.28670121, 0.64019449, 0.64772727, 0.506     , 0.63709677,\n",
       "        0.63974152, 0.        , 0.63961039, 0.63784666, 0.12844037,\n",
       "        0.64926591, 0.64215686, 0.50701403, 0.63709677, 0.64829822,\n",
       "        0.56114286, 0.64552846, 0.64926591, 0.        , 0.64563107,\n",
       "        0.64829822, 0.00582524, 0.64448052, 0.64820847, 0.506     ,\n",
       "        0.64552846, 0.64135703, 0.        , 0.64667747, 0.64724919,\n",
       "        0.52012384, 0.65252855, 0.63474026, 0.506     , 0.65024631,\n",
       "        0.64878049, 0.0019685 , 0.64878049, 0.6461039 , 0.20147874,\n",
       "        0.6465798 , 0.64332248, 0.37377049, 0.65203252, 0.65097403,\n",
       "        0.47712418, 0.63192182, 0.64772727, 0.30440771, 0.64343598,\n",
       "        0.63754045, 0.48542274, 0.6503268 , 0.64458805, 0.55231788,\n",
       "        0.6514658 , 0.63993453, 0.47820672, 0.64215686, 0.63430421,\n",
       "        0.57109557, 0.64715447, 0.63754045, 0.45823666, 0.64215686,\n",
       "        0.65048544, 0.53962901, 0.64696223, 0.64763458, 0.45822785,\n",
       "        0.63382594, 0.64715447, 0.43597884, 0.64469453, 0.64448052,\n",
       "        0.36481481, 0.64262295, 0.6514658 , 0.56658291, 0.63915858,\n",
       "        0.64102564, 0.4340836 , 0.64090177, 0.64354839, 0.16920474,\n",
       "        0.64390244, 0.63915858, 0.15168539, 0.64772727, 0.6465798 ,\n",
       "        0.53409091, 0.64991896, 0.64521452, 0.64763458, 0.65196078,\n",
       "        0.65089723, 0.64820847, 0.65316045, 0.65203252, 0.65316045,\n",
       "        0.65097403, 0.64991896, 0.65048544, 0.65422078, 0.64878049,\n",
       "        0.6525974 , 0.64926591, 0.64705882, 0.64820847, 0.65196078,\n",
       "        0.64935065, 0.6525974 , 0.65089723, 0.6514658 , 0.65097403,\n",
       "        0.65316045, 0.65691057, 0.6547812 , 0.6525974 , 0.64991896,\n",
       "        0.64869281, 0.6504065 , 0.65097403, 0.65422078, 0.65415987,\n",
       "        0.6525974 , 0.64763458, 0.65097403, 0.65203252, 0.64552846,\n",
       "        0.64320786, 0.64991896, 0.65097403, 0.65422078, 0.6525974 ,\n",
       "        0.65097403, 0.65316045, 0.65153971, 0.64926591, 0.65097403,\n",
       "        0.63857374, 0.64991896, 0.63474026, 0.63355049, 0.63961039,\n",
       "        0.62662338, 0.6411093 , 0.63295269, 0.63071895, 0.61352657,\n",
       "        0.63680782, 0.62944984, 0.63355049, 0.63902439, 0.64600326,\n",
       "        0.64552846, 0.64065041, 0.63533225, 0.62764228, 0.6372549 ,\n",
       "        0.63915858, 0.62052117, 0.62459547, 0.61550889, 0.64705882,\n",
       "        0.63947798, 0.64705882, 0.63414634, 0.6372549 , 0.64552846,\n",
       "        0.63577236, 0.63327948, 0.63739837, 0.63430421, 0.63004847,\n",
       "        0.62621359, 0.63961039, 0.64135703, 0.64065041, 0.6411093 ,\n",
       "        0.64006515, 0.63562092, 0.63398693, 0.65081967, 0.63562092,\n",
       "        0.63371151, 0.63149351, 0.62197092]),\n",
       " 'split2_test_jaccard': array([0.65105008, 0.66227348, 0.64860427, 0.63291139, 0.62861736,\n",
       "        0.64802632, 0.62279294, 0.60855784, 0.62540193, 0.60344828,\n",
       "        0.59937888, 0.60031847, 0.64569536, 0.64006515, 0.65645161,\n",
       "        0.61172742, 0.61030596, 0.63285024, 0.5993538 , 0.61464968,\n",
       "        0.6073132 , 0.6078125 , 0.58604651, 0.62123613, 0.65671642,\n",
       "        0.65139116, 0.63301282, 0.60446571, 0.61526232, 0.64433812,\n",
       "        0.64667747, 0.61538462, 0.59148265, 0.62179487, 0.58372457,\n",
       "        0.60410095, 0.67275748, 0.65353038, 0.65066225, 0.63924051,\n",
       "        0.65584416, 0.63022508, 0.61041009, 0.62461059, 0.63216561,\n",
       "        0.6263911 , 0.62640902, 0.61685215, 0.63022508, 0.66231648,\n",
       "        0.65957447, 0.61806656, 0.61356467, 0.6365105 , 0.61102362,\n",
       "        0.62861736, 0.62401264, 0.59404389, 0.58878505, 0.63422292,\n",
       "        0.6407767 , 0.63621795, 0.62969005, 0.59567901, 0.6304    ,\n",
       "        0.62197092, 0.6388443 , 0.62183544, 0.59904153, 0.58823529,\n",
       "        0.60821485, 0.61208267, 0.65789474, 0.64169381, 0.64763458,\n",
       "        0.62063492, 0.63782051, 0.61732283, 0.63207547, 0.6009539 ,\n",
       "        0.61783439, 0.59969559, 0.60697306, 0.59190031, 0.65681445,\n",
       "        0.63301282, 0.64600326, 0.64215686, 0.63285024, 0.64763458,\n",
       "        0.63479624, 0.6224    , 0.61942675, 0.60522273, 0.58267717,\n",
       "        0.6208    , 0.6503268 , 0.15508885, 0.65097403, 0.65849673,\n",
       "        0.502     , 0.65737705, 0.65640194, 0.5389755 , 0.65691057,\n",
       "        0.6514658 , 0.506     , 0.65798046, 0.64943457, 0.        ,\n",
       "        0.65372168, 0.65089723, 0.50854271, 0.65153971, 0.64991896,\n",
       "        0.33738938, 0.66284779, 0.65746753, 0.506     , 0.65960912,\n",
       "        0.65528455, 0.2189781 , 0.65359477, 0.65905383, 0.506     ,\n",
       "        0.65695793, 0.65845649, 0.        , 0.6514658 , 0.65579119,\n",
       "        0.505     , 0.65316045, 0.65640194, 0.24038462, 0.65960912,\n",
       "        0.65353038, 0.        , 0.65793781, 0.66448445, 0.        ,\n",
       "        0.65960912, 0.65960912, 0.42249657, 0.65737705, 0.64285714,\n",
       "        0.39942113, 0.67704918, 0.65909091, 0.4740533 , 0.65737705,\n",
       "        0.65746753, 0.44588745, 0.6514658 , 0.64878049, 0.50961538,\n",
       "        0.65422078, 0.6474359 , 0.40868597, 0.65153971, 0.65064103,\n",
       "        0.44065041, 0.65252855, 0.64552846, 0.33577982, 0.65802269,\n",
       "        0.66071429, 0.51589595, 0.65853659, 0.64484452, 0.292     ,\n",
       "        0.65806452, 0.65089723, 0.50393701, 0.66721311, 0.63578275,\n",
       "        0.37128713, 0.64677419, 0.66393443, 0.5008881 , 0.66447368,\n",
       "        0.64215686, 0.64012251, 0.65359477, 0.66556837, 0.31847969,\n",
       "        0.66229508, 0.65056361, 0.5611413 , 0.65409836, 0.65960912,\n",
       "        0.40562914, 0.6461039 , 0.6589404 , 0.66338259, 0.66392092,\n",
       "        0.66392092, 0.66447368, 0.66502463, 0.66280992, 0.66611842,\n",
       "        0.6650165 , 0.6650165 , 0.66392092, 0.66556291, 0.66286645,\n",
       "        0.66280992, 0.66556291, 0.66225166, 0.66176471, 0.66176471,\n",
       "        0.65901639, 0.66336634, 0.66446281, 0.66446281, 0.66176471,\n",
       "        0.66556837, 0.66392092, 0.66013072, 0.66176471, 0.66338259,\n",
       "        0.66282895, 0.66121113, 0.66227348, 0.66392092, 0.66666667,\n",
       "        0.66338259, 0.66446281, 0.66013072, 0.66      , 0.65946844,\n",
       "        0.66071429, 0.66392092, 0.66338259, 0.66338259, 0.66282895,\n",
       "        0.6661157 , 0.66118421, 0.66336634, 0.66284779, 0.66174056,\n",
       "        0.66227348, 0.66118421, 0.65625   , 0.66009852, 0.6650165 ,\n",
       "        0.6601626 , 0.66286645, 0.65359477, 0.64935065, 0.64573269,\n",
       "        0.64983713, 0.65460526, 0.66502463, 0.66503268, 0.65960912,\n",
       "        0.66282895, 0.66775777, 0.65957447, 0.6525974 , 0.6514658 ,\n",
       "        0.66013072, 0.65309446, 0.64991896, 0.64320786, 0.66229508,\n",
       "        0.66284779, 0.65686275, 0.66009852, 0.65584416, 0.66612111,\n",
       "        0.65203252, 0.65630115, 0.66557377, 0.65074135, 0.65056361,\n",
       "        0.64869281, 0.66068515, 0.66231648, 0.66611842, 0.65522876,\n",
       "        0.66776316, 0.65309446, 0.6514658 , 0.65841584, 0.66176471,\n",
       "        0.66447368, 0.65630115, 0.6514658 ]),\n",
       " 'split3_test_jaccard': array([0.58255452, 0.61732283, 0.6015873 , 0.5977918 , 0.60436137,\n",
       "        0.60932476, 0.59055118, 0.60725552, 0.575     , 0.58566978,\n",
       "        0.59936407, 0.57632399, 0.61041009, 0.59206349, 0.61189358,\n",
       "        0.57527734, 0.61295419, 0.59011164, 0.57436709, 0.59206349,\n",
       "        0.59808612, 0.5765625 , 0.58631415, 0.59247649, 0.6028481 ,\n",
       "        0.59936909, 0.6       , 0.6022187 , 0.6       , 0.6140625 ,\n",
       "        0.60189573, 0.59752322, 0.57698289, 0.58571429, 0.57961783,\n",
       "        0.58475894, 0.61526232, 0.60917722, 0.60821485, 0.61356467,\n",
       "        0.62420382, 0.61746032, 0.61403509, 0.59557662, 0.62025316,\n",
       "        0.592     , 0.6066879 , 0.60093897, 0.61611374, 0.60188088,\n",
       "        0.61392405, 0.60031348, 0.62579618, 0.61163522, 0.59968603,\n",
       "        0.58044164, 0.62035541, 0.59813084, 0.60250391, 0.6       ,\n",
       "        0.61904762, 0.60350318, 0.62460064, 0.6031746 , 0.59119497,\n",
       "        0.60383387, 0.59874608, 0.603125  , 0.61049285, 0.5609375 ,\n",
       "        0.58787879, 0.56568779, 0.62660256, 0.62420382, 0.61032864,\n",
       "        0.59968102, 0.60125589, 0.6224    , 0.584375  , 0.59873618,\n",
       "        0.61612903, 0.5977918 , 0.59006211, 0.59141494, 0.63517915,\n",
       "        0.6211878 , 0.6122449 , 0.60538827, 0.60291734, 0.59190031,\n",
       "        0.61352657, 0.58869702, 0.62939297, 0.57836991, 0.58897638,\n",
       "        0.60629921, 0.60952381, 0.        , 0.616     , 0.6272    ,\n",
       "        0.51851852, 0.62379421, 0.62123613, 0.51111111, 0.61305732,\n",
       "        0.61685215, 0.0019685 , 0.6272    , 0.61736334, 0.44023669,\n",
       "        0.62179487, 0.62300319, 0.22823985, 0.62321145, 0.62041467,\n",
       "        0.505     , 0.6208    , 0.6200318 , 0.52233677, 0.62380952,\n",
       "        0.61049285, 0.        , 0.62300319, 0.616     , 0.56840796,\n",
       "        0.61624204, 0.61501597, 0.506     , 0.62619808, 0.62019231,\n",
       "        0.38171263, 0.61587302, 0.61722488, 0.5076297 , 0.61501597,\n",
       "        0.61685215, 0.01185771, 0.61550633, 0.6208    , 0.506     ,\n",
       "        0.62700965, 0.6192    , 0.48244734, 0.61942675, 0.61661342,\n",
       "        0.45065789, 0.61392405, 0.62599049, 0.5159129 , 0.61942675,\n",
       "        0.62200957, 0.56898817, 0.61987382, 0.61695447, 0.45550528,\n",
       "        0.60759494, 0.60063391, 0.51405152, 0.61006289, 0.60386473,\n",
       "        0.45667125, 0.61987382, 0.61172742, 0.5058309 , 0.61259843,\n",
       "        0.61550633, 0.45525903, 0.6122449 , 0.60094637, 0.16156671,\n",
       "        0.61172742, 0.61111111, 0.2995338 , 0.6056338 , 0.616     ,\n",
       "        0.51368159, 0.6015748 , 0.60952381, 0.53891336, 0.60625   ,\n",
       "        0.61526232, 0.25      , 0.60416667, 0.61708861, 0.18232044,\n",
       "        0.60443038, 0.61806656, 0.50622877, 0.61881978, 0.62162162,\n",
       "        0.25882353, 0.61198738, 0.60753532, 0.61067504, 0.60759494,\n",
       "        0.61006289, 0.60883281, 0.60815047, 0.60596546, 0.60596546,\n",
       "        0.60753532, 0.61041009, 0.60725552, 0.60787402, 0.60663507,\n",
       "        0.603125  , 0.60658307, 0.60944882, 0.61102362, 0.60601266,\n",
       "        0.60534591, 0.61006289, 0.60629921, 0.60944882, 0.60663507,\n",
       "        0.6073132 , 0.61244019, 0.60567823, 0.6096    , 0.60697306,\n",
       "        0.60725552, 0.60979463, 0.61006289, 0.60572337, 0.60596546,\n",
       "        0.60883281, 0.60787402, 0.60725552, 0.60697306, 0.61198738,\n",
       "        0.61198738, 0.61137441, 0.61006289, 0.61102362, 0.61067504,\n",
       "        0.60596546, 0.60849057, 0.60725552, 0.60501567, 0.61014263,\n",
       "        0.61587302, 0.61014263, 0.61295419, 0.61698718, 0.60596546,\n",
       "        0.6073132 , 0.60543131, 0.62200957, 0.6108453 , 0.60890302,\n",
       "        0.60828025, 0.61254019, 0.61295419, 0.60849057, 0.60759494,\n",
       "        0.60572337, 0.59904913, 0.61182109, 0.61269841, 0.61708861,\n",
       "        0.60189573, 0.6112    , 0.60897436, 0.60932476, 0.61562998,\n",
       "        0.61014263, 0.60501567, 0.60567823, 0.6064    , 0.61217949,\n",
       "        0.60897436, 0.61797753, 0.61014263, 0.61624204, 0.616     ,\n",
       "        0.61550633, 0.61295419, 0.60658307, 0.60849057, 0.61526232,\n",
       "        0.61428571, 0.61014263, 0.61464968, 0.61111111, 0.61835749,\n",
       "        0.61146497, 0.61378205, 0.60890302]),\n",
       " 'split4_test_jaccard': array([0.58776167, 0.61967213, 0.61374795, 0.59216966, 0.61500816,\n",
       "        0.63333333, 0.60064412, 0.58280255, 0.58528951, 0.57480315,\n",
       "        0.58032787, 0.57097289, 0.62624585, 0.61716172, 0.62932455,\n",
       "        0.5927673 , 0.59493671, 0.60876623, 0.58466454, 0.56782334,\n",
       "        0.56487342, 0.56549521, 0.56050955, 0.56915739, 0.63011457,\n",
       "        0.61881188, 0.62962963, 0.59319287, 0.62086093, 0.60128617,\n",
       "        0.57961783, 0.55915721, 0.57392687, 0.5686901 , 0.56514914,\n",
       "        0.5640625 , 0.64321608, 0.63018242, 0.62809917, 0.59455128,\n",
       "        0.59229535, 0.62884927, 0.6091954 , 0.58105939, 0.60777958,\n",
       "        0.60517799, 0.57389937, 0.57894737, 0.61463415, 0.59277504,\n",
       "        0.63071895, 0.56369427, 0.60450161, 0.60983607, 0.56360709,\n",
       "        0.57728707, 0.59463722, 0.56964006, 0.5399361 , 0.59399684,\n",
       "        0.60790774, 0.61691542, 0.63801653, 0.61575563, 0.58267717,\n",
       "        0.57371795, 0.59399684, 0.5984127 , 0.56025039, 0.59649123,\n",
       "        0.58132045, 0.53846154, 0.62108731, 0.6127451 , 0.61943987,\n",
       "        0.58842444, 0.59738134, 0.61138211, 0.58908507, 0.57570978,\n",
       "        0.57350565, 0.57119741, 0.54783951, 0.57507987, 0.63382594,\n",
       "        0.61389338, 0.62786885, 0.58908507, 0.6016129 , 0.59967051,\n",
       "        0.56240126, 0.58267717, 0.58986928, 0.57827476, 0.57233704,\n",
       "        0.57419355, 0.64297521, 0.506     , 0.63893511, 0.63140496,\n",
       "        0.12535613, 0.63097199, 0.65008292, 0.        , 0.64119601,\n",
       "        0.63636364, 0.        , 0.63801653, 0.6461794 , 0.51173021,\n",
       "        0.64521452, 0.63426689, 0.        , 0.62582781, 0.63591433,\n",
       "        0.20597484, 0.63036304, 0.64344942, 0.        , 0.63741722,\n",
       "        0.63531353, 0.506     , 0.65074135, 0.63486842, 0.00393701,\n",
       "        0.64013267, 0.64415157, 0.53836784, 0.63546798, 0.64119601,\n",
       "        0.53624733, 0.63140496, 0.64321608, 0.27746741, 0.63157895,\n",
       "        0.6307947 , 0.        , 0.63036304, 0.64666667, 0.        ,\n",
       "        0.63289037, 0.63681592, 0.47433036, 0.63727121, 0.63333333,\n",
       "        0.47786606, 0.63132137, 0.63636364, 0.15548282, 0.64320786,\n",
       "        0.63382594, 0.17307692, 0.65116279, 0.63920923, 0.30541872,\n",
       "        0.63531353, 0.64238411, 0.44019139, 0.62626263, 0.6422629 ,\n",
       "        0.48484848, 0.6427379 , 0.63875205, 0.47255689, 0.63907285,\n",
       "        0.64166667, 0.47401575, 0.64013267, 0.63857374, 0.46642468,\n",
       "        0.6195122 , 0.64451827, 0.28442029, 0.64250412, 0.63711002,\n",
       "        0.47984887, 0.63980263, 0.64166667, 0.07692308, 0.64367816,\n",
       "        0.65245902, 0.48580786, 0.61960133, 0.63018242, 0.22521008,\n",
       "        0.64607679, 0.62805873, 0.37191011, 0.64415157, 0.64144737,\n",
       "        0.37059724, 0.62786885, 0.63907285, 0.640599  , 0.65630115,\n",
       "        0.64403974, 0.64403974, 0.64569536, 0.64793388, 0.64473684,\n",
       "        0.63861386, 0.64628099, 0.6446281 , 0.64628099, 0.64179104,\n",
       "        0.64179104, 0.63953488, 0.65016502, 0.64179104, 0.64297521,\n",
       "        0.6446281 , 0.64473684, 0.64132231, 0.64415157, 0.64132231,\n",
       "        0.64403974, 0.6446281 , 0.63907285, 0.64686469, 0.64072848,\n",
       "        0.63787375, 0.64344942, 0.64851485, 0.64851485, 0.64297521,\n",
       "        0.64132231, 0.6446281 , 0.64132231, 0.6446281 , 0.64119601,\n",
       "        0.64628099, 0.64297521, 0.64297521, 0.64403974, 0.6446281 ,\n",
       "        0.64403974, 0.64297521, 0.64297521, 0.64521452, 0.64521452,\n",
       "        0.64119601, 0.6272578 , 0.63756178, 0.63410596, 0.6369637 ,\n",
       "        0.64676617, 0.62438221, 0.6320132 , 0.62786885, 0.62006579,\n",
       "        0.62624585, 0.63801653, 0.64628099, 0.64833333, 0.63486842,\n",
       "        0.63410596, 0.64132231, 0.63920923, 0.63278689, 0.61842105,\n",
       "        0.63122924, 0.6097561 , 0.62932455, 0.6295082 , 0.64744646,\n",
       "        0.64392679, 0.64356436, 0.63515755, 0.64891847, 0.64026403,\n",
       "        0.62622951, 0.61702128, 0.62748344, 0.625     , 0.6192053 ,\n",
       "        0.62889984, 0.64793388, 0.64119601, 0.65397351, 0.64774624,\n",
       "        0.64179104, 0.63741722, 0.63531353, 0.63261944, 0.62932455,\n",
       "        0.63166667, 0.64344942, 0.63471074]),\n",
       " 'mean_test_jaccard': array([0.61089705, 0.63333717, 0.62179436, 0.61390448, 0.61650214,\n",
       "        0.61904068, 0.59761954, 0.59647691, 0.58722526, 0.57743279,\n",
       "        0.5917729 , 0.58515428, 0.62759985, 0.61809423, 0.62773468,\n",
       "        0.59816033, 0.60485185, 0.61081829, 0.59039474, 0.5970582 ,\n",
       "        0.58715577, 0.58186488, 0.58351918, 0.5851559 , 0.6288611 ,\n",
       "        0.62773629, 0.62703648, 0.6053243 , 0.6174446 , 0.61471227,\n",
       "        0.60454413, 0.5943481 , 0.58655848, 0.58966205, 0.58641447,\n",
       "        0.58520068, 0.63746273, 0.62929282, 0.62691138, 0.61867061,\n",
       "        0.62101673, 0.6236922 , 0.60212656, 0.60321949, 0.61636147,\n",
       "        0.60144457, 0.60031869, 0.60068119, 0.62170772, 0.62256312,\n",
       "        0.63548783, 0.59534093, 0.61572946, 0.61782413, 0.59365145,\n",
       "        0.59098013, 0.60242564, 0.5750599 , 0.57390905, 0.5976386 ,\n",
       "        0.62560965, 0.62483666, 0.62613969, 0.60500238, 0.60477871,\n",
       "        0.60688042, 0.61021585, 0.60252011, 0.58489648, 0.58223025,\n",
       "        0.58629157, 0.57349741, 0.63542829, 0.62252611, 0.6267999 ,\n",
       "        0.60356554, 0.61047703, 0.61716277, 0.60316247, 0.59080088,\n",
       "        0.60009131, 0.58903457, 0.58106756, 0.58616477, 0.63865496,\n",
       "        0.62651849, 0.63108723, 0.61527097, 0.61498122, 0.60415994,\n",
       "        0.60566481, 0.59971151, 0.60609317, 0.58435753, 0.58440679,\n",
       "        0.59635597, 0.63552335, 0.26065618, 0.63708494, 0.63854543,\n",
       "        0.38751517, 0.63718755, 0.64068584, 0.31121732, 0.63477214,\n",
       "        0.6344808 , 0.10198974, 0.63864147, 0.63752479, 0.21646533,\n",
       "        0.6395585 , 0.63760107, 0.31466841, 0.63518788, 0.63856197,\n",
       "        0.42433549, 0.63900463, 0.64181179, 0.20802956, 0.63977814,\n",
       "        0.63583937, 0.14616067, 0.64073053, 0.63925135, 0.31686899,\n",
       "        0.63753222, 0.63711672, 0.21837852, 0.63847944, 0.63980882,\n",
       "        0.48961676, 0.6372815 , 0.63671663, 0.30629635, 0.63812713,\n",
       "        0.63691462, 0.00276524, 0.63691753, 0.64229911, 0.16442065,\n",
       "        0.63961779, 0.6396478 , 0.35060895, 0.63946949, 0.63532563,\n",
       "        0.45586194, 0.63644715, 0.64145966, 0.38708819, 0.64054782,\n",
       "        0.63361368, 0.33663584, 0.64192584, 0.63490645, 0.43464897,\n",
       "        0.63543329, 0.63387322, 0.42809554, 0.63277861, 0.63276465,\n",
       "        0.43473742, 0.63697508, 0.63238751, 0.44787708, 0.63653389,\n",
       "        0.63931557, 0.50382762, 0.63916698, 0.63151833, 0.27564385,\n",
       "        0.63025893, 0.63533686, 0.35504922, 0.63839348, 0.63476454,\n",
       "        0.43795445, 0.63299198, 0.63851878, 0.43188537, 0.635114  ,\n",
       "        0.63693236, 0.44679321, 0.62957883, 0.63788012, 0.26438268,\n",
       "        0.64009493, 0.63633616, 0.41819312, 0.63883799, 0.64001531,\n",
       "        0.37306449, 0.63436183, 0.63780535, 0.64112066, 0.64386624,\n",
       "        0.64094071, 0.64067504, 0.64226448, 0.64110836, 0.64167624,\n",
       "        0.63943759, 0.6421836 , 0.63962456, 0.64150774, 0.63858466,\n",
       "        0.63851629, 0.64004765, 0.64120422, 0.6400054 , 0.6401954 ,\n",
       "        0.63902821, 0.64130924, 0.64016042, 0.6406258 , 0.63979196,\n",
       "        0.64187464, 0.64440443, 0.63870679, 0.6419873 , 0.63929739,\n",
       "        0.63790025, 0.64094661, 0.64117083, 0.64063971, 0.64067344,\n",
       "        0.64058702, 0.64036773, 0.63874229, 0.64052226, 0.63908389,\n",
       "        0.6392123 , 0.64076471, 0.64028472, 0.64218608, 0.64138949,\n",
       "        0.64141899, 0.64069832, 0.64070735, 0.63971237, 0.64065435,\n",
       "        0.63997035, 0.63928271, 0.63820478, 0.63627962, 0.63780703,\n",
       "        0.63777307, 0.63358712, 0.6369602 , 0.63000474, 0.62628561,\n",
       "        0.63234923, 0.6363722 , 0.64116206, 0.63874624, 0.63685874,\n",
       "        0.6396534 , 0.63890246, 0.6385614 , 0.63209874, 0.63310004,\n",
       "        0.63331991, 0.62546443, 0.63015437, 0.62462994, 0.64146507,\n",
       "        0.63941894, 0.63826918, 0.63446396, 0.6369271 , 0.6409085 ,\n",
       "        0.63333369, 0.6341566 , 0.63573869, 0.63194562, 0.63046557,\n",
       "        0.63075462, 0.63965608, 0.63849565, 0.63868529, 0.64071548,\n",
       "        0.63976001, 0.63672364, 0.63464729, 0.64081829, 0.63765353,\n",
       "        0.63667428, 0.63826567, 0.63299209]),\n",
       " 'std_test_jaccard': array([0.02460312, 0.01621742, 0.01598626, 0.01609658, 0.00913586,\n",
       "        0.01880409, 0.01354742, 0.01336589, 0.02228159, 0.0158684 ,\n",
       "        0.01707731, 0.01085495, 0.01122636, 0.01567069, 0.01557561,\n",
       "        0.01597384, 0.00637045, 0.01434159, 0.00952838, 0.01687902,\n",
       "        0.01675679, 0.01475142, 0.01468329, 0.0201575 , 0.01708318,\n",
       "        0.01890383, 0.01388389, 0.00886098, 0.01012953, 0.0221881 ,\n",
       "        0.02310524, 0.01872194, 0.01103783, 0.01757439, 0.01904416,\n",
       "        0.01540833, 0.0202765 , 0.01476313, 0.01369294, 0.01463111,\n",
       "        0.02141049, 0.01340284, 0.01159856, 0.01495208, 0.0091299 ,\n",
       "        0.01500008, 0.01711203, 0.01223425, 0.00577059, 0.02432561,\n",
       "        0.0171121 , 0.01778686, 0.00787692, 0.00960694, 0.0185771 ,\n",
       "        0.02368425, 0.01815976, 0.0185532 , 0.02121632, 0.02038913,\n",
       "        0.01299893, 0.01332572, 0.0071118 , 0.00642545, 0.01692235,\n",
       "        0.0183317 , 0.01576849, 0.0101705 , 0.01787564, 0.01183666,\n",
       "        0.01652672, 0.02369997, 0.01298254, 0.01060445, 0.0126834 ,\n",
       "        0.01307035, 0.01430175, 0.00368119, 0.01673149, 0.01311735,\n",
       "        0.01627937, 0.01692343, 0.02128554, 0.00996928, 0.00923149,\n",
       "        0.00895289, 0.01310443, 0.01804807, 0.01355652, 0.02199348,\n",
       "        0.02431757, 0.01454866, 0.01595461, 0.01164149, 0.01427473,\n",
       "        0.01787745, 0.01468519, 0.24850365, 0.01155751, 0.01100652,\n",
       "        0.15681974, 0.01138018, 0.01359845, 0.25435568, 0.01478135,\n",
       "        0.01159938, 0.20200706, 0.01071592, 0.01117012, 0.21812408,\n",
       "        0.01249587, 0.00918438, 0.19041613, 0.01011467, 0.01058489,\n",
       "        0.13279984, 0.01434948, 0.01254211, 0.25005188, 0.01218345,\n",
       "        0.01557204, 0.19859841, 0.01160341, 0.01437686, 0.25812606,\n",
       "        0.01398273, 0.01497895, 0.24887292, 0.00927863, 0.01203743,\n",
       "        0.05517551, 0.01408704, 0.01292931, 0.18942523, 0.01549769,\n",
       "        0.01313107, 0.00460972, 0.01488734, 0.0146043 , 0.18686771,\n",
       "        0.01191954, 0.01294771, 0.17962142, 0.0138042 , 0.01150513,\n",
       "        0.02995557, 0.02132414, 0.01119394, 0.13735888, 0.01221735,\n",
       "        0.01405503, 0.21053821, 0.01232456, 0.01204282, 0.09348942,\n",
       "        0.01692386, 0.01687618, 0.07348476, 0.0141392 , 0.01579466,\n",
       "        0.11623431, 0.01325438, 0.01168393, 0.0583195 , 0.01486878,\n",
       "        0.01599075, 0.03348416, 0.01525214, 0.01706963, 0.17832268,\n",
       "        0.01580614, 0.01552473, 0.09750455, 0.02001125, 0.00985256,\n",
       "        0.05962528, 0.01623301, 0.01908317, 0.18017473, 0.01978156,\n",
       "        0.01236766, 0.12514726, 0.01703118, 0.01621111, 0.0965406 ,\n",
       "        0.01912377, 0.01185645, 0.14700021, 0.01288464, 0.01305349,\n",
       "        0.09587286, 0.01360246, 0.01685038, 0.01714663, 0.01978154,\n",
       "        0.01798261, 0.01820179, 0.01908466, 0.01944463, 0.02012296,\n",
       "        0.01910534, 0.01797713, 0.01919257, 0.0197829 , 0.01874834,\n",
       "        0.02044941, 0.01927028, 0.01779238, 0.01668258, 0.01888692,\n",
       "        0.01831339, 0.01806966, 0.01928363, 0.01854457, 0.0185035 ,\n",
       "        0.01946595, 0.01765865, 0.01914041, 0.01780296, 0.01873162,\n",
       "        0.01845276, 0.01719306, 0.01796703, 0.02051381, 0.02057915,\n",
       "        0.01836899, 0.01853192, 0.0180409 , 0.01820253, 0.01548643,\n",
       "        0.01611206, 0.01740696, 0.01793045, 0.01779914, 0.01769677,\n",
       "        0.01983462, 0.01803665, 0.01876306, 0.01935388, 0.01750791,\n",
       "        0.01473381, 0.01822623, 0.01485679, 0.01379748, 0.01882386,\n",
       "        0.01864874, 0.01892963, 0.01089609, 0.01223067, 0.01528345,\n",
       "        0.01421223, 0.01462423, 0.01729742, 0.01861792, 0.01709938,\n",
       "        0.01929955, 0.02225525, 0.0157256 , 0.01283768, 0.01336524,\n",
       "        0.01868426, 0.01606963, 0.01365084, 0.0117245 , 0.01556646,\n",
       "        0.01689957, 0.01764734, 0.01728602, 0.01693613, 0.01722209,\n",
       "        0.01487324, 0.01543801, 0.0180097 , 0.01145357, 0.01246349,\n",
       "        0.01088195, 0.01568622, 0.01793167, 0.02055189, 0.01356792,\n",
       "        0.01709387, 0.0147582 , 0.01177893, 0.01712119, 0.01454615,\n",
       "        0.01715295, 0.01457427, 0.01592811]),\n",
       " 'rank_test_jaccard': array([198, 144, 182, 197, 191, 185, 224, 226, 237, 253, 231, 245, 168,\n",
       "        187, 167, 222, 207, 199, 234, 225, 238, 251, 249, 244, 165, 166,\n",
       "        169, 205, 189, 196, 209, 229, 239, 235, 240, 243, 103, 164, 170,\n",
       "        186, 184, 179, 216, 212, 192, 217, 219, 218, 183, 180, 126, 228,\n",
       "        193, 188, 230, 232, 215, 254, 255, 223, 175, 177, 174, 206, 208,\n",
       "        202, 201, 214, 246, 250, 241, 256, 128, 181, 171, 211, 200, 190,\n",
       "        213, 233, 220, 236, 252, 242,  79, 172, 157, 194, 195, 210, 204,\n",
       "        221, 203, 248, 247, 227, 125, 281, 107,  84, 269, 105,  32, 277,\n",
       "        134, 137, 287,  80, 102, 283,  60, 100, 276, 131,  82, 267,  72,\n",
       "         10, 284,  52, 123, 286,  28,  67, 275, 101, 106, 282,  88,  50,\n",
       "        258, 104, 116, 278,  93, 113, 288, 112,   3, 285,  59,  57, 273,\n",
       "         61, 130, 259, 119,  14, 270,  39, 142, 274,   8, 133, 264, 127,\n",
       "        141, 266, 150, 151, 263, 108, 152, 260, 118,  64, 257,  69, 156,\n",
       "        279, 160, 129, 272,  89, 135, 262, 149,  85, 265, 132, 110, 261,\n",
       "        163,  95, 280,  45, 121, 268,  74,  47, 271, 139,  97,  21,   2,\n",
       "         24,  33,   4,  22,  11,  62,   6,  58,  12,  81,  86,  46,  18,\n",
       "         48,  43,  71,  17,  44,  37,  51,   9,   1,  77,   7,  65,  94,\n",
       "         23,  19,  36,  34,  38,  41,  76,  40,  70,  68,  27,  42,   5,\n",
       "         16,  15,  31,  30,  54,  35,  49,  66,  92, 122,  96,  98, 143,\n",
       "        109, 162, 173, 153, 120,  20,  75, 114,  56,  73,  83, 154, 147,\n",
       "        146, 176, 161, 178,  13,  63,  90, 138, 111,  25, 145, 140, 124,\n",
       "        155, 159, 158,  55,  87,  78,  29,  53, 115, 136,  26,  99, 117,\n",
       "         91, 148]),\n",
       " 'split0_test_neg_log_loss': array([-0.48452451, -0.47287293, -0.48403131, -0.51529947, -0.52855604,\n",
       "        -0.51361344, -0.56321759, -0.5482716 , -0.63447286, -0.71876317,\n",
       "        -0.57837009, -0.60652403, -0.47158898, -0.46641579, -0.48719916,\n",
       "        -0.5339195 , -0.54432226, -0.52837015, -0.58552265, -0.54246474,\n",
       "        -0.59084675, -0.60410222, -0.60941101, -0.57765783, -0.49398139,\n",
       "        -0.48030303, -0.48903326, -0.50283733, -0.49804964, -0.50969411,\n",
       "        -0.57145828, -0.55696575, -0.57742614, -0.58662795, -0.56248246,\n",
       "        -0.55304337, -0.47574725, -0.48810918, -0.47938389, -0.49439556,\n",
       "        -0.49482092, -0.48440106, -0.53544834, -0.53144692, -0.53346083,\n",
       "        -0.5460981 , -0.54644424, -0.54667348, -0.47353991, -0.47989645,\n",
       "        -0.47375246, -0.53337871, -0.52801491, -0.51321947, -0.5963313 ,\n",
       "        -0.65553672, -0.58014681, -0.70064769, -0.74475706, -0.72077163,\n",
       "        -0.47154746, -0.46941955, -0.47579153, -0.53155975, -0.52782531,\n",
       "        -0.53425632, -0.56174734, -0.5633624 , -0.64440053, -0.67444879,\n",
       "        -0.60800496, -0.70411776, -0.48008189, -0.48025284, -0.472345  ,\n",
       "        -0.53822333, -0.52141062, -0.50281327, -0.56813217, -0.58849819,\n",
       "        -0.58059602, -0.69409423, -0.60098474, -0.62877193, -0.46961992,\n",
       "        -0.46974862, -0.46895627, -0.48966399, -0.52873271, -0.53051192,\n",
       "        -0.54205591, -0.54032311, -0.56051985, -0.62633018, -0.58276198,\n",
       "        -0.58947315, -0.46820105, -0.66002495, -0.46946627, -0.46664691,\n",
       "        -0.74173904, -0.46561   , -0.46576889, -0.73900564, -0.4621629 ,\n",
       "        -0.46290501, -0.68929092, -0.46365655, -0.46723835, -0.7492584 ,\n",
       "        -0.47227686, -0.4673741 , -0.66285627, -0.46722429, -0.46316405,\n",
       "        -0.6783462 , -0.46610626, -0.46297716, -0.69054106, -0.46237898,\n",
       "        -0.47040561, -0.74686805, -0.47084307, -0.46796768, -0.74737828,\n",
       "        -0.46467555, -0.46560942, -0.6679087 , -0.4623311 , -0.46278329,\n",
       "        -0.69687141, -0.46321305, -0.47043271, -0.75587996, -0.47335671,\n",
       "        -0.47040396, -0.71242167, -0.46520968, -0.46683927, -0.71248344,\n",
       "        -0.46278713, -0.46236583, -0.71404022, -0.4632855 , -0.47403673,\n",
       "        -0.69224545, -0.47029977, -0.46710129, -0.61307741, -0.46883061,\n",
       "        -0.46596643, -0.74848155, -0.46362954, -0.46634286, -0.67175952,\n",
       "        -0.47101713, -0.46871168, -0.69529715, -0.48073977, -0.47572414,\n",
       "        -0.75763006, -0.46498226, -0.46859612, -0.68985222, -0.46946805,\n",
       "        -0.4647191 , -0.59840393, -0.46214833, -0.48055861, -0.80638818,\n",
       "        -0.47370832, -0.47789742, -0.67673718, -0.46798291, -0.46939099,\n",
       "        -0.64891086, -0.4720254 , -0.470005  , -0.63403705, -0.46893603,\n",
       "        -0.47747933, -0.76118555, -0.47245813, -0.46511184, -0.76471068,\n",
       "        -0.46978829, -0.46341783, -0.59739826, -0.46413804, -0.46585637,\n",
       "        -0.87333374, -0.46651521, -0.46324114, -0.46465098, -0.46399132,\n",
       "        -0.46162465, -0.46135367, -0.46054938, -0.46086048, -0.46052979,\n",
       "        -0.46071022, -0.45977253, -0.45977199, -0.46067242, -0.46327201,\n",
       "        -0.46473141, -0.46321079, -0.46135539, -0.46088655, -0.46075968,\n",
       "        -0.46121076, -0.46210638, -0.46026295, -0.45982993, -0.45890089,\n",
       "        -0.4594777 , -0.46287665, -0.46334142, -0.46422148, -0.46173897,\n",
       "        -0.46214707, -0.4617287 , -0.46095661, -0.4604937 , -0.46131596,\n",
       "        -0.46133208, -0.46053272, -0.46037557, -0.46337505, -0.46335288,\n",
       "        -0.46408997, -0.46117338, -0.46084681, -0.46129598, -0.46009596,\n",
       "        -0.45983289, -0.45988388, -0.45964518, -0.45975831, -0.4594051 ,\n",
       "        -0.46821726, -0.46075583, -0.45998882, -0.46062307, -0.47046486,\n",
       "        -0.46196912, -0.46920033, -0.46704176, -0.47192495, -0.474378  ,\n",
       "        -0.47358582, -0.46915408, -0.46382101, -0.46597131, -0.46424026,\n",
       "        -0.47082546, -0.46429593, -0.46369216, -0.46958208, -0.47256095,\n",
       "        -0.47167433, -0.47152855, -0.46850376, -0.47980141, -0.46656262,\n",
       "        -0.46296176, -0.46527039, -0.47031887, -0.47026445, -0.47013358,\n",
       "        -0.47301898, -0.4744366 , -0.47560101, -0.46984781, -0.47136412,\n",
       "        -0.46806324, -0.46430922, -0.46359502, -0.46182475, -0.46269085,\n",
       "        -0.4632352 , -0.46019926, -0.47036497, -0.46179408, -0.46147344,\n",
       "        -0.46778658, -0.46878516, -0.46633724]),\n",
       " 'split1_test_neg_log_loss': array([-0.49411012, -0.48497741, -0.48404366, -0.50595817, -0.53440536,\n",
       "        -0.57190077, -0.61201404, -0.59660737, -0.57765735, -0.69441616,\n",
       "        -0.67704185, -0.68490558, -0.48627251, -0.48728928, -0.47581815,\n",
       "        -0.53461531, -0.53072244, -0.52500964, -0.58173282, -0.60025806,\n",
       "        -0.60983553, -0.6522666 , -0.6654996 , -0.60015874, -0.48934189,\n",
       "        -0.49763843, -0.48598475, -0.53016786, -0.51942339, -0.53441806,\n",
       "        -0.57682666, -0.57872247, -0.57221101, -0.61772001, -0.61269523,\n",
       "        -0.64908989, -0.49689953, -0.48401331, -0.47824054, -0.51190524,\n",
       "        -0.50321708, -0.51520739, -0.55818585, -0.54975066, -0.53557376,\n",
       "        -0.60995642, -0.58694219, -0.56505223, -0.48353271, -0.48178462,\n",
       "        -0.50011544, -0.55202665, -0.56104789, -0.52219553, -0.58092687,\n",
       "        -0.61456097, -0.66730986, -0.68331796, -0.68260579, -0.64555873,\n",
       "        -0.49162082, -0.48917341, -0.48268467, -0.54075533, -0.63157701,\n",
       "        -0.51389205, -0.57026094, -0.61475461, -0.64926852, -0.61741454,\n",
       "        -0.72606135, -0.66616285, -0.48224086, -0.48570486, -0.49012808,\n",
       "        -0.5081271 , -0.52445363, -0.51312062, -0.61114387, -0.55474354,\n",
       "        -0.57994087, -0.64952182, -0.69493314, -0.731821  , -0.49003957,\n",
       "        -0.47849578, -0.49206537, -0.52238451, -0.51415655, -0.53858375,\n",
       "        -0.55618668, -0.60537613, -0.57027651, -0.61783591, -0.59476504,\n",
       "        -0.64651153, -0.47301762, -0.69549802, -0.47608047, -0.47099997,\n",
       "        -0.6846492 , -0.47112099, -0.4660524 , -0.70996495, -0.46994191,\n",
       "        -0.46772199, -0.69453305, -0.46921273, -0.47698884, -0.6907599 ,\n",
       "        -0.47078724, -0.46800427, -0.67643839, -0.47298707, -0.46762775,\n",
       "        -0.67204789, -0.46663906, -0.46660811, -0.71006577, -0.4671142 ,\n",
       "        -0.47184523, -0.71305223, -0.4744275 , -0.46937867, -0.70066672,\n",
       "        -0.46682693, -0.46726397, -0.71179087, -0.46524027, -0.46711498,\n",
       "        -0.67344596, -0.46644228, -0.47100625, -0.83646504, -0.47482569,\n",
       "        -0.46746249, -0.72176645, -0.46892505, -0.46681356, -0.68731681,\n",
       "        -0.46628367, -0.4675427 , -0.67979445, -0.46516965, -0.46989979,\n",
       "        -0.68483294, -0.47991463, -0.46827049, -0.7356397 , -0.47216354,\n",
       "        -0.47346765, -0.62997388, -0.46689326, -0.47134021, -0.60962811,\n",
       "        -0.46126341, -0.48193005, -0.67445095, -0.46409127, -0.46130469,\n",
       "        -0.63471108, -0.46816616, -0.47235755, -0.73526625, -0.46735561,\n",
       "        -0.46719167, -0.55591397, -0.46299722, -0.47115257, -0.70157356,\n",
       "        -0.47949402, -0.46542325, -0.83766633, -0.46871796, -0.47261823,\n",
       "        -0.62923843, -0.47073972, -0.46916343, -0.62669891, -0.47005418,\n",
       "        -0.47758588, -0.62545571, -0.47272266, -0.47609263, -0.80303237,\n",
       "        -0.46587421, -0.46833409, -0.71263664, -0.47025821, -0.46673169,\n",
       "        -0.65672264, -0.47191571, -0.46957964, -0.46642548, -0.46679429,\n",
       "        -0.46503462, -0.46519993, -0.46462319, -0.46423818, -0.46425629,\n",
       "        -0.46431288, -0.4639852 , -0.46319169, -0.46283942, -0.46722291,\n",
       "        -0.46726879, -0.46577239, -0.46548326, -0.46475331, -0.46421047,\n",
       "        -0.46416364, -0.46409939, -0.46415656, -0.46395997, -0.46383654,\n",
       "        -0.46323495, -0.46734649, -0.46620602, -0.46483445, -0.46469719,\n",
       "        -0.4651278 , -0.46445317, -0.46448751, -0.46334431, -0.46372402,\n",
       "        -0.4642924 , -0.46390362, -0.46372046, -0.46539523, -0.46623292,\n",
       "        -0.4665794 , -0.46410618, -0.46384296, -0.46395346, -0.46304002,\n",
       "        -0.46315944, -0.46281055, -0.46290303, -0.46328146, -0.46305144,\n",
       "        -0.4727042 , -0.46689427, -0.47390597, -0.47035376, -0.47106509,\n",
       "        -0.47902776, -0.47545949, -0.47827874, -0.47443164, -0.47974107,\n",
       "        -0.47852292, -0.47612236, -0.47624972, -0.46797072, -0.47284317,\n",
       "        -0.46945503, -0.46948566, -0.47252589, -0.47339891, -0.47725945,\n",
       "        -0.46815161, -0.4820374 , -0.47861306, -0.48033993, -0.46973677,\n",
       "        -0.46731004, -0.47084788, -0.47961076, -0.47353782, -0.46848729,\n",
       "        -0.47354003, -0.47731731, -0.4729715 , -0.47998555, -0.47992536,\n",
       "        -0.47597494, -0.47053791, -0.4662568 , -0.46833528, -0.46957103,\n",
       "        -0.47550423, -0.47356046, -0.47246797, -0.46997757, -0.47433996,\n",
       "        -0.47173101, -0.47279494, -0.47532509]),\n",
       " 'split2_test_neg_log_loss': array([-0.45923728, -0.45482828, -0.45841616, -0.52056364, -0.52533453,\n",
       "        -0.47773833, -0.52399618, -0.563943  , -0.53035139, -0.56870144,\n",
       "        -0.60362629, -0.58753097, -0.45667352, -0.45713826, -0.4592382 ,\n",
       "        -0.51285651, -0.51444943, -0.4844484 , -0.55705412, -0.54014825,\n",
       "        -0.53587184, -0.55140017, -0.62833226, -0.54238725, -0.45751   ,\n",
       "        -0.45554763, -0.45411862, -0.50748775, -0.48934999, -0.49363706,\n",
       "        -0.49671783, -0.53276588, -0.5364814 , -0.54823933, -0.61009864,\n",
       "        -0.55741232, -0.45289907, -0.45184654, -0.45603984, -0.48077988,\n",
       "        -0.48465928, -0.4752092 , -0.51524152, -0.51659559, -0.50624132,\n",
       "        -0.51498125, -0.5155005 , -0.54294366, -0.45934142, -0.44964937,\n",
       "        -0.46352622, -0.49915838, -0.52574379, -0.48648389, -0.56900367,\n",
       "        -0.54373435, -0.58642489, -0.58773697, -0.5888939 , -0.5414459 ,\n",
       "        -0.46418863, -0.46388092, -0.46734858, -0.530904  , -0.49556625,\n",
       "        -0.50432123, -0.53840426, -0.5653649 , -0.5938986 , -0.62387106,\n",
       "        -0.57687419, -0.5945086 , -0.45766766, -0.46490523, -0.46526643,\n",
       "        -0.48335237, -0.50408056, -0.50485945, -0.51748397, -0.56030222,\n",
       "        -0.56354291, -0.61170107, -0.55477112, -0.62078499, -0.44615547,\n",
       "        -0.46338308, -0.45462728, -0.49740806, -0.48464549, -0.47816043,\n",
       "        -0.52239672, -0.50983828, -0.53915932, -0.56931671, -0.56619134,\n",
       "        -0.59385337, -0.45945506, -0.69899841, -0.45816402, -0.45504599,\n",
       "        -0.71816976, -0.45390648, -0.45170024, -0.66800931, -0.45090522,\n",
       "        -0.4515148 , -0.72543665, -0.45080059, -0.46020846, -0.71195856,\n",
       "        -0.45601319, -0.45222588, -0.68237782, -0.45182039, -0.45184084,\n",
       "        -0.70707507, -0.44644237, -0.449679  , -0.71325492, -0.44931929,\n",
       "        -0.45600818, -0.70567151, -0.45644177, -0.45384456, -0.69954241,\n",
       "        -0.45289504, -0.44799477, -0.69823516, -0.4511249 , -0.45050863,\n",
       "        -0.70536252, -0.45173352, -0.46169308, -0.69203296, -0.46301291,\n",
       "        -0.45595846, -0.69466723, -0.45386617, -0.45024633, -0.69826381,\n",
       "        -0.45058096, -0.44985108, -0.68657355, -0.44887921, -0.45346809,\n",
       "        -0.66800286, -0.44810779, -0.45410297, -0.65079728, -0.45296948,\n",
       "        -0.44740951, -0.66811363, -0.44676499, -0.45176943, -0.58323383,\n",
       "        -0.44470167, -0.46479445, -0.84840637, -0.45278821, -0.45267419,\n",
       "        -0.62915998, -0.45375694, -0.45435812, -0.65052605, -0.45215285,\n",
       "        -0.44864326, -0.6219609 , -0.44903869, -0.46636126, -0.77081582,\n",
       "        -0.45636306, -0.45848196, -0.61232048, -0.44826788, -0.45658459,\n",
       "        -0.67296594, -0.4517343 , -0.44722065, -0.58671091, -0.44959069,\n",
       "        -0.46654   , -0.50113022, -0.45647041, -0.45051949, -0.78289755,\n",
       "        -0.45131911, -0.45107303, -0.6234991 , -0.45076289, -0.44674292,\n",
       "        -0.64068635, -0.4558874 , -0.44705362, -0.448096  , -0.44669655,\n",
       "        -0.44638974, -0.44392532, -0.44538725, -0.44435003, -0.44394338,\n",
       "        -0.4452454 , -0.44407679, -0.44367718, -0.44356514, -0.44944666,\n",
       "        -0.44691779, -0.44511063, -0.44617905, -0.44688229, -0.44542622,\n",
       "        -0.44555948, -0.44454392, -0.44353426, -0.44354714, -0.44394976,\n",
       "        -0.44431716, -0.44504721, -0.44836843, -0.44700002, -0.44566152,\n",
       "        -0.44617397, -0.44558335, -0.4446818 , -0.44474958, -0.44497959,\n",
       "        -0.44333426, -0.44355419, -0.44567529, -0.45106117, -0.44889928,\n",
       "        -0.44790563, -0.44607416, -0.44610607, -0.44537419, -0.44464477,\n",
       "        -0.44420542, -0.44444339, -0.44361824, -0.44350462, -0.44445361,\n",
       "        -0.44712811, -0.4431554 , -0.44525794, -0.44512293, -0.44187436,\n",
       "        -0.44699491, -0.44705782, -0.44754924, -0.44969096, -0.45446392,\n",
       "        -0.4477502 , -0.45145966, -0.44222943, -0.44602188, -0.445487  ,\n",
       "        -0.4430253 , -0.44507178, -0.4492444 , -0.4461668 , -0.44415577,\n",
       "        -0.44497976, -0.45395161, -0.45413727, -0.45505744, -0.44446109,\n",
       "        -0.44328867, -0.44319977, -0.44272022, -0.4490384 , -0.44082501,\n",
       "        -0.44149908, -0.44459503, -0.44739395, -0.44785318, -0.45118592,\n",
       "        -0.44910353, -0.44330062, -0.44392653, -0.44314991, -0.44357821,\n",
       "        -0.44364756, -0.44446226, -0.4408086 , -0.44126018, -0.44422314,\n",
       "        -0.44380127, -0.44127128, -0.44661561]),\n",
       " 'split3_test_neg_log_loss': array([-0.50062302, -0.49644709, -0.47599375, -0.54047907, -0.52233266,\n",
       "        -0.50534347, -0.61004703, -0.56415299, -0.56181483, -0.60567436,\n",
       "        -0.64575993, -0.62491859, -0.48089167, -0.49546894, -0.48431848,\n",
       "        -0.54197231, -0.52862285, -0.56271719, -0.58110175, -0.55978355,\n",
       "        -0.56112556, -0.60395646, -0.70364533, -0.59946321, -0.50192159,\n",
       "        -0.49139882, -0.4832087 , -0.55331262, -0.51179733, -0.50880786,\n",
       "        -0.58323505, -0.54969307, -0.59733426, -0.62637437, -0.60813727,\n",
       "        -0.65849607, -0.48199894, -0.48527449, -0.49247243, -0.51778739,\n",
       "        -0.51540163, -0.51585421, -0.53729886, -0.54941703, -0.52296275,\n",
       "        -0.58217222, -0.56069524, -0.57395437, -0.47587177, -0.49501385,\n",
       "        -0.47878437, -0.5303896 , -0.49843667, -0.50930854, -0.59910644,\n",
       "        -0.63187152, -0.54903234, -0.62409007, -0.59892519, -0.64226351,\n",
       "        -0.50339743, -0.47851804, -0.47482327, -0.51828783, -0.55187265,\n",
       "        -0.50729255, -0.606412  , -0.60301806, -0.54999949, -0.68170533,\n",
       "        -0.67490735, -0.75260572, -0.47799944, -0.48327217, -0.48157962,\n",
       "        -0.53284608, -0.54437451, -0.50104423, -0.58866003, -0.52643674,\n",
       "        -0.53358802, -0.62144641, -0.62981114, -0.63343188, -0.47543827,\n",
       "        -0.46520376, -0.48306047, -0.51824237, -0.51945747, -0.53259157,\n",
       "        -0.55916391, -0.5782898 , -0.52116613, -0.6231909 , -0.63780319,\n",
       "        -0.58186219, -0.47981912, -0.72645255, -0.48014991, -0.47349328,\n",
       "        -0.6689377 , -0.47151431, -0.47543892, -0.67880808, -0.47063914,\n",
       "        -0.47009941, -0.69344806, -0.46965505, -0.47798856, -0.69021688,\n",
       "        -0.47697074, -0.4706362 , -0.65282051, -0.47477998, -0.47128052,\n",
       "        -0.7125921 , -0.47455592, -0.4686237 , -0.67814617, -0.46766135,\n",
       "        -0.47729502, -0.70618943, -0.47812854, -0.4753131 , -0.66071494,\n",
       "        -0.47349834, -0.47077009, -0.70146324, -0.47086308, -0.47042087,\n",
       "        -0.68870594, -0.4694396 , -0.4795387 , -0.69410309, -0.47571943,\n",
       "        -0.47154929, -0.68384578, -0.47461433, -0.47160549, -0.71315666,\n",
       "        -0.46919935, -0.46941743, -0.7077738 , -0.46933145, -0.4854653 ,\n",
       "        -0.6310036 , -0.47736855, -0.47693103, -0.59016398, -0.47440677,\n",
       "        -0.47271156, -0.60292129, -0.47296373, -0.47720894, -0.6341509 ,\n",
       "        -0.4727871 , -0.47831384, -0.68122267, -0.48311807, -0.47455309,\n",
       "        -0.67250159, -0.47048894, -0.47878484, -0.61869646, -0.47412916,\n",
       "        -0.47670252, -0.62326741, -0.47407472, -0.48170112, -1.0017559 ,\n",
       "        -0.47523092, -0.4703542 , -0.89459983, -0.48056485, -0.47711457,\n",
       "        -0.65357646, -0.48005478, -0.47152092, -0.61416009, -0.47493813,\n",
       "        -0.47990798, -0.72642475, -0.48159682, -0.47493808, -0.68990283,\n",
       "        -0.48194542, -0.47206983, -0.69904408, -0.47266406, -0.47621536,\n",
       "        -0.78766258, -0.47553971, -0.47442919, -0.47343474, -0.47078146,\n",
       "        -0.47041827, -0.46933327, -0.47077797, -0.46880565, -0.46826176,\n",
       "        -0.4693552 , -0.46809859, -0.46896005, -0.46846019, -0.47147434,\n",
       "        -0.47418994, -0.47337737, -0.47178347, -0.47106923, -0.46890631,\n",
       "        -0.46802969, -0.46920649, -0.46792381, -0.46830466, -0.46776605,\n",
       "        -0.46770982, -0.47209563, -0.47225889, -0.47218967, -0.46986229,\n",
       "        -0.46998409, -0.47048843, -0.4705127 , -0.46819729, -0.46894074,\n",
       "        -0.46822309, -0.46907349, -0.46877873, -0.47357571, -0.47268245,\n",
       "        -0.47445089, -0.46976701, -0.46967027, -0.4702208 , -0.46922936,\n",
       "        -0.46802215, -0.4687029 , -0.46865406, -0.46779554, -0.46768156,\n",
       "        -0.47473668, -0.46944916, -0.46968691, -0.47133816, -0.4751556 ,\n",
       "        -0.46859334, -0.47407164, -0.47295059, -0.46761784, -0.47858735,\n",
       "        -0.47148049, -0.47336015, -0.47098642, -0.47163491, -0.46660331,\n",
       "        -0.46941675, -0.46817102, -0.47288046, -0.4739461 , -0.47030768,\n",
       "        -0.47654006, -0.46804582, -0.46710604, -0.46754366, -0.46647484,\n",
       "        -0.46931657, -0.47276491, -0.46808886, -0.46946439, -0.46698501,\n",
       "        -0.47047797, -0.47094613, -0.46430321, -0.46797128, -0.46558924,\n",
       "        -0.47519142, -0.46679605, -0.46836631, -0.46936895, -0.46579717,\n",
       "        -0.46372335, -0.46891621, -0.46733933, -0.46416921, -0.45981361,\n",
       "        -0.47298094, -0.4689976 , -0.46651302]),\n",
       " 'split4_test_neg_log_loss': array([-0.49883411, -0.4885406 , -0.49092435, -0.52934338, -0.51168741,\n",
       "        -0.50609675, -0.54383019, -0.76074665, -0.65004822, -0.61216164,\n",
       "        -0.60402149, -0.65282765, -0.49891745, -0.48622817, -0.48764825,\n",
       "        -0.5630593 , -0.5285254 , -0.52493743, -0.59727761, -0.58321004,\n",
       "        -0.5947356 , -0.6235169 , -0.6373706 , -0.67300559, -0.49006779,\n",
       "        -0.49356143, -0.4963879 , -0.53129236, -0.49982832, -0.52411514,\n",
       "        -0.56248499, -0.60859352, -0.57319936, -0.65027389, -0.61683255,\n",
       "        -0.63051277, -0.4771265 , -0.48174792, -0.49880184, -0.53813102,\n",
       "        -0.52755131, -0.52619999, -0.55047596, -0.55032744, -0.53634448,\n",
       "        -0.59062017, -0.5816838 , -0.60509094, -0.47951902, -0.5069284 ,\n",
       "        -0.48211211, -0.6109002 , -0.51215001, -0.53346066, -0.64857725,\n",
       "        -0.68004906, -0.60938636, -0.69970218, -0.67534895, -0.64260363,\n",
       "        -0.49090817, -0.47892099, -0.47225122, -0.53681257, -0.54593331,\n",
       "        -0.56495061, -0.57953957, -0.58338858, -0.6859926 , -0.58357275,\n",
       "        -0.68374744, -0.72365445, -0.48618172, -0.48501696, -0.48512103,\n",
       "        -0.55066914, -0.53938501, -0.51128719, -0.61474503, -0.58468037,\n",
       "        -0.60146454, -0.66568075, -0.72369254, -0.64386351, -0.48073178,\n",
       "        -0.48875812, -0.47441889, -0.52335797, -0.52949218, -0.53369174,\n",
       "        -0.56861582, -0.57321047, -0.59410986, -0.63478145, -0.6559875 ,\n",
       "        -0.61147954, -0.47112631, -0.77380131, -0.47312043, -0.47162771,\n",
       "        -0.71756719, -0.47364813, -0.46794013, -0.74046839, -0.4687517 ,\n",
       "        -0.47033064, -0.72109915, -0.47022456, -0.47522093, -0.67270633,\n",
       "        -0.47372322, -0.47295868, -0.70418762, -0.47200725, -0.47187281,\n",
       "        -0.69748092, -0.47014891, -0.46768996, -0.78702582, -0.46827968,\n",
       "        -0.47385036, -0.72593913, -0.47717581, -0.47200983, -0.71442666,\n",
       "        -0.47032983, -0.47054629, -0.66946869, -0.46962352, -0.46935029,\n",
       "        -0.67032128, -0.46945474, -0.47581504, -0.66690606, -0.47598617,\n",
       "        -0.47454207, -0.74450311, -0.47214343, -0.46934233, -0.71736871,\n",
       "        -0.47019664, -0.47071538, -0.69121416, -0.46855506, -0.47408601,\n",
       "        -0.77846954, -0.4769706 , -0.47817187, -0.79464131, -0.47420116,\n",
       "        -0.47495713, -0.75089305, -0.4707795 , -0.46994647, -0.67663872,\n",
       "        -0.46916366, -0.47787349, -0.75595866, -0.47702601, -0.47110776,\n",
       "        -0.59569523, -0.47162238, -0.47013722, -0.65459924, -0.47626574,\n",
       "        -0.46827247, -0.60886315, -0.47974438, -0.47739532, -0.58529519,\n",
       "        -0.48204449, -0.47153169, -0.67755657, -0.47278202, -0.47729734,\n",
       "        -0.6802519 , -0.47067264, -0.47111489, -0.69658787, -0.46786153,\n",
       "        -0.47232114, -0.73064235, -0.4741445 , -0.47722448, -0.70535552,\n",
       "        -0.47462446, -0.47786916, -0.77230724, -0.47563347, -0.46956515,\n",
       "        -0.69105098, -0.47252351, -0.47185065, -0.47301257, -0.47547807,\n",
       "        -0.46894404, -0.47050138, -0.47011248, -0.46960393, -0.46783878,\n",
       "        -0.46841752, -0.46851578, -0.46888596, -0.46872025, -0.4713177 ,\n",
       "        -0.47150288, -0.47267491, -0.46950815, -0.46931147, -0.46973137,\n",
       "        -0.47070663, -0.46840992, -0.46883805, -0.46756881, -0.46959562,\n",
       "        -0.46816867, -0.47097755, -0.47136372, -0.47163575, -0.46932161,\n",
       "        -0.47072945, -0.46905371, -0.46902848, -0.46945862, -0.46914208,\n",
       "        -0.46738545, -0.46767269, -0.46929661, -0.47112016, -0.47093163,\n",
       "        -0.47175709, -0.46962918, -0.4687243 , -0.46916394, -0.46834826,\n",
       "        -0.46855949, -0.4682061 , -0.46847952, -0.46753028, -0.46746841,\n",
       "        -0.47318198, -0.47422024, -0.47432887, -0.47765681, -0.47050505,\n",
       "        -0.47437207, -0.47996558, -0.47171366, -0.47483593, -0.48184549,\n",
       "        -0.48323017, -0.4784364 , -0.47218477, -0.47388909, -0.47192881,\n",
       "        -0.47786117, -0.4756732 , -0.47866171, -0.47960763, -0.4860217 ,\n",
       "        -0.47953049, -0.4879387 , -0.4824715 , -0.47959674, -0.47218602,\n",
       "        -0.47637468, -0.47257419, -0.47776592, -0.47103453, -0.47859765,\n",
       "        -0.47661106, -0.47905377, -0.48124002, -0.48327296, -0.48018307,\n",
       "        -0.47734946, -0.47203075, -0.47216572, -0.46802733, -0.47156428,\n",
       "        -0.47591631, -0.47432645, -0.4761173 , -0.478952  , -0.47340115,\n",
       "        -0.47910125, -0.47295966, -0.47407662]),\n",
       " 'mean_test_neg_log_loss': array([-0.48746581, -0.47953326, -0.47868185, -0.52232875, -0.5244632 ,\n",
       "        -0.51493855, -0.570621  , -0.60674432, -0.59086893, -0.63994335,\n",
       "        -0.62176393, -0.63134137, -0.47886883, -0.47850809, -0.47884445,\n",
       "        -0.53728459, -0.52932848, -0.52509656, -0.58053779, -0.56517293,\n",
       "        -0.57848305, -0.60704847, -0.64885176, -0.59853453, -0.48656453,\n",
       "        -0.48368987, -0.48174665, -0.52501958, -0.50368973, -0.51413445,\n",
       "        -0.55814456, -0.56534814, -0.57133043, -0.60584711, -0.60204923,\n",
       "        -0.60971088, -0.47693426, -0.47819829, -0.48098771, -0.50859982,\n",
       "        -0.50513004, -0.50337437, -0.53933011, -0.53950753, -0.52691663,\n",
       "        -0.56876563, -0.55825319, -0.56674294, -0.47436097, -0.48265454,\n",
       "        -0.47965812, -0.54517071, -0.52507865, -0.51293362, -0.59878911,\n",
       "        -0.62515052, -0.59846005, -0.65909897, -0.65810618, -0.63852868,\n",
       "        -0.4843325 , -0.47598258, -0.47457985, -0.5316639 , -0.55055491,\n",
       "        -0.52494255, -0.57127282, -0.58597771, -0.62471195, -0.63620249,\n",
       "        -0.65391906, -0.68820988, -0.47683432, -0.47983041, -0.47888804,\n",
       "        -0.5226436 , -0.52674087, -0.50662495, -0.58003301, -0.56293221,\n",
       "        -0.57182647, -0.64848886, -0.64083854, -0.65173466, -0.472397  ,\n",
       "        -0.47311787, -0.47462565, -0.51021138, -0.51529688, -0.52270788,\n",
       "        -0.54968381, -0.56140756, -0.55704634, -0.61429103, -0.60750181,\n",
       "        -0.60463596, -0.47032383, -0.71095505, -0.47139622, -0.46756277,\n",
       "        -0.70621258, -0.46715998, -0.46538011, -0.70725127, -0.46448017,\n",
       "        -0.46451437, -0.70476156, -0.4647099 , -0.47152903, -0.70298001,\n",
       "        -0.46995425, -0.46623983, -0.67573612, -0.4677638 , -0.46515719,\n",
       "        -0.69350844, -0.4647785 , -0.46311559, -0.71580675, -0.4629507 ,\n",
       "        -0.46988088, -0.71954407, -0.47140334, -0.46770277, -0.7045458 ,\n",
       "        -0.46564514, -0.46443691, -0.68977333, -0.46383658, -0.46403561,\n",
       "        -0.68694142, -0.46405664, -0.47169716, -0.72907742, -0.47258018,\n",
       "        -0.46798326, -0.71144085, -0.46695173, -0.4649694 , -0.70571789,\n",
       "        -0.46380955, -0.46397848, -0.69587924, -0.46304418, -0.47139118,\n",
       "        -0.69091088, -0.47053227, -0.46891553, -0.67686394, -0.46851431,\n",
       "        -0.46690246, -0.68007668, -0.4642062 , -0.46732158, -0.63508222,\n",
       "        -0.46378659, -0.4743247 , -0.73106716, -0.47155267, -0.46707277,\n",
       "        -0.65793959, -0.46580334, -0.46884677, -0.66978805, -0.46787428,\n",
       "        -0.4651058 , -0.60168187, -0.46560067, -0.47543378, -0.77316573,\n",
       "        -0.47336816, -0.4687377 , -0.73977608, -0.46766312, -0.47060114,\n",
       "        -0.65698872, -0.46904537, -0.46580498, -0.63163897, -0.46627611,\n",
       "        -0.47476687, -0.66896772, -0.4714785 , -0.4687773 , -0.74917979,\n",
       "        -0.4687103 , -0.46655279, -0.68097706, -0.46669133, -0.4650223 ,\n",
       "        -0.72989126, -0.46847631, -0.46523085, -0.46512395, -0.46474834,\n",
       "        -0.46248226, -0.46206271, -0.46229005, -0.46157165, -0.460966  ,\n",
       "        -0.46160824, -0.46088978, -0.46089737, -0.46085148, -0.46454672,\n",
       "        -0.46492216, -0.46402922, -0.46286186, -0.46258057, -0.46180681,\n",
       "        -0.46193404, -0.46167322, -0.46094313, -0.4606421 , -0.46080977,\n",
       "        -0.46058166, -0.46366871, -0.4643077 , -0.46397627, -0.46225632,\n",
       "        -0.46283247, -0.46226147, -0.46193342, -0.4612487 , -0.46162048,\n",
       "        -0.46091345, -0.46094734, -0.46156933, -0.46490546, -0.46441983,\n",
       "        -0.4649566 , -0.46214998, -0.46183808, -0.46200167, -0.46107167,\n",
       "        -0.46075588, -0.46080936, -0.46066001, -0.46037404, -0.46041203,\n",
       "        -0.46719365, -0.46289498, -0.4646337 , -0.46501894, -0.46581299,\n",
       "        -0.46619144, -0.46915097, -0.4675068 , -0.46770026, -0.47380317,\n",
       "        -0.47091392, -0.46970653, -0.46509427, -0.46509758, -0.46422051,\n",
       "        -0.46611674, -0.46453952, -0.46740092, -0.46854031, -0.47006111,\n",
       "        -0.46817525, -0.47270042, -0.47016633, -0.47246784, -0.46388427,\n",
       "        -0.46385034, -0.46493143, -0.46770093, -0.46666792, -0.46500571,\n",
       "        -0.46702942, -0.46926977, -0.46830194, -0.46978616, -0.46964954,\n",
       "        -0.46913652, -0.46339491, -0.46286208, -0.46214124, -0.46264031,\n",
       "        -0.46440533, -0.46429293, -0.46541964, -0.46323061, -0.46265026,\n",
       "        -0.46708021, -0.46496173, -0.46577352]),\n",
       " 'std_test_neg_log_loss': array([0.01517893, 0.01450203, 0.01118185, 0.01182507, 0.00753813,\n",
       "        0.03098564, 0.03525401, 0.07859327, 0.04491002, 0.05692354,\n",
       "        0.03509487, 0.03437234, 0.01418789, 0.01433368, 0.01068491,\n",
       "        0.01613376, 0.0094793 , 0.02481264, 0.0131019 , 0.02335485,\n",
       "        0.02652162, 0.03295713, 0.03282659, 0.04274995, 0.01519869,\n",
       "        0.01519799, 0.01449737, 0.01825248, 0.01063607, 0.01399479,\n",
       "        0.03145756, 0.02617627, 0.01965747, 0.03528202, 0.01999635,\n",
       "        0.04540873, 0.01416615, 0.01333512, 0.01470889, 0.01971364,\n",
       "        0.01508117, 0.01984963, 0.01469165, 0.01349163, 0.01139963,\n",
       "        0.03394294, 0.02586604, 0.02233661, 0.00823735, 0.01918165,\n",
       "        0.01200335, 0.03699199, 0.02087963, 0.0156202 , 0.02717069,\n",
       "        0.04630998, 0.03945377, 0.04534233, 0.05779039, 0.05704199,\n",
       "        0.01434617, 0.00870154, 0.00499858, 0.0075983 , 0.04500681,\n",
       "        0.02256664, 0.0222495 , 0.0203053 , 0.04749189, 0.03690048,\n",
       "        0.05399772, 0.05463004, 0.00995861, 0.00769668, 0.00895313,\n",
       "        0.02402823, 0.01426851, 0.00474806, 0.03549999, 0.02250552,\n",
       "        0.02259186, 0.0298701 , 0.0614956 , 0.04073312, 0.0147398 ,\n",
       "        0.00940522, 0.01270755, 0.01394012, 0.0163716 , 0.02243102,\n",
       "        0.01608323, 0.03304996, 0.02517198, 0.02314849, 0.03391128,\n",
       "        0.02308774, 0.00664375, 0.03785652, 0.0074881 , 0.00664857,\n",
       "        0.02603239, 0.00713868, 0.00768733, 0.02989324, 0.00742507,\n",
       "        0.00702694, 0.01527308, 0.00734466, 0.0068078 , 0.02627177,\n",
       "        0.00726468, 0.0072837 , 0.01757901, 0.00835463, 0.00734842,\n",
       "        0.01583963, 0.00965275, 0.00698603, 0.0378637 , 0.00712833,\n",
       "        0.0073118 , 0.01549474, 0.00789757, 0.00736745, 0.02790512,\n",
       "        0.00705012, 0.00845157, 0.01779533, 0.00705243, 0.00725324,\n",
       "        0.01341202, 0.00657842, 0.00601007, 0.06117393, 0.00487101,\n",
       "        0.00642591, 0.02118271, 0.00726402, 0.00757399, 0.01123399,\n",
       "        0.0070994 , 0.00761388, 0.01294898, 0.00741784, 0.01035184,\n",
       "        0.04861872, 0.01165441, 0.00863768, 0.07693006, 0.00802646,\n",
       "        0.01022327, 0.06049883, 0.00929141, 0.00852792, 0.0358028 ,\n",
       "        0.01032282, 0.00646034, 0.06535662, 0.01145335, 0.00880536,\n",
       "        0.05548411, 0.00643722, 0.00803418, 0.03975435, 0.00847954,\n",
       "        0.00916545, 0.02463409, 0.01062873, 0.00583147, 0.1369551 ,\n",
       "        0.00900626, 0.00648823, 0.10737131, 0.01067753, 0.00760606,\n",
       "        0.01812965, 0.00933038, 0.00932913, 0.03625619, 0.00868673,\n",
       "        0.00480413, 0.09557397, 0.00821265, 0.01009562, 0.04407385,\n",
       "        0.01021572, 0.00906777, 0.06318082, 0.00881581, 0.0098354 ,\n",
       "        0.08801477, 0.00693437, 0.00981544, 0.00919965, 0.0098199 ,\n",
       "        0.00861341, 0.00962686, 0.00924568, 0.00917659, 0.00895822,\n",
       "        0.00874352, 0.00898476, 0.00929397, 0.0091936 , 0.00813188,\n",
       "        0.00958017, 0.01023482, 0.00906891, 0.00861845, 0.00881362,\n",
       "        0.00880609, 0.00896237, 0.00921865, 0.009062  , 0.00919398,\n",
       "        0.00873116, 0.00985237, 0.00862098, 0.00911172, 0.00882476,\n",
       "        0.00890788, 0.00891139, 0.00926326, 0.00886418, 0.00884983,\n",
       "        0.00912073, 0.00919454, 0.0086065 , 0.0078491 , 0.00843786,\n",
       "        0.0092791 , 0.00868527, 0.00850013, 0.00894016, 0.00887881,\n",
       "        0.00887966, 0.00882866, 0.00918186, 0.00894092, 0.0085474 ,\n",
       "        0.01026447, 0.01078369, 0.01097726, 0.0113427 , 0.01209613,\n",
       "        0.01116958, 0.01156785, 0.01060083, 0.00936396, 0.00997159,\n",
       "        0.01227685, 0.00963303, 0.01211494, 0.00992903, 0.00990344,\n",
       "        0.01196121, 0.01039868, 0.01026168, 0.01163592, 0.01402821,\n",
       "        0.0122408 , 0.01178084, 0.00991791, 0.00993923, 0.00994299,\n",
       "        0.01115598, 0.01119964, 0.01322261, 0.00891984, 0.01274191,\n",
       "        0.01291326, 0.01263993, 0.01179499, 0.01241189, 0.01074131,\n",
       "        0.01051999, 0.01040946, 0.00987391, 0.00985924, 0.01001004,\n",
       "        0.01173378, 0.01111708, 0.012633  , 0.0124766 , 0.01096559,\n",
       "        0.01219313, 0.0119789 , 0.01027681]),\n",
       " 'rank_test_neg_log_loss': array([184, 175, 171, 195, 198, 193, 222, 238, 230, 251, 243, 246, 173,\n",
       "        170, 172, 207, 205, 202, 228, 218, 226, 239, 254, 232, 183, 181,\n",
       "        179, 200, 186, 192, 214, 219, 224, 237, 235, 241, 168, 169, 178,\n",
       "        189, 187, 185, 208, 209, 204, 221, 215, 220, 161, 180, 176, 210,\n",
       "        201, 191, 233, 245, 231, 260, 259, 250, 182, 166, 162, 206, 212,\n",
       "        199, 223, 229, 244, 249, 256, 268, 167, 177, 174, 196, 203, 188,\n",
       "        227, 217, 225, 253, 252, 255, 153, 157, 163, 190, 194, 197, 211,\n",
       "        216, 213, 242, 240, 236, 142, 279, 147, 113, 277, 108,  88, 278,\n",
       "         65,  66, 275,  70, 150, 273, 139,  98, 263, 118,  86, 271,  72,\n",
       "         44, 281,  42, 138, 282, 148, 117, 274,  91,  64, 269,  50,  56,\n",
       "        267,  57, 152, 283, 155, 120, 280, 104,  78, 276,  49,  54, 272,\n",
       "         43, 146, 270, 143, 130, 264, 124, 103, 265,  58, 110, 248,  48,\n",
       "        160, 285, 151, 106, 258,  93, 129, 262, 119,  84, 234,  90, 165,\n",
       "        288, 158, 127, 286, 114, 144, 257, 131,  94, 247,  99, 164, 261,\n",
       "        149, 128, 287, 126, 100, 266, 102,  81, 284, 123,  87,  85,  71,\n",
       "         34,  28,  33,  19,  15,  20,  10,  11,   9,  68,  74,  55,  39,\n",
       "         35,  23,  26,  22,  13,   4,   8,   3,  47,  61,  53,  31,  38,\n",
       "         32,  25,  17,  21,  12,  14,  18,  73,  63,  76,  30,  24,  27,\n",
       "         16,   6,   7,   5,   1,   2, 109,  41,  69,  80,  95,  97, 133,\n",
       "        112, 115, 159, 145, 136,  82,  83,  59,  96,  67, 111, 125, 140,\n",
       "        121, 156, 141, 154,  52,  51,  75, 116, 101,  79, 105, 134, 122,\n",
       "        137, 135, 132,  46,  40,  29,  36,  62,  60,  89,  45,  37, 107,\n",
       "         77,  92])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRIAL_2_MLP.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL TWO NEG LOG LOSS RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.1,\n",
       " 'classifier__hidden_layer_sizes': (5,),\n",
       " 'classifier__learning_rate': 'adaptive',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL TWO NEG LOG LOSS RESULTS ---------------------------------------------\")\n",
    "TRIAL_2_MLP.cv_results_['rank_test_neg_log_loss']\n",
    "TRIAL_2_MLP.cv_results_['params'][ np.argmin(TRIAL_2_MLP.cv_results_['rank_test_neg_log_loss']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL TWO F1 RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.1,\n",
       " 'classifier__hidden_layer_sizes': (20,),\n",
       " 'classifier__learning_rate': 'invscaling',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL TWO F1 RESULTS ---------------------------------------------\")\n",
    "TRIAL_2_MLP.cv_results_['rank_test_f1_micro']\n",
    "TRIAL_2_MLP.cv_results_['params'][ np.argmin(TRIAL_2_MLP.cv_results_['rank_test_f1_micro']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL TWO ROC AUC RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.1,\n",
       " 'classifier__hidden_layer_sizes': (15,),\n",
       " 'classifier__learning_rate': 'constant',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL TWO ROC AUC RESULTS ---------------------------------------------\")\n",
    "TRIAL_2_MLP.cv_results_['rank_test_roc_auc_ovo']\n",
    "TRIAL_2_MLP.cv_results_['params'][ np.argmin(TRIAL_2_MLP.cv_results_['rank_test_roc_auc_ovo']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL TWO JACCARD SCORE RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.01,\n",
       " 'classifier__hidden_layer_sizes': (20,),\n",
       " 'classifier__learning_rate': 'adaptive',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL TWO JACCARD SCORE RESULTS ---------------------------------------------\")\n",
    "TRIAL_2_MLP.cv_results_['rank_test_jaccard']\n",
    "TRIAL_2_MLP.cv_results_['params'][ np.argmin(TRIAL_2_MLP.cv_results_['rank_test_jaccard']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 2 Multilayered Perceptrons using best NEG LOG LOSS hyperparameters :0.788503329828251\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 2 Multilayered Perceptrons using best F1 hyperparameters :0.810024535576586\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 2 Multilayered Perceptrons using best ROC_AUC hyperparameters :0.8015422362425517\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 2 Multilayered Perceptrons using best JACCARD hyperparameters :0.8096740273396424\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best NEG LOG LOSS results to train on test set, \n",
    "bestMPLT2_1 = MLPClassifier(activation = 'logistic', alpha = .01, hidden_layer_sizes = (5,), \n",
    "                          learning_rate = 'adaptive',solver = 'lbfgs')\n",
    "bestMPLT2_1.fit(X_train,y_train)\n",
    "y_pred2_1 = bestMPLT1_1.predict(X_test)\n",
    "print('Accuracy of Trial 2 Multilayered Perceptrons using best NEG LOG LOSS hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred2_1)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best F1 results to train on test set, \n",
    "bestMPLT2_2 = MLPClassifier(activation = 'logistic', alpha = 0.1, hidden_layer_sizes = (20,), \n",
    "                          learning_rate = 'invscaling',solver = 'lbfgs')\n",
    "bestMPLT2_2.fit(X_train,y_train)\n",
    "y_pred2_2 = bestMPLT2_2.predict(X_test)\n",
    "print('Accuracy of Trial 2 Multilayered Perceptrons using best F1 hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred2_2)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best ROC_AUC results to train on test set,\n",
    "bestMPLT2_3 = MLPClassifier(activation = 'logistic', alpha = 0.1, hidden_layer_sizes = (15,), \n",
    "                          learning_rate = 'constant',solver = 'lbfgs')\n",
    "bestMPLT2_3.fit(X_train,y_train)\n",
    "y_pred2_3 = bestMPLT2_3.predict(X_test)\n",
    "print('Accuracy of Trial 2 Multilayered Perceptrons using best ROC_AUC hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred2_3)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best JACCARD results to train on test set, \n",
    "bestMPLT2_4 = MLPClassifier(activation = 'logistic', alpha = 0.01, hidden_layer_sizes = (20,), \n",
    "                          learning_rate = 'adaptive',solver = 'lbfgs')\n",
    "bestMPLT2_4.fit(X_train,y_train)\n",
    "y_pred2_4 = bestMPLT2_4.predict(X_test)\n",
    "print('Accuracy of Trial 2 Multilayered Perceptrons using best JACCARD hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred2_4)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTI LAYERED PERCEPTRON TRIAL THREE ON GAMMA PARTICLES DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:    6.4s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    7.4s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:    8.5s\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:   10.1s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:   10.7s\n",
      "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed:   11.8s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   13.7s\n",
      "[Parallel(n_jobs=-1)]: Done 189 tasks      | elapsed:   14.6s\n",
      "[Parallel(n_jobs=-1)]: Done 210 tasks      | elapsed:   15.7s\n",
      "[Parallel(n_jobs=-1)]: Done 233 tasks      | elapsed:   17.9s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:   19.2s\n",
      "[Parallel(n_jobs=-1)]: Done 281 tasks      | elapsed:   21.1s\n",
      "[Parallel(n_jobs=-1)]: Done 306 tasks      | elapsed:   23.3s\n",
      "[Parallel(n_jobs=-1)]: Done 333 tasks      | elapsed:   25.4s\n",
      "[Parallel(n_jobs=-1)]: Done 360 tasks      | elapsed:   27.9s\n",
      "[Parallel(n_jobs=-1)]: Done 389 tasks      | elapsed:   29.4s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:   32.3s\n",
      "[Parallel(n_jobs=-1)]: Done 449 tasks      | elapsed:   33.9s\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:   37.1s\n",
      "[Parallel(n_jobs=-1)]: Done 513 tasks      | elapsed:   41.5s\n",
      "[Parallel(n_jobs=-1)]: Done 546 tasks      | elapsed:   45.2s\n",
      "[Parallel(n_jobs=-1)]: Done 581 tasks      | elapsed:   49.2s\n",
      "[Parallel(n_jobs=-1)]: Done 616 tasks      | elapsed:   53.0s\n",
      "[Parallel(n_jobs=-1)]: Done 653 tasks      | elapsed:   57.8s\n",
      "[Parallel(n_jobs=-1)]: Done 690 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 729 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 809 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 850 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 893 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 936 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 981 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1026 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1073 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1120 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1169 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1269 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1320 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1373 tasks      | elapsed:  2.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL THREE RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1440 out of 1440 | elapsed:  3.1min finished\n"
     ]
    }
   ],
   "source": [
    "ITERATIONS = 1\n",
    "for i in range(1):\n",
    "    #For reach \"trial\", randomly select 5000 data points\n",
    "    random5000DataPoints = gammaData.sample(n = 5000, replace = True)\n",
    "    #From that 5000, create x set and y set\n",
    "    xSet = random5000DataPoints.iloc[:,0:9].values\n",
    "    ySet = random5000DataPoints.iloc[:,10].values\n",
    "    ySet = encodeLabel.fit_transform(ySet)\n",
    "    # Run GridSearch to find best hyperparameters per 5000 data points \n",
    "    \n",
    "    pipe = Pipeline([('std', StandardScaler()),('classifier', MLPClassifier(max_iter = 250))])\n",
    "\n",
    "    search_space = [\n",
    "                {\n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                {\n",
    "                 \n",
    "                  \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['sgd'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                \n",
    "                {\n",
    "                \n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['adam'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                ]\n",
    "    \n",
    "    runHyperParamSearch = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=5), \n",
    "                          scoring=['recall_micro', 'f1_micro','precision_micro','roc_auc_ovo','jaccard','neg_log_loss'], \n",
    "                          refit=False, verbose=10, n_jobs = -1)\n",
    "    TRIAL_3_MLP = runHyperParamSearch.fit(xSet, ySet)\n",
    "    print(\"---------------------------------TRIAL THREE RESULTS ---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.55898118, 0.54576926, 0.61012526, 0.76916361, 0.78097334,\n",
       "        0.75655026, 1.11235619, 1.14598556, 1.21054177, 1.50519571,\n",
       "        1.43643308, 1.4064085 , 0.68869133, 0.55797811, 0.55928106,\n",
       "        0.73743463, 0.74063931, 0.78827925, 1.11856389, 1.15709648,\n",
       "        1.26428852, 1.51139994, 1.54622841, 1.43252935, 0.65816345,\n",
       "        0.57489424, 0.56888924, 0.73693495, 0.75284824, 0.78467746,\n",
       "        1.08983936, 1.16169906, 1.15639434, 1.44444318, 1.43863697,\n",
       "        1.42942839, 0.53455844, 0.57159019, 0.6174315 , 0.81630301,\n",
       "        0.78507438, 0.80289006, 1.12897224, 1.11785941, 1.22065077,\n",
       "        1.55553699, 1.55133328, 1.55653582, 0.78527551, 0.71351361,\n",
       "        0.72722549, 1.02768393, 1.01117005, 0.96943388, 1.49378395,\n",
       "        1.43053112, 1.41051445, 1.85279384, 1.86840653, 1.91404538,\n",
       "        0.76295576, 0.65446296, 0.66567378, 0.92189298, 0.92319498,\n",
       "        0.94211206, 1.35796905, 1.37188048, 1.47326746, 2.10670953,\n",
       "        1.93386278, 2.08158989, 0.72882628, 0.62964177, 0.63444729,\n",
       "        0.89336967, 0.89817262, 0.91378837, 1.36057029, 1.37218075,\n",
       "        1.42212234, 1.8071537 , 1.72948751, 1.82176576, 0.72472315,\n",
       "        0.6440527 , 0.63294468, 0.89797273, 0.88406153, 0.92879968,\n",
       "        1.36277294, 1.44364166, 1.49768939, 1.85119224, 1.79834766,\n",
       "        1.84948859, 2.64387226, 0.1726491 , 2.39936275, 2.47352748,\n",
       "        0.58170066, 2.47572927, 2.84464664, 0.37652407, 2.83303647,\n",
       "        2.85695672, 0.31186843, 2.91370592, 2.39806166, 0.19516854,\n",
       "        2.37794547, 2.43059072, 0.20507612, 2.39676108, 2.74876399,\n",
       "        0.21168218, 2.75937338, 2.84444633, 0.48351603, 2.79550414,\n",
       "        2.25774121, 0.15663471, 2.29557433, 2.49704776, 0.2075789 ,\n",
       "        2.62145391, 2.96064696, 0.58110027, 2.79580445, 2.8230279 ,\n",
       "        0.23690367, 2.91620712, 2.37624316, 0.43897738, 2.35582566,\n",
       "        2.51986723, 0.26362715, 2.49144325, 2.80601373, 0.25591998,\n",
       "        2.66349044, 2.85575533, 0.39063592, 2.89869256, 2.41467652,\n",
       "        0.76005349, 2.46491952, 2.5569984 , 0.22399292, 2.56110277,\n",
       "        2.90419798, 0.86034017, 2.86146073, 2.92581582, 0.62663937,\n",
       "        2.87297063, 2.35282321, 0.94631319, 2.36063013, 2.4890408 ,\n",
       "        0.39433904, 2.47843184, 2.68841219, 0.7380352 , 2.75206699,\n",
       "        2.84484658, 0.78457489, 2.83163476, 2.27575679, 0.51914701,\n",
       "        2.2964747 , 2.45010686, 0.56568651, 2.47442875, 2.75056553,\n",
       "        0.73222961, 2.85775809, 3.05092397, 1.09263964, 3.12258539,\n",
       "        2.58822589, 0.6854897 , 2.49884872, 2.59172897, 0.37872548,\n",
       "        2.69241576, 2.84144444, 0.60411959, 2.742559  , 2.87156935,\n",
       "        0.62063441, 2.90449781, 2.5096581 , 2.52687316, 2.49594612,\n",
       "        2.69501762, 2.72244134, 2.6832078 , 2.98586812, 2.95954571,\n",
       "        2.93482375, 3.02159834, 3.07154174, 3.03751202, 2.54678974,\n",
       "        2.52987599, 2.59663329, 2.72854633, 2.75436888, 2.76838098,\n",
       "        3.01088939, 2.93202176, 2.94913611, 3.16692333, 3.0707407 ,\n",
       "        3.0341094 , 2.6205533 , 2.55279541, 2.52116785, 2.68380814,\n",
       "        2.651581  , 2.67059698, 2.98046317, 3.06623673, 2.87527304,\n",
       "        3.05022297, 3.02990556, 3.06964006, 2.58011847, 2.64097133,\n",
       "        2.55900102, 2.59132853, 2.63546667, 2.62375631, 2.88998532,\n",
       "        3.13099294, 3.08795571, 3.20265431, 3.36999812, 3.18553939,\n",
       "        2.83463793, 2.76427693, 2.72494354, 2.8414432 , 2.92361426,\n",
       "        2.97806101, 3.22747631, 3.19254599, 3.12498741, 3.22217126,\n",
       "        3.17703166, 3.19765   , 2.57761612, 2.64577546, 2.73255024,\n",
       "        2.79760718, 2.75376825, 2.89008532, 3.07464414, 3.13049173,\n",
       "        3.09085808, 3.22687507, 3.39642062, 3.78965883, 2.8441463 ,\n",
       "        2.74756293, 2.77138309, 2.87377176, 2.81011696, 2.82993431,\n",
       "        3.15291138, 3.13940015, 3.14810772, 3.26620884, 3.30474248,\n",
       "        3.29113011, 2.60804362, 2.55759964, 2.60884361, 2.77548685,\n",
       "        2.80891581, 2.79590421, 3.06613684, 3.10517068, 3.21576619,\n",
       "        3.1434031 , 2.87296953, 2.58322101]),\n",
       " 'std_fit_time': array([0.02095559, 0.02169905, 0.0444967 , 0.01528991, 0.01441655,\n",
       "        0.02155629, 0.0335637 , 0.03440289, 0.05495653, 0.07301764,\n",
       "        0.02982317, 0.01192048, 0.04458552, 0.02116522, 0.02977845,\n",
       "        0.01624514, 0.02456143, 0.03285203, 0.01167727, 0.05822601,\n",
       "        0.04521729, 0.07106878, 0.06914735, 0.04382132, 0.0335476 ,\n",
       "        0.01161788, 0.0343891 , 0.01589729, 0.0095663 , 0.02526173,\n",
       "        0.02158791, 0.02703023, 0.03306204, 0.03842901, 0.02587211,\n",
       "        0.03197336, 0.09262988, 0.03392739, 0.07752565, 0.05010287,\n",
       "        0.02094181, 0.05449892, 0.0480985 , 0.0181095 , 0.08281544,\n",
       "        0.09719019, 0.08596862, 0.04365706, 0.03731195, 0.03725083,\n",
       "        0.07247424, 0.04267368, 0.05599381, 0.04332993, 0.08473948,\n",
       "        0.07601745, 0.04663181, 0.10586828, 0.12111328, 0.13974327,\n",
       "        0.04383201, 0.03214522, 0.04129106, 0.01469073, 0.03204387,\n",
       "        0.03165728, 0.04902173, 0.01533486, 0.04284838, 0.13247189,\n",
       "        0.21487442, 0.07180698, 0.04387751, 0.01367689, 0.01893148,\n",
       "        0.0297461 , 0.01913974, 0.00963589, 0.04849127, 0.03251353,\n",
       "        0.03809804, 0.0875583 , 0.04563915, 0.12665196, 0.03275628,\n",
       "        0.01714453, 0.02046505, 0.00638138, 0.01179213, 0.01640583,\n",
       "        0.02792941, 0.08354799, 0.08913624, 0.18929732, 0.1196333 ,\n",
       "        0.14914459, 0.22738843, 0.02785691, 0.03451938, 0.09725398,\n",
       "        0.81797516, 0.08878401, 0.08979262, 0.38974157, 0.21553531,\n",
       "        0.0864934 , 0.16415302, 0.11280567, 0.06784404, 0.07295481,\n",
       "        0.06858039, 0.04915941, 0.05112437, 0.04301209, 0.10239746,\n",
       "        0.06305048, 0.04020039, 0.06139851, 0.25922945, 0.05233973,\n",
       "        0.04805093, 0.00901892, 0.02277594, 0.06573936, 0.03702016,\n",
       "        0.13520663, 0.11205817, 0.29912491, 0.06176797, 0.0817645 ,\n",
       "        0.06204257, 0.07519966, 0.07705818, 0.52341957, 0.08318372,\n",
       "        0.14930189, 0.15516607, 0.04786042, 0.10632373, 0.07884143,\n",
       "        0.02961334, 0.08961166, 0.2618858 , 0.13132654, 0.06955345,\n",
       "        0.75295162, 0.05913385, 0.05026024, 0.03326205, 0.12891157,\n",
       "        0.103049  , 0.75015268, 0.08542471, 0.05037578, 0.18154111,\n",
       "        0.06534502, 0.04558286, 0.76750516, 0.04480334, 0.04966276,\n",
       "        0.18066553, 0.04026388, 0.03316942, 0.50047527, 0.0430447 ,\n",
       "        0.03311159, 0.2634227 , 0.02477731, 0.03690116, 0.41886872,\n",
       "        0.01157292, 0.04434161, 0.35091691, 0.01767485, 0.06874762,\n",
       "        0.33615089, 0.08985531, 0.09747021, 0.69451654, 0.14865757,\n",
       "        0.10085267, 0.88999577, 0.07895828, 0.06973987, 0.20443286,\n",
       "        0.06804641, 0.04238425, 0.17341485, 0.01972108, 0.04175772,\n",
       "        0.17446888, 0.05617709, 0.03766765, 0.02981462, 0.02630313,\n",
       "        0.11663547, 0.0940179 , 0.0339534 , 0.05509421, 0.06480667,\n",
       "        0.03919963, 0.05227065, 0.04176891, 0.04470438, 0.05133929,\n",
       "        0.06118009, 0.08119018, 0.14046282, 0.08405533, 0.06236369,\n",
       "        0.13298361, 0.05228131, 0.05295816, 0.07287253, 0.02907542,\n",
       "        0.04458353, 0.08156901, 0.08043853, 0.06957925, 0.07154076,\n",
       "        0.06553646, 0.05128543, 0.08315307, 0.12941122, 0.02322692,\n",
       "        0.05400654, 0.02837818, 0.08994699, 0.1061628 , 0.04130962,\n",
       "        0.04650478, 0.0272513 , 0.03769345, 0.03465522, 0.02910592,\n",
       "        0.12636889, 0.07564623, 0.12150846, 0.0416559 , 0.11450949,\n",
       "        0.17153863, 0.10099718, 0.08429874, 0.11078051, 0.05866598,\n",
       "        0.05680295, 0.13553739, 0.17164929, 0.03319583, 0.07926753,\n",
       "        0.02479118, 0.04685862, 0.01791368, 0.06433453, 0.13397968,\n",
       "        0.0834607 , 0.04542995, 0.10974134, 0.06131177, 0.08052463,\n",
       "        0.02820336, 0.04638458, 0.06614384, 0.21194988, 0.17319578,\n",
       "        0.13116539, 0.1254772 , 0.06630886, 0.07606113, 0.06700452,\n",
       "        0.13275805, 0.10877456, 0.07302326, 0.06561968, 0.11381886,\n",
       "        0.12712235, 0.06151316, 0.02426824, 0.07913133, 0.0538543 ,\n",
       "        0.08443305, 0.03114432, 0.04231155, 0.0627407 , 0.09905852,\n",
       "        0.0287615 , 0.13479797, 0.09551481]),\n",
       " 'mean_score_time': array([0.01141024, 0.01251044, 0.0115099 , 0.01170902, 0.01130924,\n",
       "        0.01241283, 0.01371188, 0.01281066, 0.01331162, 0.01741447,\n",
       "        0.01231027, 0.01110868, 0.01411219, 0.01091003, 0.01581426,\n",
       "        0.01140995, 0.01221142, 0.01281061, 0.01571412, 0.01621518,\n",
       "        0.01991844, 0.02332244, 0.0124114 , 0.01080871, 0.0119103 ,\n",
       "        0.00990853, 0.01110997, 0.0125104 , 0.01321087, 0.0135098 ,\n",
       "        0.0132102 , 0.01401305, 0.01331048, 0.01371136, 0.01231132,\n",
       "        0.01231184, 0.01311097, 0.01160884, 0.01261158, 0.01100898,\n",
       "        0.01151175, 0.01231174, 0.01571283, 0.01481304, 0.02121806,\n",
       "        0.0159133 , 0.01401186, 0.01341195, 0.01030736, 0.01411171,\n",
       "        0.01541367, 0.01251001, 0.01231122, 0.0122108 , 0.01931815,\n",
       "        0.01601462, 0.01631541, 0.01731496, 0.0123106 , 0.0147121 ,\n",
       "        0.01311398, 0.01000838, 0.01110826, 0.01151047, 0.01171007,\n",
       "        0.01241164, 0.0140142 , 0.01421146, 0.01741476, 0.01521263,\n",
       "        0.01401167, 0.0125103 , 0.01471257, 0.01030831, 0.01160851,\n",
       "        0.01080823, 0.01251168, 0.01250949, 0.01350985, 0.01621399,\n",
       "        0.01461225, 0.01301079, 0.01191034, 0.01571341, 0.01130877,\n",
       "        0.01040874, 0.01050873, 0.01110921, 0.01160941, 0.0135119 ,\n",
       "        0.01661382, 0.01441197, 0.01541381, 0.01851501, 0.01601295,\n",
       "        0.0145123 , 0.01040897, 0.01020851, 0.01070919, 0.01220994,\n",
       "        0.00920801, 0.01130972, 0.01060944, 0.00990844, 0.00990829,\n",
       "        0.01191077, 0.00980859, 0.01010885, 0.01030912, 0.00870752,\n",
       "        0.00890732, 0.00920787, 0.01000924, 0.01131015, 0.01030927,\n",
       "        0.00920796, 0.0097085 , 0.01131005, 0.01281152, 0.010109  ,\n",
       "        0.00980897, 0.00940738, 0.01251101, 0.01601424, 0.00920806,\n",
       "        0.01191049, 0.01200981, 0.01110902, 0.00980821, 0.01020851,\n",
       "        0.01110959, 0.01091003, 0.00930905, 0.00930815, 0.01120968,\n",
       "        0.01000862, 0.00920777, 0.00940771, 0.01080956, 0.01140981,\n",
       "        0.01030908, 0.01231089, 0.01020947, 0.01211076, 0.0092082 ,\n",
       "        0.00900822, 0.01000876, 0.01110997, 0.00930824, 0.01060905,\n",
       "        0.00970879, 0.01060901, 0.01020889, 0.01020885, 0.01251068,\n",
       "        0.01140976, 0.00950828, 0.00960865, 0.00950832, 0.00940809,\n",
       "        0.00940838, 0.01221061, 0.01110992, 0.00940814, 0.01241078,\n",
       "        0.00980854, 0.00970802, 0.01020951, 0.0096086 , 0.00950785,\n",
       "        0.00910807, 0.01070943, 0.009308  , 0.01000857, 0.01341152,\n",
       "        0.01040936, 0.01150942, 0.01050868, 0.01090965, 0.01161017,\n",
       "        0.01000824, 0.00980849, 0.01050901, 0.01070948, 0.01050944,\n",
       "        0.00930805, 0.00990815, 0.01030903, 0.00980878, 0.01251087,\n",
       "        0.0097085 , 0.00970836, 0.01110978, 0.00940795, 0.00930786,\n",
       "        0.01100988, 0.009308  , 0.01471272, 0.00990829, 0.01010909,\n",
       "        0.0101089 , 0.0097084 , 0.00970826, 0.01020904, 0.01391187,\n",
       "        0.0111094 , 0.00980854, 0.01231108, 0.00910802, 0.00960817,\n",
       "        0.01040859, 0.0105093 , 0.01481261, 0.01010861, 0.00950809,\n",
       "        0.01080914, 0.01050968, 0.01000881, 0.00940862, 0.01190991,\n",
       "        0.00930791, 0.01090937, 0.00980849, 0.01020865, 0.00970817,\n",
       "        0.00950832, 0.00980844, 0.01080918, 0.00940843, 0.00920806,\n",
       "        0.00960827, 0.00930805, 0.01080918, 0.0093082 , 0.01131015,\n",
       "        0.01030855, 0.01020885, 0.01060891, 0.01000843, 0.01000853,\n",
       "        0.01110926, 0.01060901, 0.00990844, 0.01080966, 0.01010904,\n",
       "        0.00960832, 0.01160998, 0.00990825, 0.01020951, 0.01010871,\n",
       "        0.01070938, 0.01090922, 0.01111026, 0.00940795, 0.01251101,\n",
       "        0.01150913, 0.01000867, 0.01080966, 0.00970836, 0.01060925,\n",
       "        0.01050949, 0.01431255, 0.0122107 , 0.01131177, 0.0188158 ,\n",
       "        0.01040893, 0.00960898, 0.01110978, 0.01020908, 0.01000853,\n",
       "        0.00980873, 0.01020856, 0.01100893, 0.01211061, 0.01020842,\n",
       "        0.01040897, 0.0095077 , 0.00940809, 0.00950823, 0.01371183,\n",
       "        0.00990858, 0.00970912, 0.01070924, 0.01070933, 0.01060896,\n",
       "        0.00840759, 0.00620537, 0.00600491]),\n",
       " 'std_score_time': array([0.00097029, 0.0017907 , 0.00100009, 0.00120927, 0.00098107,\n",
       "        0.00131919, 0.00116625, 0.00112276, 0.0012904 , 0.00575078,\n",
       "        0.00174917, 0.00037492, 0.00357327, 0.00124219, 0.00676426,\n",
       "        0.00073551, 0.00121183, 0.00147251, 0.00225104, 0.00290975,\n",
       "        0.0046862 , 0.00980842, 0.00182869, 0.00060014, 0.00198692,\n",
       "        0.00037438, 0.00124172, 0.00158341, 0.00172218, 0.00262944,\n",
       "        0.00107734, 0.00158287, 0.00067815, 0.00125022, 0.0006808 ,\n",
       "        0.00092896, 0.00323465, 0.00127993, 0.00246007, 0.00054781,\n",
       "        0.00141533, 0.00067822, 0.00225157, 0.00121049, 0.01244709,\n",
       "        0.00213234, 0.00321232, 0.00334034, 0.00051066, 0.0038684 ,\n",
       "        0.00460066, 0.00148328, 0.00087119, 0.00081326, 0.01090292,\n",
       "        0.00681145, 0.00284129, 0.00441517, 0.00120963, 0.00285958,\n",
       "        0.00115817, 0.00031689, 0.00073574, 0.00114089, 0.00102962,\n",
       "        0.00106778, 0.0020259 , 0.00050941, 0.00432191, 0.00419459,\n",
       "        0.00356676, 0.00155019, 0.00519695, 0.00074911, 0.00231175,\n",
       "        0.00040073, 0.00210093, 0.00104751, 0.00114137, 0.00438074,\n",
       "        0.00198855, 0.00114098, 0.00080257, 0.00485768, 0.00116773,\n",
       "        0.00058394, 0.00044771, 0.00037382, 0.00049259, 0.00161358,\n",
       "        0.00475138, 0.0013942 , 0.00324783, 0.00939372, 0.00223808,\n",
       "        0.00495377, 0.00073583, 0.00067894, 0.00067848, 0.00473239,\n",
       "        0.00067851, 0.00238143, 0.00146398, 0.00058377, 0.00037425,\n",
       "        0.00215584, 0.00040003, 0.0006641 , 0.00128957, 0.00024518,\n",
       "        0.00037423, 0.00024507, 0.00063305, 0.00242277, 0.00116754,\n",
       "        0.00024499, 0.00040026, 0.00067865, 0.00589213, 0.00049051,\n",
       "        0.00074895, 0.00058362, 0.0031966 , 0.00803905, 0.0004005 ,\n",
       "        0.00456885, 0.00114101, 0.00203633, 0.00051065, 0.0009282 ,\n",
       "        0.00106905, 0.00106862, 0.00040031, 0.00051057, 0.00441473,\n",
       "        0.00109637, 0.00051034, 0.00020061, 0.00112347, 0.00235525,\n",
       "        0.00024429, 0.00136457, 0.00140052, 0.00208474, 0.00024548,\n",
       "        0.00031688, 0.00054799, 0.00235559, 0.0002455 , 0.00171561,\n",
       "        0.00024458, 0.0017162 , 0.00067844, 0.00040066, 0.00476841,\n",
       "        0.00307508, 0.00063256, 0.00073544, 0.00054812, 0.00019972,\n",
       "        0.0003742 , 0.00565836, 0.00224668, 0.00020058, 0.00530931,\n",
       "        0.00024489, 0.00024569, 0.00024532, 0.00097082, 0.00077537,\n",
       "        0.00020063, 0.00244334, 0.0004002 , 0.00077546, 0.00556732,\n",
       "        0.00097031, 0.00164395, 0.00070746, 0.0017736 , 0.00131977,\n",
       "        0.00130484, 0.00067851, 0.00105007, 0.00227306, 0.00114135,\n",
       "        0.00024476, 0.00037498, 0.00116651, 0.0002446 , 0.00430479,\n",
       "        0.00024501, 0.00040021, 0.00324911, 0.00037466, 0.00067863,\n",
       "        0.00300255, 0.00024511, 0.00741336, 0.00066327, 0.00096983,\n",
       "        0.00037392, 0.00024509, 0.00024559, 0.00067892, 0.00659657,\n",
       "        0.00355816, 0.00074928, 0.00319055, 0.00019994, 0.00080092,\n",
       "        0.00037441, 0.00130526, 0.01111592, 0.00058349, 0.00031696,\n",
       "        0.00236022, 0.00217003, 0.00130517, 0.00019999, 0.00433303,\n",
       "        0.00024542, 0.0028204 , 0.00051025, 0.00087257, 0.00024499,\n",
       "        0.0003159 , 0.0003999 , 0.00081233, 0.00066368, 0.0002455 ,\n",
       "        0.00020063, 0.00024456, 0.00285935, 0.00024536, 0.0018616 ,\n",
       "        0.00067892, 0.00060022, 0.00146416, 0.00077516, 0.0008374 ,\n",
       "        0.0018564 , 0.00124207, 0.0009176 , 0.00128922, 0.00037484,\n",
       "        0.0003746 , 0.00245993, 0.00037456, 0.0009285 , 0.00037531,\n",
       "        0.00074862, 0.00091697, 0.00305849, 0.00086104, 0.00626675,\n",
       "        0.00332035, 0.00063271, 0.00188855, 0.00040009, 0.00080099,\n",
       "        0.00063331, 0.00737862, 0.00311083, 0.00143481, 0.00989639,\n",
       "        0.00139448, 0.00073507, 0.00135766, 0.0010302 , 0.00031613,\n",
       "        0.00024505, 0.00067892, 0.00083743, 0.0025398 , 0.00040058,\n",
       "        0.00066431, 0.00077503, 0.00086113, 0.00031621, 0.0079414 ,\n",
       "        0.00019987, 0.00024538, 0.00081352, 0.00098063, 0.00120065,\n",
       "        0.00142978, 0.00024563, 0.00031651]),\n",
       " 'param_classifier': masked_array(data=[MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__activation': masked_array(data=['logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__alpha': masked_array(data=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__hidden_layer_sizes': masked_array(data=[(5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__learning_rate': masked_array(data=['constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'}],\n",
       " 'split0_test_recall_micro': array([0.847, 0.843, 0.848, 0.854, 0.865, 0.856, 0.858, 0.862, 0.84 ,\n",
       "        0.866, 0.852, 0.847, 0.834, 0.85 , 0.852, 0.85 , 0.859, 0.863,\n",
       "        0.837, 0.849, 0.855, 0.854, 0.862, 0.85 , 0.85 , 0.857, 0.864,\n",
       "        0.86 , 0.863, 0.851, 0.84 , 0.856, 0.847, 0.84 , 0.848, 0.85 ,\n",
       "        0.856, 0.861, 0.859, 0.86 , 0.861, 0.868, 0.857, 0.86 , 0.855,\n",
       "        0.852, 0.86 , 0.859, 0.854, 0.847, 0.851, 0.854, 0.851, 0.846,\n",
       "        0.856, 0.836, 0.849, 0.848, 0.855, 0.847, 0.844, 0.862, 0.858,\n",
       "        0.866, 0.854, 0.845, 0.852, 0.874, 0.852, 0.853, 0.848, 0.854,\n",
       "        0.853, 0.86 , 0.852, 0.851, 0.851, 0.857, 0.843, 0.867, 0.847,\n",
       "        0.848, 0.866, 0.861, 0.855, 0.861, 0.859, 0.833, 0.844, 0.861,\n",
       "        0.852, 0.853, 0.856, 0.844, 0.853, 0.858, 0.764, 0.65 , 0.764,\n",
       "        0.764, 0.35 , 0.763, 0.769, 0.65 , 0.768, 0.768, 0.65 , 0.765,\n",
       "        0.761, 0.35 , 0.758, 0.768, 0.65 , 0.766, 0.767, 0.612, 0.763,\n",
       "        0.766, 0.379, 0.77 , 0.759, 0.405, 0.763, 0.765, 0.476, 0.765,\n",
       "        0.767, 0.337, 0.765, 0.77 , 0.65 , 0.768, 0.762, 0.65 , 0.765,\n",
       "        0.765, 0.65 , 0.766, 0.766, 0.65 , 0.767, 0.769, 0.68 , 0.767,\n",
       "        0.783, 0.376, 0.775, 0.801, 0.54 , 0.814, 0.801, 0.65 , 0.794,\n",
       "        0.805, 0.67 , 0.804, 0.784, 0.375, 0.784, 0.793, 0.381, 0.788,\n",
       "        0.802, 0.687, 0.791, 0.806, 0.618, 0.811, 0.78 , 0.604, 0.78 ,\n",
       "        0.815, 0.549, 0.787, 0.8  , 0.518, 0.797, 0.795, 0.437, 0.817,\n",
       "        0.778, 0.605, 0.786, 0.785, 0.575, 0.791, 0.807, 0.492, 0.791,\n",
       "        0.793, 0.628, 0.8  , 0.815, 0.795, 0.784, 0.816, 0.826, 0.824,\n",
       "        0.837, 0.839, 0.836, 0.835, 0.834, 0.831, 0.808, 0.819, 0.803,\n",
       "        0.816, 0.831, 0.823, 0.838, 0.84 , 0.84 , 0.83 , 0.828, 0.838,\n",
       "        0.807, 0.806, 0.818, 0.827, 0.821, 0.823, 0.832, 0.836, 0.831,\n",
       "        0.833, 0.839, 0.838, 0.813, 0.814, 0.801, 0.818, 0.821, 0.815,\n",
       "        0.828, 0.822, 0.821, 0.835, 0.829, 0.834, 0.848, 0.836, 0.845,\n",
       "        0.839, 0.846, 0.838, 0.838, 0.845, 0.843, 0.846, 0.856, 0.843,\n",
       "        0.844, 0.842, 0.836, 0.837, 0.853, 0.841, 0.85 , 0.842, 0.842,\n",
       "        0.843, 0.844, 0.851, 0.841, 0.843, 0.828, 0.841, 0.85 , 0.832,\n",
       "        0.844, 0.841, 0.849, 0.838, 0.849, 0.848, 0.837, 0.844, 0.837,\n",
       "        0.839, 0.843, 0.842, 0.841, 0.846, 0.847, 0.847, 0.838, 0.842]),\n",
       " 'split1_test_recall_micro': array([0.853, 0.851, 0.861, 0.858, 0.861, 0.866, 0.855, 0.853, 0.855,\n",
       "        0.848, 0.843, 0.87 , 0.853, 0.839, 0.842, 0.856, 0.858, 0.862,\n",
       "        0.847, 0.839, 0.857, 0.86 , 0.861, 0.862, 0.858, 0.853, 0.851,\n",
       "        0.851, 0.849, 0.842, 0.864, 0.858, 0.857, 0.858, 0.852, 0.857,\n",
       "        0.852, 0.849, 0.849, 0.845, 0.862, 0.844, 0.848, 0.862, 0.857,\n",
       "        0.849, 0.859, 0.859, 0.852, 0.851, 0.855, 0.86 , 0.854, 0.856,\n",
       "        0.853, 0.85 , 0.845, 0.864, 0.852, 0.846, 0.856, 0.855, 0.853,\n",
       "        0.852, 0.868, 0.861, 0.848, 0.845, 0.839, 0.851, 0.836, 0.846,\n",
       "        0.854, 0.852, 0.855, 0.859, 0.857, 0.85 , 0.856, 0.863, 0.847,\n",
       "        0.863, 0.85 , 0.858, 0.854, 0.851, 0.856, 0.854, 0.851, 0.846,\n",
       "        0.851, 0.842, 0.859, 0.854, 0.86 , 0.861, 0.775, 0.65 , 0.758,\n",
       "        0.772, 0.65 , 0.773, 0.77 , 0.65 , 0.772, 0.769, 0.326, 0.769,\n",
       "        0.779, 0.65 , 0.771, 0.777, 0.254, 0.774, 0.774, 0.65 , 0.772,\n",
       "        0.769, 0.65 , 0.771, 0.776, 0.441, 0.771, 0.771, 0.362, 0.772,\n",
       "        0.766, 0.35 , 0.775, 0.775, 0.619, 0.774, 0.773, 0.35 , 0.764,\n",
       "        0.771, 0.513, 0.778, 0.774, 0.493, 0.774, 0.776, 0.294, 0.771,\n",
       "        0.79 , 0.5  , 0.794, 0.832, 0.641, 0.819, 0.812, 0.613, 0.8  ,\n",
       "        0.825, 0.511, 0.823, 0.797, 0.486, 0.82 , 0.814, 0.651, 0.833,\n",
       "        0.798, 0.69 , 0.828, 0.822, 0.634, 0.827, 0.816, 0.64 , 0.809,\n",
       "        0.812, 0.504, 0.822, 0.8  , 0.532, 0.808, 0.813, 0.526, 0.81 ,\n",
       "        0.807, 0.32 , 0.795, 0.816, 0.752, 0.828, 0.829, 0.547, 0.808,\n",
       "        0.817, 0.649, 0.814, 0.809, 0.825, 0.824, 0.838, 0.831, 0.831,\n",
       "        0.836, 0.83 , 0.834, 0.835, 0.837, 0.837, 0.824, 0.806, 0.806,\n",
       "        0.829, 0.831, 0.835, 0.831, 0.832, 0.834, 0.84 , 0.835, 0.834,\n",
       "        0.803, 0.824, 0.826, 0.83 , 0.835, 0.832, 0.834, 0.832, 0.835,\n",
       "        0.837, 0.834, 0.839, 0.825, 0.825, 0.831, 0.831, 0.832, 0.828,\n",
       "        0.83 , 0.834, 0.833, 0.834, 0.831, 0.835, 0.842, 0.838, 0.833,\n",
       "        0.841, 0.853, 0.863, 0.854, 0.854, 0.856, 0.85 , 0.855, 0.852,\n",
       "        0.847, 0.838, 0.827, 0.855, 0.855, 0.856, 0.857, 0.86 , 0.856,\n",
       "        0.854, 0.859, 0.856, 0.848, 0.843, 0.85 , 0.85 , 0.849, 0.851,\n",
       "        0.858, 0.856, 0.852, 0.856, 0.857, 0.851, 0.837, 0.854, 0.836,\n",
       "        0.848, 0.846, 0.848, 0.856, 0.849, 0.854, 0.857, 0.858, 0.86 ]),\n",
       " 'split2_test_recall_micro': array([0.858, 0.85 , 0.85 , 0.862, 0.847, 0.855, 0.86 , 0.852, 0.859,\n",
       "        0.86 , 0.845, 0.851, 0.845, 0.85 , 0.857, 0.832, 0.848, 0.861,\n",
       "        0.854, 0.856, 0.854, 0.852, 0.858, 0.852, 0.852, 0.856, 0.834,\n",
       "        0.845, 0.859, 0.852, 0.853, 0.852, 0.852, 0.859, 0.837, 0.853,\n",
       "        0.858, 0.85 , 0.846, 0.849, 0.857, 0.841, 0.859, 0.851, 0.859,\n",
       "        0.854, 0.851, 0.858, 0.85 , 0.84 , 0.86 , 0.853, 0.835, 0.861,\n",
       "        0.856, 0.842, 0.865, 0.859, 0.852, 0.85 , 0.852, 0.861, 0.845,\n",
       "        0.843, 0.856, 0.85 , 0.846, 0.854, 0.851, 0.843, 0.853, 0.858,\n",
       "        0.852, 0.857, 0.845, 0.862, 0.842, 0.855, 0.846, 0.857, 0.859,\n",
       "        0.856, 0.846, 0.857, 0.839, 0.848, 0.858, 0.853, 0.854, 0.849,\n",
       "        0.853, 0.859, 0.856, 0.85 , 0.851, 0.846, 0.764, 0.726, 0.746,\n",
       "        0.764, 0.317, 0.764, 0.761, 0.651, 0.771, 0.764, 0.334, 0.766,\n",
       "        0.764, 0.349, 0.761, 0.766, 0.622, 0.764, 0.76 , 0.65 , 0.761,\n",
       "        0.764, 0.309, 0.759, 0.757, 0.633, 0.749, 0.755, 0.657, 0.761,\n",
       "        0.769, 0.65 , 0.766, 0.764, 0.65 , 0.762, 0.754, 0.464, 0.764,\n",
       "        0.762, 0.344, 0.762, 0.769, 0.646, 0.764, 0.763, 0.65 , 0.765,\n",
       "        0.796, 0.592, 0.784, 0.815, 0.663, 0.796, 0.798, 0.701, 0.783,\n",
       "        0.8  , 0.509, 0.79 , 0.784, 0.63 , 0.804, 0.817, 0.388, 0.784,\n",
       "        0.8  , 0.555, 0.799, 0.807, 0.394, 0.794, 0.783, 0.385, 0.775,\n",
       "        0.798, 0.57 , 0.781, 0.796, 0.653, 0.806, 0.809, 0.533, 0.792,\n",
       "        0.805, 0.575, 0.801, 0.791, 0.637, 0.809, 0.811, 0.441, 0.806,\n",
       "        0.798, 0.683, 0.804, 0.833, 0.783, 0.803, 0.828, 0.827, 0.817,\n",
       "        0.832, 0.833, 0.832, 0.833, 0.832, 0.836, 0.813, 0.828, 0.807,\n",
       "        0.819, 0.826, 0.834, 0.829, 0.832, 0.834, 0.835, 0.834, 0.835,\n",
       "        0.818, 0.784, 0.788, 0.832, 0.823, 0.825, 0.826, 0.829, 0.833,\n",
       "        0.832, 0.839, 0.835, 0.789, 0.802, 0.812, 0.822, 0.831, 0.821,\n",
       "        0.834, 0.826, 0.828, 0.832, 0.83 , 0.829, 0.842, 0.839, 0.834,\n",
       "        0.847, 0.841, 0.849, 0.85 , 0.843, 0.847, 0.854, 0.851, 0.853,\n",
       "        0.841, 0.828, 0.841, 0.854, 0.841, 0.853, 0.849, 0.849, 0.843,\n",
       "        0.849, 0.848, 0.855, 0.846, 0.849, 0.835, 0.853, 0.847, 0.848,\n",
       "        0.849, 0.853, 0.85 , 0.854, 0.851, 0.849, 0.836, 0.854, 0.85 ,\n",
       "        0.852, 0.845, 0.849, 0.86 , 0.847, 0.851, 0.851, 0.854, 0.85 ]),\n",
       " 'split3_test_recall_micro': array([0.824, 0.829, 0.837, 0.842, 0.841, 0.832, 0.84 , 0.843, 0.834,\n",
       "        0.843, 0.842, 0.846, 0.834, 0.835, 0.83 , 0.827, 0.839, 0.854,\n",
       "        0.846, 0.84 , 0.848, 0.839, 0.834, 0.84 , 0.846, 0.83 , 0.827,\n",
       "        0.833, 0.842, 0.839, 0.826, 0.833, 0.838, 0.846, 0.837, 0.84 ,\n",
       "        0.831, 0.839, 0.831, 0.845, 0.835, 0.835, 0.837, 0.838, 0.841,\n",
       "        0.838, 0.855, 0.841, 0.84 , 0.828, 0.839, 0.838, 0.843, 0.841,\n",
       "        0.835, 0.835, 0.846, 0.841, 0.841, 0.84 , 0.824, 0.839, 0.843,\n",
       "        0.836, 0.839, 0.836, 0.835, 0.832, 0.852, 0.84 , 0.84 , 0.841,\n",
       "        0.84 , 0.829, 0.833, 0.828, 0.835, 0.843, 0.837, 0.838, 0.84 ,\n",
       "        0.827, 0.847, 0.846, 0.844, 0.831, 0.832, 0.832, 0.838, 0.835,\n",
       "        0.85 , 0.84 , 0.835, 0.842, 0.838, 0.843, 0.762, 0.649, 0.768,\n",
       "        0.763, 0.717, 0.763, 0.77 , 0.649, 0.776, 0.774, 0.648, 0.777,\n",
       "        0.765, 0.649, 0.769, 0.773, 0.495, 0.776, 0.767, 0.629, 0.762,\n",
       "        0.773, 0.341, 0.776, 0.764, 0.656, 0.766, 0.779, 0.47 , 0.77 ,\n",
       "        0.772, 0.344, 0.771, 0.77 , 0.649, 0.774, 0.773, 0.689, 0.772,\n",
       "        0.775, 0.656, 0.774, 0.765, 0.649, 0.769, 0.769, 0.351, 0.77 ,\n",
       "        0.794, 0.592, 0.791, 0.807, 0.593, 0.794, 0.796, 0.48 , 0.788,\n",
       "        0.792, 0.621, 0.795, 0.789, 0.457, 0.796, 0.802, 0.672, 0.789,\n",
       "        0.803, 0.369, 0.789, 0.796, 0.69 , 0.804, 0.796, 0.554, 0.789,\n",
       "        0.81 , 0.301, 0.796, 0.805, 0.449, 0.787, 0.799, 0.499, 0.794,\n",
       "        0.782, 0.659, 0.793, 0.801, 0.642, 0.786, 0.802, 0.482, 0.787,\n",
       "        0.797, 0.355, 0.797, 0.798, 0.808, 0.809, 0.816, 0.815, 0.815,\n",
       "        0.816, 0.822, 0.817, 0.815, 0.817, 0.818, 0.809, 0.791, 0.788,\n",
       "        0.815, 0.817, 0.809, 0.82 , 0.812, 0.814, 0.816, 0.816, 0.817,\n",
       "        0.811, 0.798, 0.804, 0.808, 0.807, 0.807, 0.821, 0.814, 0.813,\n",
       "        0.819, 0.817, 0.818, 0.793, 0.798, 0.791, 0.808, 0.809, 0.814,\n",
       "        0.812, 0.807, 0.814, 0.802, 0.808, 0.812, 0.834, 0.821, 0.827,\n",
       "        0.837, 0.84 , 0.837, 0.845, 0.834, 0.842, 0.835, 0.851, 0.843,\n",
       "        0.83 , 0.834, 0.829, 0.839, 0.835, 0.837, 0.837, 0.836, 0.842,\n",
       "        0.846, 0.84 , 0.836, 0.828, 0.833, 0.833, 0.832, 0.831, 0.83 ,\n",
       "        0.838, 0.828, 0.833, 0.848, 0.844, 0.841, 0.837, 0.808, 0.831,\n",
       "        0.832, 0.835, 0.834, 0.836, 0.837, 0.836, 0.839, 0.833, 0.84 ]),\n",
       " 'split4_test_recall_micro': array([0.857, 0.85 , 0.851, 0.853, 0.863, 0.857, 0.851, 0.857, 0.862,\n",
       "        0.844, 0.856, 0.855, 0.855, 0.846, 0.854, 0.847, 0.853, 0.842,\n",
       "        0.858, 0.849, 0.841, 0.845, 0.852, 0.851, 0.855, 0.844, 0.846,\n",
       "        0.864, 0.858, 0.857, 0.851, 0.852, 0.851, 0.846, 0.852, 0.833,\n",
       "        0.86 , 0.867, 0.85 , 0.863, 0.847, 0.85 , 0.857, 0.865, 0.856,\n",
       "        0.84 , 0.854, 0.848, 0.859, 0.859, 0.855, 0.858, 0.86 , 0.855,\n",
       "        0.842, 0.849, 0.844, 0.845, 0.844, 0.845, 0.853, 0.855, 0.862,\n",
       "        0.864, 0.851, 0.845, 0.85 , 0.849, 0.854, 0.831, 0.843, 0.836,\n",
       "        0.845, 0.85 , 0.852, 0.855, 0.856, 0.853, 0.846, 0.853, 0.836,\n",
       "        0.855, 0.847, 0.843, 0.848, 0.848, 0.848, 0.859, 0.861, 0.859,\n",
       "        0.85 , 0.86 , 0.858, 0.848, 0.859, 0.857, 0.776, 0.553, 0.779,\n",
       "        0.781, 0.656, 0.781, 0.788, 0.351, 0.784, 0.783, 0.649, 0.781,\n",
       "        0.784, 0.649, 0.783, 0.785, 0.65 , 0.783, 0.787, 0.301, 0.787,\n",
       "        0.786, 0.649, 0.786, 0.786, 0.699, 0.784, 0.784, 0.337, 0.785,\n",
       "        0.785, 0.351, 0.782, 0.786, 0.649, 0.788, 0.776, 0.351, 0.785,\n",
       "        0.788, 0.343, 0.782, 0.784, 0.293, 0.785, 0.786, 0.649, 0.787,\n",
       "        0.81 , 0.309, 0.806, 0.812, 0.639, 0.809, 0.822, 0.425, 0.807,\n",
       "        0.813, 0.668, 0.815, 0.804, 0.389, 0.793, 0.823, 0.574, 0.821,\n",
       "        0.812, 0.602, 0.817, 0.814, 0.483, 0.825, 0.813, 0.416, 0.794,\n",
       "        0.815, 0.525, 0.807, 0.823, 0.547, 0.822, 0.814, 0.649, 0.815,\n",
       "        0.802, 0.526, 0.808, 0.811, 0.609, 0.812, 0.817, 0.652, 0.796,\n",
       "        0.808, 0.423, 0.806, 0.822, 0.832, 0.804, 0.838, 0.832, 0.836,\n",
       "        0.84 , 0.833, 0.838, 0.837, 0.833, 0.834, 0.822, 0.809, 0.835,\n",
       "        0.83 , 0.833, 0.833, 0.84 , 0.836, 0.834, 0.833, 0.838, 0.838,\n",
       "        0.828, 0.822, 0.837, 0.835, 0.837, 0.837, 0.837, 0.837, 0.838,\n",
       "        0.837, 0.835, 0.835, 0.823, 0.822, 0.826, 0.826, 0.833, 0.822,\n",
       "        0.836, 0.837, 0.834, 0.838, 0.838, 0.835, 0.852, 0.846, 0.843,\n",
       "        0.852, 0.857, 0.851, 0.852, 0.854, 0.849, 0.853, 0.855, 0.851,\n",
       "        0.842, 0.846, 0.84 , 0.852, 0.852, 0.85 , 0.862, 0.853, 0.858,\n",
       "        0.86 , 0.852, 0.862, 0.833, 0.846, 0.845, 0.853, 0.847, 0.854,\n",
       "        0.862, 0.855, 0.848, 0.86 , 0.853, 0.866, 0.849, 0.848, 0.85 ,\n",
       "        0.846, 0.856, 0.842, 0.85 , 0.853, 0.855, 0.849, 0.849, 0.861]),\n",
       " 'mean_test_recall_micro': array([0.8478, 0.8446, 0.8494, 0.8538, 0.8554, 0.8532, 0.8528, 0.8534,\n",
       "        0.85  , 0.8522, 0.8476, 0.8538, 0.8442, 0.844 , 0.847 , 0.8424,\n",
       "        0.8514, 0.8564, 0.8484, 0.8466, 0.851 , 0.85  , 0.8534, 0.851 ,\n",
       "        0.8522, 0.848 , 0.8444, 0.8506, 0.8542, 0.8482, 0.8468, 0.8502,\n",
       "        0.849 , 0.8498, 0.8452, 0.8466, 0.8514, 0.8532, 0.847 , 0.8524,\n",
       "        0.8524, 0.8476, 0.8516, 0.8552, 0.8536, 0.8466, 0.8558, 0.853 ,\n",
       "        0.851 , 0.845 , 0.852 , 0.8526, 0.8486, 0.8518, 0.8484, 0.8424,\n",
       "        0.8498, 0.8514, 0.8488, 0.8456, 0.8458, 0.8544, 0.8522, 0.8522,\n",
       "        0.8536, 0.8474, 0.8462, 0.8508, 0.8496, 0.8436, 0.844 , 0.847 ,\n",
       "        0.8488, 0.8496, 0.8474, 0.851 , 0.8482, 0.8516, 0.8456, 0.8556,\n",
       "        0.8458, 0.8498, 0.8512, 0.853 , 0.848 , 0.8478, 0.8506, 0.8462,\n",
       "        0.8496, 0.85  , 0.8512, 0.8508, 0.8528, 0.8476, 0.8522, 0.853 ,\n",
       "        0.7682, 0.6456, 0.763 , 0.7688, 0.538 , 0.7688, 0.7716, 0.5902,\n",
       "        0.7742, 0.7716, 0.5214, 0.7716, 0.7706, 0.5294, 0.7684, 0.7738,\n",
       "        0.5342, 0.7726, 0.771 , 0.5684, 0.769 , 0.7716, 0.4656, 0.7724,\n",
       "        0.7684, 0.5668, 0.7666, 0.7708, 0.4604, 0.7706, 0.7718, 0.4064,\n",
       "        0.7718, 0.773 , 0.6434, 0.7732, 0.7676, 0.5008, 0.77  , 0.7722,\n",
       "        0.5012, 0.7724, 0.7716, 0.5462, 0.7718, 0.7726, 0.5248, 0.772 ,\n",
       "        0.7946, 0.4738, 0.79  , 0.8134, 0.6152, 0.8064, 0.8058, 0.5738,\n",
       "        0.7944, 0.807 , 0.5958, 0.8054, 0.7916, 0.4674, 0.7994, 0.8098,\n",
       "        0.5332, 0.803 , 0.803 , 0.5806, 0.8048, 0.809 , 0.5638, 0.8122,\n",
       "        0.7976, 0.5198, 0.7894, 0.81  , 0.4898, 0.7986, 0.8048, 0.5398,\n",
       "        0.804 , 0.806 , 0.5288, 0.8056, 0.7948, 0.537 , 0.7966, 0.8008,\n",
       "        0.643 , 0.8052, 0.8132, 0.5228, 0.7976, 0.8026, 0.5476, 0.8042,\n",
       "        0.8154, 0.8086, 0.8048, 0.8272, 0.8262, 0.8246, 0.8322, 0.8314,\n",
       "        0.8314, 0.831 , 0.8306, 0.8312, 0.8152, 0.8106, 0.8078, 0.8218,\n",
       "        0.8276, 0.8268, 0.8316, 0.8304, 0.8312, 0.8308, 0.8302, 0.8324,\n",
       "        0.8134, 0.8068, 0.8146, 0.8264, 0.8246, 0.8248, 0.83  , 0.8296,\n",
       "        0.83  , 0.8316, 0.8328, 0.833 , 0.8086, 0.8122, 0.8122, 0.821 ,\n",
       "        0.8252, 0.82  , 0.828 , 0.8252, 0.826 , 0.8282, 0.8272, 0.829 ,\n",
       "        0.8436, 0.836 , 0.8364, 0.8432, 0.8474, 0.8476, 0.8478, 0.846 ,\n",
       "        0.8474, 0.8476, 0.8536, 0.8484, 0.8408, 0.8376, 0.8346, 0.8474,\n",
       "        0.8472, 0.8474, 0.851 , 0.848 , 0.8482, 0.8504, 0.8486, 0.852 ,\n",
       "        0.8392, 0.8428, 0.8382, 0.8458, 0.8448, 0.843 , 0.8502, 0.8466,\n",
       "        0.8464, 0.8512, 0.8508, 0.851 , 0.8392, 0.8416, 0.8408, 0.8434,\n",
       "        0.845 , 0.843 , 0.8486, 0.8464, 0.8486, 0.8486, 0.8464, 0.8506]),\n",
       " 'std_test_recall_micro': array([0.01251239, 0.00830903, 0.00765768, 0.00670522, 0.00958332,\n",
       "        0.0113031 , 0.00708237, 0.00628013, 0.01100909, 0.00917388,\n",
       "        0.0054626 , 0.00870402, 0.00897552, 0.00603324, 0.00987927,\n",
       "        0.01103812, 0.00733757, 0.00786384, 0.00722772, 0.0063435 ,\n",
       "        0.00583095, 0.00729383, 0.01030728, 0.0069857 , 0.00411825,\n",
       "        0.0100995 , 0.01297074, 0.01103812, 0.00762627, 0.00667533,\n",
       "        0.01289031, 0.00890842, 0.0063561 , 0.00744043, 0.00685274,\n",
       "        0.0088227 , 0.01053755, 0.00980612, 0.00909945, 0.00763151,\n",
       "        0.01019019, 0.01128893, 0.0082365 , 0.0097857 , 0.00643739,\n",
       "        0.00643739, 0.00331059, 0.00729383, 0.00626099, 0.01048809,\n",
       "        0.0070993 , 0.00773563, 0.00873155, 0.00724983, 0.00845222,\n",
       "        0.00628013, 0.00778203, 0.00868562, 0.00534416, 0.0032619 ,\n",
       "        0.0116    , 0.0082365 , 0.00730479, 0.01163443, 0.00930806,\n",
       "        0.00816333, 0.00594643, 0.01370255, 0.00538888, 0.00793977,\n",
       "        0.00596657, 0.00809938, 0.00541849, 0.0108922 , 0.00791454,\n",
       "        0.01208305, 0.00847113, 0.00488262, 0.00615142, 0.01003195,\n",
       "        0.00783326, 0.01235152, 0.00752064, 0.00712741, 0.00603324,\n",
       "        0.0096623 , 0.01007174, 0.01137365, 0.00796492, 0.00942338,\n",
       "        0.00116619, 0.00837616, 0.00897552, 0.00427083, 0.00788416,\n",
       "        0.00712741, 0.00601332, 0.05493487, 0.01091788, 0.00691086,\n",
       "        0.16893431, 0.00716659, 0.00886792, 0.11960167, 0.00552811,\n",
       "        0.00652993, 0.1562992 , 0.00631189, 0.00913455, 0.14688853,\n",
       "        0.00875443, 0.00679412, 0.1513782 , 0.00691665, 0.0091433 ,\n",
       "        0.13445683, 0.00981835, 0.00781281, 0.15178089, 0.00877724,\n",
       "        0.01100182, 0.11985057, 0.01135958, 0.010245  , 0.11304619,\n",
       "        0.00816333, 0.00691086, 0.12190258, 0.00624179, 0.00737564,\n",
       "        0.01220819, 0.00863481, 0.00830903, 0.14437091, 0.00807465,\n",
       "        0.00910824, 0.13855165, 0.00741889, 0.0069455 , 0.14017332,\n",
       "        0.00735935, 0.00786384, 0.16653096, 0.00779744, 0.00889044,\n",
       "        0.11432655, 0.01033441, 0.01044222, 0.04396544, 0.00985089,\n",
       "        0.00980612, 0.10436743, 0.00849941, 0.01129602, 0.07222022,\n",
       "        0.01224091, 0.00781281, 0.09118903, 0.01212601, 0.01083328,\n",
       "        0.12574164, 0.02002998, 0.00481664, 0.11762415, 0.0152368 ,\n",
       "        0.00867179, 0.10876838, 0.01251239, 0.01484049, 0.10163936,\n",
       "        0.0118423 , 0.00629285, 0.09697711, 0.01462327, 0.0095373 ,\n",
       "        0.06579787, 0.01167904, 0.00764199, 0.06897652, 0.01055651,\n",
       "        0.01225398, 0.11675787, 0.0074458 , 0.01166876, 0.05949454,\n",
       "        0.01517103, 0.00930376, 0.07291474, 0.00821219, 0.00873155,\n",
       "        0.13243806, 0.00581034, 0.01180847, 0.01820549, 0.01282809,\n",
       "        0.00984683, 0.00604649, 0.00801499, 0.0084947 , 0.00553534,\n",
       "        0.00747262, 0.00809938, 0.00700286, 0.00691086, 0.00661513,\n",
       "        0.0124996 , 0.01522367, 0.00643117, 0.00578273, 0.00988737,\n",
       "        0.00711618, 0.00966644, 0.00890842, 0.00808455, 0.00780769,\n",
       "        0.00786384, 0.0088227 , 0.01499867, 0.01710672, 0.00956243,\n",
       "        0.01083697, 0.01020588, 0.00576194, 0.00830903, 0.00880909,\n",
       "        0.00662118, 0.00815843, 0.00766812, 0.014988  , 0.01066583,\n",
       "        0.01495861, 0.00779744, 0.00917388, 0.00509902, 0.00848528,\n",
       "        0.01057166, 0.00756307, 0.01324236, 0.01010742, 0.00878635,\n",
       "        0.00611882, 0.00822192, 0.00668132, 0.00552811, 0.00665132,\n",
       "        0.00954149, 0.00574108, 0.00750999, 0.005004  , 0.00688767,\n",
       "        0.00215407, 0.00445421, 0.00577581, 0.0062482 , 0.00567803,\n",
       "        0.00776144, 0.00780769, 0.00722772, 0.00846168, 0.0083666 ,\n",
       "        0.00722219, 0.00601997, 0.00656049, 0.00874071, 0.00762627,\n",
       "        0.00538145, 0.00808455, 0.00818291, 0.00699714, 0.01      ,\n",
       "        0.00881816, 0.0107443 , 0.00682935, 0.00765245, 0.00430813,\n",
       "        0.00822192, 0.00491528, 0.01722324, 0.00778203, 0.00708802,\n",
       "        0.00672309, 0.00536656, 0.00897998, 0.00527636, 0.00688767,\n",
       "        0.0058515 , 0.0094784 , 0.00875443]),\n",
       " 'rank_test_recall_micro': array([ 83, 121,  65,   8,   4,  15,  20,  14,  56,  25,  86,   8, 123,\n",
       "        124,  98, 133,  35,   1,  74, 103,  41,  56,  13,  41,  25,  80,\n",
       "        122,  50,   7,  77, 101,  54,  66,  60, 117, 103,  35,  15, 100,\n",
       "         23,  23,  88,  33,   5,  10, 102,   2,  17,  41, 118,  30,  22,\n",
       "         69,  32,  74, 133,  59,  35,  67, 115, 112,   6,  25,  25,  10,\n",
       "         91, 109,  47,  63, 126, 124,  98,  67,  63,  91,  41,  77,  33,\n",
       "        115,   3, 112,  60,  38,  17,  80,  83,  50, 109,  62,  56,  38,\n",
       "         47,  20,  86,  25,  17, 253, 257, 256, 249, 271, 249, 238, 262,\n",
       "        225, 238, 279, 238, 245, 275, 251, 226, 273, 229, 243, 265, 248,\n",
       "        238, 286, 231, 251, 266, 255, 244, 287, 246, 235, 288, 235, 228,\n",
       "        258, 227, 254, 282, 247, 233, 281, 231, 238, 269, 235, 229, 277,\n",
       "        234, 220, 284, 223, 184, 260, 199, 201, 264, 221, 197, 261, 203,\n",
       "        222, 285, 214, 192, 274, 210, 210, 263, 205, 193, 267, 187, 217,\n",
       "        280, 224, 191, 283, 215, 205, 270, 209, 200, 276, 202, 219, 272,\n",
       "        218, 213, 259, 204, 186, 278, 216, 212, 268, 208, 181, 194, 205,\n",
       "        168, 171, 177, 148, 151, 151, 155, 157, 153, 182, 190, 196, 178,\n",
       "        166, 169, 149, 158, 153, 156, 159, 147, 184, 198, 183, 170, 176,\n",
       "        175, 160, 162, 160, 150, 146, 145, 194, 187, 187, 179, 173, 180,\n",
       "        165, 174, 172, 164, 167, 163, 126, 143, 142, 129,  91,  88,  83,\n",
       "        111,  91,  88,  10,  74, 136, 141, 144,  91,  97,  96,  41,  80,\n",
       "         77,  53,  71,  30, 138, 132, 140, 114, 120, 130,  54, 103, 106,\n",
       "         38,  47,  41, 138, 135, 136, 128, 118, 130,  71, 108,  69,  71,\n",
       "        106,  50]),\n",
       " 'split0_test_f1_micro': array([0.847, 0.843, 0.848, 0.854, 0.865, 0.856, 0.858, 0.862, 0.84 ,\n",
       "        0.866, 0.852, 0.847, 0.834, 0.85 , 0.852, 0.85 , 0.859, 0.863,\n",
       "        0.837, 0.849, 0.855, 0.854, 0.862, 0.85 , 0.85 , 0.857, 0.864,\n",
       "        0.86 , 0.863, 0.851, 0.84 , 0.856, 0.847, 0.84 , 0.848, 0.85 ,\n",
       "        0.856, 0.861, 0.859, 0.86 , 0.861, 0.868, 0.857, 0.86 , 0.855,\n",
       "        0.852, 0.86 , 0.859, 0.854, 0.847, 0.851, 0.854, 0.851, 0.846,\n",
       "        0.856, 0.836, 0.849, 0.848, 0.855, 0.847, 0.844, 0.862, 0.858,\n",
       "        0.866, 0.854, 0.845, 0.852, 0.874, 0.852, 0.853, 0.848, 0.854,\n",
       "        0.853, 0.86 , 0.852, 0.851, 0.851, 0.857, 0.843, 0.867, 0.847,\n",
       "        0.848, 0.866, 0.861, 0.855, 0.861, 0.859, 0.833, 0.844, 0.861,\n",
       "        0.852, 0.853, 0.856, 0.844, 0.853, 0.858, 0.764, 0.65 , 0.764,\n",
       "        0.764, 0.35 , 0.763, 0.769, 0.65 , 0.768, 0.768, 0.65 , 0.765,\n",
       "        0.761, 0.35 , 0.758, 0.768, 0.65 , 0.766, 0.767, 0.612, 0.763,\n",
       "        0.766, 0.379, 0.77 , 0.759, 0.405, 0.763, 0.765, 0.476, 0.765,\n",
       "        0.767, 0.337, 0.765, 0.77 , 0.65 , 0.768, 0.762, 0.65 , 0.765,\n",
       "        0.765, 0.65 , 0.766, 0.766, 0.65 , 0.767, 0.769, 0.68 , 0.767,\n",
       "        0.783, 0.376, 0.775, 0.801, 0.54 , 0.814, 0.801, 0.65 , 0.794,\n",
       "        0.805, 0.67 , 0.804, 0.784, 0.375, 0.784, 0.793, 0.381, 0.788,\n",
       "        0.802, 0.687, 0.791, 0.806, 0.618, 0.811, 0.78 , 0.604, 0.78 ,\n",
       "        0.815, 0.549, 0.787, 0.8  , 0.518, 0.797, 0.795, 0.437, 0.817,\n",
       "        0.778, 0.605, 0.786, 0.785, 0.575, 0.791, 0.807, 0.492, 0.791,\n",
       "        0.793, 0.628, 0.8  , 0.815, 0.795, 0.784, 0.816, 0.826, 0.824,\n",
       "        0.837, 0.839, 0.836, 0.835, 0.834, 0.831, 0.808, 0.819, 0.803,\n",
       "        0.816, 0.831, 0.823, 0.838, 0.84 , 0.84 , 0.83 , 0.828, 0.838,\n",
       "        0.807, 0.806, 0.818, 0.827, 0.821, 0.823, 0.832, 0.836, 0.831,\n",
       "        0.833, 0.839, 0.838, 0.813, 0.814, 0.801, 0.818, 0.821, 0.815,\n",
       "        0.828, 0.822, 0.821, 0.835, 0.829, 0.834, 0.848, 0.836, 0.845,\n",
       "        0.839, 0.846, 0.838, 0.838, 0.845, 0.843, 0.846, 0.856, 0.843,\n",
       "        0.844, 0.842, 0.836, 0.837, 0.853, 0.841, 0.85 , 0.842, 0.842,\n",
       "        0.843, 0.844, 0.851, 0.841, 0.843, 0.828, 0.841, 0.85 , 0.832,\n",
       "        0.844, 0.841, 0.849, 0.838, 0.849, 0.848, 0.837, 0.844, 0.837,\n",
       "        0.839, 0.843, 0.842, 0.841, 0.846, 0.847, 0.847, 0.838, 0.842]),\n",
       " 'split1_test_f1_micro': array([0.853, 0.851, 0.861, 0.858, 0.861, 0.866, 0.855, 0.853, 0.855,\n",
       "        0.848, 0.843, 0.87 , 0.853, 0.839, 0.842, 0.856, 0.858, 0.862,\n",
       "        0.847, 0.839, 0.857, 0.86 , 0.861, 0.862, 0.858, 0.853, 0.851,\n",
       "        0.851, 0.849, 0.842, 0.864, 0.858, 0.857, 0.858, 0.852, 0.857,\n",
       "        0.852, 0.849, 0.849, 0.845, 0.862, 0.844, 0.848, 0.862, 0.857,\n",
       "        0.849, 0.859, 0.859, 0.852, 0.851, 0.855, 0.86 , 0.854, 0.856,\n",
       "        0.853, 0.85 , 0.845, 0.864, 0.852, 0.846, 0.856, 0.855, 0.853,\n",
       "        0.852, 0.868, 0.861, 0.848, 0.845, 0.839, 0.851, 0.836, 0.846,\n",
       "        0.854, 0.852, 0.855, 0.859, 0.857, 0.85 , 0.856, 0.863, 0.847,\n",
       "        0.863, 0.85 , 0.858, 0.854, 0.851, 0.856, 0.854, 0.851, 0.846,\n",
       "        0.851, 0.842, 0.859, 0.854, 0.86 , 0.861, 0.775, 0.65 , 0.758,\n",
       "        0.772, 0.65 , 0.773, 0.77 , 0.65 , 0.772, 0.769, 0.326, 0.769,\n",
       "        0.779, 0.65 , 0.771, 0.777, 0.254, 0.774, 0.774, 0.65 , 0.772,\n",
       "        0.769, 0.65 , 0.771, 0.776, 0.441, 0.771, 0.771, 0.362, 0.772,\n",
       "        0.766, 0.35 , 0.775, 0.775, 0.619, 0.774, 0.773, 0.35 , 0.764,\n",
       "        0.771, 0.513, 0.778, 0.774, 0.493, 0.774, 0.776, 0.294, 0.771,\n",
       "        0.79 , 0.5  , 0.794, 0.832, 0.641, 0.819, 0.812, 0.613, 0.8  ,\n",
       "        0.825, 0.511, 0.823, 0.797, 0.486, 0.82 , 0.814, 0.651, 0.833,\n",
       "        0.798, 0.69 , 0.828, 0.822, 0.634, 0.827, 0.816, 0.64 , 0.809,\n",
       "        0.812, 0.504, 0.822, 0.8  , 0.532, 0.808, 0.813, 0.526, 0.81 ,\n",
       "        0.807, 0.32 , 0.795, 0.816, 0.752, 0.828, 0.829, 0.547, 0.808,\n",
       "        0.817, 0.649, 0.814, 0.809, 0.825, 0.824, 0.838, 0.831, 0.831,\n",
       "        0.836, 0.83 , 0.834, 0.835, 0.837, 0.837, 0.824, 0.806, 0.806,\n",
       "        0.829, 0.831, 0.835, 0.831, 0.832, 0.834, 0.84 , 0.835, 0.834,\n",
       "        0.803, 0.824, 0.826, 0.83 , 0.835, 0.832, 0.834, 0.832, 0.835,\n",
       "        0.837, 0.834, 0.839, 0.825, 0.825, 0.831, 0.831, 0.832, 0.828,\n",
       "        0.83 , 0.834, 0.833, 0.834, 0.831, 0.835, 0.842, 0.838, 0.833,\n",
       "        0.841, 0.853, 0.863, 0.854, 0.854, 0.856, 0.85 , 0.855, 0.852,\n",
       "        0.847, 0.838, 0.827, 0.855, 0.855, 0.856, 0.857, 0.86 , 0.856,\n",
       "        0.854, 0.859, 0.856, 0.848, 0.843, 0.85 , 0.85 , 0.849, 0.851,\n",
       "        0.858, 0.856, 0.852, 0.856, 0.857, 0.851, 0.837, 0.854, 0.836,\n",
       "        0.848, 0.846, 0.848, 0.856, 0.849, 0.854, 0.857, 0.858, 0.86 ]),\n",
       " 'split2_test_f1_micro': array([0.858, 0.85 , 0.85 , 0.862, 0.847, 0.855, 0.86 , 0.852, 0.859,\n",
       "        0.86 , 0.845, 0.851, 0.845, 0.85 , 0.857, 0.832, 0.848, 0.861,\n",
       "        0.854, 0.856, 0.854, 0.852, 0.858, 0.852, 0.852, 0.856, 0.834,\n",
       "        0.845, 0.859, 0.852, 0.853, 0.852, 0.852, 0.859, 0.837, 0.853,\n",
       "        0.858, 0.85 , 0.846, 0.849, 0.857, 0.841, 0.859, 0.851, 0.859,\n",
       "        0.854, 0.851, 0.858, 0.85 , 0.84 , 0.86 , 0.853, 0.835, 0.861,\n",
       "        0.856, 0.842, 0.865, 0.859, 0.852, 0.85 , 0.852, 0.861, 0.845,\n",
       "        0.843, 0.856, 0.85 , 0.846, 0.854, 0.851, 0.843, 0.853, 0.858,\n",
       "        0.852, 0.857, 0.845, 0.862, 0.842, 0.855, 0.846, 0.857, 0.859,\n",
       "        0.856, 0.846, 0.857, 0.839, 0.848, 0.858, 0.853, 0.854, 0.849,\n",
       "        0.853, 0.859, 0.856, 0.85 , 0.851, 0.846, 0.764, 0.726, 0.746,\n",
       "        0.764, 0.317, 0.764, 0.761, 0.651, 0.771, 0.764, 0.334, 0.766,\n",
       "        0.764, 0.349, 0.761, 0.766, 0.622, 0.764, 0.76 , 0.65 , 0.761,\n",
       "        0.764, 0.309, 0.759, 0.757, 0.633, 0.749, 0.755, 0.657, 0.761,\n",
       "        0.769, 0.65 , 0.766, 0.764, 0.65 , 0.762, 0.754, 0.464, 0.764,\n",
       "        0.762, 0.344, 0.762, 0.769, 0.646, 0.764, 0.763, 0.65 , 0.765,\n",
       "        0.796, 0.592, 0.784, 0.815, 0.663, 0.796, 0.798, 0.701, 0.783,\n",
       "        0.8  , 0.509, 0.79 , 0.784, 0.63 , 0.804, 0.817, 0.388, 0.784,\n",
       "        0.8  , 0.555, 0.799, 0.807, 0.394, 0.794, 0.783, 0.385, 0.775,\n",
       "        0.798, 0.57 , 0.781, 0.796, 0.653, 0.806, 0.809, 0.533, 0.792,\n",
       "        0.805, 0.575, 0.801, 0.791, 0.637, 0.809, 0.811, 0.441, 0.806,\n",
       "        0.798, 0.683, 0.804, 0.833, 0.783, 0.803, 0.828, 0.827, 0.817,\n",
       "        0.832, 0.833, 0.832, 0.833, 0.832, 0.836, 0.813, 0.828, 0.807,\n",
       "        0.819, 0.826, 0.834, 0.829, 0.832, 0.834, 0.835, 0.834, 0.835,\n",
       "        0.818, 0.784, 0.788, 0.832, 0.823, 0.825, 0.826, 0.829, 0.833,\n",
       "        0.832, 0.839, 0.835, 0.789, 0.802, 0.812, 0.822, 0.831, 0.821,\n",
       "        0.834, 0.826, 0.828, 0.832, 0.83 , 0.829, 0.842, 0.839, 0.834,\n",
       "        0.847, 0.841, 0.849, 0.85 , 0.843, 0.847, 0.854, 0.851, 0.853,\n",
       "        0.841, 0.828, 0.841, 0.854, 0.841, 0.853, 0.849, 0.849, 0.843,\n",
       "        0.849, 0.848, 0.855, 0.846, 0.849, 0.835, 0.853, 0.847, 0.848,\n",
       "        0.849, 0.853, 0.85 , 0.854, 0.851, 0.849, 0.836, 0.854, 0.85 ,\n",
       "        0.852, 0.845, 0.849, 0.86 , 0.847, 0.851, 0.851, 0.854, 0.85 ]),\n",
       " 'split3_test_f1_micro': array([0.824, 0.829, 0.837, 0.842, 0.841, 0.832, 0.84 , 0.843, 0.834,\n",
       "        0.843, 0.842, 0.846, 0.834, 0.835, 0.83 , 0.827, 0.839, 0.854,\n",
       "        0.846, 0.84 , 0.848, 0.839, 0.834, 0.84 , 0.846, 0.83 , 0.827,\n",
       "        0.833, 0.842, 0.839, 0.826, 0.833, 0.838, 0.846, 0.837, 0.84 ,\n",
       "        0.831, 0.839, 0.831, 0.845, 0.835, 0.835, 0.837, 0.838, 0.841,\n",
       "        0.838, 0.855, 0.841, 0.84 , 0.828, 0.839, 0.838, 0.843, 0.841,\n",
       "        0.835, 0.835, 0.846, 0.841, 0.841, 0.84 , 0.824, 0.839, 0.843,\n",
       "        0.836, 0.839, 0.836, 0.835, 0.832, 0.852, 0.84 , 0.84 , 0.841,\n",
       "        0.84 , 0.829, 0.833, 0.828, 0.835, 0.843, 0.837, 0.838, 0.84 ,\n",
       "        0.827, 0.847, 0.846, 0.844, 0.831, 0.832, 0.832, 0.838, 0.835,\n",
       "        0.85 , 0.84 , 0.835, 0.842, 0.838, 0.843, 0.762, 0.649, 0.768,\n",
       "        0.763, 0.717, 0.763, 0.77 , 0.649, 0.776, 0.774, 0.648, 0.777,\n",
       "        0.765, 0.649, 0.769, 0.773, 0.495, 0.776, 0.767, 0.629, 0.762,\n",
       "        0.773, 0.341, 0.776, 0.764, 0.656, 0.766, 0.779, 0.47 , 0.77 ,\n",
       "        0.772, 0.344, 0.771, 0.77 , 0.649, 0.774, 0.773, 0.689, 0.772,\n",
       "        0.775, 0.656, 0.774, 0.765, 0.649, 0.769, 0.769, 0.351, 0.77 ,\n",
       "        0.794, 0.592, 0.791, 0.807, 0.593, 0.794, 0.796, 0.48 , 0.788,\n",
       "        0.792, 0.621, 0.795, 0.789, 0.457, 0.796, 0.802, 0.672, 0.789,\n",
       "        0.803, 0.369, 0.789, 0.796, 0.69 , 0.804, 0.796, 0.554, 0.789,\n",
       "        0.81 , 0.301, 0.796, 0.805, 0.449, 0.787, 0.799, 0.499, 0.794,\n",
       "        0.782, 0.659, 0.793, 0.801, 0.642, 0.786, 0.802, 0.482, 0.787,\n",
       "        0.797, 0.355, 0.797, 0.798, 0.808, 0.809, 0.816, 0.815, 0.815,\n",
       "        0.816, 0.822, 0.817, 0.815, 0.817, 0.818, 0.809, 0.791, 0.788,\n",
       "        0.815, 0.817, 0.809, 0.82 , 0.812, 0.814, 0.816, 0.816, 0.817,\n",
       "        0.811, 0.798, 0.804, 0.808, 0.807, 0.807, 0.821, 0.814, 0.813,\n",
       "        0.819, 0.817, 0.818, 0.793, 0.798, 0.791, 0.808, 0.809, 0.814,\n",
       "        0.812, 0.807, 0.814, 0.802, 0.808, 0.812, 0.834, 0.821, 0.827,\n",
       "        0.837, 0.84 , 0.837, 0.845, 0.834, 0.842, 0.835, 0.851, 0.843,\n",
       "        0.83 , 0.834, 0.829, 0.839, 0.835, 0.837, 0.837, 0.836, 0.842,\n",
       "        0.846, 0.84 , 0.836, 0.828, 0.833, 0.833, 0.832, 0.831, 0.83 ,\n",
       "        0.838, 0.828, 0.833, 0.848, 0.844, 0.841, 0.837, 0.808, 0.831,\n",
       "        0.832, 0.835, 0.834, 0.836, 0.837, 0.836, 0.839, 0.833, 0.84 ]),\n",
       " 'split4_test_f1_micro': array([0.857, 0.85 , 0.851, 0.853, 0.863, 0.857, 0.851, 0.857, 0.862,\n",
       "        0.844, 0.856, 0.855, 0.855, 0.846, 0.854, 0.847, 0.853, 0.842,\n",
       "        0.858, 0.849, 0.841, 0.845, 0.852, 0.851, 0.855, 0.844, 0.846,\n",
       "        0.864, 0.858, 0.857, 0.851, 0.852, 0.851, 0.846, 0.852, 0.833,\n",
       "        0.86 , 0.867, 0.85 , 0.863, 0.847, 0.85 , 0.857, 0.865, 0.856,\n",
       "        0.84 , 0.854, 0.848, 0.859, 0.859, 0.855, 0.858, 0.86 , 0.855,\n",
       "        0.842, 0.849, 0.844, 0.845, 0.844, 0.845, 0.853, 0.855, 0.862,\n",
       "        0.864, 0.851, 0.845, 0.85 , 0.849, 0.854, 0.831, 0.843, 0.836,\n",
       "        0.845, 0.85 , 0.852, 0.855, 0.856, 0.853, 0.846, 0.853, 0.836,\n",
       "        0.855, 0.847, 0.843, 0.848, 0.848, 0.848, 0.859, 0.861, 0.859,\n",
       "        0.85 , 0.86 , 0.858, 0.848, 0.859, 0.857, 0.776, 0.553, 0.779,\n",
       "        0.781, 0.656, 0.781, 0.788, 0.351, 0.784, 0.783, 0.649, 0.781,\n",
       "        0.784, 0.649, 0.783, 0.785, 0.65 , 0.783, 0.787, 0.301, 0.787,\n",
       "        0.786, 0.649, 0.786, 0.786, 0.699, 0.784, 0.784, 0.337, 0.785,\n",
       "        0.785, 0.351, 0.782, 0.786, 0.649, 0.788, 0.776, 0.351, 0.785,\n",
       "        0.788, 0.343, 0.782, 0.784, 0.293, 0.785, 0.786, 0.649, 0.787,\n",
       "        0.81 , 0.309, 0.806, 0.812, 0.639, 0.809, 0.822, 0.425, 0.807,\n",
       "        0.813, 0.668, 0.815, 0.804, 0.389, 0.793, 0.823, 0.574, 0.821,\n",
       "        0.812, 0.602, 0.817, 0.814, 0.483, 0.825, 0.813, 0.416, 0.794,\n",
       "        0.815, 0.525, 0.807, 0.823, 0.547, 0.822, 0.814, 0.649, 0.815,\n",
       "        0.802, 0.526, 0.808, 0.811, 0.609, 0.812, 0.817, 0.652, 0.796,\n",
       "        0.808, 0.423, 0.806, 0.822, 0.832, 0.804, 0.838, 0.832, 0.836,\n",
       "        0.84 , 0.833, 0.838, 0.837, 0.833, 0.834, 0.822, 0.809, 0.835,\n",
       "        0.83 , 0.833, 0.833, 0.84 , 0.836, 0.834, 0.833, 0.838, 0.838,\n",
       "        0.828, 0.822, 0.837, 0.835, 0.837, 0.837, 0.837, 0.837, 0.838,\n",
       "        0.837, 0.835, 0.835, 0.823, 0.822, 0.826, 0.826, 0.833, 0.822,\n",
       "        0.836, 0.837, 0.834, 0.838, 0.838, 0.835, 0.852, 0.846, 0.843,\n",
       "        0.852, 0.857, 0.851, 0.852, 0.854, 0.849, 0.853, 0.855, 0.851,\n",
       "        0.842, 0.846, 0.84 , 0.852, 0.852, 0.85 , 0.862, 0.853, 0.858,\n",
       "        0.86 , 0.852, 0.862, 0.833, 0.846, 0.845, 0.853, 0.847, 0.854,\n",
       "        0.862, 0.855, 0.848, 0.86 , 0.853, 0.866, 0.849, 0.848, 0.85 ,\n",
       "        0.846, 0.856, 0.842, 0.85 , 0.853, 0.855, 0.849, 0.849, 0.861]),\n",
       " 'mean_test_f1_micro': array([0.8478, 0.8446, 0.8494, 0.8538, 0.8554, 0.8532, 0.8528, 0.8534,\n",
       "        0.85  , 0.8522, 0.8476, 0.8538, 0.8442, 0.844 , 0.847 , 0.8424,\n",
       "        0.8514, 0.8564, 0.8484, 0.8466, 0.851 , 0.85  , 0.8534, 0.851 ,\n",
       "        0.8522, 0.848 , 0.8444, 0.8506, 0.8542, 0.8482, 0.8468, 0.8502,\n",
       "        0.849 , 0.8498, 0.8452, 0.8466, 0.8514, 0.8532, 0.847 , 0.8524,\n",
       "        0.8524, 0.8476, 0.8516, 0.8552, 0.8536, 0.8466, 0.8558, 0.853 ,\n",
       "        0.851 , 0.845 , 0.852 , 0.8526, 0.8486, 0.8518, 0.8484, 0.8424,\n",
       "        0.8498, 0.8514, 0.8488, 0.8456, 0.8458, 0.8544, 0.8522, 0.8522,\n",
       "        0.8536, 0.8474, 0.8462, 0.8508, 0.8496, 0.8436, 0.844 , 0.847 ,\n",
       "        0.8488, 0.8496, 0.8474, 0.851 , 0.8482, 0.8516, 0.8456, 0.8556,\n",
       "        0.8458, 0.8498, 0.8512, 0.853 , 0.848 , 0.8478, 0.8506, 0.8462,\n",
       "        0.8496, 0.85  , 0.8512, 0.8508, 0.8528, 0.8476, 0.8522, 0.853 ,\n",
       "        0.7682, 0.6456, 0.763 , 0.7688, 0.538 , 0.7688, 0.7716, 0.5902,\n",
       "        0.7742, 0.7716, 0.5214, 0.7716, 0.7706, 0.5294, 0.7684, 0.7738,\n",
       "        0.5342, 0.7726, 0.771 , 0.5684, 0.769 , 0.7716, 0.4656, 0.7724,\n",
       "        0.7684, 0.5668, 0.7666, 0.7708, 0.4604, 0.7706, 0.7718, 0.4064,\n",
       "        0.7718, 0.773 , 0.6434, 0.7732, 0.7676, 0.5008, 0.77  , 0.7722,\n",
       "        0.5012, 0.7724, 0.7716, 0.5462, 0.7718, 0.7726, 0.5248, 0.772 ,\n",
       "        0.7946, 0.4738, 0.79  , 0.8134, 0.6152, 0.8064, 0.8058, 0.5738,\n",
       "        0.7944, 0.807 , 0.5958, 0.8054, 0.7916, 0.4674, 0.7994, 0.8098,\n",
       "        0.5332, 0.803 , 0.803 , 0.5806, 0.8048, 0.809 , 0.5638, 0.8122,\n",
       "        0.7976, 0.5198, 0.7894, 0.81  , 0.4898, 0.7986, 0.8048, 0.5398,\n",
       "        0.804 , 0.806 , 0.5288, 0.8056, 0.7948, 0.537 , 0.7966, 0.8008,\n",
       "        0.643 , 0.8052, 0.8132, 0.5228, 0.7976, 0.8026, 0.5476, 0.8042,\n",
       "        0.8154, 0.8086, 0.8048, 0.8272, 0.8262, 0.8246, 0.8322, 0.8314,\n",
       "        0.8314, 0.831 , 0.8306, 0.8312, 0.8152, 0.8106, 0.8078, 0.8218,\n",
       "        0.8276, 0.8268, 0.8316, 0.8304, 0.8312, 0.8308, 0.8302, 0.8324,\n",
       "        0.8134, 0.8068, 0.8146, 0.8264, 0.8246, 0.8248, 0.83  , 0.8296,\n",
       "        0.83  , 0.8316, 0.8328, 0.833 , 0.8086, 0.8122, 0.8122, 0.821 ,\n",
       "        0.8252, 0.82  , 0.828 , 0.8252, 0.826 , 0.8282, 0.8272, 0.829 ,\n",
       "        0.8436, 0.836 , 0.8364, 0.8432, 0.8474, 0.8476, 0.8478, 0.846 ,\n",
       "        0.8474, 0.8476, 0.8536, 0.8484, 0.8408, 0.8376, 0.8346, 0.8474,\n",
       "        0.8472, 0.8474, 0.851 , 0.848 , 0.8482, 0.8504, 0.8486, 0.852 ,\n",
       "        0.8392, 0.8428, 0.8382, 0.8458, 0.8448, 0.843 , 0.8502, 0.8466,\n",
       "        0.8464, 0.8512, 0.8508, 0.851 , 0.8392, 0.8416, 0.8408, 0.8434,\n",
       "        0.845 , 0.843 , 0.8486, 0.8464, 0.8486, 0.8486, 0.8464, 0.8506]),\n",
       " 'std_test_f1_micro': array([0.01251239, 0.00830903, 0.00765768, 0.00670522, 0.00958332,\n",
       "        0.0113031 , 0.00708237, 0.00628013, 0.01100909, 0.00917388,\n",
       "        0.0054626 , 0.00870402, 0.00897552, 0.00603324, 0.00987927,\n",
       "        0.01103812, 0.00733757, 0.00786384, 0.00722772, 0.0063435 ,\n",
       "        0.00583095, 0.00729383, 0.01030728, 0.0069857 , 0.00411825,\n",
       "        0.0100995 , 0.01297074, 0.01103812, 0.00762627, 0.00667533,\n",
       "        0.01289031, 0.00890842, 0.0063561 , 0.00744043, 0.00685274,\n",
       "        0.0088227 , 0.01053755, 0.00980612, 0.00909945, 0.00763151,\n",
       "        0.01019019, 0.01128893, 0.0082365 , 0.0097857 , 0.00643739,\n",
       "        0.00643739, 0.00331059, 0.00729383, 0.00626099, 0.01048809,\n",
       "        0.0070993 , 0.00773563, 0.00873155, 0.00724983, 0.00845222,\n",
       "        0.00628013, 0.00778203, 0.00868562, 0.00534416, 0.0032619 ,\n",
       "        0.0116    , 0.0082365 , 0.00730479, 0.01163443, 0.00930806,\n",
       "        0.00816333, 0.00594643, 0.01370255, 0.00538888, 0.00793977,\n",
       "        0.00596657, 0.00809938, 0.00541849, 0.0108922 , 0.00791454,\n",
       "        0.01208305, 0.00847113, 0.00488262, 0.00615142, 0.01003195,\n",
       "        0.00783326, 0.01235152, 0.00752064, 0.00712741, 0.00603324,\n",
       "        0.0096623 , 0.01007174, 0.01137365, 0.00796492, 0.00942338,\n",
       "        0.00116619, 0.00837616, 0.00897552, 0.00427083, 0.00788416,\n",
       "        0.00712741, 0.00601332, 0.05493487, 0.01091788, 0.00691086,\n",
       "        0.16893431, 0.00716659, 0.00886792, 0.11960167, 0.00552811,\n",
       "        0.00652993, 0.1562992 , 0.00631189, 0.00913455, 0.14688853,\n",
       "        0.00875443, 0.00679412, 0.1513782 , 0.00691665, 0.0091433 ,\n",
       "        0.13445683, 0.00981835, 0.00781281, 0.15178089, 0.00877724,\n",
       "        0.01100182, 0.11985057, 0.01135958, 0.010245  , 0.11304619,\n",
       "        0.00816333, 0.00691086, 0.12190258, 0.00624179, 0.00737564,\n",
       "        0.01220819, 0.00863481, 0.00830903, 0.14437091, 0.00807465,\n",
       "        0.00910824, 0.13855165, 0.00741889, 0.0069455 , 0.14017332,\n",
       "        0.00735935, 0.00786384, 0.16653096, 0.00779744, 0.00889044,\n",
       "        0.11432655, 0.01033441, 0.01044222, 0.04396544, 0.00985089,\n",
       "        0.00980612, 0.10436743, 0.00849941, 0.01129602, 0.07222022,\n",
       "        0.01224091, 0.00781281, 0.09118903, 0.01212601, 0.01083328,\n",
       "        0.12574164, 0.02002998, 0.00481664, 0.11762415, 0.0152368 ,\n",
       "        0.00867179, 0.10876838, 0.01251239, 0.01484049, 0.10163936,\n",
       "        0.0118423 , 0.00629285, 0.09697711, 0.01462327, 0.0095373 ,\n",
       "        0.06579787, 0.01167904, 0.00764199, 0.06897652, 0.01055651,\n",
       "        0.01225398, 0.11675787, 0.0074458 , 0.01166876, 0.05949454,\n",
       "        0.01517103, 0.00930376, 0.07291474, 0.00821219, 0.00873155,\n",
       "        0.13243806, 0.00581034, 0.01180847, 0.01820549, 0.01282809,\n",
       "        0.00984683, 0.00604649, 0.00801499, 0.0084947 , 0.00553534,\n",
       "        0.00747262, 0.00809938, 0.00700286, 0.00691086, 0.00661513,\n",
       "        0.0124996 , 0.01522367, 0.00643117, 0.00578273, 0.00988737,\n",
       "        0.00711618, 0.00966644, 0.00890842, 0.00808455, 0.00780769,\n",
       "        0.00786384, 0.0088227 , 0.01499867, 0.01710672, 0.00956243,\n",
       "        0.01083697, 0.01020588, 0.00576194, 0.00830903, 0.00880909,\n",
       "        0.00662118, 0.00815843, 0.00766812, 0.014988  , 0.01066583,\n",
       "        0.01495861, 0.00779744, 0.00917388, 0.00509902, 0.00848528,\n",
       "        0.01057166, 0.00756307, 0.01324236, 0.01010742, 0.00878635,\n",
       "        0.00611882, 0.00822192, 0.00668132, 0.00552811, 0.00665132,\n",
       "        0.00954149, 0.00574108, 0.00750999, 0.005004  , 0.00688767,\n",
       "        0.00215407, 0.00445421, 0.00577581, 0.0062482 , 0.00567803,\n",
       "        0.00776144, 0.00780769, 0.00722772, 0.00846168, 0.0083666 ,\n",
       "        0.00722219, 0.00601997, 0.00656049, 0.00874071, 0.00762627,\n",
       "        0.00538145, 0.00808455, 0.00818291, 0.00699714, 0.01      ,\n",
       "        0.00881816, 0.0107443 , 0.00682935, 0.00765245, 0.00430813,\n",
       "        0.00822192, 0.00491528, 0.01722324, 0.00778203, 0.00708802,\n",
       "        0.00672309, 0.00536656, 0.00897998, 0.00527636, 0.00688767,\n",
       "        0.0058515 , 0.0094784 , 0.00875443]),\n",
       " 'rank_test_f1_micro': array([ 83, 121,  65,   8,   4,  15,  20,  14,  56,  25,  86,   8, 123,\n",
       "        124,  98, 133,  35,   1,  74, 103,  41,  56,  13,  46,  29,  80,\n",
       "        122,  50,   7,  77, 101,  54,  66,  60, 117, 103,  35,  15, 100,\n",
       "         23,  23,  88,  33,   5,  10, 102,   2,  17,  41, 118,  30,  22,\n",
       "         69,  32,  74, 133,  59,  35,  67, 115, 112,   6,  25,  25,  10,\n",
       "         91, 109,  47,  63, 126, 124,  98,  67,  63,  91,  41,  77,  33,\n",
       "        115,   3, 112,  60,  38,  17,  80,  83,  50, 109,  62,  56,  38,\n",
       "         47,  20,  86,  25,  17, 253, 257, 256, 249, 271, 249, 242, 262,\n",
       "        225, 238, 279, 238, 245, 275, 251, 226, 273, 229, 243, 265, 248,\n",
       "        238, 286, 231, 251, 266, 255, 244, 287, 246, 235, 288, 235, 228,\n",
       "        258, 227, 254, 282, 247, 233, 281, 231, 238, 269, 235, 229, 277,\n",
       "        234, 220, 284, 223, 184, 260, 199, 201, 264, 221, 197, 261, 203,\n",
       "        222, 285, 214, 192, 274, 210, 210, 263, 206, 193, 267, 187, 217,\n",
       "        280, 224, 191, 283, 215, 205, 270, 209, 200, 276, 202, 219, 272,\n",
       "        218, 213, 259, 204, 186, 278, 216, 212, 268, 208, 181, 194, 206,\n",
       "        168, 171, 177, 148, 151, 151, 155, 157, 153, 182, 190, 196, 178,\n",
       "        166, 169, 149, 158, 153, 156, 159, 147, 184, 198, 183, 170, 176,\n",
       "        175, 160, 162, 160, 149, 146, 145, 194, 187, 187, 179, 173, 180,\n",
       "        165, 174, 172, 164, 167, 163, 126, 143, 142, 129,  91,  88,  83,\n",
       "        111,  91,  88,  10,  74, 136, 141, 144,  91,  97,  91,  41,  80,\n",
       "         77,  53,  71,  30, 139, 132, 140, 114, 120, 130,  54, 103, 106,\n",
       "         38,  47,  41, 138, 135, 136, 128, 118, 130,  71, 106,  69,  71,\n",
       "        106,  50]),\n",
       " 'split0_test_precision_micro': array([0.847, 0.843, 0.848, 0.854, 0.865, 0.856, 0.858, 0.862, 0.84 ,\n",
       "        0.866, 0.852, 0.847, 0.834, 0.85 , 0.852, 0.85 , 0.859, 0.863,\n",
       "        0.837, 0.849, 0.855, 0.854, 0.862, 0.85 , 0.85 , 0.857, 0.864,\n",
       "        0.86 , 0.863, 0.851, 0.84 , 0.856, 0.847, 0.84 , 0.848, 0.85 ,\n",
       "        0.856, 0.861, 0.859, 0.86 , 0.861, 0.868, 0.857, 0.86 , 0.855,\n",
       "        0.852, 0.86 , 0.859, 0.854, 0.847, 0.851, 0.854, 0.851, 0.846,\n",
       "        0.856, 0.836, 0.849, 0.848, 0.855, 0.847, 0.844, 0.862, 0.858,\n",
       "        0.866, 0.854, 0.845, 0.852, 0.874, 0.852, 0.853, 0.848, 0.854,\n",
       "        0.853, 0.86 , 0.852, 0.851, 0.851, 0.857, 0.843, 0.867, 0.847,\n",
       "        0.848, 0.866, 0.861, 0.855, 0.861, 0.859, 0.833, 0.844, 0.861,\n",
       "        0.852, 0.853, 0.856, 0.844, 0.853, 0.858, 0.764, 0.65 , 0.764,\n",
       "        0.764, 0.35 , 0.763, 0.769, 0.65 , 0.768, 0.768, 0.65 , 0.765,\n",
       "        0.761, 0.35 , 0.758, 0.768, 0.65 , 0.766, 0.767, 0.612, 0.763,\n",
       "        0.766, 0.379, 0.77 , 0.759, 0.405, 0.763, 0.765, 0.476, 0.765,\n",
       "        0.767, 0.337, 0.765, 0.77 , 0.65 , 0.768, 0.762, 0.65 , 0.765,\n",
       "        0.765, 0.65 , 0.766, 0.766, 0.65 , 0.767, 0.769, 0.68 , 0.767,\n",
       "        0.783, 0.376, 0.775, 0.801, 0.54 , 0.814, 0.801, 0.65 , 0.794,\n",
       "        0.805, 0.67 , 0.804, 0.784, 0.375, 0.784, 0.793, 0.381, 0.788,\n",
       "        0.802, 0.687, 0.791, 0.806, 0.618, 0.811, 0.78 , 0.604, 0.78 ,\n",
       "        0.815, 0.549, 0.787, 0.8  , 0.518, 0.797, 0.795, 0.437, 0.817,\n",
       "        0.778, 0.605, 0.786, 0.785, 0.575, 0.791, 0.807, 0.492, 0.791,\n",
       "        0.793, 0.628, 0.8  , 0.815, 0.795, 0.784, 0.816, 0.826, 0.824,\n",
       "        0.837, 0.839, 0.836, 0.835, 0.834, 0.831, 0.808, 0.819, 0.803,\n",
       "        0.816, 0.831, 0.823, 0.838, 0.84 , 0.84 , 0.83 , 0.828, 0.838,\n",
       "        0.807, 0.806, 0.818, 0.827, 0.821, 0.823, 0.832, 0.836, 0.831,\n",
       "        0.833, 0.839, 0.838, 0.813, 0.814, 0.801, 0.818, 0.821, 0.815,\n",
       "        0.828, 0.822, 0.821, 0.835, 0.829, 0.834, 0.848, 0.836, 0.845,\n",
       "        0.839, 0.846, 0.838, 0.838, 0.845, 0.843, 0.846, 0.856, 0.843,\n",
       "        0.844, 0.842, 0.836, 0.837, 0.853, 0.841, 0.85 , 0.842, 0.842,\n",
       "        0.843, 0.844, 0.851, 0.841, 0.843, 0.828, 0.841, 0.85 , 0.832,\n",
       "        0.844, 0.841, 0.849, 0.838, 0.849, 0.848, 0.837, 0.844, 0.837,\n",
       "        0.839, 0.843, 0.842, 0.841, 0.846, 0.847, 0.847, 0.838, 0.842]),\n",
       " 'split1_test_precision_micro': array([0.853, 0.851, 0.861, 0.858, 0.861, 0.866, 0.855, 0.853, 0.855,\n",
       "        0.848, 0.843, 0.87 , 0.853, 0.839, 0.842, 0.856, 0.858, 0.862,\n",
       "        0.847, 0.839, 0.857, 0.86 , 0.861, 0.862, 0.858, 0.853, 0.851,\n",
       "        0.851, 0.849, 0.842, 0.864, 0.858, 0.857, 0.858, 0.852, 0.857,\n",
       "        0.852, 0.849, 0.849, 0.845, 0.862, 0.844, 0.848, 0.862, 0.857,\n",
       "        0.849, 0.859, 0.859, 0.852, 0.851, 0.855, 0.86 , 0.854, 0.856,\n",
       "        0.853, 0.85 , 0.845, 0.864, 0.852, 0.846, 0.856, 0.855, 0.853,\n",
       "        0.852, 0.868, 0.861, 0.848, 0.845, 0.839, 0.851, 0.836, 0.846,\n",
       "        0.854, 0.852, 0.855, 0.859, 0.857, 0.85 , 0.856, 0.863, 0.847,\n",
       "        0.863, 0.85 , 0.858, 0.854, 0.851, 0.856, 0.854, 0.851, 0.846,\n",
       "        0.851, 0.842, 0.859, 0.854, 0.86 , 0.861, 0.775, 0.65 , 0.758,\n",
       "        0.772, 0.65 , 0.773, 0.77 , 0.65 , 0.772, 0.769, 0.326, 0.769,\n",
       "        0.779, 0.65 , 0.771, 0.777, 0.254, 0.774, 0.774, 0.65 , 0.772,\n",
       "        0.769, 0.65 , 0.771, 0.776, 0.441, 0.771, 0.771, 0.362, 0.772,\n",
       "        0.766, 0.35 , 0.775, 0.775, 0.619, 0.774, 0.773, 0.35 , 0.764,\n",
       "        0.771, 0.513, 0.778, 0.774, 0.493, 0.774, 0.776, 0.294, 0.771,\n",
       "        0.79 , 0.5  , 0.794, 0.832, 0.641, 0.819, 0.812, 0.613, 0.8  ,\n",
       "        0.825, 0.511, 0.823, 0.797, 0.486, 0.82 , 0.814, 0.651, 0.833,\n",
       "        0.798, 0.69 , 0.828, 0.822, 0.634, 0.827, 0.816, 0.64 , 0.809,\n",
       "        0.812, 0.504, 0.822, 0.8  , 0.532, 0.808, 0.813, 0.526, 0.81 ,\n",
       "        0.807, 0.32 , 0.795, 0.816, 0.752, 0.828, 0.829, 0.547, 0.808,\n",
       "        0.817, 0.649, 0.814, 0.809, 0.825, 0.824, 0.838, 0.831, 0.831,\n",
       "        0.836, 0.83 , 0.834, 0.835, 0.837, 0.837, 0.824, 0.806, 0.806,\n",
       "        0.829, 0.831, 0.835, 0.831, 0.832, 0.834, 0.84 , 0.835, 0.834,\n",
       "        0.803, 0.824, 0.826, 0.83 , 0.835, 0.832, 0.834, 0.832, 0.835,\n",
       "        0.837, 0.834, 0.839, 0.825, 0.825, 0.831, 0.831, 0.832, 0.828,\n",
       "        0.83 , 0.834, 0.833, 0.834, 0.831, 0.835, 0.842, 0.838, 0.833,\n",
       "        0.841, 0.853, 0.863, 0.854, 0.854, 0.856, 0.85 , 0.855, 0.852,\n",
       "        0.847, 0.838, 0.827, 0.855, 0.855, 0.856, 0.857, 0.86 , 0.856,\n",
       "        0.854, 0.859, 0.856, 0.848, 0.843, 0.85 , 0.85 , 0.849, 0.851,\n",
       "        0.858, 0.856, 0.852, 0.856, 0.857, 0.851, 0.837, 0.854, 0.836,\n",
       "        0.848, 0.846, 0.848, 0.856, 0.849, 0.854, 0.857, 0.858, 0.86 ]),\n",
       " 'split2_test_precision_micro': array([0.858, 0.85 , 0.85 , 0.862, 0.847, 0.855, 0.86 , 0.852, 0.859,\n",
       "        0.86 , 0.845, 0.851, 0.845, 0.85 , 0.857, 0.832, 0.848, 0.861,\n",
       "        0.854, 0.856, 0.854, 0.852, 0.858, 0.852, 0.852, 0.856, 0.834,\n",
       "        0.845, 0.859, 0.852, 0.853, 0.852, 0.852, 0.859, 0.837, 0.853,\n",
       "        0.858, 0.85 , 0.846, 0.849, 0.857, 0.841, 0.859, 0.851, 0.859,\n",
       "        0.854, 0.851, 0.858, 0.85 , 0.84 , 0.86 , 0.853, 0.835, 0.861,\n",
       "        0.856, 0.842, 0.865, 0.859, 0.852, 0.85 , 0.852, 0.861, 0.845,\n",
       "        0.843, 0.856, 0.85 , 0.846, 0.854, 0.851, 0.843, 0.853, 0.858,\n",
       "        0.852, 0.857, 0.845, 0.862, 0.842, 0.855, 0.846, 0.857, 0.859,\n",
       "        0.856, 0.846, 0.857, 0.839, 0.848, 0.858, 0.853, 0.854, 0.849,\n",
       "        0.853, 0.859, 0.856, 0.85 , 0.851, 0.846, 0.764, 0.726, 0.746,\n",
       "        0.764, 0.317, 0.764, 0.761, 0.651, 0.771, 0.764, 0.334, 0.766,\n",
       "        0.764, 0.349, 0.761, 0.766, 0.622, 0.764, 0.76 , 0.65 , 0.761,\n",
       "        0.764, 0.309, 0.759, 0.757, 0.633, 0.749, 0.755, 0.657, 0.761,\n",
       "        0.769, 0.65 , 0.766, 0.764, 0.65 , 0.762, 0.754, 0.464, 0.764,\n",
       "        0.762, 0.344, 0.762, 0.769, 0.646, 0.764, 0.763, 0.65 , 0.765,\n",
       "        0.796, 0.592, 0.784, 0.815, 0.663, 0.796, 0.798, 0.701, 0.783,\n",
       "        0.8  , 0.509, 0.79 , 0.784, 0.63 , 0.804, 0.817, 0.388, 0.784,\n",
       "        0.8  , 0.555, 0.799, 0.807, 0.394, 0.794, 0.783, 0.385, 0.775,\n",
       "        0.798, 0.57 , 0.781, 0.796, 0.653, 0.806, 0.809, 0.533, 0.792,\n",
       "        0.805, 0.575, 0.801, 0.791, 0.637, 0.809, 0.811, 0.441, 0.806,\n",
       "        0.798, 0.683, 0.804, 0.833, 0.783, 0.803, 0.828, 0.827, 0.817,\n",
       "        0.832, 0.833, 0.832, 0.833, 0.832, 0.836, 0.813, 0.828, 0.807,\n",
       "        0.819, 0.826, 0.834, 0.829, 0.832, 0.834, 0.835, 0.834, 0.835,\n",
       "        0.818, 0.784, 0.788, 0.832, 0.823, 0.825, 0.826, 0.829, 0.833,\n",
       "        0.832, 0.839, 0.835, 0.789, 0.802, 0.812, 0.822, 0.831, 0.821,\n",
       "        0.834, 0.826, 0.828, 0.832, 0.83 , 0.829, 0.842, 0.839, 0.834,\n",
       "        0.847, 0.841, 0.849, 0.85 , 0.843, 0.847, 0.854, 0.851, 0.853,\n",
       "        0.841, 0.828, 0.841, 0.854, 0.841, 0.853, 0.849, 0.849, 0.843,\n",
       "        0.849, 0.848, 0.855, 0.846, 0.849, 0.835, 0.853, 0.847, 0.848,\n",
       "        0.849, 0.853, 0.85 , 0.854, 0.851, 0.849, 0.836, 0.854, 0.85 ,\n",
       "        0.852, 0.845, 0.849, 0.86 , 0.847, 0.851, 0.851, 0.854, 0.85 ]),\n",
       " 'split3_test_precision_micro': array([0.824, 0.829, 0.837, 0.842, 0.841, 0.832, 0.84 , 0.843, 0.834,\n",
       "        0.843, 0.842, 0.846, 0.834, 0.835, 0.83 , 0.827, 0.839, 0.854,\n",
       "        0.846, 0.84 , 0.848, 0.839, 0.834, 0.84 , 0.846, 0.83 , 0.827,\n",
       "        0.833, 0.842, 0.839, 0.826, 0.833, 0.838, 0.846, 0.837, 0.84 ,\n",
       "        0.831, 0.839, 0.831, 0.845, 0.835, 0.835, 0.837, 0.838, 0.841,\n",
       "        0.838, 0.855, 0.841, 0.84 , 0.828, 0.839, 0.838, 0.843, 0.841,\n",
       "        0.835, 0.835, 0.846, 0.841, 0.841, 0.84 , 0.824, 0.839, 0.843,\n",
       "        0.836, 0.839, 0.836, 0.835, 0.832, 0.852, 0.84 , 0.84 , 0.841,\n",
       "        0.84 , 0.829, 0.833, 0.828, 0.835, 0.843, 0.837, 0.838, 0.84 ,\n",
       "        0.827, 0.847, 0.846, 0.844, 0.831, 0.832, 0.832, 0.838, 0.835,\n",
       "        0.85 , 0.84 , 0.835, 0.842, 0.838, 0.843, 0.762, 0.649, 0.768,\n",
       "        0.763, 0.717, 0.763, 0.77 , 0.649, 0.776, 0.774, 0.648, 0.777,\n",
       "        0.765, 0.649, 0.769, 0.773, 0.495, 0.776, 0.767, 0.629, 0.762,\n",
       "        0.773, 0.341, 0.776, 0.764, 0.656, 0.766, 0.779, 0.47 , 0.77 ,\n",
       "        0.772, 0.344, 0.771, 0.77 , 0.649, 0.774, 0.773, 0.689, 0.772,\n",
       "        0.775, 0.656, 0.774, 0.765, 0.649, 0.769, 0.769, 0.351, 0.77 ,\n",
       "        0.794, 0.592, 0.791, 0.807, 0.593, 0.794, 0.796, 0.48 , 0.788,\n",
       "        0.792, 0.621, 0.795, 0.789, 0.457, 0.796, 0.802, 0.672, 0.789,\n",
       "        0.803, 0.369, 0.789, 0.796, 0.69 , 0.804, 0.796, 0.554, 0.789,\n",
       "        0.81 , 0.301, 0.796, 0.805, 0.449, 0.787, 0.799, 0.499, 0.794,\n",
       "        0.782, 0.659, 0.793, 0.801, 0.642, 0.786, 0.802, 0.482, 0.787,\n",
       "        0.797, 0.355, 0.797, 0.798, 0.808, 0.809, 0.816, 0.815, 0.815,\n",
       "        0.816, 0.822, 0.817, 0.815, 0.817, 0.818, 0.809, 0.791, 0.788,\n",
       "        0.815, 0.817, 0.809, 0.82 , 0.812, 0.814, 0.816, 0.816, 0.817,\n",
       "        0.811, 0.798, 0.804, 0.808, 0.807, 0.807, 0.821, 0.814, 0.813,\n",
       "        0.819, 0.817, 0.818, 0.793, 0.798, 0.791, 0.808, 0.809, 0.814,\n",
       "        0.812, 0.807, 0.814, 0.802, 0.808, 0.812, 0.834, 0.821, 0.827,\n",
       "        0.837, 0.84 , 0.837, 0.845, 0.834, 0.842, 0.835, 0.851, 0.843,\n",
       "        0.83 , 0.834, 0.829, 0.839, 0.835, 0.837, 0.837, 0.836, 0.842,\n",
       "        0.846, 0.84 , 0.836, 0.828, 0.833, 0.833, 0.832, 0.831, 0.83 ,\n",
       "        0.838, 0.828, 0.833, 0.848, 0.844, 0.841, 0.837, 0.808, 0.831,\n",
       "        0.832, 0.835, 0.834, 0.836, 0.837, 0.836, 0.839, 0.833, 0.84 ]),\n",
       " 'split4_test_precision_micro': array([0.857, 0.85 , 0.851, 0.853, 0.863, 0.857, 0.851, 0.857, 0.862,\n",
       "        0.844, 0.856, 0.855, 0.855, 0.846, 0.854, 0.847, 0.853, 0.842,\n",
       "        0.858, 0.849, 0.841, 0.845, 0.852, 0.851, 0.855, 0.844, 0.846,\n",
       "        0.864, 0.858, 0.857, 0.851, 0.852, 0.851, 0.846, 0.852, 0.833,\n",
       "        0.86 , 0.867, 0.85 , 0.863, 0.847, 0.85 , 0.857, 0.865, 0.856,\n",
       "        0.84 , 0.854, 0.848, 0.859, 0.859, 0.855, 0.858, 0.86 , 0.855,\n",
       "        0.842, 0.849, 0.844, 0.845, 0.844, 0.845, 0.853, 0.855, 0.862,\n",
       "        0.864, 0.851, 0.845, 0.85 , 0.849, 0.854, 0.831, 0.843, 0.836,\n",
       "        0.845, 0.85 , 0.852, 0.855, 0.856, 0.853, 0.846, 0.853, 0.836,\n",
       "        0.855, 0.847, 0.843, 0.848, 0.848, 0.848, 0.859, 0.861, 0.859,\n",
       "        0.85 , 0.86 , 0.858, 0.848, 0.859, 0.857, 0.776, 0.553, 0.779,\n",
       "        0.781, 0.656, 0.781, 0.788, 0.351, 0.784, 0.783, 0.649, 0.781,\n",
       "        0.784, 0.649, 0.783, 0.785, 0.65 , 0.783, 0.787, 0.301, 0.787,\n",
       "        0.786, 0.649, 0.786, 0.786, 0.699, 0.784, 0.784, 0.337, 0.785,\n",
       "        0.785, 0.351, 0.782, 0.786, 0.649, 0.788, 0.776, 0.351, 0.785,\n",
       "        0.788, 0.343, 0.782, 0.784, 0.293, 0.785, 0.786, 0.649, 0.787,\n",
       "        0.81 , 0.309, 0.806, 0.812, 0.639, 0.809, 0.822, 0.425, 0.807,\n",
       "        0.813, 0.668, 0.815, 0.804, 0.389, 0.793, 0.823, 0.574, 0.821,\n",
       "        0.812, 0.602, 0.817, 0.814, 0.483, 0.825, 0.813, 0.416, 0.794,\n",
       "        0.815, 0.525, 0.807, 0.823, 0.547, 0.822, 0.814, 0.649, 0.815,\n",
       "        0.802, 0.526, 0.808, 0.811, 0.609, 0.812, 0.817, 0.652, 0.796,\n",
       "        0.808, 0.423, 0.806, 0.822, 0.832, 0.804, 0.838, 0.832, 0.836,\n",
       "        0.84 , 0.833, 0.838, 0.837, 0.833, 0.834, 0.822, 0.809, 0.835,\n",
       "        0.83 , 0.833, 0.833, 0.84 , 0.836, 0.834, 0.833, 0.838, 0.838,\n",
       "        0.828, 0.822, 0.837, 0.835, 0.837, 0.837, 0.837, 0.837, 0.838,\n",
       "        0.837, 0.835, 0.835, 0.823, 0.822, 0.826, 0.826, 0.833, 0.822,\n",
       "        0.836, 0.837, 0.834, 0.838, 0.838, 0.835, 0.852, 0.846, 0.843,\n",
       "        0.852, 0.857, 0.851, 0.852, 0.854, 0.849, 0.853, 0.855, 0.851,\n",
       "        0.842, 0.846, 0.84 , 0.852, 0.852, 0.85 , 0.862, 0.853, 0.858,\n",
       "        0.86 , 0.852, 0.862, 0.833, 0.846, 0.845, 0.853, 0.847, 0.854,\n",
       "        0.862, 0.855, 0.848, 0.86 , 0.853, 0.866, 0.849, 0.848, 0.85 ,\n",
       "        0.846, 0.856, 0.842, 0.85 , 0.853, 0.855, 0.849, 0.849, 0.861]),\n",
       " 'mean_test_precision_micro': array([0.8478, 0.8446, 0.8494, 0.8538, 0.8554, 0.8532, 0.8528, 0.8534,\n",
       "        0.85  , 0.8522, 0.8476, 0.8538, 0.8442, 0.844 , 0.847 , 0.8424,\n",
       "        0.8514, 0.8564, 0.8484, 0.8466, 0.851 , 0.85  , 0.8534, 0.851 ,\n",
       "        0.8522, 0.848 , 0.8444, 0.8506, 0.8542, 0.8482, 0.8468, 0.8502,\n",
       "        0.849 , 0.8498, 0.8452, 0.8466, 0.8514, 0.8532, 0.847 , 0.8524,\n",
       "        0.8524, 0.8476, 0.8516, 0.8552, 0.8536, 0.8466, 0.8558, 0.853 ,\n",
       "        0.851 , 0.845 , 0.852 , 0.8526, 0.8486, 0.8518, 0.8484, 0.8424,\n",
       "        0.8498, 0.8514, 0.8488, 0.8456, 0.8458, 0.8544, 0.8522, 0.8522,\n",
       "        0.8536, 0.8474, 0.8462, 0.8508, 0.8496, 0.8436, 0.844 , 0.847 ,\n",
       "        0.8488, 0.8496, 0.8474, 0.851 , 0.8482, 0.8516, 0.8456, 0.8556,\n",
       "        0.8458, 0.8498, 0.8512, 0.853 , 0.848 , 0.8478, 0.8506, 0.8462,\n",
       "        0.8496, 0.85  , 0.8512, 0.8508, 0.8528, 0.8476, 0.8522, 0.853 ,\n",
       "        0.7682, 0.6456, 0.763 , 0.7688, 0.538 , 0.7688, 0.7716, 0.5902,\n",
       "        0.7742, 0.7716, 0.5214, 0.7716, 0.7706, 0.5294, 0.7684, 0.7738,\n",
       "        0.5342, 0.7726, 0.771 , 0.5684, 0.769 , 0.7716, 0.4656, 0.7724,\n",
       "        0.7684, 0.5668, 0.7666, 0.7708, 0.4604, 0.7706, 0.7718, 0.4064,\n",
       "        0.7718, 0.773 , 0.6434, 0.7732, 0.7676, 0.5008, 0.77  , 0.7722,\n",
       "        0.5012, 0.7724, 0.7716, 0.5462, 0.7718, 0.7726, 0.5248, 0.772 ,\n",
       "        0.7946, 0.4738, 0.79  , 0.8134, 0.6152, 0.8064, 0.8058, 0.5738,\n",
       "        0.7944, 0.807 , 0.5958, 0.8054, 0.7916, 0.4674, 0.7994, 0.8098,\n",
       "        0.5332, 0.803 , 0.803 , 0.5806, 0.8048, 0.809 , 0.5638, 0.8122,\n",
       "        0.7976, 0.5198, 0.7894, 0.81  , 0.4898, 0.7986, 0.8048, 0.5398,\n",
       "        0.804 , 0.806 , 0.5288, 0.8056, 0.7948, 0.537 , 0.7966, 0.8008,\n",
       "        0.643 , 0.8052, 0.8132, 0.5228, 0.7976, 0.8026, 0.5476, 0.8042,\n",
       "        0.8154, 0.8086, 0.8048, 0.8272, 0.8262, 0.8246, 0.8322, 0.8314,\n",
       "        0.8314, 0.831 , 0.8306, 0.8312, 0.8152, 0.8106, 0.8078, 0.8218,\n",
       "        0.8276, 0.8268, 0.8316, 0.8304, 0.8312, 0.8308, 0.8302, 0.8324,\n",
       "        0.8134, 0.8068, 0.8146, 0.8264, 0.8246, 0.8248, 0.83  , 0.8296,\n",
       "        0.83  , 0.8316, 0.8328, 0.833 , 0.8086, 0.8122, 0.8122, 0.821 ,\n",
       "        0.8252, 0.82  , 0.828 , 0.8252, 0.826 , 0.8282, 0.8272, 0.829 ,\n",
       "        0.8436, 0.836 , 0.8364, 0.8432, 0.8474, 0.8476, 0.8478, 0.846 ,\n",
       "        0.8474, 0.8476, 0.8536, 0.8484, 0.8408, 0.8376, 0.8346, 0.8474,\n",
       "        0.8472, 0.8474, 0.851 , 0.848 , 0.8482, 0.8504, 0.8486, 0.852 ,\n",
       "        0.8392, 0.8428, 0.8382, 0.8458, 0.8448, 0.843 , 0.8502, 0.8466,\n",
       "        0.8464, 0.8512, 0.8508, 0.851 , 0.8392, 0.8416, 0.8408, 0.8434,\n",
       "        0.845 , 0.843 , 0.8486, 0.8464, 0.8486, 0.8486, 0.8464, 0.8506]),\n",
       " 'std_test_precision_micro': array([0.01251239, 0.00830903, 0.00765768, 0.00670522, 0.00958332,\n",
       "        0.0113031 , 0.00708237, 0.00628013, 0.01100909, 0.00917388,\n",
       "        0.0054626 , 0.00870402, 0.00897552, 0.00603324, 0.00987927,\n",
       "        0.01103812, 0.00733757, 0.00786384, 0.00722772, 0.0063435 ,\n",
       "        0.00583095, 0.00729383, 0.01030728, 0.0069857 , 0.00411825,\n",
       "        0.0100995 , 0.01297074, 0.01103812, 0.00762627, 0.00667533,\n",
       "        0.01289031, 0.00890842, 0.0063561 , 0.00744043, 0.00685274,\n",
       "        0.0088227 , 0.01053755, 0.00980612, 0.00909945, 0.00763151,\n",
       "        0.01019019, 0.01128893, 0.0082365 , 0.0097857 , 0.00643739,\n",
       "        0.00643739, 0.00331059, 0.00729383, 0.00626099, 0.01048809,\n",
       "        0.0070993 , 0.00773563, 0.00873155, 0.00724983, 0.00845222,\n",
       "        0.00628013, 0.00778203, 0.00868562, 0.00534416, 0.0032619 ,\n",
       "        0.0116    , 0.0082365 , 0.00730479, 0.01163443, 0.00930806,\n",
       "        0.00816333, 0.00594643, 0.01370255, 0.00538888, 0.00793977,\n",
       "        0.00596657, 0.00809938, 0.00541849, 0.0108922 , 0.00791454,\n",
       "        0.01208305, 0.00847113, 0.00488262, 0.00615142, 0.01003195,\n",
       "        0.00783326, 0.01235152, 0.00752064, 0.00712741, 0.00603324,\n",
       "        0.0096623 , 0.01007174, 0.01137365, 0.00796492, 0.00942338,\n",
       "        0.00116619, 0.00837616, 0.00897552, 0.00427083, 0.00788416,\n",
       "        0.00712741, 0.00601332, 0.05493487, 0.01091788, 0.00691086,\n",
       "        0.16893431, 0.00716659, 0.00886792, 0.11960167, 0.00552811,\n",
       "        0.00652993, 0.1562992 , 0.00631189, 0.00913455, 0.14688853,\n",
       "        0.00875443, 0.00679412, 0.1513782 , 0.00691665, 0.0091433 ,\n",
       "        0.13445683, 0.00981835, 0.00781281, 0.15178089, 0.00877724,\n",
       "        0.01100182, 0.11985057, 0.01135958, 0.010245  , 0.11304619,\n",
       "        0.00816333, 0.00691086, 0.12190258, 0.00624179, 0.00737564,\n",
       "        0.01220819, 0.00863481, 0.00830903, 0.14437091, 0.00807465,\n",
       "        0.00910824, 0.13855165, 0.00741889, 0.0069455 , 0.14017332,\n",
       "        0.00735935, 0.00786384, 0.16653096, 0.00779744, 0.00889044,\n",
       "        0.11432655, 0.01033441, 0.01044222, 0.04396544, 0.00985089,\n",
       "        0.00980612, 0.10436743, 0.00849941, 0.01129602, 0.07222022,\n",
       "        0.01224091, 0.00781281, 0.09118903, 0.01212601, 0.01083328,\n",
       "        0.12574164, 0.02002998, 0.00481664, 0.11762415, 0.0152368 ,\n",
       "        0.00867179, 0.10876838, 0.01251239, 0.01484049, 0.10163936,\n",
       "        0.0118423 , 0.00629285, 0.09697711, 0.01462327, 0.0095373 ,\n",
       "        0.06579787, 0.01167904, 0.00764199, 0.06897652, 0.01055651,\n",
       "        0.01225398, 0.11675787, 0.0074458 , 0.01166876, 0.05949454,\n",
       "        0.01517103, 0.00930376, 0.07291474, 0.00821219, 0.00873155,\n",
       "        0.13243806, 0.00581034, 0.01180847, 0.01820549, 0.01282809,\n",
       "        0.00984683, 0.00604649, 0.00801499, 0.0084947 , 0.00553534,\n",
       "        0.00747262, 0.00809938, 0.00700286, 0.00691086, 0.00661513,\n",
       "        0.0124996 , 0.01522367, 0.00643117, 0.00578273, 0.00988737,\n",
       "        0.00711618, 0.00966644, 0.00890842, 0.00808455, 0.00780769,\n",
       "        0.00786384, 0.0088227 , 0.01499867, 0.01710672, 0.00956243,\n",
       "        0.01083697, 0.01020588, 0.00576194, 0.00830903, 0.00880909,\n",
       "        0.00662118, 0.00815843, 0.00766812, 0.014988  , 0.01066583,\n",
       "        0.01495861, 0.00779744, 0.00917388, 0.00509902, 0.00848528,\n",
       "        0.01057166, 0.00756307, 0.01324236, 0.01010742, 0.00878635,\n",
       "        0.00611882, 0.00822192, 0.00668132, 0.00552811, 0.00665132,\n",
       "        0.00954149, 0.00574108, 0.00750999, 0.005004  , 0.00688767,\n",
       "        0.00215407, 0.00445421, 0.00577581, 0.0062482 , 0.00567803,\n",
       "        0.00776144, 0.00780769, 0.00722772, 0.00846168, 0.0083666 ,\n",
       "        0.00722219, 0.00601997, 0.00656049, 0.00874071, 0.00762627,\n",
       "        0.00538145, 0.00808455, 0.00818291, 0.00699714, 0.01      ,\n",
       "        0.00881816, 0.0107443 , 0.00682935, 0.00765245, 0.00430813,\n",
       "        0.00822192, 0.00491528, 0.01722324, 0.00778203, 0.00708802,\n",
       "        0.00672309, 0.00536656, 0.00897998, 0.00527636, 0.00688767,\n",
       "        0.0058515 , 0.0094784 , 0.00875443]),\n",
       " 'rank_test_precision_micro': array([ 83, 121,  65,   8,   4,  15,  20,  14,  56,  25,  86,   8, 123,\n",
       "        124,  98, 133,  35,   1,  74, 103,  41,  56,  13,  41,  25,  80,\n",
       "        122,  50,   7,  77, 101,  54,  66,  60, 117, 103,  35,  15, 100,\n",
       "         23,  23,  88,  33,   5,  10, 102,   2,  17,  41, 118,  30,  22,\n",
       "         69,  32,  74, 133,  59,  35,  67, 115, 112,   6,  25,  25,  10,\n",
       "         91, 109,  47,  63, 126, 124,  98,  67,  63,  91,  41,  77,  33,\n",
       "        115,   3, 112,  60,  38,  17,  80,  83,  50, 109,  62,  56,  38,\n",
       "         47,  20,  86,  25,  17, 253, 257, 256, 249, 271, 249, 238, 262,\n",
       "        225, 238, 279, 238, 245, 275, 251, 226, 273, 229, 243, 265, 248,\n",
       "        238, 286, 231, 251, 266, 255, 244, 287, 246, 235, 288, 235, 228,\n",
       "        258, 227, 254, 282, 247, 233, 281, 231, 238, 269, 235, 229, 277,\n",
       "        234, 220, 284, 223, 184, 260, 199, 201, 264, 221, 197, 261, 203,\n",
       "        222, 285, 214, 192, 274, 210, 210, 263, 205, 193, 267, 187, 217,\n",
       "        280, 224, 191, 283, 215, 205, 270, 209, 200, 276, 202, 219, 272,\n",
       "        218, 213, 259, 204, 186, 278, 216, 212, 268, 208, 181, 194, 205,\n",
       "        168, 171, 177, 148, 151, 151, 155, 157, 153, 182, 190, 196, 178,\n",
       "        166, 169, 149, 158, 153, 156, 159, 147, 184, 198, 183, 170, 176,\n",
       "        175, 160, 162, 160, 150, 146, 145, 194, 187, 187, 179, 173, 180,\n",
       "        165, 174, 172, 164, 167, 163, 126, 143, 142, 129,  91,  88,  83,\n",
       "        111,  91,  88,  10,  74, 136, 141, 144,  91,  97,  96,  41,  80,\n",
       "         77,  53,  71,  30, 138, 132, 140, 114, 120, 130,  54, 103, 106,\n",
       "         38,  47,  41, 138, 135, 136, 128, 118, 130,  71, 108,  69,  71,\n",
       "        106,  50]),\n",
       " 'split0_test_roc_auc_ovo': array([0.90837802, 0.90556484, 0.90460659, 0.90654945, 0.90886154,\n",
       "        0.91335385, 0.90471648, 0.91050989, 0.88997802, 0.90111648,\n",
       "        0.90167473, 0.89804396, 0.89747253, 0.89875165, 0.90976264,\n",
       "        0.90152527, 0.9142989 , 0.91645714, 0.89913407, 0.91010549,\n",
       "        0.90331429, 0.91079121, 0.90727912, 0.90207912, 0.90963077,\n",
       "        0.90949011, 0.91027253, 0.90559121, 0.90187253, 0.90190769,\n",
       "        0.89284396, 0.89646154, 0.89866813, 0.90122198, 0.89430769,\n",
       "        0.90266813, 0.91091429, 0.90944176, 0.91074286, 0.9084044 ,\n",
       "        0.91114286, 0.91218462, 0.91306813, 0.89981538, 0.90635604,\n",
       "        0.9094989 , 0.91344176, 0.9052967 , 0.90606154, 0.90718242,\n",
       "        0.90963956, 0.9089978 , 0.90677363, 0.89642637, 0.89752527,\n",
       "        0.89824176, 0.89014505, 0.88667692, 0.90103297, 0.8911033 ,\n",
       "        0.89945495, 0.90878681, 0.90858462, 0.90958681, 0.89959121,\n",
       "        0.90531429, 0.90094505, 0.90666374, 0.89963956, 0.89696264,\n",
       "        0.89469451, 0.90723077, 0.9101011 , 0.90658022, 0.90865495,\n",
       "        0.90831648, 0.90721319, 0.90591209, 0.89292747, 0.90506813,\n",
       "        0.90142857, 0.89827253, 0.91020659, 0.90681319, 0.90898022,\n",
       "        0.91338462, 0.9077011 , 0.8968967 , 0.90184615, 0.90676044,\n",
       "        0.89780659, 0.90144615, 0.904     , 0.90589451, 0.89647473,\n",
       "        0.90037802, 0.81332308, 0.40661099, 0.81488352, 0.81664615,\n",
       "        0.29838242, 0.81325714, 0.8151033 , 0.65482637, 0.8154022 ,\n",
       "        0.8143956 , 0.54507692, 0.81686154, 0.80553846, 0.50183736,\n",
       "        0.81362198, 0.81673846, 0.48775385, 0.81686154, 0.81353407,\n",
       "        0.60928791, 0.81522198, 0.81133187, 0.57006593, 0.81586374,\n",
       "        0.81638681, 0.45613626, 0.81038242, 0.81504615, 0.54372747,\n",
       "        0.81531429, 0.81254945, 0.66161319, 0.81036044, 0.81190769,\n",
       "        0.34089231, 0.81485714, 0.81105495, 0.36885714, 0.81155604,\n",
       "        0.8190022 , 0.42841319, 0.8130022 , 0.8137978 , 0.50923077,\n",
       "        0.81376264, 0.81368352, 0.61087912, 0.81300659, 0.83855824,\n",
       "        0.60934066, 0.83369231, 0.85780659, 0.5391956 , 0.85665495,\n",
       "        0.85712967, 0.63538901, 0.85024176, 0.85677363, 0.68920879,\n",
       "        0.8558022 , 0.8356044 , 0.55923077, 0.83697143, 0.84843516,\n",
       "        0.30156484, 0.84226813, 0.8600044 , 0.76302418, 0.85102418,\n",
       "        0.85714725, 0.44123077, 0.86231648, 0.83946813, 0.55705055,\n",
       "        0.83431209, 0.86056264, 0.48598242, 0.84639121, 0.85061538,\n",
       "        0.48798242, 0.84840879, 0.85601319, 0.44963956, 0.86569231,\n",
       "        0.83512088, 0.44075165, 0.84505934, 0.84362637, 0.5471956 ,\n",
       "        0.83783736, 0.86156044, 0.61686593, 0.84542418, 0.84741538,\n",
       "        0.52391648, 0.85559121, 0.86748132, 0.84777143, 0.84469451,\n",
       "        0.8654022 , 0.87933626, 0.87961758, 0.8845011 , 0.88564835,\n",
       "        0.88462418, 0.88418901, 0.88609231, 0.88344615, 0.8576044 ,\n",
       "        0.87297143, 0.85184615, 0.86463297, 0.88304615, 0.87782418,\n",
       "        0.88584176, 0.88541099, 0.88633407, 0.88118681, 0.88154725,\n",
       "        0.88177582, 0.85479121, 0.85190769, 0.87234725, 0.87930989,\n",
       "        0.87702418, 0.87542857, 0.88526593, 0.88144176, 0.88072527,\n",
       "        0.88423297, 0.88496264, 0.88523956, 0.87294066, 0.86124396,\n",
       "        0.84837802, 0.87479121, 0.87284396, 0.86886593, 0.88193407,\n",
       "        0.87627692, 0.87418462, 0.88287473, 0.88148571, 0.88123077,\n",
       "        0.89630769, 0.88379341, 0.89559121, 0.90177143, 0.8984    ,\n",
       "        0.89656264, 0.9026989 , 0.90203516, 0.90441319, 0.90636923,\n",
       "        0.90134945, 0.89916044, 0.89563077, 0.89846593, 0.88821538,\n",
       "        0.90095824, 0.90265055, 0.8975956 , 0.90353407, 0.89984615,\n",
       "        0.8973011 , 0.90363516, 0.90561758, 0.9039956 , 0.89777582,\n",
       "        0.9003033 , 0.88732308, 0.90361319, 0.90506374, 0.89651429,\n",
       "        0.90678242, 0.90047033, 0.90552527, 0.90215824, 0.90776703,\n",
       "        0.90682198, 0.89283956, 0.89882637, 0.89665934, 0.89484835,\n",
       "        0.90076044, 0.8987033 , 0.90008791, 0.89657143, 0.90110769,\n",
       "        0.90343297, 0.90101978, 0.90305934]),\n",
       " 'split1_test_roc_auc_ovo': array([0.89442198, 0.89564396, 0.88839121, 0.90451868, 0.89984615,\n",
       "        0.90028571, 0.89887033, 0.89470769, 0.90425055, 0.89818022,\n",
       "        0.9019956 , 0.89703736, 0.89461099, 0.89065055, 0.8956967 ,\n",
       "        0.8992044 , 0.89620659, 0.90362637, 0.89528791, 0.89186813,\n",
       "        0.89552967, 0.89926593, 0.90230769, 0.90692308, 0.90328352,\n",
       "        0.89156923, 0.90295385, 0.90252747, 0.90168352, 0.90005275,\n",
       "        0.90593407, 0.90108571, 0.89887033, 0.9000967 , 0.90267253,\n",
       "        0.9083033 , 0.88784615, 0.89549451, 0.89824615, 0.90104615,\n",
       "        0.89853626, 0.89565714, 0.89321758, 0.9045011 , 0.90127473,\n",
       "        0.90746374, 0.90594286, 0.90220659, 0.90111648, 0.89552527,\n",
       "        0.89954725, 0.89862857, 0.8961011 , 0.8877011 , 0.89993407,\n",
       "        0.88918681, 0.8972967 , 0.91701538, 0.89979341, 0.89858901,\n",
       "        0.89961758, 0.89987253, 0.89708571, 0.89485275, 0.90883077,\n",
       "        0.90409231, 0.90474286, 0.88903736, 0.90151209, 0.89272527,\n",
       "        0.89136264, 0.89186813, 0.88942418, 0.89170989, 0.88935385,\n",
       "        0.89794286, 0.90355604, 0.8958022 , 0.90764835, 0.90848791,\n",
       "        0.8921011 , 0.89821099, 0.90617143, 0.8936967 , 0.89741099,\n",
       "        0.9029011 , 0.89697143, 0.90341538, 0.89797363, 0.8921978 ,\n",
       "        0.89136264, 0.89146374, 0.90455385, 0.90443956, 0.8972    ,\n",
       "        0.89418901, 0.82555604, 0.32572747, 0.81795165, 0.81368352,\n",
       "        0.44226374, 0.82388132, 0.82544176, 0.48024615, 0.81865495,\n",
       "        0.82547692, 0.40288352, 0.8185011 , 0.82225495, 0.35463297,\n",
       "        0.8206989 , 0.82187692, 0.29601319, 0.82088352, 0.81865934,\n",
       "        0.61786813, 0.82156923, 0.82424176, 0.57443956, 0.82385055,\n",
       "        0.82561319, 0.66231648, 0.82449231, 0.82263736, 0.37673407,\n",
       "        0.82553407, 0.82371429, 0.49509011, 0.82396923, 0.82339341,\n",
       "        0.47024615, 0.81882637, 0.81906813, 0.48087912, 0.82913846,\n",
       "        0.82241319, 0.62697582, 0.81986813, 0.82389011, 0.63177582,\n",
       "        0.82436484, 0.82704176, 0.38464176, 0.82286593, 0.84847033,\n",
       "        0.5217011 , 0.84544176, 0.87224176, 0.54646154, 0.86659341,\n",
       "        0.85677363, 0.55382418, 0.85813187, 0.86565714, 0.41578022,\n",
       "        0.86592527, 0.85284835, 0.57745934, 0.86053626, 0.86595165,\n",
       "        0.60018462, 0.86841758, 0.8540967 , 0.73165714, 0.86645275,\n",
       "        0.86996044, 0.65054505, 0.86470769, 0.85454945, 0.44893187,\n",
       "        0.85292747, 0.86037363, 0.54236923, 0.8668044 , 0.85836923,\n",
       "        0.56149011, 0.86108571, 0.86492308, 0.52643956, 0.86388571,\n",
       "        0.85314286, 0.25619341, 0.84674725, 0.85662857, 0.7675956 ,\n",
       "        0.87089231, 0.86677363, 0.55761319, 0.85526593, 0.86227692,\n",
       "        0.58463736, 0.86150769, 0.85652747, 0.86939341, 0.86724396,\n",
       "        0.88266374, 0.87538462, 0.88035604, 0.88351648, 0.88287033,\n",
       "        0.8831033 , 0.88215385, 0.88343297, 0.88327033, 0.87288791,\n",
       "        0.85252308, 0.85523956, 0.87771868, 0.87930989, 0.87925275,\n",
       "        0.87756484, 0.88208791, 0.88286154, 0.88443516, 0.884     ,\n",
       "        0.88330989, 0.8532    , 0.86629011, 0.86857143, 0.86971429,\n",
       "        0.88152527, 0.87756484, 0.88264176, 0.88066813, 0.8827033 ,\n",
       "        0.88367912, 0.8835956 , 0.88401758, 0.8735033 , 0.87026374,\n",
       "        0.87654505, 0.87993846, 0.8796967 , 0.87429011, 0.88010989,\n",
       "        0.88232527, 0.88123516, 0.88048791, 0.88171429, 0.88207033,\n",
       "        0.87580659, 0.88971868, 0.88420659, 0.88339341, 0.89266813,\n",
       "        0.8951033 , 0.89483077, 0.89372747, 0.89717802, 0.89461978,\n",
       "        0.89595604, 0.89462418, 0.89400879, 0.87780659, 0.87661538,\n",
       "        0.89412747, 0.8979956 , 0.89578462, 0.89636923, 0.89358242,\n",
       "        0.89616264, 0.89363077, 0.89841319, 0.89687912, 0.88678242,\n",
       "        0.88761319, 0.88503736, 0.89051868, 0.89214945, 0.89488791,\n",
       "        0.89312527, 0.89268132, 0.89603956, 0.89727033, 0.89325275,\n",
       "        0.89122637, 0.87895824, 0.89002637, 0.88535385, 0.89174066,\n",
       "        0.89093187, 0.89120879, 0.8919956 , 0.89459341, 0.89203516,\n",
       "        0.89401758, 0.89611868, 0.89706813]),\n",
       " 'split2_test_roc_auc_ovo': array([0.91297582, 0.90739341, 0.90608791, 0.91776703, 0.90582418,\n",
       "        0.90408791, 0.9137011 , 0.90744176, 0.90974505, 0.90516923,\n",
       "        0.90077363, 0.90704615, 0.90923077, 0.9000044 , 0.90732747,\n",
       "        0.8992967 , 0.90438242, 0.90739341, 0.90549451, 0.91188132,\n",
       "        0.90280879, 0.90442198, 0.91218022, 0.90261099, 0.9076    ,\n",
       "        0.90953846, 0.90069011, 0.89985495, 0.91090989, 0.90347692,\n",
       "        0.91129231, 0.9018022 , 0.90596044, 0.90401319, 0.89956484,\n",
       "        0.90822857, 0.9143033 , 0.90708571, 0.90324396, 0.90219341,\n",
       "        0.91347692, 0.90974945, 0.91262418, 0.91116484, 0.90719121,\n",
       "        0.90709451, 0.90508571, 0.90842198, 0.90873407, 0.90479121,\n",
       "        0.9146989 , 0.89930549, 0.89311209, 0.9144044 , 0.9058989 ,\n",
       "        0.90206154, 0.91098022, 0.91126593, 0.90241319, 0.9008967 ,\n",
       "        0.90666374, 0.908     , 0.90302857, 0.90834725, 0.90389011,\n",
       "        0.89898901, 0.89442637, 0.89697582, 0.90517363, 0.90639121,\n",
       "        0.90644835, 0.90666374, 0.91088791, 0.90903736, 0.9054022 ,\n",
       "        0.91036923, 0.90620659, 0.91007033, 0.90314725, 0.90718242,\n",
       "        0.90111648, 0.91400879, 0.89865055, 0.91643516, 0.90169231,\n",
       "        0.90855824, 0.90877363, 0.91357363, 0.9102989 , 0.90854066,\n",
       "        0.90833846, 0.90654945, 0.91261099, 0.90089231, 0.90042637,\n",
       "        0.90282637, 0.81745055, 0.74767912, 0.79272527, 0.81568791,\n",
       "        0.43591648, 0.81574066, 0.81472088, 0.28494505, 0.81845275,\n",
       "        0.81060659, 0.34462857, 0.80781538, 0.81552967, 0.6259033 ,\n",
       "        0.81277802, 0.81321758, 0.57777582, 0.80628132, 0.81408352,\n",
       "        0.41637802, 0.81446593, 0.81385934, 0.44284835, 0.81345055,\n",
       "        0.81145934, 0.26701099, 0.81455385, 0.80849231, 0.44636044,\n",
       "        0.80978462, 0.81651429, 0.55529231, 0.81414945, 0.80996923,\n",
       "        0.67285714, 0.81191209, 0.80192527, 0.44563956, 0.81213187,\n",
       "        0.81064176, 0.47498022, 0.8093978 , 0.81333626, 0.40291429,\n",
       "        0.81123516, 0.81138022, 0.44947253, 0.8128    , 0.8551033 ,\n",
       "        0.6590989 , 0.83796044, 0.86413187, 0.53175824, 0.85869451,\n",
       "        0.85562637, 0.56490989, 0.84132747, 0.8574022 , 0.47052747,\n",
       "        0.84927912, 0.84612747, 0.40226813, 0.85809231, 0.86372747,\n",
       "        0.34713407, 0.84245714, 0.85901538, 0.43448791, 0.85431648,\n",
       "        0.85947692, 0.42461978, 0.85632527, 0.84303736, 0.39560879,\n",
       "        0.83284835, 0.85458462, 0.49677802, 0.83814945, 0.85012308,\n",
       "        0.57033846, 0.85930549, 0.86527033, 0.52843956, 0.85385055,\n",
       "        0.8526989 , 0.34453626, 0.85461978, 0.84757363, 0.5544967 ,\n",
       "        0.86032967, 0.86506813, 0.67808352, 0.85681758, 0.85338901,\n",
       "        0.58052308, 0.85854066, 0.88433846, 0.85080879, 0.86305495,\n",
       "        0.88947692, 0.884     , 0.87804396, 0.8887033 , 0.8849011 ,\n",
       "        0.88832088, 0.8850022 , 0.88865055, 0.88948571, 0.86719121,\n",
       "        0.8838989 , 0.86847033, 0.86972308, 0.88471648, 0.88569231,\n",
       "        0.88881319, 0.88763956, 0.8872967 , 0.8915956 , 0.88636923,\n",
       "        0.88795604, 0.8752967 , 0.85119121, 0.85410989, 0.88861538,\n",
       "        0.87776703, 0.88204835, 0.88156044, 0.89008352, 0.88826374,\n",
       "        0.88991209, 0.88693187, 0.8909011 , 0.85401758, 0.86114286,\n",
       "        0.87098022, 0.88227692, 0.88787692, 0.87589451, 0.88885714,\n",
       "        0.88403516, 0.88724396, 0.88881319, 0.88632088, 0.88478681,\n",
       "        0.89515604, 0.89058462, 0.8956    , 0.90203077, 0.89650549,\n",
       "        0.90224615, 0.90491429, 0.90239121, 0.90081758, 0.9024044 ,\n",
       "        0.90320879, 0.90582857, 0.89130989, 0.88718681, 0.88522637,\n",
       "        0.89810549, 0.89521319, 0.89561319, 0.90330549, 0.89867253,\n",
       "        0.90218901, 0.9002989 , 0.9031033 , 0.90089231, 0.89841758,\n",
       "        0.89631648, 0.88870769, 0.89778022, 0.89704176, 0.90179341,\n",
       "        0.90424176, 0.9071033 , 0.90193407, 0.9046989 , 0.90316484,\n",
       "        0.90321319, 0.88454066, 0.89867692, 0.89853187, 0.89973187,\n",
       "        0.90082637, 0.9010022 , 0.90171868, 0.90241319, 0.90016264,\n",
       "        0.90435604, 0.90177143, 0.9019956 ]),\n",
       " 'split3_test_roc_auc_ovo': array([0.88391082, 0.87881861, 0.88388009, 0.89766417, 0.88873524,\n",
       "        0.88240949, 0.90107507, 0.88566236, 0.89498198, 0.8831821 ,\n",
       "        0.88908643, 0.87736557, 0.87810306, 0.88823919, 0.87878788,\n",
       "        0.88602672, 0.89874846, 0.90356411, 0.88909521, 0.88090817,\n",
       "        0.89512245, 0.88147007, 0.88834016, 0.89250172, 0.87910834,\n",
       "        0.8780021 , 0.88057015, 0.89013999, 0.89149206, 0.89157108,\n",
       "        0.88898547, 0.88481512, 0.88520582, 0.88972296, 0.90391968,\n",
       "        0.89207152, 0.88010922, 0.88477122, 0.880118  , 0.89545169,\n",
       "        0.89172472, 0.89003025, 0.89283096, 0.8897493 , 0.89335774,\n",
       "        0.88107498, 0.89393281, 0.89124184, 0.88530678, 0.87786601,\n",
       "        0.89236125, 0.88434102, 0.89739639, 0.88783094, 0.88735684,\n",
       "        0.89768173, 0.90211107, 0.88297139, 0.87970974, 0.88842357,\n",
       "        0.87799332, 0.88883182, 0.88230853, 0.894802  , 0.88625938,\n",
       "        0.89087309, 0.88398983, 0.88718563, 0.8934192 , 0.88125058,\n",
       "        0.88989855, 0.8846966 , 0.88372205, 0.87811623, 0.88609256,\n",
       "        0.88801531, 0.89566679, 0.89165887, 0.88214171, 0.8831426 ,\n",
       "        0.89314703, 0.8817159 , 0.8770495 , 0.88913472, 0.88752804,\n",
       "        0.88397666, 0.88380985, 0.87563159, 0.88019263, 0.8865096 ,\n",
       "        0.89445959, 0.89316898, 0.88271239, 0.89145694, 0.88697492,\n",
       "        0.89496881, 0.8158157 , 0.46106436, 0.81185168, 0.81671561,\n",
       "        0.65297038, 0.81905979, 0.81893248, 0.63755328, 0.82405981,\n",
       "        0.82212389, 0.53253526, 0.81938902, 0.82147419, 0.36058104,\n",
       "        0.82378325, 0.82372618, 0.34199448, 0.82266823, 0.82006067,\n",
       "        0.36785499, 0.81773406, 0.82034601, 0.40016418, 0.82240923,\n",
       "        0.82668054, 0.56557316, 0.81839692, 0.82123714, 0.71232095,\n",
       "        0.8222073 , 0.82145664, 0.54679345, 0.82125031, 0.82515727,\n",
       "        0.67611359, 0.82042063, 0.82028455, 0.61615723, 0.82557869,\n",
       "        0.82446367, 0.74362047, 0.82606157, 0.8177165 , 0.64336542,\n",
       "        0.82100009, 0.81864275, 0.43303526, 0.82367789, 0.84793612,\n",
       "        0.6083082 , 0.85170699, 0.86438483, 0.4836325 , 0.85473158,\n",
       "        0.85810298, 0.55736417, 0.84706693, 0.84927941, 0.56449326,\n",
       "        0.84935843, 0.85383606, 0.65407223, 0.85862537, 0.85549541,\n",
       "        0.57306661, 0.85939359, 0.85619779, 0.60995   , 0.85208451,\n",
       "        0.8594331 , 0.62923893, 0.86655341, 0.85354633, 0.51237275,\n",
       "        0.84488079, 0.8604691 , 0.35550639, 0.85732159, 0.86718993,\n",
       "        0.55153008, 0.85565784, 0.85437601, 0.49261849, 0.85890193,\n",
       "        0.84742251, 0.50560801, 0.846347  , 0.85052173, 0.42712216,\n",
       "        0.84911259, 0.84577193, 0.35599366, 0.85309856, 0.85805908,\n",
       "        0.26902664, 0.85686943, 0.86085101, 0.86660609, 0.87147002,\n",
       "        0.87273869, 0.87151392, 0.87220313, 0.87258504, 0.87336643,\n",
       "        0.87111006, 0.87213728, 0.87335326, 0.87276503, 0.86909951,\n",
       "        0.85794933, 0.85696601, 0.87285282, 0.87276503, 0.87025843,\n",
       "        0.8726465 , 0.87282209, 0.8722558 , 0.87360348, 0.87249725,\n",
       "        0.8729933 , 0.86949899, 0.86639099, 0.86488088, 0.87210216,\n",
       "        0.87177731, 0.87130321, 0.87355081, 0.87336643, 0.8727343 ,\n",
       "        0.87235677, 0.87292745, 0.87404686, 0.85898972, 0.86613637,\n",
       "        0.86022766, 0.86969653, 0.87066229, 0.87089496, 0.87098714,\n",
       "        0.86945948, 0.8710486 , 0.87109689, 0.8708774 , 0.87109689,\n",
       "        0.88198368, 0.86234356, 0.88012239, 0.88851136, 0.88574577,\n",
       "        0.88839284, 0.89424888, 0.88998196, 0.89149645, 0.89258513,\n",
       "        0.89318654, 0.89154913, 0.88482829, 0.88113644, 0.88268166,\n",
       "        0.8876334 , 0.88889767, 0.8892313 , 0.89211103, 0.88997318,\n",
       "        0.89490735, 0.89231296, 0.89437179, 0.89066238, 0.88563163,\n",
       "        0.88341476, 0.88091256, 0.88885377, 0.89046045, 0.88653155,\n",
       "        0.88537263, 0.88744024, 0.89093894, 0.89292754, 0.89402061,\n",
       "        0.89334018, 0.88932348, 0.87136467, 0.8820627 , 0.88876158,\n",
       "        0.89085115, 0.89187398, 0.88848063, 0.88718124, 0.89038582,\n",
       "        0.89009609, 0.89096528, 0.88947713]),\n",
       " 'split4_test_roc_auc_ovo': array([0.90684331, 0.90997327, 0.90563611, 0.90366946, 0.90709353,\n",
       "        0.90931479, 0.89357284, 0.89087309, 0.89876163, 0.89084676,\n",
       "        0.89741834, 0.88798019, 0.91290129, 0.90877045, 0.91359927,\n",
       "        0.90578975, 0.89727347, 0.8978661 , 0.90245787, 0.89607505,\n",
       "        0.88760267, 0.88874402, 0.8893586 , 0.89463957, 0.91018398,\n",
       "        0.90374848, 0.91404264, 0.8998108 , 0.90895043, 0.91128583,\n",
       "        0.89796707, 0.89996444, 0.89794951, 0.89103113, 0.89061848,\n",
       "        0.8815842 , 0.90240958, 0.9090075 , 0.91253254, 0.90546491,\n",
       "        0.90282661, 0.90099166, 0.90219887, 0.91183895, 0.90211546,\n",
       "        0.8914306 , 0.89943327, 0.90303733, 0.91251937, 0.90880557,\n",
       "        0.91346757, 0.90298904, 0.91119803, 0.90255006, 0.89689595,\n",
       "        0.89814266, 0.90037709, 0.88988538, 0.89336652, 0.89226906,\n",
       "        0.91046054, 0.91473185, 0.91016203, 0.89955619, 0.90432355,\n",
       "        0.90216814, 0.89411279, 0.89038582, 0.90104434, 0.89198372,\n",
       "        0.87639103, 0.88269044, 0.91157117, 0.90899872, 0.91114974,\n",
       "        0.90995132, 0.90289685, 0.90440696, 0.90674674, 0.90371336,\n",
       "        0.87925759, 0.89996005, 0.89413474, 0.88511802, 0.91555275,\n",
       "        0.90484594, 0.90277833, 0.91325247, 0.9062463 , 0.91066686,\n",
       "        0.90410406, 0.90506104, 0.90812954, 0.89819534, 0.90139114,\n",
       "        0.90623313, 0.82761118, 0.59651272, 0.8311889 , 0.83193517,\n",
       "        0.470463  , 0.83395888, 0.82944174, 0.52113486, 0.83081137,\n",
       "        0.82986317, 0.46595025, 0.83098258, 0.82411249, 0.45592386,\n",
       "        0.83709762, 0.83382719, 0.29412333, 0.82952515, 0.83345844,\n",
       "        0.29253421, 0.83584212, 0.83312921, 0.25651561, 0.83235221,\n",
       "        0.83225563, 0.60366376, 0.83162788, 0.82985439, 0.51935259,\n",
       "        0.83286582, 0.82563137, 0.64435314, 0.83049969, 0.83128987,\n",
       "        0.35585319, 0.82975342, 0.82132494, 0.54105593, 0.83349795,\n",
       "        0.83008705, 0.27987831, 0.82627228, 0.83401156, 0.20913612,\n",
       "        0.831145  , 0.8290291 , 0.28458422, 0.83172885, 0.8694068 ,\n",
       "        0.40103776, 0.87223386, 0.86952972, 0.61568312, 0.87058328,\n",
       "        0.87089935, 0.49079671, 0.8661671 , 0.87403369, 0.72948959,\n",
       "        0.87446828, 0.86059201, 0.37265308, 0.85221182, 0.8792971 ,\n",
       "        0.50050703, 0.88259387, 0.87846303, 0.52441846, 0.87971852,\n",
       "        0.87042963, 0.4420871 , 0.877787  , 0.860772  , 0.62111774,\n",
       "        0.85097388, 0.88259826, 0.59673221, 0.86973165, 0.87089496,\n",
       "        0.62868143, 0.8761013 , 0.87616715, 0.32853085, 0.87685196,\n",
       "        0.85834003, 0.543185  , 0.86554814, 0.86792304, 0.61004219,\n",
       "        0.86734358, 0.87073253, 0.47550253, 0.86240501, 0.86158851,\n",
       "        0.62374725, 0.86000817, 0.88291432, 0.88993367, 0.86338395,\n",
       "        0.89479761, 0.8918652 , 0.89711983, 0.89720324, 0.89548681,\n",
       "        0.89704959, 0.89352016, 0.89629454, 0.89591262, 0.87324352,\n",
       "        0.86416534, 0.88643936, 0.8907853 , 0.89247099, 0.8938494 ,\n",
       "        0.89740517, 0.89803291, 0.89499515, 0.89765978, 0.89738322,\n",
       "        0.89741834, 0.88770363, 0.88189588, 0.89300216, 0.89011365,\n",
       "        0.89445081, 0.89449471, 0.89683888, 0.89179935, 0.89707593,\n",
       "        0.89635161, 0.89644818, 0.89468347, 0.8812418 , 0.87535064,\n",
       "        0.88798897, 0.88898986, 0.89442008, 0.87898542, 0.89582483,\n",
       "        0.89474493, 0.89558778, 0.89467908, 0.89131647, 0.89504783,\n",
       "        0.90719011, 0.90347192, 0.89653598, 0.90638238, 0.90993815,\n",
       "        0.9080593 , 0.9079715 , 0.91176871, 0.90884508, 0.90790565,\n",
       "        0.91085123, 0.91147898, 0.90293197, 0.90660626, 0.89631649,\n",
       "        0.90949916, 0.9090953 , 0.9069355 , 0.90947282, 0.91031567,\n",
       "        0.90955184, 0.91197942, 0.91148337, 0.91232622, 0.89574142,\n",
       "        0.90331828, 0.90421819, 0.90999083, 0.90891532, 0.90574147,\n",
       "        0.91110145, 0.90938503, 0.90617606, 0.91222964, 0.91076783,\n",
       "        0.91635609, 0.8983402 , 0.89922695, 0.90359045, 0.90843682,\n",
       "        0.90842365, 0.90440696, 0.91059662, 0.91101805, 0.9079715 ,\n",
       "        0.90930601, 0.90955184, 0.91216379]),\n",
       " 'mean_test_roc_auc_ovo': array([0.90130599, 0.89947881, 0.89772038, 0.90603376, 0.90207213,\n",
       "        0.90189035, 0.90238717, 0.89783896, 0.89954345, 0.89569896,\n",
       "        0.89818975, 0.89349465, 0.89846373, 0.89728325, 0.90103479,\n",
       "        0.89836857, 0.90218197, 0.90578143, 0.89829391, 0.89816763,\n",
       "        0.89687557, 0.89693864, 0.89989316, 0.8997509 , 0.90196132,\n",
       "        0.89846968, 0.90170586, 0.89958488, 0.90298169, 0.90165885,\n",
       "        0.89940457, 0.8968258 , 0.89733085, 0.89721719, 0.89821664,\n",
       "        0.89857114, 0.89911651, 0.90116014, 0.9009767 , 0.90251211,\n",
       "        0.90354148, 0.90172262, 0.90278794, 0.90341391, 0.90205904,\n",
       "        0.89931255, 0.90356728, 0.90204089, 0.90274765, 0.8988341 ,\n",
       "        0.90594291, 0.89885239, 0.90091625, 0.89778257, 0.89752221,\n",
       "        0.8970629 , 0.90018203, 0.897563  , 0.89526317, 0.89425633,\n",
       "        0.89883802, 0.9040446 , 0.90023389, 0.901429  , 0.900579  ,\n",
       "        0.90028737, 0.89564338, 0.89404968, 0.90015776, 0.89386268,\n",
       "        0.89175901, 0.89462994, 0.90114128, 0.89888849, 0.90013066,\n",
       "        0.90291904, 0.90310789, 0.90157009, 0.89852231, 0.90151888,\n",
       "        0.89341015, 0.89843365, 0.89724256, 0.89823956, 0.90223286,\n",
       "        0.90273331, 0.90000687, 0.90055395, 0.89931152, 0.90093507,\n",
       "        0.89921427, 0.89953787, 0.90240135, 0.90017573, 0.89649343,\n",
       "        0.89971907, 0.81995131, 0.50751893, 0.8137202 , 0.81893367,\n",
       "        0.4599992 , 0.82117956, 0.82072803, 0.51574114, 0.82147621,\n",
       "        0.82049324, 0.4582149 , 0.81870992, 0.81778195, 0.45977571,\n",
       "        0.82159595, 0.82187727, 0.39953213, 0.81924395, 0.81995921,\n",
       "        0.46078465, 0.82096666, 0.82058164, 0.44880673, 0.82158525,\n",
       "        0.8224791 , 0.51094013, 0.81989067, 0.81945347, 0.5196991 ,\n",
       "        0.82114122, 0.8199732 , 0.58062844, 0.82004583, 0.82034349,\n",
       "        0.50319248, 0.81915393, 0.81473157, 0.4905178 , 0.8223806 ,\n",
       "        0.82132157, 0.5107736 , 0.8189204 , 0.82055045, 0.47928449,\n",
       "        0.82030155, 0.81995547, 0.43252258, 0.82081585, 0.85189496,\n",
       "        0.55989732, 0.84820707, 0.86561895, 0.5433462 , 0.86145154,\n",
       "        0.8597064 , 0.56045679, 0.85258703, 0.86062921, 0.57389987,\n",
       "        0.85896666, 0.84980166, 0.51313671, 0.85328744, 0.86258136,\n",
       "        0.46449143, 0.85902606, 0.86155546, 0.61270754, 0.86071929,\n",
       "        0.86328947, 0.51754433, 0.86553797, 0.85027465, 0.50701634,\n",
       "        0.84318852, 0.86371765, 0.49547365, 0.85567966, 0.85943852,\n",
       "        0.5600045 , 0.86011183, 0.86334995, 0.46513361, 0.86383649,\n",
       "        0.84934503, 0.41805487, 0.8516643 , 0.85325467, 0.58129045,\n",
       "        0.8571031 , 0.86198133, 0.53681176, 0.85460225, 0.85654578,\n",
       "        0.51637016, 0.85850343, 0.87042252, 0.86490268, 0.86196948,\n",
       "        0.88101583, 0.88042   , 0.88146811, 0.88530183, 0.8844546 ,\n",
       "        0.8848416 , 0.8834005 , 0.88556473, 0.88497597, 0.86800531,\n",
       "        0.86630162, 0.86379228, 0.87514257, 0.88246171, 0.88137541,\n",
       "        0.88445429, 0.88519869, 0.88474865, 0.88569617, 0.88435939,\n",
       "        0.88469068, 0.86809811, 0.86353518, 0.87058232, 0.87997107,\n",
       "        0.88050892, 0.88016794, 0.88397156, 0.88347184, 0.88430051,\n",
       "        0.88530651, 0.88497315, 0.88577771, 0.86813861, 0.86682751,\n",
       "        0.86882399, 0.8791386 , 0.88109999, 0.87378619, 0.88354261,\n",
       "        0.88136835, 0.88186002, 0.88359036, 0.88234295, 0.88284653,\n",
       "        0.89128882, 0.88598244, 0.89041123, 0.89641787, 0.89665151,\n",
       "        0.89807284, 0.90093287, 0.8999809 , 0.90055006, 0.90077684,\n",
       "        0.90091041, 0.90052826, 0.89374194, 0.89024041, 0.88581106,\n",
       "        0.89806475, 0.89877046, 0.89703204, 0.90095853, 0.89847799,\n",
       "        0.90002239, 0.90037144, 0.90259785, 0.90095113, 0.89286978,\n",
       "        0.8941932 , 0.88923978, 0.89815134, 0.89872614, 0.89709372,\n",
       "        0.90012471, 0.89941604, 0.90012278, 0.90185693, 0.90179461,\n",
       "        0.90219156, 0.88880043, 0.89162426, 0.89323964, 0.89670386,\n",
       "        0.8983587 , 0.89743904, 0.89857589, 0.89835546, 0.89833256,\n",
       "        0.90024174, 0.8998854 , 0.9007528 ]),\n",
       " 'std_test_roc_auc_ovo': array([0.01064746, 0.01141371, 0.00957791, 0.00657071, 0.00732415,\n",
       "        0.01070927, 0.00671138, 0.00958503, 0.00691585, 0.00781547,\n",
       "        0.00483263, 0.01007419, 0.01228621, 0.00731169, 0.01262584,\n",
       "        0.00661695, 0.00668206, 0.00614463, 0.00571918, 0.01159767,\n",
       "        0.00578847, 0.01058054, 0.00954781, 0.00536129, 0.01168105,\n",
       "        0.01215213, 0.01161802, 0.00517812, 0.00683241, 0.0063307 ,\n",
       "        0.00821728, 0.00627936, 0.0067235 , 0.00574363, 0.00504399,\n",
       "        0.01034956, 0.01318308, 0.00965375, 0.01163269, 0.00437123,\n",
       "        0.00802113, 0.00834169, 0.00887146, 0.00814753, 0.00492208,\n",
       "        0.01118478, 0.00656304, 0.00581306, 0.00947787, 0.01144745,\n",
       "        0.00862924, 0.00813482, 0.00687756, 0.01001523, 0.00599522,\n",
       "        0.00424164, 0.00677386, 0.01383136, 0.00837189, 0.00470699,\n",
       "        0.01124113, 0.00895752, 0.01007316, 0.00640443, 0.00773439,\n",
       "        0.00516912, 0.00707866, 0.00711813, 0.00383291, 0.00812974,\n",
       "        0.00963522, 0.01051136, 0.01203975, 0.01222619, 0.01034486,\n",
       "        0.00872897, 0.00405144, 0.00679199, 0.00971537, 0.00933535,\n",
       "        0.00807168, 0.01024181, 0.01155155, 0.01166446, 0.00961999,\n",
       "        0.01003677, 0.00911368, 0.01395315, 0.01041691, 0.00970486,\n",
       "        0.00621929, 0.00614921, 0.010314  , 0.00512518, 0.00510994,\n",
       "        0.00459715, 0.0056103 , 0.14898683, 0.01239913, 0.00659228,\n",
       "        0.11345649, 0.00731369, 0.00581542, 0.13322066, 0.00543718,\n",
       "        0.00707416, 0.07627815, 0.0073923 , 0.00676301, 0.10027659,\n",
       "        0.00880035, 0.00703728, 0.11374678, 0.0076658 , 0.00720743,\n",
       "        0.13087744, 0.00784032, 0.00776712, 0.11820734, 0.006643  ,\n",
       "        0.00750475, 0.13928541, 0.00748408, 0.00722592, 0.1126927 ,\n",
       "        0.00801031, 0.00480182, 0.06280589, 0.00713724, 0.00813739,\n",
       "        0.14684923, 0.00608223, 0.00735707, 0.08396147, 0.00896325,\n",
       "        0.0064395 , 0.16068457, 0.00680661, 0.00772048, 0.16117412,\n",
       "        0.00720466, 0.00703053, 0.10609142, 0.00716556, 0.01021985,\n",
       "        0.09091152, 0.01351188, 0.00497841, 0.04233101, 0.00609252,\n",
       "        0.00565226, 0.0459146 , 0.00869383, 0.00847416, 0.12106236,\n",
       "        0.00984787, 0.00845047, 0.10784468, 0.00861969, 0.01041551,\n",
       "        0.1198467 , 0.01548634, 0.00870703, 0.12353858, 0.01098539,\n",
       "        0.00570294, 0.1003169 , 0.00702867, 0.00785279, 0.07916652,\n",
       "        0.00829497, 0.00971164, 0.08016871, 0.01197913, 0.00845107,\n",
       "        0.04490994, 0.00910012, 0.00780773, 0.07405655, 0.00770607,\n",
       "        0.00790702, 0.10530182, 0.00771456, 0.00847339, 0.11062052,\n",
       "        0.01216912, 0.00862554, 0.11247496, 0.00552683, 0.00554787,\n",
       "        0.12770404, 0.00211859, 0.01134051, 0.01511116, 0.0091617 ,\n",
       "        0.01073815, 0.00706621, 0.00833309, 0.00798447, 0.00705396,\n",
       "        0.00839958, 0.00684119, 0.0074645 , 0.00767209, 0.0056801 ,\n",
       "        0.01112046, 0.0126272 , 0.0089039 , 0.00647412, 0.0079365 ,\n",
       "        0.00866647, 0.00816795, 0.00739769, 0.00831972, 0.00803004,\n",
       "        0.0079986 , 0.01293975, 0.01131753, 0.01275661, 0.00830849,\n",
       "        0.00763298, 0.00795756, 0.00752884, 0.00674037, 0.00809974,\n",
       "        0.0079335 , 0.00751678, 0.00702228, 0.01006617, 0.00544831,\n",
       "        0.0135904 , 0.00656641, 0.00897138, 0.00358606, 0.00837983,\n",
       "        0.00842264, 0.00887274, 0.00795565, 0.00676664, 0.00766686,\n",
       "        0.01112907, 0.01345073, 0.00686478, 0.00884993, 0.00792989,\n",
       "        0.00666254, 0.00548497, 0.00759034, 0.00595216, 0.00616047,\n",
       "        0.00614002, 0.00728513, 0.00589005, 0.01079173, 0.00649369,\n",
       "        0.00725858, 0.00681739, 0.00570643, 0.00606589, 0.00690419,\n",
       "        0.00536603, 0.00715232, 0.00589077, 0.0072264 , 0.00552341,\n",
       "        0.00754427, 0.00794119, 0.0079353 , 0.00718883, 0.00653653,\n",
       "        0.00946974, 0.008347  , 0.0058331 , 0.00658076, 0.00709175,\n",
       "        0.00918369, 0.00667248, 0.01069968, 0.00817253, 0.00689909,\n",
       "        0.00670347, 0.00515038, 0.00777014, 0.00798756, 0.00643067,\n",
       "        0.00708444, 0.00619743, 0.00745376]),\n",
       " 'rank_test_roc_auc_ovo': array([ 34,  71, 105,   1,  21,  25,  17, 103,  69, 123,  98, 132,  89,\n",
       "        110,  37,  91,  20,   3,  95,  99, 117, 116,  64,  66,  24,  88,\n",
       "         29,  68,   9,  30,  73, 118, 109, 112,  97,  85,  77,  35,  38,\n",
       "         15,   6,  28,  11,   7,  22,  74,   5,  23,  12,  81,   2,  79,\n",
       "         43, 104, 107, 114,  55, 106, 125, 127,  80,   4,  54,  33,  47,\n",
       "         52, 124, 129,  57, 130, 136, 126,  36,  78,  58,  10,   8,  31,\n",
       "         86,  32, 133,  90, 111,  96,  18,  13,  62,  48,  75,  41,  76,\n",
       "         70,  16,  56, 121,  67, 246, 273, 256, 251, 282, 232, 236, 269,\n",
       "        230, 239, 284, 253, 254, 283, 228, 227, 288, 249, 244, 281, 234,\n",
       "        237, 285, 229, 225, 271, 247, 248, 266, 233, 243, 259, 242, 240,\n",
       "        275, 250, 255, 277, 226, 231, 272, 252, 238, 278, 241, 245, 286,\n",
       "        235, 218, 263, 223, 189, 264, 202, 206, 261, 217, 204, 260, 209,\n",
       "        221, 270, 215, 198, 280, 208, 201, 257, 203, 197, 267, 190, 220,\n",
       "        274, 224, 194, 276, 213, 207, 262, 205, 196, 279, 192, 222, 287,\n",
       "        219, 216, 258, 211, 199, 265, 214, 212, 268, 210, 182, 191, 200,\n",
       "        173, 175, 169, 149, 156, 153, 164, 147, 151, 186, 188, 193, 179,\n",
       "        166, 170, 157, 150, 154, 146, 158, 155, 185, 195, 181, 177, 174,\n",
       "        176, 160, 163, 159, 148, 152, 145, 184, 187, 183, 178, 172, 180,\n",
       "        162, 171, 168, 161, 167, 165, 138, 143, 139, 122, 120, 101,  42,\n",
       "         63,  49,  45,  44,  50, 131, 140, 144, 102,  82, 115,  39,  87,\n",
       "         61,  51,  14,  40, 135, 128, 141, 100,  83, 113,  59,  72,  60,\n",
       "         26,  27,  19, 142, 137, 134, 119,  92, 108,  84,  93,  94,  53,\n",
       "         65,  46]),\n",
       " 'split0_test_jaccard': array([0.61557789, 0.61330049, 0.62469136, 0.63037975, 0.6529563 ,\n",
       "        0.64089776, 0.63959391, 0.65151515, 0.6039604 , 0.65816327,\n",
       "        0.63275434, 0.61460957, 0.59708738, 0.61832061, 0.63275434,\n",
       "        0.62779156, 0.63846154, 0.64961637, 0.60340633, 0.61772152,\n",
       "        0.63104326, 0.63316583, 0.64795918, 0.62779156, 0.62871287,\n",
       "        0.6425    , 0.65656566, 0.64556962, 0.65139949, 0.62656642,\n",
       "        0.6039604 , 0.63909774, 0.62315271, 0.6125908 , 0.61616162,\n",
       "        0.62962963, 0.63819095, 0.64631043, 0.64483627, 0.6437659 ,\n",
       "        0.64987406, 0.66153846, 0.63979849, 0.64646465, 0.63659148,\n",
       "        0.63636364, 0.64912281, 0.64122137, 0.62467866, 0.62128713,\n",
       "        0.62182741, 0.63224181, 0.62468514, 0.61881188, 0.64179104,\n",
       "        0.59405941, 0.62623762, 0.63461538, 0.63383838, 0.62128713,\n",
       "        0.61481481, 0.64248705, 0.63959391, 0.66331658, 0.63224181,\n",
       "        0.60957179, 0.62720403, 0.67272727, 0.62436548, 0.62972292,\n",
       "        0.61904762, 0.635     , 0.63523573, 0.64285714, 0.6318408 ,\n",
       "        0.6275    , 0.63209877, 0.63797468, 0.61234568, 0.66243655,\n",
       "        0.6175    , 0.62836186, 0.6674938 , 0.65075377, 0.63840399,\n",
       "        0.64267352, 0.63846154, 0.59661836, 0.60606061, 0.64631043,\n",
       "        0.6281407 , 0.625     , 0.63451777, 0.61386139, 0.63065327,\n",
       "        0.63959391, 0.43269231, 0.        , 0.44859813, 0.44208038,\n",
       "        0.35      , 0.44366197, 0.45260664, 0.        , 0.45283019,\n",
       "        0.45023697, 0.        , 0.44835681, 0.43095238, 0.35      ,\n",
       "        0.42380952, 0.45283019, 0.        , 0.45070423, 0.44917258,\n",
       "        0.31448763, 0.43971631, 0.44680851, 0.33865815, 0.45368171,\n",
       "        0.43160377, 0.27083333, 0.43971631, 0.44575472, 0.30779392,\n",
       "        0.44444444, 0.44655582, 0.33232628, 0.44444444, 0.45107399,\n",
       "        0.        , 0.45283019, 0.43333333, 0.        , 0.43914081,\n",
       "        0.44180523, 0.        , 0.44811321, 0.44418052, 0.        ,\n",
       "        0.4478673 , 0.45260664, 0.09090909, 0.44917258, 0.48333333,\n",
       "        0.35336788, 0.47429907, 0.52278177, 0.29012346, 0.54187192,\n",
       "        0.51581509, 0.33206107, 0.50361446, 0.52669903, 0.39226519,\n",
       "        0.53221957, 0.49056604, 0.3109151 , 0.4953271 , 0.50240385,\n",
       "        0.13426573, 0.48792271, 0.52289157, 0.46768707, 0.50591017,\n",
       "        0.52567237, 0.16043956, 0.53676471, 0.47743468, 0.12967033,\n",
       "        0.47743468, 0.54320988, 0.16171004, 0.48674699, 0.51456311,\n",
       "        0.24804992, 0.50608273, 0.50956938, 0.27260982, 0.55036855,\n",
       "        0.47764706, 0.10633484, 0.48309179, 0.48564593, 0.1873805 ,\n",
       "        0.5       , 0.53605769, 0.34366925, 0.50473934, 0.50596659,\n",
       "        0.08148148, 0.51807229, 0.54433498, 0.5060241 , 0.49767442,\n",
       "        0.54901961, 0.56930693, 0.56756757, 0.5925    , 0.59649123,\n",
       "        0.5920398 , 0.58542714, 0.585     , 0.58064516, 0.52941176,\n",
       "        0.55308642, 0.52300242, 0.54567901, 0.58168317, 0.56403941,\n",
       "        0.59398496, 0.60099751, 0.59899749, 0.57920792, 0.57530864,\n",
       "        0.59296482, 0.5315534 , 0.52682927, 0.55282555, 0.57178218,\n",
       "        0.56019656, 0.55970149, 0.58104738, 0.59102244, 0.57960199,\n",
       "        0.58663366, 0.5975    , 0.595     , 0.53940887, 0.53960396,\n",
       "        0.51699029, 0.55282555, 0.5591133 , 0.54433498, 0.57530864,\n",
       "        0.55940594, 0.5591133 , 0.5875    , 0.57462687, 0.58291457,\n",
       "        0.61809045, 0.59102244, 0.61538462, 0.59950249, 0.615     ,\n",
       "        0.59701493, 0.6019656 , 0.61442786, 0.60945274, 0.61111111,\n",
       "        0.63451777, 0.61138614, 0.60902256, 0.60696517, 0.58690176,\n",
       "        0.59753086, 0.625     , 0.60740741, 0.62311558, 0.60598504,\n",
       "        0.60891089, 0.60651629, 0.6119403 , 0.62468514, 0.6025    ,\n",
       "        0.60651629, 0.58252427, 0.60643564, 0.625     , 0.59223301,\n",
       "        0.61097257, 0.60545906, 0.61868687, 0.5990099 , 0.62060302,\n",
       "        0.61518987, 0.5925    , 0.61290323, 0.59452736, 0.59850374,\n",
       "        0.60552764, 0.60401003, 0.60150376, 0.61306533, 0.62034739,\n",
       "        0.61557789, 0.60391198, 0.605     ]),\n",
       " 'split1_test_jaccard': array([0.625     , 0.62562814, 0.64175258, 0.64050633, 0.64267352,\n",
       "        0.66075949, 0.63840399, 0.63703704, 0.63659148, 0.62926829,\n",
       "        0.6235012 , 0.66666667, 0.63065327, 0.60049628, 0.60891089,\n",
       "        0.6372796 , 0.64231738, 0.65326633, 0.62315271, 0.61016949,\n",
       "        0.6425    , 0.65346535, 0.65594059, 0.65671642, 0.63682864,\n",
       "        0.6278481 , 0.62278481, 0.63300493, 0.61964736, 0.61743341,\n",
       "        0.65914787, 0.64851485, 0.64339152, 0.64588529, 0.63814181,\n",
       "        0.64427861, 0.62720403, 0.61772152, 0.61868687, 0.61728395,\n",
       "        0.65063291, 0.61764706, 0.62653563, 0.65326633, 0.64070352,\n",
       "        0.62623762, 0.6475    , 0.64661654, 0.6345679 , 0.62373737,\n",
       "        0.64019851, 0.64467005, 0.63224181, 0.6426799 , 0.63793103,\n",
       "        0.63144963, 0.61538462, 0.65829146, 0.6372549 , 0.62621359,\n",
       "        0.64179104, 0.64019851, 0.62972292, 0.62626263, 0.66917293,\n",
       "        0.64631043, 0.62282878, 0.61916462, 0.6044226 , 0.63300493,\n",
       "        0.60194175, 0.62162162, 0.62659847, 0.62531646, 0.62435233,\n",
       "        0.64483627, 0.64516129, 0.63054187, 0.64179104, 0.6575    ,\n",
       "        0.62591687, 0.66421569, 0.6350365 , 0.64764268, 0.63316583,\n",
       "        0.62842893, 0.63819095, 0.63316583, 0.62656642, 0.62162162,\n",
       "        0.63209877, 0.60891089, 0.64393939, 0.63224181, 0.65260546,\n",
       "        0.65594059, 0.45913462, 0.        , 0.38422392, 0.43980344,\n",
       "        0.        , 0.45823389, 0.44976077, 0.        , 0.45454545,\n",
       "        0.45260664, 0.26338798, 0.44604317, 0.46097561, 0.        ,\n",
       "        0.44951923, 0.46650718, 0.23955148, 0.45803357, 0.45410628,\n",
       "        0.        , 0.45454545, 0.45      , 0.        , 0.44951923,\n",
       "        0.46282974, 0.34696262, 0.4400978 , 0.4547619 , 0.22572816,\n",
       "        0.45060241, 0.43884892, 0.35      , 0.46172249, 0.46172249,\n",
       "        0.12814645, 0.45803357, 0.45563549, 0.35      , 0.41293532,\n",
       "        0.45215311, 0.33287671, 0.45853659, 0.45673077, 0.35822785,\n",
       "        0.45933014, 0.46539379, 0.27663934, 0.45215311, 0.48402948,\n",
       "        0.29676512, 0.5       , 0.57468354, 0.20222222, 0.5475    ,\n",
       "        0.53580247, 0.12244898, 0.51219512, 0.56790123, 0.12365591,\n",
       "        0.55639098, 0.49122807, 0.36855037, 0.5443038 , 0.54187192,\n",
       "        0.26526316, 0.5825    , 0.50611247, 0.35550936, 0.56892231,\n",
       "        0.55610973, 0.35335689, 0.56532663, 0.54455446, 0.13461538,\n",
       "        0.53864734, 0.53233831, 0.31491713, 0.55721393, 0.51923077,\n",
       "        0.30460624, 0.53846154, 0.5325    , 0.28721805, 0.5308642 ,\n",
       "        0.5138539 , 0.31931932, 0.52325581, 0.54567901, 0.41920375,\n",
       "        0.5678392 , 0.57142857, 0.31777108, 0.52825553, 0.54590571,\n",
       "        0.325     , 0.54074074, 0.5201005 , 0.55012853, 0.54755784,\n",
       "        0.59398496, 0.56997455, 0.57106599, 0.59305211, 0.58024691,\n",
       "        0.58706468, 0.58955224, 0.5925    , 0.59452736, 0.55329949,\n",
       "        0.51256281, 0.51378446, 0.56598985, 0.57323232, 0.58227848,\n",
       "        0.5721519 , 0.57894737, 0.58706468, 0.6       , 0.58852868,\n",
       "        0.58706468, 0.50626566, 0.55329949, 0.55725191, 0.56298201,\n",
       "        0.58438287, 0.57360406, 0.58603491, 0.58      , 0.5875    ,\n",
       "        0.5925    , 0.58910891, 0.5975    , 0.55470738, 0.55919395,\n",
       "        0.56887755, 0.56997455, 0.57468354, 0.56122449, 0.56852792,\n",
       "        0.58080808, 0.57506361, 0.57653061, 0.5764411 , 0.58121827,\n",
       "        0.6010101 , 0.6019656 , 0.58866995, 0.60740741, 0.63157895,\n",
       "        0.65139949, 0.63037975, 0.63224181, 0.63636364, 0.62406015,\n",
       "        0.63567839, 0.6281407 , 0.61845387, 0.59398496, 0.56202532,\n",
       "        0.63383838, 0.63383838, 0.63544304, 0.63613232, 0.64824121,\n",
       "        0.6372796 , 0.62849873, 0.64030612, 0.63544304, 0.61904762,\n",
       "        0.61234568, 0.62593516, 0.62216625, 0.61868687, 0.62468514,\n",
       "        0.64141414, 0.63076923, 0.62626263, 0.63171355, 0.63888889,\n",
       "        0.62468514, 0.59351621, 0.63224181, 0.5970516 , 0.61904762,\n",
       "        0.61403509, 0.62      , 0.63451777, 0.62155388, 0.63316583,\n",
       "        0.64070352, 0.64231738, 0.64467005]),\n",
       " 'split2_test_jaccard': array([0.64588529, 0.63054187, 0.63942308, 0.65326633, 0.62222222,\n",
       "        0.6375    , 0.65432099, 0.6345679 , 0.64837905, 0.64824121,\n",
       "        0.62469734, 0.63480392, 0.61442786, 0.62686567, 0.64427861,\n",
       "        0.59223301, 0.62376238, 0.64987406, 0.64039409, 0.6453202 ,\n",
       "        0.64303178, 0.6372549 , 0.65365854, 0.63      , 0.63366337,\n",
       "        0.6372796 , 0.59213759, 0.62009804, 0.65185185, 0.6318408 ,\n",
       "        0.63523573, 0.63814181, 0.6372549 , 0.65012407, 0.60340633,\n",
       "        0.63882064, 0.64411028, 0.61928934, 0.61111111, 0.62807882,\n",
       "        0.6425    , 0.61313869, 0.6475    , 0.63300493, 0.64837905,\n",
       "        0.64039409, 0.63569682, 0.64851485, 0.625     , 0.60591133,\n",
       "        0.65      , 0.63703704, 0.60431655, 0.65162907, 0.64619165,\n",
       "        0.61650485, 0.6641791 , 0.65185185, 0.63814181, 0.63592233,\n",
       "        0.63366337, 0.65422886, 0.61633663, 0.60945274, 0.63909774,\n",
       "        0.63325183, 0.617866  , 0.64390244, 0.63569682, 0.62529833,\n",
       "        0.64320388, 0.65281174, 0.6281407 , 0.6425    , 0.6125    ,\n",
       "        0.655     , 0.60401003, 0.63383838, 0.62068966, 0.64778325,\n",
       "        0.65356265, 0.64963504, 0.63157895, 0.65121951, 0.60049628,\n",
       "        0.62094763, 0.63959391, 0.63793103, 0.64127764, 0.62716049,\n",
       "        0.63793103, 0.64213198, 0.64179104, 0.63592233, 0.64009662,\n",
       "        0.62068966, 0.44075829, 0.33333333, 0.36180905, 0.44859813,\n",
       "        0.27417641, 0.44600939, 0.43632075, 0.00285714, 0.45605701,\n",
       "        0.44075829, 0.23271889, 0.44418052, 0.45243619, 0.349     ,\n",
       "        0.42959427, 0.45070423, 0.18004338, 0.43809524, 0.43661972,\n",
       "        0.        , 0.44675926, 0.44600939, 0.29918864, 0.43822844,\n",
       "        0.41304348, 0.02133333, 0.37562189, 0.4138756 , 0.02      ,\n",
       "        0.42548077, 0.45518868, 0.        , 0.44811321, 0.44470588,\n",
       "        0.        , 0.44392523, 0.40435835, 0.23971631, 0.42718447,\n",
       "        0.43467933, 0.344     , 0.43333333, 0.45390071, 0.0483871 ,\n",
       "        0.44339623, 0.43705463, 0.        , 0.44964871, 0.52447552,\n",
       "        0.33980583, 0.47699758, 0.54878049, 0.1511335 , 0.50724638,\n",
       "        0.50851582, 0.22938144, 0.48456057, 0.52267303, 0.22798742,\n",
       "        0.49880668, 0.4679803 , 0.08641975, 0.52997602, 0.55036855,\n",
       "        0.10917031, 0.48448687, 0.5157385 , 0.19384058, 0.51566265,\n",
       "        0.52811736, 0.22802548, 0.50952381, 0.47584541, 0.25      ,\n",
       "        0.46682464, 0.53669725, 0.12955466, 0.50564334, 0.52      ,\n",
       "        0.03611111, 0.53026634, 0.53186275, 0.2870229 , 0.49391727,\n",
       "        0.52898551, 0.03846154, 0.53504673, 0.49759615, 0.12106538,\n",
       "        0.53186275, 0.54676259, 0.36405006, 0.53140097, 0.51089588,\n",
       "        0.20351759, 0.52195122, 0.58560794, 0.48699764, 0.51477833,\n",
       "        0.57739558, 0.5707196 , 0.54814815, 0.58924205, 0.58560794,\n",
       "        0.58518519, 0.58560794, 0.58823529, 0.59605911, 0.53366584,\n",
       "        0.57530864, 0.52345679, 0.54862843, 0.57352941, 0.58910891,\n",
       "        0.57985258, 0.58823529, 0.59313725, 0.59459459, 0.59213759,\n",
       "        0.59359606, 0.55392157, 0.47951807, 0.48668281, 0.5862069 ,\n",
       "        0.55639098, 0.57002457, 0.56823821, 0.58292683, 0.58968059,\n",
       "        0.58722359, 0.59950249, 0.59359606, 0.49033816, 0.51351351,\n",
       "        0.53117207, 0.55831266, 0.57960199, 0.55025126, 0.58706468,\n",
       "        0.56716418, 0.57425743, 0.5862069 , 0.57711443, 0.57142857,\n",
       "        0.60794045, 0.6044226 , 0.59512195, 0.6175    , 0.61029412,\n",
       "        0.62623762, 0.62871287, 0.61613692, 0.62128713, 0.63771712,\n",
       "        0.63118812, 0.63793103, 0.60643564, 0.57843137, 0.60447761,\n",
       "        0.635     , 0.60643564, 0.63703704, 0.62716049, 0.62990196,\n",
       "        0.61234568, 0.62899263, 0.62561576, 0.64197531, 0.62068966,\n",
       "        0.62437811, 0.60144928, 0.63703704, 0.62315271, 0.62653563,\n",
       "        0.62623762, 0.63341646, 0.62962963, 0.63861386, 0.62842893,\n",
       "        0.62807882, 0.59506173, 0.63408521, 0.62686567, 0.63092269,\n",
       "        0.61728395, 0.6234414 , 0.65      , 0.62128713, 0.63027295,\n",
       "        0.63027295, 0.63771712, 0.62871287]),\n",
       " 'split3_test_jaccard': array([0.56862745, 0.57356608, 0.5955335 , 0.60401003, 0.60150376,\n",
       "        0.58415842, 0.6       , 0.61042184, 0.585     , 0.60651629,\n",
       "        0.60794045, 0.61306533, 0.5839599 , 0.5875    , 0.57605985,\n",
       "        0.56857855, 0.5975    , 0.63224181, 0.61209068, 0.59493671,\n",
       "        0.62094763, 0.60344828, 0.59213759, 0.60591133, 0.59791123,\n",
       "        0.57711443, 0.57283951, 0.58145363, 0.60401003, 0.59343434,\n",
       "        0.57560976, 0.58968059, 0.6019656 , 0.62530414, 0.59653465,\n",
       "        0.60493827, 0.57537688, 0.59547739, 0.56997455, 0.60858586,\n",
       "        0.59259259, 0.59359606, 0.5955335 , 0.59801489, 0.60050251,\n",
       "        0.59398496, 0.6319797 , 0.6025    , 0.59899749, 0.5721393 ,\n",
       "        0.58717949, 0.60487805, 0.60651629, 0.59949622, 0.59057072,\n",
       "        0.59558824, 0.61881188, 0.6025    , 0.60447761, 0.59899749,\n",
       "        0.56435644, 0.59950249, 0.5994898 , 0.5920398 , 0.59547739,\n",
       "        0.58690176, 0.58955224, 0.58823529, 0.63      , 0.6       ,\n",
       "        0.60297767, 0.60837438, 0.59697733, 0.57462687, 0.58560794,\n",
       "        0.57635468, 0.59057072, 0.60552764, 0.58942065, 0.6       ,\n",
       "        0.6039604 , 0.57701711, 0.61363636, 0.6159601 , 0.60506329,\n",
       "        0.58168317, 0.58415842, 0.58      , 0.6019656 , 0.58542714,\n",
       "        0.61928934, 0.60099751, 0.58955224, 0.60987654, 0.59701493,\n",
       "        0.61042184, 0.38974359, 0.        , 0.42997543, 0.41625616,\n",
       "        0.33723653, 0.41769042, 0.43627451, 0.        , 0.44827586,\n",
       "        0.44471744, 0.00564972, 0.45073892, 0.41831683, 0.        ,\n",
       "        0.42821782, 0.4408867 , 0.14983165, 0.44827586, 0.42892157,\n",
       "        0.02368421, 0.4137931 , 0.4408867 , 0.341     , 0.44963145,\n",
       "        0.41871921, 0.01994302, 0.42647059, 0.45833333, 0.37720329,\n",
       "        0.43209877, 0.43564356, 0.34268537, 0.43316832, 0.43765281,\n",
       "        0.        , 0.44471744, 0.43950617, 0.1456044 , 0.43564356,\n",
       "        0.43181818, 0.02549575, 0.44334975, 0.42118227, 0.        ,\n",
       "        0.43382353, 0.43382353, 0.351     , 0.43209877, 0.49509804,\n",
       "        0.32225914, 0.49148418, 0.51870324, 0.1573499 , 0.50241546,\n",
       "        0.4987715 , 0.3426043 , 0.48418491, 0.49019608, 0.27946768,\n",
       "        0.49382716, 0.49881235, 0.36192714, 0.49379653, 0.51231527,\n",
       "        0.19410319, 0.48029557, 0.50995025, 0.33926702, 0.48410758,\n",
       "        0.49127182, 0.19689119, 0.51      , 0.4987715 , 0.25293132,\n",
       "        0.5058548 , 0.52853598, 0.25796178, 0.49127182, 0.51612903,\n",
       "        0.3296837 , 0.48300971, 0.49875312, 0.25223881, 0.49261084,\n",
       "        0.47215496, 0.04213483, 0.50240385, 0.51105651, 0.1352657 ,\n",
       "        0.48309179, 0.51824818, 0.12351946, 0.47536946, 0.49752475,\n",
       "        0.1388518 , 0.49752475, 0.49751244, 0.52      , 0.52605459,\n",
       "        0.53884712, 0.54433498, 0.54545455, 0.54567901, 0.55050505,\n",
       "        0.54926108, 0.54656863, 0.54926108, 0.5483871 , 0.51889169,\n",
       "        0.48395062, 0.47524752, 0.53517588, 0.53670886, 0.52130326,\n",
       "        0.55665025, 0.53349876, 0.54187192, 0.54791155, 0.54679803,\n",
       "        0.55147059, 0.52392947, 0.49751244, 0.51      , 0.5235732 ,\n",
       "        0.5199005 , 0.5175    , 0.55025126, 0.54411765, 0.53482587,\n",
       "        0.55637255, 0.54135338, 0.54950495, 0.4825    , 0.49246231,\n",
       "        0.47487437, 0.51879699, 0.5201005 , 0.53846154, 0.53233831,\n",
       "        0.5175    , 0.53960396, 0.51231527, 0.5270936 , 0.53580247,\n",
       "        0.57974684, 0.56658596, 0.5675    , 0.59351621, 0.6       ,\n",
       "        0.5925    , 0.60358056, 0.5839599 , 0.60201511, 0.58542714,\n",
       "        0.61696658, 0.60353535, 0.57605985, 0.585     , 0.57462687,\n",
       "        0.59445844, 0.58542714, 0.58734177, 0.59351621, 0.58690176,\n",
       "        0.60301508, 0.61111111, 0.59697733, 0.5879397 , 0.57425743,\n",
       "        0.58145363, 0.57721519, 0.57894737, 0.5775    , 0.58024691,\n",
       "        0.595     , 0.57425743, 0.58145363, 0.6122449 , 0.60606061,\n",
       "        0.5964467 , 0.58734177, 0.51879699, 0.5743073 , 0.57788945,\n",
       "        0.5875    , 0.5839599 , 0.5879397 , 0.59045226, 0.58481013,\n",
       "        0.59343434, 0.58560794, 0.59390863]),\n",
       " 'split4_test_jaccard': array([0.64339152, 0.62121212, 0.62278481, 0.63157895, 0.65316456,\n",
       "        0.64516129, 0.62935323, 0.6425    , 0.655     , 0.61858191,\n",
       "        0.64089776, 0.6375    , 0.6319797 , 0.60814249, 0.62273902,\n",
       "        0.62682927, 0.63341646, 0.61179361, 0.64321608, 0.62437811,\n",
       "        0.61313869, 0.62287105, 0.64164649, 0.63569682, 0.63659148,\n",
       "        0.61097257, 0.61691542, 0.65656566, 0.64588529, 0.6425    ,\n",
       "        0.62656642, 0.6318408 , 0.6275    , 0.61691542, 0.6372549 ,\n",
       "        0.60332542, 0.64467005, 0.65454545, 0.62406015, 0.65139949,\n",
       "        0.61363636, 0.62962963, 0.64070352, 0.65648855, 0.64179104,\n",
       "        0.6097561 , 0.63316583, 0.62469136, 0.64213198, 0.64303797,\n",
       "        0.63383838, 0.63307494, 0.64735516, 0.63476071, 0.6127451 ,\n",
       "        0.62155388, 0.61858191, 0.6182266 , 0.61670762, 0.61633663,\n",
       "        0.62690355, 0.63291139, 0.6443299 , 0.65914787, 0.6275    ,\n",
       "        0.61055276, 0.62593516, 0.62807882, 0.63316583, 0.59472422,\n",
       "        0.61234568, 0.6       , 0.61346633, 0.62216625, 0.6281407 ,\n",
       "        0.63567839, 0.63636364, 0.62690355, 0.61975309, 0.63432836,\n",
       "        0.60194175, 0.63840399, 0.62953995, 0.61425061, 0.6142132 ,\n",
       "        0.62189055, 0.61809045, 0.64572864, 0.65075377, 0.64661654,\n",
       "        0.62962963, 0.64735516, 0.64411028, 0.62926829, 0.64303797,\n",
       "        0.63797468, 0.43147208, 0.28135048, 0.44611529, 0.45522388,\n",
       "        0.01994302, 0.44974874, 0.47524752, 0.351     , 0.46268657,\n",
       "        0.460199  , 0.        , 0.45657568, 0.45316456, 0.        ,\n",
       "        0.46551724, 0.4638404 , 0.002849  , 0.46683047, 0.47014925,\n",
       "        0.3003003 , 0.47407407, 0.47029703, 0.        , 0.46633416,\n",
       "        0.45822785, 0.16850829, 0.46401985, 0.46534653, 0.33433735,\n",
       "        0.4638404 , 0.46650124, 0.351     , 0.4563591 , 0.46633416,\n",
       "        0.        , 0.4713217 , 0.43147208, 0.351     , 0.4638404 ,\n",
       "        0.4713217 , 0.14896373, 0.45363409, 0.46134663, 0.29158317,\n",
       "        0.45979899, 0.46095718, 0.        , 0.4675    , 0.52380952,\n",
       "        0.23898678, 0.51378446, 0.52882206, 0.15058824, 0.53300733,\n",
       "        0.55050505, 0.2739899 , 0.5315534 , 0.5325    , 0.44850498,\n",
       "        0.53634085, 0.51485149, 0.18533333, 0.48762376, 0.55189873,\n",
       "        0.06167401, 0.54568528, 0.53117207, 0.21807466, 0.53553299,\n",
       "        0.54187192, 0.19092332, 0.55696203, 0.55263158, 0.35326689,\n",
       "        0.49261084, 0.52685422, 0.30758017, 0.50890585, 0.55189873,\n",
       "        0.34156977, 0.54936709, 0.53030303, 0.        , 0.53634085,\n",
       "        0.50990099, 0.28614458, 0.5270936 , 0.52392947, 0.34505863,\n",
       "        0.54146341, 0.54135338, 0.0984456 , 0.50605327, 0.53056235,\n",
       "        0.34357224, 0.52567237, 0.54005168, 0.56701031, 0.50629723,\n",
       "        0.58883249, 0.57360406, 0.5879397 , 0.59493671, 0.58663366,\n",
       "        0.59090909, 0.58734177, 0.5825    , 0.58291457, 0.54707379,\n",
       "        0.5225    , 0.57692308, 0.56743003, 0.57506361, 0.57614213,\n",
       "        0.5959596 , 0.5920398 , 0.58186398, 0.58560794, 0.59090909,\n",
       "        0.59296482, 0.55784062, 0.54241645, 0.58524173, 0.57474227,\n",
       "        0.58838384, 0.58734177, 0.58942065, 0.58418367, 0.59193955,\n",
       "        0.59045226, 0.5875    , 0.58121827, 0.54615385, 0.54358974,\n",
       "        0.56060606, 0.55725191, 0.57721519, 0.54358974, 0.58585859,\n",
       "        0.58629442, 0.5839599 , 0.59398496, 0.58673469, 0.58542714,\n",
       "        0.62626263, 0.61403509, 0.60152284, 0.62531646, 0.63613232,\n",
       "        0.62086514, 0.62626263, 0.63037975, 0.61964736, 0.62972292,\n",
       "        0.63383838, 0.62373737, 0.60401003, 0.60512821, 0.59798995,\n",
       "        0.62720403, 0.62148338, 0.625     , 0.64885496, 0.62404092,\n",
       "        0.6377551 , 0.6437659 , 0.6281407 , 0.64705882, 0.58663366,\n",
       "        0.61403509, 0.60659898, 0.62690355, 0.61654135, 0.62944162,\n",
       "        0.64705882, 0.6319797 , 0.61712846, 0.64194373, 0.62972292,\n",
       "        0.65194805, 0.60677083, 0.61712846, 0.62025316, 0.61209068,\n",
       "        0.63171355, 0.60598504, 0.62406015, 0.6259542 , 0.6319797 ,\n",
       "        0.61964736, 0.61964736, 0.64540816]),\n",
       " 'mean_test_jaccard': array([0.61969643, 0.61284974, 0.62483706, 0.63194828, 0.63450407,\n",
       "        0.63369539, 0.63233442, 0.63520839, 0.62578619, 0.63215419,\n",
       "        0.62595822, 0.6333291 , 0.61162162, 0.60826501, 0.61694854,\n",
       "        0.6105424 , 0.62709155, 0.63935844, 0.62445198, 0.61850521,\n",
       "        0.63013227, 0.63004108, 0.63826848, 0.63122323, 0.62674152,\n",
       "        0.61914294, 0.6122486 , 0.62733838, 0.6345588 , 0.62235499,\n",
       "        0.62010403, 0.62945516, 0.62665295, 0.63016394, 0.61829986,\n",
       "        0.62419851, 0.62591044, 0.62666883, 0.61373379, 0.6298228 ,\n",
       "        0.62984718, 0.62310998, 0.63001423, 0.63744787, 0.63359352,\n",
       "        0.62134728, 0.63949303, 0.63270882, 0.62507521, 0.61322262,\n",
       "        0.62660876, 0.63038038, 0.62302299, 0.62947556, 0.62584591,\n",
       "        0.6118312 , 0.62863903, 0.63309706, 0.62608406, 0.61975144,\n",
       "        0.61630584, 0.63386566, 0.62589463, 0.63004392, 0.63269798,\n",
       "        0.61731772, 0.61667724, 0.63042169, 0.62553015, 0.61655008,\n",
       "        0.61590332, 0.62356155, 0.62008371, 0.62149334, 0.61648835,\n",
       "        0.62787387, 0.62164089, 0.62695723, 0.61680002, 0.64040963,\n",
       "        0.62057633, 0.63152674, 0.63545711, 0.63596533, 0.61826852,\n",
       "        0.61912476, 0.62369905, 0.61868877, 0.62532481, 0.62542725,\n",
       "        0.62941789, 0.62487911, 0.63078214, 0.62423407, 0.63268165,\n",
       "        0.63292414, 0.43076018, 0.12293676, 0.41414436, 0.4403924 ,\n",
       "        0.19627119, 0.44306888, 0.45004204, 0.07077143, 0.45487902,\n",
       "        0.44970367, 0.10035132, 0.44917902, 0.44316911, 0.1398    ,\n",
       "        0.43933162, 0.45495374, 0.1144551 , 0.45238787, 0.44779388,\n",
       "        0.12769443, 0.44577764, 0.45080033, 0.19576936, 0.451479  ,\n",
       "        0.43688481, 0.16551612, 0.42918529, 0.44761442, 0.25301254,\n",
       "        0.44329336, 0.44854764, 0.27520233, 0.44876151, 0.45229787,\n",
       "        0.02562929, 0.45416563, 0.43286109, 0.21726414, 0.43574891,\n",
       "        0.44635551, 0.17026724, 0.44739339, 0.44746818, 0.13963962,\n",
       "        0.44884324, 0.44996715, 0.14370969, 0.45011463, 0.50214918,\n",
       "        0.31023695, 0.49131306, 0.53875422, 0.19028346, 0.52640822,\n",
       "        0.52188198, 0.26009714, 0.50322169, 0.52799387, 0.29437624,\n",
       "        0.52351705, 0.49268765, 0.26262914, 0.51020544, 0.53177166,\n",
       "        0.15289528, 0.51617808, 0.51717297, 0.31487574, 0.52202714,\n",
       "        0.52860864, 0.22592729, 0.53571543, 0.50984752, 0.22409679,\n",
       "        0.49627446, 0.53352713, 0.23434476, 0.50995639, 0.52436433,\n",
       "        0.25200415, 0.52143748, 0.52059765, 0.21981791, 0.52082034,\n",
       "        0.50050848, 0.15847902, 0.51417835, 0.51278142, 0.24159479,\n",
       "        0.52485143, 0.54277008, 0.24949109, 0.50916371, 0.51817106,\n",
       "        0.21848462, 0.52079227, 0.53752151, 0.52603212, 0.51847248,\n",
       "        0.56961595, 0.56558802, 0.56403519, 0.58308198, 0.57989696,\n",
       "        0.58089197, 0.57889954, 0.57949928, 0.58050666, 0.53646851,\n",
       "        0.5294817 , 0.52248285, 0.55258064, 0.56804348, 0.56657444,\n",
       "        0.57971986, 0.57874375, 0.58058707, 0.5814644 , 0.57873641,\n",
       "        0.58361219, 0.53470214, 0.51991514, 0.5384004 , 0.56385731,\n",
       "        0.56185095, 0.56163438, 0.57499848, 0.57645012, 0.5767096 ,\n",
       "        0.58263641, 0.58299296, 0.58336386, 0.52262165, 0.5296727 ,\n",
       "        0.53050407, 0.55143233, 0.56214291, 0.5475724 , 0.56981963,\n",
       "        0.56223452, 0.56639964, 0.57130755, 0.56840214, 0.5713582 ,\n",
       "        0.60661009, 0.59560634, 0.59363987, 0.60864851, 0.61860108,\n",
       "        0.61760344, 0.61818028, 0.61542925, 0.61775319, 0.61760769,\n",
       "        0.63043785, 0.62094612, 0.60279639, 0.59390194, 0.5852043 ,\n",
       "        0.61760634, 0.61443691, 0.61844585, 0.62575591, 0.61901418,\n",
       "        0.61986127, 0.62377693, 0.62059604, 0.6274204 , 0.60062567,\n",
       "        0.60774576, 0.59874458, 0.61429797, 0.61217619, 0.61062846,\n",
       "        0.62413663, 0.61517637, 0.61463224, 0.62470519, 0.62474087,\n",
       "        0.62326972, 0.59503811, 0.60303114, 0.60260102, 0.60769084,\n",
       "        0.61121205, 0.60747927, 0.61960427, 0.61446256, 0.6201152 ,\n",
       "        0.61992721, 0.61784036, 0.62353994]),\n",
       " 'std_test_jaccard': array([0.02793007, 0.02044327, 0.01650344, 0.01619162, 0.0199765 ,\n",
       "        0.02601612, 0.01804079, 0.01369091, 0.02690917, 0.01889498,\n",
       "        0.01096945, 0.01945254, 0.0187667 , 0.01370149, 0.0235235 ,\n",
       "        0.02597642, 0.01604899, 0.01561857, 0.01551549, 0.01660787,\n",
       "        0.01177563, 0.01654742, 0.02359041, 0.01627194, 0.01470888,\n",
       "        0.02359416, 0.02847568, 0.02598674, 0.01932111, 0.01657808,\n",
       "        0.02840954, 0.02058843, 0.01424792, 0.01518897, 0.01704764,\n",
       "        0.01704791, 0.02603567, 0.02130876, 0.02457755, 0.01594513,\n",
       "        0.02299527, 0.02244814, 0.01852979, 0.02130152, 0.01697272,\n",
       "        0.017293  , 0.00731764, 0.01727757, 0.01456884, 0.02368901,\n",
       "        0.02173305, 0.01348777, 0.01613819, 0.01846911, 0.02111974,\n",
       "        0.01470324, 0.01812325, 0.0207177 , 0.01330461, 0.01223113,\n",
       "        0.0274374 , 0.01849774, 0.01631414, 0.02770097, 0.02359652,\n",
       "        0.02061731, 0.01393946, 0.02786896, 0.01121186, 0.01594422,\n",
       "        0.01503238, 0.0188466 , 0.01352505, 0.02493683, 0.01675007,\n",
       "        0.0273453 , 0.02076313, 0.01132091, 0.01683967, 0.02236764,\n",
       "        0.01870518, 0.02974925, 0.01762228, 0.017085  , 0.01506017,\n",
       "        0.02026799, 0.02133312, 0.02567072, 0.01907579, 0.02237226,\n",
       "        0.00606505, 0.0180453 , 0.02090841, 0.01039051, 0.01915945,\n",
       "        0.01584795, 0.02276696, 0.15146086, 0.03491144, 0.01321409,\n",
       "        0.15439365, 0.01362103, 0.0142814 , 0.14011866, 0.00469587,\n",
       "        0.00668948, 0.1210052 , 0.00430438, 0.01593577, 0.17121963,\n",
       "        0.01580343, 0.00930488, 0.096704  , 0.00963831, 0.0143006 ,\n",
       "        0.14704717, 0.01968896, 0.01017772, 0.16053409, 0.00903725,\n",
       "        0.0202729 , 0.13115159, 0.02939333, 0.01801209, 0.12655224,\n",
       "        0.01356123, 0.01123372, 0.13776314, 0.00987841, 0.01057785,\n",
       "        0.05125858, 0.01005195, 0.01659752, 0.13292631, 0.01670004,\n",
       "        0.01431988, 0.14630685, 0.00869105, 0.01429365, 0.15374858,\n",
       "        0.00986122, 0.01259427, 0.144729  , 0.01124351, 0.01843754,\n",
       "        0.04034155, 0.01466196, 0.02072318, 0.05348572, 0.01827767,\n",
       "        0.0187756 , 0.0800871 , 0.01785496, 0.02478533, 0.11585602,\n",
       "        0.02371907, 0.01513825, 0.10994119, 0.02260143, 0.0204645 ,\n",
       "        0.07057604, 0.04088894, 0.00900264, 0.09961655, 0.02870443,\n",
       "        0.02161076, 0.06723053, 0.0231371 , 0.03275503, 0.08379809,\n",
       "        0.02499158, 0.00591029, 0.075723  , 0.025063  , 0.01390991,\n",
       "        0.1126486 , 0.02391774, 0.01386773, 0.11065017, 0.02338552,\n",
       "        0.02192636, 0.12068743, 0.01891934, 0.02086352, 0.11918231,\n",
       "        0.03011487, 0.01723738, 0.11431451, 0.0201509 , 0.01761194,\n",
       "        0.10230122, 0.01394185, 0.02922104, 0.02905253, 0.01730832,\n",
       "        0.02188414, 0.01072711, 0.01569059, 0.0187913 , 0.01560369,\n",
       "        0.0160104 , 0.01623362, 0.01548514, 0.01718105, 0.01234788,\n",
       "        0.0318288 , 0.03247089, 0.01238135, 0.01596372, 0.02409094,\n",
       "        0.01453768, 0.0237034 , 0.02019358, 0.01824294, 0.01706144,\n",
       "        0.01624576, 0.01918006, 0.02757692, 0.03532383, 0.02146796,\n",
       "        0.02451151, 0.02377443, 0.01431445, 0.01656562, 0.02135061,\n",
       "        0.01330555, 0.02132881, 0.01783792, 0.03005674, 0.02370464,\n",
       "        0.03363446, 0.01727441, 0.02220812, 0.00778339, 0.01995639,\n",
       "        0.0243201 , 0.01559661, 0.03001939, 0.02107963, 0.01840066,\n",
       "        0.01595484, 0.01625411, 0.01578078, 0.01157882, 0.01344428,\n",
       "        0.02136085, 0.01265823, 0.01730898, 0.01165341, 0.01827893,\n",
       "        0.00689508, 0.01218662, 0.01423714, 0.01109294, 0.01540754,\n",
       "        0.01787168, 0.01698962, 0.0187979 , 0.01838248, 0.02097139,\n",
       "        0.01472261, 0.01347153, 0.01485512, 0.02110963, 0.01809146,\n",
       "        0.01435366, 0.01752112, 0.02024528, 0.01760009, 0.02032729,\n",
       "        0.01923845, 0.0229215 , 0.01722602, 0.01646355, 0.01099694,\n",
       "        0.0180684 , 0.00641325, 0.04291667, 0.01895725, 0.01822405,\n",
       "        0.01455832, 0.01399276, 0.02234476, 0.01270721, 0.01822508,\n",
       "        0.01587171, 0.02112063, 0.02084804]),\n",
       " 'rank_test_jaccard': array([ 88, 121,  61,  22,  10,  12,  20,   8,  54,  21,  50,  14, 125,\n",
       "        130, 106, 128,  43,   3,  64,  95,  30,  32,   4,  24,  45,  90,\n",
       "        122,  42,   9,  75,  83,  37,  47,  29,  97,  66,  51,  46, 119,\n",
       "         35,  34,  73,  33,   5,  13,  78,   2,  17,  59, 120,  48,  28,\n",
       "         74,  36,  53, 124,  39,  15,  49,  87, 111,  11,  52,  31,  18,\n",
       "        105, 108,  27,  56, 109, 112,  70,  84,  77, 110,  40,  76,  44,\n",
       "        107,   1,  81,  23,   7,   6,  98,  91,  69,  93,  58,  57,  38,\n",
       "         60,  25,  65,  19,  16, 254, 284, 256, 249, 273, 248, 233, 287,\n",
       "        226, 235, 286, 236, 247, 281, 250, 225, 285, 228, 240, 283, 245,\n",
       "        231, 274, 230, 251, 277, 255, 241, 263, 246, 239, 260, 238, 229,\n",
       "        288, 227, 253, 272, 252, 244, 276, 243, 242, 282, 237, 234, 280,\n",
       "        232, 220, 258, 224, 182, 275, 195, 203, 262, 219, 194, 259, 199,\n",
       "        223, 261, 215, 189, 279, 212, 211, 257, 202, 193, 268, 186, 217,\n",
       "        269, 222, 188, 267, 216, 198, 264, 204, 207, 270, 205, 221, 278,\n",
       "        213, 214, 266, 197, 181, 265, 218, 210, 271, 206, 184, 196, 209,\n",
       "        166, 171, 172, 147, 154, 151, 157, 156, 153, 185, 192, 201, 178,\n",
       "        168, 169, 155, 158, 152, 150, 159, 145, 187, 208, 183, 173, 176,\n",
       "        177, 162, 161, 160, 149, 148, 146, 200, 191, 190, 179, 175, 180,\n",
       "        165, 174, 170, 164, 167, 163, 134, 140, 143, 129,  94, 104,  99,\n",
       "        113, 101, 102,  26,  79, 136, 142, 144, 103, 117,  96,  55,  92,\n",
       "         86,  68,  80,  41, 138, 131, 139, 118, 123, 127,  67, 114, 115,\n",
       "         63,  62,  72, 141, 135, 137, 132, 126, 133,  89, 116,  82,  85,\n",
       "        100,  71]),\n",
       " 'split0_test_neg_log_loss': array([-0.34913998, -0.35710446, -0.35794215, -0.35689807, -0.34288886,\n",
       "        -0.33696361, -0.35104645, -0.34228687, -0.39152833, -0.37322209,\n",
       "        -0.35856515, -0.36996899, -0.37187757, -0.36047121, -0.34937011,\n",
       "        -0.36552485, -0.33939672, -0.3304321 , -0.36332885, -0.34600811,\n",
       "        -0.36474723, -0.34672415, -0.34780937, -0.36425629, -0.35242301,\n",
       "        -0.34952419, -0.34945833, -0.35190888, -0.35791141, -0.35933766,\n",
       "        -0.38622738, -0.37150152, -0.37023805, -0.3679384 , -0.38269347,\n",
       "        -0.36364069, -0.34397971, -0.35165771, -0.34751154, -0.34925235,\n",
       "        -0.3367437 , -0.3347585 , -0.33904266, -0.36189987, -0.34904383,\n",
       "        -0.34219003, -0.33791398, -0.34749921, -0.34886867, -0.35929221,\n",
       "        -0.34560753, -0.35185859, -0.34825642, -0.36852483, -0.37677529,\n",
       "        -0.36619342, -0.40856862, -0.41757948, -0.36968983, -0.39793163,\n",
       "        -0.36735952, -0.34365781, -0.3492735 , -0.3445731 , -0.36074351,\n",
       "        -0.35558056, -0.36338312, -0.34453943, -0.36761504, -0.38454743,\n",
       "        -0.39425058, -0.36217257, -0.34916623, -0.35511487, -0.35185938,\n",
       "        -0.3516834 , -0.35682648, -0.3487164 , -0.39371098, -0.35684146,\n",
       "        -0.36802797, -0.38144025, -0.35147654, -0.3629439 , -0.35050834,\n",
       "        -0.33905547, -0.34938359, -0.375305  , -0.35848771, -0.3493853 ,\n",
       "        -0.37746433, -0.36322265, -0.3602916 , -0.36010723, -0.38055997,\n",
       "        -0.37599987, -0.50535511, -0.66915383, -0.49983977, -0.49585479,\n",
       "        -0.91334807, -0.49884071, -0.4953559 , -0.64335932, -0.49560565,\n",
       "        -0.49592229, -0.65328019, -0.49355064, -0.51322506, -0.78935905,\n",
       "        -0.51227763, -0.49695098, -0.65500634, -0.4945517 , -0.49631765,\n",
       "        -0.6829145 , -0.49691796, -0.4992439 , -0.70258511, -0.49519873,\n",
       "        -0.50206667, -0.70567618, -0.5079194 , -0.49745884, -0.69821404,\n",
       "        -0.49812385, -0.49986144, -0.69953198, -0.49969612, -0.49938872,\n",
       "        -0.66597265, -0.49547298, -0.513827  , -0.6754576 , -0.50676934,\n",
       "        -0.49655908, -0.65832026, -0.50068987, -0.49854409, -0.66061694,\n",
       "        -0.49890218, -0.4971193 , -0.65366324, -0.49695832, -0.46347149,\n",
       "        -0.84993051, -0.47741056, -0.43606309, -0.70770896, -0.43879019,\n",
       "        -0.43786474, -0.63808886, -0.4491417 , -0.43594069, -0.63807403,\n",
       "        -0.43966801, -0.46476398, -0.90280974, -0.46089968, -0.44706201,\n",
       "        -0.78486   , -0.45803985, -0.43259949, -0.63540548, -0.4439552 ,\n",
       "        -0.43607627, -0.70212847, -0.43019835, -0.46129608, -0.68374874,\n",
       "        -0.47175253, -0.43523874, -0.77453725, -0.451706  , -0.44215759,\n",
       "        -0.79211639, -0.45017676, -0.439335  , -0.80735799, -0.42543908,\n",
       "        -0.46212046, -0.68320235, -0.45176387, -0.45884496, -0.68858414,\n",
       "        -0.46239708, -0.43100143, -0.73794052, -0.45014413, -0.45011783,\n",
       "        -0.68561944, -0.43807255, -0.42025452, -0.44484034, -0.45107725,\n",
       "        -0.42241768, -0.40242859, -0.40159995, -0.3924245 , -0.39104555,\n",
       "        -0.39350027, -0.39329585, -0.3901358 , -0.39481413, -0.43400836,\n",
       "        -0.41342924, -0.4389878 , -0.42428627, -0.39604976, -0.40542317,\n",
       "        -0.39064311, -0.39046221, -0.38915365, -0.39862243, -0.40021101,\n",
       "        -0.39694793, -0.43594787, -0.43907553, -0.41408701, -0.40289191,\n",
       "        -0.40634519, -0.40783749, -0.39211677, -0.39667459, -0.40021392,\n",
       "        -0.39318346, -0.39099968, -0.39127277, -0.4158762 , -0.42877478,\n",
       "        -0.44301232, -0.41138841, -0.41280562, -0.41854082, -0.39888124,\n",
       "        -0.40856948, -0.4122214 , -0.39680196, -0.39944872, -0.39922603,\n",
       "        -0.37640466, -0.39319209, -0.37514253, -0.36270037, -0.36336484,\n",
       "        -0.37102239, -0.36006277, -0.3575075 , -0.35762893, -0.35302352,\n",
       "        -0.3571754 , -0.36437097, -0.37482794, -0.37037533, -0.38973158,\n",
       "        -0.36518564, -0.3584926 , -0.3686548 , -0.35522736, -0.36437664,\n",
       "        -0.36691692, -0.35837329, -0.35389409, -0.35339327, -0.37230983,\n",
       "        -0.3659331 , -0.38840949, -0.36203493, -0.36107737, -0.37074649,\n",
       "        -0.35230502, -0.36295037, -0.35343027, -0.36124181, -0.35139515,\n",
       "        -0.35135085, -0.37808358, -0.36843487, -0.37272982, -0.37356867,\n",
       "        -0.3646016 , -0.37069886, -0.36747773, -0.37069684, -0.36232391,\n",
       "        -0.36006226, -0.36598199, -0.35979666]),\n",
       " 'split1_test_neg_log_loss': array([-0.37288944, -0.37909648, -0.38757726, -0.35354978, -0.36244224,\n",
       "        -0.3666939 , -0.3657559 , -0.39003669, -0.36894178, -0.39093796,\n",
       "        -0.37785066, -0.37693202, -0.37240673, -0.38156141, -0.37586338,\n",
       "        -0.36938561, -0.36572069, -0.3643413 , -0.38442337, -0.39444497,\n",
       "        -0.37889382, -0.39112615, -0.36679699, -0.36793298, -0.34668615,\n",
       "        -0.37720925, -0.35885626, -0.36108865, -0.36342169, -0.36563807,\n",
       "        -0.36274528, -0.37147786, -0.37278785, -0.3804337 , -0.37625313,\n",
       "        -0.36651208, -0.38333007, -0.36980413, -0.36810684, -0.36273981,\n",
       "        -0.35966979, -0.38166778, -0.3804791 , -0.35832645, -0.36578219,\n",
       "        -0.35883024, -0.36173548, -0.36788884, -0.35700819, -0.37251854,\n",
       "        -0.36325717, -0.3740494 , -0.37255743, -0.38847548, -0.37613826,\n",
       "        -0.40503096, -0.37369373, -0.34012358, -0.38172633, -0.39877719,\n",
       "        -0.36354893, -0.36224442, -0.37053431, -0.37217871, -0.34608503,\n",
       "        -0.35908767, -0.37442376, -0.38588201, -0.37179433, -0.42645509,\n",
       "        -0.40419669, -0.40921586, -0.37670885, -0.37872236, -0.394167  ,\n",
       "        -0.37054358, -0.36038919, -0.37285874, -0.36453678, -0.35743297,\n",
       "        -0.39199263, -0.3930629 , -0.38179887, -0.4045029 , -0.36241289,\n",
       "        -0.36149044, -0.36778806, -0.36579077, -0.37646055, -0.38791243,\n",
       "        -0.39170507, -0.39854415, -0.37597249, -0.36793341, -0.38978451,\n",
       "        -0.40535607, -0.49061038, -0.66113338, -0.53598807, -0.4947181 ,\n",
       "        -0.66870254, -0.48277118, -0.48027051, -0.66269342, -0.48516795,\n",
       "        -0.47677135, -0.72523891, -0.48734529, -0.48799168, -0.67384792,\n",
       "        -0.49451874, -0.48353354, -0.72972029, -0.48564259, -0.48436134,\n",
       "        -0.64654952, -0.48141029, -0.47932761, -0.66221775, -0.48049347,\n",
       "        -0.48732836, -0.69347484, -0.494581  , -0.48611768, -0.70136638,\n",
       "        -0.48382051, -0.48635462, -0.74644862, -0.48095893, -0.47970256,\n",
       "        -0.68425075, -0.48385389, -0.49031478, -0.76393046, -0.51145051,\n",
       "        -0.48591405, -0.68785356, -0.49120196, -0.48246359, -0.69167301,\n",
       "        -0.48086263, -0.47664986, -0.71699937, -0.48197188, -0.4562196 ,\n",
       "        -0.72472391, -0.45370515, -0.41677436, -0.70621617, -0.42406803,\n",
       "        -0.43817802, -0.66035324, -0.43775485, -0.42459779, -0.71666861,\n",
       "        -0.42624753, -0.4473598 , -0.81728626, -0.43304153, -0.42511927,\n",
       "        -0.64975386, -0.42155375, -0.44086179, -0.57198105, -0.42309124,\n",
       "        -0.42059525, -0.66413421, -0.42706613, -0.43617976, -0.6778784 ,\n",
       "        -0.44007286, -0.43367999, -0.77686259, -0.42511315, -0.43360363,\n",
       "        -0.70963647, -0.43113553, -0.42628274, -0.76716352, -0.42888652,\n",
       "        -0.44229049, -1.19755821, -0.45046744, -0.43732738, -0.53926671,\n",
       "        -0.41908023, -0.42451766, -0.70857928, -0.43746958, -0.42755119,\n",
       "        -0.67350389, -0.42959479, -0.43424129, -0.42330265, -0.42512614,\n",
       "        -0.39839964, -0.41467567, -0.40702899, -0.39600735, -0.39825109,\n",
       "        -0.39915356, -0.4008621 , -0.39781898, -0.39926577, -0.41745399,\n",
       "        -0.44048335, -0.43657703, -0.41103899, -0.40786884, -0.40719114,\n",
       "        -0.41075295, -0.40068691, -0.39963095, -0.39315359, -0.39718479,\n",
       "        -0.39752075, -0.43945616, -0.42544344, -0.42372784, -0.42387401,\n",
       "        -0.40381638, -0.41088438, -0.4010368 , -0.40459831, -0.40106149,\n",
       "        -0.39652593, -0.39686085, -0.39650187, -0.4173487 , -0.4209681 ,\n",
       "        -0.41153228, -0.4070416 , -0.40673927, -0.41498267, -0.40612397,\n",
       "        -0.40052812, -0.40264691, -0.40393771, -0.40117976, -0.40153749,\n",
       "        -0.41275839, -0.38053215, -0.39375908, -0.391782  , -0.37508276,\n",
       "        -0.36587736, -0.36909579, -0.37202196, -0.37105966, -0.37109494,\n",
       "        -0.36887895, -0.36846192, -0.37576522, -0.40669484, -0.4133644 ,\n",
       "        -0.37362629, -0.36562352, -0.36688817, -0.37099036, -0.37048584,\n",
       "        -0.3662293 , -0.37229885, -0.36573113, -0.36498145, -0.38772979,\n",
       "        -0.39209458, -0.39069082, -0.38054219, -0.37751243, -0.37077269,\n",
       "        -0.37527328, -0.37750541, -0.37124802, -0.36622574, -0.37234861,\n",
       "        -0.3785134 , -0.40643807, -0.38340315, -0.39153755, -0.37776436,\n",
       "        -0.38079382, -0.37877564, -0.37379779, -0.37380276, -0.37540506,\n",
       "        -0.37378976, -0.3684324 , -0.36586834]),\n",
       " 'split2_test_neg_log_loss': array([-0.34369005, -0.35064443, -0.3516122 , -0.33354095, -0.357282  ,\n",
       "        -0.35687364, -0.34707842, -0.36348038, -0.34415783, -0.36552025,\n",
       "        -0.38063624, -0.37651715, -0.36180757, -0.3611996 , -0.34773601,\n",
       "        -0.37096598, -0.35201701, -0.35245937, -0.36237159, -0.35538339,\n",
       "        -0.36993556, -0.36767324, -0.35159744, -0.39263679, -0.35210878,\n",
       "        -0.34721224, -0.36217531, -0.36837927, -0.35165139, -0.35922261,\n",
       "        -0.34683138, -0.38143347, -0.35698405, -0.3746346 , -0.39779666,\n",
       "        -0.36429126, -0.34192015, -0.35441106, -0.35851355, -0.35967571,\n",
       "        -0.33491582, -0.34864405, -0.3387189 , -0.34887536, -0.35125069,\n",
       "        -0.35649784, -0.36080764, -0.35968862, -0.34802277, -0.35703713,\n",
       "        -0.34132036, -0.37289847, -0.39217004, -0.34790121, -0.3804933 ,\n",
       "        -0.38175346, -0.35426412, -0.37299263, -0.4057153 , -0.39923547,\n",
       "        -0.35486808, -0.35018992, -0.36539181, -0.3597521 , -0.36755129,\n",
       "        -0.3670181 , -0.40121167, -0.38521793, -0.36927882, -0.39232798,\n",
       "        -0.37629121, -0.38803346, -0.34917805, -0.34939788, -0.35546154,\n",
       "        -0.35064937, -0.35917175, -0.34333369, -0.39301128, -0.36987726,\n",
       "        -0.37194013, -0.35707329, -0.40164593, -0.3588313 , -0.36527793,\n",
       "        -0.34440015, -0.35012764, -0.34203567, -0.34385379, -0.35319427,\n",
       "        -0.36460966, -0.36361941, -0.35946639, -0.38569929, -0.39417073,\n",
       "        -0.38420897, -0.49930322, -0.64289401, -0.5361278 , -0.49538134,\n",
       "        -0.7155034 , -0.49381295, -0.49516232, -0.6842042 , -0.48870517,\n",
       "        -0.49481817, -0.71033171, -0.49880341, -0.5003134 , -0.7420228 ,\n",
       "        -0.50045345, -0.49439461, -0.67045172, -0.50333481, -0.49382111,\n",
       "        -0.65178099, -0.49225742, -0.49303252, -0.73141276, -0.49418881,\n",
       "        -0.5084094 , -0.69686244, -0.52237219, -0.50526897, -0.65487887,\n",
       "        -0.50265815, -0.4910479 , -0.65395765, -0.49454214, -0.49626187,\n",
       "        -0.64742515, -0.49479025, -0.520385  , -0.69789943, -0.50353027,\n",
       "        -0.50029982, -0.77645842, -0.50193016, -0.49269629, -0.68632961,\n",
       "        -0.49684011, -0.49538586, -0.66048753, -0.49332012, -0.44211824,\n",
       "        -0.67758013, -0.46511058, -0.431735  , -0.66067844, -0.43709464,\n",
       "        -0.44055382, -0.63171413, -0.46268521, -0.43879842, -0.73521668,\n",
       "        -0.45024227, -0.45305283, -0.70355295, -0.43663099, -0.43278084,\n",
       "        -0.81315536, -0.46280695, -0.43778115, -0.79924389, -0.44433936,\n",
       "        -0.43831711, -0.78184583, -0.44234453, -0.46077249, -1.00854716,\n",
       "        -0.47099515, -0.43867504, -0.67438269, -0.45905608, -0.44684529,\n",
       "        -0.65240207, -0.43682034, -0.42952117, -0.69714402, -0.44707416,\n",
       "        -0.4435735 , -0.76534646, -0.44424088, -0.45207392, -0.68146677,\n",
       "        -0.43376824, -0.4308743 , -0.72726956, -0.43913284, -0.44499313,\n",
       "        -0.63184486, -0.44056676, -0.40073232, -0.4471996 , -0.43211826,\n",
       "        -0.39143247, -0.40140355, -0.41128988, -0.39475868, -0.40153531,\n",
       "        -0.39469211, -0.40175669, -0.39469384, -0.39265404, -0.42667443,\n",
       "        -0.40205613, -0.4261021 , -0.42301719, -0.40070091, -0.3994966 ,\n",
       "        -0.39406199, -0.39670186, -0.39751765, -0.38864641, -0.39923906,\n",
       "        -0.39542956, -0.41526896, -0.44737239, -0.4423178 , -0.39502409,\n",
       "        -0.41425288, -0.40488037, -0.40620867, -0.39143214, -0.39525971,\n",
       "        -0.39307133, -0.39845319, -0.39059902, -0.44264368, -0.43577258,\n",
       "        -0.42297043, -0.40772885, -0.39736999, -0.41731605, -0.39611214,\n",
       "        -0.40472734, -0.39877195, -0.39630019, -0.40108177, -0.40480244,\n",
       "        -0.37494315, -0.39081077, -0.37998683, -0.36306417, -0.37364898,\n",
       "        -0.36306576, -0.35457158, -0.36156502, -0.36309646, -0.35878447,\n",
       "        -0.35607681, -0.3514371 , -0.38860899, -0.3972546 , -0.39797371,\n",
       "        -0.36415746, -0.37693949, -0.37315978, -0.35750747, -0.36658039,\n",
       "        -0.36130529, -0.36019287, -0.35649047, -0.35890332, -0.37498035,\n",
       "        -0.38016582, -0.39077617, -0.36967752, -0.37038535, -0.36511198,\n",
       "        -0.36020081, -0.35109487, -0.36197923, -0.35740925, -0.35352225,\n",
       "        -0.35562349, -0.40020443, -0.37377989, -0.37141856, -0.36472846,\n",
       "        -0.36704173, -0.36445221, -0.36197044, -0.36145646, -0.36448845,\n",
       "        -0.35489435, -0.36341656, -0.36187582]),\n",
       " 'split3_test_neg_log_loss': array([-0.39648148, -0.3987754 , -0.39586371, -0.38307092, -0.39948265,\n",
       "        -0.39956557, -0.36874703, -0.39771355, -0.39836494, -0.41982193,\n",
       "        -0.4169934 , -0.43975899, -0.40092999, -0.40184005, -0.39699822,\n",
       "        -0.39379115, -0.37898864, -0.36214568, -0.39449685, -0.41128083,\n",
       "        -0.38273203, -0.41461953, -0.42058085, -0.39414191, -0.3914755 ,\n",
       "        -0.39681946, -0.39670434, -0.38182233, -0.38488237, -0.38373578,\n",
       "        -0.39778564, -0.3997638 , -0.406963  , -0.40356414, -0.38086878,\n",
       "        -0.39453339, -0.39615576, -0.39221679, -0.38714751, -0.37523254,\n",
       "        -0.38282214, -0.38455173, -0.38071075, -0.3949741 , -0.38977613,\n",
       "        -0.41262309, -0.38749598, -0.40176562, -0.38775285, -0.40494083,\n",
       "        -0.39202637, -0.39359221, -0.37688398, -0.39282147, -0.40633189,\n",
       "        -0.3867794 , -0.36848016, -0.41316923, -0.42892184, -0.40403379,\n",
       "        -0.40697624, -0.38349501, -0.38847133, -0.38694583, -0.39067635,\n",
       "        -0.38325009, -0.42862467, -0.40265626, -0.38031423, -0.440525  ,\n",
       "        -0.40443091, -0.42363634, -0.39441263, -0.40420669, -0.38936443,\n",
       "        -0.38401698, -0.38808727, -0.38769163, -0.41893212, -0.42434767,\n",
       "        -0.4005094 , -0.42464189, -0.44496797, -0.4198873 , -0.38363933,\n",
       "        -0.39586933, -0.39580043, -0.41742346, -0.41014848, -0.39805004,\n",
       "        -0.39305725, -0.39158357, -0.4071716 , -0.41051126, -0.4025109 ,\n",
       "        -0.38639614, -0.52156453, -0.66041636, -0.50526694, -0.48986139,\n",
       "        -0.6700338 , -0.49383255, -0.48642341, -0.6457983 , -0.48273905,\n",
       "        -0.48317242, -0.66960932, -0.48586056, -0.50514357, -0.66008334,\n",
       "        -0.49342577, -0.48524181, -0.70097507, -0.48325436, -0.48988145,\n",
       "        -0.6855327 , -0.49282826, -0.48383025, -0.71330932, -0.48124898,\n",
       "        -0.49098565, -0.66013463, -0.49264183, -0.48380742, -0.69171649,\n",
       "        -0.48907325, -0.48755519, -0.73694873, -0.48856758, -0.48093026,\n",
       "        -0.64261977, -0.48115038, -0.48835125, -0.66403174, -0.49399755,\n",
       "        -0.49042487, -0.65775107, -0.48316788, -0.48793686, -0.66262612,\n",
       "        -0.48252712, -0.48870025, -0.72399104, -0.48143018, -0.44809275,\n",
       "        -0.67893335, -0.44356004, -0.42625042, -0.68958021, -0.43364853,\n",
       "        -0.43120672, -0.82938699, -0.44669898, -0.44276656, -0.69193193,\n",
       "        -0.43979861, -0.43993166, -0.77605054, -0.43373767, -0.43863774,\n",
       "        -0.6618504 , -0.43329433, -0.43293695, -0.76063506, -0.43935586,\n",
       "        -0.4304285 , -0.65523175, -0.42125698, -0.43788553, -0.71946622,\n",
       "        -0.44778156, -0.42983426, -0.86273032, -0.43392546, -0.42027744,\n",
       "        -0.79049977, -0.43494582, -0.43496962, -0.74288235, -0.43077465,\n",
       "        -0.44980042, -0.66846234, -0.44683374, -0.43757999, -0.68826424,\n",
       "        -0.44313031, -0.4440458 , -0.74625967, -0.43929764, -0.43444478,\n",
       "        -0.81235935, -0.43291656, -0.42718204, -0.42146454, -0.41276021,\n",
       "        -0.41172424, -0.4135312 , -0.41334608, -0.41308131, -0.4116104 ,\n",
       "        -0.41568974, -0.41426819, -0.41295801, -0.41254521, -0.41781872,\n",
       "        -0.43052954, -0.42965216, -0.41274392, -0.41175739, -0.41790259,\n",
       "        -0.41177527, -0.41220624, -0.41391741, -0.41022532, -0.4140556 ,\n",
       "        -0.41281065, -0.41686644, -0.42138982, -0.42220103, -0.41403546,\n",
       "        -0.41328669, -0.41475617, -0.41185402, -0.41363454, -0.4125332 ,\n",
       "        -0.41418091, -0.41307281, -0.41152988, -0.43125971, -0.42270064,\n",
       "        -0.42801226, -0.41871548, -0.41703547, -0.41538036, -0.41656026,\n",
       "        -0.41913379, -0.41589006, -0.41743214, -0.41659499, -0.41644814,\n",
       "        -0.39502221, -0.42055246, -0.40387173, -0.38402683, -0.38661871,\n",
       "        -0.38360245, -0.37156843, -0.38393874, -0.37533459, -0.37205925,\n",
       "        -0.37492341, -0.37942038, -0.38948194, -0.39170306, -0.39517671,\n",
       "        -0.38215055, -0.38464535, -0.37830353, -0.37615367, -0.37572858,\n",
       "        -0.37192745, -0.37604172, -0.37221343, -0.38300781, -0.3910211 ,\n",
       "        -0.39202897, -0.39692242, -0.38029969, -0.37706182, -0.38382313,\n",
       "        -0.38763726, -0.38597038, -0.37628105, -0.37115539, -0.37540118,\n",
       "        -0.37600663, -0.38438757, -0.41406735, -0.39367152, -0.38318873,\n",
       "        -0.38098016, -0.3787165 , -0.3824835 , -0.38405425, -0.38121039,\n",
       "        -0.38188254, -0.38026934, -0.37798701]),\n",
       " 'split4_test_neg_log_loss': array([-0.35274709, -0.35023556, -0.34851957, -0.36052545, -0.3502681 ,\n",
       "        -0.35666319, -0.37634893, -0.3834926 , -0.36907314, -0.40216923,\n",
       "        -0.38038705, -0.3851362 , -0.34131364, -0.35440749, -0.34187972,\n",
       "        -0.35742259, -0.36153828, -0.3732822 , -0.36325765, -0.38363034,\n",
       "        -0.39763227, -0.40463046, -0.40418592, -0.38849613, -0.34738928,\n",
       "        -0.35518579, -0.34249694, -0.36085048, -0.34759891, -0.34811117,\n",
       "        -0.37235192, -0.37357642, -0.37491763, -0.384903  , -0.40147671,\n",
       "        -0.43043272, -0.35353254, -0.34547357, -0.34705237, -0.35159346,\n",
       "        -0.36585914, -0.35618345, -0.36020241, -0.33960497, -0.35891408,\n",
       "        -0.38528018, -0.36395608, -0.36010705, -0.34110492, -0.34432716,\n",
       "        -0.33805044, -0.35146434, -0.34847981, -0.3619037 , -0.36906774,\n",
       "        -0.37647421, -0.3652298 , -0.39762264, -0.39412575, -0.40573332,\n",
       "        -0.35409035, -0.33890074, -0.34362305, -0.36386087, -0.35704444,\n",
       "        -0.36934794, -0.38294453, -0.38795039, -0.36006403, -0.40255261,\n",
       "        -0.43404829, -0.41607035, -0.34394898, -0.34935825, -0.34456505,\n",
       "        -0.34977992, -0.35701526, -0.36168809, -0.3662965 , -0.36666945,\n",
       "        -0.41392192, -0.38239311, -0.40835534, -0.40762109, -0.34203632,\n",
       "        -0.35671409, -0.3584839 , -0.34444386, -0.34707319, -0.34338045,\n",
       "        -0.3625654 , -0.35161574, -0.35233294, -0.37704994, -0.39214892,\n",
       "        -0.36288899, -0.49951337, -0.67915036, -0.48293524, -0.47734454,\n",
       "        -0.65776698, -0.48260265, -0.47323087, -0.77582811, -0.47421891,\n",
       "        -0.47608644, -0.64999376, -0.47308042, -0.49405474, -0.64782017,\n",
       "        -0.47870557, -0.47655276, -0.67011956, -0.47626522, -0.47181283,\n",
       "        -0.7054266 , -0.47380822, -0.46942512, -0.65718698, -0.47068997,\n",
       "        -0.4881688 , -0.63622011, -0.48916122, -0.47648079, -0.70928899,\n",
       "        -0.47623698, -0.47590341, -0.72340362, -0.47495191, -0.47318852,\n",
       "        -0.68360798, -0.47188544, -0.50371719, -0.95996301, -0.47663333,\n",
       "        -0.47603403, -0.71075652, -0.48091745, -0.47374763, -0.7331556 ,\n",
       "        -0.47669248, -0.47651552, -0.6858675 , -0.47266341, -0.42055647,\n",
       "        -0.96073673, -0.42331957, -0.41814268, -0.64895252, -0.41911935,\n",
       "        -0.41862185, -0.78055254, -0.42252796, -0.41501364, -0.63740957,\n",
       "        -0.41392061, -0.42911294, -0.80118617, -0.44400908, -0.40794687,\n",
       "        -0.71209291, -0.40347854, -0.41002063, -0.74901972, -0.41057527,\n",
       "        -0.41719596, -0.74917133, -0.4107577 , -0.43826971, -0.85198315,\n",
       "        -0.43747742, -0.40515797, -0.68870751, -0.41956967, -0.41804734,\n",
       "        -0.67958385, -0.41315096, -0.41212173, -0.72302542, -0.41271829,\n",
       "        -0.43318776, -0.70254925, -0.42582749, -0.42087421, -0.70192158,\n",
       "        -0.4237956 , -0.41823875, -0.66395919, -0.42859889, -0.42901093,\n",
       "        -0.72534555, -0.43117919, -0.40520742, -0.39619979, -0.4244508 ,\n",
       "        -0.3858809 , -0.39266899, -0.38058816, -0.37927913, -0.38096928,\n",
       "        -0.38135927, -0.38847939, -0.38079157, -0.38139106, -0.41579511,\n",
       "        -0.42369496, -0.40155374, -0.39331485, -0.39021211, -0.38831706,\n",
       "        -0.37966555, -0.37694518, -0.38422466, -0.37671265, -0.37930733,\n",
       "        -0.37870879, -0.40019408, -0.40737951, -0.39038734, -0.39511145,\n",
       "        -0.38511608, -0.38621523, -0.38095839, -0.39098986, -0.37853764,\n",
       "        -0.38042749, -0.38075236, -0.38491873, -0.40900149, -0.41749092,\n",
       "        -0.40060438, -0.39703708, -0.38813682, -0.4094246 , -0.38404659,\n",
       "        -0.38588322, -0.38375733, -0.38362517, -0.39130643, -0.38509084,\n",
       "        -0.35794526, -0.36238299, -0.38182895, -0.3545729 , -0.35077568,\n",
       "        -0.35575591, -0.34914486, -0.34465328, -0.35172893, -0.34802235,\n",
       "        -0.34525456, -0.34572377, -0.3681368 , -0.36489181, -0.37788482,\n",
       "        -0.35076469, -0.34988866, -0.35585442, -0.34492122, -0.34966869,\n",
       "        -0.34740817, -0.34283606, -0.34341693, -0.34282483, -0.3799656 ,\n",
       "        -0.36314129, -0.36455643, -0.35106477, -0.35359895, -0.35564796,\n",
       "        -0.34381433, -0.34967011, -0.35418184, -0.34359983, -0.34519008,\n",
       "        -0.33712521, -0.37565175, -0.37342089, -0.36715318, -0.35824088,\n",
       "        -0.35113062, -0.36044932, -0.34681487, -0.34782006, -0.35352899,\n",
       "        -0.34876196, -0.35029579, -0.34397982]),\n",
       " 'mean_test_neg_log_loss': array([-0.36298961, -0.36717126, -0.36830298, -0.35751703, -0.36247277,\n",
       "        -0.36335198, -0.36179535, -0.37540202, -0.37441321, -0.39033429,\n",
       "        -0.3828865 , -0.38966267, -0.3696671 , -0.37189595, -0.36236949,\n",
       "        -0.37141804, -0.35953227, -0.35653213, -0.37357566, -0.37814953,\n",
       "        -0.37878818, -0.38495471, -0.37819412, -0.38149282, -0.35801654,\n",
       "        -0.36519019, -0.36193824, -0.36480992, -0.36109316, -0.36320906,\n",
       "        -0.37318832, -0.37955062, -0.37637812, -0.38229477, -0.38781775,\n",
       "        -0.38388203, -0.36378365, -0.36271265, -0.36166636, -0.35969877,\n",
       "        -0.35600212, -0.3611611 , -0.35983076, -0.36073615, -0.36295338,\n",
       "        -0.37108428, -0.36238183, -0.36738987, -0.35655148, -0.36762318,\n",
       "        -0.35605237, -0.3687726 , -0.36766954, -0.37192534, -0.38176129,\n",
       "        -0.38324629, -0.37404729, -0.38829751, -0.39603581, -0.40114228,\n",
       "        -0.36936862, -0.35569758, -0.3634588 , -0.36546212, -0.36442013,\n",
       "        -0.36685687, -0.39011755, -0.3812492 , -0.36981329, -0.40928162,\n",
       "        -0.40264354, -0.39982572, -0.36268295, -0.36736001, -0.36708348,\n",
       "        -0.36133465, -0.36429799, -0.36285771, -0.38729753, -0.37503376,\n",
       "        -0.38927841, -0.38772229, -0.39764893, -0.3907573 , -0.36077496,\n",
       "        -0.3595059 , -0.36431672, -0.36899975, -0.36720474, -0.3663845 ,\n",
       "        -0.37788034, -0.3737171 , -0.37104701, -0.38026023, -0.39183501,\n",
       "        -0.38297001, -0.50326932, -0.66254959, -0.51203156, -0.49063203,\n",
       "        -0.72507096, -0.49037201, -0.4860886 , -0.68237667, -0.48528734,\n",
       "        -0.48535414, -0.68169078, -0.48772806, -0.50014569, -0.70262665,\n",
       "        -0.49587623, -0.48733474, -0.6852546 , -0.48860973, -0.48723888,\n",
       "        -0.67444086, -0.48744443, -0.48497188, -0.69334238, -0.48436399,\n",
       "        -0.49539178, -0.67847364, -0.50133513, -0.48982674, -0.69109295,\n",
       "        -0.48998255, -0.48814451, -0.71205812, -0.48774334, -0.48589439,\n",
       "        -0.66477526, -0.48543059, -0.50331904, -0.75225645, -0.4984762 ,\n",
       "        -0.48984637, -0.69822797, -0.49158146, -0.48707769, -0.68688026,\n",
       "        -0.48716491, -0.48687416, -0.68820174, -0.48526878, -0.44609171,\n",
       "        -0.77838093, -0.45262118, -0.42579311, -0.68262726, -0.43054415,\n",
       "        -0.43328503, -0.70801915, -0.44376174, -0.43142342, -0.68386017,\n",
       "        -0.43397541, -0.44684424, -0.80017713, -0.44166379, -0.43030934,\n",
       "        -0.72434251, -0.43583469, -0.43084   , -0.70325704, -0.43226339,\n",
       "        -0.42852262, -0.71050232, -0.42632474, -0.44688071, -0.78832473,\n",
       "        -0.4536159 , -0.4285172 , -0.75544407, -0.43787407, -0.43218626,\n",
       "        -0.72484771, -0.43324588, -0.42844605, -0.74751466, -0.42897854,\n",
       "        -0.44619452, -0.80342372, -0.44382668, -0.44134009, -0.65990069,\n",
       "        -0.43643429, -0.42973559, -0.71680164, -0.43892862, -0.43722357,\n",
       "        -0.70573462, -0.43446597, -0.41752352, -0.42660138, -0.42910653,\n",
       "        -0.40197099, -0.4049416 , -0.40277061, -0.39511019, -0.39668232,\n",
       "        -0.39687899, -0.39973244, -0.39527964, -0.39613404, -0.42235012,\n",
       "        -0.42203865, -0.42657456, -0.41288024, -0.4013178 , -0.40366611,\n",
       "        -0.39737978, -0.39540048, -0.39688886, -0.39347208, -0.39799956,\n",
       "        -0.39628354, -0.4215467 , -0.42813214, -0.4185442 , -0.40618738,\n",
       "        -0.40456344, -0.40491473, -0.39843493, -0.39946589, -0.39752119,\n",
       "        -0.39547783, -0.39602778, -0.39496445, -0.42322596, -0.4251414 ,\n",
       "        -0.42122634, -0.40838228, -0.40441743, -0.4151289 , -0.40034484,\n",
       "        -0.40376839, -0.40265753, -0.39961943, -0.40192234, -0.40142099,\n",
       "        -0.38341473, -0.38949409, -0.38691782, -0.37122926, -0.36989819,\n",
       "        -0.36786477, -0.36088868, -0.3639373 , -0.36376971, -0.36059691,\n",
       "        -0.36046183, -0.36188283, -0.37936418, -0.38618393, -0.39482625,\n",
       "        -0.36717693, -0.36711792, -0.36857214, -0.36096002, -0.36536803,\n",
       "        -0.36275743, -0.36194856, -0.35834921, -0.36062213, -0.38120133,\n",
       "        -0.37867275, -0.38627106, -0.36872382, -0.36792719, -0.36922045,\n",
       "        -0.36384614, -0.36543823, -0.36342408, -0.35992641, -0.35957145,\n",
       "        -0.35972392, -0.38895308, -0.38262123, -0.37930212, -0.37149822,\n",
       "        -0.36890959, -0.37061851, -0.36650887, -0.36756607, -0.36739136,\n",
       "        -0.36387818, -0.36567922, -0.36190153]),\n",
       " 'std_test_neg_log_loss': array([0.01943659, 0.01898592, 0.01953665, 0.01583052, 0.01964261,\n",
       "        0.02052144, 0.01102674, 0.02008071, 0.01918569, 0.01958834,\n",
       "        0.0189287 , 0.02550625, 0.0192637 , 0.01755479, 0.02090066,\n",
       "        0.01212842, 0.01328752, 0.01463461, 0.01335933, 0.0242678 ,\n",
       "        0.01136822, 0.02476065, 0.02909543, 0.01276089, 0.01689383,\n",
       "        0.01904514, 0.0187187 , 0.00998122, 0.0130592 , 0.01171385,\n",
       "        0.01777352, 0.01074957, 0.0165184 , 0.01206181, 0.00994417,\n",
       "        0.02598324, 0.02193312, 0.01678748, 0.01493075, 0.00922356,\n",
       "        0.01814081, 0.0192156 , 0.01865648, 0.01879682, 0.01465747,\n",
       "        0.0249992 , 0.01571477, 0.01838439, 0.0163957 , 0.02069176,\n",
       "        0.01998116, 0.01578949, 0.01705395, 0.01673147, 0.01282909,\n",
       "        0.01285259, 0.01839545, 0.0286983 , 0.02038482, 0.00312974,\n",
       "        0.01947269, 0.01595608, 0.01596312, 0.01398701, 0.01485455,\n",
       "        0.00961753, 0.02288386, 0.01943188, 0.00654768, 0.02105024,\n",
       "        0.01874435, 0.02225231, 0.01959359, 0.02137019, 0.02051283,\n",
       "        0.01371223, 0.01196954, 0.01612003, 0.02016741, 0.02517602,\n",
       "        0.01728006, 0.02190344, 0.03083118, 0.02495893, 0.01415824,\n",
       "        0.01990194, 0.01709751, 0.02729455, 0.02431612, 0.02217341,\n",
       "        0.01290189, 0.01808956, 0.01964308, 0.01739602, 0.00708322,\n",
       "        0.01390136, 0.01028738, 0.01193398, 0.0209549 , 0.00698113,\n",
       "        0.09620408, 0.00653712, 0.0085732 , 0.04895768, 0.00703089,\n",
       "        0.00855065, 0.03057635, 0.0086574 , 0.00872808, 0.05425212,\n",
       "        0.01088702, 0.00744841, 0.02678613, 0.00940219, 0.00870605,\n",
       "        0.02211942, 0.00853528, 0.01042619, 0.02901522, 0.00922585,\n",
       "        0.00837397, 0.0261719 , 0.0122907 , 0.01024515, 0.01896944,\n",
       "        0.00953614, 0.00773449, 0.03305613, 0.0089378 , 0.01013906,\n",
       "        0.01747737, 0.00886262, 0.01260927, 0.10946657, 0.01232738,\n",
       "        0.00849604, 0.0438534 , 0.00865693, 0.00851781, 0.02624007,\n",
       "        0.00896965, 0.00886097, 0.02855385, 0.008786  , 0.01467292,\n",
       "        0.1107273 , 0.01850601, 0.00749473, 0.02387304, 0.00765519,\n",
       "        0.0079634 , 0.08121082, 0.01328627, 0.0101915 , 0.04008258,\n",
       "        0.01259239, 0.01201759, 0.06441354, 0.01037319, 0.01329014,\n",
       "        0.06506746, 0.02226158, 0.01085868, 0.08536645, 0.01332166,\n",
       "        0.00833977, 0.04870349, 0.01039426, 0.01157892, 0.12682771,\n",
       "        0.01489214, 0.01202001, 0.06834963, 0.0151887 , 0.01147216,\n",
       "        0.05720811, 0.01192125, 0.0093107 , 0.03775098, 0.0110237 ,\n",
       "        0.00957001, 0.19981756, 0.00938385, 0.01319642, 0.06068054,\n",
       "        0.01540721, 0.00856837, 0.02927162, 0.0068527 , 0.00889216,\n",
       "        0.06108446, 0.00417514, 0.01275821, 0.01853899, 0.01262145,\n",
       "        0.01338581, 0.00822172, 0.01179817, 0.01078633, 0.01027164,\n",
       "        0.01110025, 0.0087718 , 0.0105421 , 0.01010289, 0.00695827,\n",
       "        0.01333018, 0.01334072, 0.01112716, 0.00778936, 0.00970741,\n",
       "        0.01229842, 0.01163665, 0.0101794 , 0.01105861, 0.0110841 ,\n",
       "        0.01081405, 0.01445954, 0.01395209, 0.01684103, 0.01124779,\n",
       "        0.01050631, 0.00990662, 0.01088688, 0.00861705, 0.0110511 ,\n",
       "        0.0108487 , 0.01054258, 0.00905957, 0.0121036 , 0.00645177,\n",
       "        0.01444271, 0.00702495, 0.01048515, 0.00313204, 0.01078915,\n",
       "        0.01086687, 0.01130383, 0.01105181, 0.00818976, 0.01008717,\n",
       "        0.01879035, 0.01894919, 0.01045783, 0.0141636 , 0.01207377,\n",
       "        0.00928709, 0.00848432, 0.01329859, 0.0086027 , 0.00959536,\n",
       "        0.01040457, 0.01206043, 0.0083357 , 0.01598114, 0.01154695,\n",
       "        0.01047137, 0.01246716, 0.00748772, 0.01125594, 0.00874591,\n",
       "        0.00838037, 0.01172091, 0.00992961, 0.01335722, 0.00718798,\n",
       "        0.01236356, 0.01121945, 0.01123537, 0.00931213, 0.00915498,\n",
       "        0.01577565, 0.01433695, 0.00909834, 0.00938504, 0.01203416,\n",
       "        0.01559356, 0.01223301, 0.0164532 , 0.01103746, 0.00896444,\n",
       "        0.01118099, 0.00739673, 0.01198087, 0.0122294 , 0.00981281,\n",
       "        0.01221611, 0.00961574, 0.01095534]),\n",
       " 'rank_test_neg_log_loss': array([ 40,  64,  75,   6,  34,  42,  27, 100,  98, 135, 117, 133,  83,\n",
       "         92,  32,  90,  10,   4,  95, 103, 106, 122, 104, 113,   7,  54,\n",
       "         30,  53,  23,  41,  94, 109, 101, 115, 128, 121,  46,  36,  26,\n",
       "         12,   2,  24,  14,  19,  39,  88,  33,  68,   5,  71,   3,  78,\n",
       "         72,  93, 114, 119,  97, 129, 146, 162,  82,   1,  44,  57,  52,\n",
       "         61, 134, 112,  84, 178, 167, 160,  35,  67,  62,  25,  50,  38,\n",
       "        126,  99, 131, 127, 154, 136,  20,   9,  51,  80,  66,  59, 102,\n",
       "         96,  87, 110, 137, 118, 254, 258, 256, 247, 281, 246, 232, 263,\n",
       "        228, 229, 262, 239, 252, 272, 250, 237, 266, 242, 236, 260, 238,\n",
       "        226, 270, 225, 249, 261, 253, 243, 269, 245, 241, 277, 240, 231,\n",
       "        259, 230, 255, 283, 251, 244, 271, 248, 234, 267, 235, 233, 268,\n",
       "        227, 219, 285, 223, 189, 264, 201, 207, 275, 217, 203, 265, 208,\n",
       "        221, 287, 216, 200, 279, 210, 202, 273, 205, 196, 276, 190, 222,\n",
       "        286, 224, 195, 284, 213, 204, 280, 206, 194, 282, 197, 220, 288,\n",
       "        218, 215, 257, 211, 199, 278, 214, 212, 274, 209, 181, 192, 198,\n",
       "        166, 175, 169, 141, 149, 150, 159, 142, 147, 186, 185, 191, 179,\n",
       "        163, 170, 152, 143, 151, 138, 155, 148, 184, 193, 182, 176, 173,\n",
       "        174, 156, 157, 153, 144, 145, 140, 187, 188, 183, 177, 172, 180,\n",
       "        161, 171, 168, 158, 165, 164, 120, 132, 125,  89,  85,  73,  21,\n",
       "         49,  45,  17,  16,  28, 108, 123, 139,  65,  63,  76,  22,  55,\n",
       "         37,  31,   8,  18, 111, 105, 124,  77,  74,  81,  47,  56,  43,\n",
       "         15,  11,  13, 130, 116, 107,  91,  79,  86,  60,  70,  69,  48,\n",
       "         58,  29])}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRIAL_3_MLP.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL THREE NEG LOG LOSS RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'tanh',\n",
       " 'classifier__alpha': 0.001,\n",
       " 'classifier__hidden_layer_sizes': (5,),\n",
       " 'classifier__learning_rate': 'invscaling',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL THREE NEG LOG LOSS RESULTS ---------------------------------------------\")\n",
    "TRIAL_3_MLP.cv_results_['rank_test_neg_log_loss']\n",
    "TRIAL_3_MLP.cv_results_['params'][ np.argmin(TRIAL_3_MLP.cv_results_['rank_test_neg_log_loss']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL THREE F1 RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.001,\n",
       " 'classifier__hidden_layer_sizes': (10,),\n",
       " 'classifier__learning_rate': 'adaptive',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL THREE F1 RESULTS ---------------------------------------------\")\n",
    "TRIAL_3_MLP.cv_results_['rank_test_f1_micro']\n",
    "TRIAL_3_MLP.cv_results_['params'][ np.argmin(TRIAL_3_MLP.cv_results_['rank_test_f1_micro']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL THREE ROC AUC RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.0001,\n",
       " 'classifier__hidden_layer_sizes': (10,),\n",
       " 'classifier__learning_rate': 'constant',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL THREE ROC AUC RESULTS ---------------------------------------------\")\n",
    "TRIAL_3_MLP.cv_results_['rank_test_roc_auc_ovo']\n",
    "TRIAL_3_MLP.cv_results_['params'][ np.argmin(TRIAL_3_MLP.cv_results_['rank_test_roc_auc_ovo']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL THREE JACCARD SCORE RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'tanh',\n",
       " 'classifier__alpha': 0.01,\n",
       " 'classifier__hidden_layer_sizes': (15,),\n",
       " 'classifier__learning_rate': 'invscaling',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL THREE JACCARD SCORE RESULTS ---------------------------------------------\")\n",
    "TRIAL_3_MLP.cv_results_['rank_test_jaccard']\n",
    "TRIAL_3_MLP.cv_results_['params'][ np.argmin(TRIAL_3_MLP.cv_results_['rank_test_jaccard']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 3 Multilayered Perceptrons using best NEG LOG LOSS hyperparameters :0.8217315106905012\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 3 Multilayered Perceptrons using best F1 hyperparameters :0.8071503680336488\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 3 Multilayered Perceptrons using best ROC_AUC hyperparameters :0.8100946372239748\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 3 Multilayered Perceptrons using best JACCARD hyperparameters :0.8085524009814231\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best NEG LOG LOSS results to train on test set, \n",
    "bestMPLT3_1 = MLPClassifier(activation = 'tanh', alpha = 0.001, hidden_layer_sizes = (20,), \n",
    "                          learning_rate = 'invscaling',solver = 'adam')\n",
    "bestMPLT3_1.fit(X_train,y_train)\n",
    "y_pred3_1 = bestMPLT3_1.predict(X_test)\n",
    "print('Accuracy of Trial 3 Multilayered Perceptrons using best NEG LOG LOSS hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred3_1)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best F1 results to train on test set, \n",
    "bestMPLT3_2 = MLPClassifier(activation = 'logistic', alpha = 0.001, hidden_layer_sizes = (15,), \n",
    "                          learning_rate = 'adaptive',solver = 'lbfgs')\n",
    "bestMPLT3_2.fit(X_train,y_train)\n",
    "y_pred3_2 = bestMPLT3_2.predict(X_test)\n",
    "print('Accuracy of Trial 3 Multilayered Perceptrons using best F1 hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred3_2)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best ROC_AUC results to train on test set,\n",
    "bestMPLT3_3 = MLPClassifier(activation = 'logistic', alpha = 0.0001, hidden_layer_sizes = (10,), \n",
    "                          learning_rate = 'constant',solver = 'lbfgs')\n",
    "bestMPLT3_3.fit(X_train,y_train)\n",
    "y_pred3_3 = bestMPLT3_3.predict(X_test)\n",
    "print('Accuracy of Trial 3 Multilayered Perceptrons using best ROC_AUC hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred3_3)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best JACCARD results to train on test set, \n",
    "bestMPLT3_4 = MLPClassifier(activation = 'tanh', alpha = 0.01, hidden_layer_sizes = (15,), \n",
    "                          learning_rate = 'invscaling',solver = 'lbfgs')\n",
    "bestMPLT3_4.fit(X_train,y_train)\n",
    "y_pred3_4 = bestMPLT3_4.predict(X_test)\n",
    "print('Accuracy of Trial 3 Multilayered Perceptrons using best JACCARD hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred3_4)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTI LAYERED PERCEPTRON TRIAL FOUR ON GAMMA PARTICLES DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:    6.4s\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:    8.0s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    8.7s\n",
      "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed:    9.8s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   11.9s\n",
      "[Parallel(n_jobs=-1)]: Done 189 tasks      | elapsed:   12.8s\n",
      "[Parallel(n_jobs=-1)]: Done 210 tasks      | elapsed:   14.0s\n",
      "[Parallel(n_jobs=-1)]: Done 233 tasks      | elapsed:   16.5s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:   17.7s\n",
      "[Parallel(n_jobs=-1)]: Done 281 tasks      | elapsed:   19.8s\n",
      "[Parallel(n_jobs=-1)]: Done 306 tasks      | elapsed:   22.0s\n",
      "[Parallel(n_jobs=-1)]: Done 333 tasks      | elapsed:   24.1s\n",
      "[Parallel(n_jobs=-1)]: Done 360 tasks      | elapsed:   26.4s\n",
      "[Parallel(n_jobs=-1)]: Done 389 tasks      | elapsed:   28.0s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:   30.9s\n",
      "[Parallel(n_jobs=-1)]: Done 449 tasks      | elapsed:   32.6s\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:   35.5s\n",
      "[Parallel(n_jobs=-1)]: Done 513 tasks      | elapsed:   39.6s\n",
      "[Parallel(n_jobs=-1)]: Done 546 tasks      | elapsed:   43.5s\n",
      "[Parallel(n_jobs=-1)]: Done 581 tasks      | elapsed:   47.4s\n",
      "[Parallel(n_jobs=-1)]: Done 616 tasks      | elapsed:   51.3s\n",
      "[Parallel(n_jobs=-1)]: Done 653 tasks      | elapsed:   55.9s\n",
      "[Parallel(n_jobs=-1)]: Done 690 tasks      | elapsed:   59.2s\n",
      "[Parallel(n_jobs=-1)]: Done 729 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 809 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 850 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 893 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 936 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 981 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1026 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1073 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1120 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1169 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1269 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1320 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1373 tasks      | elapsed:  2.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FOUR RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1440 out of 1440 | elapsed:  3.0min finished\n"
     ]
    }
   ],
   "source": [
    "ITERATIONS = 1\n",
    "for i in range(1):\n",
    "    #For reach \"trial\", randomly select 5000 data points\n",
    "    random5000DataPoints = gammaData.sample(n = 5000, replace = True)\n",
    "    #From that 5000, create x set and y set\n",
    "    xSet = random5000DataPoints.iloc[:,0:9].values\n",
    "    ySet = random5000DataPoints.iloc[:,10].values\n",
    "    ySet = encodeLabel.fit_transform(ySet)\n",
    "    # Run GridSearch to find best hyperparameters per 5000 data points \n",
    "    \n",
    "    pipe = Pipeline([('std', StandardScaler()),('classifier', MLPClassifier(max_iter = 250))])\n",
    "\n",
    "    search_space = [\n",
    "                {\n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                {\n",
    "                 \n",
    "                  \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['sgd'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                \n",
    "                {\n",
    "                \n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['adam'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                ]\n",
    "    \n",
    "    runHyperParamSearch = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=5), \n",
    "                          scoring=['recall_micro', 'f1_micro','precision_micro','roc_auc_ovo','jaccard','neg_log_loss'], \n",
    "                          refit=False, verbose=10, n_jobs = -1)\n",
    "    TRIAL_4_MLP = runHyperParamSearch.fit(xSet, ySet)\n",
    "    print(\"---------------------------------TRIAL FOUR RESULTS ---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.58980703, 0.58620286, 0.57289228, 0.81890512, 0.83492064,\n",
       "        0.80709562, 1.18842177, 1.28000245, 1.27329578, 1.53622136,\n",
       "        1.50599608, 1.54352427, 0.7080101 , 0.57369237, 0.54046507,\n",
       "        0.78257294, 0.83371701, 0.86214175, 1.21764951, 1.23145838,\n",
       "        1.34665885, 1.50029235, 1.62890191, 1.4755693 , 0.65115795,\n",
       "        0.60992208, 0.61713161, 0.75865307, 0.81039748, 0.87225122,\n",
       "        1.20803943, 1.24507208, 1.29341378, 1.64791579, 1.50259242,\n",
       "        1.5982748 , 0.62944183, 0.62083578, 0.54416795, 0.86694541,\n",
       "        0.81109781, 0.92309585, 1.37688484, 1.24557133, 1.33655143,\n",
       "        1.70846906, 1.69215407, 1.60357833, 0.79467707, 0.6800848 ,\n",
       "        0.70310745, 0.96853285, 0.93520365, 0.99325285, 1.48757825,\n",
       "        1.47887268, 1.5492331 , 2.03955336, 1.88492184, 1.8984314 ,\n",
       "        0.79158206, 0.64655561, 0.71781659, 0.94031081, 0.99685621,\n",
       "        0.98554811, 1.3609251 , 1.39249816, 1.46966305, 1.86860633,\n",
       "        1.85379333, 1.71537437, 0.72762489, 0.59671445, 0.66997442,\n",
       "        0.92119222, 0.91688972, 0.93090215, 1.38749213, 1.38679361,\n",
       "        1.45235004, 1.8844223 , 1.91784787, 1.67143664, 0.6847878 ,\n",
       "        0.65436215, 0.62403784, 0.93700714, 0.98404722, 1.0122704 ,\n",
       "        1.33865237, 1.45345039, 1.43203034, 1.86740646, 1.66993642,\n",
       "        1.64041066, 2.41497445, 0.23039746, 2.29787607, 2.44790506,\n",
       "        0.16584315, 2.51746554, 2.80821519, 0.21058092, 2.94943624,\n",
       "        2.93632517, 0.34639859, 2.89098587, 2.3617281 , 0.25521975,\n",
       "        2.36813645, 2.42838821, 0.52725325, 2.42588639, 2.61114607,\n",
       "        0.18505926, 2.68921251, 2.75576978, 0.36391287, 2.80661311,\n",
       "        2.26424713, 0.27763891, 2.27215433, 2.37664375, 0.15313148,\n",
       "        2.46832323, 2.60013633, 0.58330245, 2.69762015, 2.7469624 ,\n",
       "        0.24891391, 2.75556936, 2.28616548, 0.2123827 , 2.27005239,\n",
       "        2.40686975, 0.27333512, 2.38395023, 2.61675067, 0.19807053,\n",
       "        2.65027981, 2.70782824, 0.43577538, 2.72624407, 2.33781013,\n",
       "        1.20313468, 2.29727607, 2.56050191, 0.32497921, 2.50615559,\n",
       "        2.81141815, 0.51964703, 2.81452045, 2.93632565, 1.55824032,\n",
       "        2.85885873, 2.31829338, 0.18836155, 2.37023849, 2.29227133,\n",
       "        0.41155419, 2.51045866, 2.67900429, 0.70040255, 2.76047406,\n",
       "        2.90589914, 0.48301592, 2.91880999, 2.34001265, 0.27503662,\n",
       "        2.38264885, 2.53157744, 0.62513785, 2.5680089 , 2.87657399,\n",
       "        1.17240906, 2.88738298, 2.97025414, 0.63004193, 2.93932781,\n",
       "        2.33740964, 0.29275188, 2.4195807 , 2.53908343, 0.50183172,\n",
       "        2.53938484, 2.92961941, 0.92319365, 2.86936755, 3.1613183 ,\n",
       "        1.2171463 , 3.10637121, 2.69171462, 2.70392466, 2.6269587 ,\n",
       "        2.63997021, 2.6681941 , 2.70542727, 2.86426344, 2.86446319,\n",
       "        2.89719133, 3.00388298, 3.00308228, 3.03360858, 2.52457113,\n",
       "        2.48283486, 2.46131606, 2.63926978, 2.64377389, 2.67099705,\n",
       "        2.93792706, 2.88388062, 2.94903636, 3.08765526, 3.06353469,\n",
       "        3.08405194, 2.5001502 , 2.51055932, 2.49934926, 2.60644112,\n",
       "        2.66409101, 2.6778038 , 2.86156158, 2.90569916, 2.88388052,\n",
       "        3.00248194, 3.00958786, 2.98276486, 2.49995012, 2.49444571,\n",
       "        2.49954967, 2.62185464, 2.66008782, 2.65828614, 2.83183575,\n",
       "        2.89128695, 2.9302197 , 2.99097266, 3.00228243, 3.11788096,\n",
       "        2.56850891, 2.77178383, 2.80361123, 2.9230135 , 2.91290536,\n",
       "        2.9183104 , 3.26881127, 3.19835062, 3.30594344, 3.36619453,\n",
       "        3.45917482, 3.3498806 , 2.70472627, 2.70882936, 2.70182338,\n",
       "        2.86076031, 2.85225282, 2.92601633, 3.16562219, 3.2973351 ,\n",
       "        3.28142333, 3.3961206 , 3.55425644, 3.51121984, 2.78619599,\n",
       "        2.66719356, 2.5796185 , 2.72734594, 2.84905033, 2.85335469,\n",
       "        3.13739781, 3.3925168 , 3.38481097, 3.5694695 , 3.67445974,\n",
       "        3.50281243, 2.7075285 , 2.66829476, 2.59983559, 2.80571313,\n",
       "        2.87277079, 2.86106057, 3.09105835, 3.16021805, 3.14730654,\n",
       "        3.22407322, 3.03210697, 2.35712628]),\n",
       " 'std_fit_time': array([0.02917689, 0.03495816, 0.04714001, 0.03218377, 0.03683365,\n",
       "        0.04446634, 0.10953825, 0.06633049, 0.10225227, 0.04259309,\n",
       "        0.0872312 , 0.04121092, 0.04563575, 0.05366041, 0.09795953,\n",
       "        0.05212865, 0.04196641, 0.03683428, 0.05986029, 0.08685224,\n",
       "        0.09219379, 0.07952939, 0.1087991 , 0.05323131, 0.14156245,\n",
       "        0.0524888 , 0.02854815, 0.02874212, 0.03048249, 0.04974489,\n",
       "        0.07248918, 0.07599503, 0.06924232, 0.13722505, 0.06632361,\n",
       "        0.06436505, 0.10216588, 0.03868712, 0.12198633, 0.06594211,\n",
       "        0.04578899, 0.04851873, 0.09688397, 0.02854216, 0.09840959,\n",
       "        0.08259258, 0.07608065, 0.04689154, 0.11241792, 0.01402308,\n",
       "        0.07342447, 0.04961535, 0.03072299, 0.03889021, 0.06633705,\n",
       "        0.11535731, 0.05539168, 0.26847628, 0.10569798, 0.04471181,\n",
       "        0.04795604, 0.06752401, 0.03739731, 0.0551949 , 0.03336646,\n",
       "        0.02639135, 0.04526127, 0.04766814, 0.04466494, 0.16390145,\n",
       "        0.1271252 , 0.03653782, 0.04012563, 0.13326607, 0.01050332,\n",
       "        0.01801881, 0.02475311, 0.02667116, 0.03938361, 0.01814519,\n",
       "        0.05606427, 0.18505262, 0.1639139 , 0.06003834, 0.03790508,\n",
       "        0.0127725 , 0.01347311, 0.06173497, 0.02519087, 0.05025318,\n",
       "        0.03805292, 0.03232878, 0.0332666 , 0.16048503, 0.0339511 ,\n",
       "        0.08165421, 0.12570652, 0.15062384, 0.06585182, 0.08149008,\n",
       "        0.03801087, 0.04168905, 0.09942003, 0.06572721, 0.12425094,\n",
       "        0.08797859, 0.26092386, 0.07577863, 0.02299948, 0.10933135,\n",
       "        0.06227535, 0.03587005, 0.40095069, 0.06367578, 0.09760077,\n",
       "        0.03619698, 0.02615923, 0.04282784, 0.23787361, 0.05709317,\n",
       "        0.04030615, 0.159525  , 0.0334499 , 0.01693638, 0.01926495,\n",
       "        0.10345779, 0.01062729, 0.71678755, 0.09273668, 0.03161248,\n",
       "        0.12249971, 0.03572459, 0.02721979, 0.11238193, 0.04930861,\n",
       "        0.00708882, 0.17541712, 0.02671846, 0.03706422, 0.08049878,\n",
       "        0.05263541, 0.07459288, 0.20238042, 0.04412332, 0.0482587 ,\n",
       "        0.77159151, 0.02541985, 0.0665712 , 0.15764335, 0.03298603,\n",
       "        0.12722267, 0.18369392, 0.07024908, 0.07475149, 0.79299155,\n",
       "        0.0350983 , 0.02719991, 0.03706298, 0.05868873, 0.39254205,\n",
       "        0.20437351, 0.037695  , 0.02227566, 0.31156994, 0.02266546,\n",
       "        0.08044943, 0.11034869, 0.05950752, 0.18354274, 0.09726556,\n",
       "        0.06744878, 0.09800516, 0.62389797, 0.12612391, 0.15563216,\n",
       "        0.8404402 , 0.06887556, 0.10239868, 0.40442588, 0.11423176,\n",
       "        0.03818208, 0.21108632, 0.09278296, 0.08014837, 0.39969186,\n",
       "        0.11158384, 0.11202861, 0.92463352, 0.10183733, 0.10356398,\n",
       "        1.08205117, 0.20474121, 0.08317126, 0.09424425, 0.05604648,\n",
       "        0.01895452, 0.04227549, 0.03103861, 0.03250833, 0.0149795 ,\n",
       "        0.02432442, 0.02765811, 0.04152705, 0.07146441, 0.05732537,\n",
       "        0.04783313, 0.03329806, 0.04915155, 0.07429681, 0.07713217,\n",
       "        0.09808595, 0.04428903, 0.03291766, 0.0807146 , 0.07776316,\n",
       "        0.07588028, 0.04968879, 0.03539619, 0.02270612, 0.03671061,\n",
       "        0.04228505, 0.02038404, 0.03335476, 0.04248073, 0.04060771,\n",
       "        0.01850202, 0.05636524, 0.0471961 , 0.0374441 , 0.02126607,\n",
       "        0.0745419 , 0.0466443 , 0.04514166, 0.04780501, 0.03645465,\n",
       "        0.03446299, 0.03153406, 0.05016672, 0.02397385, 0.13399317,\n",
       "        0.24119258, 0.12303081, 0.10161716, 0.06690982, 0.05420329,\n",
       "        0.08847828, 0.0852041 , 0.11923936, 0.06299081, 0.10635998,\n",
       "        0.06972925, 0.1074473 , 0.06116915, 0.09460538, 0.01228638,\n",
       "        0.05729867, 0.08274892, 0.09262865, 0.11559582, 0.05232946,\n",
       "        0.09397133, 0.13237307, 0.18605825, 0.05887809, 0.12823437,\n",
       "        0.08755432, 0.0422356 , 0.04113774, 0.03385765, 0.02939082,\n",
       "        0.05053778, 0.09917009, 0.1627297 , 0.12176338, 0.08479589,\n",
       "        0.0983522 , 0.06571067, 0.08231151, 0.04184711, 0.08366966,\n",
       "        0.10963637, 0.09831105, 0.07360738, 0.08877729, 0.04480812,\n",
       "        0.04658724, 0.23457658, 0.12114997]),\n",
       " 'mean_score_time': array([0.01231155, 0.01010919, 0.01291161, 0.01130991, 0.01271133,\n",
       "        0.01361108, 0.01341238, 0.01341114, 0.01791487, 0.01651497,\n",
       "        0.01251183, 0.01101074, 0.01421142, 0.0115099 , 0.01060886,\n",
       "        0.01481271, 0.01241097, 0.0133122 , 0.01641235, 0.01661439,\n",
       "        0.01531348, 0.02081661, 0.0135118 , 0.01130934, 0.01161156,\n",
       "        0.01000838, 0.01160979, 0.01471305, 0.0123105 , 0.01291084,\n",
       "        0.01411247, 0.01661372, 0.01561251, 0.01451225, 0.01461315,\n",
       "        0.01191139, 0.01281071, 0.01260853, 0.01241002, 0.01541348,\n",
       "        0.01381226, 0.01601381, 0.0163146 , 0.01461349, 0.02321992,\n",
       "        0.01771674, 0.01341224, 0.01150932, 0.01291113, 0.01070933,\n",
       "        0.01461062, 0.01421261, 0.01280975, 0.01421251, 0.01671524,\n",
       "        0.01561356, 0.01841464, 0.01491241, 0.01561298, 0.01351171,\n",
       "        0.01541271, 0.01431274, 0.01391239, 0.0168128 , 0.01321235,\n",
       "        0.01271195, 0.01415677, 0.01471219, 0.01401272, 0.01301203,\n",
       "        0.01211095, 0.01240997, 0.01271086, 0.01201   , 0.01020875,\n",
       "        0.01271238, 0.01150885, 0.01511288, 0.01581402, 0.01431003,\n",
       "        0.01481509, 0.0155139 , 0.01351275, 0.01270967, 0.01060882,\n",
       "        0.01050997, 0.01010923, 0.0118113 , 0.01171083, 0.01170945,\n",
       "        0.01270952, 0.01371193, 0.01411047, 0.01301022, 0.0115099 ,\n",
       "        0.01111007, 0.00920811, 0.00920801, 0.00940843, 0.01141043,\n",
       "        0.00920758, 0.01221075, 0.01211085, 0.00970879, 0.01030865,\n",
       "        0.00990834, 0.01010847, 0.01020908, 0.0095088 , 0.00880728,\n",
       "        0.00910792, 0.00960884, 0.00930815, 0.01020899, 0.00972042,\n",
       "        0.00970802, 0.00990868, 0.0105092 , 0.009408  , 0.01040902,\n",
       "        0.00920782, 0.00890779, 0.00930758, 0.00970831, 0.00900779,\n",
       "        0.01381245, 0.010109  , 0.00920753, 0.01030912, 0.00960817,\n",
       "        0.00980887, 0.0097084 , 0.00950851, 0.00870767, 0.00900822,\n",
       "        0.00890751, 0.00950837, 0.01111021, 0.009408  , 0.0090075 ,\n",
       "        0.00980816, 0.01231112, 0.00960827, 0.00970802, 0.00930834,\n",
       "        0.00960851, 0.00930743, 0.00940809, 0.00950832, 0.01010823,\n",
       "        0.00980878, 0.00980873, 0.01111012, 0.00980802, 0.01050897,\n",
       "        0.01020813, 0.0094079 , 0.00940843, 0.00990844, 0.00950823,\n",
       "        0.00930791, 0.01100998, 0.01010938, 0.00970812, 0.01020861,\n",
       "        0.01110945, 0.00970759, 0.00980864, 0.0094079 , 0.00910797,\n",
       "        0.0102088 , 0.00960813, 0.00970812, 0.01010942, 0.01481266,\n",
       "        0.00980821, 0.01100965, 0.01211095, 0.01311097, 0.00970831,\n",
       "        0.00910807, 0.00930853, 0.00910797, 0.01020885, 0.00970778,\n",
       "        0.01521277, 0.01100984, 0.01000853, 0.01090975, 0.01080971,\n",
       "        0.01060958, 0.01151018, 0.01030874, 0.00890784, 0.01000881,\n",
       "        0.00970807, 0.00920868, 0.00940771, 0.00970869, 0.00930839,\n",
       "        0.01141009, 0.01090984, 0.01251121, 0.01010904, 0.00920811,\n",
       "        0.00900812, 0.00890818, 0.00910802, 0.00950828, 0.00920835,\n",
       "        0.00970817, 0.0136117 , 0.01251111, 0.01130967, 0.00990858,\n",
       "        0.00950861, 0.00880723, 0.00890756, 0.01060925, 0.01181054,\n",
       "        0.00920749, 0.01060882, 0.00970855, 0.01120944, 0.01000867,\n",
       "        0.01060972, 0.00970855, 0.00970817, 0.00920768, 0.00930815,\n",
       "        0.00960822, 0.00920792, 0.00960813, 0.00930829, 0.00960832,\n",
       "        0.00950813, 0.00980887, 0.01030903, 0.01060882, 0.01080947,\n",
       "        0.01611314, 0.01010866, 0.00930781, 0.01050935, 0.01160955,\n",
       "        0.01000819, 0.01291103, 0.01090918, 0.01010861, 0.0116107 ,\n",
       "        0.0127111 , 0.01070895, 0.01140985, 0.01000924, 0.00950794,\n",
       "        0.01050892, 0.01201029, 0.00980811, 0.01020937, 0.01171041,\n",
       "        0.01040759, 0.01100965, 0.01171017, 0.01010885, 0.0097084 ,\n",
       "        0.01070943, 0.0091074 , 0.00980854, 0.00960846, 0.01000876,\n",
       "        0.01341171, 0.01070991, 0.01461267, 0.01321144, 0.01080947,\n",
       "        0.01060882, 0.00960803, 0.00900784, 0.00910807, 0.00950837,\n",
       "        0.01050897, 0.01030917, 0.01141019, 0.01321149, 0.00990887,\n",
       "        0.009408  , 0.00740643, 0.00570469]),\n",
       " 'std_score_time': array([1.53724178e-03, 8.61317877e-04, 3.60157089e-03, 4.00448726e-04,\n",
       "        1.80609815e-03, 2.08443711e-03, 1.20138375e-03, 1.20130190e-03,\n",
       "        4.78213667e-03, 4.66125897e-03, 1.30548364e-03, 5.49078445e-04,\n",
       "        3.93663588e-03, 2.55199954e-03, 7.35300128e-04, 4.50379155e-03,\n",
       "        1.46532400e-03, 1.96603142e-03, 3.15452090e-03, 2.43986795e-03,\n",
       "        2.01571663e-03, 1.08000878e-02, 1.41621983e-03, 9.27814353e-04,\n",
       "        1.77389602e-03, 1.04968779e-03, 1.56353055e-03, 3.48977665e-03,\n",
       "        2.06553232e-03, 9.71123365e-04, 1.74476525e-03, 4.00732244e-03,\n",
       "        2.43947913e-03, 7.08279129e-04, 3.41571396e-03, 1.56374567e-03,\n",
       "        1.28916113e-03, 1.19934597e-03, 2.26913629e-03, 2.55929400e-03,\n",
       "        2.29568633e-03, 2.98590052e-03, 3.64379969e-03, 2.26827624e-03,\n",
       "        1.17892999e-02, 2.91101166e-03, 1.39421750e-03, 1.55031286e-03,\n",
       "        2.81933638e-03, 8.71749702e-04, 3.51648263e-03, 3.76690937e-03,\n",
       "        2.78782682e-03, 1.96621379e-03, 2.69644690e-03, 2.01151940e-03,\n",
       "        5.20628529e-03, 1.35817168e-03, 1.35680377e-03, 2.12345943e-03,\n",
       "        6.58016368e-03, 8.36422896e-03, 4.89705938e-03, 8.17583439e-03,\n",
       "        1.69230606e-03, 6.79494413e-04, 1.82439180e-03, 1.63268673e-03,\n",
       "        8.95510677e-04, 1.14189145e-03, 9.71566118e-04, 4.90622393e-04,\n",
       "        1.03067428e-03, 1.76287032e-03, 4.00471925e-04, 2.46328604e-03,\n",
       "        8.40107947e-04, 6.48223201e-03, 5.40538458e-03, 1.72184966e-03,\n",
       "        8.12013164e-04, 2.93864921e-03, 1.58266823e-03, 2.01588993e-03,\n",
       "        1.46449238e-03, 3.16432530e-04, 5.83227156e-04, 1.07808752e-03,\n",
       "        8.14580938e-04, 6.76835502e-04, 7.49922310e-04, 6.01972437e-04,\n",
       "        6.63311829e-04, 1.76178686e-03, 7.07463801e-04, 1.06911339e-03,\n",
       "        6.78303474e-04, 9.27736848e-04, 1.06870718e-03, 2.47974529e-03,\n",
       "        2.44814606e-04, 1.77924571e-03, 3.97067182e-03, 5.10024177e-04,\n",
       "        1.03080394e-03, 5.83789098e-04, 1.46458359e-03, 5.10221263e-04,\n",
       "        5.48597107e-04, 4.00472436e-04, 3.74584849e-04, 3.74712480e-04,\n",
       "        2.45224208e-04, 1.43628559e-03, 4.10211027e-04, 5.10697374e-04,\n",
       "        2.00630446e-04, 8.37283313e-04, 5.83887363e-04, 2.00224178e-04,\n",
       "        2.45301580e-04, 1.99794883e-04, 6.78718450e-04, 6.79112103e-04,\n",
       "        3.16431976e-04, 9.35974234e-03, 8.60480634e-04, 4.00400346e-04,\n",
       "        1.63230604e-03, 3.74572557e-04, 9.28256198e-04, 2.44990050e-04,\n",
       "        6.32598305e-04, 2.45456882e-04, 3.16355973e-04, 2.00367258e-04,\n",
       "        1.26625193e-03, 3.47323855e-03, 3.74355891e-04, 4.86280395e-07,\n",
       "        5.09977477e-04, 5.10983891e-03, 7.35423446e-04, 2.45787821e-04,\n",
       "        2.45379262e-04, 1.06826473e-03, 8.72127190e-04, 4.89999186e-04,\n",
       "        4.47661073e-04, 5.83764510e-04, 2.45185336e-04, 6.00481365e-04,\n",
       "        2.24732959e-03, 5.09781327e-04, 5.48075173e-04, 5.10033526e-04,\n",
       "        4.90398112e-04, 4.90505199e-04, 7.35001380e-04, 3.16583483e-04,\n",
       "        4.00448328e-04, 2.30419138e-03, 4.90329760e-04, 2.44736940e-04,\n",
       "        9.27875771e-04, 1.39421005e-03, 2.45262461e-04, 4.00293011e-04,\n",
       "        7.35377774e-04, 2.00200670e-04, 1.96615990e-03, 1.24203027e-03,\n",
       "        1.20910497e-03, 7.35358981e-04, 7.06001165e-03, 5.10323562e-04,\n",
       "        2.02645342e-03, 4.71003402e-03, 5.70932905e-03, 2.45360015e-04,\n",
       "        3.74291595e-04, 4.00758339e-04, 3.73998452e-04, 1.91495055e-03,\n",
       "        9.27803695e-04, 1.11609871e-02, 1.22619410e-03, 6.33013125e-04,\n",
       "        2.13257864e-03, 1.88842869e-03, 1.02058589e-03, 1.81773171e-03,\n",
       "        1.32742724e-03, 2.00295753e-04, 2.53215508e-03, 8.72630299e-04,\n",
       "        2.45281994e-04, 5.83347589e-04, 9.28225368e-04, 2.45223280e-04,\n",
       "        3.09118852e-03, 2.33448722e-03, 3.16513975e-03, 9.70400282e-04,\n",
       "        4.00650759e-04, 3.16205508e-04, 3.74202486e-04, 2.00176409e-04,\n",
       "        3.16506590e-04, 4.00114130e-04, 2.44990143e-04, 7.50607584e-03,\n",
       "        4.04063552e-03, 2.11382932e-03, 3.74521837e-04, 4.47767662e-04,\n",
       "        2.45223512e-04, 2.00629539e-04, 3.21812700e-03, 4.15830971e-03,\n",
       "        2.45769106e-04, 2.26811565e-03, 5.10164633e-04, 1.91499298e-03,\n",
       "        5.48205407e-04, 1.46407174e-03, 5.10257906e-04, 5.10407461e-04,\n",
       "        5.10285885e-04, 4.00328959e-04, 1.02077271e-03, 2.45028582e-04,\n",
       "        6.64570953e-04, 2.45243029e-04, 5.83511037e-04, 5.00111031e-07,\n",
       "        5.10444937e-04, 2.45808302e-04, 1.32041929e-03, 1.47101495e-03,\n",
       "        9.19163724e-03, 7.34846339e-04, 4.00757885e-04, 1.04993787e-03,\n",
       "        1.59503444e-03, 6.32862070e-04, 3.20238477e-03, 8.61217948e-04,\n",
       "        4.90680742e-04, 1.39394653e-03, 3.68553707e-03, 5.10678696e-04,\n",
       "        2.37679252e-03, 3.16355829e-04, 7.07865242e-04, 1.37986176e-03,\n",
       "        3.01942113e-03, 5.10754021e-04, 6.78542617e-04, 1.96614293e-03,\n",
       "        4.88842668e-04, 5.48335933e-04, 2.92849871e-03, 1.99961854e-04,\n",
       "        1.16734493e-03, 1.69224900e-03, 1.99890194e-04, 5.10173934e-04,\n",
       "        5.83609185e-04, 8.37368836e-04, 5.62066444e-03, 7.48799947e-04,\n",
       "        4.49160315e-03, 5.41374292e-03, 8.12902631e-04, 2.00557879e-04,\n",
       "        5.83601015e-04, 3.16431258e-04, 5.83601405e-04, 3.16959115e-04,\n",
       "        1.22557107e-03, 9.27916943e-04, 2.22514464e-03, 3.44663577e-03,\n",
       "        1.99771119e-04, 1.28150799e-03, 1.32017356e-03, 4.00519467e-04]),\n",
       " 'param_classifier': masked_array(data=[MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__activation': masked_array(data=['logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__alpha': masked_array(data=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__hidden_layer_sizes': masked_array(data=[(5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__learning_rate': masked_array(data=['constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'}],\n",
       " 'split0_test_recall_micro': array([0.846, 0.852, 0.857, 0.87 , 0.866, 0.855, 0.864, 0.859, 0.861,\n",
       "        0.841, 0.861, 0.855, 0.849, 0.856, 0.858, 0.865, 0.849, 0.855,\n",
       "        0.862, 0.861, 0.852, 0.857, 0.858, 0.866, 0.85 , 0.854, 0.861,\n",
       "        0.863, 0.85 , 0.861, 0.865, 0.862, 0.852, 0.836, 0.862, 0.853,\n",
       "        0.852, 0.871, 0.851, 0.858, 0.855, 0.849, 0.85 , 0.853, 0.866,\n",
       "        0.855, 0.866, 0.855, 0.852, 0.857, 0.856, 0.849, 0.863, 0.854,\n",
       "        0.852, 0.853, 0.856, 0.854, 0.848, 0.854, 0.852, 0.851, 0.853,\n",
       "        0.849, 0.852, 0.858, 0.835, 0.848, 0.86 , 0.854, 0.857, 0.846,\n",
       "        0.855, 0.857, 0.85 , 0.867, 0.859, 0.855, 0.849, 0.859, 0.853,\n",
       "        0.85 , 0.849, 0.848, 0.851, 0.851, 0.862, 0.856, 0.846, 0.862,\n",
       "        0.844, 0.859, 0.862, 0.859, 0.856, 0.852, 0.78 , 0.611, 0.781,\n",
       "        0.781, 0.639, 0.781, 0.782, 0.632, 0.779, 0.785, 0.287, 0.781,\n",
       "        0.782, 0.682, 0.778, 0.78 , 0.358, 0.785, 0.783, 0.659, 0.781,\n",
       "        0.782, 0.342, 0.781, 0.78 , 0.639, 0.781, 0.781, 0.666, 0.781,\n",
       "        0.779, 0.365, 0.778, 0.779, 0.639, 0.779, 0.785, 0.354, 0.786,\n",
       "        0.78 , 0.357, 0.78 , 0.778, 0.377, 0.78 , 0.784, 0.359, 0.782,\n",
       "        0.797, 0.341, 0.786, 0.798, 0.532, 0.812, 0.804, 0.652, 0.798,\n",
       "        0.793, 0.658, 0.806, 0.789, 0.672, 0.785, 0.784, 0.609, 0.803,\n",
       "        0.791, 0.394, 0.795, 0.797, 0.632, 0.805, 0.801, 0.612, 0.791,\n",
       "        0.795, 0.675, 0.799, 0.796, 0.645, 0.797, 0.802, 0.344, 0.793,\n",
       "        0.781, 0.663, 0.791, 0.805, 0.631, 0.796, 0.794, 0.564, 0.793,\n",
       "        0.806, 0.345, 0.795, 0.816, 0.807, 0.808, 0.831, 0.83 , 0.832,\n",
       "        0.832, 0.836, 0.839, 0.836, 0.837, 0.838, 0.816, 0.816, 0.822,\n",
       "        0.835, 0.822, 0.825, 0.838, 0.831, 0.835, 0.84 , 0.836, 0.828,\n",
       "        0.816, 0.805, 0.829, 0.831, 0.832, 0.824, 0.833, 0.83 , 0.836,\n",
       "        0.837, 0.835, 0.833, 0.823, 0.808, 0.826, 0.818, 0.821, 0.814,\n",
       "        0.831, 0.817, 0.825, 0.825, 0.832, 0.829, 0.837, 0.85 , 0.83 ,\n",
       "        0.837, 0.858, 0.85 , 0.839, 0.849, 0.853, 0.848, 0.853, 0.846,\n",
       "        0.836, 0.84 , 0.83 , 0.843, 0.848, 0.839, 0.843, 0.85 , 0.846,\n",
       "        0.847, 0.852, 0.858, 0.835, 0.834, 0.833, 0.84 , 0.849, 0.852,\n",
       "        0.853, 0.843, 0.853, 0.847, 0.854, 0.851, 0.833, 0.831, 0.834,\n",
       "        0.841, 0.843, 0.843, 0.847, 0.848, 0.845, 0.851, 0.844, 0.848]),\n",
       " 'split1_test_recall_micro': array([0.831, 0.864, 0.836, 0.862, 0.853, 0.861, 0.863, 0.843, 0.86 ,\n",
       "        0.862, 0.863, 0.87 , 0.851, 0.859, 0.85 , 0.862, 0.873, 0.859,\n",
       "        0.855, 0.862, 0.865, 0.855, 0.854, 0.855, 0.872, 0.861, 0.857,\n",
       "        0.856, 0.857, 0.846, 0.849, 0.84 , 0.861, 0.859, 0.858, 0.85 ,\n",
       "        0.849, 0.87 , 0.851, 0.859, 0.854, 0.864, 0.861, 0.86 , 0.858,\n",
       "        0.852, 0.853, 0.877, 0.863, 0.838, 0.852, 0.857, 0.854, 0.868,\n",
       "        0.862, 0.85 , 0.872, 0.849, 0.85 , 0.854, 0.855, 0.874, 0.875,\n",
       "        0.866, 0.849, 0.864, 0.857, 0.861, 0.863, 0.852, 0.858, 0.856,\n",
       "        0.867, 0.855, 0.849, 0.859, 0.856, 0.854, 0.853, 0.839, 0.85 ,\n",
       "        0.86 , 0.872, 0.861, 0.867, 0.859, 0.868, 0.86 , 0.861, 0.859,\n",
       "        0.861, 0.859, 0.857, 0.857, 0.845, 0.861, 0.777, 0.673, 0.779,\n",
       "        0.779, 0.639, 0.775, 0.777, 0.639, 0.782, 0.776, 0.649, 0.777,\n",
       "        0.766, 0.361, 0.762, 0.777, 0.361, 0.773, 0.772, 0.639, 0.779,\n",
       "        0.779, 0.639, 0.779, 0.767, 0.361, 0.774, 0.778, 0.42 , 0.781,\n",
       "        0.779, 0.639, 0.774, 0.78 , 0.633, 0.78 , 0.769, 0.361, 0.782,\n",
       "        0.779, 0.296, 0.779, 0.778, 0.639, 0.776, 0.776, 0.407, 0.781,\n",
       "        0.788, 0.422, 0.795, 0.809, 0.656, 0.791, 0.807, 0.515, 0.807,\n",
       "        0.821, 0.577, 0.794, 0.798, 0.582, 0.78 , 0.802, 0.715, 0.791,\n",
       "        0.818, 0.629, 0.801, 0.818, 0.53 , 0.811, 0.793, 0.645, 0.773,\n",
       "        0.811, 0.616, 0.796, 0.797, 0.371, 0.807, 0.802, 0.528, 0.806,\n",
       "        0.784, 0.615, 0.796, 0.808, 0.68 , 0.786, 0.797, 0.549, 0.812,\n",
       "        0.819, 0.701, 0.806, 0.823, 0.812, 0.818, 0.828, 0.824, 0.816,\n",
       "        0.819, 0.829, 0.829, 0.832, 0.826, 0.832, 0.81 , 0.807, 0.811,\n",
       "        0.83 , 0.826, 0.826, 0.824, 0.826, 0.828, 0.828, 0.828, 0.828,\n",
       "        0.81 , 0.814, 0.808, 0.823, 0.817, 0.828, 0.825, 0.824, 0.823,\n",
       "        0.826, 0.824, 0.826, 0.796, 0.805, 0.806, 0.822, 0.819, 0.821,\n",
       "        0.824, 0.82 , 0.829, 0.823, 0.825, 0.825, 0.834, 0.844, 0.843,\n",
       "        0.844, 0.857, 0.851, 0.857, 0.854, 0.849, 0.857, 0.853, 0.856,\n",
       "        0.833, 0.831, 0.831, 0.847, 0.844, 0.845, 0.85 , 0.854, 0.854,\n",
       "        0.859, 0.858, 0.861, 0.835, 0.834, 0.832, 0.854, 0.853, 0.855,\n",
       "        0.853, 0.853, 0.855, 0.858, 0.856, 0.849, 0.832, 0.832, 0.832,\n",
       "        0.844, 0.849, 0.854, 0.853, 0.854, 0.865, 0.856, 0.847, 0.855]),\n",
       " 'split2_test_recall_micro': array([0.84 , 0.843, 0.856, 0.842, 0.839, 0.852, 0.852, 0.852, 0.842,\n",
       "        0.856, 0.847, 0.854, 0.853, 0.85 , 0.853, 0.85 , 0.853, 0.843,\n",
       "        0.838, 0.839, 0.843, 0.851, 0.852, 0.844, 0.842, 0.845, 0.848,\n",
       "        0.846, 0.847, 0.845, 0.853, 0.855, 0.854, 0.854, 0.856, 0.849,\n",
       "        0.848, 0.843, 0.849, 0.847, 0.852, 0.854, 0.847, 0.848, 0.857,\n",
       "        0.844, 0.846, 0.84 , 0.843, 0.836, 0.844, 0.841, 0.853, 0.851,\n",
       "        0.847, 0.842, 0.854, 0.866, 0.854, 0.852, 0.853, 0.843, 0.844,\n",
       "        0.843, 0.841, 0.84 , 0.83 , 0.85 , 0.843, 0.849, 0.847, 0.846,\n",
       "        0.854, 0.851, 0.849, 0.856, 0.842, 0.848, 0.836, 0.846, 0.838,\n",
       "        0.847, 0.846, 0.831, 0.855, 0.85 , 0.84 , 0.844, 0.845, 0.847,\n",
       "        0.835, 0.861, 0.84 , 0.846, 0.843, 0.845, 0.776, 0.361, 0.777,\n",
       "        0.776, 0.551, 0.781, 0.779, 0.639, 0.781, 0.783, 0.639, 0.785,\n",
       "        0.781, 0.361, 0.735, 0.777, 0.36 , 0.777, 0.779, 0.639, 0.782,\n",
       "        0.783, 0.639, 0.779, 0.772, 0.361, 0.782, 0.782, 0.638, 0.783,\n",
       "        0.781, 0.567, 0.783, 0.783, 0.398, 0.783, 0.774, 0.67 , 0.783,\n",
       "        0.782, 0.676, 0.783, 0.776, 0.693, 0.777, 0.776, 0.655, 0.78 ,\n",
       "        0.779, 0.343, 0.776, 0.798, 0.681, 0.789, 0.796, 0.603, 0.79 ,\n",
       "        0.79 , 0.426, 0.793, 0.781, 0.645, 0.786, 0.803, 0.438, 0.79 ,\n",
       "        0.796, 0.559, 0.787, 0.793, 0.678, 0.801, 0.776, 0.31 , 0.791,\n",
       "        0.794, 0.419, 0.802, 0.8  , 0.536, 0.791, 0.797, 0.555, 0.789,\n",
       "        0.777, 0.35 , 0.789, 0.786, 0.505, 0.787, 0.801, 0.281, 0.801,\n",
       "        0.789, 0.731, 0.787, 0.804, 0.802, 0.801, 0.808, 0.812, 0.799,\n",
       "        0.808, 0.811, 0.815, 0.808, 0.809, 0.814, 0.809, 0.807, 0.802,\n",
       "        0.807, 0.807, 0.803, 0.818, 0.813, 0.81 , 0.812, 0.812, 0.808,\n",
       "        0.805, 0.803, 0.807, 0.8  , 0.805, 0.814, 0.812, 0.802, 0.81 ,\n",
       "        0.81 , 0.817, 0.811, 0.79 , 0.79 , 0.798, 0.805, 0.803, 0.804,\n",
       "        0.806, 0.8  , 0.805, 0.805, 0.809, 0.806, 0.809, 0.829, 0.816,\n",
       "        0.826, 0.829, 0.829, 0.829, 0.832, 0.845, 0.833, 0.835, 0.842,\n",
       "        0.808, 0.825, 0.825, 0.83 , 0.828, 0.84 , 0.834, 0.834, 0.833,\n",
       "        0.837, 0.841, 0.839, 0.819, 0.821, 0.821, 0.838, 0.822, 0.831,\n",
       "        0.84 , 0.834, 0.837, 0.837, 0.831, 0.834, 0.821, 0.818, 0.815,\n",
       "        0.825, 0.832, 0.826, 0.829, 0.833, 0.835, 0.84 , 0.835, 0.835]),\n",
       " 'split3_test_recall_micro': array([0.848, 0.861, 0.847, 0.839, 0.857, 0.858, 0.854, 0.861, 0.853,\n",
       "        0.848, 0.837, 0.845, 0.85 , 0.85 , 0.846, 0.856, 0.847, 0.865,\n",
       "        0.856, 0.853, 0.865, 0.855, 0.849, 0.853, 0.863, 0.853, 0.845,\n",
       "        0.843, 0.849, 0.845, 0.861, 0.849, 0.844, 0.852, 0.846, 0.846,\n",
       "        0.852, 0.851, 0.853, 0.856, 0.84 , 0.85 , 0.854, 0.864, 0.87 ,\n",
       "        0.866, 0.866, 0.862, 0.851, 0.856, 0.855, 0.859, 0.857, 0.852,\n",
       "        0.848, 0.852, 0.857, 0.849, 0.853, 0.853, 0.85 , 0.852, 0.857,\n",
       "        0.866, 0.858, 0.846, 0.861, 0.841, 0.849, 0.85 , 0.846, 0.851,\n",
       "        0.853, 0.842, 0.851, 0.854, 0.86 , 0.86 , 0.851, 0.857, 0.844,\n",
       "        0.851, 0.851, 0.845, 0.857, 0.849, 0.846, 0.857, 0.856, 0.853,\n",
       "        0.864, 0.857, 0.854, 0.857, 0.845, 0.862, 0.786, 0.639, 0.779,\n",
       "        0.787, 0.361, 0.79 , 0.79 , 0.639, 0.791, 0.788, 0.361, 0.79 ,\n",
       "        0.778, 0.361, 0.785, 0.789, 0.361, 0.789, 0.786, 0.643, 0.788,\n",
       "        0.789, 0.357, 0.789, 0.785, 0.663, 0.786, 0.785, 0.639, 0.787,\n",
       "        0.784, 0.361, 0.79 , 0.788, 0.643, 0.79 , 0.789, 0.639, 0.781,\n",
       "        0.792, 0.361, 0.792, 0.788, 0.639, 0.788, 0.791, 0.362, 0.783,\n",
       "        0.79 , 0.392, 0.801, 0.8  , 0.471, 0.792, 0.791, 0.436, 0.795,\n",
       "        0.796, 0.409, 0.796, 0.781, 0.628, 0.797, 0.804, 0.592, 0.796,\n",
       "        0.793, 0.431, 0.801, 0.803, 0.526, 0.8  , 0.79 , 0.468, 0.789,\n",
       "        0.784, 0.262, 0.802, 0.796, 0.332, 0.799, 0.801, 0.568, 0.796,\n",
       "        0.79 , 0.658, 0.795, 0.805, 0.34 , 0.805, 0.804, 0.491, 0.8  ,\n",
       "        0.805, 0.486, 0.801, 0.791, 0.796, 0.815, 0.829, 0.819, 0.821,\n",
       "        0.822, 0.829, 0.828, 0.828, 0.828, 0.83 , 0.823, 0.816, 0.822,\n",
       "        0.829, 0.818, 0.826, 0.819, 0.826, 0.824, 0.825, 0.826, 0.829,\n",
       "        0.797, 0.816, 0.799, 0.826, 0.798, 0.797, 0.804, 0.827, 0.815,\n",
       "        0.827, 0.829, 0.827, 0.8  , 0.791, 0.799, 0.816, 0.818, 0.822,\n",
       "        0.822, 0.828, 0.81 , 0.822, 0.823, 0.818, 0.838, 0.839, 0.835,\n",
       "        0.849, 0.852, 0.843, 0.853, 0.845, 0.852, 0.843, 0.852, 0.856,\n",
       "        0.838, 0.841, 0.844, 0.86 , 0.842, 0.846, 0.85 , 0.852, 0.85 ,\n",
       "        0.856, 0.851, 0.852, 0.843, 0.82 , 0.845, 0.851, 0.855, 0.85 ,\n",
       "        0.853, 0.852, 0.852, 0.85 , 0.847, 0.858, 0.84 , 0.824, 0.835,\n",
       "        0.848, 0.852, 0.852, 0.845, 0.855, 0.857, 0.851, 0.861, 0.853]),\n",
       " 'split4_test_recall_micro': array([0.858, 0.862, 0.85 , 0.872, 0.854, 0.868, 0.862, 0.859, 0.855,\n",
       "        0.849, 0.854, 0.869, 0.857, 0.85 , 0.869, 0.855, 0.854, 0.869,\n",
       "        0.866, 0.868, 0.856, 0.858, 0.852, 0.86 , 0.849, 0.845, 0.865,\n",
       "        0.855, 0.868, 0.862, 0.867, 0.865, 0.867, 0.866, 0.859, 0.872,\n",
       "        0.86 , 0.866, 0.854, 0.871, 0.863, 0.867, 0.857, 0.87 , 0.86 ,\n",
       "        0.87 , 0.866, 0.846, 0.854, 0.864, 0.853, 0.863, 0.865, 0.864,\n",
       "        0.858, 0.862, 0.859, 0.856, 0.852, 0.867, 0.86 , 0.862, 0.862,\n",
       "        0.86 , 0.87 , 0.859, 0.853, 0.862, 0.864, 0.863, 0.859, 0.863,\n",
       "        0.867, 0.86 , 0.841, 0.856, 0.861, 0.864, 0.861, 0.862, 0.872,\n",
       "        0.863, 0.863, 0.861, 0.859, 0.862, 0.86 , 0.853, 0.861, 0.863,\n",
       "        0.853, 0.858, 0.872, 0.852, 0.856, 0.864, 0.767, 0.361, 0.76 ,\n",
       "        0.774, 0.639, 0.771, 0.769, 0.348, 0.771, 0.771, 0.639, 0.77 ,\n",
       "        0.774, 0.639, 0.774, 0.77 , 0.488, 0.774, 0.77 , 0.537, 0.77 ,\n",
       "        0.775, 0.639, 0.769, 0.764, 0.361, 0.776, 0.769, 0.642, 0.768,\n",
       "        0.774, 0.65 , 0.763, 0.766, 0.647, 0.773, 0.773, 0.49 , 0.767,\n",
       "        0.774, 0.639, 0.766, 0.767, 0.639, 0.765, 0.765, 0.692, 0.768,\n",
       "        0.78 , 0.588, 0.774, 0.788, 0.671, 0.789, 0.787, 0.593, 0.781,\n",
       "        0.803, 0.549, 0.794, 0.781, 0.59 , 0.781, 0.785, 0.665, 0.781,\n",
       "        0.792, 0.55 , 0.787, 0.784, 0.612, 0.779, 0.778, 0.483, 0.787,\n",
       "        0.788, 0.692, 0.782, 0.784, 0.694, 0.782, 0.791, 0.639, 0.783,\n",
       "        0.785, 0.641, 0.777, 0.8  , 0.658, 0.799, 0.787, 0.649, 0.773,\n",
       "        0.802, 0.571, 0.793, 0.808, 0.787, 0.815, 0.822, 0.815, 0.829,\n",
       "        0.83 , 0.827, 0.833, 0.83 , 0.836, 0.824, 0.81 , 0.803, 0.791,\n",
       "        0.823, 0.828, 0.816, 0.832, 0.834, 0.825, 0.836, 0.837, 0.836,\n",
       "        0.829, 0.791, 0.796, 0.833, 0.821, 0.821, 0.836, 0.833, 0.835,\n",
       "        0.823, 0.835, 0.84 , 0.806, 0.798, 0.803, 0.823, 0.821, 0.81 ,\n",
       "        0.819, 0.818, 0.813, 0.83 , 0.829, 0.83 , 0.835, 0.854, 0.84 ,\n",
       "        0.855, 0.849, 0.858, 0.856, 0.859, 0.853, 0.865, 0.857, 0.861,\n",
       "        0.847, 0.855, 0.84 , 0.865, 0.86 , 0.843, 0.858, 0.865, 0.87 ,\n",
       "        0.86 , 0.87 , 0.865, 0.841, 0.839, 0.844, 0.859, 0.855, 0.838,\n",
       "        0.867, 0.855, 0.857, 0.864, 0.868, 0.854, 0.835, 0.833, 0.846,\n",
       "        0.852, 0.843, 0.848, 0.855, 0.858, 0.854, 0.86 , 0.857, 0.855]),\n",
       " 'mean_test_recall_micro': array([0.8446, 0.8564, 0.8492, 0.857 , 0.8538, 0.8588, 0.859 , 0.8548,\n",
       "        0.8542, 0.8512, 0.8524, 0.8586, 0.852 , 0.853 , 0.8552, 0.8576,\n",
       "        0.8552, 0.8582, 0.8554, 0.8566, 0.8562, 0.8552, 0.853 , 0.8556,\n",
       "        0.8552, 0.8516, 0.8552, 0.8526, 0.8542, 0.8518, 0.859 , 0.8542,\n",
       "        0.8556, 0.8534, 0.8562, 0.854 , 0.8522, 0.8602, 0.8516, 0.8582,\n",
       "        0.8528, 0.8568, 0.8538, 0.859 , 0.8622, 0.8574, 0.8594, 0.856 ,\n",
       "        0.8526, 0.8502, 0.852 , 0.8538, 0.8584, 0.8578, 0.8534, 0.8518,\n",
       "        0.8596, 0.8548, 0.8514, 0.856 , 0.854 , 0.8564, 0.8582, 0.8568,\n",
       "        0.854 , 0.8534, 0.8472, 0.8524, 0.8558, 0.8536, 0.8534, 0.8524,\n",
       "        0.8592, 0.853 , 0.848 , 0.8584, 0.8556, 0.8562, 0.85  , 0.8526,\n",
       "        0.8514, 0.8542, 0.8562, 0.8492, 0.8578, 0.8542, 0.8552, 0.854 ,\n",
       "        0.8538, 0.8568, 0.8514, 0.8588, 0.857 , 0.8542, 0.849 , 0.8568,\n",
       "        0.7772, 0.529 , 0.7752, 0.7794, 0.5658, 0.7796, 0.7794, 0.5794,\n",
       "        0.7808, 0.7806, 0.515 , 0.7806, 0.7762, 0.4808, 0.7668, 0.7786,\n",
       "        0.3856, 0.7796, 0.778 , 0.6234, 0.78  , 0.7816, 0.5232, 0.7794,\n",
       "        0.7736, 0.477 , 0.7798, 0.779 , 0.601 , 0.78  , 0.7794, 0.5164,\n",
       "        0.7776, 0.7792, 0.592 , 0.781 , 0.778 , 0.5028, 0.7798, 0.7814,\n",
       "        0.4658, 0.78  , 0.7774, 0.5974, 0.7772, 0.7784, 0.495 , 0.7788,\n",
       "        0.7868, 0.4172, 0.7864, 0.7986, 0.6022, 0.7946, 0.797 , 0.5598,\n",
       "        0.7942, 0.8006, 0.5238, 0.7966, 0.786 , 0.6234, 0.7858, 0.7956,\n",
       "        0.6038, 0.7922, 0.798 , 0.5126, 0.7942, 0.799 , 0.5956, 0.7992,\n",
       "        0.7876, 0.5036, 0.7862, 0.7944, 0.5328, 0.7962, 0.7946, 0.5156,\n",
       "        0.7952, 0.7986, 0.5268, 0.7934, 0.7834, 0.5854, 0.7896, 0.8008,\n",
       "        0.5628, 0.7946, 0.7966, 0.5068, 0.7958, 0.8042, 0.5668, 0.7964,\n",
       "        0.8084, 0.8008, 0.8114, 0.8236, 0.82  , 0.8194, 0.8222, 0.8264,\n",
       "        0.8288, 0.8268, 0.8272, 0.8276, 0.8136, 0.8098, 0.8096, 0.8248,\n",
       "        0.8202, 0.8192, 0.8262, 0.826 , 0.8244, 0.8282, 0.8278, 0.8258,\n",
       "        0.8114, 0.8058, 0.8078, 0.8226, 0.8146, 0.8168, 0.822 , 0.8232,\n",
       "        0.8238, 0.8246, 0.828 , 0.8274, 0.803 , 0.7984, 0.8064, 0.8168,\n",
       "        0.8164, 0.8142, 0.8204, 0.8166, 0.8164, 0.821 , 0.8236, 0.8216,\n",
       "        0.8306, 0.8432, 0.8328, 0.8422, 0.849 , 0.8462, 0.8468, 0.8478,\n",
       "        0.8504, 0.8492, 0.85  , 0.8522, 0.8324, 0.8384, 0.834 , 0.849 ,\n",
       "        0.8444, 0.8426, 0.847 , 0.851 , 0.8506, 0.8518, 0.8544, 0.855 ,\n",
       "        0.8346, 0.8296, 0.835 , 0.8484, 0.8468, 0.8452, 0.8532, 0.8474,\n",
       "        0.8508, 0.8512, 0.8512, 0.8492, 0.8322, 0.8276, 0.8324, 0.842 ,\n",
       "        0.8438, 0.8446, 0.8458, 0.8496, 0.8512, 0.8516, 0.8488, 0.8492]),\n",
       " 'std_test_recall_micro': array([0.00893532, 0.00786384, 0.00757364, 0.01391402, 0.00870402,\n",
       "        0.00549181, 0.00497996, 0.0066453 , 0.00679412, 0.00719444,\n",
       "        0.00954149, 0.00956243, 0.00282843, 0.00379473, 0.00793473,\n",
       "        0.00531413, 0.00926067, 0.00899778, 0.00958332, 0.01001199,\n",
       "        0.00832827, 0.0024    , 0.00296648, 0.00733757, 0.0107963 ,\n",
       "        0.0060531 , 0.0076    , 0.00722772, 0.00767854, 0.00793473,\n",
       "        0.0069282 , 0.00901998, 0.00786384, 0.00995188, 0.00545527,\n",
       "        0.00927362, 0.00421426, 0.01119643, 0.00174356, 0.00767854,\n",
       "        0.0074135 , 0.00735935, 0.0049558 , 0.00779744, 0.004996  ,\n",
       "        0.00945727, 0.00838093, 0.01291511, 0.00640625, 0.01114271,\n",
       "        0.00424264, 0.00785875, 0.0048    , 0.00688186, 0.00578273,\n",
       "        0.0064    , 0.00640625, 0.00624179, 0.00215407, 0.00554977,\n",
       "        0.00340588, 0.01066958, 0.0102645 , 0.00928224, 0.00969536,\n",
       "        0.00893532, 0.0123677 , 0.00801499, 0.00832827, 0.005004  ,\n",
       "        0.00567803, 0.00646838, 0.0064    , 0.00622896, 0.00357771,\n",
       "        0.00458694, 0.00700286, 0.00545527, 0.00809938, 0.00868562,\n",
       "        0.01151694, 0.00617738, 0.0097857 , 0.01121428, 0.0053066 ,\n",
       "        0.00526878, 0.01047664, 0.00547723, 0.00702567, 0.00601332,\n",
       "        0.0107443 , 0.00132665, 0.010469  , 0.00470744, 0.00576194,\n",
       "        0.00719444, 0.00617738, 0.13856984, 0.00770454, 0.00449889,\n",
       "        0.10792294, 0.00643739, 0.00682935, 0.11573176, 0.0064    ,\n",
       "        0.00621611, 0.15773902, 0.00682935, 0.00581034, 0.14735318,\n",
       "        0.01756588, 0.00615142, 0.05121172, 0.00631189, 0.00616441,\n",
       "        0.04382511, 0.00583095, 0.00463033, 0.14190476, 0.00637495,\n",
       "        0.00786384, 0.14227298, 0.00430813, 0.00547723, 0.09108238,\n",
       "        0.00638749, 0.0032619 , 0.12846104, 0.00904655, 0.00730479,\n",
       "        0.09711025, 0.00554977, 0.00764199, 0.13335576, 0.00661513,\n",
       "        0.00591946, 0.15864098, 0.0083666 , 0.00668132, 0.11216702,\n",
       "        0.0074135 , 0.00873155, 0.14719918, 0.00549181, 0.00667533,\n",
       "        0.09071141, 0.01048046, 0.00668132, 0.08482783, 0.00877724,\n",
       "        0.00756307, 0.07589835, 0.00861162, 0.01107429, 0.09404127,\n",
       "        0.0048    , 0.00675278, 0.03370223, 0.00604649, 0.00909065,\n",
       "        0.09355939, 0.00724983, 0.01013903, 0.08697724, 0.00627375,\n",
       "        0.01133137, 0.05921351, 0.0108148 , 0.00939361, 0.11909425,\n",
       "        0.00676461, 0.00922171, 0.1665958 , 0.00744043, 0.00549909,\n",
       "        0.14394805, 0.00835225, 0.00422374, 0.09848736, 0.00765768,\n",
       "        0.00431741, 0.11888919, 0.0068    , 0.00783326, 0.12683123,\n",
       "        0.00722772, 0.00588558, 0.12370028, 0.0129213 , 0.00957914,\n",
       "        0.14192449, 0.00656049, 0.0108922 , 0.00870402, 0.00615142,\n",
       "        0.00835703, 0.00641872, 0.01167219, 0.00858836, 0.00828493,\n",
       "        0.00790949, 0.00976524, 0.01006777, 0.0081388 , 0.00531413,\n",
       "        0.00526878, 0.0119432 , 0.00968297, 0.00744043, 0.00893085,\n",
       "        0.00770454, 0.00718331, 0.00816333, 0.0097242 , 0.00899778,\n",
       "        0.00938936, 0.01078147, 0.00893085, 0.01154816, 0.0118423 ,\n",
       "        0.01197664, 0.01090688, 0.01224745, 0.01101635, 0.01041921,\n",
       "        0.00868562, 0.00687023, 0.00960417, 0.0112783 , 0.00722772,\n",
       "        0.0102098 , 0.00643117, 0.0068    , 0.00676461, 0.00821219,\n",
       "        0.00915642, 0.00911263, 0.00846168, 0.00793977, 0.00886792,\n",
       "        0.0108922 , 0.00874986, 0.00949526, 0.01002796, 0.01052616,\n",
       "        0.00982649, 0.01099818, 0.00919565, 0.00307246, 0.01107068,\n",
       "        0.00769415, 0.00705408, 0.01306292, 0.01019019, 0.00695701,\n",
       "        0.01247397, 0.01030728, 0.00272764, 0.00804984, 0.00995992,\n",
       "        0.01199333, 0.00870402, 0.0095205 , 0.00905539, 0.00842852,\n",
       "        0.00765768, 0.00883176, 0.00811419, 0.01259206, 0.00915205,\n",
       "        0.00854166, 0.00786384, 0.00711056, 0.00928224, 0.01215566,\n",
       "        0.00818291, 0.00624179, 0.00574804, 0.00997196, 0.00927362,\n",
       "        0.00685274, 0.01003195, 0.00917388, 0.00891291, 0.01032279,\n",
       "        0.00671118, 0.00930376, 0.00754718]),\n",
       " 'rank_test_recall_micro': array([126,  28, 108,  21,  61,   9,   6,  48,  51,  97,  80,  11,  83,\n",
       "         72,  41,  19,  41,  16,  40,  27,  32,  41,  71,  37,  41,  90,\n",
       "         41,  75,  52,  86,   6,  52,  37,  66,  30,  57,  81,   2,  88,\n",
       "         14,  74,  23,  61,   6,   1,  20,   4,  34,  75, 102,  83,  61,\n",
       "         12,  17,  66,  86,   3,  48,  91,  34,  57,  28,  14,  25,  57,\n",
       "         66, 119,  78,  36,  65,  66,  78,   5,  72, 116,  13,  39,  32,\n",
       "        103,  75,  92,  52,  30, 108,  17,  52,  41,  57,  64,  23,  92,\n",
       "         10,  21,  52, 111,  25, 252, 272, 254, 238, 268, 236, 238, 266,\n",
       "        228, 229, 278, 229, 253, 284, 256, 245, 288, 236, 248, 257, 231,\n",
       "        225, 275, 238, 255, 285, 234, 243, 261, 231, 238, 276, 249, 242,\n",
       "        264, 227, 247, 282, 234, 226, 286, 231, 250, 262, 251, 246, 283,\n",
       "        244, 219, 287, 220, 197, 260, 209, 201, 270, 213, 194, 274, 203,\n",
       "        222, 257, 223, 207, 259, 216, 200, 279, 213, 196, 263, 195, 218,\n",
       "        281, 221, 212, 271, 205, 211, 277, 208, 197, 273, 215, 224, 265,\n",
       "        217, 192, 269, 209, 202, 280, 206, 190, 267, 204, 186, 192, 182,\n",
       "        162, 171, 172, 165, 153, 144, 152, 151, 148, 181, 184, 185, 157,\n",
       "        170, 173, 154, 155, 159, 145, 147, 156, 182, 189, 187, 164, 179,\n",
       "        174, 166, 163, 160, 158, 146, 150, 191, 199, 188, 174, 177, 180,\n",
       "        169, 176, 177, 168, 161, 167, 142, 130, 138, 132, 113, 123, 121,\n",
       "        117, 101, 106, 103,  81, 139, 134, 137, 111, 128, 131, 120,  98,\n",
       "        100,  85,  50,  47, 136, 143, 135, 115, 121, 125,  70, 118,  99,\n",
       "         94,  94, 108, 141, 148, 140, 133, 129, 126, 124, 105,  94,  88,\n",
       "        114, 106]),\n",
       " 'split0_test_f1_micro': array([0.846, 0.852, 0.857, 0.87 , 0.866, 0.855, 0.864, 0.859, 0.861,\n",
       "        0.841, 0.861, 0.855, 0.849, 0.856, 0.858, 0.865, 0.849, 0.855,\n",
       "        0.862, 0.861, 0.852, 0.857, 0.858, 0.866, 0.85 , 0.854, 0.861,\n",
       "        0.863, 0.85 , 0.861, 0.865, 0.862, 0.852, 0.836, 0.862, 0.853,\n",
       "        0.852, 0.871, 0.851, 0.858, 0.855, 0.849, 0.85 , 0.853, 0.866,\n",
       "        0.855, 0.866, 0.855, 0.852, 0.857, 0.856, 0.849, 0.863, 0.854,\n",
       "        0.852, 0.853, 0.856, 0.854, 0.848, 0.854, 0.852, 0.851, 0.853,\n",
       "        0.849, 0.852, 0.858, 0.835, 0.848, 0.86 , 0.854, 0.857, 0.846,\n",
       "        0.855, 0.857, 0.85 , 0.867, 0.859, 0.855, 0.849, 0.859, 0.853,\n",
       "        0.85 , 0.849, 0.848, 0.851, 0.851, 0.862, 0.856, 0.846, 0.862,\n",
       "        0.844, 0.859, 0.862, 0.859, 0.856, 0.852, 0.78 , 0.611, 0.781,\n",
       "        0.781, 0.639, 0.781, 0.782, 0.632, 0.779, 0.785, 0.287, 0.781,\n",
       "        0.782, 0.682, 0.778, 0.78 , 0.358, 0.785, 0.783, 0.659, 0.781,\n",
       "        0.782, 0.342, 0.781, 0.78 , 0.639, 0.781, 0.781, 0.666, 0.781,\n",
       "        0.779, 0.365, 0.778, 0.779, 0.639, 0.779, 0.785, 0.354, 0.786,\n",
       "        0.78 , 0.357, 0.78 , 0.778, 0.377, 0.78 , 0.784, 0.359, 0.782,\n",
       "        0.797, 0.341, 0.786, 0.798, 0.532, 0.812, 0.804, 0.652, 0.798,\n",
       "        0.793, 0.658, 0.806, 0.789, 0.672, 0.785, 0.784, 0.609, 0.803,\n",
       "        0.791, 0.394, 0.795, 0.797, 0.632, 0.805, 0.801, 0.612, 0.791,\n",
       "        0.795, 0.675, 0.799, 0.796, 0.645, 0.797, 0.802, 0.344, 0.793,\n",
       "        0.781, 0.663, 0.791, 0.805, 0.631, 0.796, 0.794, 0.564, 0.793,\n",
       "        0.806, 0.345, 0.795, 0.816, 0.807, 0.808, 0.831, 0.83 , 0.832,\n",
       "        0.832, 0.836, 0.839, 0.836, 0.837, 0.838, 0.816, 0.816, 0.822,\n",
       "        0.835, 0.822, 0.825, 0.838, 0.831, 0.835, 0.84 , 0.836, 0.828,\n",
       "        0.816, 0.805, 0.829, 0.831, 0.832, 0.824, 0.833, 0.83 , 0.836,\n",
       "        0.837, 0.835, 0.833, 0.823, 0.808, 0.826, 0.818, 0.821, 0.814,\n",
       "        0.831, 0.817, 0.825, 0.825, 0.832, 0.829, 0.837, 0.85 , 0.83 ,\n",
       "        0.837, 0.858, 0.85 , 0.839, 0.849, 0.853, 0.848, 0.853, 0.846,\n",
       "        0.836, 0.84 , 0.83 , 0.843, 0.848, 0.839, 0.843, 0.85 , 0.846,\n",
       "        0.847, 0.852, 0.858, 0.835, 0.834, 0.833, 0.84 , 0.849, 0.852,\n",
       "        0.853, 0.843, 0.853, 0.847, 0.854, 0.851, 0.833, 0.831, 0.834,\n",
       "        0.841, 0.843, 0.843, 0.847, 0.848, 0.845, 0.851, 0.844, 0.848]),\n",
       " 'split1_test_f1_micro': array([0.831, 0.864, 0.836, 0.862, 0.853, 0.861, 0.863, 0.843, 0.86 ,\n",
       "        0.862, 0.863, 0.87 , 0.851, 0.859, 0.85 , 0.862, 0.873, 0.859,\n",
       "        0.855, 0.862, 0.865, 0.855, 0.854, 0.855, 0.872, 0.861, 0.857,\n",
       "        0.856, 0.857, 0.846, 0.849, 0.84 , 0.861, 0.859, 0.858, 0.85 ,\n",
       "        0.849, 0.87 , 0.851, 0.859, 0.854, 0.864, 0.861, 0.86 , 0.858,\n",
       "        0.852, 0.853, 0.877, 0.863, 0.838, 0.852, 0.857, 0.854, 0.868,\n",
       "        0.862, 0.85 , 0.872, 0.849, 0.85 , 0.854, 0.855, 0.874, 0.875,\n",
       "        0.866, 0.849, 0.864, 0.857, 0.861, 0.863, 0.852, 0.858, 0.856,\n",
       "        0.867, 0.855, 0.849, 0.859, 0.856, 0.854, 0.853, 0.839, 0.85 ,\n",
       "        0.86 , 0.872, 0.861, 0.867, 0.859, 0.868, 0.86 , 0.861, 0.859,\n",
       "        0.861, 0.859, 0.857, 0.857, 0.845, 0.861, 0.777, 0.673, 0.779,\n",
       "        0.779, 0.639, 0.775, 0.777, 0.639, 0.782, 0.776, 0.649, 0.777,\n",
       "        0.766, 0.361, 0.762, 0.777, 0.361, 0.773, 0.772, 0.639, 0.779,\n",
       "        0.779, 0.639, 0.779, 0.767, 0.361, 0.774, 0.778, 0.42 , 0.781,\n",
       "        0.779, 0.639, 0.774, 0.78 , 0.633, 0.78 , 0.769, 0.361, 0.782,\n",
       "        0.779, 0.296, 0.779, 0.778, 0.639, 0.776, 0.776, 0.407, 0.781,\n",
       "        0.788, 0.422, 0.795, 0.809, 0.656, 0.791, 0.807, 0.515, 0.807,\n",
       "        0.821, 0.577, 0.794, 0.798, 0.582, 0.78 , 0.802, 0.715, 0.791,\n",
       "        0.818, 0.629, 0.801, 0.818, 0.53 , 0.811, 0.793, 0.645, 0.773,\n",
       "        0.811, 0.616, 0.796, 0.797, 0.371, 0.807, 0.802, 0.528, 0.806,\n",
       "        0.784, 0.615, 0.796, 0.808, 0.68 , 0.786, 0.797, 0.549, 0.812,\n",
       "        0.819, 0.701, 0.806, 0.823, 0.812, 0.818, 0.828, 0.824, 0.816,\n",
       "        0.819, 0.829, 0.829, 0.832, 0.826, 0.832, 0.81 , 0.807, 0.811,\n",
       "        0.83 , 0.826, 0.826, 0.824, 0.826, 0.828, 0.828, 0.828, 0.828,\n",
       "        0.81 , 0.814, 0.808, 0.823, 0.817, 0.828, 0.825, 0.824, 0.823,\n",
       "        0.826, 0.824, 0.826, 0.796, 0.805, 0.806, 0.822, 0.819, 0.821,\n",
       "        0.824, 0.82 , 0.829, 0.823, 0.825, 0.825, 0.834, 0.844, 0.843,\n",
       "        0.844, 0.857, 0.851, 0.857, 0.854, 0.849, 0.857, 0.853, 0.856,\n",
       "        0.833, 0.831, 0.831, 0.847, 0.844, 0.845, 0.85 , 0.854, 0.854,\n",
       "        0.859, 0.858, 0.861, 0.835, 0.834, 0.832, 0.854, 0.853, 0.855,\n",
       "        0.853, 0.853, 0.855, 0.858, 0.856, 0.849, 0.832, 0.832, 0.832,\n",
       "        0.844, 0.849, 0.854, 0.853, 0.854, 0.865, 0.856, 0.847, 0.855]),\n",
       " 'split2_test_f1_micro': array([0.84 , 0.843, 0.856, 0.842, 0.839, 0.852, 0.852, 0.852, 0.842,\n",
       "        0.856, 0.847, 0.854, 0.853, 0.85 , 0.853, 0.85 , 0.853, 0.843,\n",
       "        0.838, 0.839, 0.843, 0.851, 0.852, 0.844, 0.842, 0.845, 0.848,\n",
       "        0.846, 0.847, 0.845, 0.853, 0.855, 0.854, 0.854, 0.856, 0.849,\n",
       "        0.848, 0.843, 0.849, 0.847, 0.852, 0.854, 0.847, 0.848, 0.857,\n",
       "        0.844, 0.846, 0.84 , 0.843, 0.836, 0.844, 0.841, 0.853, 0.851,\n",
       "        0.847, 0.842, 0.854, 0.866, 0.854, 0.852, 0.853, 0.843, 0.844,\n",
       "        0.843, 0.841, 0.84 , 0.83 , 0.85 , 0.843, 0.849, 0.847, 0.846,\n",
       "        0.854, 0.851, 0.849, 0.856, 0.842, 0.848, 0.836, 0.846, 0.838,\n",
       "        0.847, 0.846, 0.831, 0.855, 0.85 , 0.84 , 0.844, 0.845, 0.847,\n",
       "        0.835, 0.861, 0.84 , 0.846, 0.843, 0.845, 0.776, 0.361, 0.777,\n",
       "        0.776, 0.551, 0.781, 0.779, 0.639, 0.781, 0.783, 0.639, 0.785,\n",
       "        0.781, 0.361, 0.735, 0.777, 0.36 , 0.777, 0.779, 0.639, 0.782,\n",
       "        0.783, 0.639, 0.779, 0.772, 0.361, 0.782, 0.782, 0.638, 0.783,\n",
       "        0.781, 0.567, 0.783, 0.783, 0.398, 0.783, 0.774, 0.67 , 0.783,\n",
       "        0.782, 0.676, 0.783, 0.776, 0.693, 0.777, 0.776, 0.655, 0.78 ,\n",
       "        0.779, 0.343, 0.776, 0.798, 0.681, 0.789, 0.796, 0.603, 0.79 ,\n",
       "        0.79 , 0.426, 0.793, 0.781, 0.645, 0.786, 0.803, 0.438, 0.79 ,\n",
       "        0.796, 0.559, 0.787, 0.793, 0.678, 0.801, 0.776, 0.31 , 0.791,\n",
       "        0.794, 0.419, 0.802, 0.8  , 0.536, 0.791, 0.797, 0.555, 0.789,\n",
       "        0.777, 0.35 , 0.789, 0.786, 0.505, 0.787, 0.801, 0.281, 0.801,\n",
       "        0.789, 0.731, 0.787, 0.804, 0.802, 0.801, 0.808, 0.812, 0.799,\n",
       "        0.808, 0.811, 0.815, 0.808, 0.809, 0.814, 0.809, 0.807, 0.802,\n",
       "        0.807, 0.807, 0.803, 0.818, 0.813, 0.81 , 0.812, 0.812, 0.808,\n",
       "        0.805, 0.803, 0.807, 0.8  , 0.805, 0.814, 0.812, 0.802, 0.81 ,\n",
       "        0.81 , 0.817, 0.811, 0.79 , 0.79 , 0.798, 0.805, 0.803, 0.804,\n",
       "        0.806, 0.8  , 0.805, 0.805, 0.809, 0.806, 0.809, 0.829, 0.816,\n",
       "        0.826, 0.829, 0.829, 0.829, 0.832, 0.845, 0.833, 0.835, 0.842,\n",
       "        0.808, 0.825, 0.825, 0.83 , 0.828, 0.84 , 0.834, 0.834, 0.833,\n",
       "        0.837, 0.841, 0.839, 0.819, 0.821, 0.821, 0.838, 0.822, 0.831,\n",
       "        0.84 , 0.834, 0.837, 0.837, 0.831, 0.834, 0.821, 0.818, 0.815,\n",
       "        0.825, 0.832, 0.826, 0.829, 0.833, 0.835, 0.84 , 0.835, 0.835]),\n",
       " 'split3_test_f1_micro': array([0.848, 0.861, 0.847, 0.839, 0.857, 0.858, 0.854, 0.861, 0.853,\n",
       "        0.848, 0.837, 0.845, 0.85 , 0.85 , 0.846, 0.856, 0.847, 0.865,\n",
       "        0.856, 0.853, 0.865, 0.855, 0.849, 0.853, 0.863, 0.853, 0.845,\n",
       "        0.843, 0.849, 0.845, 0.861, 0.849, 0.844, 0.852, 0.846, 0.846,\n",
       "        0.852, 0.851, 0.853, 0.856, 0.84 , 0.85 , 0.854, 0.864, 0.87 ,\n",
       "        0.866, 0.866, 0.862, 0.851, 0.856, 0.855, 0.859, 0.857, 0.852,\n",
       "        0.848, 0.852, 0.857, 0.849, 0.853, 0.853, 0.85 , 0.852, 0.857,\n",
       "        0.866, 0.858, 0.846, 0.861, 0.841, 0.849, 0.85 , 0.846, 0.851,\n",
       "        0.853, 0.842, 0.851, 0.854, 0.86 , 0.86 , 0.851, 0.857, 0.844,\n",
       "        0.851, 0.851, 0.845, 0.857, 0.849, 0.846, 0.857, 0.856, 0.853,\n",
       "        0.864, 0.857, 0.854, 0.857, 0.845, 0.862, 0.786, 0.639, 0.779,\n",
       "        0.787, 0.361, 0.79 , 0.79 , 0.639, 0.791, 0.788, 0.361, 0.79 ,\n",
       "        0.778, 0.361, 0.785, 0.789, 0.361, 0.789, 0.786, 0.643, 0.788,\n",
       "        0.789, 0.357, 0.789, 0.785, 0.663, 0.786, 0.785, 0.639, 0.787,\n",
       "        0.784, 0.361, 0.79 , 0.788, 0.643, 0.79 , 0.789, 0.639, 0.781,\n",
       "        0.792, 0.361, 0.792, 0.788, 0.639, 0.788, 0.791, 0.362, 0.783,\n",
       "        0.79 , 0.392, 0.801, 0.8  , 0.471, 0.792, 0.791, 0.436, 0.795,\n",
       "        0.796, 0.409, 0.796, 0.781, 0.628, 0.797, 0.804, 0.592, 0.796,\n",
       "        0.793, 0.431, 0.801, 0.803, 0.526, 0.8  , 0.79 , 0.468, 0.789,\n",
       "        0.784, 0.262, 0.802, 0.796, 0.332, 0.799, 0.801, 0.568, 0.796,\n",
       "        0.79 , 0.658, 0.795, 0.805, 0.34 , 0.805, 0.804, 0.491, 0.8  ,\n",
       "        0.805, 0.486, 0.801, 0.791, 0.796, 0.815, 0.829, 0.819, 0.821,\n",
       "        0.822, 0.829, 0.828, 0.828, 0.828, 0.83 , 0.823, 0.816, 0.822,\n",
       "        0.829, 0.818, 0.826, 0.819, 0.826, 0.824, 0.825, 0.826, 0.829,\n",
       "        0.797, 0.816, 0.799, 0.826, 0.798, 0.797, 0.804, 0.827, 0.815,\n",
       "        0.827, 0.829, 0.827, 0.8  , 0.791, 0.799, 0.816, 0.818, 0.822,\n",
       "        0.822, 0.828, 0.81 , 0.822, 0.823, 0.818, 0.838, 0.839, 0.835,\n",
       "        0.849, 0.852, 0.843, 0.853, 0.845, 0.852, 0.843, 0.852, 0.856,\n",
       "        0.838, 0.841, 0.844, 0.86 , 0.842, 0.846, 0.85 , 0.852, 0.85 ,\n",
       "        0.856, 0.851, 0.852, 0.843, 0.82 , 0.845, 0.851, 0.855, 0.85 ,\n",
       "        0.853, 0.852, 0.852, 0.85 , 0.847, 0.858, 0.84 , 0.824, 0.835,\n",
       "        0.848, 0.852, 0.852, 0.845, 0.855, 0.857, 0.851, 0.861, 0.853]),\n",
       " 'split4_test_f1_micro': array([0.858, 0.862, 0.85 , 0.872, 0.854, 0.868, 0.862, 0.859, 0.855,\n",
       "        0.849, 0.854, 0.869, 0.857, 0.85 , 0.869, 0.855, 0.854, 0.869,\n",
       "        0.866, 0.868, 0.856, 0.858, 0.852, 0.86 , 0.849, 0.845, 0.865,\n",
       "        0.855, 0.868, 0.862, 0.867, 0.865, 0.867, 0.866, 0.859, 0.872,\n",
       "        0.86 , 0.866, 0.854, 0.871, 0.863, 0.867, 0.857, 0.87 , 0.86 ,\n",
       "        0.87 , 0.866, 0.846, 0.854, 0.864, 0.853, 0.863, 0.865, 0.864,\n",
       "        0.858, 0.862, 0.859, 0.856, 0.852, 0.867, 0.86 , 0.862, 0.862,\n",
       "        0.86 , 0.87 , 0.859, 0.853, 0.862, 0.864, 0.863, 0.859, 0.863,\n",
       "        0.867, 0.86 , 0.841, 0.856, 0.861, 0.864, 0.861, 0.862, 0.872,\n",
       "        0.863, 0.863, 0.861, 0.859, 0.862, 0.86 , 0.853, 0.861, 0.863,\n",
       "        0.853, 0.858, 0.872, 0.852, 0.856, 0.864, 0.767, 0.361, 0.76 ,\n",
       "        0.774, 0.639, 0.771, 0.769, 0.348, 0.771, 0.771, 0.639, 0.77 ,\n",
       "        0.774, 0.639, 0.774, 0.77 , 0.488, 0.774, 0.77 , 0.537, 0.77 ,\n",
       "        0.775, 0.639, 0.769, 0.764, 0.361, 0.776, 0.769, 0.642, 0.768,\n",
       "        0.774, 0.65 , 0.763, 0.766, 0.647, 0.773, 0.773, 0.49 , 0.767,\n",
       "        0.774, 0.639, 0.766, 0.767, 0.639, 0.765, 0.765, 0.692, 0.768,\n",
       "        0.78 , 0.588, 0.774, 0.788, 0.671, 0.789, 0.787, 0.593, 0.781,\n",
       "        0.803, 0.549, 0.794, 0.781, 0.59 , 0.781, 0.785, 0.665, 0.781,\n",
       "        0.792, 0.55 , 0.787, 0.784, 0.612, 0.779, 0.778, 0.483, 0.787,\n",
       "        0.788, 0.692, 0.782, 0.784, 0.694, 0.782, 0.791, 0.639, 0.783,\n",
       "        0.785, 0.641, 0.777, 0.8  , 0.658, 0.799, 0.787, 0.649, 0.773,\n",
       "        0.802, 0.571, 0.793, 0.808, 0.787, 0.815, 0.822, 0.815, 0.829,\n",
       "        0.83 , 0.827, 0.833, 0.83 , 0.836, 0.824, 0.81 , 0.803, 0.791,\n",
       "        0.823, 0.828, 0.816, 0.832, 0.834, 0.825, 0.836, 0.837, 0.836,\n",
       "        0.829, 0.791, 0.796, 0.833, 0.821, 0.821, 0.836, 0.833, 0.835,\n",
       "        0.823, 0.835, 0.84 , 0.806, 0.798, 0.803, 0.823, 0.821, 0.81 ,\n",
       "        0.819, 0.818, 0.813, 0.83 , 0.829, 0.83 , 0.835, 0.854, 0.84 ,\n",
       "        0.855, 0.849, 0.858, 0.856, 0.859, 0.853, 0.865, 0.857, 0.861,\n",
       "        0.847, 0.855, 0.84 , 0.865, 0.86 , 0.843, 0.858, 0.865, 0.87 ,\n",
       "        0.86 , 0.87 , 0.865, 0.841, 0.839, 0.844, 0.859, 0.855, 0.838,\n",
       "        0.867, 0.855, 0.857, 0.864, 0.868, 0.854, 0.835, 0.833, 0.846,\n",
       "        0.852, 0.843, 0.848, 0.855, 0.858, 0.854, 0.86 , 0.857, 0.855]),\n",
       " 'mean_test_f1_micro': array([0.8446, 0.8564, 0.8492, 0.857 , 0.8538, 0.8588, 0.859 , 0.8548,\n",
       "        0.8542, 0.8512, 0.8524, 0.8586, 0.852 , 0.853 , 0.8552, 0.8576,\n",
       "        0.8552, 0.8582, 0.8554, 0.8566, 0.8562, 0.8552, 0.853 , 0.8556,\n",
       "        0.8552, 0.8516, 0.8552, 0.8526, 0.8542, 0.8518, 0.859 , 0.8542,\n",
       "        0.8556, 0.8534, 0.8562, 0.854 , 0.8522, 0.8602, 0.8516, 0.8582,\n",
       "        0.8528, 0.8568, 0.8538, 0.859 , 0.8622, 0.8574, 0.8594, 0.856 ,\n",
       "        0.8526, 0.8502, 0.852 , 0.8538, 0.8584, 0.8578, 0.8534, 0.8518,\n",
       "        0.8596, 0.8548, 0.8514, 0.856 , 0.854 , 0.8564, 0.8582, 0.8568,\n",
       "        0.854 , 0.8534, 0.8472, 0.8524, 0.8558, 0.8536, 0.8534, 0.8524,\n",
       "        0.8592, 0.853 , 0.848 , 0.8584, 0.8556, 0.8562, 0.85  , 0.8526,\n",
       "        0.8514, 0.8542, 0.8562, 0.8492, 0.8578, 0.8542, 0.8552, 0.854 ,\n",
       "        0.8538, 0.8568, 0.8514, 0.8588, 0.857 , 0.8542, 0.849 , 0.8568,\n",
       "        0.7772, 0.529 , 0.7752, 0.7794, 0.5658, 0.7796, 0.7794, 0.5794,\n",
       "        0.7808, 0.7806, 0.515 , 0.7806, 0.7762, 0.4808, 0.7668, 0.7786,\n",
       "        0.3856, 0.7796, 0.778 , 0.6234, 0.78  , 0.7816, 0.5232, 0.7794,\n",
       "        0.7736, 0.477 , 0.7798, 0.779 , 0.601 , 0.78  , 0.7794, 0.5164,\n",
       "        0.7776, 0.7792, 0.592 , 0.781 , 0.778 , 0.5028, 0.7798, 0.7814,\n",
       "        0.4658, 0.78  , 0.7774, 0.5974, 0.7772, 0.7784, 0.495 , 0.7788,\n",
       "        0.7868, 0.4172, 0.7864, 0.7986, 0.6022, 0.7946, 0.797 , 0.5598,\n",
       "        0.7942, 0.8006, 0.5238, 0.7966, 0.786 , 0.6234, 0.7858, 0.7956,\n",
       "        0.6038, 0.7922, 0.798 , 0.5126, 0.7942, 0.799 , 0.5956, 0.7992,\n",
       "        0.7876, 0.5036, 0.7862, 0.7944, 0.5328, 0.7962, 0.7946, 0.5156,\n",
       "        0.7952, 0.7986, 0.5268, 0.7934, 0.7834, 0.5854, 0.7896, 0.8008,\n",
       "        0.5628, 0.7946, 0.7966, 0.5068, 0.7958, 0.8042, 0.5668, 0.7964,\n",
       "        0.8084, 0.8008, 0.8114, 0.8236, 0.82  , 0.8194, 0.8222, 0.8264,\n",
       "        0.8288, 0.8268, 0.8272, 0.8276, 0.8136, 0.8098, 0.8096, 0.8248,\n",
       "        0.8202, 0.8192, 0.8262, 0.826 , 0.8244, 0.8282, 0.8278, 0.8258,\n",
       "        0.8114, 0.8058, 0.8078, 0.8226, 0.8146, 0.8168, 0.822 , 0.8232,\n",
       "        0.8238, 0.8246, 0.828 , 0.8274, 0.803 , 0.7984, 0.8064, 0.8168,\n",
       "        0.8164, 0.8142, 0.8204, 0.8166, 0.8164, 0.821 , 0.8236, 0.8216,\n",
       "        0.8306, 0.8432, 0.8328, 0.8422, 0.849 , 0.8462, 0.8468, 0.8478,\n",
       "        0.8504, 0.8492, 0.85  , 0.8522, 0.8324, 0.8384, 0.834 , 0.849 ,\n",
       "        0.8444, 0.8426, 0.847 , 0.851 , 0.8506, 0.8518, 0.8544, 0.855 ,\n",
       "        0.8346, 0.8296, 0.835 , 0.8484, 0.8468, 0.8452, 0.8532, 0.8474,\n",
       "        0.8508, 0.8512, 0.8512, 0.8492, 0.8322, 0.8276, 0.8324, 0.842 ,\n",
       "        0.8438, 0.8446, 0.8458, 0.8496, 0.8512, 0.8516, 0.8488, 0.8492]),\n",
       " 'std_test_f1_micro': array([0.00893532, 0.00786384, 0.00757364, 0.01391402, 0.00870402,\n",
       "        0.00549181, 0.00497996, 0.0066453 , 0.00679412, 0.00719444,\n",
       "        0.00954149, 0.00956243, 0.00282843, 0.00379473, 0.00793473,\n",
       "        0.00531413, 0.00926067, 0.00899778, 0.00958332, 0.01001199,\n",
       "        0.00832827, 0.0024    , 0.00296648, 0.00733757, 0.0107963 ,\n",
       "        0.0060531 , 0.0076    , 0.00722772, 0.00767854, 0.00793473,\n",
       "        0.0069282 , 0.00901998, 0.00786384, 0.00995188, 0.00545527,\n",
       "        0.00927362, 0.00421426, 0.01119643, 0.00174356, 0.00767854,\n",
       "        0.0074135 , 0.00735935, 0.0049558 , 0.00779744, 0.004996  ,\n",
       "        0.00945727, 0.00838093, 0.01291511, 0.00640625, 0.01114271,\n",
       "        0.00424264, 0.00785875, 0.0048    , 0.00688186, 0.00578273,\n",
       "        0.0064    , 0.00640625, 0.00624179, 0.00215407, 0.00554977,\n",
       "        0.00340588, 0.01066958, 0.0102645 , 0.00928224, 0.00969536,\n",
       "        0.00893532, 0.0123677 , 0.00801499, 0.00832827, 0.005004  ,\n",
       "        0.00567803, 0.00646838, 0.0064    , 0.00622896, 0.00357771,\n",
       "        0.00458694, 0.00700286, 0.00545527, 0.00809938, 0.00868562,\n",
       "        0.01151694, 0.00617738, 0.0097857 , 0.01121428, 0.0053066 ,\n",
       "        0.00526878, 0.01047664, 0.00547723, 0.00702567, 0.00601332,\n",
       "        0.0107443 , 0.00132665, 0.010469  , 0.00470744, 0.00576194,\n",
       "        0.00719444, 0.00617738, 0.13856984, 0.00770454, 0.00449889,\n",
       "        0.10792294, 0.00643739, 0.00682935, 0.11573176, 0.0064    ,\n",
       "        0.00621611, 0.15773902, 0.00682935, 0.00581034, 0.14735318,\n",
       "        0.01756588, 0.00615142, 0.05121172, 0.00631189, 0.00616441,\n",
       "        0.04382511, 0.00583095, 0.00463033, 0.14190476, 0.00637495,\n",
       "        0.00786384, 0.14227298, 0.00430813, 0.00547723, 0.09108238,\n",
       "        0.00638749, 0.0032619 , 0.12846104, 0.00904655, 0.00730479,\n",
       "        0.09711025, 0.00554977, 0.00764199, 0.13335576, 0.00661513,\n",
       "        0.00591946, 0.15864098, 0.0083666 , 0.00668132, 0.11216702,\n",
       "        0.0074135 , 0.00873155, 0.14719918, 0.00549181, 0.00667533,\n",
       "        0.09071141, 0.01048046, 0.00668132, 0.08482783, 0.00877724,\n",
       "        0.00756307, 0.07589835, 0.00861162, 0.01107429, 0.09404127,\n",
       "        0.0048    , 0.00675278, 0.03370223, 0.00604649, 0.00909065,\n",
       "        0.09355939, 0.00724983, 0.01013903, 0.08697724, 0.00627375,\n",
       "        0.01133137, 0.05921351, 0.0108148 , 0.00939361, 0.11909425,\n",
       "        0.00676461, 0.00922171, 0.1665958 , 0.00744043, 0.00549909,\n",
       "        0.14394805, 0.00835225, 0.00422374, 0.09848736, 0.00765768,\n",
       "        0.00431741, 0.11888919, 0.0068    , 0.00783326, 0.12683123,\n",
       "        0.00722772, 0.00588558, 0.12370028, 0.0129213 , 0.00957914,\n",
       "        0.14192449, 0.00656049, 0.0108922 , 0.00870402, 0.00615142,\n",
       "        0.00835703, 0.00641872, 0.01167219, 0.00858836, 0.00828493,\n",
       "        0.00790949, 0.00976524, 0.01006777, 0.0081388 , 0.00531413,\n",
       "        0.00526878, 0.0119432 , 0.00968297, 0.00744043, 0.00893085,\n",
       "        0.00770454, 0.00718331, 0.00816333, 0.0097242 , 0.00899778,\n",
       "        0.00938936, 0.01078147, 0.00893085, 0.01154816, 0.0118423 ,\n",
       "        0.01197664, 0.01090688, 0.01224745, 0.01101635, 0.01041921,\n",
       "        0.00868562, 0.00687023, 0.00960417, 0.0112783 , 0.00722772,\n",
       "        0.0102098 , 0.00643117, 0.0068    , 0.00676461, 0.00821219,\n",
       "        0.00915642, 0.00911263, 0.00846168, 0.00793977, 0.00886792,\n",
       "        0.0108922 , 0.00874986, 0.00949526, 0.01002796, 0.01052616,\n",
       "        0.00982649, 0.01099818, 0.00919565, 0.00307246, 0.01107068,\n",
       "        0.00769415, 0.00705408, 0.01306292, 0.01019019, 0.00695701,\n",
       "        0.01247397, 0.01030728, 0.00272764, 0.00804984, 0.00995992,\n",
       "        0.01199333, 0.00870402, 0.0095205 , 0.00905539, 0.00842852,\n",
       "        0.00765768, 0.00883176, 0.00811419, 0.01259206, 0.00915205,\n",
       "        0.00854166, 0.00786384, 0.00711056, 0.00928224, 0.01215566,\n",
       "        0.00818291, 0.00624179, 0.00574804, 0.00997196, 0.00927362,\n",
       "        0.00685274, 0.01003195, 0.00917388, 0.00891291, 0.01032279,\n",
       "        0.00671118, 0.00930376, 0.00754718]),\n",
       " 'rank_test_f1_micro': array([127,  28, 108,  21,  61,   9,   6,  48,  56,  97,  80,  11,  83,\n",
       "         72,  41,  19,  41,  16,  40,  27,  31,  41,  71,  37,  41,  89,\n",
       "         41,  75,  51,  86,   6,  51,  37,  66,  31,  57,  82,   2,  88,\n",
       "         14,  74,  23,  61,   6,   1,  20,   4,  35,  75, 102,  83,  61,\n",
       "         12,  17,  66,  86,   3,  48,  91,  34,  57,  28,  14,  24,  57,\n",
       "         66, 119,  78,  36,  65,  66,  78,   5,  72, 116,  13,  39,  31,\n",
       "        103,  75,  92,  51,  30, 108,  17,  51,  41,  57,  64,  26,  92,\n",
       "         10,  21,  51, 111,  24, 252, 272, 254, 238, 268, 236, 238, 266,\n",
       "        228, 230, 278, 229, 253, 284, 256, 245, 288, 236, 248, 257, 231,\n",
       "        225, 275, 238, 255, 285, 234, 243, 261, 231, 238, 276, 249, 242,\n",
       "        264, 227, 247, 282, 234, 226, 286, 231, 250, 262, 251, 246, 283,\n",
       "        244, 219, 287, 220, 197, 260, 209, 201, 270, 213, 194, 274, 203,\n",
       "        222, 257, 223, 207, 259, 216, 200, 279, 213, 196, 263, 195, 218,\n",
       "        281, 221, 212, 271, 205, 209, 277, 208, 197, 273, 215, 224, 265,\n",
       "        217, 192, 269, 209, 202, 280, 206, 190, 267, 204, 186, 192, 182,\n",
       "        162, 171, 172, 165, 153, 144, 152, 151, 148, 181, 184, 185, 157,\n",
       "        170, 173, 154, 155, 159, 145, 147, 156, 182, 189, 187, 164, 179,\n",
       "        174, 166, 163, 160, 158, 146, 150, 191, 199, 188, 175, 177, 180,\n",
       "        169, 176, 177, 168, 161, 167, 142, 130, 138, 132, 113, 123, 121,\n",
       "        117, 101, 106, 103,  81, 139, 134, 137, 111, 128, 131, 120,  98,\n",
       "        100,  85,  50,  47, 136, 143, 135, 115, 121, 125,  70, 118,  99,\n",
       "         94,  94, 108, 141, 148, 140, 133, 129, 126, 124, 105,  94,  89,\n",
       "        114, 106]),\n",
       " 'split0_test_precision_micro': array([0.846, 0.852, 0.857, 0.87 , 0.866, 0.855, 0.864, 0.859, 0.861,\n",
       "        0.841, 0.861, 0.855, 0.849, 0.856, 0.858, 0.865, 0.849, 0.855,\n",
       "        0.862, 0.861, 0.852, 0.857, 0.858, 0.866, 0.85 , 0.854, 0.861,\n",
       "        0.863, 0.85 , 0.861, 0.865, 0.862, 0.852, 0.836, 0.862, 0.853,\n",
       "        0.852, 0.871, 0.851, 0.858, 0.855, 0.849, 0.85 , 0.853, 0.866,\n",
       "        0.855, 0.866, 0.855, 0.852, 0.857, 0.856, 0.849, 0.863, 0.854,\n",
       "        0.852, 0.853, 0.856, 0.854, 0.848, 0.854, 0.852, 0.851, 0.853,\n",
       "        0.849, 0.852, 0.858, 0.835, 0.848, 0.86 , 0.854, 0.857, 0.846,\n",
       "        0.855, 0.857, 0.85 , 0.867, 0.859, 0.855, 0.849, 0.859, 0.853,\n",
       "        0.85 , 0.849, 0.848, 0.851, 0.851, 0.862, 0.856, 0.846, 0.862,\n",
       "        0.844, 0.859, 0.862, 0.859, 0.856, 0.852, 0.78 , 0.611, 0.781,\n",
       "        0.781, 0.639, 0.781, 0.782, 0.632, 0.779, 0.785, 0.287, 0.781,\n",
       "        0.782, 0.682, 0.778, 0.78 , 0.358, 0.785, 0.783, 0.659, 0.781,\n",
       "        0.782, 0.342, 0.781, 0.78 , 0.639, 0.781, 0.781, 0.666, 0.781,\n",
       "        0.779, 0.365, 0.778, 0.779, 0.639, 0.779, 0.785, 0.354, 0.786,\n",
       "        0.78 , 0.357, 0.78 , 0.778, 0.377, 0.78 , 0.784, 0.359, 0.782,\n",
       "        0.797, 0.341, 0.786, 0.798, 0.532, 0.812, 0.804, 0.652, 0.798,\n",
       "        0.793, 0.658, 0.806, 0.789, 0.672, 0.785, 0.784, 0.609, 0.803,\n",
       "        0.791, 0.394, 0.795, 0.797, 0.632, 0.805, 0.801, 0.612, 0.791,\n",
       "        0.795, 0.675, 0.799, 0.796, 0.645, 0.797, 0.802, 0.344, 0.793,\n",
       "        0.781, 0.663, 0.791, 0.805, 0.631, 0.796, 0.794, 0.564, 0.793,\n",
       "        0.806, 0.345, 0.795, 0.816, 0.807, 0.808, 0.831, 0.83 , 0.832,\n",
       "        0.832, 0.836, 0.839, 0.836, 0.837, 0.838, 0.816, 0.816, 0.822,\n",
       "        0.835, 0.822, 0.825, 0.838, 0.831, 0.835, 0.84 , 0.836, 0.828,\n",
       "        0.816, 0.805, 0.829, 0.831, 0.832, 0.824, 0.833, 0.83 , 0.836,\n",
       "        0.837, 0.835, 0.833, 0.823, 0.808, 0.826, 0.818, 0.821, 0.814,\n",
       "        0.831, 0.817, 0.825, 0.825, 0.832, 0.829, 0.837, 0.85 , 0.83 ,\n",
       "        0.837, 0.858, 0.85 , 0.839, 0.849, 0.853, 0.848, 0.853, 0.846,\n",
       "        0.836, 0.84 , 0.83 , 0.843, 0.848, 0.839, 0.843, 0.85 , 0.846,\n",
       "        0.847, 0.852, 0.858, 0.835, 0.834, 0.833, 0.84 , 0.849, 0.852,\n",
       "        0.853, 0.843, 0.853, 0.847, 0.854, 0.851, 0.833, 0.831, 0.834,\n",
       "        0.841, 0.843, 0.843, 0.847, 0.848, 0.845, 0.851, 0.844, 0.848]),\n",
       " 'split1_test_precision_micro': array([0.831, 0.864, 0.836, 0.862, 0.853, 0.861, 0.863, 0.843, 0.86 ,\n",
       "        0.862, 0.863, 0.87 , 0.851, 0.859, 0.85 , 0.862, 0.873, 0.859,\n",
       "        0.855, 0.862, 0.865, 0.855, 0.854, 0.855, 0.872, 0.861, 0.857,\n",
       "        0.856, 0.857, 0.846, 0.849, 0.84 , 0.861, 0.859, 0.858, 0.85 ,\n",
       "        0.849, 0.87 , 0.851, 0.859, 0.854, 0.864, 0.861, 0.86 , 0.858,\n",
       "        0.852, 0.853, 0.877, 0.863, 0.838, 0.852, 0.857, 0.854, 0.868,\n",
       "        0.862, 0.85 , 0.872, 0.849, 0.85 , 0.854, 0.855, 0.874, 0.875,\n",
       "        0.866, 0.849, 0.864, 0.857, 0.861, 0.863, 0.852, 0.858, 0.856,\n",
       "        0.867, 0.855, 0.849, 0.859, 0.856, 0.854, 0.853, 0.839, 0.85 ,\n",
       "        0.86 , 0.872, 0.861, 0.867, 0.859, 0.868, 0.86 , 0.861, 0.859,\n",
       "        0.861, 0.859, 0.857, 0.857, 0.845, 0.861, 0.777, 0.673, 0.779,\n",
       "        0.779, 0.639, 0.775, 0.777, 0.639, 0.782, 0.776, 0.649, 0.777,\n",
       "        0.766, 0.361, 0.762, 0.777, 0.361, 0.773, 0.772, 0.639, 0.779,\n",
       "        0.779, 0.639, 0.779, 0.767, 0.361, 0.774, 0.778, 0.42 , 0.781,\n",
       "        0.779, 0.639, 0.774, 0.78 , 0.633, 0.78 , 0.769, 0.361, 0.782,\n",
       "        0.779, 0.296, 0.779, 0.778, 0.639, 0.776, 0.776, 0.407, 0.781,\n",
       "        0.788, 0.422, 0.795, 0.809, 0.656, 0.791, 0.807, 0.515, 0.807,\n",
       "        0.821, 0.577, 0.794, 0.798, 0.582, 0.78 , 0.802, 0.715, 0.791,\n",
       "        0.818, 0.629, 0.801, 0.818, 0.53 , 0.811, 0.793, 0.645, 0.773,\n",
       "        0.811, 0.616, 0.796, 0.797, 0.371, 0.807, 0.802, 0.528, 0.806,\n",
       "        0.784, 0.615, 0.796, 0.808, 0.68 , 0.786, 0.797, 0.549, 0.812,\n",
       "        0.819, 0.701, 0.806, 0.823, 0.812, 0.818, 0.828, 0.824, 0.816,\n",
       "        0.819, 0.829, 0.829, 0.832, 0.826, 0.832, 0.81 , 0.807, 0.811,\n",
       "        0.83 , 0.826, 0.826, 0.824, 0.826, 0.828, 0.828, 0.828, 0.828,\n",
       "        0.81 , 0.814, 0.808, 0.823, 0.817, 0.828, 0.825, 0.824, 0.823,\n",
       "        0.826, 0.824, 0.826, 0.796, 0.805, 0.806, 0.822, 0.819, 0.821,\n",
       "        0.824, 0.82 , 0.829, 0.823, 0.825, 0.825, 0.834, 0.844, 0.843,\n",
       "        0.844, 0.857, 0.851, 0.857, 0.854, 0.849, 0.857, 0.853, 0.856,\n",
       "        0.833, 0.831, 0.831, 0.847, 0.844, 0.845, 0.85 , 0.854, 0.854,\n",
       "        0.859, 0.858, 0.861, 0.835, 0.834, 0.832, 0.854, 0.853, 0.855,\n",
       "        0.853, 0.853, 0.855, 0.858, 0.856, 0.849, 0.832, 0.832, 0.832,\n",
       "        0.844, 0.849, 0.854, 0.853, 0.854, 0.865, 0.856, 0.847, 0.855]),\n",
       " 'split2_test_precision_micro': array([0.84 , 0.843, 0.856, 0.842, 0.839, 0.852, 0.852, 0.852, 0.842,\n",
       "        0.856, 0.847, 0.854, 0.853, 0.85 , 0.853, 0.85 , 0.853, 0.843,\n",
       "        0.838, 0.839, 0.843, 0.851, 0.852, 0.844, 0.842, 0.845, 0.848,\n",
       "        0.846, 0.847, 0.845, 0.853, 0.855, 0.854, 0.854, 0.856, 0.849,\n",
       "        0.848, 0.843, 0.849, 0.847, 0.852, 0.854, 0.847, 0.848, 0.857,\n",
       "        0.844, 0.846, 0.84 , 0.843, 0.836, 0.844, 0.841, 0.853, 0.851,\n",
       "        0.847, 0.842, 0.854, 0.866, 0.854, 0.852, 0.853, 0.843, 0.844,\n",
       "        0.843, 0.841, 0.84 , 0.83 , 0.85 , 0.843, 0.849, 0.847, 0.846,\n",
       "        0.854, 0.851, 0.849, 0.856, 0.842, 0.848, 0.836, 0.846, 0.838,\n",
       "        0.847, 0.846, 0.831, 0.855, 0.85 , 0.84 , 0.844, 0.845, 0.847,\n",
       "        0.835, 0.861, 0.84 , 0.846, 0.843, 0.845, 0.776, 0.361, 0.777,\n",
       "        0.776, 0.551, 0.781, 0.779, 0.639, 0.781, 0.783, 0.639, 0.785,\n",
       "        0.781, 0.361, 0.735, 0.777, 0.36 , 0.777, 0.779, 0.639, 0.782,\n",
       "        0.783, 0.639, 0.779, 0.772, 0.361, 0.782, 0.782, 0.638, 0.783,\n",
       "        0.781, 0.567, 0.783, 0.783, 0.398, 0.783, 0.774, 0.67 , 0.783,\n",
       "        0.782, 0.676, 0.783, 0.776, 0.693, 0.777, 0.776, 0.655, 0.78 ,\n",
       "        0.779, 0.343, 0.776, 0.798, 0.681, 0.789, 0.796, 0.603, 0.79 ,\n",
       "        0.79 , 0.426, 0.793, 0.781, 0.645, 0.786, 0.803, 0.438, 0.79 ,\n",
       "        0.796, 0.559, 0.787, 0.793, 0.678, 0.801, 0.776, 0.31 , 0.791,\n",
       "        0.794, 0.419, 0.802, 0.8  , 0.536, 0.791, 0.797, 0.555, 0.789,\n",
       "        0.777, 0.35 , 0.789, 0.786, 0.505, 0.787, 0.801, 0.281, 0.801,\n",
       "        0.789, 0.731, 0.787, 0.804, 0.802, 0.801, 0.808, 0.812, 0.799,\n",
       "        0.808, 0.811, 0.815, 0.808, 0.809, 0.814, 0.809, 0.807, 0.802,\n",
       "        0.807, 0.807, 0.803, 0.818, 0.813, 0.81 , 0.812, 0.812, 0.808,\n",
       "        0.805, 0.803, 0.807, 0.8  , 0.805, 0.814, 0.812, 0.802, 0.81 ,\n",
       "        0.81 , 0.817, 0.811, 0.79 , 0.79 , 0.798, 0.805, 0.803, 0.804,\n",
       "        0.806, 0.8  , 0.805, 0.805, 0.809, 0.806, 0.809, 0.829, 0.816,\n",
       "        0.826, 0.829, 0.829, 0.829, 0.832, 0.845, 0.833, 0.835, 0.842,\n",
       "        0.808, 0.825, 0.825, 0.83 , 0.828, 0.84 , 0.834, 0.834, 0.833,\n",
       "        0.837, 0.841, 0.839, 0.819, 0.821, 0.821, 0.838, 0.822, 0.831,\n",
       "        0.84 , 0.834, 0.837, 0.837, 0.831, 0.834, 0.821, 0.818, 0.815,\n",
       "        0.825, 0.832, 0.826, 0.829, 0.833, 0.835, 0.84 , 0.835, 0.835]),\n",
       " 'split3_test_precision_micro': array([0.848, 0.861, 0.847, 0.839, 0.857, 0.858, 0.854, 0.861, 0.853,\n",
       "        0.848, 0.837, 0.845, 0.85 , 0.85 , 0.846, 0.856, 0.847, 0.865,\n",
       "        0.856, 0.853, 0.865, 0.855, 0.849, 0.853, 0.863, 0.853, 0.845,\n",
       "        0.843, 0.849, 0.845, 0.861, 0.849, 0.844, 0.852, 0.846, 0.846,\n",
       "        0.852, 0.851, 0.853, 0.856, 0.84 , 0.85 , 0.854, 0.864, 0.87 ,\n",
       "        0.866, 0.866, 0.862, 0.851, 0.856, 0.855, 0.859, 0.857, 0.852,\n",
       "        0.848, 0.852, 0.857, 0.849, 0.853, 0.853, 0.85 , 0.852, 0.857,\n",
       "        0.866, 0.858, 0.846, 0.861, 0.841, 0.849, 0.85 , 0.846, 0.851,\n",
       "        0.853, 0.842, 0.851, 0.854, 0.86 , 0.86 , 0.851, 0.857, 0.844,\n",
       "        0.851, 0.851, 0.845, 0.857, 0.849, 0.846, 0.857, 0.856, 0.853,\n",
       "        0.864, 0.857, 0.854, 0.857, 0.845, 0.862, 0.786, 0.639, 0.779,\n",
       "        0.787, 0.361, 0.79 , 0.79 , 0.639, 0.791, 0.788, 0.361, 0.79 ,\n",
       "        0.778, 0.361, 0.785, 0.789, 0.361, 0.789, 0.786, 0.643, 0.788,\n",
       "        0.789, 0.357, 0.789, 0.785, 0.663, 0.786, 0.785, 0.639, 0.787,\n",
       "        0.784, 0.361, 0.79 , 0.788, 0.643, 0.79 , 0.789, 0.639, 0.781,\n",
       "        0.792, 0.361, 0.792, 0.788, 0.639, 0.788, 0.791, 0.362, 0.783,\n",
       "        0.79 , 0.392, 0.801, 0.8  , 0.471, 0.792, 0.791, 0.436, 0.795,\n",
       "        0.796, 0.409, 0.796, 0.781, 0.628, 0.797, 0.804, 0.592, 0.796,\n",
       "        0.793, 0.431, 0.801, 0.803, 0.526, 0.8  , 0.79 , 0.468, 0.789,\n",
       "        0.784, 0.262, 0.802, 0.796, 0.332, 0.799, 0.801, 0.568, 0.796,\n",
       "        0.79 , 0.658, 0.795, 0.805, 0.34 , 0.805, 0.804, 0.491, 0.8  ,\n",
       "        0.805, 0.486, 0.801, 0.791, 0.796, 0.815, 0.829, 0.819, 0.821,\n",
       "        0.822, 0.829, 0.828, 0.828, 0.828, 0.83 , 0.823, 0.816, 0.822,\n",
       "        0.829, 0.818, 0.826, 0.819, 0.826, 0.824, 0.825, 0.826, 0.829,\n",
       "        0.797, 0.816, 0.799, 0.826, 0.798, 0.797, 0.804, 0.827, 0.815,\n",
       "        0.827, 0.829, 0.827, 0.8  , 0.791, 0.799, 0.816, 0.818, 0.822,\n",
       "        0.822, 0.828, 0.81 , 0.822, 0.823, 0.818, 0.838, 0.839, 0.835,\n",
       "        0.849, 0.852, 0.843, 0.853, 0.845, 0.852, 0.843, 0.852, 0.856,\n",
       "        0.838, 0.841, 0.844, 0.86 , 0.842, 0.846, 0.85 , 0.852, 0.85 ,\n",
       "        0.856, 0.851, 0.852, 0.843, 0.82 , 0.845, 0.851, 0.855, 0.85 ,\n",
       "        0.853, 0.852, 0.852, 0.85 , 0.847, 0.858, 0.84 , 0.824, 0.835,\n",
       "        0.848, 0.852, 0.852, 0.845, 0.855, 0.857, 0.851, 0.861, 0.853]),\n",
       " 'split4_test_precision_micro': array([0.858, 0.862, 0.85 , 0.872, 0.854, 0.868, 0.862, 0.859, 0.855,\n",
       "        0.849, 0.854, 0.869, 0.857, 0.85 , 0.869, 0.855, 0.854, 0.869,\n",
       "        0.866, 0.868, 0.856, 0.858, 0.852, 0.86 , 0.849, 0.845, 0.865,\n",
       "        0.855, 0.868, 0.862, 0.867, 0.865, 0.867, 0.866, 0.859, 0.872,\n",
       "        0.86 , 0.866, 0.854, 0.871, 0.863, 0.867, 0.857, 0.87 , 0.86 ,\n",
       "        0.87 , 0.866, 0.846, 0.854, 0.864, 0.853, 0.863, 0.865, 0.864,\n",
       "        0.858, 0.862, 0.859, 0.856, 0.852, 0.867, 0.86 , 0.862, 0.862,\n",
       "        0.86 , 0.87 , 0.859, 0.853, 0.862, 0.864, 0.863, 0.859, 0.863,\n",
       "        0.867, 0.86 , 0.841, 0.856, 0.861, 0.864, 0.861, 0.862, 0.872,\n",
       "        0.863, 0.863, 0.861, 0.859, 0.862, 0.86 , 0.853, 0.861, 0.863,\n",
       "        0.853, 0.858, 0.872, 0.852, 0.856, 0.864, 0.767, 0.361, 0.76 ,\n",
       "        0.774, 0.639, 0.771, 0.769, 0.348, 0.771, 0.771, 0.639, 0.77 ,\n",
       "        0.774, 0.639, 0.774, 0.77 , 0.488, 0.774, 0.77 , 0.537, 0.77 ,\n",
       "        0.775, 0.639, 0.769, 0.764, 0.361, 0.776, 0.769, 0.642, 0.768,\n",
       "        0.774, 0.65 , 0.763, 0.766, 0.647, 0.773, 0.773, 0.49 , 0.767,\n",
       "        0.774, 0.639, 0.766, 0.767, 0.639, 0.765, 0.765, 0.692, 0.768,\n",
       "        0.78 , 0.588, 0.774, 0.788, 0.671, 0.789, 0.787, 0.593, 0.781,\n",
       "        0.803, 0.549, 0.794, 0.781, 0.59 , 0.781, 0.785, 0.665, 0.781,\n",
       "        0.792, 0.55 , 0.787, 0.784, 0.612, 0.779, 0.778, 0.483, 0.787,\n",
       "        0.788, 0.692, 0.782, 0.784, 0.694, 0.782, 0.791, 0.639, 0.783,\n",
       "        0.785, 0.641, 0.777, 0.8  , 0.658, 0.799, 0.787, 0.649, 0.773,\n",
       "        0.802, 0.571, 0.793, 0.808, 0.787, 0.815, 0.822, 0.815, 0.829,\n",
       "        0.83 , 0.827, 0.833, 0.83 , 0.836, 0.824, 0.81 , 0.803, 0.791,\n",
       "        0.823, 0.828, 0.816, 0.832, 0.834, 0.825, 0.836, 0.837, 0.836,\n",
       "        0.829, 0.791, 0.796, 0.833, 0.821, 0.821, 0.836, 0.833, 0.835,\n",
       "        0.823, 0.835, 0.84 , 0.806, 0.798, 0.803, 0.823, 0.821, 0.81 ,\n",
       "        0.819, 0.818, 0.813, 0.83 , 0.829, 0.83 , 0.835, 0.854, 0.84 ,\n",
       "        0.855, 0.849, 0.858, 0.856, 0.859, 0.853, 0.865, 0.857, 0.861,\n",
       "        0.847, 0.855, 0.84 , 0.865, 0.86 , 0.843, 0.858, 0.865, 0.87 ,\n",
       "        0.86 , 0.87 , 0.865, 0.841, 0.839, 0.844, 0.859, 0.855, 0.838,\n",
       "        0.867, 0.855, 0.857, 0.864, 0.868, 0.854, 0.835, 0.833, 0.846,\n",
       "        0.852, 0.843, 0.848, 0.855, 0.858, 0.854, 0.86 , 0.857, 0.855]),\n",
       " 'mean_test_precision_micro': array([0.8446, 0.8564, 0.8492, 0.857 , 0.8538, 0.8588, 0.859 , 0.8548,\n",
       "        0.8542, 0.8512, 0.8524, 0.8586, 0.852 , 0.853 , 0.8552, 0.8576,\n",
       "        0.8552, 0.8582, 0.8554, 0.8566, 0.8562, 0.8552, 0.853 , 0.8556,\n",
       "        0.8552, 0.8516, 0.8552, 0.8526, 0.8542, 0.8518, 0.859 , 0.8542,\n",
       "        0.8556, 0.8534, 0.8562, 0.854 , 0.8522, 0.8602, 0.8516, 0.8582,\n",
       "        0.8528, 0.8568, 0.8538, 0.859 , 0.8622, 0.8574, 0.8594, 0.856 ,\n",
       "        0.8526, 0.8502, 0.852 , 0.8538, 0.8584, 0.8578, 0.8534, 0.8518,\n",
       "        0.8596, 0.8548, 0.8514, 0.856 , 0.854 , 0.8564, 0.8582, 0.8568,\n",
       "        0.854 , 0.8534, 0.8472, 0.8524, 0.8558, 0.8536, 0.8534, 0.8524,\n",
       "        0.8592, 0.853 , 0.848 , 0.8584, 0.8556, 0.8562, 0.85  , 0.8526,\n",
       "        0.8514, 0.8542, 0.8562, 0.8492, 0.8578, 0.8542, 0.8552, 0.854 ,\n",
       "        0.8538, 0.8568, 0.8514, 0.8588, 0.857 , 0.8542, 0.849 , 0.8568,\n",
       "        0.7772, 0.529 , 0.7752, 0.7794, 0.5658, 0.7796, 0.7794, 0.5794,\n",
       "        0.7808, 0.7806, 0.515 , 0.7806, 0.7762, 0.4808, 0.7668, 0.7786,\n",
       "        0.3856, 0.7796, 0.778 , 0.6234, 0.78  , 0.7816, 0.5232, 0.7794,\n",
       "        0.7736, 0.477 , 0.7798, 0.779 , 0.601 , 0.78  , 0.7794, 0.5164,\n",
       "        0.7776, 0.7792, 0.592 , 0.781 , 0.778 , 0.5028, 0.7798, 0.7814,\n",
       "        0.4658, 0.78  , 0.7774, 0.5974, 0.7772, 0.7784, 0.495 , 0.7788,\n",
       "        0.7868, 0.4172, 0.7864, 0.7986, 0.6022, 0.7946, 0.797 , 0.5598,\n",
       "        0.7942, 0.8006, 0.5238, 0.7966, 0.786 , 0.6234, 0.7858, 0.7956,\n",
       "        0.6038, 0.7922, 0.798 , 0.5126, 0.7942, 0.799 , 0.5956, 0.7992,\n",
       "        0.7876, 0.5036, 0.7862, 0.7944, 0.5328, 0.7962, 0.7946, 0.5156,\n",
       "        0.7952, 0.7986, 0.5268, 0.7934, 0.7834, 0.5854, 0.7896, 0.8008,\n",
       "        0.5628, 0.7946, 0.7966, 0.5068, 0.7958, 0.8042, 0.5668, 0.7964,\n",
       "        0.8084, 0.8008, 0.8114, 0.8236, 0.82  , 0.8194, 0.8222, 0.8264,\n",
       "        0.8288, 0.8268, 0.8272, 0.8276, 0.8136, 0.8098, 0.8096, 0.8248,\n",
       "        0.8202, 0.8192, 0.8262, 0.826 , 0.8244, 0.8282, 0.8278, 0.8258,\n",
       "        0.8114, 0.8058, 0.8078, 0.8226, 0.8146, 0.8168, 0.822 , 0.8232,\n",
       "        0.8238, 0.8246, 0.828 , 0.8274, 0.803 , 0.7984, 0.8064, 0.8168,\n",
       "        0.8164, 0.8142, 0.8204, 0.8166, 0.8164, 0.821 , 0.8236, 0.8216,\n",
       "        0.8306, 0.8432, 0.8328, 0.8422, 0.849 , 0.8462, 0.8468, 0.8478,\n",
       "        0.8504, 0.8492, 0.85  , 0.8522, 0.8324, 0.8384, 0.834 , 0.849 ,\n",
       "        0.8444, 0.8426, 0.847 , 0.851 , 0.8506, 0.8518, 0.8544, 0.855 ,\n",
       "        0.8346, 0.8296, 0.835 , 0.8484, 0.8468, 0.8452, 0.8532, 0.8474,\n",
       "        0.8508, 0.8512, 0.8512, 0.8492, 0.8322, 0.8276, 0.8324, 0.842 ,\n",
       "        0.8438, 0.8446, 0.8458, 0.8496, 0.8512, 0.8516, 0.8488, 0.8492]),\n",
       " 'std_test_precision_micro': array([0.00893532, 0.00786384, 0.00757364, 0.01391402, 0.00870402,\n",
       "        0.00549181, 0.00497996, 0.0066453 , 0.00679412, 0.00719444,\n",
       "        0.00954149, 0.00956243, 0.00282843, 0.00379473, 0.00793473,\n",
       "        0.00531413, 0.00926067, 0.00899778, 0.00958332, 0.01001199,\n",
       "        0.00832827, 0.0024    , 0.00296648, 0.00733757, 0.0107963 ,\n",
       "        0.0060531 , 0.0076    , 0.00722772, 0.00767854, 0.00793473,\n",
       "        0.0069282 , 0.00901998, 0.00786384, 0.00995188, 0.00545527,\n",
       "        0.00927362, 0.00421426, 0.01119643, 0.00174356, 0.00767854,\n",
       "        0.0074135 , 0.00735935, 0.0049558 , 0.00779744, 0.004996  ,\n",
       "        0.00945727, 0.00838093, 0.01291511, 0.00640625, 0.01114271,\n",
       "        0.00424264, 0.00785875, 0.0048    , 0.00688186, 0.00578273,\n",
       "        0.0064    , 0.00640625, 0.00624179, 0.00215407, 0.00554977,\n",
       "        0.00340588, 0.01066958, 0.0102645 , 0.00928224, 0.00969536,\n",
       "        0.00893532, 0.0123677 , 0.00801499, 0.00832827, 0.005004  ,\n",
       "        0.00567803, 0.00646838, 0.0064    , 0.00622896, 0.00357771,\n",
       "        0.00458694, 0.00700286, 0.00545527, 0.00809938, 0.00868562,\n",
       "        0.01151694, 0.00617738, 0.0097857 , 0.01121428, 0.0053066 ,\n",
       "        0.00526878, 0.01047664, 0.00547723, 0.00702567, 0.00601332,\n",
       "        0.0107443 , 0.00132665, 0.010469  , 0.00470744, 0.00576194,\n",
       "        0.00719444, 0.00617738, 0.13856984, 0.00770454, 0.00449889,\n",
       "        0.10792294, 0.00643739, 0.00682935, 0.11573176, 0.0064    ,\n",
       "        0.00621611, 0.15773902, 0.00682935, 0.00581034, 0.14735318,\n",
       "        0.01756588, 0.00615142, 0.05121172, 0.00631189, 0.00616441,\n",
       "        0.04382511, 0.00583095, 0.00463033, 0.14190476, 0.00637495,\n",
       "        0.00786384, 0.14227298, 0.00430813, 0.00547723, 0.09108238,\n",
       "        0.00638749, 0.0032619 , 0.12846104, 0.00904655, 0.00730479,\n",
       "        0.09711025, 0.00554977, 0.00764199, 0.13335576, 0.00661513,\n",
       "        0.00591946, 0.15864098, 0.0083666 , 0.00668132, 0.11216702,\n",
       "        0.0074135 , 0.00873155, 0.14719918, 0.00549181, 0.00667533,\n",
       "        0.09071141, 0.01048046, 0.00668132, 0.08482783, 0.00877724,\n",
       "        0.00756307, 0.07589835, 0.00861162, 0.01107429, 0.09404127,\n",
       "        0.0048    , 0.00675278, 0.03370223, 0.00604649, 0.00909065,\n",
       "        0.09355939, 0.00724983, 0.01013903, 0.08697724, 0.00627375,\n",
       "        0.01133137, 0.05921351, 0.0108148 , 0.00939361, 0.11909425,\n",
       "        0.00676461, 0.00922171, 0.1665958 , 0.00744043, 0.00549909,\n",
       "        0.14394805, 0.00835225, 0.00422374, 0.09848736, 0.00765768,\n",
       "        0.00431741, 0.11888919, 0.0068    , 0.00783326, 0.12683123,\n",
       "        0.00722772, 0.00588558, 0.12370028, 0.0129213 , 0.00957914,\n",
       "        0.14192449, 0.00656049, 0.0108922 , 0.00870402, 0.00615142,\n",
       "        0.00835703, 0.00641872, 0.01167219, 0.00858836, 0.00828493,\n",
       "        0.00790949, 0.00976524, 0.01006777, 0.0081388 , 0.00531413,\n",
       "        0.00526878, 0.0119432 , 0.00968297, 0.00744043, 0.00893085,\n",
       "        0.00770454, 0.00718331, 0.00816333, 0.0097242 , 0.00899778,\n",
       "        0.00938936, 0.01078147, 0.00893085, 0.01154816, 0.0118423 ,\n",
       "        0.01197664, 0.01090688, 0.01224745, 0.01101635, 0.01041921,\n",
       "        0.00868562, 0.00687023, 0.00960417, 0.0112783 , 0.00722772,\n",
       "        0.0102098 , 0.00643117, 0.0068    , 0.00676461, 0.00821219,\n",
       "        0.00915642, 0.00911263, 0.00846168, 0.00793977, 0.00886792,\n",
       "        0.0108922 , 0.00874986, 0.00949526, 0.01002796, 0.01052616,\n",
       "        0.00982649, 0.01099818, 0.00919565, 0.00307246, 0.01107068,\n",
       "        0.00769415, 0.00705408, 0.01306292, 0.01019019, 0.00695701,\n",
       "        0.01247397, 0.01030728, 0.00272764, 0.00804984, 0.00995992,\n",
       "        0.01199333, 0.00870402, 0.0095205 , 0.00905539, 0.00842852,\n",
       "        0.00765768, 0.00883176, 0.00811419, 0.01259206, 0.00915205,\n",
       "        0.00854166, 0.00786384, 0.00711056, 0.00928224, 0.01215566,\n",
       "        0.00818291, 0.00624179, 0.00574804, 0.00997196, 0.00927362,\n",
       "        0.00685274, 0.01003195, 0.00917388, 0.00891291, 0.01032279,\n",
       "        0.00671118, 0.00930376, 0.00754718]),\n",
       " 'rank_test_precision_micro': array([126,  28, 108,  21,  61,   9,   6,  48,  51,  97,  80,  11,  83,\n",
       "         72,  41,  19,  41,  16,  40,  27,  32,  41,  71,  37,  41,  90,\n",
       "         41,  75,  52,  86,   6,  52,  37,  66,  30,  57,  81,   2,  88,\n",
       "         14,  74,  23,  61,   6,   1,  20,   4,  34,  75, 102,  83,  61,\n",
       "         12,  17,  66,  86,   3,  48,  91,  34,  57,  28,  14,  25,  57,\n",
       "         66, 119,  78,  36,  65,  66,  78,   5,  72, 116,  13,  39,  32,\n",
       "        103,  75,  92,  52,  30, 108,  17,  52,  41,  57,  64,  23,  92,\n",
       "         10,  21,  52, 111,  25, 252, 272, 254, 238, 268, 236, 238, 266,\n",
       "        228, 229, 278, 229, 253, 284, 256, 245, 288, 236, 248, 257, 231,\n",
       "        225, 275, 238, 255, 285, 234, 243, 261, 231, 238, 276, 249, 242,\n",
       "        264, 227, 247, 282, 234, 226, 286, 231, 250, 262, 251, 246, 283,\n",
       "        244, 219, 287, 220, 197, 260, 209, 201, 270, 213, 194, 274, 203,\n",
       "        222, 257, 223, 207, 259, 216, 200, 279, 213, 196, 263, 195, 218,\n",
       "        281, 221, 212, 271, 205, 211, 277, 208, 197, 273, 215, 224, 265,\n",
       "        217, 192, 269, 209, 202, 280, 206, 190, 267, 204, 186, 192, 182,\n",
       "        162, 171, 172, 165, 153, 144, 152, 151, 148, 181, 184, 185, 157,\n",
       "        170, 173, 154, 155, 159, 145, 147, 156, 182, 189, 187, 164, 179,\n",
       "        174, 166, 163, 160, 158, 146, 150, 191, 199, 188, 174, 177, 180,\n",
       "        169, 176, 177, 168, 161, 167, 142, 130, 138, 132, 113, 123, 121,\n",
       "        117, 101, 106, 103,  81, 139, 134, 137, 111, 128, 131, 120,  98,\n",
       "        100,  85,  50,  47, 136, 143, 135, 115, 121, 125,  70, 118,  99,\n",
       "         94,  94, 108, 141, 148, 140, 133, 129, 126, 124, 105,  94,  88,\n",
       "        114, 106]),\n",
       " 'split0_test_roc_auc_ovo': array([0.91354219, 0.91060738, 0.90846588, 0.91914739, 0.9214753 ,\n",
       "        0.91314337, 0.91885694, 0.90967535, 0.90725207, 0.90560476,\n",
       "        0.91631661, 0.92131924, 0.91093251, 0.91573138, 0.91940315,\n",
       "        0.92520776, 0.90493283, 0.9131217 , 0.91070709, 0.91300032,\n",
       "        0.91031693, 0.90747749, 0.91208996, 0.91565769, 0.91111024,\n",
       "        0.91178651, 0.92043923, 0.91954621, 0.90951495, 0.91779919,\n",
       "        0.91736569, 0.91596981, 0.90955397, 0.90755119, 0.92150131,\n",
       "        0.91208996, 0.91436152, 0.9186662 , 0.91101921, 0.91673278,\n",
       "        0.91085881, 0.91626459, 0.91907369, 0.90822312, 0.92237698,\n",
       "        0.91369826, 0.9186792 , 0.91701455, 0.91378062, 0.91238041,\n",
       "        0.91027792, 0.90201102, 0.9214016 , 0.9174654 , 0.90648477,\n",
       "        0.91114926, 0.90457736, 0.91032994, 0.9075772 , 0.91254947,\n",
       "        0.9160175 , 0.91258849, 0.91347283, 0.91013053, 0.91547562,\n",
       "        0.91000915, 0.90536633, 0.91196858, 0.90522328, 0.89994321,\n",
       "        0.91144404, 0.91058571, 0.9119339 , 0.9109065 , 0.91417511,\n",
       "        0.92378153, 0.91945951, 0.91912571, 0.9165507 , 0.91391067,\n",
       "        0.91146572, 0.90346759, 0.90344158, 0.898556  , 0.91390634,\n",
       "        0.91388466, 0.91294396, 0.91508113, 0.90999181, 0.92098544,\n",
       "        0.90191131, 0.91251046, 0.90866529, 0.90785897, 0.91177784,\n",
       "        0.90405282, 0.843991  , 0.43834939, 0.84530451, 0.84745902,\n",
       "        0.50873291, 0.84624088, 0.84516146, 0.58506843, 0.84720759,\n",
       "        0.8462799 , 0.31077818, 0.84685212, 0.84775814, 0.47131295,\n",
       "        0.84165009, 0.84646197, 0.51808357, 0.84768878, 0.84377858,\n",
       "        0.51294223, 0.84533486, 0.84656167, 0.45504359, 0.8467264 ,\n",
       "        0.84624955, 0.44796015, 0.84582472, 0.84786652, 0.6912246 ,\n",
       "        0.84673941, 0.84509643, 0.45877171, 0.84400834, 0.84468894,\n",
       "        0.5991356 , 0.84494471, 0.84678276, 0.34969807, 0.84798356,\n",
       "        0.8461325 , 0.58595277, 0.84623655, 0.84728996, 0.54682047,\n",
       "        0.84525683, 0.84412105, 0.69607983, 0.8458594 , 0.85693973,\n",
       "        0.53143979, 0.8374321 , 0.85954942, 0.53199034, 0.87045201,\n",
       "        0.86916017, 0.67325157, 0.86087594, 0.86507224, 0.42969668,\n",
       "        0.87406743, 0.85940636, 0.38266162, 0.8526914 , 0.84993866,\n",
       "        0.69209161, 0.87012689, 0.86406218, 0.50123765, 0.85869542,\n",
       "        0.86367203, 0.6099645 , 0.8693856 , 0.8642746 , 0.43472965,\n",
       "        0.84533052, 0.85706978, 0.67073726, 0.8643743 , 0.85761166,\n",
       "        0.70948374, 0.86856194, 0.87128   , 0.46729871, 0.86533668,\n",
       "        0.8535454 , 0.65445056, 0.85774171, 0.8679507 , 0.63839795,\n",
       "        0.86418356, 0.87098956, 0.38555308, 0.86057248, 0.86589156,\n",
       "        0.525995  , 0.85732555, 0.88100781, 0.87252416, 0.87378999,\n",
       "        0.89357072, 0.89361407, 0.89406925, 0.89450709, 0.89450275,\n",
       "        0.89662691, 0.89681332, 0.89664859, 0.89406058, 0.8774184 ,\n",
       "        0.8757104 , 0.88130259, 0.89687401, 0.89107374, 0.8897819 ,\n",
       "        0.89552148, 0.89360973, 0.89654021, 0.89449408, 0.89419496,\n",
       "        0.89167631, 0.87940818, 0.87033063, 0.89178902, 0.89425999,\n",
       "        0.89533941, 0.8858457 , 0.89321958, 0.89351003, 0.89441605,\n",
       "        0.89544345, 0.89513566, 0.89536542, 0.88547289, 0.87184789,\n",
       "        0.89025442, 0.88219994, 0.88353513, 0.8774921 , 0.89274706,\n",
       "        0.88503071, 0.89111276, 0.89194942, 0.89495359, 0.89314155,\n",
       "        0.90610762, 0.91063773, 0.90303842, 0.9115871 , 0.91747407,\n",
       "        0.91231105, 0.91250612, 0.91281825, 0.91617789, 0.91606085,\n",
       "        0.91384998, 0.91438319, 0.90582151, 0.90846588, 0.90351961,\n",
       "        0.91310436, 0.91522852, 0.90990077, 0.91404939, 0.9141491 ,\n",
       "        0.91562734, 0.91287894, 0.91659405, 0.91646834, 0.90867396,\n",
       "        0.90779395, 0.9063157 , 0.91009585, 0.91366791, 0.91591779,\n",
       "        0.91436585, 0.91249745, 0.91427915, 0.91290495, 0.91371993,\n",
       "        0.91633829, 0.90411784, 0.90189831, 0.90004292, 0.90933288,\n",
       "        0.91254514, 0.91054236, 0.91593513, 0.91606085, 0.91102788,\n",
       "        0.91536291, 0.91281825, 0.9156967 ]),\n",
       " 'split1_test_roc_auc_ovo': array([0.90962766, 0.91913872, 0.90988343, 0.9208814 , 0.91414043,\n",
       "        0.92150564, 0.92287551, 0.91582242, 0.91936414, 0.91989301,\n",
       "        0.91231105, 0.919854  , 0.9162039 , 0.91812432, 0.91703189,\n",
       "        0.91832807, 0.9304358 , 0.92385089, 0.91020422, 0.92445346,\n",
       "        0.91811998, 0.91732234, 0.91730933, 0.91425314, 0.93003264,\n",
       "        0.91560133, 0.91706224, 0.91870088, 0.92731892, 0.90991811,\n",
       "        0.91602183, 0.9144959 , 0.92467888, 0.91573572, 0.9129613 ,\n",
       "        0.89983484, 0.91769515, 0.92639555, 0.91813299, 0.92020947,\n",
       "        0.91858383, 0.92470923, 0.92529012, 0.91948985, 0.91504211,\n",
       "        0.90832282, 0.91587444, 0.92706748, 0.91972395, 0.90387508,\n",
       "        0.91851881, 0.91797693, 0.91836275, 0.924991  , 0.91217666,\n",
       "        0.91420545, 0.91923842, 0.90986176, 0.90926352, 0.91759978,\n",
       "        0.92006208, 0.93029708, 0.93019304, 0.92434075, 0.91599582,\n",
       "        0.92385089, 0.91932079, 0.9118602 , 0.91576606, 0.9196806 ,\n",
       "        0.90909879, 0.91309135, 0.93086063, 0.9202008 , 0.9231963 ,\n",
       "        0.91655504, 0.92104613, 0.92279315, 0.91357688, 0.90441696,\n",
       "        0.92050425, 0.90808873, 0.91767348, 0.9194335 , 0.93058753,\n",
       "        0.9166114 , 0.92989392, 0.91861418, 0.92259373, 0.92244201,\n",
       "        0.91208129, 0.91512882, 0.92517308, 0.91469965, 0.90768991,\n",
       "        0.91541926, 0.84051431, 0.4977046 , 0.84177147, 0.83983371,\n",
       "        0.44265408, 0.84021953, 0.84242172, 0.78383815, 0.83986839,\n",
       "        0.84543456, 0.60531735, 0.84265148, 0.83537296, 0.70980453,\n",
       "        0.83525158, 0.84248675, 0.48596968, 0.8362443 , 0.83846384,\n",
       "        0.47387062, 0.8463796 , 0.84211827, 0.55019313, 0.84387395,\n",
       "        0.84265581, 0.68684189, 0.84542156, 0.84003312, 0.52534908,\n",
       "        0.84327572, 0.84123392, 0.42396144, 0.84112121, 0.84579871,\n",
       "        0.55244734, 0.8434838 , 0.84250409, 0.51432944, 0.84505742,\n",
       "        0.8415157 , 0.32772814, 0.84001578, 0.84309798, 0.58279254,\n",
       "        0.84033224, 0.84550826, 0.56199741, 0.84368321, 0.86519796,\n",
       "        0.68365131, 0.86394514, 0.86906914, 0.62313431, 0.86872234,\n",
       "        0.87052571, 0.41971744, 0.87707594, 0.87652539, 0.51551723,\n",
       "        0.8648685 , 0.85895985, 0.54510814, 0.85858271, 0.8704997 ,\n",
       "        0.76739538, 0.86627738, 0.88085608, 0.36304128, 0.86444366,\n",
       "        0.88079973, 0.64694662, 0.87316141, 0.86780765, 0.51724258,\n",
       "        0.85452945, 0.8771453 , 0.71725645, 0.86972806, 0.86687128,\n",
       "        0.43564434, 0.87046935, 0.87261519, 0.45926157, 0.86978875,\n",
       "        0.86749552, 0.2364151 , 0.86711838, 0.86841889, 0.65901534,\n",
       "        0.85554385, 0.86632073, 0.40428908, 0.87990238, 0.87418014,\n",
       "        0.6926595 , 0.87216435, 0.89011137, 0.8813286 , 0.88899726,\n",
       "        0.8936921 , 0.89478019, 0.89027176, 0.8913902 , 0.89598533,\n",
       "        0.89459812, 0.89688268, 0.89584661, 0.89726416, 0.8787796 ,\n",
       "        0.88087342, 0.87603119, 0.8944594 , 0.89334096, 0.89324559,\n",
       "        0.89331929, 0.89222686, 0.89650553, 0.89482354, 0.89472817,\n",
       "        0.8964145 , 0.88284586, 0.88149333, 0.87506882, 0.89480187,\n",
       "        0.8895218 , 0.89396521, 0.89488423, 0.89172833, 0.89528306,\n",
       "        0.89273406, 0.89289879, 0.89415161, 0.86991447, 0.87865389,\n",
       "        0.87912207, 0.88516943, 0.8895218 , 0.8888412 , 0.89152459,\n",
       "        0.88890189, 0.89389151, 0.89160695, 0.89119512, 0.89151592,\n",
       "        0.9097924 , 0.91496408, 0.91239775, 0.91506812, 0.9237902 ,\n",
       "        0.91638164, 0.91961123, 0.91769515, 0.91851014, 0.92225127,\n",
       "        0.91682381, 0.92104179, 0.91449157, 0.90202836, 0.90678389,\n",
       "        0.91894364, 0.91450891, 0.916464  , 0.91775151, 0.91740904,\n",
       "        0.92146229, 0.91866186, 0.92141461, 0.92610077, 0.91059438,\n",
       "        0.9023795 , 0.89652721, 0.9194465 , 0.9168715 , 0.9158441 ,\n",
       "        0.92111549, 0.92181343, 0.91835841, 0.92311394, 0.92346941,\n",
       "        0.92154899, 0.90976205, 0.90363666, 0.90390109, 0.91592646,\n",
       "        0.91366791, 0.91561   , 0.91101921, 0.92094642, 0.92239432,\n",
       "        0.92071667, 0.91802461, 0.91736135]),\n",
       " 'split2_test_roc_auc_ovo': array([0.90504987, 0.90711768, 0.90641107, 0.89126015, 0.9005848 ,\n",
       "        0.901972  , 0.9077506 , 0.89621509, 0.90374937, 0.89806614,\n",
       "        0.90250088, 0.90217142, 0.90678822, 0.91113625, 0.90679689,\n",
       "        0.9154366 , 0.91297431, 0.9023925 , 0.90417853, 0.89990853,\n",
       "        0.89519202, 0.9024662 , 0.90573914, 0.90074953, 0.90591255,\n",
       "        0.89570355, 0.90961032, 0.89840428, 0.90782429, 0.90782429,\n",
       "        0.9108198 , 0.90048509, 0.9041872 , 0.90258324, 0.90583018,\n",
       "        0.90322049, 0.90361932, 0.90772459, 0.90719571, 0.90527963,\n",
       "        0.90177693, 0.91188188, 0.90223211, 0.90209339, 0.91284426,\n",
       "        0.90162954, 0.90742981, 0.90270462, 0.90186796, 0.90360198,\n",
       "        0.90563944, 0.90232314, 0.90454268, 0.90145614, 0.90147348,\n",
       "        0.89930596, 0.90771158, 0.91072443, 0.89861669, 0.90090992,\n",
       "        0.90635038, 0.9032335 , 0.90814942, 0.90873465, 0.90475509,\n",
       "        0.90029435, 0.90364099, 0.91575306, 0.90110066, 0.89859502,\n",
       "        0.89742022, 0.90499352, 0.91059871, 0.90609895, 0.90288236,\n",
       "        0.90310345, 0.89683066, 0.90944559, 0.8946328 , 0.90391843,\n",
       "        0.89235258, 0.89772801, 0.90429991, 0.89937966, 0.90748616,\n",
       "        0.90624634, 0.90362799, 0.90461204, 0.90200668, 0.91114926,\n",
       "        0.90513658, 0.90439095, 0.90014696, 0.9012047 , 0.90105731,\n",
       "        0.89448975, 0.82984147, 0.43596079, 0.82561915, 0.82904816,\n",
       "        0.3790332 , 0.82850194, 0.82951634, 0.43170813, 0.82778233,\n",
       "        0.82777366, 0.67897815, 0.8232609 , 0.82277537, 0.38538835,\n",
       "        0.8154405 , 0.82170462, 0.59296685, 0.82398484, 0.82877072,\n",
       "        0.37862571, 0.82779967, 0.82880106, 0.51797519, 0.82881407,\n",
       "        0.83024029, 0.29855774, 0.82020037, 0.82626507, 0.73290156,\n",
       "        0.82915653, 0.83122434, 0.50257717, 0.8264168 , 0.82279705,\n",
       "        0.61694823, 0.82716242, 0.82577088, 0.73280619, 0.82910018,\n",
       "        0.82535038, 0.62543621, 0.82065121, 0.82794273, 0.61354523,\n",
       "        0.82977211, 0.82632143, 0.59295384, 0.82716676, 0.84473229,\n",
       "        0.39868822, 0.84523515, 0.86845357, 0.70842599, 0.85168568,\n",
       "        0.86353764, 0.63399356, 0.85323762, 0.85426502, 0.47968389,\n",
       "        0.85762466, 0.84383494, 0.61828775, 0.8505499 , 0.86445233,\n",
       "        0.4921601 , 0.84754572, 0.84957885, 0.53842786, 0.86039475,\n",
       "        0.85385752, 0.73628289, 0.86298276, 0.84900663, 0.26632247,\n",
       "        0.84899362, 0.85067128, 0.61422583, 0.86976274, 0.86319951,\n",
       "        0.58032157, 0.85866074, 0.85591233, 0.49310514, 0.8555005 ,\n",
       "        0.84754572, 0.55255138, 0.84933176, 0.8552534 , 0.6279722 ,\n",
       "        0.84967422, 0.85792378, 0.26877609, 0.86655482, 0.85178972,\n",
       "        0.75471976, 0.86026903, 0.86742616, 0.86687128, 0.85922863,\n",
       "        0.87647337, 0.87646036, 0.86401016, 0.87624795, 0.87723633,\n",
       "        0.8766511 , 0.87540695, 0.87607021, 0.87640401, 0.86969338,\n",
       "        0.86995349, 0.8657225 , 0.87387669, 0.87330446, 0.86773395,\n",
       "        0.87642568, 0.87624361, 0.87501246, 0.87636499, 0.87489108,\n",
       "        0.87577976, 0.86238886, 0.86145249, 0.86944195, 0.87009221,\n",
       "        0.87145774, 0.87520754, 0.87498645, 0.87283194, 0.87509483,\n",
       "        0.87452261, 0.87524222, 0.87645603, 0.85060192, 0.8510701 ,\n",
       "        0.86094963, 0.86384543, 0.87146208, 0.87486507, 0.87441856,\n",
       "        0.85978784, 0.87307037, 0.87180021, 0.87418014, 0.87383767,\n",
       "        0.88032721, 0.89448975, 0.88627053, 0.89312855, 0.89165897,\n",
       "        0.89764565, 0.89638415, 0.8946328 , 0.90391843, 0.89952271,\n",
       "        0.8990502 , 0.90387508, 0.87936917, 0.89118212, 0.89721214,\n",
       "        0.89559518, 0.89584227, 0.89066625, 0.90088825, 0.89627144,\n",
       "        0.9014518 , 0.89696505, 0.90185496, 0.90066283, 0.88038356,\n",
       "        0.89096537, 0.88421139, 0.90047208, 0.88640925, 0.89546513,\n",
       "        0.89785373, 0.90009494, 0.89670928, 0.89707776, 0.89941867,\n",
       "        0.89657489, 0.88951313, 0.88665635, 0.88803489, 0.89204912,\n",
       "        0.89618474, 0.89204479, 0.89755461, 0.89825689, 0.89316323,\n",
       "        0.90091859, 0.89716879, 0.8993233 ]),\n",
       " 'split3_test_roc_auc_ovo': array([0.89634514, 0.90465539, 0.90175959, 0.89764131, 0.90448632,\n",
       "        0.90704832, 0.91088482, 0.91378929, 0.90867396, 0.9100655 ,\n",
       "        0.90002124, 0.90559609, 0.90430858, 0.90411351, 0.90297773,\n",
       "        0.90302542, 0.9109932 , 0.90872164, 0.90774626, 0.90857425,\n",
       "        0.91127931, 0.90837051, 0.89771934, 0.90892539, 0.90812774,\n",
       "        0.89659223, 0.90114401, 0.90632437, 0.90742547, 0.90454268,\n",
       "        0.91293963, 0.90870864, 0.90737345, 0.90863061, 0.90003858,\n",
       "        0.90769424, 0.90284768, 0.90472908, 0.90416986, 0.90507155,\n",
       "        0.89728584, 0.91002649, 0.9118472 , 0.915597  , 0.91781653,\n",
       "        0.91428782, 0.91200759, 0.91625592, 0.90419154, 0.90206738,\n",
       "        0.90139545, 0.90038972, 0.90493283, 0.90389242, 0.90076253,\n",
       "        0.90739512, 0.9123544 , 0.89918025, 0.89514867, 0.90328552,\n",
       "        0.90264827, 0.9000906 , 0.89863837, 0.91222868, 0.9045947 ,\n",
       "        0.90116135, 0.90913781, 0.89673096, 0.90291271, 0.90723039,\n",
       "        0.89820486, 0.90907712, 0.90482445, 0.90018164, 0.90508889,\n",
       "        0.90364533, 0.90853957, 0.91012186, 0.90115268, 0.90267428,\n",
       "        0.8992366 , 0.91255814, 0.9032335 , 0.90508889, 0.90968835,\n",
       "        0.90435627, 0.90172057, 0.90688793, 0.90031169, 0.90185496,\n",
       "        0.90667118, 0.90654979, 0.89987385, 0.90008627, 0.90360631,\n",
       "        0.90550939, 0.82666823, 0.47346312, 0.82148787, 0.83142375,\n",
       "        0.54524686, 0.82613068, 0.82990216, 0.37062758, 0.82545008,\n",
       "        0.82624339, 0.75406084, 0.82950334, 0.82785169, 0.26210882,\n",
       "        0.82917387, 0.82707572, 0.52177268, 0.8236077 , 0.82927358,\n",
       "        0.46343187, 0.82772164, 0.82987181, 0.24974965, 0.82821583,\n",
       "        0.823521  , 0.60670455, 0.83063044, 0.82893111, 0.34031273,\n",
       "        0.82923023, 0.82874037, 0.28495442, 0.82968974, 0.83076483,\n",
       "        0.48574426, 0.83025763, 0.83180524, 0.549868  , 0.82755691,\n",
       "        0.8300929 , 0.74315824, 0.83221273, 0.83139774, 0.32270818,\n",
       "        0.82769996, 0.83137607, 0.43420511, 0.82759159, 0.84804425,\n",
       "        0.39295731, 0.86316917, 0.85602937, 0.48299585, 0.83898404,\n",
       "        0.85489793, 0.58836738, 0.85208883, 0.86329055, 0.68986774,\n",
       "        0.85584297, 0.85118281, 0.42266526, 0.85450344, 0.85775038,\n",
       "        0.51650129, 0.85819255, 0.85835728, 0.38905145, 0.86187299,\n",
       "        0.86603895, 0.47314233, 0.86046844, 0.85490227, 0.62563129,\n",
       "        0.85640219, 0.84520481, 0.19522367, 0.8608586 , 0.84876387,\n",
       "        0.31877197, 0.86089328, 0.85903788, 0.53335588, 0.8564412 ,\n",
       "        0.85824024, 0.36924904, 0.85825758, 0.86330355, 0.30879274,\n",
       "        0.86025603, 0.86139614, 0.32884658, 0.85884714, 0.85843531,\n",
       "        0.57020795, 0.86110136, 0.85997425, 0.86310414, 0.87038699,\n",
       "        0.88452352, 0.87736638, 0.87817703, 0.8793605 , 0.88216092,\n",
       "        0.88428075, 0.88418538, 0.8824427 , 0.88344843, 0.87943853,\n",
       "        0.87519453, 0.88025785, 0.88271581, 0.87988504, 0.88529082,\n",
       "        0.87747476, 0.88406834, 0.88495702, 0.88166673, 0.88618817,\n",
       "        0.88529082, 0.85818388, 0.87693722, 0.86417489, 0.88373021,\n",
       "        0.86518929, 0.8650549 , 0.86942895, 0.88583269, 0.87481305,\n",
       "        0.88502204, 0.88385159, 0.88392528, 0.8664031 , 0.85654091,\n",
       "        0.86707069, 0.87716697, 0.87776954, 0.8781857 , 0.88066534,\n",
       "        0.88257709, 0.87553267, 0.88112919, 0.88253807, 0.87994139,\n",
       "        0.88914466, 0.89444206, 0.8906099 , 0.90302542, 0.90085357,\n",
       "        0.89572523, 0.90390976, 0.90032469, 0.90118303, 0.89908921,\n",
       "        0.90181594, 0.90396612, 0.8937788 , 0.89287278, 0.89591163,\n",
       "        0.90453834, 0.89703874, 0.89853433, 0.9027393 , 0.90380139,\n",
       "        0.90036371, 0.90515825, 0.90344591, 0.9025529 , 0.89601568,\n",
       "        0.8868731 , 0.89755895, 0.90021198, 0.89972646, 0.90006026,\n",
       "        0.90266561, 0.903654  , 0.90131308, 0.90436928, 0.90086224,\n",
       "        0.90454701, 0.8947932 , 0.8869468 , 0.88837736, 0.89712544,\n",
       "        0.90231881, 0.8998175 , 0.89799245, 0.90403114, 0.90060214,\n",
       "        0.89865137, 0.90381873, 0.90154284]),\n",
       " 'split4_test_roc_auc_ovo': array([0.91341214, 0.91214632, 0.91258415, 0.91999705, 0.9196199 ,\n",
       "        0.91715327, 0.913889  , 0.91196858, 0.91916039, 0.91495108,\n",
       "        0.90717404, 0.92456184, 0.91235006, 0.91358988, 0.91913872,\n",
       "        0.91832807, 0.91482536, 0.9179856 , 0.92519042, 0.91264918,\n",
       "        0.91341214, 0.91613454, 0.90892972, 0.90961032, 0.91381097,\n",
       "        0.90165121, 0.91460861, 0.91812865, 0.92245501, 0.92103312,\n",
       "        0.922416  , 0.90827514, 0.91633829, 0.92001439, 0.91179951,\n",
       "        0.9166244 , 0.91926877, 0.92092475, 0.91359422, 0.92353877,\n",
       "        0.91998405, 0.91658972, 0.92266743, 0.91988868, 0.92095509,\n",
       "        0.92265442, 0.92207353, 0.90644575, 0.91574439, 0.91875723,\n",
       "        0.91331677, 0.9178122 , 0.91516783, 0.9127012 , 0.91181252,\n",
       "        0.91033861, 0.91716194, 0.9133818 , 0.90681857, 0.91715327,\n",
       "        0.91351618, 0.91591779, 0.90960599, 0.91922108, 0.92394626,\n",
       "        0.9148427 , 0.91727465, 0.91479935, 0.9153499 , 0.91587444,\n",
       "        0.91122729, 0.90959298, 0.91772983, 0.91476034, 0.90614664,\n",
       "        0.90978373, 0.91299165, 0.92034819, 0.91358988, 0.92160968,\n",
       "        0.92082071, 0.9110929 , 0.91040797, 0.91630361, 0.91839743,\n",
       "        0.92108514, 0.91986267, 0.91895665, 0.91625159, 0.91978464,\n",
       "        0.91262317, 0.9193468 , 0.92389858, 0.91020856, 0.92116751,\n",
       "        0.92288418, 0.82455273, 0.59578895, 0.80884693, 0.82463943,\n",
       "        0.50231274, 0.82090697, 0.82279705, 0.59096407, 0.82147053,\n",
       "        0.82009199, 0.68253287, 0.82033909, 0.81822359, 0.5041898 ,\n",
       "        0.83013625, 0.81204184, 0.38669753, 0.82280138, 0.82289675,\n",
       "        0.46364862, 0.8272838 , 0.82457874, 0.43970192, 0.82799908,\n",
       "        0.82219881, 0.56916321, 0.81925966, 0.82163092, 0.41924926,\n",
       "        0.82465244, 0.82461342, 0.45438466, 0.8207119 , 0.813299  ,\n",
       "        0.52791541, 0.82049081, 0.8283849 , 0.35241613, 0.81359812,\n",
       "        0.82320887, 0.64035304, 0.81996194, 0.82073791, 0.54524686,\n",
       "        0.82177398, 0.8188305 , 0.73424542, 0.82693266, 0.84233068,\n",
       "        0.41132049, 0.85319427, 0.85444709, 0.58677643, 0.86759089,\n",
       "        0.86075889, 0.58488202, 0.86186432, 0.87659908, 0.69378227,\n",
       "        0.86964136, 0.85605538, 0.59852002, 0.86287438, 0.86058982,\n",
       "        0.63122348, 0.85859138, 0.86545806, 0.64293672, 0.86771228,\n",
       "        0.87091153, 0.61435154, 0.86558378, 0.84307197, 0.63205147,\n",
       "        0.86312148, 0.85948873, 0.77978923, 0.86236285, 0.86457371,\n",
       "        0.64102064, 0.85725619, 0.86503756, 0.55719853, 0.86519362,\n",
       "        0.87164415, 0.67377178, 0.84127727, 0.87088985, 0.6836253 ,\n",
       "        0.86872667, 0.8681241 , 0.66354545, 0.85494128, 0.87965961,\n",
       "        0.63774769, 0.87031763, 0.87858019, 0.86348562, 0.88663901,\n",
       "        0.88739764, 0.88131126, 0.89405624, 0.89348402, 0.89148124,\n",
       "        0.89576858, 0.89423831, 0.89637548, 0.8909567 , 0.88050061,\n",
       "        0.87579277, 0.86469943, 0.88800021, 0.89197976, 0.88255975,\n",
       "        0.89566454, 0.89538276, 0.89100438, 0.8969347 , 0.8976153 ,\n",
       "        0.89807048, 0.89066625, 0.86538437, 0.8675302 , 0.89398688,\n",
       "        0.88851174, 0.88614482, 0.89735086, 0.89357072, 0.89449408,\n",
       "        0.89042349, 0.89726416, 0.89643184, 0.87520754, 0.87499512,\n",
       "        0.86813711, 0.88565929, 0.88608846, 0.88108584, 0.88552924,\n",
       "        0.88642226, 0.88168407, 0.89253898, 0.89219218, 0.89298983,\n",
       "        0.89524838, 0.91370259, 0.91255381, 0.91625592, 0.91357688,\n",
       "        0.91538458, 0.91838876, 0.91883093, 0.91756077, 0.91697554,\n",
       "        0.91735268, 0.92217757, 0.91001348, 0.91279657, 0.90190698,\n",
       "        0.91817201, 0.91451758, 0.91370259, 0.91513749, 0.92053026,\n",
       "        0.91728332, 0.92213856, 0.92284083, 0.92030917, 0.89970478,\n",
       "        0.89918892, 0.90240984, 0.91761279, 0.91169547, 0.91078512,\n",
       "        0.92178742, 0.91396703, 0.9191734 , 0.92151865, 0.91773417,\n",
       "        0.91759111, 0.90029435, 0.89921059, 0.90493716, 0.91657238,\n",
       "        0.90989644, 0.91625159, 0.9156967 , 0.92036119, 0.91963724,\n",
       "        0.91942483, 0.91678913, 0.91981065]),\n",
       " 'mean_test_roc_auc_ovo': array([0.9075954 , 0.9107331 , 0.90782082, 0.90978546, 0.91206135,\n",
       "        0.91216452, 0.91485137, 0.90949415, 0.91163998, 0.9097161 ,\n",
       "        0.90766476, 0.91470051, 0.91011666, 0.91253907, 0.91306968,\n",
       "        0.91606518, 0.9148323 , 0.91321447, 0.9116053 , 0.91171715,\n",
       "        0.90966408, 0.91035422, 0.9083575 , 0.90983921, 0.91379883,\n",
       "        0.90426697, 0.91257288, 0.91222088, 0.91490773, 0.91222348,\n",
       "        0.91591259, 0.90958692, 0.91242636, 0.91090303, 0.91042618,\n",
       "        0.90789279, 0.91155849, 0.91568803, 0.9108224 , 0.91416644,\n",
       "        0.90969789, 0.91589438, 0.91622211, 0.91305841, 0.917807  ,\n",
       "        0.91211857, 0.91521291, 0.91389767, 0.91106169, 0.90813641,\n",
       "        0.90982968, 0.9081026 , 0.91288154, 0.91210123, 0.90654199,\n",
       "        0.90847888, 0.91220874, 0.90869563, 0.90348493, 0.91029959,\n",
       "        0.91171888, 0.91242549, 0.91201193, 0.91493114, 0.9129535 ,\n",
       "        0.91003169, 0.91094811, 0.91022243, 0.90807052, 0.90826473,\n",
       "        0.90547904, 0.90946814, 0.91518951, 0.91042964, 0.91029786,\n",
       "        0.91137381, 0.9117735 , 0.9163669 , 0.90790059, 0.90930601,\n",
       "        0.90887597, 0.90658708, 0.90781129, 0.90775233, 0.91601316,\n",
       "        0.91243676, 0.91360982, 0.91283038, 0.9102311 , 0.91524326,\n",
       "        0.9076847 , 0.91158536, 0.91155155, 0.90681163, 0.90905978,\n",
       "        0.90847108, 0.83311355, 0.48825337, 0.82860598, 0.83448082,\n",
       "        0.47559596, 0.8324    , 0.83395974, 0.55244127, 0.83235578,\n",
       "        0.8331647 , 0.60633348, 0.83252138, 0.83039635, 0.46656089,\n",
       "        0.83033046, 0.82995418, 0.50109806, 0.8308654 , 0.83263669,\n",
       "        0.45850381, 0.83490391, 0.83438631, 0.4425327 , 0.83512587,\n",
       "        0.83297309, 0.52184551, 0.83226735, 0.83294535, 0.54180745,\n",
       "        0.83461087, 0.8341817 , 0.42492988, 0.8323896 , 0.8314697 ,\n",
       "        0.55643817, 0.83326787, 0.83504957, 0.49982356, 0.83265924,\n",
       "        0.83326007, 0.58452568, 0.83181564, 0.83409326, 0.52222266,\n",
       "        0.83296702, 0.83323146, 0.60389632, 0.83424672, 0.85144898,\n",
       "        0.48361143, 0.85259516, 0.86150972, 0.58666459, 0.85948699,\n",
       "        0.86377607, 0.5800424 , 0.86102853, 0.86715046, 0.56170956,\n",
       "        0.86440898, 0.85388787, 0.51344856, 0.85584037, 0.86064618,\n",
       "        0.61987437, 0.86014678, 0.86366249, 0.48693899, 0.86262382,\n",
       "        0.86705595, 0.61613758, 0.8663164 , 0.85581262, 0.49519549,\n",
       "        0.85367545, 0.85791598, 0.59544649, 0.86541731, 0.86020401,\n",
       "        0.53704845, 0.8631683 , 0.86477659, 0.50204397, 0.86245215,\n",
       "        0.85969421, 0.49728757, 0.85474534, 0.86516328, 0.58356071,\n",
       "        0.85967687, 0.86495086, 0.41020206, 0.86416362, 0.86599127,\n",
       "        0.63626598, 0.86423558, 0.87541996, 0.86946276, 0.87580837,\n",
       "        0.88713147, 0.88470645, 0.88411689, 0.88699795, 0.88827331,\n",
       "        0.88958509, 0.88950533, 0.88947672, 0.88842677, 0.87716611,\n",
       "        0.87550492, 0.87360271, 0.88718522, 0.88591679, 0.8837224 ,\n",
       "        0.88768115, 0.88830626, 0.88880392, 0.88885681, 0.88952354,\n",
       "        0.88944637, 0.87469861, 0.87111961, 0.87360098, 0.88737423,\n",
       "        0.882004  , 0.88124363, 0.88597402, 0.88749474, 0.88682021,\n",
       "        0.88762913, 0.88887848, 0.88926604, 0.86951998, 0.86662158,\n",
       "        0.87310678, 0.87880821, 0.8816754 , 0.88009398, 0.88497696,\n",
       "        0.88054396, 0.88305828, 0.88580495, 0.88701182, 0.88628527,\n",
       "        0.89612405, 0.90564724, 0.90097408, 0.90781302, 0.90947074,\n",
       "        0.90748963, 0.91016001, 0.90886036, 0.91147005, 0.91077991,\n",
       "        0.90977852, 0.91308875, 0.90069491, 0.90146914, 0.90106685,\n",
       "        0.9100707 , 0.9074272 , 0.90585359, 0.91011319, 0.91043225,\n",
       "        0.91123769, 0.91116053, 0.91323007, 0.9132188 , 0.89907447,\n",
       "        0.89744017, 0.89740462, 0.90956784, 0.90567412, 0.90761448,\n",
       "        0.91155762, 0.91040537, 0.90996666, 0.91179691, 0.91104088,\n",
       "        0.91132006, 0.89969611, 0.89566974, 0.89705868, 0.90620126,\n",
       "        0.90692261, 0.90685325, 0.90763962, 0.9119313 , 0.90936496,\n",
       "        0.91101487, 0.9097239 , 0.91074697]),\n",
       " 'std_test_roc_auc_ovo': array([0.00642459, 0.00495333, 0.00363554, 0.01269417, 0.00823613,\n",
       "        0.00697374, 0.00543161, 0.0069424 , 0.00642701, 0.00753288,\n",
       "        0.00602947, 0.00902752, 0.00418379, 0.00480517, 0.00683872,\n",
       "        0.00726992, 0.00848127, 0.00739197, 0.0071746 , 0.00792528,\n",
       "        0.00772044, 0.00559282, 0.00654859, 0.00523158, 0.00854541,\n",
       "        0.00805093, 0.00671675, 0.00844062, 0.00832154, 0.00620259,\n",
       "        0.00397977, 0.00548042, 0.00730881, 0.0061966 , 0.00721106,\n",
       "        0.00601095, 0.00698388, 0.00817874, 0.00486982, 0.00765032,\n",
       "        0.00897679, 0.0050765 , 0.00832782, 0.00690032, 0.00355158,\n",
       "        0.0069661 , 0.0051063 , 0.00859693, 0.00687137, 0.00642443,\n",
       "        0.00594232, 0.00802218, 0.00693658, 0.0086716 , 0.0048708 ,\n",
       "        0.00507346, 0.00552149, 0.00491222, 0.00555778, 0.00696693,\n",
       "        0.00636261, 0.01066076, 0.01031743, 0.00592773, 0.0073965 ,\n",
       "        0.00880472, 0.00629213, 0.00691791, 0.00625299, 0.00839107,\n",
       "        0.00631798, 0.00262935, 0.00884473, 0.00690267, 0.00749344,\n",
       "        0.00789245, 0.00872109, 0.00550751, 0.00849684, 0.00734155,\n",
       "        0.01140649, 0.00541157, 0.0055926 , 0.00861791, 0.00818839,\n",
       "        0.00629181, 0.01044705, 0.00598169, 0.00842821, 0.00776436,\n",
       "        0.00411268, 0.00549123, 0.01107031, 0.005505  , 0.00706599,\n",
       "        0.00979577, 0.00772821, 0.05847546, 0.01343265, 0.00815843,\n",
       "        0.05843394, 0.00937341, 0.00846046, 0.14403177, 0.00963441,\n",
       "        0.01068103, 0.15508767, 0.01050012, 0.01038199, 0.14764526,\n",
       "        0.00866531, 0.01285896, 0.06704054, 0.00976733, 0.00747289,\n",
       "        0.04388837, 0.00895117, 0.00843548, 0.10451046, 0.00836038,\n",
       "        0.00982766, 0.13563208, 0.01161316, 0.00960869, 0.15148198,\n",
       "        0.0087184 , 0.00773009, 0.07434517, 0.0088382 , 0.01253758,\n",
       "        0.04756394, 0.00949005, 0.00817649, 0.1423112 , 0.01257439,\n",
       "        0.00902747, 0.13849308, 0.01039728, 0.00978466, 0.10290745,\n",
       "        0.0085851 , 0.01027504, 0.10598024, 0.00862338, 0.00847387,\n",
       "        0.11221373, 0.01024772, 0.00615011, 0.0773206 , 0.01226726,\n",
       "        0.00570136, 0.08648179, 0.00893149, 0.00851373, 0.10968779,\n",
       "        0.00693222, 0.00581817, 0.094422  , 0.0043975 , 0.00685009,\n",
       "        0.1040226 , 0.00777447, 0.0102518 , 0.10210138, 0.00316762,\n",
       "        0.00883675, 0.08470305, 0.00451935, 0.00921911, 0.13584411,\n",
       "        0.00614734, 0.01083527, 0.20737386, 0.0037058 , 0.00648255,\n",
       "        0.14166448, 0.00534497, 0.00656223, 0.0377887 , 0.00555192,\n",
       "        0.00884499, 0.16932175, 0.00877612, 0.00552859, 0.13869626,\n",
       "        0.00662841, 0.00469953, 0.13523714, 0.00871366, 0.01012588,\n",
       "        0.08214265, 0.00588493, 0.01057846, 0.00682707, 0.01095083,\n",
       "        0.00640271, 0.00792752, 0.0116214 , 0.00763729, 0.00731776,\n",
       "        0.00785337, 0.00845339, 0.00858855, 0.00755739, 0.00386759,\n",
       "        0.00346008, 0.00708297, 0.00830112, 0.0079237 , 0.00879603,\n",
       "        0.00880738, 0.00716997, 0.00811184, 0.00825077, 0.00824161,\n",
       "        0.00814797, 0.01239185, 0.00732704, 0.00975666, 0.00957348,\n",
       "        0.01158153, 0.01005533, 0.01145241, 0.00785856, 0.00969393,\n",
       "        0.00739608, 0.00820795, 0.00781076, 0.01144209, 0.0108229 ,\n",
       "        0.01038416, 0.00806827, 0.00638854, 0.00480105, 0.00683551,\n",
       "        0.01057819, 0.00825246, 0.00818243, 0.00764296, 0.00793104,\n",
       "        0.01082141, 0.00923735, 0.01088543, 0.00868007, 0.01164023,\n",
       "        0.00894371, 0.0088474 , 0.00967896, 0.0073712 , 0.00960482,\n",
       "        0.00777278, 0.0079454 , 0.01269713, 0.00845338, 0.00402098,\n",
       "        0.00887368, 0.00898237, 0.00974468, 0.00690731, 0.00904386,\n",
       "        0.00865296, 0.00913542, 0.00889711, 0.0099819 , 0.01080705,\n",
       "        0.00759265, 0.00747415, 0.00815908, 0.01124321, 0.00838329,\n",
       "        0.00970312, 0.00773424, 0.00921331, 0.00995629, 0.00943532,\n",
       "        0.00929983, 0.00705641, 0.00737747, 0.00741065, 0.00994585,\n",
       "        0.00667012, 0.00945951, 0.00824535, 0.00915195, 0.01111155,\n",
       "        0.00936535, 0.00801119, 0.00855122]),\n",
       " 'rank_test_roc_auc_ovo': array([119,  67, 111,  85,  41,  38,  14,  93,  48,  88, 116,  16,  78,\n",
       "         31,  25,   4,  15,  23,  49,  47,  90,  72, 104,  83,  19, 132,\n",
       "         30,  36,  13,  35,   6,  91,  33,  63,  70, 110,  51,   8,  64,\n",
       "         17,  89,   7,   3,  26,   1,  39,  10,  18,  59, 106,  84, 107,\n",
       "         28,  40, 126, 102,  37, 101, 133,  73,  46,  34,  42,  12,  27,\n",
       "         81,  62,  76, 108, 105, 131,  95,  11,  69,  74,  55,  45,   2,\n",
       "        109,  97,  99, 125, 113, 114,   5,  32,  20,  29,  75,   9, 115,\n",
       "         50,  53, 124,  98, 103, 239, 280, 256, 229, 283, 246, 234, 269,\n",
       "        248, 238, 260, 245, 253, 284, 254, 255, 276, 252, 244, 285, 227,\n",
       "        230, 286, 225, 240, 273, 249, 242, 270, 228, 232, 287, 247, 251,\n",
       "        268, 235, 226, 277, 243, 236, 264, 250, 233, 272, 241, 237, 261,\n",
       "        231, 224, 282, 223, 209, 263, 216, 204, 266, 210, 192, 267, 201,\n",
       "        221, 274, 218, 211, 258, 213, 205, 281, 207, 193, 259, 195, 219,\n",
       "        279, 222, 217, 262, 197, 212, 271, 206, 200, 275, 208, 214, 278,\n",
       "        220, 198, 265, 215, 199, 288, 203, 196, 257, 202, 184, 191, 182,\n",
       "        162, 171, 172, 164, 156, 145, 147, 148, 154, 181, 183, 186, 161,\n",
       "        168, 173, 157, 155, 153, 152, 146, 149, 185, 189, 187, 160, 175,\n",
       "        177, 167, 159, 165, 158, 151, 150, 190, 194, 188, 180, 176, 179,\n",
       "        170, 178, 174, 169, 163, 166, 143, 130, 136, 112,  94, 120,  77,\n",
       "        100,  54,  65,  86,  24, 137, 134, 135,  80, 121, 128,  79,  68,\n",
       "         57,  58,  21,  22, 139, 140, 141,  92, 129, 118,  52,  71,  82,\n",
       "         44,  60,  56, 138, 144, 142, 127, 122, 123, 117,  43,  96,  61,\n",
       "         87,  66]),\n",
       " 'split0_test_jaccard': array([0.63679245, 0.64845606, 0.65625   , 0.6882494 , 0.6763285 ,\n",
       "        0.6572104 , 0.67386091, 0.65776699, 0.66904762, 0.63023256,\n",
       "        0.6713948 , 0.65882353, 0.63875598, 0.65048544, 0.65617433,\n",
       "        0.67391304, 0.64553991, 0.65558195, 0.67298578, 0.66904762,\n",
       "        0.6501182 , 0.66352941, 0.66109785, 0.67710843, 0.64200477,\n",
       "        0.64903846, 0.6601467 , 0.66987952, 0.64285714, 0.66746411,\n",
       "        0.67625899, 0.66423358, 0.65176471, 0.62642369, 0.67064439,\n",
       "        0.65492958, 0.64761905, 0.68069307, 0.64608076, 0.66350711,\n",
       "        0.65144231, 0.643026  , 0.64871194, 0.64916468, 0.6763285 ,\n",
       "        0.65144231, 0.6763285 , 0.6547619 , 0.64761905, 0.65789474,\n",
       "        0.65301205, 0.63701923, 0.67303103, 0.65155131, 0.64593301,\n",
       "        0.64916468, 0.65795724, 0.6507177 , 0.64485981, 0.65238095,\n",
       "        0.6407767 , 0.64096386, 0.65      , 0.64218009, 0.64845606,\n",
       "        0.65783133, 0.61981567, 0.64066194, 0.66426859, 0.65647059,\n",
       "        0.65789474, 0.64351852, 0.65060241, 0.65291262, 0.64539007,\n",
       "        0.67718447, 0.66587678, 0.65311005, 0.64386792, 0.66024096,\n",
       "        0.64832536, 0.64871194, 0.64883721, 0.64150943, 0.64691943,\n",
       "        0.64096386, 0.66666667, 0.65714286, 0.63420428, 0.67064439,\n",
       "        0.63294118, 0.65942029, 0.67064439, 0.66105769, 0.65876777,\n",
       "        0.65176471, 0.48837209, 0.17409766, 0.48591549, 0.49187935,\n",
       "        0.        , 0.49305556, 0.48946136, 0.18584071, 0.48723898,\n",
       "        0.49766355, 0.25651721, 0.49422633, 0.49885057, 0.22815534,\n",
       "        0.48130841, 0.48955916, 0.34623218, 0.5       , 0.49061033,\n",
       "        0.19575472, 0.49422633, 0.49302326, 0.34134134, 0.49187935,\n",
       "        0.49191686, 0.        , 0.48349057, 0.48951049, 0.07479224,\n",
       "        0.49422633, 0.48604651, 0.35663627, 0.48251748, 0.48484848,\n",
       "        0.        , 0.48604651, 0.4988345 , 0.25058005, 0.50348028,\n",
       "        0.48837209, 0.357     , 0.49074074, 0.48965517, 0.34558824,\n",
       "        0.48356808, 0.4941452 , 0.35835836, 0.49302326, 0.52900232,\n",
       "        0.33967936, 0.50804598, 0.5334873 , 0.25596184, 0.55971897,\n",
       "        0.54312354, 0.26736842, 0.53775744, 0.52083333, 0.15970516,\n",
       "        0.54883721, 0.51716247, 0.13910761, 0.50688073, 0.51131222,\n",
       "        0.40030675, 0.53864169, 0.5260771 , 0.28621908, 0.53302961,\n",
       "        0.52790698, 0.28543689, 0.54439252, 0.53286385, 0.08920188,\n",
       "        0.5162037 , 0.52214452, 0.36399217, 0.53899083, 0.53103448,\n",
       "        0.4218241 , 0.53225806, 0.53846154, 0.30434783, 0.51972158,\n",
       "        0.51333333, 0.08672087, 0.51508121, 0.54332553, 0.262     ,\n",
       "        0.52995392, 0.52314815, 0.128     , 0.52083333, 0.53809524,\n",
       "        0.34236948, 0.52873563, 0.56085919, 0.54047619, 0.54285714,\n",
       "        0.59375   , 0.59134615, 0.5942029 , 0.59615385, 0.60290557,\n",
       "        0.60731707, 0.60290557, 0.60340633, 0.60487805, 0.56085919,\n",
       "        0.56085919, 0.56900726, 0.60048426, 0.57211538, 0.57831325,\n",
       "        0.60679612, 0.58880779, 0.59951456, 0.6097561 , 0.6       ,\n",
       "        0.58454106, 0.55662651, 0.5368171 , 0.58894231, 0.58980583,\n",
       "        0.5971223 , 0.57384988, 0.59466019, 0.59036145, 0.6       ,\n",
       "        0.60628019, 0.60048426, 0.59661836, 0.57246377, 0.54066986,\n",
       "        0.58472554, 0.56354916, 0.56658596, 0.55608592, 0.58980583,\n",
       "        0.56220096, 0.58033573, 0.58133971, 0.59322034, 0.58894231,\n",
       "        0.61097852, 0.63414634, 0.6       , 0.61826698, 0.65365854,\n",
       "        0.64028777, 0.62117647, 0.63961814, 0.64916468, 0.63549161,\n",
       "        0.6405868 , 0.63507109, 0.60859189, 0.62352941, 0.59810875,\n",
       "        0.62884161, 0.63981043, 0.61938534, 0.62707838, 0.64028777,\n",
       "        0.63333333, 0.63915094, 0.64761905, 0.65947242, 0.61084906,\n",
       "        0.60570071, 0.6042654 , 0.61722488, 0.63614458, 0.6492891 ,\n",
       "        0.65083135, 0.62884161, 0.64663462, 0.63397129, 0.6507177 ,\n",
       "        0.64268585, 0.60238095, 0.59375   , 0.60570071, 0.61778846,\n",
       "        0.62884161, 0.62619048, 0.63571429, 0.63549161, 0.63270142,\n",
       "        0.64691943, 0.62768496, 0.63549161]),\n",
       " 'split1_test_jaccard': array([0.58980583, 0.66169154, 0.60576923, 0.66009852, 0.63970588,\n",
       "        0.65763547, 0.6575    , 0.61893204, 0.65517241, 0.65841584,\n",
       "        0.65920398, 0.67980296, 0.63390663, 0.65270936, 0.63680387,\n",
       "        0.66009852, 0.6840796 , 0.65012407, 0.64634146, 0.65925926,\n",
       "        0.66080402, 0.65060241, 0.63681592, 0.64197531, 0.68316832,\n",
       "        0.65422886, 0.6495098 , 0.64878049, 0.64339152, 0.62891566,\n",
       "        0.63438257, 0.62264151, 0.65594059, 0.65356265, 0.64676617,\n",
       "        0.63054187, 0.62807882, 0.67581047, 0.63834951, 0.65270936,\n",
       "        0.64039409, 0.66253102, 0.65679012, 0.65853659, 0.65024631,\n",
       "        0.63902439, 0.63613861, 0.68939394, 0.65920398, 0.60391198,\n",
       "        0.63636364, 0.65206813, 0.64127764, 0.67326733, 0.65841584,\n",
       "        0.63592233, 0.67594937, 0.63788969, 0.64539007, 0.65155131,\n",
       "        0.64720195, 0.685     , 0.6875    , 0.66995074, 0.63260341,\n",
       "        0.66336634, 0.6495098 , 0.65931373, 0.65835411, 0.64164649,\n",
       "        0.65024631, 0.64089776, 0.67079208, 0.64019851, 0.62716049,\n",
       "        0.65859564, 0.65217391, 0.64215686, 0.64233577, 0.61204819,\n",
       "        0.63592233, 0.65601966, 0.68780488, 0.65508685, 0.66997519,\n",
       "        0.65270936, 0.67407407, 0.65601966, 0.65931373, 0.65356265,\n",
       "        0.65931373, 0.65356265, 0.65206813, 0.64864865, 0.61916462,\n",
       "        0.66097561, 0.47529412, 0.10899183, 0.48604651, 0.48      ,\n",
       "        0.        , 0.47795824, 0.48139535, 0.        , 0.49065421,\n",
       "        0.48027842, 0.02770083, 0.48259861, 0.45327103, 0.361     ,\n",
       "        0.43467933, 0.48259861, 0.361     , 0.46713615, 0.46853147,\n",
       "        0.        , 0.48364486, 0.48604651, 0.        , 0.48842593,\n",
       "        0.44655582, 0.361     , 0.47806005, 0.48251748, 0.31360947,\n",
       "        0.49305556, 0.48484848, 0.        , 0.47563805, 0.49191686,\n",
       "        0.24329897, 0.48717949, 0.45518868, 0.361     , 0.49537037,\n",
       "        0.48484848, 0.22891566, 0.48723898, 0.48491879, 0.        ,\n",
       "        0.48027842, 0.47906977, 0.32460137, 0.48831776, 0.5046729 ,\n",
       "        0.36692223, 0.5361991 , 0.54196643, 0.21640091, 0.51732102,\n",
       "        0.5415677 , 0.18487395, 0.53493976, 0.57177033, 0.2295082 ,\n",
       "        0.51643192, 0.52803738, 0.21132075, 0.48113208, 0.52631579,\n",
       "        0.46927374, 0.50238095, 0.56144578, 0.07940447, 0.52505967,\n",
       "        0.55932203, 0.37912814, 0.54567308, 0.50359712, 0.1986456 ,\n",
       "        0.46962617, 0.54892601, 0.42686567, 0.50243902, 0.52122642,\n",
       "        0.33227176, 0.55116279, 0.52289157, 0.20538721, 0.53809524,\n",
       "        0.48815166, 0.02284264, 0.54157303, 0.5450237 , 0.34560327,\n",
       "        0.5011655 , 0.52570093, 0.11568627, 0.54698795, 0.56490385,\n",
       "        0.20266667, 0.5391924 , 0.56617647, 0.5403423 , 0.55717762,\n",
       "        0.5804878 , 0.57384988, 0.55231144, 0.56174334, 0.58595642,\n",
       "        0.58695652, 0.59223301, 0.57971014, 0.59223301, 0.53658537,\n",
       "        0.52579853, 0.54567308, 0.58230958, 0.57560976, 0.57971014,\n",
       "        0.57384988, 0.5776699 , 0.58653846, 0.58150852, 0.58353511,\n",
       "        0.58653846, 0.53545232, 0.54744526, 0.53510896, 0.57553957,\n",
       "        0.55474453, 0.5804878 , 0.57627119, 0.57177616, 0.57451923,\n",
       "        0.5776699 , 0.57487923, 0.57971014, 0.51312649, 0.52554745,\n",
       "        0.52912621, 0.56157635, 0.56067961, 0.55802469, 0.57281553,\n",
       "        0.55773956, 0.58394161, 0.56511057, 0.57107843, 0.57524272,\n",
       "        0.59512195, 0.62227603, 0.61800487, 0.61858191, 0.64691358,\n",
       "        0.63922518, 0.6460396 , 0.63861386, 0.62437811, 0.64691358,\n",
       "        0.63703704, 0.63909774, 0.60238095, 0.59277108, 0.58980583,\n",
       "        0.62682927, 0.61764706, 0.62469734, 0.63235294, 0.64303178,\n",
       "        0.64127764, 0.6509901 , 0.64851485, 0.65422886, 0.59854015,\n",
       "        0.59512195, 0.59904535, 0.64215686, 0.63793103, 0.64197531,\n",
       "        0.63970588, 0.63432836, 0.64197531, 0.64676617, 0.64356436,\n",
       "        0.62990196, 0.59615385, 0.59518072, 0.5971223 , 0.61764706,\n",
       "        0.63080685, 0.64039409, 0.64146341, 0.63861386, 0.65909091,\n",
       "        0.6453202 , 0.625     , 0.64197531]),\n",
       " 'split2_test_jaccard': array([0.60591133, 0.61707317, 0.6426799 , 0.6127451 , 0.6092233 ,\n",
       "        0.6372549 , 0.63636364, 0.64164649, 0.61557178, 0.64792176,\n",
       "        0.62407862, 0.64215686, 0.63793103, 0.62686567, 0.63793103,\n",
       "        0.63054187, 0.63882064, 0.62077295, 0.61057692, 0.61483254,\n",
       "        0.61800487, 0.63209877, 0.63902439, 0.62318841, 0.61369193,\n",
       "        0.61916462, 0.62561576, 0.61975309, 0.62682927, 0.62650602,\n",
       "        0.6405868 , 0.6375    , 0.63950617, 0.64390244, 0.64963504,\n",
       "        0.63260341, 0.62745098, 0.61519608, 0.62716049, 0.62773723,\n",
       "        0.62720403, 0.63861386, 0.62953995, 0.62836186, 0.6495098 ,\n",
       "        0.62043796, 0.62621359, 0.60687961, 0.61519608, 0.6       ,\n",
       "        0.61858191, 0.61124694, 0.64146341, 0.63027295, 0.625     ,\n",
       "        0.62110312, 0.64127764, 0.66666667, 0.64563107, 0.63814181,\n",
       "        0.63613861, 0.61893204, 0.61858191, 0.61138614, 0.61501211,\n",
       "        0.6039604 , 0.5971564 , 0.63855422, 0.62168675, 0.63438257,\n",
       "        0.63043478, 0.62347188, 0.63861386, 0.63118812, 0.62807882,\n",
       "        0.64356436, 0.60987654, 0.63017032, 0.60859189, 0.62621359,\n",
       "        0.61244019, 0.62773723, 0.63333333, 0.59952607, 0.6375    ,\n",
       "        0.62593516, 0.60880196, 0.62318841, 0.62009804, 0.625     ,\n",
       "        0.60144928, 0.65594059, 0.6125908 , 0.62891566, 0.62259615,\n",
       "        0.6291866 , 0.46919431, 0.361     , 0.47775176, 0.47663551,\n",
       "        0.15122873, 0.48831776, 0.47877358, 0.        , 0.48711944,\n",
       "        0.48578199, 0.        , 0.49292453, 0.48470588, 0.361     ,\n",
       "        0.31524548, 0.4740566 , 0.35222672, 0.47529412, 0.4824356 ,\n",
       "        0.        , 0.48584906, 0.49180328, 0.        , 0.48484848,\n",
       "        0.46853147, 0.361     , 0.48826291, 0.49302326, 0.        ,\n",
       "        0.49061033, 0.48831776, 0.21129326, 0.48699764, 0.48820755,\n",
       "        0.35268817, 0.49061033, 0.45933014, 0.42807626, 0.48578199,\n",
       "        0.48584906, 0.1147541 , 0.48820755, 0.47906977, 0.17027027,\n",
       "        0.47652582, 0.47540984, 0.07258065, 0.48235294, 0.49195402,\n",
       "        0.30696203, 0.4741784 , 0.51558753, 0.20050125, 0.50815851,\n",
       "        0.51312649, 0.35656402, 0.5035461 , 0.50588235, 0.29310345,\n",
       "        0.5106383 , 0.51116071, 0.28427419, 0.50232558, 0.53095238,\n",
       "        0.30702836, 0.5035461 , 0.52224824, 0.27227723, 0.49882353,\n",
       "        0.51522248, 0.4300885 , 0.52619048, 0.47294118, 0.18823529,\n",
       "        0.50119332, 0.51643192, 0.35300668, 0.51941748, 0.52380952,\n",
       "        0.33714286, 0.51282051, 0.52347418, 0.23143351, 0.50235849,\n",
       "        0.46904762, 0.3285124 , 0.50235849, 0.49408983, 0.36046512,\n",
       "        0.50808314, 0.52955083, 0.23265742, 0.52278177, 0.51382488,\n",
       "        0.36406619, 0.5       , 0.52311436, 0.51707317, 0.51815981,\n",
       "        0.53170732, 0.5403423 , 0.51094891, 0.53056235, 0.53789731,\n",
       "        0.55205811, 0.53284672, 0.53864734, 0.5496368 , 0.53186275,\n",
       "        0.52696078, 0.51824818, 0.52811736, 0.5315534 , 0.5195122 ,\n",
       "        0.55825243, 0.5472155 , 0.5377129 , 0.54368932, 0.54257908,\n",
       "        0.53284672, 0.53125   , 0.52068127, 0.52463054, 0.51456311,\n",
       "        0.52322738, 0.54523227, 0.54146341, 0.51707317, 0.53658537,\n",
       "        0.5377129 , 0.55582524, 0.54014599, 0.51276102, 0.51388889,\n",
       "        0.51089588, 0.52784504, 0.52184466, 0.52195122, 0.52682927,\n",
       "        0.51923077, 0.52554745, 0.52554745, 0.53527981, 0.52798054,\n",
       "        0.54196643, 0.58695652, 0.56085919, 0.58173077, 0.58795181,\n",
       "        0.58992806, 0.58992806, 0.59223301, 0.61728395, 0.59466019,\n",
       "        0.59657702, 0.605     , 0.54176611, 0.57932692, 0.57729469,\n",
       "        0.58536585, 0.58150852, 0.60199005, 0.59512195, 0.59413203,\n",
       "        0.59268293, 0.5995086 , 0.61219512, 0.60539216, 0.55853659,\n",
       "        0.56341463, 0.56234719, 0.60391198, 0.57211538, 0.58780488,\n",
       "        0.6039604 , 0.59512195, 0.59852217, 0.5995086 , 0.59079903,\n",
       "        0.59610706, 0.57482185, 0.56354916, 0.55635492, 0.57420925,\n",
       "        0.58924205, 0.57560976, 0.58292683, 0.59367397, 0.59756098,\n",
       "        0.60784314, 0.60048426, 0.59756098]),\n",
       " 'split3_test_jaccard': array([0.63461538, 0.65679012, 0.63043478, 0.61111111, 0.6495098 ,\n",
       "        0.65110565, 0.64215686, 0.66180049, 0.64492754, 0.63636364,\n",
       "        0.61374408, 0.62829736, 0.62779156, 0.63325183, 0.62254902,\n",
       "        0.6453202 , 0.62591687, 0.66334165, 0.64705882, 0.64320388,\n",
       "        0.66334165, 0.65227818, 0.63788969, 0.64233577, 0.66256158,\n",
       "        0.64320388, 0.62469734, 0.62259615, 0.62899263, 0.62829736,\n",
       "        0.66097561, 0.63349515, 0.62679426, 0.64164649, 0.63157895,\n",
       "        0.63157895, 0.6345679 , 0.63480392, 0.63882064, 0.64619165,\n",
       "        0.61165049, 0.63054187, 0.63861386, 0.66336634, 0.67661692,\n",
       "        0.6674938 , 0.66666667, 0.65586035, 0.63480392, 0.6372796 ,\n",
       "        0.64285714, 0.65012407, 0.64864865, 0.6492891 , 0.62745098,\n",
       "        0.64251208, 0.65036675, 0.6352657 , 0.63970588, 0.64492754,\n",
       "        0.63325183, 0.63366337, 0.64516129, 0.6674938 , 0.64851485,\n",
       "        0.62711864, 0.65422886, 0.61778846, 0.63349515, 0.64200477,\n",
       "        0.63245823, 0.64691943, 0.64320388, 0.61650485, 0.63569682,\n",
       "        0.64127764, 0.65432099, 0.66183575, 0.63569682, 0.6495098 ,\n",
       "        0.62589928, 0.63834951, 0.64009662, 0.62009804, 0.64864865,\n",
       "        0.63170732, 0.62530414, 0.64691358, 0.64792176, 0.63793103,\n",
       "        0.66084788, 0.64516129, 0.63861386, 0.65036675, 0.62829736,\n",
       "        0.66259169, 0.49765258, 0.        , 0.47877358, 0.5       ,\n",
       "        0.361     , 0.50934579, 0.50819672, 0.        , 0.50823529,\n",
       "        0.50351288, 0.361     , 0.50704225, 0.47641509, 0.361     ,\n",
       "        0.49530516, 0.5058548 , 0.361     , 0.50700935, 0.49647059,\n",
       "        0.01381215, 0.50351288, 0.50700935, 0.357     , 0.50352941,\n",
       "        0.49648712, 0.07417582, 0.49765258, 0.49411765, 0.        ,\n",
       "        0.49882353, 0.48815166, 0.361     , 0.50471698, 0.50351288,\n",
       "        0.01108033, 0.50819672, 0.50118203, 0.        , 0.48104265,\n",
       "        0.51288056, 0.361     , 0.51401869, 0.50234742, 0.        ,\n",
       "        0.5       , 0.51053864, 0.35294118, 0.48820755, 0.52164009,\n",
       "        0.29384437, 0.53066038, 0.53379953, 0.24643875, 0.52511416,\n",
       "        0.51732102, 0.33802817, 0.52102804, 0.52112676, 0.35690968,\n",
       "        0.52995392, 0.48951049, 0.15068493, 0.52570093, 0.53444181,\n",
       "        0.14465409, 0.52777778, 0.5106383 , 0.19859155, 0.52619048,\n",
       "        0.54186047, 0.20735786, 0.52267303, 0.50236967, 0.33913043,\n",
       "        0.50930233, 0.53548387, 0.11084337, 0.52631579, 0.52886836,\n",
       "        0.20853081, 0.52256532, 0.53286385, 0.22302158, 0.52      ,\n",
       "        0.50118765, 0.08064516, 0.51764706, 0.53900709, 0.17396746,\n",
       "        0.54009434, 0.53773585, 0.09911504, 0.52718676, 0.53791469,\n",
       "        0.35264484, 0.52843602, 0.50707547, 0.51196172, 0.55952381,\n",
       "        0.58992806, 0.56174334, 0.56341463, 0.56796117, 0.58292683,\n",
       "        0.58150852, 0.58150852, 0.58150852, 0.58435208, 0.56934307,\n",
       "        0.55447942, 0.57004831, 0.58595642, 0.55717762, 0.58173077,\n",
       "        0.55961071, 0.57971014, 0.57793765, 0.57627119, 0.58072289,\n",
       "        0.58595642, 0.51435407, 0.55769231, 0.5259434 , 0.57971014,\n",
       "        0.51904762, 0.52235294, 0.53554502, 0.58313253, 0.55205811,\n",
       "        0.5811138 , 0.58394161, 0.5811138 , 0.52606635, 0.51053864,\n",
       "        0.52369668, 0.55447942, 0.55932203, 0.57004831, 0.57004831,\n",
       "        0.5804878 , 0.54436451, 0.56796117, 0.57246377, 0.55932203,\n",
       "        0.60679612, 0.60635697, 0.60144928, 0.63788969, 0.63636364,\n",
       "        0.61985472, 0.63970588, 0.62560386, 0.63902439, 0.61707317,\n",
       "        0.63990268, 0.64619165, 0.60774818, 0.60933661, 0.62318841,\n",
       "        0.65174129, 0.61743341, 0.62162162, 0.63768116, 0.63546798,\n",
       "        0.63325183, 0.64356436, 0.63480392, 0.6407767 , 0.61425061,\n",
       "        0.5754717 , 0.62009804, 0.63569682, 0.64460784, 0.62962963,\n",
       "        0.63882064, 0.63636364, 0.63990268, 0.63144963, 0.62315271,\n",
       "        0.64676617, 0.61165049, 0.57073171, 0.60240964, 0.62376238,\n",
       "        0.6345679 , 0.6345679 , 0.62560386, 0.64285714, 0.65121951,\n",
       "        0.63569682, 0.65508685, 0.63882064]),\n",
       " 'split4_test_jaccard': array([0.64764268, 0.65925926, 0.63235294, 0.67758186, 0.64563107,\n",
       "        0.67407407, 0.66176471, 0.65693431, 0.64891041, 0.64218009,\n",
       "        0.6464891 , 0.6797066 , 0.6460396 , 0.63054187, 0.67331671,\n",
       "        0.65227818, 0.64903846, 0.67412935, 0.66995074, 0.67245658,\n",
       "        0.65133172, 0.65533981, 0.64508393, 0.65517241, 0.63170732,\n",
       "        0.63095238, 0.66334165, 0.64975845, 0.67726161, 0.65586035,\n",
       "        0.67401961, 0.66334165, 0.67241379, 0.66995074, 0.65693431,\n",
       "        0.68395062, 0.65346535, 0.6691358 , 0.64039409, 0.68226601,\n",
       "        0.66421569, 0.67160494, 0.65206813, 0.67821782, 0.65517241,\n",
       "        0.67741935, 0.67317073, 0.62801932, 0.64563107, 0.66336634,\n",
       "        0.63613861, 0.66339066, 0.66584158, 0.66169154, 0.65533981,\n",
       "        0.65925926, 0.6509901 , 0.65632458, 0.63636364, 0.67079208,\n",
       "        0.65686275, 0.65841584, 0.655     , 0.66346154, 0.67901235,\n",
       "        0.65693431, 0.64832536, 0.65925926, 0.66253102, 0.6617284 ,\n",
       "        0.6618705 , 0.66828087, 0.66582915, 0.65346535, 0.62232779,\n",
       "        0.6453202 , 0.66425121, 0.66419753, 0.66180049, 0.66093366,\n",
       "        0.68316832, 0.66503667, 0.66503667, 0.65931373, 0.65356265,\n",
       "        0.65586035, 0.65517241, 0.64492754, 0.65679012, 0.66339066,\n",
       "        0.64832536, 0.65865385, 0.68550369, 0.6407767 , 0.64792176,\n",
       "        0.66748166, 0.46313364, 0.361     , 0.45080092, 0.48752834,\n",
       "        0.        , 0.47954545, 0.47972973, 0.34734735, 0.47954545,\n",
       "        0.48072562, 0.        , 0.48198198, 0.48401826, 0.        ,\n",
       "        0.47685185, 0.48081264, 0.18730159, 0.48283753, 0.48198198,\n",
       "        0.19757366, 0.48198198, 0.48747153, 0.        , 0.4785553 ,\n",
       "        0.46726862, 0.361     , 0.48623853, 0.4785553 , 0.00831025,\n",
       "        0.47865169, 0.47685185, 0.03314917, 0.46501129, 0.4717833 ,\n",
       "        0.03287671, 0.48173516, 0.48173516, 0.14858097, 0.47285068,\n",
       "        0.48636364, 0.        , 0.46938776, 0.47285068, 0.        ,\n",
       "        0.46952596, 0.46712018, 0.31858407, 0.4739229 , 0.51859956,\n",
       "        0.13987474, 0.49777778, 0.5214447 , 0.26888889, 0.52690583,\n",
       "        0.5170068 , 0.28596491, 0.50564334, 0.53971963, 0.39463087,\n",
       "        0.52968037, 0.51548673, 0.32119205, 0.52494577, 0.52222222,\n",
       "        0.32459677, 0.50896861, 0.51963048, 0.3543759 , 0.52242152,\n",
       "        0.51241535, 0.20491803, 0.50559284, 0.50884956, 0.34474018,\n",
       "        0.52455357, 0.51818182, 0.15151515, 0.51121076, 0.51351351,\n",
       "        0.21134021, 0.50454545, 0.52173913, 0.24634656, 0.50904977,\n",
       "        0.52008929, 0.36572438, 0.50989011, 0.53703704, 0.35349716,\n",
       "        0.53793103, 0.51480638, 0.33898305, 0.49555556, 0.53738318,\n",
       "        0.36818851, 0.52304147, 0.54394299, 0.51480638, 0.56057007,\n",
       "        0.575179  , 0.56057007, 0.58695652, 0.59134615, 0.58513189,\n",
       "        0.59855769, 0.58837772, 0.60671463, 0.57590361, 0.55503513,\n",
       "        0.53647059, 0.51954023, 0.57349398, 0.58653846, 0.56085919,\n",
       "        0.59518072, 0.60191847, 0.57932692, 0.60481928, 0.60628019,\n",
       "        0.60671463, 0.59090909, 0.51843318, 0.52886836, 0.59952038,\n",
       "        0.57177033, 0.57279236, 0.60481928, 0.59661836, 0.60048426,\n",
       "        0.57756563, 0.60240964, 0.61352657, 0.55092593, 0.54090909,\n",
       "        0.53971963, 0.57553957, 0.57279236, 0.55503513, 0.56904762,\n",
       "        0.56769596, 0.5547619 , 0.59232614, 0.59285714, 0.58937198,\n",
       "        0.60620525, 0.64563107, 0.62174941, 0.64460784, 0.63438257,\n",
       "        0.64938272, 0.64705882, 0.65609756, 0.64146341, 0.6723301 ,\n",
       "        0.65121951, 0.65763547, 0.6313253 , 0.64547677, 0.6107056 ,\n",
       "        0.66501241, 0.6593674 , 0.62259615, 0.65024631, 0.66911765,\n",
       "        0.67741935, 0.65260546, 0.67821782, 0.66584158, 0.61961722,\n",
       "        0.61390887, 0.62227603, 0.65356265, 0.64891041, 0.61702128,\n",
       "        0.67321867, 0.64460784, 0.65542169, 0.66253102, 0.67567568,\n",
       "        0.647343  , 0.60714286, 0.59759036, 0.62621359, 0.64337349,\n",
       "        0.6235012 , 0.63196126, 0.64373464, 0.65196078, 0.64563107,\n",
       "        0.65517241, 0.65036675, 0.64634146]),\n",
       " 'mean_test_jaccard': array([0.62295353, 0.64865403, 0.63349737, 0.6499572 , 0.64407971,\n",
       "        0.6554561 , 0.65432922, 0.64741606, 0.64672595, 0.64302278,\n",
       "        0.64298212, 0.65775746, 0.63688496, 0.63877083, 0.64535499,\n",
       "        0.65243036, 0.6486791 , 0.65278999, 0.64938275, 0.65175997,\n",
       "        0.64872009, 0.65076971, 0.64398236, 0.64795607, 0.64662678,\n",
       "        0.63931764, 0.64466225, 0.64215354, 0.64386644, 0.6414087 ,\n",
       "        0.65724471, 0.64424238, 0.6492839 , 0.6470972 , 0.65111177,\n",
       "        0.64672088, 0.63823642, 0.65512787, 0.6381611 , 0.65448227,\n",
       "        0.63898132, 0.64926354, 0.6451448 , 0.65552946, 0.66157479,\n",
       "        0.65116356, 0.65570362, 0.64698302, 0.64049082, 0.63249053,\n",
       "        0.63739067, 0.64276981, 0.65405246, 0.65321445, 0.64242793,\n",
       "        0.64159229, 0.65530822, 0.64937287, 0.64239009, 0.65155874,\n",
       "        0.64284637, 0.64739502, 0.65124864, 0.65089446, 0.64471975,\n",
       "        0.6418422 , 0.63380722, 0.64311552, 0.64806712, 0.64724656,\n",
       "        0.64658091, 0.64461769, 0.65380828, 0.63885389, 0.6317308 ,\n",
       "        0.65318846, 0.64929989, 0.6502941 , 0.63845858, 0.64178924,\n",
       "        0.6411511 , 0.647171  , 0.65502174, 0.63510682, 0.65132118,\n",
       "        0.64143521, 0.64600385, 0.64563841, 0.64366558, 0.65010575,\n",
       "        0.64057548, 0.65454773, 0.65188417, 0.64595309, 0.63534953,\n",
       "        0.65440005, 0.47872935, 0.2010179 , 0.47585765, 0.48720864,\n",
       "        0.10244575, 0.48964456, 0.48751135, 0.10663761, 0.49055867,\n",
       "        0.48959249, 0.12904361, 0.49175474, 0.47945217, 0.26223107,\n",
       "        0.44067805, 0.48657636, 0.3215521 , 0.48645543, 0.48400599,\n",
       "        0.08142811, 0.48984302, 0.49307078, 0.13966827, 0.4894477 ,\n",
       "        0.47415198, 0.23143516, 0.48674093, 0.48754484, 0.07934239,\n",
       "        0.49107349, 0.48484325, 0.19241574, 0.48297629, 0.48805381,\n",
       "        0.12798884, 0.49075364, 0.4792541 , 0.23764745, 0.48770519,\n",
       "        0.49166277, 0.21233395, 0.48991874, 0.48576837, 0.1031717 ,\n",
       "        0.48197966, 0.48525673, 0.28541312, 0.48516488, 0.51317378,\n",
       "        0.28945654, 0.50937233, 0.5292571 , 0.23763833, 0.5274437 ,\n",
       "        0.52642911, 0.28655989, 0.52058293, 0.53186648, 0.28677147,\n",
       "        0.52710834, 0.51227156, 0.22131591, 0.50819702, 0.52504888,\n",
       "        0.32917194, 0.51626303, 0.52800798, 0.23817364, 0.52110496,\n",
       "        0.53134546, 0.30138588, 0.52890439, 0.50412427, 0.23199068,\n",
       "        0.50417582, 0.52823363, 0.28124461, 0.51967478, 0.52369046,\n",
       "        0.30222195, 0.52467043, 0.52788605, 0.24210734, 0.51784502,\n",
       "        0.49836191, 0.17688909, 0.51730998, 0.53169664, 0.2991066 ,\n",
       "        0.52344559, 0.52618843, 0.18288836, 0.52266908, 0.53842437,\n",
       "        0.32598714, 0.5238811 , 0.5402337 , 0.52493195, 0.54765769,\n",
       "        0.57421044, 0.56557035, 0.56156688, 0.56955337, 0.5789636 ,\n",
       "        0.58527958, 0.57957431, 0.58199739, 0.58140071, 0.5507371 ,\n",
       "        0.5409137 , 0.54450341, 0.57407232, 0.56459892, 0.56402511,\n",
       "        0.57873797, 0.57906436, 0.5762061 , 0.58320888, 0.58262345,\n",
       "        0.57931946, 0.5457184 , 0.53621382, 0.54069871, 0.57182781,\n",
       "        0.55318243, 0.55894305, 0.57055182, 0.57179233, 0.57272939,\n",
       "        0.57606849, 0.583508  , 0.58222297, 0.53506871, 0.52631078,\n",
       "        0.53763279, 0.55659791, 0.55624492, 0.55222905, 0.56570931,\n",
       "        0.55747101, 0.55779024, 0.56645701, 0.5729799 , 0.56817192,\n",
       "        0.59221365, 0.61907339, 0.60041255, 0.62021544, 0.63185403,\n",
       "        0.62773569, 0.62878177, 0.63043329, 0.63426291, 0.63329373,\n",
       "        0.63306461, 0.63659919, 0.59836249, 0.61008816, 0.59982065,\n",
       "        0.63155809, 0.62315336, 0.6180581 , 0.62849615, 0.63640744,\n",
       "        0.63559302, 0.63716389, 0.64427015, 0.64514234, 0.60035873,\n",
       "        0.59072357, 0.6016064 , 0.63051064, 0.62794185, 0.62514404,\n",
       "        0.64130739, 0.62785268, 0.63649129, 0.63484534, 0.6367819 ,\n",
       "        0.63256081, 0.59843   , 0.58416039, 0.59756023, 0.61535613,\n",
       "        0.62139192, 0.6217447 , 0.62588861, 0.63251947, 0.63724078,\n",
       "        0.6381904 , 0.63172456, 0.632038  ]),\n",
       " 'std_test_jaccard': array([0.02156977, 0.01640752, 0.01662251, 0.03232962, 0.02147546,\n",
       "        0.01187153, 0.0135481 , 0.01580682, 0.01759652, 0.00969113,\n",
       "        0.02142397, 0.02039697, 0.00599938, 0.01069039, 0.01759128,\n",
       "        0.01449131, 0.01938292, 0.01792595, 0.02236375, 0.0210727 ,\n",
       "        0.01619884, 0.01034025, 0.00902456, 0.01779174, 0.02415723,\n",
       "        0.0127127 , 0.0165744 , 0.01873367, 0.01804426, 0.01695764,\n",
       "        0.01707058, 0.01668526, 0.01540679, 0.01436243, 0.01279079,\n",
       "        0.02070641, 0.01051584, 0.02563272, 0.00615128, 0.01811386,\n",
       "        0.0183288 , 0.01534918, 0.00982278, 0.01651551, 0.01231931,\n",
       "        0.02023632, 0.02050915, 0.02794963, 0.01482957, 0.0264348 ,\n",
       "        0.01122766, 0.0178467 , 0.01303879, 0.01427211, 0.01387557,\n",
       "        0.01282391, 0.01160169, 0.01166264, 0.00371824, 0.0109049 ,\n",
       "        0.00844704, 0.02269649, 0.02206338, 0.02206109, 0.0211362 ,\n",
       "        0.02279705, 0.02197015, 0.01543559, 0.01723822, 0.01018946,\n",
       "        0.01292561, 0.01433482, 0.01254234, 0.01393696, 0.00806159,\n",
       "        0.01342718, 0.02042551, 0.01269351, 0.01725564, 0.01946131,\n",
       "        0.02408813, 0.01307934, 0.01952653, 0.02243846, 0.01068213,\n",
       "        0.0115784 , 0.02494559, 0.01221685, 0.01469815, 0.01666612,\n",
       "        0.0219604 , 0.00513065, 0.0253074 , 0.01069546, 0.01536755,\n",
       "        0.01359509, 0.01262549, 0.14197904, 0.01299926, 0.00835814,\n",
       "        0.14192637, 0.01131456, 0.01100904, 0.14023484, 0.00955627,\n",
       "        0.00936816, 0.15075022, 0.0091709 , 0.01496279, 0.14084899,\n",
       "        0.06589811, 0.01082906, 0.06735802, 0.01494535, 0.00943406,\n",
       "        0.0942267 , 0.00802799, 0.00743753, 0.17112965, 0.00830721,\n",
       "        0.01819323, 0.16040815, 0.00644291, 0.00605251, 0.1204473 ,\n",
       "        0.00675941, 0.00420305, 0.1537108 , 0.01315774, 0.01028527,\n",
       "        0.14344426, 0.00917111, 0.01921713, 0.15235182, 0.01073641,\n",
       "        0.01067087, 0.13993738, 0.01423456, 0.01002429, 0.13798637,\n",
       "        0.01014878, 0.01537761, 0.10753115, 0.00656112, 0.01322273,\n",
       "        0.07902364, 0.02257282, 0.00946516, 0.0253771 , 0.0174459 ,\n",
       "        0.01308876, 0.0604289 , 0.01424588, 0.02265496, 0.08487408,\n",
       "        0.01320685, 0.01266586, 0.07181341, 0.01646588, 0.0080171 ,\n",
       "        0.10885475, 0.01442938, 0.017475  , 0.09354013, 0.01167814,\n",
       "        0.01745495, 0.09056479, 0.01490433, 0.01909991, 0.09757709,\n",
       "        0.01891443, 0.01231371, 0.12575274, 0.01253786, 0.00617092,\n",
       "        0.08181034, 0.01618199, 0.00661534, 0.03381838, 0.01212787,\n",
       "        0.01826354, 0.14125998, 0.01320892, 0.01902144, 0.07200256,\n",
       "        0.01588568, 0.00753234, 0.09098278, 0.01643376, 0.0161725 ,\n",
       "        0.06231841, 0.01303658, 0.02238185, 0.01274062, 0.01607068,\n",
       "        0.02225173, 0.01678717, 0.02953303, 0.02352166, 0.0217362 ,\n",
       "        0.01888205, 0.02436924, 0.0243063 , 0.01852781, 0.01430821,\n",
       "        0.01431604, 0.02265765, 0.0245718 , 0.01900611, 0.02347375,\n",
       "        0.0193237 , 0.01807737, 0.02070993, 0.02359908, 0.02222423,\n",
       "        0.02463185, 0.0263045 , 0.01513437, 0.02439079, 0.02981692,\n",
       "        0.02946575, 0.02193178, 0.02778415, 0.02857947, 0.02549869,\n",
       "        0.02197344, 0.01724207, 0.02435225, 0.0232808 , 0.01282919,\n",
       "        0.02531213, 0.01589407, 0.01785087, 0.01606619, 0.02084596,\n",
       "        0.02058681, 0.02200551, 0.0226644 , 0.02111852, 0.02291076,\n",
       "        0.02566622, 0.02066863, 0.02158918, 0.02188563, 0.02304935,\n",
       "        0.02121171, 0.02153449, 0.02141272, 0.01168397, 0.02634322,\n",
       "        0.01886832, 0.0175592 , 0.02999526, 0.02315047, 0.01596985,\n",
       "        0.02716622, 0.02602401, 0.0082136 , 0.01837474, 0.02415363,\n",
       "        0.02695782, 0.01945478, 0.02144805, 0.02151981, 0.02202871,\n",
       "        0.01875962, 0.02155963, 0.01776472, 0.02828816, 0.02166374,\n",
       "        0.02241688, 0.01713117, 0.01972112, 0.02083066, 0.02847144,\n",
       "        0.01928091, 0.01287502, 0.01413453, 0.02284237, 0.02262684,\n",
       "        0.01646699, 0.02351515, 0.02237586, 0.02019792, 0.02162555,\n",
       "        0.01638654, 0.01965107, 0.01760645]),\n",
       " 'rank_test_jaccard': array([127,  39, 106,  31,  63,   6,  13,  42,  48,  68,  69,   2,  95,\n",
       "         87,  55,  19,  38,  18,  32,  21,  37,  28,  64,  41,  50,  84,\n",
       "         59,  74,  65,  79,   3,  62,  35,  46,  26,  49,  89,   8,  91,\n",
       "         11,  85,  36,  56,   5,   1,  25,   4,  47,  83, 111,  92,  71,\n",
       "         14,  16,  72,  77,   7,  33,  73,  22,  70,  43,  24,  27,  58,\n",
       "         75, 105,  67,  40,  44,  51,  60,  15,  86, 114,  17,  34,  29,\n",
       "         88,  76,  81,  45,   9, 102,  23,  78,  52,  54,  66,  30,  82,\n",
       "         10,  20,  53, 101,  12, 253, 277, 254, 240, 286, 233, 239, 284,\n",
       "        230, 234, 282, 226, 251, 268, 256, 242, 259, 243, 248, 287, 232,\n",
       "        225, 281, 235, 255, 274, 241, 238, 288, 228, 247, 278, 249, 236,\n",
       "        283, 229, 252, 271, 237, 227, 276, 231, 244, 285, 250, 245, 266,\n",
       "        246, 218, 263, 220, 195, 272, 200, 202, 265, 213, 192, 264, 201,\n",
       "        219, 275, 221, 205, 257, 217, 198, 270, 212, 194, 261, 196, 223,\n",
       "        273, 222, 197, 267, 214, 209, 260, 207, 199, 269, 215, 224, 280,\n",
       "        216, 193, 262, 210, 204, 279, 211, 188, 258, 208, 187, 206, 182,\n",
       "        159, 170, 173, 166, 155, 144, 152, 150, 151, 181, 185, 184, 160,\n",
       "        171, 172, 156, 154, 157, 147, 148, 153, 183, 190, 186, 163, 179,\n",
       "        174, 165, 164, 162, 158, 146, 149, 191, 203, 189, 177, 178, 180,\n",
       "        169, 176, 175, 168, 161, 167, 142, 131, 136, 130, 113, 123, 119,\n",
       "        118, 104, 107, 108,  97, 140, 134, 138, 116, 126, 132, 120,  99,\n",
       "        100,  94,  61,  57, 137, 143, 135, 117, 121, 125,  80, 122,  98,\n",
       "        103,  96, 109, 139, 145, 141, 133, 129, 128, 124, 110,  93,  90,\n",
       "        115, 112]),\n",
       " 'split0_test_neg_log_loss': array([-0.34031771, -0.34593638, -0.35274938, -0.32866397, -0.33446963,\n",
       "        -0.34443131, -0.3337596 , -0.35599064, -0.36182591, -0.37263979,\n",
       "        -0.34683126, -0.33748963, -0.34592641, -0.33016031, -0.32133537,\n",
       "        -0.31303474, -0.3635251 , -0.34295992, -0.35566431, -0.35207536,\n",
       "        -0.3482607 , -0.36534354, -0.35578634, -0.34535948, -0.34970315,\n",
       "        -0.34180064, -0.319874  , -0.32604066, -0.35147381, -0.33187347,\n",
       "        -0.33781021, -0.33748746, -0.36249193, -0.36817257, -0.33863603,\n",
       "        -0.35489715, -0.33603711, -0.32565761, -0.34699482, -0.33395792,\n",
       "        -0.34684494, -0.34036417, -0.33730152, -0.35399681, -0.31844192,\n",
       "        -0.34352322, -0.33943042, -0.33748303, -0.33677247, -0.34061492,\n",
       "        -0.34539915, -0.36202952, -0.32655076, -0.33983034, -0.36218584,\n",
       "        -0.35049426, -0.36010227, -0.36539132, -0.37970158, -0.35316353,\n",
       "        -0.33271879, -0.33786732, -0.34072731, -0.34935143, -0.33830436,\n",
       "        -0.35017413, -0.37666314, -0.35413712, -0.36526397, -0.38473904,\n",
       "        -0.35084187, -0.36634895, -0.34164622, -0.34675088, -0.3360794 ,\n",
       "        -0.32081818, -0.33348587, -0.33110879, -0.34676792, -0.35117081,\n",
       "        -0.35628828, -0.37777584, -0.38658268, -0.38529117, -0.33676507,\n",
       "        -0.34340889, -0.34172985, -0.34116555, -0.35230906, -0.3234763 ,\n",
       "        -0.37668229, -0.345301  , -0.36570758, -0.35950023, -0.34914515,\n",
       "        -0.38334727, -0.47475195, -0.69686041, -0.47791089, -0.4650783 ,\n",
       "        -0.65123147, -0.46470226, -0.46548974, -0.67723699, -0.46193731,\n",
       "        -0.46359556, -0.70572762, -0.46003806, -0.46779638, -0.67520474,\n",
       "        -0.48367767, -0.46243563, -0.72674712, -0.46440774, -0.46969348,\n",
       "        -0.68708277, -0.46495276, -0.46301011, -0.71777101, -0.46125601,\n",
       "        -0.47073207, -0.66957959, -0.47679444, -0.46432595, -0.64768283,\n",
       "        -0.46287051, -0.46328324, -0.7288971 , -0.46799522, -0.46590896,\n",
       "        -0.6537467 , -0.46386998, -0.46881205, -0.72748129, -0.46604337,\n",
       "        -0.46815586, -0.71703504, -0.46587494, -0.46120811, -0.70714109,\n",
       "        -0.46792675, -0.46778555, -0.70301503, -0.46234463, -0.4349994 ,\n",
       "        -0.8825785 , -0.46263824, -0.43116902, -0.68665888, -0.41502128,\n",
       "        -0.41731617, -0.61345976, -0.42986138, -0.4250787 , -0.72682008,\n",
       "        -0.4124993 , -0.4308763 , -0.7062891 , -0.4467002 , -0.44984022,\n",
       "        -0.6636891 , -0.41616495, -0.42618003, -0.77580835, -0.43515198,\n",
       "        -0.42654486, -0.65446833, -0.41774957, -0.42534951, -0.72828896,\n",
       "        -0.45166696, -0.43511665, -0.63907991, -0.42625598, -0.43580548,\n",
       "        -0.63988439, -0.42394565, -0.41784883, -0.75933907, -0.4252265 ,\n",
       "        -0.4425133 , -0.63787841, -0.43730289, -0.41919554, -0.63969153,\n",
       "        -0.42806881, -0.41808904, -0.73564123, -0.42896236, -0.42277302,\n",
       "        -0.93243774, -0.43475819, -0.40338103, -0.41177428, -0.41019062,\n",
       "        -0.37954064, -0.37960713, -0.37960189, -0.37878831, -0.37911565,\n",
       "        -0.37591559, -0.37379275, -0.37368923, -0.37798616, -0.40575726,\n",
       "        -0.40710985, -0.40104579, -0.37472825, -0.38378013, -0.38546994,\n",
       "        -0.37690001, -0.37982708, -0.37548099, -0.37890207, -0.37796026,\n",
       "        -0.38335987, -0.40156831, -0.41447998, -0.38348962, -0.37896855,\n",
       "        -0.37664245, -0.39233345, -0.38038704, -0.3804999 , -0.38001579,\n",
       "        -0.37672282, -0.37732771, -0.37726094, -0.39573264, -0.41428477,\n",
       "        -0.39683108, -0.39945863, -0.3973256 , -0.40598208, -0.382475  ,\n",
       "        -0.39569812, -0.38549328, -0.38480856, -0.37861158, -0.38256899,\n",
       "        -0.35853151, -0.35125727, -0.36348043, -0.34823464, -0.33475541,\n",
       "        -0.3462407 , -0.34176114, -0.34328601, -0.33743348, -0.33572261,\n",
       "        -0.34062894, -0.33859496, -0.35664109, -0.35738265, -0.36385681,\n",
       "        -0.34528161, -0.34005482, -0.35100391, -0.34169305, -0.34321515,\n",
       "        -0.3379694 , -0.34222202, -0.33796885, -0.3351371 , -0.35496436,\n",
       "        -0.35452566, -0.35672062, -0.35097338, -0.33852061, -0.33915221,\n",
       "        -0.33997816, -0.34246147, -0.34079887, -0.34200768, -0.3422777 ,\n",
       "        -0.33569269, -0.36340175, -0.36501833, -0.36882377, -0.34846355,\n",
       "        -0.3481879 , -0.34646391, -0.33732205, -0.33827099, -0.34731207,\n",
       "        -0.33904935, -0.34118526, -0.33794742]),\n",
       " 'split1_test_neg_log_loss': array([-0.35491332, -0.33068218, -0.35601275, -0.33075434, -0.34316875,\n",
       "        -0.33227274, -0.33341995, -0.35450643, -0.33535848, -0.33656624,\n",
       "        -0.34632634, -0.3469409 , -0.34894799, -0.33734324, -0.34159088,\n",
       "        -0.34179452, -0.31253975, -0.32964542, -0.35979708, -0.32576468,\n",
       "        -0.34770532, -0.34804796, -0.34539794, -0.36129494, -0.31519888,\n",
       "        -0.33860303, -0.34603503, -0.33939771, -0.31753974, -0.35493478,\n",
       "        -0.35235114, -0.34959156, -0.32687152, -0.35062855, -0.35550874,\n",
       "        -0.38715652, -0.33933336, -0.32532034, -0.33875398, -0.33510114,\n",
       "        -0.33821361, -0.32181126, -0.32851852, -0.33675459, -0.34913183,\n",
       "        -0.3641029 , -0.34116228, -0.31685694, -0.3352836 , -0.36673673,\n",
       "        -0.3337562 , -0.33575928, -0.33677174, -0.32802349, -0.35974427,\n",
       "        -0.34913857, -0.33474914, -0.36814287, -0.37855001, -0.34839773,\n",
       "        -0.33824453, -0.3144646 , -0.31474367, -0.32729217, -0.34830349,\n",
       "        -0.32637185, -0.33932347, -0.35963365, -0.34071707, -0.33438273,\n",
       "        -0.36310915, -0.35876311, -0.31535291, -0.33710976, -0.33084381,\n",
       "        -0.34309473, -0.32985943, -0.33314332, -0.35267334, -0.36559053,\n",
       "        -0.33861019, -0.36087495, -0.36069199, -0.34407083, -0.31594469,\n",
       "        -0.34539944, -0.31580532, -0.33868885, -0.33222903, -0.33156415,\n",
       "        -0.35683559, -0.35261829, -0.32865346, -0.35263062, -0.3805585 ,\n",
       "        -0.35384611, -0.48474404, -0.67471444, -0.48189806, -0.47795872,\n",
       "        -0.66660034, -0.47692092, -0.47161084, -0.63935267, -0.47245657,\n",
       "        -0.46758892, -0.64860747, -0.46910993, -0.49832881, -0.7275881 ,\n",
       "        -0.49877059, -0.4724829 , -0.85954762, -0.48004196, -0.47806262,\n",
       "        -0.65862522, -0.46489138, -0.47023512, -0.66560478, -0.46655412,\n",
       "        -0.50081415, -0.76472896, -0.47321706, -0.47377017, -0.69536372,\n",
       "        -0.47211675, -0.47238922, -0.65994456, -0.47392642, -0.46478843,\n",
       "        -0.67757709, -0.46767057, -0.49245627, -0.8061384 , -0.47436626,\n",
       "        -0.47222418, -0.7276411 , -0.47439738, -0.46856699, -0.65461988,\n",
       "        -0.47236586, -0.46590193, -0.70761959, -0.46778175, -0.43383244,\n",
       "        -0.95832156, -0.43424645, -0.42622434, -0.63178852, -0.43051156,\n",
       "        -0.4253558 , -0.74090154, -0.41753206, -0.41794294, -0.80621922,\n",
       "        -0.43322875, -0.44273413, -0.67086929, -0.44636316, -0.42423815,\n",
       "        -0.61546398, -0.43459137, -0.41130053, -0.73954221, -0.43267473,\n",
       "        -0.41240598, -0.69577322, -0.42104694, -0.43112961, -0.66433492,\n",
       "        -0.45243122, -0.41829484, -0.66026376, -0.42976746, -0.43034353,\n",
       "        -0.83176487, -0.42696179, -0.42319954, -0.70867323, -0.42594709,\n",
       "        -0.43259484, -0.81638227, -0.43073796, -0.42659525, -0.63889666,\n",
       "        -0.45303622, -0.43316941, -0.74342346, -0.41467706, -0.42066613,\n",
       "        -0.59432071, -0.42343076, -0.39737123, -0.40991961, -0.39995328,\n",
       "        -0.3897515 , -0.38809245, -0.39660252, -0.39413219, -0.38401114,\n",
       "        -0.38638   , -0.38204335, -0.38378738, -0.38186708, -0.41287378,\n",
       "        -0.41242955, -0.41589362, -0.38801134, -0.38996994, -0.38867891,\n",
       "        -0.38984156, -0.39072253, -0.38236396, -0.38575414, -0.38589844,\n",
       "        -0.38086374, -0.4090061 , -0.40939798, -0.41890748, -0.38642441,\n",
       "        -0.39687467, -0.38825109, -0.3874221 , -0.39211909, -0.38395287,\n",
       "        -0.39052739, -0.38931103, -0.38738637, -0.42615278, -0.41495148,\n",
       "        -0.41331652, -0.40511492, -0.39820052, -0.40042607, -0.39371603,\n",
       "        -0.39857368, -0.3900722 , -0.39409169, -0.39463997, -0.39295557,\n",
       "        -0.36068969, -0.3496328 , -0.3557247 , -0.34578057, -0.33125875,\n",
       "        -0.3451158 , -0.33680607, -0.34017751, -0.34017626, -0.3336203 ,\n",
       "        -0.34274186, -0.33524776, -0.34988309, -0.37171101, -0.36657096,\n",
       "        -0.33898948, -0.34695316, -0.34476669, -0.34296   , -0.34121353,\n",
       "        -0.33369986, -0.33928082, -0.33522552, -0.32706793, -0.35957602,\n",
       "        -0.37349149, -0.3851374 , -0.34087754, -0.34471688, -0.34317922,\n",
       "        -0.33692006, -0.33355479, -0.33793146, -0.33236149, -0.3314814 ,\n",
       "        -0.33556489, -0.36052621, -0.37333061, -0.37205708, -0.34574287,\n",
       "        -0.34827126, -0.34584596, -0.35272965, -0.33580767, -0.33395803,\n",
       "        -0.3385231 , -0.34233499, -0.34398288]),\n",
       " 'split2_test_neg_log_loss': array([-0.3750193 , -0.36864783, -0.37885129, -0.39978489, -0.38991842,\n",
       "        -0.38485424, -0.41174849, -0.40425745, -0.38346086, -0.41491272,\n",
       "        -0.41010875, -0.40638425, -0.37922249, -0.36378348, -0.37923132,\n",
       "        -0.35853335, -0.36263872, -0.39002823, -0.39761466, -0.39658675,\n",
       "        -0.41007292, -0.39682042, -0.39660642, -0.3934129 , -0.374136  ,\n",
       "        -0.40028627, -0.36193738, -0.39178178, -0.38343312, -0.37043066,\n",
       "        -0.37095245, -0.40175392, -0.39805455, -0.38864498, -0.40735625,\n",
       "        -0.40768119, -0.38481196, -0.36936339, -0.38365924, -0.3806975 ,\n",
       "        -0.40032141, -0.37063426, -0.39308303, -0.39170941, -0.37862659,\n",
       "        -0.39920883, -0.37673511, -0.39252312, -0.39236097, -0.37747967,\n",
       "        -0.3738808 , -0.38668796, -0.37897995, -0.39276236, -0.41401843,\n",
       "        -0.40009904, -0.39367448, -0.40836782, -0.44569681, -0.44858874,\n",
       "        -0.38607448, -0.37414328, -0.36953047, -0.37666352, -0.38856558,\n",
       "        -0.41299798, -0.40652207, -0.36996048, -0.39838151, -0.43794275,\n",
       "        -0.45221247, -0.41021416, -0.36155808, -0.38736455, -0.38163339,\n",
       "        -0.38819945, -0.41683347, -0.36937763, -0.42149787, -0.40647855,\n",
       "        -0.43949958, -0.4170957 , -0.40827052, -0.40644767, -0.38171506,\n",
       "        -0.37231841, -0.37559575, -0.38098935, -0.40237721, -0.36643117,\n",
       "        -0.40412099, -0.40224252, -0.40529453, -0.42610467, -0.40924596,\n",
       "        -0.42243729, -0.49167879, -0.85639276, -0.49333201, -0.48494073,\n",
       "        -0.69321587, -0.48270927, -0.48253662, -0.66734597, -0.48401887,\n",
       "        -0.48315047, -0.65228703, -0.48627173, -0.49688757, -0.78485701,\n",
       "        -0.54883046, -0.48869045, -0.71131405, -0.48758819, -0.48284507,\n",
       "        -0.67310032, -0.48412515, -0.48210702, -0.65972757, -0.4819893 ,\n",
       "        -0.48310489, -0.86884726, -0.49084876, -0.48474259, -0.64893203,\n",
       "        -0.48665032, -0.47991179, -0.68706461, -0.48460232, -0.48627297,\n",
       "        -0.69566931, -0.48321501, -0.49588923, -0.67426362, -0.49347918,\n",
       "        -0.48742175, -0.66246703, -0.48742224, -0.48230519, -0.66859612,\n",
       "        -0.48286721, -0.48478658, -0.66770913, -0.48340496, -0.46861124,\n",
       "        -0.92863178, -0.46153598, -0.43881537, -0.59226618, -0.45845649,\n",
       "        -0.44399862, -0.66518987, -0.45943137, -0.45762081, -0.83686604,\n",
       "        -0.45256487, -0.46698827, -0.64227259, -0.45678258, -0.44111572,\n",
       "        -0.77210533, -0.46249718, -0.45869696, -0.72779369, -0.44893281,\n",
       "        -0.45548407, -0.60673308, -0.44751798, -0.46206299, -0.87204742,\n",
       "        -0.45549736, -0.45602702, -0.78288513, -0.43587903, -0.44544959,\n",
       "        -0.73612147, -0.45130879, -0.45582331, -0.70270381, -0.45462393,\n",
       "        -0.46437043, -0.90491394, -0.45888992, -0.45538894, -0.73981605,\n",
       "        -0.460037  , -0.45209026, -0.92995408, -0.43810053, -0.45766162,\n",
       "        -0.57668158, -0.45114426, -0.43450744, -0.43799437, -0.44810593,\n",
       "        -0.423406  , -0.4243444 , -0.44323822, -0.42921192, -0.42608303,\n",
       "        -0.42383854, -0.42863879, -0.42278183, -0.42242741, -0.43366285,\n",
       "        -0.43188912, -0.43959045, -0.42881294, -0.42981473, -0.43876733,\n",
       "        -0.42488209, -0.42307098, -0.42913082, -0.42549728, -0.43000136,\n",
       "        -0.42816077, -0.44007527, -0.44565746, -0.43486417, -0.43829179,\n",
       "        -0.43699565, -0.42503081, -0.42669018, -0.43429766, -0.42894029,\n",
       "        -0.42695429, -0.42651854, -0.42322414, -0.46022708, -0.46231626,\n",
       "        -0.44664892, -0.44615818, -0.43435592, -0.42786609, -0.43012751,\n",
       "        -0.45210767, -0.43135736, -0.4343599 , -0.42872513, -0.43052432,\n",
       "        -0.41715975, -0.39170645, -0.40561414, -0.39533054, -0.39743456,\n",
       "        -0.39162374, -0.38704056, -0.39302287, -0.37493909, -0.38284976,\n",
       "        -0.38716392, -0.37655538, -0.41890389, -0.39948671, -0.38606838,\n",
       "        -0.39295703, -0.39400577, -0.39818494, -0.38202272, -0.39203523,\n",
       "        -0.38172264, -0.39180268, -0.38300091, -0.38495428, -0.4265498 ,\n",
       "        -0.40050532, -0.41341792, -0.38158842, -0.41347598, -0.39367509,\n",
       "        -0.3930217 , -0.38403274, -0.38627957, -0.3921907 , -0.38937981,\n",
       "        -0.39282761, -0.40170513, -0.40570989, -0.40734101, -0.3983209 ,\n",
       "        -0.39343313, -0.39525916, -0.38699424, -0.38925372, -0.39898722,\n",
       "        -0.38266189, -0.38813916, -0.38866041]),\n",
       " 'split3_test_neg_log_loss': array([-0.37518635, -0.36112792, -0.36563154, -0.37754717, -0.36595751,\n",
       "        -0.35419915, -0.36098478, -0.34829795, -0.36153749, -0.3640244 ,\n",
       "        -0.38188988, -0.37505834, -0.36054919, -0.35754369, -0.36291528,\n",
       "        -0.36038984, -0.35278845, -0.35964544, -0.36043933, -0.36398256,\n",
       "        -0.35909435, -0.36642648, -0.39979268, -0.36148334, -0.35373587,\n",
       "        -0.37501139, -0.366599  , -0.36222211, -0.35604661, -0.36597735,\n",
       "        -0.34796783, -0.35911291, -0.3668752 , -0.36000583, -0.39253933,\n",
       "        -0.37787923, -0.36304783, -0.35599905, -0.35605889, -0.36110473,\n",
       "        -0.38276074, -0.34862831, -0.35112022, -0.34243802, -0.34169124,\n",
       "        -0.34697954, -0.35416355, -0.35128053, -0.35574327, -0.36423977,\n",
       "        -0.36893623, -0.36719739, -0.35974654, -0.36716893, -0.37971509,\n",
       "        -0.36867848, -0.35847404, -0.39522242, -0.39299974, -0.39469104,\n",
       "        -0.36421131, -0.3677544 , -0.37171989, -0.3427947 , -0.36032505,\n",
       "        -0.36916239, -0.35380196, -0.39005568, -0.37322119, -0.38748152,\n",
       "        -0.39170398, -0.37159691, -0.35923821, -0.36688093, -0.36218846,\n",
       "        -0.36209239, -0.35140298, -0.35075314, -0.38095546, -0.38202895,\n",
       "        -0.37927891, -0.36841378, -0.38510774, -0.38927662, -0.35150082,\n",
       "        -0.35561303, -0.36452963, -0.35965651, -0.36967276, -0.3705055 ,\n",
       "        -0.35683048, -0.36103164, -0.38587629, -0.38509304, -0.39099138,\n",
       "        -0.37742944, -0.48863713, -0.6699386 , -0.49193724, -0.4793963 ,\n",
       "        -0.71338532, -0.48192415, -0.47794909, -0.67391465, -0.4809362 ,\n",
       "        -0.47915226, -0.71366734, -0.47951095, -0.50183852, -0.82394042,\n",
       "        -0.49224288, -0.4815193 , -0.79007845, -0.48585543, -0.48492379,\n",
       "        -0.67181209, -0.48023612, -0.47631524, -0.74424846, -0.47899238,\n",
       "        -0.49231839, -0.65518861, -0.48111849, -0.48061341, -0.67269143,\n",
       "        -0.48425111, -0.48196157, -0.83924308, -0.47739176, -0.47579706,\n",
       "        -0.66258442, -0.47709788, -0.4960333 , -0.65459619, -0.49162675,\n",
       "        -0.48012273, -0.76685543, -0.47517798, -0.47617104, -0.6629546 ,\n",
       "        -0.47998752, -0.47626969, -0.73303976, -0.48053787, -0.4511459 ,\n",
       "        -1.00703719, -0.42999418, -0.43970663, -0.72252229, -0.46395482,\n",
       "        -0.44360225, -0.70734877, -0.44742889, -0.42984586, -0.78259629,\n",
       "        -0.44190219, -0.45274087, -0.71096569, -0.4435467 , -0.43947959,\n",
       "        -0.6951765 , -0.43836511, -0.43838863, -0.78691294, -0.43548982,\n",
       "        -0.42931541, -0.73415412, -0.43573301, -0.44199039, -0.70726748,\n",
       "        -0.44056362, -0.45858111, -0.94033657, -0.43737248, -0.45013937,\n",
       "        -0.89328168, -0.43339474, -0.43838398, -0.67412491, -0.44326364,\n",
       "        -0.44020423, -0.79074806, -0.43957252, -0.43173559, -0.8209733 ,\n",
       "        -0.43527145, -0.43535858, -0.74248161, -0.43976183, -0.43843433,\n",
       "        -0.77319866, -0.43474044, -0.4367357 , -0.43343001, -0.42344743,\n",
       "        -0.39972389, -0.41308114, -0.41209452, -0.40922287, -0.40513582,\n",
       "        -0.4013475 , -0.40137536, -0.40435797, -0.40309572, -0.40929874,\n",
       "        -0.41652211, -0.41018412, -0.40329749, -0.40961906, -0.39796521,\n",
       "        -0.41281302, -0.40122686, -0.39906265, -0.40636853, -0.39757818,\n",
       "        -0.39850537, -0.4383684 , -0.41280092, -0.43038055, -0.40176549,\n",
       "        -0.43003546, -0.42977688, -0.4252078 , -0.39785108, -0.41727266,\n",
       "        -0.40041297, -0.40189128, -0.40228739, -0.42750681, -0.44102903,\n",
       "        -0.42652091, -0.41395903, -0.41367895, -0.41228994, -0.40811606,\n",
       "        -0.40510296, -0.4163509 , -0.40712615, -0.40469784, -0.40974022,\n",
       "        -0.39157958, -0.38247978, -0.38750765, -0.36337058, -0.36482683,\n",
       "        -0.3754416 , -0.36011835, -0.36493051, -0.3635573 , -0.36686162,\n",
       "        -0.36249133, -0.35920618, -0.3807221 , -0.38483561, -0.379307  ,\n",
       "        -0.35716497, -0.37548135, -0.37110488, -0.36166869, -0.35866954,\n",
       "        -0.36554234, -0.35481415, -0.35725736, -0.35962867, -0.3795    ,\n",
       "        -0.3960243 , -0.37348414, -0.36672318, -0.36790309, -0.36917257,\n",
       "        -0.36047507, -0.35809971, -0.36435275, -0.35562   , -0.36263478,\n",
       "        -0.35608078, -0.38226596, -0.3989218 , -0.3935246 , -0.37279079,\n",
       "        -0.36441136, -0.36963679, -0.37214586, -0.3597821 , -0.36601749,\n",
       "        -0.36914363, -0.35850221, -0.36262275]),\n",
       " 'split4_test_neg_log_loss': array([-0.34346455, -0.34447826, -0.3458969 , -0.32892332, -0.33663814,\n",
       "        -0.33528467, -0.35606831, -0.36680044, -0.33417857, -0.35239087,\n",
       "        -0.38289855, -0.33357221, -0.35030079, -0.34594262, -0.32875979,\n",
       "        -0.33561068, -0.34665792, -0.33561411, -0.32308656, -0.34955194,\n",
       "        -0.36235292, -0.3669302 , -0.38135235, -0.36727703, -0.34562996,\n",
       "        -0.36883768, -0.33900544, -0.33477613, -0.3305822 , -0.32595996,\n",
       "        -0.32346741, -0.35816291, -0.33596683, -0.34953251, -0.36709382,\n",
       "        -0.34803441, -0.33440941, -0.33055383, -0.34630435, -0.32422269,\n",
       "        -0.33606872, -0.33602904, -0.33132498, -0.33773984, -0.33854634,\n",
       "        -0.33359061, -0.33626266, -0.36517299, -0.33603179, -0.33057182,\n",
       "        -0.34815759, -0.33357958, -0.33974638, -0.34686818, -0.36798687,\n",
       "        -0.36555033, -0.35042071, -0.37215676, -0.38186088, -0.34988821,\n",
       "        -0.34268065, -0.33786951, -0.34842436, -0.34170793, -0.3250954 ,\n",
       "        -0.34784123, -0.35029703, -0.34599894, -0.34759216, -0.35602239,\n",
       "        -0.38362745, -0.38533956, -0.33382067, -0.34147901, -0.36447525,\n",
       "        -0.36706607, -0.35535881, -0.33648016, -0.36415209, -0.33947042,\n",
       "        -0.33626302, -0.37131643, -0.37609393, -0.34935373, -0.33487718,\n",
       "        -0.33674656, -0.33182247, -0.33878226, -0.35062019, -0.33147869,\n",
       "        -0.35479518, -0.34028034, -0.33943254, -0.36987769, -0.3387819 ,\n",
       "        -0.33984215, -0.49574526, -0.71712416, -0.50599179, -0.48102647,\n",
       "        -0.65880797, -0.48903526, -0.48486512, -0.70808226, -0.48343376,\n",
       "        -0.48526039, -0.65361877, -0.48404724, -0.49349709, -0.65565322,\n",
       "        -0.49512937, -0.49631827, -0.70463666, -0.48513049, -0.48256404,\n",
       "        -0.69211886, -0.47841731, -0.4801613 , -0.6601646 , -0.47838552,\n",
       "        -0.4919296 , -0.78712713, -0.48715019, -0.48400116, -0.6818465 ,\n",
       "        -0.48314261, -0.48239914, -0.67597208, -0.48652419, -0.48945279,\n",
       "        -0.67697565, -0.48371702, -0.48973158, -0.70099348, -0.50075157,\n",
       "        -0.48581557, -0.64718165, -0.487102  , -0.48934223, -0.65694808,\n",
       "        -0.48507972, -0.48677954, -0.67751246, -0.47958626, -0.4697    ,\n",
       "        -0.75247601, -0.46141549, -0.44414633, -0.65591153, -0.42514474,\n",
       "        -0.43250833, -0.67931377, -0.43457994, -0.41056569, -0.66961631,\n",
       "        -0.42138985, -0.44173324, -0.69293375, -0.43318826, -0.43690136,\n",
       "        -0.63485526, -0.43347108, -0.42781293, -0.68931444, -0.42658009,\n",
       "        -0.42005089, -0.64918154, -0.42588768, -0.46931152, -0.76703532,\n",
       "        -0.43124668, -0.43458447, -0.58715888, -0.42882705, -0.42889849,\n",
       "        -0.61572555, -0.4403778 , -0.42719937, -0.66430562, -0.42600087,\n",
       "        -0.42898747, -0.64003672, -0.46943691, -0.41761354, -0.62467357,\n",
       "        -0.41997205, -0.42032469, -0.62870972, -0.44259709, -0.40704957,\n",
       "        -0.69735002, -0.41708079, -0.40302944, -0.42867376, -0.39421436,\n",
       "        -0.39172678, -0.39872804, -0.38244021, -0.38418777, -0.38716708,\n",
       "        -0.37988533, -0.3830563 , -0.37902785, -0.38679387, -0.4030166 ,\n",
       "        -0.40765207, -0.42180401, -0.3908627 , -0.385949  , -0.39767562,\n",
       "        -0.38027127, -0.38163294, -0.38708468, -0.37792619, -0.37778675,\n",
       "        -0.37689796, -0.39422647, -0.42029586, -0.41712488, -0.38383011,\n",
       "        -0.38992824, -0.39297399, -0.37800021, -0.38329066, -0.38206243,\n",
       "        -0.38788986, -0.37879011, -0.37844158, -0.41714691, -0.41489495,\n",
       "        -0.41610176, -0.39529621, -0.39457448, -0.40186064, -0.39550893,\n",
       "        -0.394364  , -0.40076733, -0.38674331, -0.38729985, -0.38539524,\n",
       "        -0.38255287, -0.35001577, -0.35011949, -0.34205397, -0.34614521,\n",
       "        -0.34347285, -0.33582421, -0.33555582, -0.33720783, -0.33802876,\n",
       "        -0.33768772, -0.32883142, -0.35509931, -0.34752005, -0.37079157,\n",
       "        -0.33669533, -0.34482577, -0.3459052 , -0.34239684, -0.33088979,\n",
       "        -0.33624737, -0.32804579, -0.32569544, -0.3313564 , -0.37091058,\n",
       "        -0.37131505, -0.36863729, -0.33876397, -0.34853746, -0.35124489,\n",
       "        -0.33018886, -0.3454888 , -0.33663164, -0.32922989, -0.33577951,\n",
       "        -0.33540643, -0.37263684, -0.37731672, -0.36355613, -0.34177579,\n",
       "        -0.35341589, -0.34245235, -0.3421701 , -0.3330741 , -0.33518656,\n",
       "        -0.3335306 , -0.33820181, -0.33170356]),\n",
       " 'mean_test_neg_log_loss': array([-0.35778025, -0.35017451, -0.35982837, -0.35313474, -0.35403049,\n",
       "        -0.35020842, -0.35919622, -0.36597058, -0.35527226, -0.3681068 ,\n",
       "        -0.37361096, -0.35988906, -0.35698937, -0.34695467, -0.34676653,\n",
       "        -0.34187263, -0.34762999, -0.35157862, -0.35932039, -0.35759226,\n",
       "        -0.36549724, -0.36871372, -0.37578715, -0.36576554, -0.34768077,\n",
       "        -0.3649078 , -0.34669017, -0.35084368, -0.3478151 , -0.34983524,\n",
       "        -0.34650981, -0.36122175, -0.35805201, -0.36339689, -0.37222683,\n",
       "        -0.3751297 , -0.35152793, -0.34137884, -0.35435426, -0.3470168 ,\n",
       "        -0.36084189, -0.34349341, -0.34826965, -0.35252774, -0.34528758,\n",
       "        -0.35748102, -0.3495508 , -0.35266332, -0.35123842, -0.35592858,\n",
       "        -0.35402599, -0.35705075, -0.34835907, -0.35493066, -0.3767301 ,\n",
       "        -0.36679214, -0.35948413, -0.38185624, -0.3957618 , -0.37894585,\n",
       "        -0.35278595, -0.34641982, -0.34902914, -0.34756195, -0.35211878,\n",
       "        -0.36130951, -0.36532153, -0.36395718, -0.36503518, -0.38011368,\n",
       "        -0.38829898, -0.37845254, -0.34232322, -0.35591703, -0.35504406,\n",
       "        -0.35625416, -0.35738811, -0.34417261, -0.37320934, -0.36894785,\n",
       "        -0.369988  , -0.37909534, -0.38334937, -0.374888  , -0.34416056,\n",
       "        -0.35069727, -0.3458966 , -0.3518565 , -0.36144165, -0.34469116,\n",
       "        -0.3698529 , -0.36029476, -0.36499288, -0.37864125, -0.37374458,\n",
       "        -0.37538045, -0.48711144, -0.72300607, -0.490214  , -0.4776801 ,\n",
       "        -0.67664819, -0.47905837, -0.47649028, -0.67318651, -0.47655654,\n",
       "        -0.47574952, -0.67478165, -0.47579558, -0.49166968, -0.7334487 ,\n",
       "        -0.50373019, -0.48028931, -0.75846478, -0.48060476, -0.4796178 ,\n",
       "        -0.67654785, -0.47452454, -0.47436576, -0.68950328, -0.47343547,\n",
       "        -0.48777982, -0.74909431, -0.48182579, -0.47749066, -0.6693033 ,\n",
       "        -0.47780626, -0.47598899, -0.71822429, -0.47808798, -0.47644404,\n",
       "        -0.67331063, -0.47511409, -0.48858448, -0.7126946 , -0.48525343,\n",
       "        -0.47874802, -0.70423605, -0.47799491, -0.47551871, -0.67005195,\n",
       "        -0.47764541, -0.47630466, -0.69777919, -0.47473109, -0.4516578 ,\n",
       "        -0.90580901, -0.44996607, -0.43601234, -0.65782948, -0.43861778,\n",
       "        -0.43255623, -0.68124274, -0.43776673, -0.4282108 , -0.76442359,\n",
       "        -0.43231699, -0.44701456, -0.68466608, -0.44531618, -0.43831501,\n",
       "        -0.67625803, -0.43701794, -0.43247582, -0.74387433, -0.43576589,\n",
       "        -0.42876024, -0.66806206, -0.42958703, -0.4459688 , -0.74779482,\n",
       "        -0.44628117, -0.44052082, -0.72194485, -0.4316204 , -0.43812729,\n",
       "        -0.74335559, -0.43519776, -0.43249101, -0.70182933, -0.4350124 ,\n",
       "        -0.44173406, -0.75799188, -0.44718804, -0.43010577, -0.69281022,\n",
       "        -0.43927711, -0.4318064 , -0.75604202, -0.43281977, -0.42931694,\n",
       "        -0.71479774, -0.43223089, -0.41500497, -0.42435841, -0.41518233,\n",
       "        -0.39682976, -0.40077063, -0.40279547, -0.39910861, -0.39630254,\n",
       "        -0.39347339, -0.39378131, -0.39272885, -0.39443405, -0.41292185,\n",
       "        -0.41512054, -0.4177036 , -0.39714254, -0.39982657, -0.4017114 ,\n",
       "        -0.39694159, -0.39529608, -0.39462462, -0.39488964, -0.393845  ,\n",
       "        -0.39355754, -0.41664891, -0.42052644, -0.41695334, -0.39785607,\n",
       "        -0.40609529, -0.40567324, -0.39954147, -0.39761168, -0.39844881,\n",
       "        -0.39650146, -0.39476773, -0.39372008, -0.42535324, -0.4294953 ,\n",
       "        -0.41988384, -0.4119974 , -0.40762709, -0.40968496, -0.40198871,\n",
       "        -0.40916929, -0.40480821, -0.40142592, -0.39879487, -0.40023687,\n",
       "        -0.38210268, -0.36501841, -0.37248928, -0.35895406, -0.35488415,\n",
       "        -0.36037894, -0.35231006, -0.35539455, -0.35066279, -0.35141661,\n",
       "        -0.35414275, -0.34768714, -0.37224989, -0.37218721, -0.37331894,\n",
       "        -0.35421768, -0.36026418, -0.36219313, -0.35414826, -0.35320465,\n",
       "        -0.35103632, -0.35123309, -0.34782961, -0.34762888, -0.37830015,\n",
       "        -0.37917236, -0.37947948, -0.3557853 , -0.36263081, -0.35928479,\n",
       "        -0.35211677, -0.3527275 , -0.35319886, -0.35028195, -0.35231064,\n",
       "        -0.35111448, -0.37610718, -0.38405947, -0.38106052, -0.36141878,\n",
       "        -0.36154391, -0.35993163, -0.35827238, -0.35123772, -0.35629228,\n",
       "        -0.35258171, -0.35367269, -0.3529834 ]),\n",
       " 'std_test_neg_log_loss': array([0.01495497, 0.01335366, 0.01143945, 0.02985999, 0.02113431,\n",
       "        0.01894223, 0.02858728, 0.02004954, 0.01853809, 0.02634314,\n",
       "        0.0242837 , 0.02740028, 0.01215544, 0.01241604, 0.0215059 ,\n",
       "        0.01727026, 0.01863757, 0.02169635, 0.02363974, 0.02310621,\n",
       "        0.02302836, 0.01572788, 0.02174496, 0.01562941, 0.0189732 ,\n",
       "        0.02278177, 0.01677755, 0.02370554, 0.02264225, 0.01790753,\n",
       "        0.01575008, 0.02170363, 0.02513063, 0.01433195, 0.0248228 ,\n",
       "        0.02170398, 0.0196074 , 0.01799499, 0.01564621, 0.0208084 ,\n",
       "        0.02592579, 0.01611594, 0.02372296, 0.02052627, 0.01952094,\n",
       "        0.02306908, 0.01489717, 0.02553329, 0.02193818, 0.01748036,\n",
       "        0.01507484, 0.02005158, 0.01871555, 0.02279488, 0.01987877,\n",
       "        0.01839413, 0.01930983, 0.01693959, 0.02548939, 0.03883376,\n",
       "        0.01977334, 0.02186844, 0.0208804 , 0.01623554, 0.02159338,\n",
       "        0.02918697, 0.02391841, 0.01519735, 0.02036993, 0.03490441,\n",
       "        0.03508821, 0.01809642, 0.01706602, 0.01873682, 0.018933  ,\n",
       "        0.0228051 , 0.03131401, 0.01435325, 0.02681304, 0.02356218,\n",
       "        0.03802053, 0.01976036, 0.01549586, 0.02413697, 0.02191451,\n",
       "        0.0123891 , 0.02168021, 0.01654196, 0.02365314, 0.01967722,\n",
       "        0.0189035 , 0.02211097, 0.02840681, 0.02613095, 0.02619828,\n",
       "        0.02831506, 0.00715671, 0.06878727, 0.0098199 , 0.00671928,\n",
       "        0.02319201, 0.00814551, 0.00712243, 0.02197039, 0.00839876,\n",
       "        0.00861537, 0.02866605, 0.00984521, 0.01223315, 0.06373446,\n",
       "        0.02309402, 0.01190557, 0.05889624, 0.00847916, 0.00544352,\n",
       "        0.01190777, 0.00805437, 0.00697609, 0.03497024, 0.00805285,\n",
       "        0.01019994, 0.07895895, 0.00647593, 0.00764042, 0.01860317,\n",
       "        0.00898529, 0.00730129, 0.06467172, 0.00683042, 0.0101301 ,\n",
       "        0.01434813, 0.0080673 , 0.01015972, 0.05280089, 0.01293083,\n",
       "        0.0075077 , 0.04389226, 0.00824117, 0.00990648, 0.01917267,\n",
       "        0.0064861 , 0.00851421, 0.02316249, 0.00817612, 0.01554637,\n",
       "        0.08669055, 0.01463912, 0.00643318, 0.04471387, 0.01918107,\n",
       "        0.01036389, 0.04265359, 0.01445549, 0.01608857, 0.05954406,\n",
       "        0.01424659, 0.01215235, 0.02535799, 0.00754596, 0.00827269,\n",
       "        0.05497762, 0.01486151, 0.01569893, 0.03500354, 0.00731786,\n",
       "        0.01457871, 0.04344785, 0.01082623, 0.01711777, 0.07042358,\n",
       "        0.00906084, 0.01500037, 0.12668629, 0.00427166, 0.00835529,\n",
       "        0.10709069, 0.00983962, 0.01347479, 0.0332582 , 0.01193186,\n",
       "        0.01233981, 0.10431602, 0.01454829, 0.01363311, 0.07616586,\n",
       "        0.01506415, 0.01221398, 0.09717842, 0.01015591, 0.01732076,\n",
       "        0.13013305, 0.01163932, 0.01698263, 0.01143412, 0.019229  ,\n",
       "        0.01476298, 0.01624853, 0.02329257, 0.0182692 , 0.01729063,\n",
       "        0.01747725, 0.01962173, 0.01826797, 0.01640419, 0.01088819,\n",
       "        0.00905914, 0.01290852, 0.01825439, 0.01756366, 0.01916818,\n",
       "        0.01877524, 0.01583356, 0.01888929, 0.01841418, 0.01946566,\n",
       "        0.0187858 , 0.01902175, 0.01305149, 0.01802641, 0.02160778,\n",
       "        0.0234172 , 0.0178799 , 0.02178812, 0.01935971, 0.02050605,\n",
       "        0.01699246, 0.01815529, 0.01725306, 0.02081435, 0.01931992,\n",
       "        0.01642732, 0.0181897 , 0.0149426 , 0.00997996, 0.01625144,\n",
       "        0.02178612, 0.01699786, 0.01823538, 0.01724778, 0.01784986,\n",
       "        0.02160152, 0.01826645, 0.02090658, 0.01957965, 0.02428616,\n",
       "        0.01960505, 0.01944538, 0.02134748, 0.0156309 , 0.01984357,\n",
       "        0.01867334, 0.0176579 , 0.02563757, 0.01861782, 0.00824451,\n",
       "        0.02063108, 0.02094757, 0.0203421 , 0.0158237 , 0.02135045,\n",
       "        0.01919419, 0.02200192, 0.02035643, 0.02182096, 0.02560948,\n",
       "        0.01697382, 0.01926288, 0.01625152, 0.02725385, 0.020049  ,\n",
       "        0.02281313, 0.0175172 , 0.01939107, 0.02288071, 0.02139396,\n",
       "        0.02232046, 0.0148958 , 0.01557413, 0.01663263, 0.02140331,\n",
       "        0.01700445, 0.02013721, 0.01868872, 0.02123376, 0.02425841,\n",
       "        0.01961035, 0.01862842, 0.02061886]),\n",
       " 'rank_test_neg_log_loss': array([ 78,  28,  86,  53,  58,  29,  82, 109,  66, 111, 122,  87,  73,\n",
       "         14,  13,   2,  18,  41,  84,  77, 107, 112, 127, 108,  19, 102,\n",
       "         12,  33,  21,  27,  11,  93,  79, 100, 117, 125,  40,   1,  62,\n",
       "         15,  92,   4,  23,  47,   8,  76,  26,  49,  38,  70,  57,  74,\n",
       "         24,  64, 129, 110,  85, 139, 155, 133,  51,  10,  25,  16,  44,\n",
       "         94, 106, 101, 105, 137, 143, 131,   3,  69,  65,  71,  75,   6,\n",
       "        120, 113, 115, 134, 141, 124,   5,  32,   9,  42,  96,   7, 114,\n",
       "         90, 103, 132, 123, 126, 251, 278, 254, 240, 266, 245, 236, 261,\n",
       "        237, 231, 263, 232, 255, 279, 256, 247, 286, 248, 246, 265, 227,\n",
       "        226, 269, 225, 252, 283, 249, 238, 259, 241, 233, 276, 243, 235,\n",
       "        262, 229, 253, 274, 250, 244, 273, 242, 230, 260, 239, 234, 271,\n",
       "        228, 224, 288, 223, 209, 257, 214, 204, 267, 211, 192, 287, 201,\n",
       "        221, 268, 218, 213, 264, 210, 202, 281, 208, 193, 258, 196, 219,\n",
       "        282, 220, 216, 277, 198, 212, 280, 207, 203, 272, 206, 217, 285,\n",
       "        222, 197, 270, 215, 199, 284, 205, 194, 275, 200, 182, 190, 184,\n",
       "        158, 169, 173, 165, 156, 145, 148, 144, 150, 181, 183, 187, 160,\n",
       "        167, 171, 159, 154, 151, 153, 149, 146, 185, 189, 186, 162, 176,\n",
       "        175, 166, 161, 163, 157, 152, 147, 191, 195, 188, 180, 177, 179,\n",
       "        172, 178, 174, 170, 164, 168, 140, 104, 119,  81,  63,  91,  45,\n",
       "         67,  31,  39,  59,  20, 118, 116, 121,  61,  89,  98,  60,  55,\n",
       "         34,  36,  22,  17, 130, 135, 136,  68,  99,  83,  43,  50,  54,\n",
       "         30,  46,  35, 128, 142, 138,  95,  97,  88,  80,  37,  72,  48,\n",
       "         56,  52])}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRIAL_4_MLP.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FOUR NEG LOG LOSS RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.1,\n",
       " 'classifier__hidden_layer_sizes': (5,),\n",
       " 'classifier__learning_rate': 'invscaling',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL FOUR NEG LOG LOSS RESULTS ---------------------------------------------\")\n",
    "TRIAL_4_MLP.cv_results_['rank_test_neg_log_loss']\n",
    "TRIAL_4_MLP.cv_results_['params'][ np.argmin(TRIAL_4_MLP.cv_results_['rank_test_neg_log_loss']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FOUR F1 RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.1,\n",
       " 'classifier__hidden_layer_sizes': (15,),\n",
       " 'classifier__learning_rate': 'adaptive',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL FOUR F1 RESULTS ---------------------------------------------\")\n",
    "TRIAL_4_MLP.cv_results_['rank_test_f1_micro']\n",
    "TRIAL_4_MLP.cv_results_['params'][ np.argmin(TRIAL_4_MLP.cv_results_['rank_test_f1_micro']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FOUR ROC AUC RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.1,\n",
       " 'classifier__hidden_layer_sizes': (15,),\n",
       " 'classifier__learning_rate': 'adaptive',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL FOUR ROC AUC RESULTS ---------------------------------------------\")\n",
    "TRIAL_4_MLP.cv_results_['rank_test_roc_auc_ovo']\n",
    "TRIAL_4_MLP.cv_results_['params'][ np.argmin(TRIAL_4_MLP.cv_results_['rank_test_roc_auc_ovo']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FOUR JACCARD SCORE RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.1,\n",
       " 'classifier__hidden_layer_sizes': (15,),\n",
       " 'classifier__learning_rate': 'adaptive',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL FOUR JACCARD SCORE RESULTS ---------------------------------------------\")\n",
    "TRIAL_4_MLP.cv_results_['rank_test_jaccard']\n",
    "TRIAL_4_MLP.cv_results_['params'][ np.argmin(TRIAL_4_MLP.cv_results_['rank_test_jaccard']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 4 Multilayered Perceptrons using best NEG LOG LOSS hyperparameters :0.814931650893796\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 4 Multilayered Perceptrons using best F1 hyperparameters :0.8009113214160533\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 4 Multilayered Perceptrons using best ROC_AUC hyperparameters :0.8098142306344199\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 4 Multilayered Perceptrons using best JACCARD hyperparameters :0.8063091482649842\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best NEG LOG LOSS results to train on test set, \n",
    "bestMPLT4_1 = MLPClassifier(activation = 'logistic', alpha = .1, hidden_layer_sizes = (20,), \n",
    "                          learning_rate = 'invscaling',solver = 'lbfgs')\n",
    "bestMPLT4_1.fit(X_train,y_train)\n",
    "y_pred4_1 = bestMPLT4_1.predict(X_test)\n",
    "print('Accuracy of Trial 4 Multilayered Perceptrons using best NEG LOG LOSS hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred4_1)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best F1 results to train on test set, \n",
    "bestMPLT4_2 = MLPClassifier(activation = 'tanh', alpha = 0.1, hidden_layer_sizes = (15,), \n",
    "                          learning_rate = 'adaptive',solver = 'lbfgs')\n",
    "bestMPLT4_2.fit(X_train,y_train)\n",
    "y_pred4_2 = bestMPLT4_2.predict(X_test)\n",
    "print('Accuracy of Trial 4 Multilayered Perceptrons using best F1 hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred4_2)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best ROC_AUC results to train on test set,\n",
    "bestMPLT4_3 = MLPClassifier(activation = 'logistic', alpha = 0.1, hidden_layer_sizes = (15,), \n",
    "                          learning_rate = 'adaptive',solver = 'lbfgs')\n",
    "bestMPLT4_3.fit(X_train,y_train)\n",
    "y_pred4_3 = bestMPLT4_3.predict(X_test)\n",
    "print('Accuracy of Trial 4 Multilayered Perceptrons using best ROC_AUC hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred4_3)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best JACCARD results to train on test set, \n",
    "bestMPLT4_4 = MLPClassifier(activation = 'logistic', alpha = 0.1, hidden_layer_sizes = (15,), \n",
    "                          learning_rate = 'adaptive',solver = 'lbfgs')\n",
    "bestMPLT4_4.fit(X_train,y_train)\n",
    "y_pred4_4 = bestMPLT4_4.predict(X_test)\n",
    "print('Accuracy of Trial 4 Multilayered Perceptrons using best JACCARD hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred4_4)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTI LAYERED PERCEPTRON TRIAL FIVE ON GAMMA PARTICLES DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    8.5s\n",
      "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed:    9.5s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   11.6s\n",
      "[Parallel(n_jobs=-1)]: Done 189 tasks      | elapsed:   12.4s\n",
      "[Parallel(n_jobs=-1)]: Done 210 tasks      | elapsed:   13.4s\n",
      "[Parallel(n_jobs=-1)]: Done 233 tasks      | elapsed:   15.7s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:   16.7s\n",
      "[Parallel(n_jobs=-1)]: Done 281 tasks      | elapsed:   18.8s\n",
      "[Parallel(n_jobs=-1)]: Done 306 tasks      | elapsed:   20.8s\n",
      "[Parallel(n_jobs=-1)]: Done 333 tasks      | elapsed:   23.1s\n",
      "[Parallel(n_jobs=-1)]: Done 360 tasks      | elapsed:   25.2s\n",
      "[Parallel(n_jobs=-1)]: Done 389 tasks      | elapsed:   26.7s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:   29.8s\n",
      "[Parallel(n_jobs=-1)]: Done 449 tasks      | elapsed:   31.4s\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:   34.5s\n",
      "[Parallel(n_jobs=-1)]: Done 513 tasks      | elapsed:   38.7s\n",
      "[Parallel(n_jobs=-1)]: Done 546 tasks      | elapsed:   42.4s\n",
      "[Parallel(n_jobs=-1)]: Done 581 tasks      | elapsed:   46.5s\n",
      "[Parallel(n_jobs=-1)]: Done 616 tasks      | elapsed:   50.5s\n",
      "[Parallel(n_jobs=-1)]: Done 653 tasks      | elapsed:   54.7s\n",
      "[Parallel(n_jobs=-1)]: Done 690 tasks      | elapsed:   58.9s\n",
      "[Parallel(n_jobs=-1)]: Done 729 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 809 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 850 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 893 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 936 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 981 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1026 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1073 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1120 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1169 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1269 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1320 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1373 tasks      | elapsed:  2.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FIVE RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1440 out of 1440 | elapsed:  3.0min finished\n"
     ]
    }
   ],
   "source": [
    "ITERATIONS = 1\n",
    "for i in range(1):\n",
    "    #For reach \"trial\", randomly select 5000 data points\n",
    "    random5000DataPoints = gammaData.sample(n = 5000, replace = True)\n",
    "    #From that 5000, create x set and y set\n",
    "    xSet = random5000DataPoints.iloc[:,0:9].values\n",
    "    ySet = random5000DataPoints.iloc[:,10].values\n",
    "    ySet = encodeLabel.fit_transform(ySet)\n",
    "    # Run GridSearch to find best hyperparameters per 5000 data points \n",
    "    \n",
    "    pipe = Pipeline([('std', StandardScaler()),('classifier', MLPClassifier(max_iter = 250))])\n",
    "\n",
    "    search_space = [\n",
    "                {\n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                {\n",
    "                 \n",
    "                  \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['sgd'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                \n",
    "                {\n",
    "                \n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['adam'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                ]\n",
    "    \n",
    "    runHyperParamSearch = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=5), \n",
    "                          scoring=['recall_micro', 'f1_micro','precision_micro','roc_auc_ovo','jaccard','neg_log_loss'], \n",
    "                          refit=False, verbose=10, n_jobs = -1)\n",
    "    TRIAL_5_MLP = runHyperParamSearch.fit(xSet, ySet)\n",
    "    print(\"---------------------------------TRIAL FIVE RESULTS ---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.56388664, 0.57359514, 0.56488552, 0.78837929, 0.84763012,\n",
       "        0.79047995, 1.18922377, 1.21924911, 1.19732938, 1.55437927,\n",
       "        1.58826394, 1.49658661, 0.76335478, 0.56998997, 0.56340432,\n",
       "        0.75244737, 0.79908819, 0.81300154, 1.1111577 , 1.19432812,\n",
       "        1.23316183, 1.56594434, 1.51440268, 1.421522  , 0.71301184,\n",
       "        0.62113256, 0.5677876 , 0.76996255, 0.81039715, 0.83551831,\n",
       "        1.18251662, 1.1878232 , 1.18682079, 1.48757844, 1.51460047,\n",
       "        1.45815258, 0.63014255, 0.443782  , 0.53936357, 0.7692626 ,\n",
       "        0.76786046, 0.79838705, 1.16369834, 1.17220707, 1.173211  ,\n",
       "        1.49828839, 1.5122015 , 1.42702479, 0.70230384, 0.63985052,\n",
       "        0.6585649 , 0.90738091, 0.95602436, 0.95252061, 1.39159827,\n",
       "        1.43843517, 1.44814653, 1.90844045, 1.75320768, 1.84418478,\n",
       "        0.74013605, 0.65586457, 0.64445457, 0.95762415, 0.9656322 ,\n",
       "        0.952321  , 1.40280676, 1.41041369, 1.45314918, 1.83868132,\n",
       "        1.83247547, 1.81486006, 0.7373332 , 0.65566297, 0.64255395,\n",
       "        0.93830719, 0.93740721, 0.9543211 , 1.34535809, 1.43543324,\n",
       "        1.44434242, 1.89923563, 1.84098334, 1.73589272, 0.69640098,\n",
       "        0.654562  , 0.65246148, 0.90988269, 0.94971828, 0.96022654,\n",
       "        1.38899488, 1.4422411 , 1.47026386, 1.850492  , 1.76251345,\n",
       "        1.69655519, 2.44249983, 0.39093614, 2.26454835, 2.42598667,\n",
       "        0.59411135, 2.41898088, 2.66669402, 0.66797414, 2.67820349,\n",
       "        2.80381112, 0.41465592, 2.7950038 , 2.27695799, 0.2110817 ,\n",
       "        2.31028709, 2.43619504, 0.1789537 , 2.44690452, 2.65568352,\n",
       "        0.82831221, 2.72814651, 2.86826696, 0.44918609, 2.82352781,\n",
       "        2.32760196, 0.26172547, 2.2997777 , 2.39055753, 0.20837913,\n",
       "        2.45911517, 2.64707637, 0.27253432, 2.72084007, 2.81602192,\n",
       "        0.25411882, 2.79510365, 2.28756752, 0.13631687, 2.2763576 ,\n",
       "        2.42238336, 0.19406705, 2.43349323, 2.63606739, 0.56828847,\n",
       "        2.68651023, 2.80531282, 0.27133331, 2.76087451, 2.34711866,\n",
       "        0.73323078, 2.3575274 , 2.52487135, 0.45699358, 2.52507257,\n",
       "        2.76187501, 0.51194015, 2.80010858, 2.94313111, 1.23546247,\n",
       "        2.91310534, 2.36763639, 0.26432748, 2.32800255, 2.50855756,\n",
       "        0.71731734, 2.52296934, 2.8043118 , 1.29301205, 2.86526399,\n",
       "        2.95013709, 0.85403423, 2.89719162, 2.33200583, 0.24731293,\n",
       "        2.34982066, 2.49234433, 0.51184025, 2.5660068 , 2.74976549,\n",
       "        0.81440091, 2.80301089, 2.97215648, 0.78507528, 2.95333905,\n",
       "        2.3789463 , 0.42706785, 2.31859393, 2.49834881, 0.85923891,\n",
       "        2.50695643, 2.72354217, 0.62323565, 2.83643913, 2.93003764,\n",
       "        1.11616035, 2.89078612, 2.5414856 , 2.55559769, 2.55609832,\n",
       "        2.6512804 , 2.67339911, 2.68090577, 2.96965337, 2.94162965,\n",
       "        2.96595087, 3.03761253, 3.1109756 , 3.03160701, 2.54038477,\n",
       "        2.60944414, 2.52807422, 2.72844729, 2.74005733, 2.73995619,\n",
       "        2.93472428, 2.94032884, 2.98977189, 3.12348652, 3.04561949,\n",
       "        3.0732429 , 2.58452244, 2.52537241, 2.52527237, 2.66078858,\n",
       "        2.68420849, 2.74816413, 3.03170767, 2.92851906, 2.95814404,\n",
       "        3.07164249, 3.06883869, 3.05302582, 2.5753149 , 2.53397884,\n",
       "        2.53988442, 2.64207296, 2.67960458, 2.70002189, 2.89939322,\n",
       "        2.9993794 , 2.94773521, 3.07754827, 3.10697184, 3.0276041 ,\n",
       "        2.62395658, 2.60704179, 2.58442235, 2.75416832, 2.80100913,\n",
       "        2.80431161, 3.11097598, 3.11067567, 3.1174808 , 3.30484204,\n",
       "        3.27561741, 3.28042116, 2.68400788, 2.76828046, 2.85105162,\n",
       "        2.94573321, 3.08465261, 2.98546762, 3.22867551, 3.22527385,\n",
       "        3.21376386, 3.38200908, 3.39301786, 3.455972  , 2.85185299,\n",
       "        2.84534678, 2.83834109, 2.98686862, 2.93712616, 3.05092354,\n",
       "        3.20225463, 3.19514813, 3.30494275, 3.43075051, 3.48379602,\n",
       "        3.52483025, 2.79770584, 2.80881534, 2.70232396, 2.84594765,\n",
       "        2.99647727, 3.07264223, 3.30804553, 3.63973002, 3.55886059,\n",
       "        3.51262035, 3.15981784, 2.66228957]),\n",
       " 'std_fit_time': array([0.02156797, 0.02862065, 0.04228289, 0.02713223, 0.05496549,\n",
       "        0.04940286, 0.07447605, 0.0289157 , 0.05375248, 0.0624212 ,\n",
       "        0.01379375, 0.06738373, 0.07877258, 0.05828869, 0.01620404,\n",
       "        0.01788593, 0.00818853, 0.01373343, 0.04451212, 0.02055085,\n",
       "        0.02185284, 0.07562295, 0.03713039, 0.01729836, 0.05790351,\n",
       "        0.02432995, 0.03882425, 0.02128477, 0.05050274, 0.08105413,\n",
       "        0.03918205, 0.0651048 , 0.03164127, 0.0581412 , 0.03285247,\n",
       "        0.08344473, 0.05715515, 0.09761296, 0.05773069, 0.02411739,\n",
       "        0.01597813, 0.02847854, 0.03418333, 0.03878595, 0.02728252,\n",
       "        0.05036222, 0.05490086, 0.05984943, 0.04376058, 0.01284362,\n",
       "        0.0360132 , 0.02555885, 0.02256078, 0.03017931, 0.04432762,\n",
       "        0.02163914, 0.03385273, 0.16066215, 0.06008719, 0.15556596,\n",
       "        0.05666468, 0.0375901 , 0.02257242, 0.03138099, 0.03216558,\n",
       "        0.03695517, 0.02315796, 0.02935178, 0.01543139, 0.14289751,\n",
       "        0.14041385, 0.16570674, 0.05411827, 0.03015922, 0.01297351,\n",
       "        0.00699289, 0.03583407, 0.02550075, 0.03076455, 0.0299621 ,\n",
       "        0.03046813, 0.1196534 , 0.12552606, 0.05934566, 0.03731107,\n",
       "        0.02817936, 0.01276352, 0.01858622, 0.04169022, 0.03988926,\n",
       "        0.03209416, 0.04241997, 0.03916529, 0.1604595 , 0.11850312,\n",
       "        0.03549147, 0.11070749, 0.29060527, 0.05549513, 0.06084147,\n",
       "        0.53001102, 0.0317122 , 0.06300772, 0.75171321, 0.05843227,\n",
       "        0.0388409 , 0.24865672, 0.05624989, 0.04298202, 0.09461063,\n",
       "        0.0412999 , 0.03975499, 0.05039061, 0.01629134, 0.02688455,\n",
       "        0.40648605, 0.06529533, 0.09025826, 0.27893659, 0.05016231,\n",
       "        0.03908424, 0.11439704, 0.03692699, 0.02122206, 0.08544418,\n",
       "        0.01914478, 0.04052533, 0.15230952, 0.07560238, 0.07291412,\n",
       "        0.18038423, 0.07663141, 0.03809218, 0.0175269 , 0.03462919,\n",
       "        0.04936795, 0.05793358, 0.04862706, 0.02748744, 0.61382472,\n",
       "        0.06324781, 0.02160868, 0.12280835, 0.0462163 , 0.05315388,\n",
       "        0.82385512, 0.04228725, 0.05184192, 0.29419963, 0.06096168,\n",
       "        0.03209506, 0.12342402, 0.06321948, 0.05613845, 0.8413585 ,\n",
       "        0.0560478 , 0.07933182, 0.08212137, 0.02664814, 0.03325998,\n",
       "        0.73296561, 0.02571767, 0.10289359, 0.53726224, 0.08954001,\n",
       "        0.08738204, 0.4081136 , 0.0717674 , 0.02988233, 0.16886553,\n",
       "        0.05569753, 0.04626363, 0.15059475, 0.04468662, 0.06774268,\n",
       "        0.55853803, 0.05285638, 0.0650037 , 0.30372213, 0.06706408,\n",
       "        0.04042172, 0.27655816, 0.02035817, 0.03503661, 0.8423824 ,\n",
       "        0.03563484, 0.04804611, 0.37795501, 0.01166767, 0.07565647,\n",
       "        0.63701793, 0.05929549, 0.06708325, 0.05445382, 0.05133638,\n",
       "        0.02522471, 0.05900796, 0.06541837, 0.07259732, 0.05761707,\n",
       "        0.05558631, 0.03193705, 0.06006768, 0.07163451, 0.03499127,\n",
       "        0.0416615 , 0.0750557 , 0.1185992 , 0.0838361 , 0.08019201,\n",
       "        0.07280238, 0.03633454, 0.07028062, 0.03930008, 0.05614393,\n",
       "        0.06680969, 0.04476252, 0.01604385, 0.0215296 , 0.04480785,\n",
       "        0.06482107, 0.02374896, 0.1297708 , 0.05877977, 0.02627718,\n",
       "        0.06655466, 0.04998844, 0.05359976, 0.04101717, 0.02348197,\n",
       "        0.04869723, 0.03343895, 0.03716311, 0.03175813, 0.03002479,\n",
       "        0.04218627, 0.05053635, 0.0623334 , 0.04484412, 0.04870577,\n",
       "        0.02900672, 0.03699968, 0.04995089, 0.06391111, 0.03606747,\n",
       "        0.03998874, 0.04562017, 0.04907655, 0.06474064, 0.03927163,\n",
       "        0.0924582 , 0.08086136, 0.06080944, 0.06529654, 0.07667391,\n",
       "        0.12048437, 0.05848267, 0.06690799, 0.08235851, 0.07756966,\n",
       "        0.03476731, 0.06548041, 0.09420705, 0.15490859, 0.11594529,\n",
       "        0.17334662, 0.17946174, 0.09395936, 0.1066699 , 0.09692423,\n",
       "        0.05563292, 0.05814859, 0.05857104, 0.11407466, 0.13215463,\n",
       "        0.17553619, 0.16554005, 0.11478038, 0.04415674, 0.08978949,\n",
       "        0.18258985, 0.12670802, 0.07857217, 0.34839068, 0.29368828,\n",
       "        0.31499557, 0.10574893, 0.11994101]),\n",
       " 'mean_score_time': array([0.01301093, 0.01030784, 0.01051002, 0.01120954, 0.01211095,\n",
       "        0.01491294, 0.0137115 , 0.01261115, 0.01241035, 0.0127686 ,\n",
       "        0.01341248, 0.02001834, 0.01251054, 0.01010833, 0.01059051,\n",
       "        0.01161027, 0.01241074, 0.01220875, 0.01461296, 0.01581206,\n",
       "        0.0142117 , 0.01451302, 0.0132112 , 0.01090851, 0.01571479,\n",
       "        0.01101007, 0.01111073, 0.0118113 , 0.01160994, 0.01341286,\n",
       "        0.01521339, 0.01361027, 0.01461291, 0.01471219, 0.01781492,\n",
       "        0.01151042, 0.01120872, 0.01030898, 0.01031013, 0.0124104 ,\n",
       "        0.01241059, 0.01231008, 0.01251097, 0.0115099 , 0.01381049,\n",
       "        0.014714  , 0.01161008, 0.01251106, 0.01060872, 0.0111095 ,\n",
       "        0.01171017, 0.01121025, 0.01240916, 0.01321049, 0.01831527,\n",
       "        0.016013  , 0.0153111 , 0.0140131 , 0.01240988, 0.01221046,\n",
       "        0.01120911, 0.01100893, 0.01161013, 0.01100883, 0.01511374,\n",
       "        0.01221037, 0.01481214, 0.01251097, 0.01411366, 0.0136117 ,\n",
       "        0.0124104 , 0.01190968, 0.01261029, 0.01050925, 0.01111078,\n",
       "        0.01200967, 0.01271071, 0.01221056, 0.01461134, 0.01521273,\n",
       "        0.01421194, 0.01661196, 0.01371193, 0.01140957, 0.01140695,\n",
       "        0.01000867, 0.01090922, 0.01241126, 0.01180997, 0.01251049,\n",
       "        0.01401396, 0.01471229, 0.01521339, 0.01671352, 0.01241112,\n",
       "        0.01261096, 0.00930748, 0.01000886, 0.01000862, 0.00950818,\n",
       "        0.00900755, 0.01040907, 0.01020842, 0.00930805, 0.01211076,\n",
       "        0.00970879, 0.00970883, 0.00970831, 0.00920801, 0.009308  ,\n",
       "        0.00900741, 0.00920796, 0.00940804, 0.00930815, 0.01180973,\n",
       "        0.00990911, 0.00990834, 0.01020856, 0.01130948, 0.01120944,\n",
       "        0.01060886, 0.00870714, 0.00930824, 0.00920687, 0.01050925,\n",
       "        0.00920815, 0.00960851, 0.00970826, 0.01010861, 0.01050901,\n",
       "        0.00980816, 0.00980887, 0.00910769, 0.00940828, 0.01361179,\n",
       "        0.01130986, 0.00920806, 0.01130939, 0.00990868, 0.01000881,\n",
       "        0.00980844, 0.00990806, 0.00940838, 0.00980844, 0.01120949,\n",
       "        0.0092082 , 0.01050868, 0.00950818, 0.00960798, 0.01291037,\n",
       "        0.01010909, 0.00970836, 0.0119102 , 0.00990834, 0.00980864,\n",
       "        0.00990853, 0.00970783, 0.01000867, 0.01080899, 0.00930758,\n",
       "        0.00940814, 0.00970869, 0.01010876, 0.00970817, 0.01000853,\n",
       "        0.00990868, 0.01000853, 0.0120101 , 0.00970798, 0.00910764,\n",
       "        0.00930829, 0.0095078 , 0.00940795, 0.010109  , 0.01050863,\n",
       "        0.00980854, 0.01050906, 0.0106091 , 0.01030912, 0.01050911,\n",
       "        0.00940785, 0.00960827, 0.01301088, 0.01030889, 0.00930843,\n",
       "        0.01110926, 0.01070986, 0.00950875, 0.00980864, 0.01029105,\n",
       "        0.01000824, 0.01271076, 0.01010852, 0.00930781, 0.00910769,\n",
       "        0.01030898, 0.00920825, 0.00960846, 0.00990877, 0.00980811,\n",
       "        0.01020861, 0.00980883, 0.00990863, 0.01020885, 0.01080933,\n",
       "        0.01010861, 0.0089076 , 0.00990791, 0.00940752, 0.01000862,\n",
       "        0.01481295, 0.01000853, 0.01020851, 0.01020899, 0.00980844,\n",
       "        0.00930781, 0.00940809, 0.01050873, 0.00930762, 0.01381216,\n",
       "        0.00940781, 0.01221004, 0.00980835, 0.01010866, 0.01030917,\n",
       "        0.01100936, 0.01020918, 0.00960836, 0.00920792, 0.01000905,\n",
       "        0.00900788, 0.01080937, 0.00920806, 0.00940852, 0.01060896,\n",
       "        0.01020856, 0.00980844, 0.00950742, 0.01191068, 0.00950813,\n",
       "        0.00940814, 0.01030879, 0.01010852, 0.0093081 , 0.00950871,\n",
       "        0.01050911, 0.01010852, 0.01020837, 0.01411252, 0.01040945,\n",
       "        0.0103085 , 0.0128109 , 0.01231065, 0.01441221, 0.01441283,\n",
       "        0.01371188, 0.01050897, 0.01040883, 0.01241035, 0.01090927,\n",
       "        0.00970831, 0.01010823, 0.01431241, 0.0107091 , 0.00950723,\n",
       "        0.01070924, 0.00950847, 0.01321173, 0.01601386, 0.01231084,\n",
       "        0.01060882, 0.01110954, 0.01040945, 0.01181045, 0.01050916,\n",
       "        0.01221099, 0.01291113, 0.0092083 , 0.00940828, 0.02512174,\n",
       "        0.01341114, 0.01751552, 0.0106082 , 0.01281133, 0.01010847,\n",
       "        0.00880795, 0.00730624, 0.00620461]),\n",
       " 'std_score_time': array([2.19278854e-03, 8.12257863e-04, 6.32901694e-04, 6.78472780e-04,\n",
       "        1.20029909e-03, 3.20350683e-03, 1.69341666e-03, 1.15851814e-03,\n",
       "        1.11450642e-03, 2.13151987e-03, 1.53161218e-03, 1.48550460e-02,\n",
       "        1.87255454e-03, 5.82603956e-04, 7.79721963e-04, 5.82865251e-04,\n",
       "        1.39405255e-03, 9.79146103e-04, 2.08544459e-03, 2.44250979e-03,\n",
       "        1.60099130e-03, 1.38001743e-03, 2.84084057e-03, 3.74267435e-04,\n",
       "        3.47526220e-03, 8.36941689e-04, 9.70661593e-04, 1.94182090e-03,\n",
       "        3.74317795e-04, 1.85568265e-03, 6.46004062e-03, 1.35664598e-03,\n",
       "        1.02121352e-03, 3.87218855e-03, 1.14316640e-02, 3.00202374e-03,\n",
       "        6.78571736e-04, 6.78523175e-04, 6.00483504e-04, 1.59551400e-03,\n",
       "        1.59521106e-03, 8.71580854e-04, 1.00081063e-03, 5.47552485e-04,\n",
       "        2.38267598e-03, 4.96163044e-03, 1.39436759e-03, 1.95028196e-03,\n",
       "        1.20154656e-03, 1.02124512e-03, 1.53705834e-03, 8.72571811e-04,\n",
       "        1.85424479e-03, 1.16713180e-03, 1.16312718e-02, 2.28196312e-03,\n",
       "        1.03098396e-03, 6.29721005e-04, 5.82899040e-04, 8.72176817e-04,\n",
       "        8.13573246e-04, 2.05089383e-03, 1.93603624e-03, 3.15754516e-04,\n",
       "        3.56976219e-03, 8.12393345e-04, 1.66351260e-03, 5.47814435e-04,\n",
       "        3.73973902e-04, 1.24302653e-03, 2.15476673e-03, 6.63967028e-04,\n",
       "        3.33969360e-03, 6.34068444e-04, 8.00493182e-04, 7.07730722e-04,\n",
       "        7.49405364e-04, 2.45128223e-04, 2.01202423e-03, 1.50429930e-03,\n",
       "        1.36461459e-03, 5.83181119e-03, 1.56976316e-03, 8.60297694e-04,\n",
       "        5.79567749e-04, 3.15452364e-04, 8.61140667e-04, 9.70572566e-04,\n",
       "        1.20870648e-03, 7.07292195e-04, 2.05041756e-03, 1.69292077e-03,\n",
       "        3.98505507e-03, 6.30322484e-03, 1.15792020e-03, 3.24911545e-03,\n",
       "        2.44678969e-04, 1.22568797e-03, 8.37425952e-04, 8.38195329e-04,\n",
       "        3.16205119e-04, 1.59515119e-03, 6.79069846e-04, 2.44853415e-04,\n",
       "        4.21531981e-03, 2.45165564e-04, 5.10426157e-04, 2.45846603e-04,\n",
       "        5.10295354e-04, 5.10276510e-04, 3.16431502e-04, 2.45184316e-04,\n",
       "        5.84058863e-04, 4.00269148e-04, 2.92791362e-03, 8.61007500e-04,\n",
       "        3.74520987e-04, 5.10099420e-04, 3.12706654e-03, 1.12337177e-03,\n",
       "        2.71198282e-03, 2.45106990e-04, 4.00614863e-04, 2.45597968e-04,\n",
       "        2.28176436e-03, 5.09584861e-04, 2.00391348e-04, 7.49035558e-04,\n",
       "        8.01009086e-04, 1.22562951e-03, 8.72028521e-04, 5.10304626e-04,\n",
       "        2.00224632e-04, 5.83282103e-04, 8.73339214e-03, 2.73358647e-03,\n",
       "        4.00257366e-04, 4.10973575e-03, 5.83887110e-04, 6.32862311e-04,\n",
       "        2.45495970e-04, 2.00796665e-04, 3.74521170e-04, 4.00734373e-04,\n",
       "        2.97870318e-03, 2.45087017e-04, 2.26037136e-03, 3.16431574e-04,\n",
       "        2.00176864e-04, 5.08753589e-03, 5.83494636e-04, 6.79063049e-04,\n",
       "        1.62628063e-03, 3.74394020e-04, 2.44484023e-04, 4.90485618e-04,\n",
       "        5.10913083e-04, 1.55063546e-03, 1.63286441e-03, 2.44561936e-04,\n",
       "        3.74521321e-04, 2.45340452e-04, 7.35384217e-04, 2.45476955e-04,\n",
       "        3.16657587e-04, 1.99199315e-04, 3.16582291e-04, 3.78513752e-03,\n",
       "        8.12849765e-04, 2.00010277e-04, 4.00162283e-04, 4.47554734e-04,\n",
       "        3.74469981e-04, 5.83511115e-04, 1.26596934e-03, 6.00616237e-04,\n",
       "        1.04986968e-03, 3.74879760e-04, 5.10417475e-04, 4.47714457e-04,\n",
       "        3.74572041e-04, 8.00907762e-04, 5.58180551e-03, 1.43657474e-03,\n",
       "        4.00412135e-04, 2.26938821e-03, 1.25033702e-03, 3.16808491e-04,\n",
       "        3.99935312e-04, 7.45112255e-04, 5.48379807e-04, 3.41756173e-03,\n",
       "        1.59559999e-03, 5.10342070e-04, 2.00343324e-04, 1.32773281e-03,\n",
       "        2.45048624e-04, 4.90524922e-04, 3.74890593e-04, 6.78360135e-04,\n",
       "        4.00424043e-04, 6.00672024e-04, 3.74151679e-04, 1.16728350e-03,\n",
       "        2.13755866e-03, 1.24262200e-03, 2.00057335e-04, 5.83511232e-04,\n",
       "        1.99915422e-04, 1.51817885e-03, 8.96648728e-03, 7.74448379e-04,\n",
       "        7.48799749e-04, 4.00590968e-04, 2.45593616e-04, 2.45242983e-04,\n",
       "        3.74954422e-04, 2.28237076e-03, 5.10501180e-04, 7.61339497e-03,\n",
       "        3.74240697e-04, 2.62149574e-03, 5.10707120e-04, 1.24146546e-03,\n",
       "        5.10763013e-04, 1.89813394e-03, 6.78950388e-04, 5.83134952e-04,\n",
       "        5.10285885e-04, 2.02717164e-03, 2.33601546e-07, 2.87671841e-03,\n",
       "        4.00734033e-04, 4.90232417e-04, 1.06839421e-03, 5.10098952e-04,\n",
       "        4.00436580e-04, 3.16742685e-04, 2.74800275e-03, 4.47181385e-04,\n",
       "        5.84525061e-04, 1.43647836e-03, 9.70454287e-04, 2.45087852e-04,\n",
       "        4.47501387e-04, 1.51770716e-03, 4.90310376e-04, 5.09706470e-04,\n",
       "        7.71022479e-03, 3.74954816e-04, 2.44600730e-04, 5.10958390e-03,\n",
       "        2.58308624e-03, 6.68705492e-03, 9.09845170e-03, 5.69368837e-03,\n",
       "        4.47607790e-04, 8.00901721e-04, 3.60000396e-03, 2.08517804e-03,\n",
       "        2.45359969e-04, 4.90660779e-04, 7.67815496e-03, 7.48697864e-04,\n",
       "        3.16282151e-04, 2.46393022e-03, 7.07493892e-04, 3.46178578e-03,\n",
       "        6.40111034e-03, 4.27680932e-03, 1.02016965e-03, 7.35818950e-04,\n",
       "        3.74699616e-04, 3.61739242e-03, 5.48684080e-04, 2.76954083e-03,\n",
       "        5.58486823e-03, 2.45399350e-04, 3.74431878e-04, 2.19083960e-02,\n",
       "        4.25052347e-03, 1.08537247e-02, 1.20115332e-03, 3.96068301e-03,\n",
       "        8.00550031e-04, 7.48914958e-04, 1.16738979e-03, 9.27952945e-04]),\n",
       " 'param_classifier': masked_array(data=[MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__activation': masked_array(data=['logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__alpha': masked_array(data=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__hidden_layer_sizes': masked_array(data=[(5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__learning_rate': masked_array(data=['constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'}],\n",
       " 'split0_test_recall_micro': array([0.835, 0.854, 0.857, 0.846, 0.851, 0.853, 0.845, 0.855, 0.847,\n",
       "        0.841, 0.861, 0.854, 0.846, 0.853, 0.831, 0.853, 0.853, 0.847,\n",
       "        0.847, 0.849, 0.847, 0.837, 0.842, 0.843, 0.848, 0.851, 0.85 ,\n",
       "        0.851, 0.85 , 0.851, 0.847, 0.844, 0.849, 0.843, 0.842, 0.845,\n",
       "        0.856, 0.848, 0.852, 0.855, 0.871, 0.847, 0.858, 0.841, 0.858,\n",
       "        0.843, 0.842, 0.847, 0.843, 0.849, 0.85 , 0.858, 0.85 , 0.853,\n",
       "        0.852, 0.843, 0.856, 0.848, 0.842, 0.847, 0.836, 0.846, 0.843,\n",
       "        0.841, 0.843, 0.849, 0.85 , 0.851, 0.855, 0.838, 0.846, 0.848,\n",
       "        0.838, 0.854, 0.848, 0.857, 0.856, 0.847, 0.861, 0.858, 0.847,\n",
       "        0.838, 0.849, 0.855, 0.856, 0.858, 0.833, 0.841, 0.85 , 0.836,\n",
       "        0.854, 0.863, 0.843, 0.855, 0.834, 0.858, 0.766, 0.352, 0.767,\n",
       "        0.762, 0.352, 0.769, 0.769, 0.352, 0.769, 0.771, 0.351, 0.769,\n",
       "        0.741, 0.716, 0.763, 0.764, 0.435, 0.766, 0.767, 0.352, 0.766,\n",
       "        0.769, 0.351, 0.767, 0.76 , 0.722, 0.757, 0.765, 0.65 , 0.76 ,\n",
       "        0.767, 0.352, 0.767, 0.769, 0.656, 0.769, 0.765, 0.352, 0.745,\n",
       "        0.769, 0.336, 0.771, 0.767, 0.315, 0.768, 0.765, 0.648, 0.765,\n",
       "        0.788, 0.546, 0.802, 0.799, 0.569, 0.798, 0.791, 0.51 , 0.799,\n",
       "        0.807, 0.419, 0.8  , 0.787, 0.419, 0.784, 0.804, 0.571, 0.79 ,\n",
       "        0.802, 0.39 , 0.821, 0.805, 0.624, 0.786, 0.784, 0.618, 0.792,\n",
       "        0.796, 0.542, 0.8  , 0.8  , 0.518, 0.796, 0.793, 0.664, 0.805,\n",
       "        0.779, 0.543, 0.805, 0.786, 0.612, 0.795, 0.787, 0.63 , 0.796,\n",
       "        0.802, 0.605, 0.8  , 0.784, 0.809, 0.798, 0.838, 0.822, 0.818,\n",
       "        0.836, 0.838, 0.837, 0.837, 0.834, 0.838, 0.801, 0.827, 0.81 ,\n",
       "        0.834, 0.843, 0.842, 0.836, 0.831, 0.831, 0.838, 0.837, 0.838,\n",
       "        0.821, 0.796, 0.786, 0.838, 0.83 , 0.823, 0.837, 0.842, 0.835,\n",
       "        0.838, 0.84 , 0.838, 0.79 , 0.826, 0.828, 0.838, 0.795, 0.841,\n",
       "        0.832, 0.836, 0.84 , 0.836, 0.836, 0.835, 0.831, 0.837, 0.843,\n",
       "        0.849, 0.846, 0.846, 0.851, 0.845, 0.851, 0.851, 0.849, 0.855,\n",
       "        0.839, 0.838, 0.831, 0.845, 0.845, 0.842, 0.848, 0.852, 0.851,\n",
       "        0.854, 0.857, 0.85 , 0.827, 0.834, 0.844, 0.847, 0.844, 0.846,\n",
       "        0.85 , 0.852, 0.853, 0.853, 0.85 , 0.853, 0.842, 0.837, 0.839,\n",
       "        0.846, 0.838, 0.849, 0.855, 0.847, 0.849, 0.851, 0.846, 0.849]),\n",
       " 'split1_test_recall_micro': array([0.857, 0.846, 0.85 , 0.853, 0.869, 0.843, 0.863, 0.85 , 0.864,\n",
       "        0.86 , 0.858, 0.846, 0.854, 0.848, 0.855, 0.856, 0.856, 0.861,\n",
       "        0.862, 0.861, 0.872, 0.859, 0.851, 0.858, 0.853, 0.852, 0.853,\n",
       "        0.843, 0.86 , 0.86 , 0.854, 0.854, 0.864, 0.855, 0.855, 0.865,\n",
       "        0.862, 0.853, 0.857, 0.862, 0.863, 0.859, 0.872, 0.861, 0.862,\n",
       "        0.86 , 0.856, 0.871, 0.86 , 0.858, 0.864, 0.859, 0.859, 0.855,\n",
       "        0.852, 0.866, 0.868, 0.855, 0.853, 0.857, 0.852, 0.862, 0.849,\n",
       "        0.858, 0.863, 0.86 , 0.852, 0.854, 0.863, 0.848, 0.842, 0.863,\n",
       "        0.849, 0.856, 0.842, 0.867, 0.859, 0.844, 0.865, 0.858, 0.856,\n",
       "        0.853, 0.859, 0.855, 0.862, 0.861, 0.858, 0.87 , 0.855, 0.867,\n",
       "        0.861, 0.873, 0.859, 0.863, 0.858, 0.865, 0.791, 0.353, 0.795,\n",
       "        0.794, 0.287, 0.795, 0.794, 0.647, 0.796, 0.791, 0.604, 0.795,\n",
       "        0.792, 0.353, 0.791, 0.793, 0.696, 0.794, 0.79 , 0.353, 0.792,\n",
       "        0.794, 0.647, 0.797, 0.767, 0.353, 0.793, 0.79 , 0.647, 0.792,\n",
       "        0.786, 0.662, 0.793, 0.795, 0.354, 0.795, 0.793, 0.647, 0.794,\n",
       "        0.796, 0.647, 0.791, 0.797, 0.353, 0.794, 0.795, 0.665, 0.792,\n",
       "        0.797, 0.485, 0.809, 0.8  , 0.456, 0.813, 0.815, 0.528, 0.829,\n",
       "        0.81 , 0.658, 0.831, 0.81 , 0.507, 0.811, 0.812, 0.361, 0.825,\n",
       "        0.818, 0.479, 0.828, 0.821, 0.541, 0.812, 0.813, 0.648, 0.807,\n",
       "        0.807, 0.549, 0.82 , 0.806, 0.299, 0.808, 0.818, 0.534, 0.822,\n",
       "        0.805, 0.385, 0.819, 0.813, 0.542, 0.806, 0.818, 0.538, 0.83 ,\n",
       "        0.822, 0.461, 0.807, 0.833, 0.834, 0.838, 0.843, 0.836, 0.844,\n",
       "        0.839, 0.842, 0.838, 0.84 , 0.841, 0.839, 0.842, 0.83 , 0.834,\n",
       "        0.84 , 0.842, 0.84 , 0.842, 0.838, 0.838, 0.839, 0.839, 0.844,\n",
       "        0.808, 0.824, 0.822, 0.84 , 0.841, 0.838, 0.84 , 0.845, 0.838,\n",
       "        0.838, 0.842, 0.839, 0.833, 0.837, 0.824, 0.842, 0.829, 0.839,\n",
       "        0.839, 0.84 , 0.837, 0.84 , 0.838, 0.841, 0.847, 0.848, 0.846,\n",
       "        0.858, 0.854, 0.859, 0.853, 0.86 , 0.858, 0.86 , 0.858, 0.857,\n",
       "        0.848, 0.843, 0.85 , 0.859, 0.852, 0.865, 0.853, 0.855, 0.862,\n",
       "        0.861, 0.86 , 0.855, 0.845, 0.847, 0.854, 0.854, 0.857, 0.859,\n",
       "        0.859, 0.86 , 0.858, 0.862, 0.865, 0.86 , 0.844, 0.849, 0.846,\n",
       "        0.857, 0.858, 0.856, 0.86 , 0.863, 0.858, 0.858, 0.859, 0.855]),\n",
       " 'split2_test_recall_micro': array([0.838, 0.85 , 0.857, 0.868, 0.856, 0.846, 0.848, 0.853, 0.855,\n",
       "        0.845, 0.857, 0.852, 0.838, 0.849, 0.859, 0.846, 0.86 , 0.853,\n",
       "        0.858, 0.857, 0.855, 0.852, 0.837, 0.847, 0.843, 0.847, 0.853,\n",
       "        0.846, 0.862, 0.859, 0.871, 0.858, 0.854, 0.852, 0.858, 0.856,\n",
       "        0.847, 0.858, 0.84 , 0.855, 0.851, 0.864, 0.86 , 0.845, 0.86 ,\n",
       "        0.862, 0.86 , 0.845, 0.854, 0.851, 0.833, 0.859, 0.859, 0.845,\n",
       "        0.85 , 0.855, 0.868, 0.843, 0.859, 0.848, 0.843, 0.86 , 0.85 ,\n",
       "        0.85 , 0.848, 0.855, 0.848, 0.845, 0.855, 0.863, 0.85 , 0.854,\n",
       "        0.849, 0.845, 0.842, 0.865, 0.858, 0.859, 0.846, 0.856, 0.856,\n",
       "        0.849, 0.833, 0.851, 0.853, 0.862, 0.852, 0.857, 0.853, 0.855,\n",
       "        0.865, 0.861, 0.851, 0.86 , 0.863, 0.865, 0.768, 0.353, 0.762,\n",
       "        0.773, 0.647, 0.769, 0.772, 0.647, 0.776, 0.772, 0.645, 0.771,\n",
       "        0.767, 0.654, 0.769, 0.774, 0.632, 0.769, 0.77 , 0.353, 0.772,\n",
       "        0.773, 0.386, 0.769, 0.771, 0.353, 0.768, 0.766, 0.343, 0.767,\n",
       "        0.77 , 0.578, 0.771, 0.771, 0.647, 0.767, 0.771, 0.647, 0.771,\n",
       "        0.767, 0.352, 0.769, 0.77 , 0.647, 0.772, 0.771, 0.647, 0.774,\n",
       "        0.785, 0.594, 0.832, 0.813, 0.568, 0.791, 0.798, 0.468, 0.821,\n",
       "        0.807, 0.541, 0.811, 0.771, 0.366, 0.798, 0.822, 0.628, 0.817,\n",
       "        0.822, 0.362, 0.794, 0.826, 0.439, 0.803, 0.801, 0.649, 0.804,\n",
       "        0.787, 0.524, 0.815, 0.802, 0.399, 0.82 , 0.816, 0.559, 0.804,\n",
       "        0.791, 0.652, 0.78 , 0.806, 0.518, 0.822, 0.833, 0.682, 0.829,\n",
       "        0.794, 0.648, 0.797, 0.793, 0.831, 0.817, 0.836, 0.838, 0.826,\n",
       "        0.839, 0.833, 0.835, 0.835, 0.838, 0.841, 0.827, 0.823, 0.804,\n",
       "        0.823, 0.839, 0.835, 0.834, 0.839, 0.835, 0.838, 0.837, 0.84 ,\n",
       "        0.816, 0.811, 0.832, 0.839, 0.839, 0.84 , 0.831, 0.837, 0.838,\n",
       "        0.838, 0.837, 0.843, 0.825, 0.805, 0.819, 0.829, 0.83 , 0.813,\n",
       "        0.836, 0.835, 0.84 , 0.84 , 0.835, 0.836, 0.834, 0.825, 0.837,\n",
       "        0.838, 0.844, 0.847, 0.847, 0.852, 0.849, 0.846, 0.849, 0.85 ,\n",
       "        0.839, 0.845, 0.837, 0.845, 0.845, 0.839, 0.845, 0.847, 0.851,\n",
       "        0.847, 0.858, 0.847, 0.826, 0.839, 0.836, 0.833, 0.848, 0.842,\n",
       "        0.852, 0.85 , 0.843, 0.845, 0.844, 0.845, 0.836, 0.835, 0.834,\n",
       "        0.838, 0.842, 0.842, 0.846, 0.847, 0.846, 0.851, 0.851, 0.851]),\n",
       " 'split3_test_recall_micro': array([0.854, 0.841, 0.844, 0.859, 0.861, 0.853, 0.85 , 0.858, 0.842,\n",
       "        0.853, 0.844, 0.853, 0.853, 0.849, 0.843, 0.863, 0.851, 0.862,\n",
       "        0.853, 0.847, 0.854, 0.868, 0.865, 0.859, 0.853, 0.851, 0.852,\n",
       "        0.856, 0.858, 0.844, 0.863, 0.845, 0.843, 0.846, 0.862, 0.853,\n",
       "        0.847, 0.857, 0.84 , 0.851, 0.846, 0.854, 0.868, 0.857, 0.874,\n",
       "        0.857, 0.85 , 0.86 , 0.852, 0.849, 0.852, 0.862, 0.857, 0.852,\n",
       "        0.855, 0.867, 0.847, 0.865, 0.867, 0.845, 0.862, 0.845, 0.848,\n",
       "        0.861, 0.85 , 0.865, 0.838, 0.858, 0.861, 0.859, 0.865, 0.851,\n",
       "        0.852, 0.854, 0.851, 0.856, 0.861, 0.863, 0.856, 0.848, 0.866,\n",
       "        0.866, 0.847, 0.856, 0.852, 0.852, 0.851, 0.857, 0.855, 0.852,\n",
       "        0.848, 0.851, 0.854, 0.859, 0.847, 0.855, 0.788, 0.621, 0.779,\n",
       "        0.789, 0.353, 0.791, 0.792, 0.317, 0.793, 0.788, 0.381, 0.79 ,\n",
       "        0.785, 0.647, 0.778, 0.788, 0.342, 0.792, 0.793, 0.315, 0.79 ,\n",
       "        0.792, 0.648, 0.787, 0.791, 0.353, 0.775, 0.79 , 0.435, 0.783,\n",
       "        0.792, 0.535, 0.785, 0.788, 0.647, 0.788, 0.789, 0.648, 0.789,\n",
       "        0.79 , 0.647, 0.789, 0.788, 0.647, 0.79 , 0.793, 0.68 , 0.791,\n",
       "        0.812, 0.497, 0.796, 0.804, 0.52 , 0.822, 0.83 , 0.547, 0.835,\n",
       "        0.815, 0.625, 0.813, 0.835, 0.719, 0.795, 0.816, 0.47 , 0.8  ,\n",
       "        0.814, 0.301, 0.803, 0.811, 0.59 , 0.813, 0.8  , 0.436, 0.798,\n",
       "        0.813, 0.304, 0.808, 0.825, 0.769, 0.816, 0.813, 0.415, 0.804,\n",
       "        0.808, 0.67 , 0.811, 0.836, 0.613, 0.801, 0.8  , 0.377, 0.825,\n",
       "        0.811, 0.577, 0.815, 0.831, 0.827, 0.805, 0.848, 0.841, 0.842,\n",
       "        0.841, 0.845, 0.842, 0.847, 0.844, 0.845, 0.818, 0.807, 0.835,\n",
       "        0.847, 0.842, 0.846, 0.845, 0.844, 0.848, 0.847, 0.843, 0.842,\n",
       "        0.838, 0.81 , 0.814, 0.838, 0.844, 0.835, 0.841, 0.841, 0.843,\n",
       "        0.842, 0.847, 0.847, 0.829, 0.814, 0.824, 0.84 , 0.835, 0.841,\n",
       "        0.838, 0.841, 0.843, 0.845, 0.844, 0.846, 0.864, 0.847, 0.843,\n",
       "        0.853, 0.852, 0.857, 0.857, 0.856, 0.856, 0.848, 0.854, 0.852,\n",
       "        0.851, 0.854, 0.85 , 0.854, 0.852, 0.854, 0.858, 0.861, 0.858,\n",
       "        0.857, 0.853, 0.853, 0.845, 0.856, 0.855, 0.85 , 0.852, 0.853,\n",
       "        0.852, 0.855, 0.852, 0.85 , 0.86 , 0.856, 0.854, 0.852, 0.855,\n",
       "        0.852, 0.858, 0.856, 0.858, 0.86 , 0.855, 0.86 , 0.861, 0.85 ]),\n",
       " 'split4_test_recall_micro': array([0.865, 0.859, 0.858, 0.863, 0.856, 0.856, 0.855, 0.851, 0.85 ,\n",
       "        0.842, 0.862, 0.846, 0.863, 0.854, 0.861, 0.856, 0.862, 0.862,\n",
       "        0.867, 0.865, 0.862, 0.864, 0.848, 0.861, 0.862, 0.86 , 0.861,\n",
       "        0.867, 0.851, 0.858, 0.86 , 0.852, 0.857, 0.841, 0.843, 0.859,\n",
       "        0.85 , 0.861, 0.859, 0.857, 0.867, 0.86 , 0.858, 0.858, 0.859,\n",
       "        0.862, 0.861, 0.867, 0.857, 0.868, 0.858, 0.86 , 0.862, 0.856,\n",
       "        0.863, 0.85 , 0.84 , 0.858, 0.863, 0.849, 0.853, 0.856, 0.861,\n",
       "        0.842, 0.869, 0.849, 0.847, 0.852, 0.871, 0.862, 0.856, 0.869,\n",
       "        0.863, 0.862, 0.863, 0.843, 0.855, 0.864, 0.863, 0.847, 0.872,\n",
       "        0.846, 0.863, 0.837, 0.857, 0.841, 0.843, 0.847, 0.86 , 0.865,\n",
       "        0.865, 0.853, 0.852, 0.852, 0.86 , 0.868, 0.784, 0.353, 0.794,\n",
       "        0.791, 0.623, 0.793, 0.793, 0.647, 0.794, 0.794, 0.647, 0.796,\n",
       "        0.795, 0.353, 0.793, 0.793, 0.647, 0.793, 0.791, 0.647, 0.79 ,\n",
       "        0.797, 0.348, 0.793, 0.786, 0.353, 0.79 , 0.792, 0.592, 0.794,\n",
       "        0.796, 0.647, 0.792, 0.792, 0.647, 0.788, 0.78 , 0.647, 0.794,\n",
       "        0.793, 0.42 , 0.797, 0.793, 0.585, 0.791, 0.792, 0.575, 0.79 ,\n",
       "        0.822, 0.355, 0.825, 0.822, 0.705, 0.822, 0.829, 0.66 , 0.821,\n",
       "        0.828, 0.393, 0.822, 0.832, 0.435, 0.833, 0.818, 0.413, 0.81 ,\n",
       "        0.825, 0.396, 0.831, 0.833, 0.364, 0.834, 0.817, 0.581, 0.818,\n",
       "        0.831, 0.612, 0.825, 0.824, 0.549, 0.832, 0.816, 0.704, 0.821,\n",
       "        0.819, 0.358, 0.815, 0.837, 0.366, 0.82 , 0.832, 0.601, 0.829,\n",
       "        0.82 , 0.367, 0.835, 0.845, 0.834, 0.834, 0.847, 0.845, 0.837,\n",
       "        0.852, 0.849, 0.85 , 0.849, 0.849, 0.849, 0.842, 0.843, 0.817,\n",
       "        0.849, 0.842, 0.849, 0.846, 0.851, 0.85 , 0.85 , 0.849, 0.851,\n",
       "        0.835, 0.84 , 0.838, 0.843, 0.848, 0.84 , 0.85 , 0.849, 0.847,\n",
       "        0.844, 0.843, 0.849, 0.846, 0.843, 0.831, 0.843, 0.841, 0.843,\n",
       "        0.845, 0.844, 0.849, 0.847, 0.848, 0.849, 0.844, 0.847, 0.845,\n",
       "        0.852, 0.855, 0.849, 0.842, 0.851, 0.854, 0.856, 0.853, 0.854,\n",
       "        0.854, 0.842, 0.852, 0.853, 0.854, 0.85 , 0.847, 0.846, 0.847,\n",
       "        0.846, 0.852, 0.855, 0.853, 0.846, 0.847, 0.854, 0.852, 0.857,\n",
       "        0.847, 0.852, 0.851, 0.85 , 0.85 , 0.849, 0.844, 0.856, 0.848,\n",
       "        0.853, 0.859, 0.853, 0.853, 0.849, 0.857, 0.853, 0.85 , 0.856]),\n",
       " 'mean_test_recall_micro': array([0.8498, 0.85  , 0.8532, 0.8578, 0.8586, 0.8502, 0.8522, 0.8534,\n",
       "        0.8516, 0.8482, 0.8564, 0.8502, 0.8508, 0.8506, 0.8498, 0.8548,\n",
       "        0.8564, 0.857 , 0.8574, 0.8558, 0.858 , 0.856 , 0.8486, 0.8536,\n",
       "        0.8518, 0.8522, 0.8538, 0.8526, 0.8562, 0.8544, 0.859 , 0.8506,\n",
       "        0.8534, 0.8474, 0.852 , 0.8556, 0.8524, 0.8554, 0.8496, 0.856 ,\n",
       "        0.8596, 0.8568, 0.8632, 0.8524, 0.8626, 0.8568, 0.8538, 0.858 ,\n",
       "        0.8532, 0.855 , 0.8514, 0.8596, 0.8574, 0.8522, 0.8544, 0.8562,\n",
       "        0.8558, 0.8538, 0.8568, 0.8492, 0.8492, 0.8538, 0.8502, 0.8504,\n",
       "        0.8546, 0.8556, 0.847 , 0.852 , 0.861 , 0.854 , 0.8518, 0.857 ,\n",
       "        0.8502, 0.8542, 0.8492, 0.8576, 0.8578, 0.8554, 0.8582, 0.8534,\n",
       "        0.8594, 0.8504, 0.8502, 0.8508, 0.856 , 0.8548, 0.8474, 0.8544,\n",
       "        0.8546, 0.855 , 0.8586, 0.8602, 0.8518, 0.8578, 0.8524, 0.8622,\n",
       "        0.7794, 0.4064, 0.7794, 0.7818, 0.4524, 0.7834, 0.784 , 0.522 ,\n",
       "        0.7856, 0.7832, 0.5256, 0.7842, 0.776 , 0.5446, 0.7788, 0.7824,\n",
       "        0.5504, 0.7828, 0.7822, 0.404 , 0.782 , 0.785 , 0.476 , 0.7826,\n",
       "        0.775 , 0.4268, 0.7766, 0.7806, 0.5334, 0.7792, 0.7822, 0.5548,\n",
       "        0.7816, 0.783 , 0.5902, 0.7814, 0.7796, 0.5882, 0.7786, 0.783 ,\n",
       "        0.4804, 0.7834, 0.783 , 0.5094, 0.783 , 0.7832, 0.643 , 0.7824,\n",
       "        0.8008, 0.4954, 0.8128, 0.8076, 0.5636, 0.8092, 0.8126, 0.5426,\n",
       "        0.821 , 0.8134, 0.5272, 0.8154, 0.807 , 0.4892, 0.8042, 0.8144,\n",
       "        0.4886, 0.8084, 0.8162, 0.3856, 0.8154, 0.8192, 0.5116, 0.8096,\n",
       "        0.803 , 0.5864, 0.8038, 0.8068, 0.5062, 0.8136, 0.8114, 0.5068,\n",
       "        0.8144, 0.8112, 0.5752, 0.8112, 0.8004, 0.5216, 0.806 , 0.8156,\n",
       "        0.5302, 0.8088, 0.814 , 0.5656, 0.8218, 0.8098, 0.5316, 0.8108,\n",
       "        0.8172, 0.827 , 0.8184, 0.8424, 0.8364, 0.8334, 0.8414, 0.8414,\n",
       "        0.8404, 0.8416, 0.8412, 0.8424, 0.826 , 0.826 , 0.82  , 0.8386,\n",
       "        0.8416, 0.8424, 0.8406, 0.8406, 0.8404, 0.8424, 0.841 , 0.843 ,\n",
       "        0.8236, 0.8162, 0.8184, 0.8396, 0.8404, 0.8352, 0.8398, 0.8428,\n",
       "        0.8402, 0.84  , 0.8418, 0.8432, 0.8246, 0.825 , 0.8252, 0.8384,\n",
       "        0.826 , 0.8354, 0.838 , 0.8392, 0.8418, 0.8416, 0.8402, 0.8414,\n",
       "        0.844 , 0.8408, 0.8428, 0.85  , 0.8502, 0.8516, 0.85  , 0.8528,\n",
       "        0.8536, 0.8522, 0.8526, 0.8536, 0.8462, 0.8444, 0.844 , 0.8512,\n",
       "        0.8496, 0.85  , 0.8502, 0.8522, 0.8538, 0.853 , 0.856 , 0.852 ,\n",
       "        0.8392, 0.8444, 0.8472, 0.8476, 0.8506, 0.8514, 0.852 , 0.8538,\n",
       "        0.8514, 0.852 , 0.8538, 0.8526, 0.844 , 0.8458, 0.8444, 0.8492,\n",
       "        0.851 , 0.8512, 0.8544, 0.8532, 0.853 , 0.8546, 0.8534, 0.8522]),\n",
       " 'std_test_recall_micro': array([0.01147868, 0.00622896, 0.00541849, 0.00767854, 0.00608605,\n",
       "        0.00487442, 0.00630555, 0.00287054, 0.00749933, 0.00724983,\n",
       "        0.00646838, 0.00348712, 0.00837616, 0.00241661, 0.01128539,\n",
       "        0.00549181, 0.00412795, 0.00603324, 0.0069455 , 0.00688186,\n",
       "        0.00846168, 0.01089954, 0.0095205 , 0.0072    , 0.00630555,\n",
       "        0.00426146, 0.00376298, 0.00845222, 0.00483322, 0.00608605,\n",
       "        0.00812404, 0.00535164, 0.00711618, 0.00531413, 0.00807465,\n",
       "        0.00662118, 0.00581722, 0.00449889, 0.00816333, 0.00357771,\n",
       "        0.00954149, 0.00584466, 0.00574108, 0.00788923, 0.0058515 ,\n",
       "        0.00713863, 0.00705408, 0.01043072, 0.00577581, 0.00729383,\n",
       "        0.01042305, 0.00135647, 0.00402989, 0.00386782, 0.00458694,\n",
       "        0.00923905, 0.01117855, 0.00767854, 0.00872697, 0.00411825,\n",
       "        0.00893085, 0.00705408, 0.0059127 , 0.00811419, 0.00976934,\n",
       "        0.0062482 , 0.00481664, 0.00424264, 0.00593296, 0.00961249,\n",
       "        0.00805978, 0.00782304, 0.00798499, 0.00545527, 0.00773046,\n",
       "        0.00847585, 0.00213542, 0.00830903, 0.00679412, 0.00488262,\n",
       "        0.00870862, 0.00922171, 0.01047664, 0.00711056, 0.00352136,\n",
       "        0.00773046, 0.00863944, 0.00991161, 0.0032619 , 0.01108152,\n",
       "        0.00665132, 0.00785875, 0.0051923 , 0.00386782, 0.01066958,\n",
       "        0.00487442, 0.0103846 , 0.1073007 , 0.01351444, 0.01228658,\n",
       "        0.15118942, 0.0118254 , 0.01108152, 0.15349267, 0.0109654 ,\n",
       "        0.00974474, 0.13155622, 0.01178813, 0.02001999, 0.15827394,\n",
       "        0.01180508, 0.0115343 , 0.13706728, 0.01254432, 0.01126765,\n",
       "        0.1223732 , 0.0108074 , 0.01161034, 0.14066556, 0.01235476,\n",
       "        0.01167904, 0.1476    , 0.01348481, 0.01235476, 0.12318214,\n",
       "        0.01352627, 0.01166876, 0.1114009 , 0.01072567, 0.01086278,\n",
       "        0.11815143, 0.01125344, 0.01053755, 0.11810064, 0.01881064,\n",
       "        0.01240967, 0.13892242, 0.0112712 , 0.01221475, 0.14548897,\n",
       "        0.01077033, 0.01259206, 0.0361054 , 0.01092886, 0.01416192,\n",
       "        0.08013139, 0.01364405, 0.00873155, 0.08187942, 0.01263962,\n",
       "        0.01585686, 0.06425449, 0.01219836, 0.00786384, 0.10637744,\n",
       "        0.01048046, 0.02495596, 0.12342512, 0.01677379, 0.00611882,\n",
       "        0.09855476, 0.01233856, 0.00800999, 0.05755901, 0.01445822,\n",
       "        0.01008762, 0.09664492, 0.01557691, 0.01157584, 0.07919747,\n",
       "        0.00877268, 0.01505191, 0.10536489, 0.0088227 , 0.01087382,\n",
       "        0.15840253, 0.01202664, 0.00923905, 0.10207723, 0.00842378,\n",
       "        0.013937  , 0.13031439, 0.01379855, 0.01923122, 0.09032918,\n",
       "        0.01057166, 0.0180333 , 0.10514866, 0.01301384, 0.01062826,\n",
       "        0.10302932, 0.0136    , 0.02408651, 0.00935949, 0.01565375,\n",
       "        0.00475815, 0.00781281, 0.00991161, 0.00553534, 0.00553534,\n",
       "        0.00531413, 0.00549909, 0.00511468, 0.00407922, 0.01550484,\n",
       "        0.01162755, 0.01253794, 0.0094361 , 0.00135647, 0.00484149,\n",
       "        0.0048    , 0.00665132, 0.00739189, 0.00508331, 0.0045607 ,\n",
       "        0.00447214, 0.01135958, 0.01483779, 0.01817251, 0.00185472,\n",
       "        0.00601997, 0.00636867, 0.00617738, 0.00401995, 0.00426146,\n",
       "        0.00252982, 0.00331059, 0.00430813, 0.01868261, 0.01407125,\n",
       "        0.0040694 , 0.005004  , 0.01607483, 0.0112712 , 0.00424264,\n",
       "        0.00331059, 0.0040694 , 0.00392938, 0.004996  , 0.0054626 ,\n",
       "        0.01164474, 0.00886341, 0.0031241 , 0.00666333, 0.0044    ,\n",
       "        0.00535164, 0.00513809, 0.00503587, 0.0032619 , 0.00515364,\n",
       "        0.00338231, 0.00241661, 0.00617738, 0.00531413, 0.00841427,\n",
       "        0.00545527, 0.00382623, 0.00923038, 0.00470744, 0.00549181,\n",
       "        0.00541849, 0.00576194, 0.00303315, 0.00309839, 0.01077775,\n",
       "        0.00749933, 0.0069685 , 0.00776144, 0.00436348, 0.00646838,\n",
       "        0.00394968, 0.00348712, 0.00484149, 0.00562139, 0.0076    ,\n",
       "        0.00523832, 0.00579655, 0.00832827, 0.00728286, 0.00661513,\n",
       "        0.00907744, 0.00526878, 0.00484149, 0.00688186, 0.00469042,\n",
       "        0.00372022, 0.00567803, 0.00278568]),\n",
       " 'rank_test_recall_micro': array([119, 115,  67,  15,  10, 110,  79,  64,  93, 128,  27, 108, 101,\n",
       "        103, 119,  42,  26,  21,  19,  34,  13,  30, 127,  60,  90,  79,\n",
       "         53,  73,  28,  50,   9, 103,  63, 130,  85,  37,  76,  38, 121,\n",
       "         30,   6,  23,   1,  76,   2,  23,  53,  14,  67,  40,  96,   6,\n",
       "         19,  79,  47,  29,  34,  58,  23, 123, 125,  53, 110, 106,  44,\n",
       "         36, 133,  85,   4,  52,  92,  21, 110,  51, 123,  18,  15,  38,\n",
       "         12,  64,   8, 106, 110, 101,  33,  42, 130,  47,  44,  41,  10,\n",
       "          5,  90,  15,  76,   3, 249, 286, 250, 244, 284, 229, 228, 273,\n",
       "        225, 232, 272, 227, 255, 266, 252, 239, 265, 237, 242, 287, 243,\n",
       "        226, 283, 238, 256, 285, 254, 247, 268, 251, 241, 264, 245, 233,\n",
       "        258, 246, 248, 259, 253, 233, 282, 229, 233, 276, 233, 231, 257,\n",
       "        239, 223, 279, 205, 216, 263, 213, 206, 267, 189, 204, 271, 198,\n",
       "        217, 280, 220, 200, 281, 215, 195, 288, 198, 191, 275, 212, 222,\n",
       "        260, 221, 218, 278, 203, 207, 277, 200, 208, 261, 208, 224, 274,\n",
       "        219, 197, 270, 214, 202, 262, 188, 211, 269, 210, 194, 180, 192,\n",
       "        146, 176, 179, 155, 155, 163, 152, 158, 146, 181, 181, 190, 173,\n",
       "        152, 146, 161, 162, 163, 146, 159, 143, 187, 195, 193, 170, 163,\n",
       "        178, 169, 144, 166, 168, 150, 142, 186, 185, 184, 174, 181, 177,\n",
       "        175, 171, 150, 152, 167, 155, 139, 160, 145, 115, 110,  93, 115,\n",
       "         72,  60,  79,  73,  60, 134, 137, 139,  98, 121, 115, 108,  79,\n",
       "         53,  70,  30,  85, 172, 137, 132, 129, 103,  95,  85,  53,  96,\n",
       "         85,  58,  73, 139, 135, 136, 125, 100,  98,  47,  67,  70,  44,\n",
       "         64,  79]),\n",
       " 'split0_test_f1_micro': array([0.835, 0.854, 0.857, 0.846, 0.851, 0.853, 0.845, 0.855, 0.847,\n",
       "        0.841, 0.861, 0.854, 0.846, 0.853, 0.831, 0.853, 0.853, 0.847,\n",
       "        0.847, 0.849, 0.847, 0.837, 0.842, 0.843, 0.848, 0.851, 0.85 ,\n",
       "        0.851, 0.85 , 0.851, 0.847, 0.844, 0.849, 0.843, 0.842, 0.845,\n",
       "        0.856, 0.848, 0.852, 0.855, 0.871, 0.847, 0.858, 0.841, 0.858,\n",
       "        0.843, 0.842, 0.847, 0.843, 0.849, 0.85 , 0.858, 0.85 , 0.853,\n",
       "        0.852, 0.843, 0.856, 0.848, 0.842, 0.847, 0.836, 0.846, 0.843,\n",
       "        0.841, 0.843, 0.849, 0.85 , 0.851, 0.855, 0.838, 0.846, 0.848,\n",
       "        0.838, 0.854, 0.848, 0.857, 0.856, 0.847, 0.861, 0.858, 0.847,\n",
       "        0.838, 0.849, 0.855, 0.856, 0.858, 0.833, 0.841, 0.85 , 0.836,\n",
       "        0.854, 0.863, 0.843, 0.855, 0.834, 0.858, 0.766, 0.352, 0.767,\n",
       "        0.762, 0.352, 0.769, 0.769, 0.352, 0.769, 0.771, 0.351, 0.769,\n",
       "        0.741, 0.716, 0.763, 0.764, 0.435, 0.766, 0.767, 0.352, 0.766,\n",
       "        0.769, 0.351, 0.767, 0.76 , 0.722, 0.757, 0.765, 0.65 , 0.76 ,\n",
       "        0.767, 0.352, 0.767, 0.769, 0.656, 0.769, 0.765, 0.352, 0.745,\n",
       "        0.769, 0.336, 0.771, 0.767, 0.315, 0.768, 0.765, 0.648, 0.765,\n",
       "        0.788, 0.546, 0.802, 0.799, 0.569, 0.798, 0.791, 0.51 , 0.799,\n",
       "        0.807, 0.419, 0.8  , 0.787, 0.419, 0.784, 0.804, 0.571, 0.79 ,\n",
       "        0.802, 0.39 , 0.821, 0.805, 0.624, 0.786, 0.784, 0.618, 0.792,\n",
       "        0.796, 0.542, 0.8  , 0.8  , 0.518, 0.796, 0.793, 0.664, 0.805,\n",
       "        0.779, 0.543, 0.805, 0.786, 0.612, 0.795, 0.787, 0.63 , 0.796,\n",
       "        0.802, 0.605, 0.8  , 0.784, 0.809, 0.798, 0.838, 0.822, 0.818,\n",
       "        0.836, 0.838, 0.837, 0.837, 0.834, 0.838, 0.801, 0.827, 0.81 ,\n",
       "        0.834, 0.843, 0.842, 0.836, 0.831, 0.831, 0.838, 0.837, 0.838,\n",
       "        0.821, 0.796, 0.786, 0.838, 0.83 , 0.823, 0.837, 0.842, 0.835,\n",
       "        0.838, 0.84 , 0.838, 0.79 , 0.826, 0.828, 0.838, 0.795, 0.841,\n",
       "        0.832, 0.836, 0.84 , 0.836, 0.836, 0.835, 0.831, 0.837, 0.843,\n",
       "        0.849, 0.846, 0.846, 0.851, 0.845, 0.851, 0.851, 0.849, 0.855,\n",
       "        0.839, 0.838, 0.831, 0.845, 0.845, 0.842, 0.848, 0.852, 0.851,\n",
       "        0.854, 0.857, 0.85 , 0.827, 0.834, 0.844, 0.847, 0.844, 0.846,\n",
       "        0.85 , 0.852, 0.853, 0.853, 0.85 , 0.853, 0.842, 0.837, 0.839,\n",
       "        0.846, 0.838, 0.849, 0.855, 0.847, 0.849, 0.851, 0.846, 0.849]),\n",
       " 'split1_test_f1_micro': array([0.857, 0.846, 0.85 , 0.853, 0.869, 0.843, 0.863, 0.85 , 0.864,\n",
       "        0.86 , 0.858, 0.846, 0.854, 0.848, 0.855, 0.856, 0.856, 0.861,\n",
       "        0.862, 0.861, 0.872, 0.859, 0.851, 0.858, 0.853, 0.852, 0.853,\n",
       "        0.843, 0.86 , 0.86 , 0.854, 0.854, 0.864, 0.855, 0.855, 0.865,\n",
       "        0.862, 0.853, 0.857, 0.862, 0.863, 0.859, 0.872, 0.861, 0.862,\n",
       "        0.86 , 0.856, 0.871, 0.86 , 0.858, 0.864, 0.859, 0.859, 0.855,\n",
       "        0.852, 0.866, 0.868, 0.855, 0.853, 0.857, 0.852, 0.862, 0.849,\n",
       "        0.858, 0.863, 0.86 , 0.852, 0.854, 0.863, 0.848, 0.842, 0.863,\n",
       "        0.849, 0.856, 0.842, 0.867, 0.859, 0.844, 0.865, 0.858, 0.856,\n",
       "        0.853, 0.859, 0.855, 0.862, 0.861, 0.858, 0.87 , 0.855, 0.867,\n",
       "        0.861, 0.873, 0.859, 0.863, 0.858, 0.865, 0.791, 0.353, 0.795,\n",
       "        0.794, 0.287, 0.795, 0.794, 0.647, 0.796, 0.791, 0.604, 0.795,\n",
       "        0.792, 0.353, 0.791, 0.793, 0.696, 0.794, 0.79 , 0.353, 0.792,\n",
       "        0.794, 0.647, 0.797, 0.767, 0.353, 0.793, 0.79 , 0.647, 0.792,\n",
       "        0.786, 0.662, 0.793, 0.795, 0.354, 0.795, 0.793, 0.647, 0.794,\n",
       "        0.796, 0.647, 0.791, 0.797, 0.353, 0.794, 0.795, 0.665, 0.792,\n",
       "        0.797, 0.485, 0.809, 0.8  , 0.456, 0.813, 0.815, 0.528, 0.829,\n",
       "        0.81 , 0.658, 0.831, 0.81 , 0.507, 0.811, 0.812, 0.361, 0.825,\n",
       "        0.818, 0.479, 0.828, 0.821, 0.541, 0.812, 0.813, 0.648, 0.807,\n",
       "        0.807, 0.549, 0.82 , 0.806, 0.299, 0.808, 0.818, 0.534, 0.822,\n",
       "        0.805, 0.385, 0.819, 0.813, 0.542, 0.806, 0.818, 0.538, 0.83 ,\n",
       "        0.822, 0.461, 0.807, 0.833, 0.834, 0.838, 0.843, 0.836, 0.844,\n",
       "        0.839, 0.842, 0.838, 0.84 , 0.841, 0.839, 0.842, 0.83 , 0.834,\n",
       "        0.84 , 0.842, 0.84 , 0.842, 0.838, 0.838, 0.839, 0.839, 0.844,\n",
       "        0.808, 0.824, 0.822, 0.84 , 0.841, 0.838, 0.84 , 0.845, 0.838,\n",
       "        0.838, 0.842, 0.839, 0.833, 0.837, 0.824, 0.842, 0.829, 0.839,\n",
       "        0.839, 0.84 , 0.837, 0.84 , 0.838, 0.841, 0.847, 0.848, 0.846,\n",
       "        0.858, 0.854, 0.859, 0.853, 0.86 , 0.858, 0.86 , 0.858, 0.857,\n",
       "        0.848, 0.843, 0.85 , 0.859, 0.852, 0.865, 0.853, 0.855, 0.862,\n",
       "        0.861, 0.86 , 0.855, 0.845, 0.847, 0.854, 0.854, 0.857, 0.859,\n",
       "        0.859, 0.86 , 0.858, 0.862, 0.865, 0.86 , 0.844, 0.849, 0.846,\n",
       "        0.857, 0.858, 0.856, 0.86 , 0.863, 0.858, 0.858, 0.859, 0.855]),\n",
       " 'split2_test_f1_micro': array([0.838, 0.85 , 0.857, 0.868, 0.856, 0.846, 0.848, 0.853, 0.855,\n",
       "        0.845, 0.857, 0.852, 0.838, 0.849, 0.859, 0.846, 0.86 , 0.853,\n",
       "        0.858, 0.857, 0.855, 0.852, 0.837, 0.847, 0.843, 0.847, 0.853,\n",
       "        0.846, 0.862, 0.859, 0.871, 0.858, 0.854, 0.852, 0.858, 0.856,\n",
       "        0.847, 0.858, 0.84 , 0.855, 0.851, 0.864, 0.86 , 0.845, 0.86 ,\n",
       "        0.862, 0.86 , 0.845, 0.854, 0.851, 0.833, 0.859, 0.859, 0.845,\n",
       "        0.85 , 0.855, 0.868, 0.843, 0.859, 0.848, 0.843, 0.86 , 0.85 ,\n",
       "        0.85 , 0.848, 0.855, 0.848, 0.845, 0.855, 0.863, 0.85 , 0.854,\n",
       "        0.849, 0.845, 0.842, 0.865, 0.858, 0.859, 0.846, 0.856, 0.856,\n",
       "        0.849, 0.833, 0.851, 0.853, 0.862, 0.852, 0.857, 0.853, 0.855,\n",
       "        0.865, 0.861, 0.851, 0.86 , 0.863, 0.865, 0.768, 0.353, 0.762,\n",
       "        0.773, 0.647, 0.769, 0.772, 0.647, 0.776, 0.772, 0.645, 0.771,\n",
       "        0.767, 0.654, 0.769, 0.774, 0.632, 0.769, 0.77 , 0.353, 0.772,\n",
       "        0.773, 0.386, 0.769, 0.771, 0.353, 0.768, 0.766, 0.343, 0.767,\n",
       "        0.77 , 0.578, 0.771, 0.771, 0.647, 0.767, 0.771, 0.647, 0.771,\n",
       "        0.767, 0.352, 0.769, 0.77 , 0.647, 0.772, 0.771, 0.647, 0.774,\n",
       "        0.785, 0.594, 0.832, 0.813, 0.568, 0.791, 0.798, 0.468, 0.821,\n",
       "        0.807, 0.541, 0.811, 0.771, 0.366, 0.798, 0.822, 0.628, 0.817,\n",
       "        0.822, 0.362, 0.794, 0.826, 0.439, 0.803, 0.801, 0.649, 0.804,\n",
       "        0.787, 0.524, 0.815, 0.802, 0.399, 0.82 , 0.816, 0.559, 0.804,\n",
       "        0.791, 0.652, 0.78 , 0.806, 0.518, 0.822, 0.833, 0.682, 0.829,\n",
       "        0.794, 0.648, 0.797, 0.793, 0.831, 0.817, 0.836, 0.838, 0.826,\n",
       "        0.839, 0.833, 0.835, 0.835, 0.838, 0.841, 0.827, 0.823, 0.804,\n",
       "        0.823, 0.839, 0.835, 0.834, 0.839, 0.835, 0.838, 0.837, 0.84 ,\n",
       "        0.816, 0.811, 0.832, 0.839, 0.839, 0.84 , 0.831, 0.837, 0.838,\n",
       "        0.838, 0.837, 0.843, 0.825, 0.805, 0.819, 0.829, 0.83 , 0.813,\n",
       "        0.836, 0.835, 0.84 , 0.84 , 0.835, 0.836, 0.834, 0.825, 0.837,\n",
       "        0.838, 0.844, 0.847, 0.847, 0.852, 0.849, 0.846, 0.849, 0.85 ,\n",
       "        0.839, 0.845, 0.837, 0.845, 0.845, 0.839, 0.845, 0.847, 0.851,\n",
       "        0.847, 0.858, 0.847, 0.826, 0.839, 0.836, 0.833, 0.848, 0.842,\n",
       "        0.852, 0.85 , 0.843, 0.845, 0.844, 0.845, 0.836, 0.835, 0.834,\n",
       "        0.838, 0.842, 0.842, 0.846, 0.847, 0.846, 0.851, 0.851, 0.851]),\n",
       " 'split3_test_f1_micro': array([0.854, 0.841, 0.844, 0.859, 0.861, 0.853, 0.85 , 0.858, 0.842,\n",
       "        0.853, 0.844, 0.853, 0.853, 0.849, 0.843, 0.863, 0.851, 0.862,\n",
       "        0.853, 0.847, 0.854, 0.868, 0.865, 0.859, 0.853, 0.851, 0.852,\n",
       "        0.856, 0.858, 0.844, 0.863, 0.845, 0.843, 0.846, 0.862, 0.853,\n",
       "        0.847, 0.857, 0.84 , 0.851, 0.846, 0.854, 0.868, 0.857, 0.874,\n",
       "        0.857, 0.85 , 0.86 , 0.852, 0.849, 0.852, 0.862, 0.857, 0.852,\n",
       "        0.855, 0.867, 0.847, 0.865, 0.867, 0.845, 0.862, 0.845, 0.848,\n",
       "        0.861, 0.85 , 0.865, 0.838, 0.858, 0.861, 0.859, 0.865, 0.851,\n",
       "        0.852, 0.854, 0.851, 0.856, 0.861, 0.863, 0.856, 0.848, 0.866,\n",
       "        0.866, 0.847, 0.856, 0.852, 0.852, 0.851, 0.857, 0.855, 0.852,\n",
       "        0.848, 0.851, 0.854, 0.859, 0.847, 0.855, 0.788, 0.621, 0.779,\n",
       "        0.789, 0.353, 0.791, 0.792, 0.317, 0.793, 0.788, 0.381, 0.79 ,\n",
       "        0.785, 0.647, 0.778, 0.788, 0.342, 0.792, 0.793, 0.315, 0.79 ,\n",
       "        0.792, 0.648, 0.787, 0.791, 0.353, 0.775, 0.79 , 0.435, 0.783,\n",
       "        0.792, 0.535, 0.785, 0.788, 0.647, 0.788, 0.789, 0.648, 0.789,\n",
       "        0.79 , 0.647, 0.789, 0.788, 0.647, 0.79 , 0.793, 0.68 , 0.791,\n",
       "        0.812, 0.497, 0.796, 0.804, 0.52 , 0.822, 0.83 , 0.547, 0.835,\n",
       "        0.815, 0.625, 0.813, 0.835, 0.719, 0.795, 0.816, 0.47 , 0.8  ,\n",
       "        0.814, 0.301, 0.803, 0.811, 0.59 , 0.813, 0.8  , 0.436, 0.798,\n",
       "        0.813, 0.304, 0.808, 0.825, 0.769, 0.816, 0.813, 0.415, 0.804,\n",
       "        0.808, 0.67 , 0.811, 0.836, 0.613, 0.801, 0.8  , 0.377, 0.825,\n",
       "        0.811, 0.577, 0.815, 0.831, 0.827, 0.805, 0.848, 0.841, 0.842,\n",
       "        0.841, 0.845, 0.842, 0.847, 0.844, 0.845, 0.818, 0.807, 0.835,\n",
       "        0.847, 0.842, 0.846, 0.845, 0.844, 0.848, 0.847, 0.843, 0.842,\n",
       "        0.838, 0.81 , 0.814, 0.838, 0.844, 0.835, 0.841, 0.841, 0.843,\n",
       "        0.842, 0.847, 0.847, 0.829, 0.814, 0.824, 0.84 , 0.835, 0.841,\n",
       "        0.838, 0.841, 0.843, 0.845, 0.844, 0.846, 0.864, 0.847, 0.843,\n",
       "        0.853, 0.852, 0.857, 0.857, 0.856, 0.856, 0.848, 0.854, 0.852,\n",
       "        0.851, 0.854, 0.85 , 0.854, 0.852, 0.854, 0.858, 0.861, 0.858,\n",
       "        0.857, 0.853, 0.853, 0.845, 0.856, 0.855, 0.85 , 0.852, 0.853,\n",
       "        0.852, 0.855, 0.852, 0.85 , 0.86 , 0.856, 0.854, 0.852, 0.855,\n",
       "        0.852, 0.858, 0.856, 0.858, 0.86 , 0.855, 0.86 , 0.861, 0.85 ]),\n",
       " 'split4_test_f1_micro': array([0.865, 0.859, 0.858, 0.863, 0.856, 0.856, 0.855, 0.851, 0.85 ,\n",
       "        0.842, 0.862, 0.846, 0.863, 0.854, 0.861, 0.856, 0.862, 0.862,\n",
       "        0.867, 0.865, 0.862, 0.864, 0.848, 0.861, 0.862, 0.86 , 0.861,\n",
       "        0.867, 0.851, 0.858, 0.86 , 0.852, 0.857, 0.841, 0.843, 0.859,\n",
       "        0.85 , 0.861, 0.859, 0.857, 0.867, 0.86 , 0.858, 0.858, 0.859,\n",
       "        0.862, 0.861, 0.867, 0.857, 0.868, 0.858, 0.86 , 0.862, 0.856,\n",
       "        0.863, 0.85 , 0.84 , 0.858, 0.863, 0.849, 0.853, 0.856, 0.861,\n",
       "        0.842, 0.869, 0.849, 0.847, 0.852, 0.871, 0.862, 0.856, 0.869,\n",
       "        0.863, 0.862, 0.863, 0.843, 0.855, 0.864, 0.863, 0.847, 0.872,\n",
       "        0.846, 0.863, 0.837, 0.857, 0.841, 0.843, 0.847, 0.86 , 0.865,\n",
       "        0.865, 0.853, 0.852, 0.852, 0.86 , 0.868, 0.784, 0.353, 0.794,\n",
       "        0.791, 0.623, 0.793, 0.793, 0.647, 0.794, 0.794, 0.647, 0.796,\n",
       "        0.795, 0.353, 0.793, 0.793, 0.647, 0.793, 0.791, 0.647, 0.79 ,\n",
       "        0.797, 0.348, 0.793, 0.786, 0.353, 0.79 , 0.792, 0.592, 0.794,\n",
       "        0.796, 0.647, 0.792, 0.792, 0.647, 0.788, 0.78 , 0.647, 0.794,\n",
       "        0.793, 0.42 , 0.797, 0.793, 0.585, 0.791, 0.792, 0.575, 0.79 ,\n",
       "        0.822, 0.355, 0.825, 0.822, 0.705, 0.822, 0.829, 0.66 , 0.821,\n",
       "        0.828, 0.393, 0.822, 0.832, 0.435, 0.833, 0.818, 0.413, 0.81 ,\n",
       "        0.825, 0.396, 0.831, 0.833, 0.364, 0.834, 0.817, 0.581, 0.818,\n",
       "        0.831, 0.612, 0.825, 0.824, 0.549, 0.832, 0.816, 0.704, 0.821,\n",
       "        0.819, 0.358, 0.815, 0.837, 0.366, 0.82 , 0.832, 0.601, 0.829,\n",
       "        0.82 , 0.367, 0.835, 0.845, 0.834, 0.834, 0.847, 0.845, 0.837,\n",
       "        0.852, 0.849, 0.85 , 0.849, 0.849, 0.849, 0.842, 0.843, 0.817,\n",
       "        0.849, 0.842, 0.849, 0.846, 0.851, 0.85 , 0.85 , 0.849, 0.851,\n",
       "        0.835, 0.84 , 0.838, 0.843, 0.848, 0.84 , 0.85 , 0.849, 0.847,\n",
       "        0.844, 0.843, 0.849, 0.846, 0.843, 0.831, 0.843, 0.841, 0.843,\n",
       "        0.845, 0.844, 0.849, 0.847, 0.848, 0.849, 0.844, 0.847, 0.845,\n",
       "        0.852, 0.855, 0.849, 0.842, 0.851, 0.854, 0.856, 0.853, 0.854,\n",
       "        0.854, 0.842, 0.852, 0.853, 0.854, 0.85 , 0.847, 0.846, 0.847,\n",
       "        0.846, 0.852, 0.855, 0.853, 0.846, 0.847, 0.854, 0.852, 0.857,\n",
       "        0.847, 0.852, 0.851, 0.85 , 0.85 , 0.849, 0.844, 0.856, 0.848,\n",
       "        0.853, 0.859, 0.853, 0.853, 0.849, 0.857, 0.853, 0.85 , 0.856]),\n",
       " 'mean_test_f1_micro': array([0.8498, 0.85  , 0.8532, 0.8578, 0.8586, 0.8502, 0.8522, 0.8534,\n",
       "        0.8516, 0.8482, 0.8564, 0.8502, 0.8508, 0.8506, 0.8498, 0.8548,\n",
       "        0.8564, 0.857 , 0.8574, 0.8558, 0.858 , 0.856 , 0.8486, 0.8536,\n",
       "        0.8518, 0.8522, 0.8538, 0.8526, 0.8562, 0.8544, 0.859 , 0.8506,\n",
       "        0.8534, 0.8474, 0.852 , 0.8556, 0.8524, 0.8554, 0.8496, 0.856 ,\n",
       "        0.8596, 0.8568, 0.8632, 0.8524, 0.8626, 0.8568, 0.8538, 0.858 ,\n",
       "        0.8532, 0.855 , 0.8514, 0.8596, 0.8574, 0.8522, 0.8544, 0.8562,\n",
       "        0.8558, 0.8538, 0.8568, 0.8492, 0.8492, 0.8538, 0.8502, 0.8504,\n",
       "        0.8546, 0.8556, 0.847 , 0.852 , 0.861 , 0.854 , 0.8518, 0.857 ,\n",
       "        0.8502, 0.8542, 0.8492, 0.8576, 0.8578, 0.8554, 0.8582, 0.8534,\n",
       "        0.8594, 0.8504, 0.8502, 0.8508, 0.856 , 0.8548, 0.8474, 0.8544,\n",
       "        0.8546, 0.855 , 0.8586, 0.8602, 0.8518, 0.8578, 0.8524, 0.8622,\n",
       "        0.7794, 0.4064, 0.7794, 0.7818, 0.4524, 0.7834, 0.784 , 0.522 ,\n",
       "        0.7856, 0.7832, 0.5256, 0.7842, 0.776 , 0.5446, 0.7788, 0.7824,\n",
       "        0.5504, 0.7828, 0.7822, 0.404 , 0.782 , 0.785 , 0.476 , 0.7826,\n",
       "        0.775 , 0.4268, 0.7766, 0.7806, 0.5334, 0.7792, 0.7822, 0.5548,\n",
       "        0.7816, 0.783 , 0.5902, 0.7814, 0.7796, 0.5882, 0.7786, 0.783 ,\n",
       "        0.4804, 0.7834, 0.783 , 0.5094, 0.783 , 0.7832, 0.643 , 0.7824,\n",
       "        0.8008, 0.4954, 0.8128, 0.8076, 0.5636, 0.8092, 0.8126, 0.5426,\n",
       "        0.821 , 0.8134, 0.5272, 0.8154, 0.807 , 0.4892, 0.8042, 0.8144,\n",
       "        0.4886, 0.8084, 0.8162, 0.3856, 0.8154, 0.8192, 0.5116, 0.8096,\n",
       "        0.803 , 0.5864, 0.8038, 0.8068, 0.5062, 0.8136, 0.8114, 0.5068,\n",
       "        0.8144, 0.8112, 0.5752, 0.8112, 0.8004, 0.5216, 0.806 , 0.8156,\n",
       "        0.5302, 0.8088, 0.814 , 0.5656, 0.8218, 0.8098, 0.5316, 0.8108,\n",
       "        0.8172, 0.827 , 0.8184, 0.8424, 0.8364, 0.8334, 0.8414, 0.8414,\n",
       "        0.8404, 0.8416, 0.8412, 0.8424, 0.826 , 0.826 , 0.82  , 0.8386,\n",
       "        0.8416, 0.8424, 0.8406, 0.8406, 0.8404, 0.8424, 0.841 , 0.843 ,\n",
       "        0.8236, 0.8162, 0.8184, 0.8396, 0.8404, 0.8352, 0.8398, 0.8428,\n",
       "        0.8402, 0.84  , 0.8418, 0.8432, 0.8246, 0.825 , 0.8252, 0.8384,\n",
       "        0.826 , 0.8354, 0.838 , 0.8392, 0.8418, 0.8416, 0.8402, 0.8414,\n",
       "        0.844 , 0.8408, 0.8428, 0.85  , 0.8502, 0.8516, 0.85  , 0.8528,\n",
       "        0.8536, 0.8522, 0.8526, 0.8536, 0.8462, 0.8444, 0.844 , 0.8512,\n",
       "        0.8496, 0.85  , 0.8502, 0.8522, 0.8538, 0.853 , 0.856 , 0.852 ,\n",
       "        0.8392, 0.8444, 0.8472, 0.8476, 0.8506, 0.8514, 0.852 , 0.8538,\n",
       "        0.8514, 0.852 , 0.8538, 0.8526, 0.844 , 0.8458, 0.8444, 0.8492,\n",
       "        0.851 , 0.8512, 0.8544, 0.8532, 0.853 , 0.8546, 0.8534, 0.8522]),\n",
       " 'std_test_f1_micro': array([0.01147868, 0.00622896, 0.00541849, 0.00767854, 0.00608605,\n",
       "        0.00487442, 0.00630555, 0.00287054, 0.00749933, 0.00724983,\n",
       "        0.00646838, 0.00348712, 0.00837616, 0.00241661, 0.01128539,\n",
       "        0.00549181, 0.00412795, 0.00603324, 0.0069455 , 0.00688186,\n",
       "        0.00846168, 0.01089954, 0.0095205 , 0.0072    , 0.00630555,\n",
       "        0.00426146, 0.00376298, 0.00845222, 0.00483322, 0.00608605,\n",
       "        0.00812404, 0.00535164, 0.00711618, 0.00531413, 0.00807465,\n",
       "        0.00662118, 0.00581722, 0.00449889, 0.00816333, 0.00357771,\n",
       "        0.00954149, 0.00584466, 0.00574108, 0.00788923, 0.0058515 ,\n",
       "        0.00713863, 0.00705408, 0.01043072, 0.00577581, 0.00729383,\n",
       "        0.01042305, 0.00135647, 0.00402989, 0.00386782, 0.00458694,\n",
       "        0.00923905, 0.01117855, 0.00767854, 0.00872697, 0.00411825,\n",
       "        0.00893085, 0.00705408, 0.0059127 , 0.00811419, 0.00976934,\n",
       "        0.0062482 , 0.00481664, 0.00424264, 0.00593296, 0.00961249,\n",
       "        0.00805978, 0.00782304, 0.00798499, 0.00545527, 0.00773046,\n",
       "        0.00847585, 0.00213542, 0.00830903, 0.00679412, 0.00488262,\n",
       "        0.00870862, 0.00922171, 0.01047664, 0.00711056, 0.00352136,\n",
       "        0.00773046, 0.00863944, 0.00991161, 0.0032619 , 0.01108152,\n",
       "        0.00665132, 0.00785875, 0.0051923 , 0.00386782, 0.01066958,\n",
       "        0.00487442, 0.0103846 , 0.1073007 , 0.01351444, 0.01228658,\n",
       "        0.15118942, 0.0118254 , 0.01108152, 0.15349267, 0.0109654 ,\n",
       "        0.00974474, 0.13155622, 0.01178813, 0.02001999, 0.15827394,\n",
       "        0.01180508, 0.0115343 , 0.13706728, 0.01254432, 0.01126765,\n",
       "        0.1223732 , 0.0108074 , 0.01161034, 0.14066556, 0.01235476,\n",
       "        0.01167904, 0.1476    , 0.01348481, 0.01235476, 0.12318214,\n",
       "        0.01352627, 0.01166876, 0.1114009 , 0.01072567, 0.01086278,\n",
       "        0.11815143, 0.01125344, 0.01053755, 0.11810064, 0.01881064,\n",
       "        0.01240967, 0.13892242, 0.0112712 , 0.01221475, 0.14548897,\n",
       "        0.01077033, 0.01259206, 0.0361054 , 0.01092886, 0.01416192,\n",
       "        0.08013139, 0.01364405, 0.00873155, 0.08187942, 0.01263962,\n",
       "        0.01585686, 0.06425449, 0.01219836, 0.00786384, 0.10637744,\n",
       "        0.01048046, 0.02495596, 0.12342512, 0.01677379, 0.00611882,\n",
       "        0.09855476, 0.01233856, 0.00800999, 0.05755901, 0.01445822,\n",
       "        0.01008762, 0.09664492, 0.01557691, 0.01157584, 0.07919747,\n",
       "        0.00877268, 0.01505191, 0.10536489, 0.0088227 , 0.01087382,\n",
       "        0.15840253, 0.01202664, 0.00923905, 0.10207723, 0.00842378,\n",
       "        0.013937  , 0.13031439, 0.01379855, 0.01923122, 0.09032918,\n",
       "        0.01057166, 0.0180333 , 0.10514866, 0.01301384, 0.01062826,\n",
       "        0.10302932, 0.0136    , 0.02408651, 0.00935949, 0.01565375,\n",
       "        0.00475815, 0.00781281, 0.00991161, 0.00553534, 0.00553534,\n",
       "        0.00531413, 0.00549909, 0.00511468, 0.00407922, 0.01550484,\n",
       "        0.01162755, 0.01253794, 0.0094361 , 0.00135647, 0.00484149,\n",
       "        0.0048    , 0.00665132, 0.00739189, 0.00508331, 0.0045607 ,\n",
       "        0.00447214, 0.01135958, 0.01483779, 0.01817251, 0.00185472,\n",
       "        0.00601997, 0.00636867, 0.00617738, 0.00401995, 0.00426146,\n",
       "        0.00252982, 0.00331059, 0.00430813, 0.01868261, 0.01407125,\n",
       "        0.0040694 , 0.005004  , 0.01607483, 0.0112712 , 0.00424264,\n",
       "        0.00331059, 0.0040694 , 0.00392938, 0.004996  , 0.0054626 ,\n",
       "        0.01164474, 0.00886341, 0.0031241 , 0.00666333, 0.0044    ,\n",
       "        0.00535164, 0.00513809, 0.00503587, 0.0032619 , 0.00515364,\n",
       "        0.00338231, 0.00241661, 0.00617738, 0.00531413, 0.00841427,\n",
       "        0.00545527, 0.00382623, 0.00923038, 0.00470744, 0.00549181,\n",
       "        0.00541849, 0.00576194, 0.00303315, 0.00309839, 0.01077775,\n",
       "        0.00749933, 0.0069685 , 0.00776144, 0.00436348, 0.00646838,\n",
       "        0.00394968, 0.00348712, 0.00484149, 0.00562139, 0.0076    ,\n",
       "        0.00523832, 0.00579655, 0.00832827, 0.00728286, 0.00661513,\n",
       "        0.00907744, 0.00526878, 0.00484149, 0.00688186, 0.00469042,\n",
       "        0.00372022, 0.00567803, 0.00278568]),\n",
       " 'rank_test_f1_micro': array([119, 115,  67,  15,  10, 109,  79,  64,  93, 128,  27, 109, 101,\n",
       "        103, 119,  42,  26,  21,  19,  34,  13,  30, 127,  60,  90,  83,\n",
       "         53,  73,  28,  50,   9, 103,  63, 130,  85,  37,  76,  38, 121,\n",
       "         30,   6,  23,   1,  76,   2,  23,  53,  14,  67,  40,  95,   6,\n",
       "         19,  79,  47,  29,  34,  57,  23, 123, 125,  57, 109, 106,  44,\n",
       "         36, 133,  85,   4,  52,  92,  21, 109,  51, 123,  18,  15,  38,\n",
       "         12,  64,   8, 106, 109, 101,  33,  43, 130,  47,  44,  41,  10,\n",
       "          5,  90,  15,  76,   3, 249, 286, 250, 244, 284, 229, 228, 273,\n",
       "        225, 232, 272, 227, 255, 266, 252, 239, 265, 237, 242, 287, 243,\n",
       "        226, 283, 238, 256, 285, 254, 247, 268, 251, 241, 264, 245, 234,\n",
       "        258, 246, 248, 259, 253, 234, 282, 229, 234, 276, 233, 231, 257,\n",
       "        239, 223, 279, 205, 216, 263, 213, 206, 267, 189, 204, 271, 198,\n",
       "        217, 280, 220, 200, 281, 215, 195, 288, 198, 191, 275, 212, 222,\n",
       "        260, 221, 218, 278, 203, 207, 277, 200, 208, 261, 208, 224, 274,\n",
       "        219, 197, 270, 214, 202, 262, 188, 211, 269, 210, 194, 180, 192,\n",
       "        146, 176, 179, 155, 155, 163, 152, 158, 146, 181, 181, 190, 173,\n",
       "        152, 146, 161, 162, 163, 146, 159, 143, 187, 196, 193, 170, 163,\n",
       "        178, 169, 144, 166, 168, 150, 142, 186, 185, 184, 174, 181, 177,\n",
       "        175, 171, 150, 152, 167, 155, 140, 160, 145, 115, 109,  93, 115,\n",
       "         72,  60,  83,  73,  60, 134, 136, 139,  98, 121, 115, 108,  79,\n",
       "         53,  70,  30,  85, 171, 136, 132, 129, 103,  95,  85,  53,  95,\n",
       "         85,  57,  73, 140, 135, 136, 125, 100,  98,  47,  67,  70,  44,\n",
       "         64,  79]),\n",
       " 'split0_test_precision_micro': array([0.835, 0.854, 0.857, 0.846, 0.851, 0.853, 0.845, 0.855, 0.847,\n",
       "        0.841, 0.861, 0.854, 0.846, 0.853, 0.831, 0.853, 0.853, 0.847,\n",
       "        0.847, 0.849, 0.847, 0.837, 0.842, 0.843, 0.848, 0.851, 0.85 ,\n",
       "        0.851, 0.85 , 0.851, 0.847, 0.844, 0.849, 0.843, 0.842, 0.845,\n",
       "        0.856, 0.848, 0.852, 0.855, 0.871, 0.847, 0.858, 0.841, 0.858,\n",
       "        0.843, 0.842, 0.847, 0.843, 0.849, 0.85 , 0.858, 0.85 , 0.853,\n",
       "        0.852, 0.843, 0.856, 0.848, 0.842, 0.847, 0.836, 0.846, 0.843,\n",
       "        0.841, 0.843, 0.849, 0.85 , 0.851, 0.855, 0.838, 0.846, 0.848,\n",
       "        0.838, 0.854, 0.848, 0.857, 0.856, 0.847, 0.861, 0.858, 0.847,\n",
       "        0.838, 0.849, 0.855, 0.856, 0.858, 0.833, 0.841, 0.85 , 0.836,\n",
       "        0.854, 0.863, 0.843, 0.855, 0.834, 0.858, 0.766, 0.352, 0.767,\n",
       "        0.762, 0.352, 0.769, 0.769, 0.352, 0.769, 0.771, 0.351, 0.769,\n",
       "        0.741, 0.716, 0.763, 0.764, 0.435, 0.766, 0.767, 0.352, 0.766,\n",
       "        0.769, 0.351, 0.767, 0.76 , 0.722, 0.757, 0.765, 0.65 , 0.76 ,\n",
       "        0.767, 0.352, 0.767, 0.769, 0.656, 0.769, 0.765, 0.352, 0.745,\n",
       "        0.769, 0.336, 0.771, 0.767, 0.315, 0.768, 0.765, 0.648, 0.765,\n",
       "        0.788, 0.546, 0.802, 0.799, 0.569, 0.798, 0.791, 0.51 , 0.799,\n",
       "        0.807, 0.419, 0.8  , 0.787, 0.419, 0.784, 0.804, 0.571, 0.79 ,\n",
       "        0.802, 0.39 , 0.821, 0.805, 0.624, 0.786, 0.784, 0.618, 0.792,\n",
       "        0.796, 0.542, 0.8  , 0.8  , 0.518, 0.796, 0.793, 0.664, 0.805,\n",
       "        0.779, 0.543, 0.805, 0.786, 0.612, 0.795, 0.787, 0.63 , 0.796,\n",
       "        0.802, 0.605, 0.8  , 0.784, 0.809, 0.798, 0.838, 0.822, 0.818,\n",
       "        0.836, 0.838, 0.837, 0.837, 0.834, 0.838, 0.801, 0.827, 0.81 ,\n",
       "        0.834, 0.843, 0.842, 0.836, 0.831, 0.831, 0.838, 0.837, 0.838,\n",
       "        0.821, 0.796, 0.786, 0.838, 0.83 , 0.823, 0.837, 0.842, 0.835,\n",
       "        0.838, 0.84 , 0.838, 0.79 , 0.826, 0.828, 0.838, 0.795, 0.841,\n",
       "        0.832, 0.836, 0.84 , 0.836, 0.836, 0.835, 0.831, 0.837, 0.843,\n",
       "        0.849, 0.846, 0.846, 0.851, 0.845, 0.851, 0.851, 0.849, 0.855,\n",
       "        0.839, 0.838, 0.831, 0.845, 0.845, 0.842, 0.848, 0.852, 0.851,\n",
       "        0.854, 0.857, 0.85 , 0.827, 0.834, 0.844, 0.847, 0.844, 0.846,\n",
       "        0.85 , 0.852, 0.853, 0.853, 0.85 , 0.853, 0.842, 0.837, 0.839,\n",
       "        0.846, 0.838, 0.849, 0.855, 0.847, 0.849, 0.851, 0.846, 0.849]),\n",
       " 'split1_test_precision_micro': array([0.857, 0.846, 0.85 , 0.853, 0.869, 0.843, 0.863, 0.85 , 0.864,\n",
       "        0.86 , 0.858, 0.846, 0.854, 0.848, 0.855, 0.856, 0.856, 0.861,\n",
       "        0.862, 0.861, 0.872, 0.859, 0.851, 0.858, 0.853, 0.852, 0.853,\n",
       "        0.843, 0.86 , 0.86 , 0.854, 0.854, 0.864, 0.855, 0.855, 0.865,\n",
       "        0.862, 0.853, 0.857, 0.862, 0.863, 0.859, 0.872, 0.861, 0.862,\n",
       "        0.86 , 0.856, 0.871, 0.86 , 0.858, 0.864, 0.859, 0.859, 0.855,\n",
       "        0.852, 0.866, 0.868, 0.855, 0.853, 0.857, 0.852, 0.862, 0.849,\n",
       "        0.858, 0.863, 0.86 , 0.852, 0.854, 0.863, 0.848, 0.842, 0.863,\n",
       "        0.849, 0.856, 0.842, 0.867, 0.859, 0.844, 0.865, 0.858, 0.856,\n",
       "        0.853, 0.859, 0.855, 0.862, 0.861, 0.858, 0.87 , 0.855, 0.867,\n",
       "        0.861, 0.873, 0.859, 0.863, 0.858, 0.865, 0.791, 0.353, 0.795,\n",
       "        0.794, 0.287, 0.795, 0.794, 0.647, 0.796, 0.791, 0.604, 0.795,\n",
       "        0.792, 0.353, 0.791, 0.793, 0.696, 0.794, 0.79 , 0.353, 0.792,\n",
       "        0.794, 0.647, 0.797, 0.767, 0.353, 0.793, 0.79 , 0.647, 0.792,\n",
       "        0.786, 0.662, 0.793, 0.795, 0.354, 0.795, 0.793, 0.647, 0.794,\n",
       "        0.796, 0.647, 0.791, 0.797, 0.353, 0.794, 0.795, 0.665, 0.792,\n",
       "        0.797, 0.485, 0.809, 0.8  , 0.456, 0.813, 0.815, 0.528, 0.829,\n",
       "        0.81 , 0.658, 0.831, 0.81 , 0.507, 0.811, 0.812, 0.361, 0.825,\n",
       "        0.818, 0.479, 0.828, 0.821, 0.541, 0.812, 0.813, 0.648, 0.807,\n",
       "        0.807, 0.549, 0.82 , 0.806, 0.299, 0.808, 0.818, 0.534, 0.822,\n",
       "        0.805, 0.385, 0.819, 0.813, 0.542, 0.806, 0.818, 0.538, 0.83 ,\n",
       "        0.822, 0.461, 0.807, 0.833, 0.834, 0.838, 0.843, 0.836, 0.844,\n",
       "        0.839, 0.842, 0.838, 0.84 , 0.841, 0.839, 0.842, 0.83 , 0.834,\n",
       "        0.84 , 0.842, 0.84 , 0.842, 0.838, 0.838, 0.839, 0.839, 0.844,\n",
       "        0.808, 0.824, 0.822, 0.84 , 0.841, 0.838, 0.84 , 0.845, 0.838,\n",
       "        0.838, 0.842, 0.839, 0.833, 0.837, 0.824, 0.842, 0.829, 0.839,\n",
       "        0.839, 0.84 , 0.837, 0.84 , 0.838, 0.841, 0.847, 0.848, 0.846,\n",
       "        0.858, 0.854, 0.859, 0.853, 0.86 , 0.858, 0.86 , 0.858, 0.857,\n",
       "        0.848, 0.843, 0.85 , 0.859, 0.852, 0.865, 0.853, 0.855, 0.862,\n",
       "        0.861, 0.86 , 0.855, 0.845, 0.847, 0.854, 0.854, 0.857, 0.859,\n",
       "        0.859, 0.86 , 0.858, 0.862, 0.865, 0.86 , 0.844, 0.849, 0.846,\n",
       "        0.857, 0.858, 0.856, 0.86 , 0.863, 0.858, 0.858, 0.859, 0.855]),\n",
       " 'split2_test_precision_micro': array([0.838, 0.85 , 0.857, 0.868, 0.856, 0.846, 0.848, 0.853, 0.855,\n",
       "        0.845, 0.857, 0.852, 0.838, 0.849, 0.859, 0.846, 0.86 , 0.853,\n",
       "        0.858, 0.857, 0.855, 0.852, 0.837, 0.847, 0.843, 0.847, 0.853,\n",
       "        0.846, 0.862, 0.859, 0.871, 0.858, 0.854, 0.852, 0.858, 0.856,\n",
       "        0.847, 0.858, 0.84 , 0.855, 0.851, 0.864, 0.86 , 0.845, 0.86 ,\n",
       "        0.862, 0.86 , 0.845, 0.854, 0.851, 0.833, 0.859, 0.859, 0.845,\n",
       "        0.85 , 0.855, 0.868, 0.843, 0.859, 0.848, 0.843, 0.86 , 0.85 ,\n",
       "        0.85 , 0.848, 0.855, 0.848, 0.845, 0.855, 0.863, 0.85 , 0.854,\n",
       "        0.849, 0.845, 0.842, 0.865, 0.858, 0.859, 0.846, 0.856, 0.856,\n",
       "        0.849, 0.833, 0.851, 0.853, 0.862, 0.852, 0.857, 0.853, 0.855,\n",
       "        0.865, 0.861, 0.851, 0.86 , 0.863, 0.865, 0.768, 0.353, 0.762,\n",
       "        0.773, 0.647, 0.769, 0.772, 0.647, 0.776, 0.772, 0.645, 0.771,\n",
       "        0.767, 0.654, 0.769, 0.774, 0.632, 0.769, 0.77 , 0.353, 0.772,\n",
       "        0.773, 0.386, 0.769, 0.771, 0.353, 0.768, 0.766, 0.343, 0.767,\n",
       "        0.77 , 0.578, 0.771, 0.771, 0.647, 0.767, 0.771, 0.647, 0.771,\n",
       "        0.767, 0.352, 0.769, 0.77 , 0.647, 0.772, 0.771, 0.647, 0.774,\n",
       "        0.785, 0.594, 0.832, 0.813, 0.568, 0.791, 0.798, 0.468, 0.821,\n",
       "        0.807, 0.541, 0.811, 0.771, 0.366, 0.798, 0.822, 0.628, 0.817,\n",
       "        0.822, 0.362, 0.794, 0.826, 0.439, 0.803, 0.801, 0.649, 0.804,\n",
       "        0.787, 0.524, 0.815, 0.802, 0.399, 0.82 , 0.816, 0.559, 0.804,\n",
       "        0.791, 0.652, 0.78 , 0.806, 0.518, 0.822, 0.833, 0.682, 0.829,\n",
       "        0.794, 0.648, 0.797, 0.793, 0.831, 0.817, 0.836, 0.838, 0.826,\n",
       "        0.839, 0.833, 0.835, 0.835, 0.838, 0.841, 0.827, 0.823, 0.804,\n",
       "        0.823, 0.839, 0.835, 0.834, 0.839, 0.835, 0.838, 0.837, 0.84 ,\n",
       "        0.816, 0.811, 0.832, 0.839, 0.839, 0.84 , 0.831, 0.837, 0.838,\n",
       "        0.838, 0.837, 0.843, 0.825, 0.805, 0.819, 0.829, 0.83 , 0.813,\n",
       "        0.836, 0.835, 0.84 , 0.84 , 0.835, 0.836, 0.834, 0.825, 0.837,\n",
       "        0.838, 0.844, 0.847, 0.847, 0.852, 0.849, 0.846, 0.849, 0.85 ,\n",
       "        0.839, 0.845, 0.837, 0.845, 0.845, 0.839, 0.845, 0.847, 0.851,\n",
       "        0.847, 0.858, 0.847, 0.826, 0.839, 0.836, 0.833, 0.848, 0.842,\n",
       "        0.852, 0.85 , 0.843, 0.845, 0.844, 0.845, 0.836, 0.835, 0.834,\n",
       "        0.838, 0.842, 0.842, 0.846, 0.847, 0.846, 0.851, 0.851, 0.851]),\n",
       " 'split3_test_precision_micro': array([0.854, 0.841, 0.844, 0.859, 0.861, 0.853, 0.85 , 0.858, 0.842,\n",
       "        0.853, 0.844, 0.853, 0.853, 0.849, 0.843, 0.863, 0.851, 0.862,\n",
       "        0.853, 0.847, 0.854, 0.868, 0.865, 0.859, 0.853, 0.851, 0.852,\n",
       "        0.856, 0.858, 0.844, 0.863, 0.845, 0.843, 0.846, 0.862, 0.853,\n",
       "        0.847, 0.857, 0.84 , 0.851, 0.846, 0.854, 0.868, 0.857, 0.874,\n",
       "        0.857, 0.85 , 0.86 , 0.852, 0.849, 0.852, 0.862, 0.857, 0.852,\n",
       "        0.855, 0.867, 0.847, 0.865, 0.867, 0.845, 0.862, 0.845, 0.848,\n",
       "        0.861, 0.85 , 0.865, 0.838, 0.858, 0.861, 0.859, 0.865, 0.851,\n",
       "        0.852, 0.854, 0.851, 0.856, 0.861, 0.863, 0.856, 0.848, 0.866,\n",
       "        0.866, 0.847, 0.856, 0.852, 0.852, 0.851, 0.857, 0.855, 0.852,\n",
       "        0.848, 0.851, 0.854, 0.859, 0.847, 0.855, 0.788, 0.621, 0.779,\n",
       "        0.789, 0.353, 0.791, 0.792, 0.317, 0.793, 0.788, 0.381, 0.79 ,\n",
       "        0.785, 0.647, 0.778, 0.788, 0.342, 0.792, 0.793, 0.315, 0.79 ,\n",
       "        0.792, 0.648, 0.787, 0.791, 0.353, 0.775, 0.79 , 0.435, 0.783,\n",
       "        0.792, 0.535, 0.785, 0.788, 0.647, 0.788, 0.789, 0.648, 0.789,\n",
       "        0.79 , 0.647, 0.789, 0.788, 0.647, 0.79 , 0.793, 0.68 , 0.791,\n",
       "        0.812, 0.497, 0.796, 0.804, 0.52 , 0.822, 0.83 , 0.547, 0.835,\n",
       "        0.815, 0.625, 0.813, 0.835, 0.719, 0.795, 0.816, 0.47 , 0.8  ,\n",
       "        0.814, 0.301, 0.803, 0.811, 0.59 , 0.813, 0.8  , 0.436, 0.798,\n",
       "        0.813, 0.304, 0.808, 0.825, 0.769, 0.816, 0.813, 0.415, 0.804,\n",
       "        0.808, 0.67 , 0.811, 0.836, 0.613, 0.801, 0.8  , 0.377, 0.825,\n",
       "        0.811, 0.577, 0.815, 0.831, 0.827, 0.805, 0.848, 0.841, 0.842,\n",
       "        0.841, 0.845, 0.842, 0.847, 0.844, 0.845, 0.818, 0.807, 0.835,\n",
       "        0.847, 0.842, 0.846, 0.845, 0.844, 0.848, 0.847, 0.843, 0.842,\n",
       "        0.838, 0.81 , 0.814, 0.838, 0.844, 0.835, 0.841, 0.841, 0.843,\n",
       "        0.842, 0.847, 0.847, 0.829, 0.814, 0.824, 0.84 , 0.835, 0.841,\n",
       "        0.838, 0.841, 0.843, 0.845, 0.844, 0.846, 0.864, 0.847, 0.843,\n",
       "        0.853, 0.852, 0.857, 0.857, 0.856, 0.856, 0.848, 0.854, 0.852,\n",
       "        0.851, 0.854, 0.85 , 0.854, 0.852, 0.854, 0.858, 0.861, 0.858,\n",
       "        0.857, 0.853, 0.853, 0.845, 0.856, 0.855, 0.85 , 0.852, 0.853,\n",
       "        0.852, 0.855, 0.852, 0.85 , 0.86 , 0.856, 0.854, 0.852, 0.855,\n",
       "        0.852, 0.858, 0.856, 0.858, 0.86 , 0.855, 0.86 , 0.861, 0.85 ]),\n",
       " 'split4_test_precision_micro': array([0.865, 0.859, 0.858, 0.863, 0.856, 0.856, 0.855, 0.851, 0.85 ,\n",
       "        0.842, 0.862, 0.846, 0.863, 0.854, 0.861, 0.856, 0.862, 0.862,\n",
       "        0.867, 0.865, 0.862, 0.864, 0.848, 0.861, 0.862, 0.86 , 0.861,\n",
       "        0.867, 0.851, 0.858, 0.86 , 0.852, 0.857, 0.841, 0.843, 0.859,\n",
       "        0.85 , 0.861, 0.859, 0.857, 0.867, 0.86 , 0.858, 0.858, 0.859,\n",
       "        0.862, 0.861, 0.867, 0.857, 0.868, 0.858, 0.86 , 0.862, 0.856,\n",
       "        0.863, 0.85 , 0.84 , 0.858, 0.863, 0.849, 0.853, 0.856, 0.861,\n",
       "        0.842, 0.869, 0.849, 0.847, 0.852, 0.871, 0.862, 0.856, 0.869,\n",
       "        0.863, 0.862, 0.863, 0.843, 0.855, 0.864, 0.863, 0.847, 0.872,\n",
       "        0.846, 0.863, 0.837, 0.857, 0.841, 0.843, 0.847, 0.86 , 0.865,\n",
       "        0.865, 0.853, 0.852, 0.852, 0.86 , 0.868, 0.784, 0.353, 0.794,\n",
       "        0.791, 0.623, 0.793, 0.793, 0.647, 0.794, 0.794, 0.647, 0.796,\n",
       "        0.795, 0.353, 0.793, 0.793, 0.647, 0.793, 0.791, 0.647, 0.79 ,\n",
       "        0.797, 0.348, 0.793, 0.786, 0.353, 0.79 , 0.792, 0.592, 0.794,\n",
       "        0.796, 0.647, 0.792, 0.792, 0.647, 0.788, 0.78 , 0.647, 0.794,\n",
       "        0.793, 0.42 , 0.797, 0.793, 0.585, 0.791, 0.792, 0.575, 0.79 ,\n",
       "        0.822, 0.355, 0.825, 0.822, 0.705, 0.822, 0.829, 0.66 , 0.821,\n",
       "        0.828, 0.393, 0.822, 0.832, 0.435, 0.833, 0.818, 0.413, 0.81 ,\n",
       "        0.825, 0.396, 0.831, 0.833, 0.364, 0.834, 0.817, 0.581, 0.818,\n",
       "        0.831, 0.612, 0.825, 0.824, 0.549, 0.832, 0.816, 0.704, 0.821,\n",
       "        0.819, 0.358, 0.815, 0.837, 0.366, 0.82 , 0.832, 0.601, 0.829,\n",
       "        0.82 , 0.367, 0.835, 0.845, 0.834, 0.834, 0.847, 0.845, 0.837,\n",
       "        0.852, 0.849, 0.85 , 0.849, 0.849, 0.849, 0.842, 0.843, 0.817,\n",
       "        0.849, 0.842, 0.849, 0.846, 0.851, 0.85 , 0.85 , 0.849, 0.851,\n",
       "        0.835, 0.84 , 0.838, 0.843, 0.848, 0.84 , 0.85 , 0.849, 0.847,\n",
       "        0.844, 0.843, 0.849, 0.846, 0.843, 0.831, 0.843, 0.841, 0.843,\n",
       "        0.845, 0.844, 0.849, 0.847, 0.848, 0.849, 0.844, 0.847, 0.845,\n",
       "        0.852, 0.855, 0.849, 0.842, 0.851, 0.854, 0.856, 0.853, 0.854,\n",
       "        0.854, 0.842, 0.852, 0.853, 0.854, 0.85 , 0.847, 0.846, 0.847,\n",
       "        0.846, 0.852, 0.855, 0.853, 0.846, 0.847, 0.854, 0.852, 0.857,\n",
       "        0.847, 0.852, 0.851, 0.85 , 0.85 , 0.849, 0.844, 0.856, 0.848,\n",
       "        0.853, 0.859, 0.853, 0.853, 0.849, 0.857, 0.853, 0.85 , 0.856]),\n",
       " 'mean_test_precision_micro': array([0.8498, 0.85  , 0.8532, 0.8578, 0.8586, 0.8502, 0.8522, 0.8534,\n",
       "        0.8516, 0.8482, 0.8564, 0.8502, 0.8508, 0.8506, 0.8498, 0.8548,\n",
       "        0.8564, 0.857 , 0.8574, 0.8558, 0.858 , 0.856 , 0.8486, 0.8536,\n",
       "        0.8518, 0.8522, 0.8538, 0.8526, 0.8562, 0.8544, 0.859 , 0.8506,\n",
       "        0.8534, 0.8474, 0.852 , 0.8556, 0.8524, 0.8554, 0.8496, 0.856 ,\n",
       "        0.8596, 0.8568, 0.8632, 0.8524, 0.8626, 0.8568, 0.8538, 0.858 ,\n",
       "        0.8532, 0.855 , 0.8514, 0.8596, 0.8574, 0.8522, 0.8544, 0.8562,\n",
       "        0.8558, 0.8538, 0.8568, 0.8492, 0.8492, 0.8538, 0.8502, 0.8504,\n",
       "        0.8546, 0.8556, 0.847 , 0.852 , 0.861 , 0.854 , 0.8518, 0.857 ,\n",
       "        0.8502, 0.8542, 0.8492, 0.8576, 0.8578, 0.8554, 0.8582, 0.8534,\n",
       "        0.8594, 0.8504, 0.8502, 0.8508, 0.856 , 0.8548, 0.8474, 0.8544,\n",
       "        0.8546, 0.855 , 0.8586, 0.8602, 0.8518, 0.8578, 0.8524, 0.8622,\n",
       "        0.7794, 0.4064, 0.7794, 0.7818, 0.4524, 0.7834, 0.784 , 0.522 ,\n",
       "        0.7856, 0.7832, 0.5256, 0.7842, 0.776 , 0.5446, 0.7788, 0.7824,\n",
       "        0.5504, 0.7828, 0.7822, 0.404 , 0.782 , 0.785 , 0.476 , 0.7826,\n",
       "        0.775 , 0.4268, 0.7766, 0.7806, 0.5334, 0.7792, 0.7822, 0.5548,\n",
       "        0.7816, 0.783 , 0.5902, 0.7814, 0.7796, 0.5882, 0.7786, 0.783 ,\n",
       "        0.4804, 0.7834, 0.783 , 0.5094, 0.783 , 0.7832, 0.643 , 0.7824,\n",
       "        0.8008, 0.4954, 0.8128, 0.8076, 0.5636, 0.8092, 0.8126, 0.5426,\n",
       "        0.821 , 0.8134, 0.5272, 0.8154, 0.807 , 0.4892, 0.8042, 0.8144,\n",
       "        0.4886, 0.8084, 0.8162, 0.3856, 0.8154, 0.8192, 0.5116, 0.8096,\n",
       "        0.803 , 0.5864, 0.8038, 0.8068, 0.5062, 0.8136, 0.8114, 0.5068,\n",
       "        0.8144, 0.8112, 0.5752, 0.8112, 0.8004, 0.5216, 0.806 , 0.8156,\n",
       "        0.5302, 0.8088, 0.814 , 0.5656, 0.8218, 0.8098, 0.5316, 0.8108,\n",
       "        0.8172, 0.827 , 0.8184, 0.8424, 0.8364, 0.8334, 0.8414, 0.8414,\n",
       "        0.8404, 0.8416, 0.8412, 0.8424, 0.826 , 0.826 , 0.82  , 0.8386,\n",
       "        0.8416, 0.8424, 0.8406, 0.8406, 0.8404, 0.8424, 0.841 , 0.843 ,\n",
       "        0.8236, 0.8162, 0.8184, 0.8396, 0.8404, 0.8352, 0.8398, 0.8428,\n",
       "        0.8402, 0.84  , 0.8418, 0.8432, 0.8246, 0.825 , 0.8252, 0.8384,\n",
       "        0.826 , 0.8354, 0.838 , 0.8392, 0.8418, 0.8416, 0.8402, 0.8414,\n",
       "        0.844 , 0.8408, 0.8428, 0.85  , 0.8502, 0.8516, 0.85  , 0.8528,\n",
       "        0.8536, 0.8522, 0.8526, 0.8536, 0.8462, 0.8444, 0.844 , 0.8512,\n",
       "        0.8496, 0.85  , 0.8502, 0.8522, 0.8538, 0.853 , 0.856 , 0.852 ,\n",
       "        0.8392, 0.8444, 0.8472, 0.8476, 0.8506, 0.8514, 0.852 , 0.8538,\n",
       "        0.8514, 0.852 , 0.8538, 0.8526, 0.844 , 0.8458, 0.8444, 0.8492,\n",
       "        0.851 , 0.8512, 0.8544, 0.8532, 0.853 , 0.8546, 0.8534, 0.8522]),\n",
       " 'std_test_precision_micro': array([0.01147868, 0.00622896, 0.00541849, 0.00767854, 0.00608605,\n",
       "        0.00487442, 0.00630555, 0.00287054, 0.00749933, 0.00724983,\n",
       "        0.00646838, 0.00348712, 0.00837616, 0.00241661, 0.01128539,\n",
       "        0.00549181, 0.00412795, 0.00603324, 0.0069455 , 0.00688186,\n",
       "        0.00846168, 0.01089954, 0.0095205 , 0.0072    , 0.00630555,\n",
       "        0.00426146, 0.00376298, 0.00845222, 0.00483322, 0.00608605,\n",
       "        0.00812404, 0.00535164, 0.00711618, 0.00531413, 0.00807465,\n",
       "        0.00662118, 0.00581722, 0.00449889, 0.00816333, 0.00357771,\n",
       "        0.00954149, 0.00584466, 0.00574108, 0.00788923, 0.0058515 ,\n",
       "        0.00713863, 0.00705408, 0.01043072, 0.00577581, 0.00729383,\n",
       "        0.01042305, 0.00135647, 0.00402989, 0.00386782, 0.00458694,\n",
       "        0.00923905, 0.01117855, 0.00767854, 0.00872697, 0.00411825,\n",
       "        0.00893085, 0.00705408, 0.0059127 , 0.00811419, 0.00976934,\n",
       "        0.0062482 , 0.00481664, 0.00424264, 0.00593296, 0.00961249,\n",
       "        0.00805978, 0.00782304, 0.00798499, 0.00545527, 0.00773046,\n",
       "        0.00847585, 0.00213542, 0.00830903, 0.00679412, 0.00488262,\n",
       "        0.00870862, 0.00922171, 0.01047664, 0.00711056, 0.00352136,\n",
       "        0.00773046, 0.00863944, 0.00991161, 0.0032619 , 0.01108152,\n",
       "        0.00665132, 0.00785875, 0.0051923 , 0.00386782, 0.01066958,\n",
       "        0.00487442, 0.0103846 , 0.1073007 , 0.01351444, 0.01228658,\n",
       "        0.15118942, 0.0118254 , 0.01108152, 0.15349267, 0.0109654 ,\n",
       "        0.00974474, 0.13155622, 0.01178813, 0.02001999, 0.15827394,\n",
       "        0.01180508, 0.0115343 , 0.13706728, 0.01254432, 0.01126765,\n",
       "        0.1223732 , 0.0108074 , 0.01161034, 0.14066556, 0.01235476,\n",
       "        0.01167904, 0.1476    , 0.01348481, 0.01235476, 0.12318214,\n",
       "        0.01352627, 0.01166876, 0.1114009 , 0.01072567, 0.01086278,\n",
       "        0.11815143, 0.01125344, 0.01053755, 0.11810064, 0.01881064,\n",
       "        0.01240967, 0.13892242, 0.0112712 , 0.01221475, 0.14548897,\n",
       "        0.01077033, 0.01259206, 0.0361054 , 0.01092886, 0.01416192,\n",
       "        0.08013139, 0.01364405, 0.00873155, 0.08187942, 0.01263962,\n",
       "        0.01585686, 0.06425449, 0.01219836, 0.00786384, 0.10637744,\n",
       "        0.01048046, 0.02495596, 0.12342512, 0.01677379, 0.00611882,\n",
       "        0.09855476, 0.01233856, 0.00800999, 0.05755901, 0.01445822,\n",
       "        0.01008762, 0.09664492, 0.01557691, 0.01157584, 0.07919747,\n",
       "        0.00877268, 0.01505191, 0.10536489, 0.0088227 , 0.01087382,\n",
       "        0.15840253, 0.01202664, 0.00923905, 0.10207723, 0.00842378,\n",
       "        0.013937  , 0.13031439, 0.01379855, 0.01923122, 0.09032918,\n",
       "        0.01057166, 0.0180333 , 0.10514866, 0.01301384, 0.01062826,\n",
       "        0.10302932, 0.0136    , 0.02408651, 0.00935949, 0.01565375,\n",
       "        0.00475815, 0.00781281, 0.00991161, 0.00553534, 0.00553534,\n",
       "        0.00531413, 0.00549909, 0.00511468, 0.00407922, 0.01550484,\n",
       "        0.01162755, 0.01253794, 0.0094361 , 0.00135647, 0.00484149,\n",
       "        0.0048    , 0.00665132, 0.00739189, 0.00508331, 0.0045607 ,\n",
       "        0.00447214, 0.01135958, 0.01483779, 0.01817251, 0.00185472,\n",
       "        0.00601997, 0.00636867, 0.00617738, 0.00401995, 0.00426146,\n",
       "        0.00252982, 0.00331059, 0.00430813, 0.01868261, 0.01407125,\n",
       "        0.0040694 , 0.005004  , 0.01607483, 0.0112712 , 0.00424264,\n",
       "        0.00331059, 0.0040694 , 0.00392938, 0.004996  , 0.0054626 ,\n",
       "        0.01164474, 0.00886341, 0.0031241 , 0.00666333, 0.0044    ,\n",
       "        0.00535164, 0.00513809, 0.00503587, 0.0032619 , 0.00515364,\n",
       "        0.00338231, 0.00241661, 0.00617738, 0.00531413, 0.00841427,\n",
       "        0.00545527, 0.00382623, 0.00923038, 0.00470744, 0.00549181,\n",
       "        0.00541849, 0.00576194, 0.00303315, 0.00309839, 0.01077775,\n",
       "        0.00749933, 0.0069685 , 0.00776144, 0.00436348, 0.00646838,\n",
       "        0.00394968, 0.00348712, 0.00484149, 0.00562139, 0.0076    ,\n",
       "        0.00523832, 0.00579655, 0.00832827, 0.00728286, 0.00661513,\n",
       "        0.00907744, 0.00526878, 0.00484149, 0.00688186, 0.00469042,\n",
       "        0.00372022, 0.00567803, 0.00278568]),\n",
       " 'rank_test_precision_micro': array([119, 115,  67,  15,  10, 110,  79,  64,  93, 128,  27, 108, 101,\n",
       "        103, 119,  42,  26,  21,  19,  34,  13,  30, 127,  60,  90,  79,\n",
       "         53,  73,  28,  50,   9, 103,  63, 130,  85,  37,  76,  38, 121,\n",
       "         30,   6,  23,   1,  76,   2,  23,  53,  14,  67,  40,  96,   6,\n",
       "         19,  79,  47,  29,  34,  58,  23, 123, 125,  53, 110, 106,  44,\n",
       "         36, 133,  85,   4,  52,  92,  21, 110,  51, 123,  18,  15,  38,\n",
       "         12,  64,   8, 106, 110, 101,  33,  42, 130,  47,  44,  41,  10,\n",
       "          5,  90,  15,  76,   3, 249, 286, 250, 244, 284, 229, 228, 273,\n",
       "        225, 232, 272, 227, 255, 266, 252, 239, 265, 237, 242, 287, 243,\n",
       "        226, 283, 238, 256, 285, 254, 247, 268, 251, 241, 264, 245, 233,\n",
       "        258, 246, 248, 259, 253, 233, 282, 229, 233, 276, 233, 231, 257,\n",
       "        239, 223, 279, 205, 216, 263, 213, 206, 267, 189, 204, 271, 198,\n",
       "        217, 280, 220, 200, 281, 215, 195, 288, 198, 191, 275, 212, 222,\n",
       "        260, 221, 218, 278, 203, 207, 277, 200, 208, 261, 208, 224, 274,\n",
       "        219, 197, 270, 214, 202, 262, 188, 211, 269, 210, 194, 180, 192,\n",
       "        146, 176, 179, 155, 155, 163, 152, 158, 146, 181, 181, 190, 173,\n",
       "        152, 146, 161, 162, 163, 146, 159, 143, 187, 195, 193, 170, 163,\n",
       "        178, 169, 144, 166, 168, 150, 142, 186, 185, 184, 174, 181, 177,\n",
       "        175, 171, 150, 152, 167, 155, 139, 160, 145, 115, 110,  93, 115,\n",
       "         72,  60,  79,  73,  60, 134, 137, 139,  98, 121, 115, 108,  79,\n",
       "         53,  70,  30,  85, 172, 137, 132, 129, 103,  95,  85,  53,  96,\n",
       "         85,  58,  73, 139, 135, 136, 125, 100,  98,  47,  67,  70,  44,\n",
       "         64,  79]),\n",
       " 'split0_test_roc_auc_ovo': array([0.898797  , 0.90657004, 0.91352764, 0.90185711, 0.90513643,\n",
       "        0.90521096, 0.90362391, 0.90635522, 0.89570181, 0.89560098,\n",
       "        0.90553539, 0.89973958, 0.9075477 , 0.9104456 , 0.90076108,\n",
       "        0.90160283, 0.90734165, 0.90560553, 0.90029637, 0.90522412,\n",
       "        0.9009233 , 0.89140099, 0.89253209, 0.89761329, 0.9061667 ,\n",
       "        0.90619739, 0.9058642 , 0.90796419, 0.89637258, 0.91006418,\n",
       "        0.90154146, 0.90391326, 0.90743371, 0.90341348, 0.89257155,\n",
       "        0.89468031, 0.91063412, 0.90800365, 0.90897254, 0.90761785,\n",
       "        0.9115504 , 0.90775814, 0.91382576, 0.90904268, 0.91060781,\n",
       "        0.89980973, 0.90595626, 0.90843329, 0.90703914, 0.90831054,\n",
       "        0.9060571 , 0.90770991, 0.91326897, 0.90572829, 0.9095907 ,\n",
       "        0.89585087, 0.90479886, 0.90949863, 0.90564499, 0.90359761,\n",
       "        0.90565814, 0.90857358, 0.9087884 , 0.89688552, 0.89749491,\n",
       "        0.90937588, 0.90714436, 0.89259785, 0.90820093, 0.90326003,\n",
       "        0.90199302, 0.90660073, 0.89911265, 0.90767484, 0.90642975,\n",
       "        0.90604833, 0.91363286, 0.90418946, 0.90240074, 0.90877087,\n",
       "        0.89502227, 0.89659179, 0.90487777, 0.9105289 , 0.90710929,\n",
       "        0.914365  , 0.90439552, 0.90504875, 0.90480324, 0.89780619,\n",
       "        0.8991828 , 0.91266835, 0.89578072, 0.90219031, 0.89243126,\n",
       "        0.90724958, 0.82642396, 0.6175733 , 0.81944006, 0.82360059,\n",
       "        0.21040702, 0.82968136, 0.82554275, 0.42465015, 0.83155338,\n",
       "        0.8313561 , 0.68180941, 0.82737532, 0.81616074, 0.60202722,\n",
       "        0.83184273, 0.82386364, 0.49327476, 0.82812062, 0.83168929,\n",
       "        0.62420647, 0.82884838, 0.82469223, 0.44547471, 0.82795402,\n",
       "        0.82753753, 0.76203879, 0.82779619, 0.82627929, 0.63056783,\n",
       "        0.82070269, 0.8272964 , 0.60407898, 0.82972959, 0.82883084,\n",
       "        0.76988636, 0.83092645, 0.8313824 , 0.64889345, 0.82509557,\n",
       "        0.82919911, 0.4230894 , 0.82807677, 0.8284275 , 0.39652164,\n",
       "        0.8290369 , 0.82062816, 0.44481271, 0.82569181, 0.86319795,\n",
       "        0.69052066, 0.87026077, 0.87313237, 0.54493284, 0.87059396,\n",
       "        0.86527164, 0.43011276, 0.87341733, 0.87634592, 0.4162984 ,\n",
       "        0.87219416, 0.86706474, 0.28749737, 0.85449109, 0.87610041,\n",
       "        0.41902971, 0.86311027, 0.87934028, 0.54703283, 0.88500456,\n",
       "        0.87992337, 0.60613514, 0.86720942, 0.86495598, 0.24076266,\n",
       "        0.8685115 , 0.86910336, 0.651502  , 0.87746387, 0.87580668,\n",
       "        0.51430976, 0.86917789, 0.87048436, 0.8028944 , 0.87278602,\n",
       "        0.8590155 , 0.63571917, 0.87384698, 0.86897622, 0.57558221,\n",
       "        0.8769597 , 0.86503051, 0.53386293, 0.87859059, 0.87782776,\n",
       "        0.61693322, 0.87443445, 0.86718312, 0.88432941, 0.87675803,\n",
       "        0.89261101, 0.88799014, 0.88753858, 0.89631559, 0.89416298,\n",
       "        0.89564482, 0.89531601, 0.89617091, 0.89460578, 0.8777883 ,\n",
       "        0.88806029, 0.8832553 , 0.89060746, 0.89251894, 0.8924181 ,\n",
       "        0.89491705, 0.89574127, 0.89160266, 0.89529409, 0.89364127,\n",
       "        0.89603062, 0.88477659, 0.87288685, 0.86650796, 0.89491705,\n",
       "        0.89290474, 0.88736322, 0.8956229 , 0.89383856, 0.89450056,\n",
       "        0.89468469, 0.89431643, 0.89489952, 0.87307976, 0.88479851,\n",
       "        0.88904672, 0.89283021, 0.87474572, 0.89334315, 0.89305819,\n",
       "        0.89345276, 0.89230412, 0.89328616, 0.8940753 , 0.89170349,\n",
       "        0.89412791, 0.89968259, 0.90073039, 0.90922682, 0.90507067,\n",
       "        0.90807818, 0.91108568, 0.90593873, 0.91141011, 0.91226063,\n",
       "        0.90625877, 0.91336543, 0.89649095, 0.89354044, 0.89315902,\n",
       "        0.9036546 , 0.90659635, 0.90140993, 0.90992828, 0.90803434,\n",
       "        0.91149779, 0.91127858, 0.91046314, 0.90905145, 0.88927908,\n",
       "        0.8877271 , 0.89946777, 0.90668403, 0.90247966, 0.90602641,\n",
       "        0.91057713, 0.91230885, 0.91224309, 0.91035354, 0.91054205,\n",
       "        0.91098923, 0.8988233 , 0.89742477, 0.89601747, 0.90264625,\n",
       "        0.90011223, 0.90555293, 0.91065604, 0.91029654, 0.90973537,\n",
       "        0.90991512, 0.91070865, 0.90733288]),\n",
       " 'split1_test_roc_auc_ovo': array([0.9198436 , 0.90702348, 0.91244839, 0.91595991, 0.92093384,\n",
       "        0.9073037 , 0.91264542, 0.91739604, 0.9194977 , 0.91383198,\n",
       "        0.91272861, 0.91517179, 0.90634044, 0.91551769, 0.91028981,\n",
       "        0.92117903, 0.9096418 , 0.91857823, 0.92079811, 0.91358241,\n",
       "        0.91332408, 0.90933969, 0.90479485, 0.91358679, 0.91770254,\n",
       "        0.91804406, 0.92077621, 0.89992163, 0.91961154, 0.91441869,\n",
       "        0.9149047 , 0.91606062, 0.91336786, 0.91187481, 0.90664256,\n",
       "        0.90835015, 0.92013258, 0.90696218, 0.92134979, 0.91425231,\n",
       "        0.92211164, 0.91831114, 0.91859136, 0.91437491, 0.92013696,\n",
       "        0.91097723, 0.91514114, 0.922567  , 0.90710229, 0.92280782,\n",
       "        0.92094259, 0.91634522, 0.91858261, 0.91583294, 0.91470329,\n",
       "        0.91666922, 0.9150886 , 0.91272861, 0.90967245, 0.91853006,\n",
       "        0.91567969, 0.92036464, 0.90859535, 0.91962906, 0.92152493,\n",
       "        0.91516741, 0.92019826, 0.91828049, 0.91448875, 0.9137882 ,\n",
       "        0.91109107, 0.91687501, 0.91800027, 0.91828925, 0.90933969,\n",
       "        0.91479086, 0.91864828, 0.91061819, 0.92087254, 0.92409946,\n",
       "        0.91349046, 0.90958926, 0.91275488, 0.91786454, 0.92045659,\n",
       "        0.92289539, 0.92219483, 0.91722966, 0.92059232, 0.92179639,\n",
       "        0.91442307, 0.91913867, 0.92705492, 0.91759745, 0.91934446,\n",
       "        0.92109146, 0.83222631, 0.37765936, 0.83762933, 0.83826858,\n",
       "        0.31016984, 0.84065922, 0.83956023, 0.48182284, 0.8401557 ,\n",
       "        0.84188957, 0.46034213, 0.8412722 , 0.84175821, 0.52909703,\n",
       "        0.83841745, 0.84318121, 0.67963274, 0.84111458, 0.83843059,\n",
       "        0.42017855, 0.83591297, 0.83868892, 0.44849841, 0.83819853,\n",
       "        0.83591297, 0.7185178 , 0.83877211, 0.84156994, 0.57503142,\n",
       "        0.84012067, 0.84011629, 0.43186903, 0.84019948, 0.83964342,\n",
       "        0.72337789, 0.84008126, 0.84103577, 0.41259069, 0.83994991,\n",
       "        0.84378106, 0.34763191, 0.8401951 , 0.84211725, 0.64063383,\n",
       "        0.83819415, 0.83928439, 0.55833198, 0.84100074, 0.87309482,\n",
       "        0.39275628, 0.86423721, 0.86664536, 0.4572597 , 0.87725874,\n",
       "        0.8770836 , 0.52317736, 0.87949613, 0.87256065, 0.62211733,\n",
       "        0.88044187, 0.87160177, 0.6824043 , 0.87002553, 0.87335753,\n",
       "        0.46947997, 0.88814358, 0.87526216, 0.63516513, 0.87977197,\n",
       "        0.87597585, 0.50510747, 0.8722673 , 0.86658406, 0.53562093,\n",
       "        0.85837008, 0.87098003, 0.67207114, 0.8747061 , 0.86501657,\n",
       "        0.43105026, 0.86844053, 0.88006533, 0.7002684 , 0.87584449,\n",
       "        0.85790158, 0.39024743, 0.88346301, 0.8762911 , 0.69168225,\n",
       "        0.85850581, 0.87860292, 0.55906756, 0.88329225, 0.87608531,\n",
       "        0.40781817, 0.86694747, 0.88537639, 0.88911997, 0.89441791,\n",
       "        0.9018569 , 0.89929988, 0.90066158, 0.90177371, 0.89887517,\n",
       "        0.90048207, 0.90155917, 0.90256183, 0.90135776, 0.89489954,\n",
       "        0.88721097, 0.89671222, 0.89591534, 0.89961513, 0.90261875,\n",
       "        0.90250054, 0.8985643 , 0.90085862, 0.90122641, 0.90139279,\n",
       "        0.90042953, 0.87375159, 0.88396653, 0.88457076, 0.89780245,\n",
       "        0.89766234, 0.89267528, 0.90124392, 0.90112132, 0.90106002,\n",
       "        0.90095494, 0.90198388, 0.9014103 , 0.88970669, 0.89211046,\n",
       "        0.88280186, 0.89937432, 0.88805163, 0.89366919, 0.89910285,\n",
       "        0.89968081, 0.89905469, 0.89968081, 0.89865625, 0.89942248,\n",
       "        0.90576686, 0.90544724, 0.90260124, 0.91790395, 0.9110429 ,\n",
       "        0.91252282, 0.91354738, 0.91433113, 0.90905509, 0.91149826,\n",
       "        0.91401588, 0.91735664, 0.90823194, 0.90376591, 0.90889308,\n",
       "        0.91449313, 0.90976877, 0.91416912, 0.91236082, 0.91337224,\n",
       "        0.9151893 , 0.91322338, 0.9140115 , 0.91404215, 0.9086654 ,\n",
       "        0.90785101, 0.90978629, 0.91141507, 0.91213314, 0.9098038 ,\n",
       "        0.91588548, 0.91730848, 0.91147199, 0.91086777, 0.91501416,\n",
       "        0.91802216, 0.90810058, 0.90821004, 0.90749635, 0.91251407,\n",
       "        0.91404653, 0.91004024, 0.91159459, 0.91274612, 0.91447999,\n",
       "        0.91523747, 0.91250969, 0.91536882]),\n",
       " 'split2_test_roc_auc_ovo': array([0.90648055, 0.91086339, 0.91398085, 0.92163877, 0.91385825,\n",
       "        0.90550416, 0.91019348, 0.91613943, 0.91082398, 0.90704975,\n",
       "        0.90651996, 0.91597305, 0.907203  , 0.90279389, 0.91768502,\n",
       "        0.91155081, 0.92093822, 0.91510173, 0.91365684, 0.91427421,\n",
       "        0.913219  , 0.91013219, 0.90653747, 0.90771528, 0.90782474,\n",
       "        0.90475544, 0.91470767, 0.91152891, 0.92292166, 0.91483027,\n",
       "        0.92125346, 0.91181789, 0.91461135, 0.90450149, 0.91315332,\n",
       "        0.90917768, 0.91391079, 0.91530752, 0.90244362, 0.91533817,\n",
       "        0.9118967 , 0.91845563, 0.91999247, 0.9093572 , 0.91505795,\n",
       "        0.9154345 , 0.92402065, 0.91058317, 0.91643716, 0.91126183,\n",
       "        0.9056574 , 0.91361744, 0.9159205 , 0.90962866, 0.90680894,\n",
       "        0.90732122, 0.92043907, 0.91020662, 0.90696656, 0.91427421,\n",
       "        0.90831513, 0.91835493, 0.91894164, 0.91079771, 0.91233455,\n",
       "        0.91579791, 0.90260124, 0.90241297, 0.90560048, 0.90943163,\n",
       "        0.9113494 , 0.91172157, 0.91200179, 0.90895   , 0.90681769,\n",
       "        0.92138044, 0.91085025, 0.91123118, 0.90609087, 0.90390164,\n",
       "        0.91163837, 0.91368749, 0.89878323, 0.91478648, 0.91875337,\n",
       "        0.91959841, 0.91408593, 0.91566218, 0.91500541, 0.91018473,\n",
       "        0.91356927, 0.91236082, 0.90711543, 0.91681371, 0.91093344,\n",
       "        0.91912116, 0.82075914, 0.61229208, 0.81885451, 0.82226533,\n",
       "        0.55794668, 0.82278198, 0.82062778, 0.42708338, 0.82362265,\n",
       "        0.82289144, 0.39776524, 0.82253241, 0.82213835, 0.30837905,\n",
       "        0.81977398, 0.82703346, 0.41439461, 0.81539553, 0.82215586,\n",
       "        0.36843834, 0.82280825, 0.82300966, 0.51887771, 0.82132396,\n",
       "        0.82148596, 0.75979351, 0.82039135, 0.82500186, 0.3507231 ,\n",
       "        0.82010237, 0.82094303, 0.45523247, 0.82431882, 0.82227846,\n",
       "        0.34824927, 0.81693237, 0.82410428, 0.37809283, 0.82031253,\n",
       "        0.82089049, 0.46545617, 0.8242619 , 0.8221077 , 0.51168829,\n",
       "        0.82169613, 0.82061903, 0.63234103, 0.81908657, 0.85839635,\n",
       "        0.66131765, 0.86992044, 0.87122522, 0.67206676, 0.85692081,\n",
       "        0.85928955, 0.48320643, 0.87200897, 0.8657828 , 0.51428034,\n",
       "        0.86565583, 0.83431922, 0.33535472, 0.85798039, 0.86421094,\n",
       "        0.42261298, 0.86804646, 0.86775311, 0.39817681, 0.86139559,\n",
       "        0.86927681, 0.56588044, 0.86043671, 0.85854084, 0.48913486,\n",
       "        0.8571047 , 0.85536645, 0.42013477, 0.87024883, 0.8593815 ,\n",
       "        0.39333424, 0.86375558, 0.86650525, 0.56356424, 0.86175024,\n",
       "        0.85315096, 0.62166197, 0.84548428, 0.86487646, 0.52911017,\n",
       "        0.86833982, 0.87388732, 0.66757009, 0.87855038, 0.85310717,\n",
       "        0.70147685, 0.85928517, 0.8670438 , 0.87992522, 0.86909729,\n",
       "        0.88753935, 0.8865936 , 0.88055134, 0.89069184, 0.89118661,\n",
       "        0.89040286, 0.89070498, 0.88974609, 0.89089325, 0.87767031,\n",
       "        0.87597585, 0.85960918, 0.8794611 , 0.88904116, 0.88629149,\n",
       "        0.88639657, 0.89055611, 0.8898862 , 0.89024524, 0.89113406,\n",
       "        0.89087136, 0.86886961, 0.87030137, 0.88225455, 0.8877539 ,\n",
       "        0.88720659, 0.88758314, 0.88422924, 0.88912435, 0.88945274,\n",
       "        0.89002194, 0.88900613, 0.88952717, 0.87481556, 0.86019589,\n",
       "        0.86957017, 0.87967127, 0.88105486, 0.87076986, 0.88580986,\n",
       "        0.88487287, 0.88681253, 0.88969355, 0.88633528, 0.88932138,\n",
       "        0.89416396, 0.89102898, 0.89247825, 0.90715921, 0.90270632,\n",
       "        0.90643677, 0.91126183, 0.91096847, 0.90619595, 0.90939223,\n",
       "        0.90866102, 0.91117426, 0.90234291, 0.90008363, 0.89349405,\n",
       "        0.90899379, 0.90167301, 0.90033758, 0.9101497 , 0.90873546,\n",
       "        0.90993078, 0.90937033, 0.91511049, 0.90814437, 0.89340648,\n",
       "        0.89486451, 0.89917729, 0.90133587, 0.91085463, 0.90532464,\n",
       "        0.907365  , 0.90888433, 0.90253994, 0.91109545, 0.91381009,\n",
       "        0.90778533, 0.89768423, 0.89233814, 0.89435223, 0.90478171,\n",
       "        0.90732997, 0.90409429, 0.90829761, 0.90806555, 0.90846399,\n",
       "        0.90999645, 0.9113494 , 0.90881427]),\n",
       " 'split3_test_roc_auc_ovo': array([0.91955024, 0.91000959, 0.91313143, 0.91558336, 0.91616132,\n",
       "        0.91119615, 0.90290335, 0.91890223, 0.91160773, 0.90357764,\n",
       "        0.89844609, 0.8999873 , 0.91332846, 0.90848589, 0.91450188,\n",
       "        0.91928754, 0.91890223, 0.91540823, 0.90716359, 0.91209373,\n",
       "        0.91643716, 0.91970349, 0.91634522, 0.91438367, 0.91116988,\n",
       "        0.91011467, 0.91981733, 0.90291649, 0.90915141, 0.90648931,\n",
       "        0.91263666, 0.9053947 , 0.91057003, 0.90928277, 0.92062735,\n",
       "        0.9105569 , 0.91376193, 0.91006213, 0.90918206, 0.91276364,\n",
       "        0.91109107, 0.91558774, 0.92663897, 0.91953273, 0.92528602,\n",
       "        0.91263666, 0.9091339 , 0.9210433 , 0.91936197, 0.91237396,\n",
       "        0.91020224, 0.92292603, 0.91779448, 0.91821044, 0.91471205,\n",
       "        0.92385865, 0.9160431 , 0.90686586, 0.90736938, 0.90146722,\n",
       "        0.91608251, 0.90857783, 0.91454567, 0.92109584, 0.90648931,\n",
       "        0.91782513, 0.90003109, 0.91110858, 0.91874023, 0.91398523,\n",
       "        0.91614818, 0.90761019, 0.91889785, 0.91624013, 0.91708955,\n",
       "        0.91737853, 0.91920435, 0.91659479, 0.91212438, 0.90916893,\n",
       "        0.91276364, 0.91760621, 0.90830637, 0.91011029, 0.91951522,\n",
       "        0.91919997, 0.91222947, 0.91048684, 0.92006252, 0.91701512,\n",
       "        0.91174346, 0.91235206, 0.91304824, 0.91977355, 0.90556983,\n",
       "        0.91311829, 0.84435026, 0.5865993 , 0.84159183, 0.84813762,\n",
       "        0.63632542, 0.84401312, 0.84412258, 0.47581122, 0.84192022,\n",
       "        0.84392117, 0.46292104, 0.84618045, 0.8449107 , 0.43015268,\n",
       "        0.84593526, 0.84733637, 0.45836745, 0.84431961, 0.84254634,\n",
       "        0.37938886, 0.84661392, 0.84676279, 0.43712756, 0.84725755,\n",
       "        0.84738015, 0.65698298, 0.84311991, 0.84506395, 0.49540481,\n",
       "        0.84717874, 0.84193773, 0.47993572, 0.84457794, 0.84582142,\n",
       "        0.73378548, 0.84673214, 0.84793184, 0.46465929, 0.84239309,\n",
       "        0.84573385, 0.5585728 , 0.84513838, 0.84536168, 0.54616863,\n",
       "        0.84092631, 0.84308051, 0.5348547 , 0.84724442, 0.87234611,\n",
       "        0.39334737, 0.85961794, 0.86597545, 0.69249664, 0.88391837,\n",
       "        0.88609446, 0.3683639 , 0.88268802, 0.88392275, 0.55251739,\n",
       "        0.8768384 , 0.87677711, 0.79972941, 0.86009519, 0.8796844 ,\n",
       "        0.30772666, 0.8718601 , 0.87577006, 0.34527192, 0.86617248,\n",
       "        0.87703106, 0.59718203, 0.8773682 , 0.85889111, 0.71741881,\n",
       "        0.86851934, 0.88046814, 0.26994496, 0.87038456, 0.87921591,\n",
       "        0.82745817, 0.87637429, 0.87441712, 0.34626583, 0.86911481,\n",
       "        0.87517459, 0.63787102, 0.86364612, 0.88587116, 0.36516325,\n",
       "        0.86465754, 0.86739845, 0.40396513, 0.88652793, 0.87651002,\n",
       "        0.64161898, 0.8730598 , 0.8885858 , 0.88951404, 0.87734631,\n",
       "        0.901025  , 0.89792505, 0.90060029, 0.90060904, 0.90235167,\n",
       "        0.90048207, 0.90239107, 0.90143657, 0.90281579, 0.88263986,\n",
       "        0.87598023, 0.89323572, 0.90244362, 0.90063094, 0.90105127,\n",
       "        0.90146284, 0.90312228, 0.90205831, 0.90223783, 0.90000919,\n",
       "        0.90289022, 0.89394941, 0.87990332, 0.88277121, 0.89602918,\n",
       "        0.89889707, 0.89410266, 0.90011428, 0.9003945 , 0.90135338,\n",
       "        0.89908534, 0.90223345, 0.90207145, 0.88703145, 0.88387021,\n",
       "        0.88816985, 0.89800824, 0.89484262, 0.89933053, 0.89924296,\n",
       "        0.90072726, 0.89932178, 0.90094181, 0.90086737, 0.90093305,\n",
       "        0.90853405, 0.90412494, 0.90594638, 0.91026792, 0.91448875,\n",
       "        0.91016721, 0.916389  , 0.9134423 , 0.91615256, 0.9156578 ,\n",
       "        0.91587672, 0.91630143, 0.9065725 , 0.91095096, 0.90001795,\n",
       "        0.91666484, 0.91305262, 0.91751864, 0.91683122, 0.91697571,\n",
       "        0.9153732 , 0.91800903, 0.91379695, 0.91694069, 0.90125268,\n",
       "        0.90782912, 0.91188795, 0.90694029, 0.90997018, 0.91262353,\n",
       "        0.9159818 , 0.91659041, 0.91316208, 0.91111734, 0.91811849,\n",
       "        0.91472081, 0.90756203, 0.90442268, 0.90386224, 0.91038614,\n",
       "        0.91066198, 0.91486968, 0.91340289, 0.91405966, 0.9156972 ,\n",
       "        0.91498789, 0.91249655, 0.91391517]),\n",
       " 'split4_test_roc_auc_ovo': array([0.91204119, 0.90745695, 0.90558297, 0.91192297, 0.91152016,\n",
       "        0.9172428 , 0.90563551, 0.89841544, 0.90380094, 0.88711464,\n",
       "        0.90817939, 0.90914703, 0.90946228, 0.90782474, 0.91069701,\n",
       "        0.91254909, 0.91500979, 0.91180476, 0.91849066, 0.90570119,\n",
       "        0.90702348, 0.91285559, 0.90512761, 0.90849902, 0.91067074,\n",
       "        0.90095056, 0.91326716, 0.91952397, 0.89957573, 0.90805242,\n",
       "        0.91273737, 0.90476858, 0.91198427, 0.8966553 , 0.90609525,\n",
       "        0.91200616, 0.90249616, 0.91283369, 0.91253596, 0.91757994,\n",
       "        0.91210249, 0.91006651, 0.90770652, 0.91405966, 0.91645468,\n",
       "        0.90784663, 0.90608211, 0.91215503, 0.90764522, 0.91532503,\n",
       "        0.91477335, 0.91539947, 0.91053938, 0.90997894, 0.9131971 ,\n",
       "        0.91312705, 0.90240421, 0.90936596, 0.90377905, 0.90273259,\n",
       "        0.90624849, 0.91005337, 0.90416435, 0.89492581, 0.9195196 ,\n",
       "        0.91381009, 0.90670385, 0.89296426, 0.90923898, 0.92649448,\n",
       "        0.91135815, 0.91469892, 0.90885805, 0.91206746, 0.91516303,\n",
       "        0.89340648, 0.89883577, 0.91330657, 0.91279429, 0.89826657,\n",
       "        0.91226449, 0.90145846, 0.9110429 , 0.90151101, 0.90950607,\n",
       "        0.90172117, 0.89540744, 0.9010031 , 0.91381885, 0.91866142,\n",
       "        0.90725992, 0.90209334, 0.90724678, 0.90178247, 0.91046057,\n",
       "        0.90812247, 0.84540109, 0.46697549, 0.84539233, 0.84136415,\n",
       "        0.53728037, 0.84027392, 0.84244125, 0.31501241, 0.84086063,\n",
       "        0.84058479, 0.58151591, 0.84246752, 0.84755091, 0.68797369,\n",
       "        0.84217417, 0.83664855, 0.60800995, 0.8407862 , 0.84274337,\n",
       "        0.32985538, 0.84122404, 0.84286596, 0.6254362 , 0.84234055,\n",
       "        0.83877211, 0.50973988, 0.8382642 , 0.84123718, 0.38786992,\n",
       "        0.84098323, 0.84299732, 0.47923079, 0.83925374, 0.84280029,\n",
       "        0.64810785, 0.84111458, 0.83579913, 0.58402476, 0.843225  ,\n",
       "        0.83998932, 0.69298265, 0.84235368, 0.8407862 , 0.45519745,\n",
       "        0.84041841, 0.84236682, 0.8132063 , 0.84076868, 0.87229357,\n",
       "        0.49690224, 0.87402306, 0.86919362, 0.66429938, 0.87390046,\n",
       "        0.87767469, 0.41873366, 0.87221038, 0.87416317, 0.53829617,\n",
       "        0.87508702, 0.87376473, 0.3266197 , 0.87922466, 0.87400992,\n",
       "        0.49739263, 0.85963983, 0.87518335, 0.49143793, 0.87805562,\n",
       "        0.88238153, 0.41954368, 0.88138324, 0.87317801, 0.58475597,\n",
       "        0.87369905, 0.87965375, 0.59092083, 0.87544606, 0.87372532,\n",
       "        0.50217828, 0.87718868, 0.86685115, 0.75322583, 0.87306418,\n",
       "        0.87472361, 0.6018407 , 0.86923303, 0.88121686, 0.27568512,\n",
       "        0.87394862, 0.88270116, 0.3971435 , 0.87922466, 0.86616373,\n",
       "        0.45931757, 0.88010473, 0.89169888, 0.88267489, 0.87957494,\n",
       "        0.89521479, 0.89581026, 0.88499547, 0.89627437, 0.89554317,\n",
       "        0.89616053, 0.89589782, 0.89561322, 0.89598539, 0.88998691,\n",
       "        0.8898468 , 0.86825663, 0.89527171, 0.8952936 , 0.89559133,\n",
       "        0.89464121, 0.89635318, 0.89565263, 0.89634443, 0.89532425,\n",
       "        0.89586718, 0.88230272, 0.89390563, 0.8863703 , 0.89556506,\n",
       "        0.89477694, 0.89224181, 0.89549063, 0.89591096, 0.89570517,\n",
       "        0.8954556 , 0.89235565, 0.89603793, 0.89056486, 0.89173829,\n",
       "        0.88015727, 0.89394065, 0.89461494, 0.89200976, 0.89400633,\n",
       "        0.89409828, 0.89534176, 0.8955782 , 0.89471564, 0.89454488,\n",
       "        0.89916415, 0.8960248 , 0.89875258, 0.90132273, 0.90640612,\n",
       "        0.90264503, 0.9036258 , 0.90240421, 0.90307849, 0.90574497,\n",
       "        0.90255746, 0.90481236, 0.90450587, 0.89436974, 0.90377905,\n",
       "        0.90310914, 0.90432197, 0.89723325, 0.90150663, 0.90342439,\n",
       "        0.90294276, 0.90445771, 0.90426068, 0.9029559 , 0.90042515,\n",
       "        0.90404613, 0.89684795, 0.9036258 , 0.90282892, 0.9002719 ,\n",
       "        0.90179998, 0.90549978, 0.90468101, 0.90291649, 0.90771528,\n",
       "        0.90502253, 0.89207105, 0.9003945 , 0.90091554, 0.89824906,\n",
       "        0.90790793, 0.90710667, 0.90251367, 0.90173868, 0.90629666,\n",
       "        0.90357764, 0.8986869 , 0.90205393]),\n",
       " 'mean_test_roc_auc_ovo': array([0.91134252, 0.90838469, 0.91173425, 0.91339243, 0.913522  ,\n",
       "        0.90929155, 0.90700034, 0.91144167, 0.90828643, 0.901435  ,\n",
       "        0.90628189, 0.90800375, 0.90877638, 0.90901356, 0.91078696,\n",
       "        0.91323386, 0.91436674, 0.9132997 , 0.91208111, 0.91017513,\n",
       "        0.9101854 , 0.90868639, 0.90506745, 0.90835961, 0.91070692,\n",
       "        0.90801242, 0.91488651, 0.90837104, 0.90952658, 0.91077098,\n",
       "        0.91261473, 0.90839101, 0.91159345, 0.90514557, 0.907818  ,\n",
       "        0.90695424, 0.91218712, 0.91063384, 0.91089679, 0.91351038,\n",
       "        0.91375046, 0.91403583, 0.91735102, 0.91327344, 0.91750868,\n",
       "        0.90934095, 0.91206681, 0.91495636, 0.91151716, 0.91401583,\n",
       "        0.91152654, 0.91519961, 0.91522119, 0.91187585, 0.91180242,\n",
       "        0.9113654 , 0.91175477, 0.90973313, 0.90668649, 0.90812034,\n",
       "        0.91039679, 0.91318487, 0.91100708, 0.90866679, 0.91147266,\n",
       "        0.91439528, 0.90733576, 0.90347283, 0.91125388, 0.91339191,\n",
       "        0.91038796, 0.91150128, 0.91137412, 0.91264434, 0.91096794,\n",
       "        0.91060093, 0.9122343 , 0.91118804, 0.91085656, 0.90884149,\n",
       "        0.90903585, 0.90778664, 0.90715303, 0.91096024, 0.9150681 ,\n",
       "        0.91555599, 0.90966264, 0.90988611, 0.91485647, 0.91309277,\n",
       "        0.9092357 , 0.91172265, 0.91004922, 0.9116315 , 0.90774791,\n",
       "        0.91374059, 0.83383215, 0.53221991, 0.83258161, 0.83472725,\n",
       "        0.45042587, 0.83548192, 0.83445892, 0.424876  , 0.83562251,\n",
       "        0.83612861, 0.51687075, 0.83596558, 0.83450378, 0.51152593,\n",
       "        0.83562872, 0.83561265, 0.5307359 , 0.83394731, 0.83551309,\n",
       "        0.42441352, 0.83508151, 0.83520391, 0.49508292, 0.83541492,\n",
       "        0.83421774, 0.68141459, 0.83366875, 0.83583044, 0.48791942,\n",
       "        0.83381754, 0.83465815, 0.4900694 , 0.83561591, 0.83587489,\n",
       "        0.64468137, 0.83515736, 0.83605068, 0.4976522 , 0.83419522,\n",
       "        0.83591877, 0.49754659, 0.83600517, 0.83576007, 0.51004197,\n",
       "        0.83405438, 0.83319578, 0.59670934, 0.83475844, 0.86786576,\n",
       "        0.52696884, 0.86761188, 0.8692344 , 0.60621106, 0.87251847,\n",
       "        0.87308279, 0.44471882, 0.87596417, 0.87455506, 0.52870193,\n",
       "        0.87404346, 0.86470551, 0.4863211 , 0.86436337, 0.87347264,\n",
       "        0.42324839, 0.87016005, 0.87466179, 0.48341693, 0.87408004,\n",
       "        0.87691772, 0.53876975, 0.87173297, 0.86443   , 0.51353865,\n",
       "        0.86524093, 0.87111435, 0.52091474, 0.87364988, 0.8706292 ,\n",
       "        0.53366614, 0.87098739, 0.87166464, 0.63324374, 0.87051195,\n",
       "        0.86399325, 0.57746806, 0.86713468, 0.87544636, 0.4874446 ,\n",
       "        0.8684823 , 0.87352407, 0.51232184, 0.88123716, 0.8699388 ,\n",
       "        0.56543296, 0.87076632, 0.8799776 , 0.8851127 , 0.8794389 ,\n",
       "        0.89564941, 0.89352379, 0.89086945, 0.89713291, 0.89642392,\n",
       "        0.89663447, 0.89717381, 0.89710573, 0.89713159, 0.88459698,\n",
       "        0.88341483, 0.88021381, 0.89273984, 0.89541995, 0.89559419,\n",
       "        0.89598364, 0.89686743, 0.89601168, 0.8970696 , 0.89630031,\n",
       "        0.89721778, 0.88072998, 0.88019274, 0.88049496, 0.89441353,\n",
       "        0.89428954, 0.89079322, 0.89534019, 0.89607794, 0.89641438,\n",
       "        0.8960405 , 0.89597911, 0.89678927, 0.88303966, 0.88254267,\n",
       "        0.88194917, 0.89276494, 0.88666195, 0.8898245 , 0.89424404,\n",
       "        0.8945664 , 0.89456697, 0.89583611, 0.89492997, 0.89518506,\n",
       "        0.90035139, 0.89926171, 0.90010177, 0.90917613, 0.90794295,\n",
       "        0.90797   , 0.91118194, 0.90941697, 0.90917844, 0.91091078,\n",
       "        0.90947397, 0.91260202, 0.90362883, 0.90054214, 0.89986863,\n",
       "        0.9093831 , 0.90708254, 0.90613371, 0.91015533, 0.91010843,\n",
       "        0.91098677, 0.91126781, 0.91152855, 0.91022691, 0.89860576,\n",
       "        0.90046357, 0.90343345, 0.90600021, 0.90765331, 0.90681006,\n",
       "        0.91032188, 0.91211837, 0.90881962, 0.90927012, 0.91304001,\n",
       "        0.91130801, 0.90084824, 0.90055803, 0.90052876, 0.90571544,\n",
       "        0.90801173, 0.90833276, 0.90929296, 0.90938131, 0.91093464,\n",
       "        0.91074291, 0.90915024, 0.90949701]),\n",
       " 'std_test_roc_auc_ovo': array([0.00801439, 0.00171993, 0.00311653, 0.0065516 , 0.00522187,\n",
       "        0.00451214, 0.00379767, 0.00785797, 0.00802066, 0.00926186,\n",
       "        0.00462978, 0.00705347, 0.00249443, 0.00411656, 0.00569486,\n",
       "        0.00690442, 0.0052124 , 0.00440434, 0.00751816, 0.00391463,\n",
       "        0.00554838, 0.00938148, 0.0075705 , 0.00599217, 0.00395158,\n",
       "        0.00581077, 0.00534898, 0.00686543, 0.01051972, 0.00334683,\n",
       "        0.00636403, 0.00474798, 0.00248031, 0.00525164, 0.00926659,\n",
       "        0.00626167, 0.00574296, 0.00307924, 0.00616525, 0.00333863,\n",
       "        0.00419462, 0.00436775, 0.00632812, 0.00385347, 0.00494234,\n",
       "        0.00536186, 0.00684256, 0.00573577, 0.00529685, 0.00493647,\n",
       "        0.00575012, 0.00489181, 0.00297121, 0.00452164, 0.00311938,\n",
       "        0.00942731, 0.00693886, 0.0018766 , 0.00194929, 0.0069279 ,\n",
       "        0.0045658 , 0.00511026, 0.00515831, 0.01101641, 0.00879494,\n",
       "        0.00282363, 0.00695203, 0.01007363, 0.00472972, 0.00762266,\n",
       "        0.00460468, 0.00395743, 0.00717747, 0.00408587, 0.00437136,\n",
       "        0.00996057, 0.00739071, 0.00407499, 0.00632289, 0.00859243,\n",
       "        0.00703303, 0.00774421, 0.0049613 , 0.00552279, 0.00559773,\n",
       "        0.0074338 , 0.00910125, 0.00616376, 0.00569507, 0.00853562,\n",
       "        0.00560254, 0.0054666 , 0.01018075, 0.00793575, 0.00884681,\n",
       "        0.00560481, 0.00972455, 0.09465693, 0.01124199, 0.01015411,\n",
       "        0.16182946, 0.00796849, 0.00952812, 0.05985361, 0.00704561,\n",
       "        0.00789645, 0.10166212, 0.00926306, 0.01281038, 0.13230518,\n",
       "        0.00919294, 0.00902711, 0.09830297, 0.01080596, 0.00778835,\n",
       "        0.10395849, 0.00849592, 0.00962969, 0.07148159, 0.00948874,\n",
       "        0.0089843 , 0.09390344, 0.00833383, 0.008437  , 0.10660753,\n",
       "        0.01122233, 0.00888398, 0.05969798, 0.00744089, 0.00889469,\n",
       "        0.15342275, 0.01042842, 0.00813124, 0.10291022, 0.00956442,\n",
       "        0.00944165, 0.1191365 , 0.00827089, 0.00892211, 0.08277302,\n",
       "        0.00752015, 0.01034415, 0.12370283, 0.01057166, 0.00597462,\n",
       "        0.12772667, 0.00507426, 0.00270122, 0.09066196, 0.00895911,\n",
       "        0.00956727, 0.05358298, 0.00433373, 0.00586591, 0.06667563,\n",
       "        0.00497218, 0.0155193 , 0.21189604, 0.00904809, 0.00512976,\n",
       "        0.06480463, 0.009911  , 0.00378098, 0.10342993, 0.00884277,\n",
       "        0.00443104, 0.06932977, 0.00739239, 0.0054186 , 0.15628752,\n",
       "        0.00642482, 0.00908266, 0.15358683, 0.00286768, 0.00732306,\n",
       "        0.15355974, 0.00508988, 0.00508586, 0.16424427, 0.00487526,\n",
       "        0.00916084, 0.09448852, 0.01262523, 0.00769772, 0.14900475,\n",
       "        0.00657155, 0.00663037, 0.10174323, 0.0031777 , 0.00938883,\n",
       "        0.11230977, 0.00710382, 0.01069223, 0.00371205, 0.00828135,\n",
       "        0.00534086, 0.00522706, 0.00827813, 0.00391185, 0.00385912,\n",
       "        0.00373216, 0.0043215 , 0.0045998 , 0.00439957, 0.00683171,\n",
       "        0.00613144, 0.01427265, 0.00763583, 0.00433301, 0.00592878,\n",
       "        0.00578521, 0.00408333, 0.00484034, 0.00434143, 0.00385752,\n",
       "        0.00414735, 0.00875178, 0.00840932, 0.00714206, 0.00346483,\n",
       "        0.00412137, 0.00278074, 0.00602   , 0.00441505, 0.00444085,\n",
       "        0.00379015, 0.00528565, 0.00460715, 0.00753456, 0.01168188,\n",
       "        0.00701783, 0.0069858 , 0.00781717, 0.00985273, 0.00492332,\n",
       "        0.00564867, 0.00466047, 0.00412673, 0.0049756 , 0.00442513,\n",
       "        0.00591116, 0.00529801, 0.00448866, 0.00535001, 0.0042542 ,\n",
       "        0.00335481, 0.00423839, 0.0045615 , 0.00446629, 0.00327698,\n",
       "        0.00490837, 0.00446301, 0.00407985, 0.00641979, 0.00604001,\n",
       "        0.00550393, 0.00399945, 0.00811556, 0.00498565, 0.00466209,\n",
       "        0.00453838, 0.00445525, 0.00395217, 0.00486245, 0.00671613,\n",
       "        0.00794012, 0.00614894, 0.00340782, 0.00414069, 0.00420302,\n",
       "        0.00537437, 0.00449972, 0.00433992, 0.0031887 , 0.00360205,\n",
       "        0.00466359, 0.00614527, 0.00549362, 0.00486437, 0.00517833,\n",
       "        0.00461129, 0.00381686, 0.0037682 , 0.00434022, 0.00358622,\n",
       "        0.00426156, 0.00527693, 0.00478638]),\n",
       " 'rank_test_roc_auc_ovo': array([ 49, 103,  37,  18,  16,  89, 121,  46, 107, 134, 125, 111,  99,\n",
       "         96,  63,  22,  11,  20,  32,  74,  73, 100, 130, 105,  66, 109,\n",
       "          8, 104,  81,  64,  27, 102,  40, 129, 114, 122,  30,  67,  61,\n",
       "         17,  14,  12,   2,  21,   1,  87,  33,   7,  43,  13,  42,   5,\n",
       "          4,  34,  35,  48,  36,  79, 124, 108,  69,  23,  55, 101,  45,\n",
       "         10, 118, 132,  52,  19,  70,  44,  47,  26,  57,  68,  29,  53,\n",
       "         62,  97,  95, 115, 119,  58,   6,   3,  80,  78,   9,  24,  91,\n",
       "         38,  77,  39, 116,  15, 252, 266, 256, 244, 284, 238, 247, 286,\n",
       "        234, 225, 271, 228, 246, 274, 233, 236, 267, 251, 237, 287, 242,\n",
       "        240, 278, 239, 248, 257, 254, 231, 280, 253, 245, 279, 235, 230,\n",
       "        258, 241, 226, 276, 249, 229, 277, 227, 232, 275, 250, 255, 261,\n",
       "        243, 217, 269, 218, 215, 260, 205, 204, 285, 195, 198, 268, 200,\n",
       "        221, 282, 223, 203, 288, 213, 197, 283, 199, 194, 264, 206, 222,\n",
       "        272, 220, 208, 270, 201, 211, 265, 209, 207, 259, 212, 224, 262,\n",
       "        219, 196, 281, 216, 202, 273, 187, 214, 263, 210, 192, 181, 193,\n",
       "        163, 174, 177, 147, 154, 153, 146, 149, 148, 182, 183, 190, 176,\n",
       "        165, 164, 160, 151, 159, 150, 156, 145, 188, 191, 189, 171, 172,\n",
       "        178, 166, 157, 155, 158, 161, 152, 184, 185, 186, 175, 180, 179,\n",
       "        173, 170, 169, 162, 168, 167, 140, 143, 141,  93, 113, 112,  54,\n",
       "         84,  92,  60,  83,  28, 131, 137, 142,  85, 120, 126,  75,  76,\n",
       "         56,  51,  41,  72, 144, 139, 133, 127, 117, 123,  71,  31,  98,\n",
       "         90,  25,  50, 135, 136, 138, 128, 110, 106,  88,  86,  59,  65,\n",
       "         94,  82]),\n",
       " 'split0_test_jaccard': array([0.59951456, 0.64563107, 0.6460396 , 0.62711864, 0.63569682,\n",
       "        0.63703704, 0.62102689, 0.64197531, 0.63221154, 0.61961722,\n",
       "        0.65763547, 0.63861386, 0.62254902, 0.63882064, 0.59178744,\n",
       "        0.63793103, 0.63793103, 0.63221154, 0.62682927, 0.63170732,\n",
       "        0.62682927, 0.61004785, 0.6127451 , 0.61800487, 0.62745098,\n",
       "        0.63922518, 0.63235294, 0.63390663, 0.63680387, 0.63209877,\n",
       "        0.62315271, 0.62135922, 0.63349515, 0.62529833, 0.62019231,\n",
       "        0.62195122, 0.64356436, 0.63285024, 0.6345679 , 0.64720195,\n",
       "        0.67341772, 0.62222222, 0.64588529, 0.61961722, 0.64588529,\n",
       "        0.61893204, 0.61835749, 0.62407862, 0.61425061, 0.6352657 ,\n",
       "        0.63325183, 0.64676617, 0.63414634, 0.63970588, 0.6407767 ,\n",
       "        0.61707317, 0.6453202 , 0.63106796, 0.62110312, 0.62682927,\n",
       "        0.61411765, 0.62347188, 0.61800487, 0.61124694, 0.62259615,\n",
       "        0.63260341, 0.63414634, 0.63834951, 0.64373464, 0.61611374,\n",
       "        0.63157895, 0.63285024, 0.60774818, 0.64303178, 0.62926829,\n",
       "        0.64864865, 0.6453202 , 0.62682927, 0.65508685, 0.65196078,\n",
       "        0.62773723, 0.61244019, 0.63170732, 0.64891041, 0.64878049,\n",
       "        0.64851485, 0.60981308, 0.62142857, 0.63768116, 0.61137441,\n",
       "        0.64039409, 0.66089109, 0.61138614, 0.64547677, 0.60570071,\n",
       "        0.65281174, 0.4741573 , 0.352     , 0.47640449, 0.46031746,\n",
       "        0.352     , 0.4738041 , 0.4785553 , 0.352     , 0.475     ,\n",
       "        0.47954545, 0.351     , 0.47737557, 0.39767442, 0.2755102 ,\n",
       "        0.46136364, 0.46241458, 0.2765685 , 0.4717833 , 0.47285068,\n",
       "        0.352     , 0.47297297, 0.475     , 0.351     , 0.47404063,\n",
       "        0.45330296, 0.42915811, 0.44520548, 0.46832579, 0.00568182,\n",
       "        0.45454545, 0.47404063, 0.352     , 0.47045455, 0.47737557,\n",
       "        0.02272727, 0.47737557, 0.46832579, 0.352     , 0.4       ,\n",
       "        0.4738041 , 0.30105263, 0.47835991, 0.47045455, 0.315     ,\n",
       "        0.47629797, 0.46712018, 0.        , 0.46952596, 0.52783964,\n",
       "        0.39304813, 0.53521127, 0.5259434 , 0.29228243, 0.53775744,\n",
       "        0.51843318, 0.1609589 , 0.52816901, 0.54481132, 0.24250326,\n",
       "        0.53488372, 0.50808314, 0.07484076, 0.54237288, 0.53773585,\n",
       "        0.15049505, 0.51162791, 0.54166667, 0.3551797 , 0.57074341,\n",
       "        0.53349282, 0.26705653, 0.50804598, 0.4953271 , 0.00261097,\n",
       "        0.51740139, 0.53530752, 0.37260274, 0.53379953, 0.53379953,\n",
       "        0.23613312, 0.52668213, 0.51294118, 0.47906977, 0.54332553,\n",
       "        0.48960739, 0.36439499, 0.54861111, 0.50232558, 0.20816327,\n",
       "        0.52436195, 0.50465116, 0.27021696, 0.52336449, 0.53953488,\n",
       "        0.34166667, 0.53703704, 0.5       , 0.54086538, 0.52913753,\n",
       "        0.6       , 0.57004831, 0.56144578, 0.59405941, 0.5990099 ,\n",
       "        0.5955335 , 0.5955335 , 0.591133  , 0.59801489, 0.5404157 ,\n",
       "        0.5821256 , 0.55813953, 0.59313725, 0.6084788 , 0.60401003,\n",
       "        0.59405941, 0.58271605, 0.58679707, 0.59701493, 0.59852217,\n",
       "        0.59801489, 0.56763285, 0.5255814 , 0.50691244, 0.59801489,\n",
       "        0.5863747 , 0.57142857, 0.59452736, 0.60987654, 0.59057072,\n",
       "        0.5990099 , 0.6039604 , 0.5990099 , 0.52054795, 0.57664234,\n",
       "        0.58252427, 0.6       , 0.51990632, 0.60349127, 0.58924205,\n",
       "        0.59506173, 0.6039604 , 0.5920398 , 0.59405941, 0.59259259,\n",
       "        0.59569378, 0.6004902 , 0.61519608, 0.62899263, 0.61975309,\n",
       "        0.62068966, 0.63209877, 0.62287105, 0.63658537, 0.63480392,\n",
       "        0.63080685, 0.64197531, 0.60148515, 0.60583942, 0.59079903,\n",
       "        0.61538462, 0.62102689, 0.60987654, 0.62469136, 0.63902439,\n",
       "        0.63480392, 0.63950617, 0.64427861, 0.62871287, 0.58313253,\n",
       "        0.59012346, 0.62043796, 0.62407862, 0.61481481, 0.62254902,\n",
       "        0.62871287, 0.6345679 , 0.63703704, 0.63970588, 0.62871287,\n",
       "        0.63882064, 0.61650485, 0.60532688, 0.60049628, 0.61881188,\n",
       "        0.6019656 , 0.63260341, 0.64285714, 0.62591687, 0.62990196,\n",
       "        0.63300493, 0.62347188, 0.62899263]),\n",
       " 'split1_test_jaccard': array([0.63888889, 0.62162162, 0.62121212, 0.63882064, 0.66835443,\n",
       "        0.61042184, 0.65920398, 0.62962963, 0.65914787, 0.65087282,\n",
       "        0.64588529, 0.62621359, 0.63316583, 0.62376238, 0.63659148,\n",
       "        0.64444444, 0.63819095, 0.6525    , 0.66341463, 0.65336658,\n",
       "        0.67839196, 0.65185185, 0.63118812, 0.65281174, 0.6325    ,\n",
       "        0.63092269, 0.62972292, 0.61042184, 0.65601966, 0.64646465,\n",
       "        0.64039409, 0.64390244, 0.66084788, 0.64108911, 0.64019851,\n",
       "        0.65994962, 0.65326633, 0.63065327, 0.63888889, 0.65413534,\n",
       "        0.6540404 , 0.64483627, 0.67839196, 0.6489899 , 0.65586035,\n",
       "        0.65260546, 0.64      , 0.67091837, 0.64102564, 0.64141414,\n",
       "        0.65217391, 0.64572864, 0.64661654, 0.6375    , 0.63275434,\n",
       "        0.6691358 , 0.67164179, 0.64019851, 0.64663462, 0.6495098 ,\n",
       "        0.6281407 , 0.64795918, 0.62531017, 0.645     , 0.65316456,\n",
       "        0.64556962, 0.6345679 , 0.64039409, 0.66089109, 0.63809524,\n",
       "        0.61463415, 0.6617284 , 0.62060302, 0.63909774, 0.60891089,\n",
       "        0.66498741, 0.6475    , 0.62043796, 0.66911765, 0.65450122,\n",
       "        0.64444444, 0.63793103, 0.64925373, 0.64197531, 0.64795918,\n",
       "        0.64810127, 0.63867684, 0.67581047, 0.64373464, 0.66157761,\n",
       "        0.64540816, 0.68090452, 0.65441176, 0.66421569, 0.64938272,\n",
       "        0.66165414, 0.47880299, 0.353     , 0.49382716, 0.49385749,\n",
       "        0.27319062, 0.49877751, 0.49135802, 0.        , 0.5       ,\n",
       "        0.48648649, 0.15744681, 0.4963145 , 0.49514563, 0.353     ,\n",
       "        0.49148418, 0.49635036, 0.1388102 , 0.49756098, 0.48529412,\n",
       "        0.353     , 0.48894349, 0.49509804, 0.        , 0.49627792,\n",
       "        0.39637306, 0.353     , 0.48379052, 0.48529412, 0.        ,\n",
       "        0.49391727, 0.47804878, 0.04519774, 0.49512195, 0.49382716,\n",
       "        0.35270541, 0.49877751, 0.48379052, 0.        , 0.49261084,\n",
       "        0.49379653, 0.        , 0.48899756, 0.49876543, 0.353     ,\n",
       "        0.49633252, 0.49382716, 0.22632794, 0.48514851, 0.51318945,\n",
       "        0.13299663, 0.53753027, 0.513382  , 0.31658291, 0.54390244,\n",
       "        0.55205811, 0.25316456, 0.57777778, 0.5377129 , 0.16788321,\n",
       "        0.58880779, 0.54761905, 0.35131579, 0.55      , 0.54146341,\n",
       "        0.28762542, 0.56683168, 0.54950495, 0.35916359, 0.57530864,\n",
       "        0.56127451, 0.23372287, 0.54589372, 0.55263158, 0.00564972,\n",
       "        0.52109181, 0.53605769, 0.36923077, 0.56730769, 0.53253012,\n",
       "        0.28102564, 0.53398058, 0.55061728, 0.38522427, 0.56479218,\n",
       "        0.55069124, 0.28488372, 0.55086849, 0.54166667, 0.37087912,\n",
       "        0.53253012, 0.55825243, 0.30630631, 0.57711443, 0.56265356,\n",
       "        0.18702866, 0.53041363, 0.57828283, 0.58080808, 0.59701493,\n",
       "        0.60945274, 0.58690176, 0.6070529 , 0.5975    , 0.60301508,\n",
       "        0.59398496, 0.60199005, 0.60447761, 0.59950249, 0.6010101 ,\n",
       "        0.57070707, 0.585     , 0.59697733, 0.60696517, 0.59899749,\n",
       "        0.605     , 0.59296482, 0.59398496, 0.5975    , 0.59850374,\n",
       "        0.61      , 0.52592593, 0.55889724, 0.55721393, 0.59697733,\n",
       "        0.60050251, 0.59090909, 0.60493827, 0.61152882, 0.595     ,\n",
       "        0.595     , 0.60598504, 0.59850374, 0.57721519, 0.59045226,\n",
       "        0.56862745, 0.60201511, 0.57142857, 0.59343434, 0.59547739,\n",
       "        0.59697733, 0.5914787 , 0.59697733, 0.59296482, 0.60150376,\n",
       "        0.61940299, 0.61809045, 0.62621359, 0.64321608, 0.63224181,\n",
       "        0.64393939, 0.63157895, 0.64824121, 0.64050633, 0.65      ,\n",
       "        0.645     , 0.63705584, 0.62282878, 0.6075    , 0.61538462,\n",
       "        0.64572864, 0.63      , 0.66165414, 0.63523573, 0.63659148,\n",
       "        0.65326633, 0.64987406, 0.64646465, 0.63567839, 0.60659898,\n",
       "        0.62034739, 0.63316583, 0.63224181, 0.63979849, 0.6475    ,\n",
       "        0.64661654, 0.64646465, 0.64321608, 0.65151515, 0.65648855,\n",
       "        0.64735516, 0.61097257, 0.62155388, 0.615     , 0.64339152,\n",
       "        0.64321608, 0.6372796 , 0.6437659 , 0.65139949, 0.64411028,\n",
       "        0.64321608, 0.64661654, 0.63840399]),\n",
       " 'split2_test_jaccard': array([0.5990099 , 0.62406015, 0.63979849, 0.6675063 , 0.64356436,\n",
       "        0.62254902, 0.62561576, 0.64146341, 0.63930348, 0.62102689,\n",
       "        0.63797468, 0.6345679 , 0.60963855, 0.62623762, 0.64393939,\n",
       "        0.62439024, 0.65      , 0.63432836, 0.64588529, 0.64339152,\n",
       "        0.63476071, 0.6281407 , 0.60628019, 0.62407862, 0.61425061,\n",
       "        0.61845387, 0.63970588, 0.62347188, 0.65151515, 0.6475    ,\n",
       "        0.67007673, 0.64321608, 0.63771712, 0.6372549 , 0.64231738,\n",
       "        0.6426799 , 0.61845387, 0.64141414, 0.60099751, 0.63659148,\n",
       "        0.62086514, 0.65482234, 0.64556962, 0.61916462, 0.64556962,\n",
       "        0.655     , 0.6437659 , 0.61633663, 0.63771712, 0.62842893,\n",
       "        0.5825    , 0.65012407, 0.65185185, 0.61346633, 0.63680387,\n",
       "        0.64019851, 0.66834171, 0.61613692, 0.65012407, 0.62836186,\n",
       "        0.61330049, 0.65      , 0.62121212, 0.63144963, 0.62653563,\n",
       "        0.63840399, 0.61712846, 0.61916462, 0.64634146, 0.66004963,\n",
       "        0.63855422, 0.64303178, 0.62807882, 0.62102689, 0.60891089,\n",
       "        0.65994962, 0.64411028, 0.6475    , 0.61975309, 0.6426799 ,\n",
       "        0.64      , 0.62899263, 0.59759036, 0.63390663, 0.6259542 ,\n",
       "        0.64974619, 0.63092269, 0.64339152, 0.62878788, 0.63840399,\n",
       "        0.65909091, 0.65162907, 0.63027295, 0.64912281, 0.6575    ,\n",
       "        0.66501241, 0.44096386, 0.353     , 0.42512077, 0.47086247,\n",
       "        0.        , 0.44868735, 0.47222222, 0.        , 0.47906977,\n",
       "        0.46728972, 0.02472527, 0.46244131, 0.46189376, 0.02535211,\n",
       "        0.45647059, 0.47196262, 0.05398458, 0.45518868, 0.46635731,\n",
       "        0.353     , 0.46728972, 0.46838407, 0.31319911, 0.46774194,\n",
       "        0.45862884, 0.353     , 0.46666667, 0.45707657, 0.343     ,\n",
       "        0.46436782, 0.46386946, 0.17898833, 0.47113164, 0.46990741,\n",
       "        0.        , 0.45560748, 0.45990566, 0.        , 0.45990566,\n",
       "        0.46313364, 0.34279919, 0.46153846, 0.46386946, 0.        ,\n",
       "        0.46853147, 0.46117647, 0.        , 0.46948357, 0.4988345 ,\n",
       "        0.36263736, 0.5862069 , 0.5382716 , 0.3748191 , 0.52283105,\n",
       "        0.5258216 , 0.27619048, 0.56341463, 0.52926829, 0.24382208,\n",
       "        0.54457831, 0.5       , 0.15915119, 0.50970874, 0.56900726,\n",
       "        0.0723192 , 0.55690073, 0.55721393, 0.23684211, 0.52093023,\n",
       "        0.565     , 0.31334149, 0.53537736, 0.52619048, 0.01126761,\n",
       "        0.51604938, 0.49764151, 0.1986532 , 0.54433498, 0.52403846,\n",
       "        0.17445055, 0.55773956, 0.54791155, 0.22767075, 0.52311436,\n",
       "        0.51508121, 0.01416431, 0.47990544, 0.53253012, 0.32016925,\n",
       "        0.56585366, 0.5825    , 0.3562753 , 0.57356608, 0.51300236,\n",
       "        0.35294118, 0.52900232, 0.52631579, 0.57855362, 0.55365854,\n",
       "        0.59405941, 0.6       , 0.565     , 0.60344828, 0.58765432,\n",
       "        0.59359606, 0.59359606, 0.5990099 , 0.60933661, 0.56857855,\n",
       "        0.56188119, 0.52311436, 0.55860349, 0.60344828, 0.59057072,\n",
       "        0.58706468, 0.6044226 , 0.59359606, 0.6019656 , 0.59852217,\n",
       "        0.60687961, 0.55231144, 0.53448276, 0.58518519, 0.60049628,\n",
       "        0.59950249, 0.60199005, 0.58064516, 0.59753086, 0.6019656 ,\n",
       "        0.59801489, 0.5955335 , 0.61138614, 0.56683168, 0.51970443,\n",
       "        0.54975124, 0.57142857, 0.57816377, 0.55048077, 0.59      ,\n",
       "        0.58852868, 0.60099751, 0.60199005, 0.58852868, 0.5920398 ,\n",
       "        0.58910891, 0.56790123, 0.59753086, 0.60294118, 0.61386139,\n",
       "        0.625     , 0.62128713, 0.6318408 , 0.62531017, 0.617866  ,\n",
       "        0.6234414 , 0.62779156, 0.60539216, 0.61538462, 0.5995086 ,\n",
       "        0.62009804, 0.61442786, 0.60731707, 0.61442786, 0.62222222,\n",
       "        0.62842893, 0.62315271, 0.64321608, 0.62128713, 0.57352941,\n",
       "        0.60635697, 0.59605911, 0.59268293, 0.62376238, 0.61179361,\n",
       "        0.62907268, 0.62593516, 0.60945274, 0.61538462, 0.61764706,\n",
       "        0.61916462, 0.59605911, 0.59558824, 0.585     , 0.6       ,\n",
       "        0.60891089, 0.6127451 , 0.615     , 0.62222222, 0.61975309,\n",
       "        0.62935323, 0.62842893, 0.6275    ]),\n",
       " 'split3_test_jaccard': array([0.63771712, 0.60740741, 0.61290323, 0.6475    , 0.6525    ,\n",
       "        0.63523573, 0.62593516, 0.64851485, 0.61463415, 0.63432836,\n",
       "        0.61576355, 0.64146341, 0.63613861, 0.6234414 , 0.61330049,\n",
       "        0.65491184, 0.63209877, 0.65586035, 0.63432836, 0.62591687,\n",
       "        0.64039409, 0.67164179, 0.6641791 , 0.65356265, 0.63703704,\n",
       "        0.63118812, 0.6345679 , 0.63451777, 0.645     , 0.61670762,\n",
       "        0.6575    , 0.61728395, 0.61985472, 0.62439024, 0.66176471,\n",
       "        0.63793103, 0.61557789, 0.63979849, 0.60591133, 0.63390663,\n",
       "        0.61691542, 0.63771712, 0.66497462, 0.64339152, 0.67938931,\n",
       "        0.6425    , 0.62686567, 0.65      , 0.62720403, 0.62437811,\n",
       "        0.6318408 , 0.65586035, 0.64516129, 0.63546798, 0.63930348,\n",
       "        0.66666667, 0.62128713, 0.65736041, 0.66329114, 0.61633663,\n",
       "        0.65671642, 0.61728395, 0.62282878, 0.64631043, 0.63144963,\n",
       "        0.6625    , 0.60487805, 0.64676617, 0.65162907, 0.64925373,\n",
       "        0.66584158, 0.63922518, 0.62907268, 0.63037975, 0.63300493,\n",
       "        0.64705882, 0.64810127, 0.64961637, 0.63909774, 0.62282878,\n",
       "        0.66161616, 0.6691358 , 0.63221154, 0.65217391, 0.6345679 ,\n",
       "        0.6345679 , 0.62935323, 0.64160401, 0.64547677, 0.62720403,\n",
       "        0.63106796, 0.6275    , 0.64039409, 0.64837905, 0.62407862,\n",
       "        0.64108911, 0.4952381 , 0.29026217, 0.46097561, 0.49278846,\n",
       "        0.353     , 0.49880096, 0.50119904, 0.31425703, 0.50240385,\n",
       "        0.48792271, 0.33153348, 0.49760766, 0.48564593, 0.        ,\n",
       "        0.45853659, 0.49160671, 0.34134134, 0.49758454, 0.50120482,\n",
       "        0.3101712 , 0.5       , 0.5       , 0.00283286, 0.49164678,\n",
       "        0.49759615, 0.353     , 0.44987775, 0.49152542, 0.30589681,\n",
       "        0.48086124, 0.5       , 0.20918367, 0.48441247, 0.49160671,\n",
       "        0.        , 0.49403341, 0.49278846, 0.00283286, 0.49156627,\n",
       "        0.49519231, 0.        , 0.48786408, 0.48915663, 0.        ,\n",
       "        0.49029126, 0.5       , 0.10614525, 0.5       , 0.55023923,\n",
       "        0.18211382, 0.55164835, 0.5462963 , 0.38853503, 0.56265356,\n",
       "        0.58128079, 0.12209302, 0.59259259, 0.55205811, 0.16107383,\n",
       "        0.54278729, 0.59854015, 0.30788177, 0.51073986, 0.54791155,\n",
       "        0.0619469 , 0.54954955, 0.54634146, 0.25240642, 0.52068127,\n",
       "        0.54347826, 0.30153322, 0.54501217, 0.53161593, 0.36700337,\n",
       "        0.52693208, 0.54166667, 0.15942029, 0.54285714, 0.57524272,\n",
       "        0.49230769, 0.54567901, 0.54166667, 0.17021277, 0.52884615,\n",
       "        0.54285714, 0.08077994, 0.56551724, 0.60194175, 0.06971154,\n",
       "        0.51815981, 0.51807229, 0.24939759, 0.57212714, 0.54567308,\n",
       "        0.3974359 , 0.5531401 , 0.58064516, 0.56641604, 0.54117647,\n",
       "        0.62189055, 0.60349127, 0.60794045, 0.6025    , 0.61538462,\n",
       "        0.60598504, 0.62128713, 0.6119403 , 0.61442786, 0.55282555,\n",
       "        0.53937947, 0.58955224, 0.61845387, 0.60598504, 0.615     ,\n",
       "        0.6125    , 0.61481481, 0.62094763, 0.61845387, 0.60945274,\n",
       "        0.60598504, 0.59296482, 0.53995157, 0.54411765, 0.59701493,\n",
       "        0.61097257, 0.58852868, 0.60447761, 0.60545906, 0.60945274,\n",
       "        0.60696517, 0.6175    , 0.61845387, 0.57462687, 0.54074074,\n",
       "        0.56756757, 0.59798995, 0.59057072, 0.6025    , 0.59398496,\n",
       "        0.6025    , 0.6075    , 0.61346633, 0.61097257, 0.615     ,\n",
       "        0.65482234, 0.62128713, 0.61519608, 0.63432836, 0.62907268,\n",
       "        0.64160401, 0.64339152, 0.64179104, 0.64      , 0.62376238,\n",
       "        0.63224181, 0.63275434, 0.62656642, 0.63224181, 0.625     ,\n",
       "        0.63591022, 0.63092269, 0.635     , 0.64676617, 0.65162907,\n",
       "        0.64588529, 0.64070352, 0.63157895, 0.63065327, 0.61538462,\n",
       "        0.63819095, 0.63291139, 0.62593516, 0.63      , 0.63523573,\n",
       "        0.6318408 , 0.63291139, 0.63092269, 0.62406015, 0.64824121,\n",
       "        0.63909774, 0.635     , 0.63092269, 0.63659148, 0.63092269,\n",
       "        0.64321608, 0.63909774, 0.645     , 0.64646465, 0.6375    ,\n",
       "        0.64556962, 0.6525    , 0.63054187]),\n",
       " 'split4_test_jaccard': array([0.65561224, 0.64213198, 0.64231738, 0.65920398, 0.63451777,\n",
       "        0.63636364, 0.64019851, 0.62842893, 0.62962963, 0.61463415,\n",
       "        0.65239295, 0.62254902, 0.64961637, 0.635     , 0.6489899 ,\n",
       "        0.63544304, 0.65239295, 0.65151515, 0.66498741, 0.65994962,\n",
       "        0.65413534, 0.66084788, 0.63196126, 0.64720812, 0.64615385,\n",
       "        0.6437659 , 0.64631043, 0.66329114, 0.62656642, 0.64141414,\n",
       "        0.64824121, 0.62531646, 0.63979849, 0.60933661, 0.61519608,\n",
       "        0.64572864, 0.62406015, 0.64631043, 0.64213198, 0.63705584,\n",
       "        0.65984655, 0.64735516, 0.64141414, 0.645     , 0.64122137,\n",
       "        0.65063291, 0.64631043, 0.66329114, 0.64070352, 0.66066838,\n",
       "        0.63959391, 0.64646465, 0.65671642, 0.63544304, 0.6566416 ,\n",
       "        0.62779156, 0.60199005, 0.64141414, 0.6566416 , 0.63170732,\n",
       "        0.6325    , 0.64      , 0.64267352, 0.6127451 , 0.66835443,\n",
       "        0.6234414 , 0.61654135, 0.6372549 , 0.67091837, 0.65413534,\n",
       "        0.64356436, 0.67412935, 0.64781491, 0.64974619, 0.65139949,\n",
       "        0.6084788 , 0.63659148, 0.65914787, 0.64871795, 0.62222222,\n",
       "        0.6751269 , 0.62530414, 0.65835411, 0.60243902, 0.63979849,\n",
       "        0.60837438, 0.60945274, 0.61460957, 0.65174129, 0.65736041,\n",
       "        0.65561224, 0.6325    , 0.63092269, 0.6345679 , 0.64735516,\n",
       "        0.66240409, 0.47058824, 0.353     , 0.50121065, 0.49516908,\n",
       "        0.20964361, 0.50478469, 0.50478469, 0.        , 0.50361446,\n",
       "        0.50241546, 0.        , 0.50724638, 0.50721154, 0.353     ,\n",
       "        0.50240385, 0.49878935, 0.        , 0.50240385, 0.49759615,\n",
       "        0.        , 0.49640288, 0.50728155, 0.33874239, 0.5       ,\n",
       "        0.48557692, 0.353     , 0.4939759 , 0.49144254, 0.03773585,\n",
       "        0.5059952 , 0.50605327, 0.        , 0.5       , 0.49636804,\n",
       "        0.        , 0.48792271, 0.46859903, 0.        , 0.50241546,\n",
       "        0.50359712, 0.36612022, 0.50847458, 0.50240385, 0.1798419 ,\n",
       "        0.49880096, 0.50239234, 0.42645074, 0.49640288, 0.56900726,\n",
       "        0.33981576, 0.56896552, 0.57211538, 0.30751174, 0.56049383,\n",
       "        0.57881773, 0.07103825, 0.57177033, 0.58150852, 0.33076075,\n",
       "        0.57004831, 0.59124088, 0.15671642, 0.58560794, 0.55717762,\n",
       "        0.3020214 , 0.53658537, 0.57317073, 0.32963374, 0.58679707,\n",
       "        0.58765432, 0.27727273, 0.59213759, 0.55474453, 0.07709251,\n",
       "        0.55825243, 0.58374384, 0.27204503, 0.56683168, 0.57281553,\n",
       "        0.22508591, 0.59124088, 0.55769231, 0.44360902, 0.56447689,\n",
       "        0.55961071, 0.35477387, 0.55421687, 0.59852217, 0.11328671,\n",
       "        0.5620438 , 0.5862069 , 0.09111617, 0.57462687, 0.56416465,\n",
       "        0.27324914, 0.58852868, 0.61152882, 0.58603491, 0.591133  ,\n",
       "        0.62034739, 0.61346633, 0.5925    , 0.63275434, 0.6234414 ,\n",
       "        0.625     , 0.62437811, 0.6234414 , 0.62155388, 0.60598504,\n",
       "        0.60651629, 0.56220096, 0.62155388, 0.60794045, 0.62155388,\n",
       "        0.6159601 , 0.62842893, 0.62686567, 0.625     , 0.6225    ,\n",
       "        0.62842893, 0.5875    , 0.6039604 , 0.59701493, 0.6084788 ,\n",
       "        0.62094763, 0.60297767, 0.625     , 0.62155388, 0.6175    ,\n",
       "        0.60902256, 0.61138614, 0.6234414 , 0.60913706, 0.6084788 ,\n",
       "        0.58168317, 0.6075    , 0.6025    , 0.60552764, 0.61152882,\n",
       "        0.61      , 0.62155388, 0.61654135, 0.61904762, 0.6234414 ,\n",
       "        0.60902256, 0.61654135, 0.61538462, 0.62720403, 0.63659148,\n",
       "        0.6234414 , 0.61369193, 0.63027295, 0.63408521, 0.64089776,\n",
       "        0.63157895, 0.63316583, 0.62849873, 0.60401003, 0.62626263,\n",
       "        0.63065327, 0.63408521, 0.625     , 0.62128713, 0.61691542,\n",
       "        0.62222222, 0.62162162, 0.63092269, 0.63659148, 0.6325    ,\n",
       "        0.61111111, 0.62128713, 0.63037975, 0.6281407 , 0.64339152,\n",
       "        0.61940299, 0.6281407 , 0.62935323, 0.62406015, 0.625     ,\n",
       "        0.6225    , 0.61576355, 0.63544304, 0.61323155, 0.62972292,\n",
       "        0.64572864, 0.63157895, 0.62878788, 0.62531017, 0.64070352,\n",
       "        0.63065327, 0.62311558, 0.63909774]),\n",
       " 'mean_test_jaccard': array([0.62614854, 0.62817045, 0.63245416, 0.64802991, 0.64692667,\n",
       "        0.62832145, 0.63439606, 0.63800243, 0.63498533, 0.62809589,\n",
       "        0.64193039, 0.63268156, 0.63022168, 0.62945241, 0.62692174,\n",
       "        0.63942412, 0.64212274, 0.64528308, 0.64708899, 0.64286638,\n",
       "        0.64690227, 0.64450601, 0.62927075, 0.6391332 , 0.6314785 ,\n",
       "        0.63271115, 0.63653202, 0.63312185, 0.64318102, 0.63683703,\n",
       "        0.64787295, 0.63021563, 0.63834267, 0.62747384, 0.6359338 ,\n",
       "        0.64164808, 0.63098452, 0.63820531, 0.62449952, 0.64177825,\n",
       "        0.64501705, 0.64139062, 0.65524713, 0.63523265, 0.65358519,\n",
       "        0.64393408, 0.6350599 , 0.64492495, 0.63218018, 0.63803105,\n",
       "        0.62787209, 0.64898878, 0.64689849, 0.63231665, 0.641256  ,\n",
       "        0.64417314, 0.64171618, 0.63723559, 0.64755891, 0.63054898,\n",
       "        0.62895505, 0.635743  , 0.62600589, 0.62935042, 0.64042008,\n",
       "        0.64050368, 0.62145242, 0.63638586, 0.65470293, 0.64352954,\n",
       "        0.63883465, 0.65019299, 0.62666352, 0.63665647, 0.6262989 ,\n",
       "        0.64582466, 0.64432464, 0.64070629, 0.64635466, 0.63883858,\n",
       "        0.64978495, 0.63476076, 0.63382341, 0.63588106, 0.63941205,\n",
       "        0.63786092, 0.62364372, 0.63936883, 0.64148435, 0.63918409,\n",
       "        0.64631467, 0.65068494, 0.63347753, 0.64835244, 0.63680344,\n",
       "        0.6565943 , 0.4719501 , 0.34025243, 0.47150774, 0.48259899,\n",
       "        0.23756685, 0.48497092, 0.48962386, 0.13325141, 0.49201761,\n",
       "        0.48473196, 0.17294111, 0.48819708, 0.46951426, 0.20137246,\n",
       "        0.47405177, 0.48422472, 0.16214092, 0.48490427, 0.48466062,\n",
       "        0.27363424, 0.48512181, 0.48915273, 0.20115487, 0.48594145,\n",
       "        0.45829559, 0.36823162, 0.46790326, 0.47873289, 0.13846289,\n",
       "        0.4799374 , 0.48440243, 0.15707395, 0.48422412, 0.48581698,\n",
       "        0.07508654, 0.48274333, 0.47468189, 0.07096657, 0.46929964,\n",
       "        0.48590474, 0.20199441, 0.48504692, 0.48492998, 0.16956838,\n",
       "        0.48605084, 0.48490323, 0.15178479, 0.48411218, 0.53182202,\n",
       "        0.28212234, 0.55591246, 0.53920174, 0.33594624, 0.54552766,\n",
       "        0.55128228, 0.17668904, 0.56674487, 0.54907183, 0.22920862,\n",
       "        0.55622108, 0.54909664, 0.20998119, 0.53968588, 0.55065914,\n",
       "        0.1748816 , 0.54429905, 0.55357955, 0.30664511, 0.55489212,\n",
       "        0.55817998, 0.27858537, 0.54529336, 0.53210192, 0.09272483,\n",
       "        0.52794542, 0.53888345, 0.27439041, 0.55102621, 0.54768527,\n",
       "        0.28180058, 0.55106443, 0.5421658 , 0.34115732, 0.54491102,\n",
       "        0.53156954, 0.21979937, 0.53982383, 0.55539726, 0.21644198,\n",
       "        0.54058987, 0.54993656, 0.25466247, 0.5641598 , 0.54500571,\n",
       "        0.31046431, 0.54762435, 0.55935452, 0.57053561, 0.56242409,\n",
       "        0.60915002, 0.59478154, 0.58678783, 0.6060524 , 0.60570106,\n",
       "        0.60281991, 0.60735697, 0.60600044, 0.60856715, 0.57376299,\n",
       "        0.57212193, 0.56360142, 0.59774517, 0.60656355, 0.60602642,\n",
       "        0.60291684, 0.60466944, 0.60443828, 0.60798688, 0.60550016,\n",
       "        0.60986169, 0.56526701, 0.55257467, 0.55808883, 0.60019644,\n",
       "        0.60365998, 0.59116681, 0.60191768, 0.60918983, 0.60289781,\n",
       "        0.6016025 , 0.60687301, 0.61015901, 0.56967175, 0.56720371,\n",
       "        0.57003074, 0.59578673, 0.57251388, 0.5910868 , 0.59604665,\n",
       "        0.59861355, 0.6050981 , 0.60420297, 0.60111462, 0.60491551,\n",
       "        0.61361011, 0.60486207, 0.61390425, 0.62733645, 0.62630409,\n",
       "        0.63093489, 0.62840966, 0.63500341, 0.63529742, 0.63346601,\n",
       "        0.6326138 , 0.63454858, 0.61695425, 0.61299517, 0.61139097,\n",
       "        0.62955496, 0.62609253, 0.62776955, 0.62848165, 0.63327652,\n",
       "        0.63692134, 0.63497162, 0.63929219, 0.63058463, 0.60222911,\n",
       "        0.61322598, 0.62077228, 0.62106365, 0.62730328, 0.63209398,\n",
       "        0.63112918, 0.63360396, 0.62999636, 0.63094519, 0.63521794,\n",
       "        0.63338763, 0.61486002, 0.61776695, 0.61006386, 0.6245698 ,\n",
       "        0.62860746, 0.63066096, 0.63508218, 0.63426268, 0.63439377,\n",
       "        0.63635943, 0.63482659, 0.63290725]),\n",
       " 'std_test_jaccard': array([0.0228479 , 0.0140761 , 0.01299545, 0.01432873, 0.01250285,\n",
       "        0.01041309, 0.01397509, 0.00774634, 0.01451222, 0.01312028,\n",
       "        0.01464314, 0.00721629, 0.01343404, 0.00628304, 0.02140124,\n",
       "        0.01009127, 0.00775935, 0.00993652, 0.01524217, 0.0127607 ,\n",
       "        0.01809208, 0.02241677, 0.02015525, 0.01505698, 0.0105861 ,\n",
       "        0.00864209, 0.00588858, 0.01744226, 0.0105291 , 0.01144539,\n",
       "        0.01582938, 0.01118934, 0.01322345, 0.01117359, 0.01675721,\n",
       "        0.01228369, 0.01481256, 0.00573128, 0.0174195 , 0.00765872,\n",
       "        0.02227532, 0.01103432, 0.01416303, 0.01306333, 0.0137654 ,\n",
       "        0.01318848, 0.01070411, 0.0214037 , 0.01027403, 0.01272906,\n",
       "        0.02379621, 0.00375359, 0.00757155, 0.00955518, 0.00815915,\n",
       "        0.02072513, 0.02688108, 0.01352362, 0.01440376, 0.01078392,\n",
       "        0.01580593, 0.01312963, 0.00866469, 0.01510252, 0.01751393,\n",
       "        0.01317153, 0.01140769, 0.00921953, 0.01000899, 0.01548754,\n",
       "        0.01667413, 0.01535162, 0.01303617, 0.01001625, 0.01605312,\n",
       "        0.01985415, 0.00412758, 0.01462199, 0.01649422, 0.01389016,\n",
       "        0.01668868, 0.01903996, 0.02078346, 0.01785849, 0.00855044,\n",
       "        0.01574757, 0.011868  , 0.02137309, 0.00777011, 0.01870485,\n",
       "        0.01017855, 0.01942428, 0.01407769, 0.00949231, 0.01911662,\n",
       "        0.00877427, 0.01764181, 0.02499813, 0.02708163, 0.01430244,\n",
       "        0.13031234, 0.02105076, 0.01258708, 0.16363483, 0.01235577,\n",
       "        0.01146383, 0.14763004, 0.01611081, 0.03887755, 0.15685169,\n",
       "        0.01907111, 0.01442009, 0.12941272, 0.01834335, 0.01353459,\n",
       "        0.13780398, 0.01287483, 0.0149218 , 0.1635437 , 0.01272704,\n",
       "        0.03506635, 0.03046324, 0.01883521, 0.01375164, 0.15285174,\n",
       "        0.01877478, 0.01600863, 0.12520411, 0.01207221, 0.01032842,\n",
       "        0.13908824, 0.01533984, 0.01188742, 0.140521  , 0.03748749,\n",
       "        0.01501005, 0.16624036, 0.01529551, 0.01528204, 0.14993994,\n",
       "        0.01173292, 0.01727781, 0.16072229, 0.01289428, 0.02519184,\n",
       "        0.10426496, 0.01935336, 0.01988441, 0.03838403, 0.0148016 ,\n",
       "        0.02602316, 0.07763928, 0.02151405, 0.01789086, 0.06181633,\n",
       "        0.02012158, 0.04078129, 0.10319278, 0.02813872, 0.01129837,\n",
       "        0.10270705, 0.01908399, 0.01102662, 0.05187788, 0.02831895,\n",
       "        0.01871054, 0.0278635 , 0.02713122, 0.02154866, 0.1398649 ,\n",
       "        0.01561541, 0.02735643, 0.08671806, 0.01358797, 0.02178351,\n",
       "        0.11058024, 0.02268947, 0.01549091, 0.12129248, 0.01739984,\n",
       "        0.0257373 , 0.14489594, 0.03051795, 0.03886949, 0.11562832,\n",
       "        0.01964659, 0.0331999 , 0.08945312, 0.02046263, 0.01861774,\n",
       "        0.07343468, 0.02217507, 0.04036892, 0.01616993, 0.02704448,\n",
       "        0.01094754, 0.01500691, 0.02003828, 0.01377948, 0.01253923,\n",
       "        0.01198501, 0.01297482, 0.01106071, 0.00891194, 0.0259151 ,\n",
       "        0.02219004, 0.02367421, 0.0225814 , 0.00177567, 0.01108131,\n",
       "        0.01091832, 0.01604028, 0.01620854, 0.01153792, 0.00949686,\n",
       "        0.01008989, 0.02442392, 0.02791331, 0.03182589, 0.00433431,\n",
       "        0.01164794, 0.01143034, 0.01453002, 0.007857  , 0.00970971,\n",
       "        0.0054222 , 0.00736421, 0.01006857, 0.02849625, 0.03252595,\n",
       "        0.01192612, 0.01258506, 0.02837559, 0.02072212, 0.00808872,\n",
       "        0.00723537, 0.00979959, 0.00941377, 0.01177602, 0.01244334,\n",
       "        0.02312727, 0.01983126, 0.00922103, 0.0134085 , 0.00832452,\n",
       "        0.00979068, 0.01015334, 0.00895416, 0.0055158 , 0.01155781,\n",
       "        0.00695737, 0.00473775, 0.01125303, 0.01037571, 0.01405375,\n",
       "        0.01089418, 0.00726921, 0.01973108, 0.01134529, 0.01241552,\n",
       "        0.0113153 , 0.01089411, 0.00665204, 0.00551326, 0.02143288,\n",
       "        0.01587039, 0.01350611, 0.01449157, 0.00815602, 0.01325188,\n",
       "        0.00880836, 0.00714792, 0.01138107, 0.01293686, 0.01467993,\n",
       "        0.0107522 , 0.01247178, 0.01514272, 0.01707542, 0.01454648,\n",
       "        0.01906674, 0.00938701, 0.01163729, 0.01214356, 0.0086994 ,\n",
       "        0.00670434, 0.01231552, 0.00487225]),\n",
       " 'rank_test_jaccard': array([125, 114,  90,  10,  14, 113,  76,  52,  71, 115,  31,  88, 102,\n",
       "        106, 121,  41,  30,  20,  13,  29,  15,  23, 108,  46,  94,  87,\n",
       "         59,  85,  28,  56,  11, 103,  49, 118,  62,  34,  96,  50, 129,\n",
       "         32,  21,  36,   2,  66,   4,  26,  69,  22,  92,  51, 116,   8,\n",
       "         16,  91,  37,  25,  33,  54,  12, 101, 109,  64, 127, 107,  40,\n",
       "         39, 131,  60,   3,  27,  48,   6, 122,  58, 124,  19,  24,  38,\n",
       "         17,  47,   7,  74,  79,  63,  42,  53, 130,  43,  35,  45,  18,\n",
       "          5,  81,   9,  57,   1, 251, 259, 252, 246, 269, 235, 226, 285,\n",
       "        225, 239, 279, 228, 253, 275, 250, 242, 281, 237, 240, 267, 233,\n",
       "        227, 276, 230, 256, 257, 255, 248, 284, 247, 241, 282, 243, 232,\n",
       "        287, 245, 249, 288, 254, 231, 274, 234, 236, 280, 229, 238, 283,\n",
       "        244, 222, 263, 196, 219, 260, 210, 201, 277, 187, 207, 270, 195,\n",
       "        206, 273, 218, 204, 278, 214, 199, 262, 198, 193, 265, 211, 221,\n",
       "        286, 224, 220, 266, 203, 208, 264, 202, 215, 258, 213, 223, 271,\n",
       "        217, 197, 272, 216, 205, 268, 189, 212, 261, 209, 192, 183, 191,\n",
       "        146, 176, 179, 152, 155, 166, 149, 154, 147, 180, 182, 190, 173,\n",
       "        151, 153, 164, 160, 161, 148, 156, 144, 188, 200, 194, 171, 163,\n",
       "        177, 168, 145, 165, 169, 150, 142, 185, 186, 184, 175, 181, 178,\n",
       "        174, 172, 157, 162, 170, 158, 138, 159, 137, 119, 123,  98, 112,\n",
       "         70,  65,  82,  89,  75, 135, 140, 141, 105, 126, 117, 111,  84,\n",
       "         55,  72,  44, 100, 167, 139, 133, 132, 120,  93,  95,  80, 104,\n",
       "         97,  67,  83, 136, 134, 143, 128, 110,  99,  68,  78,  77,  61,\n",
       "         73,  86]),\n",
       " 'split0_test_neg_log_loss': array([-0.37500499, -0.35743531, -0.33871577, -0.36712947, -0.35894553,\n",
       "        -0.35801001, -0.37555735, -0.36380742, -0.39539063, -0.42349999,\n",
       "        -0.3860058 , -0.38847752, -0.35260408, -0.35282007, -0.37190214,\n",
       "        -0.36642049, -0.36441833, -0.35917519, -0.38120897, -0.35862736,\n",
       "        -0.3900448 , -0.40443236, -0.40185664, -0.40229329, -0.35380324,\n",
       "        -0.35636282, -0.35822384, -0.36009002, -0.37804072, -0.3575057 ,\n",
       "        -0.38545696, -0.3740069 , -0.36119465, -0.38471995, -0.4105345 ,\n",
       "        -0.38906637, -0.34383245, -0.35232596, -0.3486749 , -0.36145493,\n",
       "        -0.34481057, -0.35767449, -0.34582465, -0.36283309, -0.34976635,\n",
       "        -0.38392777, -0.36968984, -0.35714784, -0.35573077, -0.35280138,\n",
       "        -0.3581942 , -0.35827224, -0.34801226, -0.36689209, -0.36482272,\n",
       "        -0.38714917, -0.36724383, -0.38357268, -0.3889724 , -0.39519774,\n",
       "        -0.36240331, -0.35466192, -0.35268851, -0.37828646, -0.37843543,\n",
       "        -0.35535778, -0.36805681, -0.40417499, -0.36885168, -0.39052429,\n",
       "        -0.38788512, -0.38823175, -0.37345004, -0.35566266, -0.35454449,\n",
       "        -0.35740123, -0.34850418, -0.37108767, -0.38382137, -0.3641456 ,\n",
       "        -0.39249636, -0.40268868, -0.39018606, -0.36598747, -0.35510415,\n",
       "        -0.33660059, -0.36438817, -0.37240363, -0.36630731, -0.38804369,\n",
       "        -0.38268282, -0.34872781, -0.38806668, -0.39547062, -0.40686674,\n",
       "        -0.38620541, -0.48971949, -0.70555211, -0.49838859, -0.49241689,\n",
       "        -0.83480163, -0.48747958, -0.48658022, -0.82608576, -0.48012056,\n",
       "        -0.48111792, -0.7161714 , -0.48599794, -0.51406843, -0.65547747,\n",
       "        -0.48614199, -0.49575814, -0.69722925, -0.48606357, -0.4813057 ,\n",
       "        -0.76090984, -0.48511341, -0.48814082, -0.73680511, -0.4843304 ,\n",
       "        -0.49514811, -0.67080375, -0.49620132, -0.48874847, -0.66241112,\n",
       "        -0.4961278 , -0.48606598, -0.72402751, -0.48477219, -0.48293319,\n",
       "        -0.63987403, -0.48103009, -0.48759498, -0.72515748, -0.51445718,\n",
       "        -0.48401128, -0.71065301, -0.48699043, -0.48413988, -0.74488598,\n",
       "        -0.48371761, -0.49062244, -0.65178351, -0.48634408, -0.4362338 ,\n",
       "        -0.70447795, -0.42637817, -0.42140986, -0.69805909, -0.42487757,\n",
       "        -0.43190011, -0.73583345, -0.42366652, -0.41509058, -0.83656885,\n",
       "        -0.42263898, -0.42665604, -0.77300231, -0.44633237, -0.41635249,\n",
       "        -0.7110132 , -0.43703732, -0.41718131, -0.85907976, -0.40268384,\n",
       "        -0.41051071, -0.64162872, -0.42877619, -0.4351434 , -0.71769829,\n",
       "        -0.42788705, -0.42723206, -0.6955347 , -0.41472919, -0.41753146,\n",
       "        -0.70758402, -0.42825163, -0.42622846, -0.63186443, -0.42152741,\n",
       "        -0.4529585 , -0.70764024, -0.42364156, -0.42605092, -0.66182099,\n",
       "        -0.4165802 , -0.43076335, -0.66858666, -0.4163404 , -0.41574236,\n",
       "        -0.69841393, -0.41780233, -0.42740717, -0.40209931, -0.4148353 ,\n",
       "        -0.3887508 , -0.39742847, -0.39811615, -0.37928383, -0.38468936,\n",
       "        -0.38145345, -0.38143868, -0.38037291, -0.38338021, -0.41277938,\n",
       "        -0.39587127, -0.40407566, -0.39163749, -0.38883505, -0.39077484,\n",
       "        -0.38360248, -0.38122241, -0.39023019, -0.38302259, -0.38656126,\n",
       "        -0.38078066, -0.4020701 , -0.41836952, -0.42632434, -0.38335564,\n",
       "        -0.38659175, -0.3959614 , -0.38232291, -0.38445648, -0.38442926,\n",
       "        -0.38316764, -0.38460937, -0.3827106 , -0.41870365, -0.40277543,\n",
       "        -0.39564056, -0.3879083 , -0.41649806, -0.38837473, -0.386215  ,\n",
       "        -0.38662731, -0.38887478, -0.3866343 , -0.38460825, -0.39021759,\n",
       "        -0.38568191, -0.37650947, -0.36992224, -0.35204806, -0.35882132,\n",
       "        -0.35438593, -0.3492563 , -0.35881866, -0.34709622, -0.34560518,\n",
       "        -0.35514775, -0.34256753, -0.37930274, -0.3828376 , -0.39122602,\n",
       "        -0.36139618, -0.35891585, -0.36994226, -0.34969354, -0.35416216,\n",
       "        -0.3464116 , -0.34638148, -0.34715459, -0.35010765, -0.39624439,\n",
       "        -0.39180422, -0.3736229 , -0.36058434, -0.36839201, -0.36283792,\n",
       "        -0.34962207, -0.35113706, -0.34657319, -0.34861892, -0.34893198,\n",
       "        -0.34593404, -0.37656542, -0.37768336, -0.38563022, -0.36704149,\n",
       "        -0.3724155 , -0.35813514, -0.34865155, -0.350076  , -0.34901462,\n",
       "        -0.34922275, -0.34960168, -0.35465875]),\n",
       " 'split1_test_neg_log_loss': array([-0.32650585, -0.35211912, -0.33835253, -0.33643776, -0.32577223,\n",
       "        -0.35239528, -0.34405584, -0.33975744, -0.3347466 , -0.34776234,\n",
       "        -0.35473075, -0.35018738, -0.35196463, -0.33542862, -0.34527684,\n",
       "        -0.32834147, -0.34692893, -0.33011038, -0.32862239, -0.35008487,\n",
       "        -0.34236615, -0.36384966, -0.367862  , -0.36143145, -0.33243832,\n",
       "        -0.33376481, -0.32509963, -0.36326085, -0.33338756, -0.33476426,\n",
       "        -0.35102847, -0.33815837, -0.35046747, -0.36390762, -0.37593191,\n",
       "        -0.365811  , -0.32814191, -0.35114671, -0.32617351, -0.3346752 ,\n",
       "        -0.32261071, -0.33068196, -0.32359404, -0.34200426, -0.32783595,\n",
       "        -0.35164532, -0.3401861 , -0.32500018, -0.36064752, -0.32126877,\n",
       "        -0.32275344, -0.33380812, -0.33158849, -0.340809  , -0.34552886,\n",
       "        -0.3473323 , -0.34829032, -0.35907701, -0.36342669, -0.35603169,\n",
       "        -0.33277337, -0.32353296, -0.35835545, -0.32625603, -0.3218082 ,\n",
       "        -0.34135032, -0.3312652 , -0.34203491, -0.34121132, -0.35403016,\n",
       "        -0.3667758 , -0.34904596, -0.33667731, -0.33343011, -0.35722121,\n",
       "        -0.33149417, -0.33223465, -0.34762985, -0.3337633 , -0.32255592,\n",
       "        -0.3601145 , -0.36214203, -0.35420637, -0.36149129, -0.32502224,\n",
       "        -0.32207984, -0.32624737, -0.33097671, -0.33728203, -0.32249858,\n",
       "        -0.34250025, -0.34060627, -0.31698648, -0.35098672, -0.34174974,\n",
       "        -0.33030304, -0.49057223, -0.9472287 , -0.47457792, -0.46721511,\n",
       "        -0.71657111, -0.4673715 , -0.46804317, -0.65588198, -0.46489124,\n",
       "        -0.46380035, -0.69106039, -0.46168655, -0.46585982, -0.76886131,\n",
       "        -0.46916737, -0.46586714, -0.65494787, -0.46433151, -0.46608943,\n",
       "        -0.77418868, -0.4696846 , -0.46542888, -0.66480744, -0.46774177,\n",
       "        -0.51539427, -0.70174386, -0.47720745, -0.46984735, -0.6474076 ,\n",
       "        -0.46847199, -0.46648161, -0.67743549, -0.46285293, -0.46519425,\n",
       "        -0.70122544, -0.46418719, -0.48471504, -0.65349482, -0.47856627,\n",
       "        -0.46375648, -0.6603052 , -0.46557495, -0.46316477, -0.81324419,\n",
       "        -0.46724401, -0.4668834 , -0.67991402, -0.46549094, -0.41555499,\n",
       "        -0.74471426, -0.43008392, -0.43022782, -0.78664197, -0.40920892,\n",
       "        -0.40924468, -0.71401162, -0.40561363, -0.41418749, -0.62679612,\n",
       "        -0.4021432 , -0.41814014, -0.68873429, -0.42016322, -0.41508222,\n",
       "        -0.82990834, -0.39478484, -0.41545493, -0.70920275, -0.40361727,\n",
       "        -0.41079632, -0.71375283, -0.41593769, -0.42611717, -0.65961184,\n",
       "        -0.43596642, -0.41632932, -0.68573187, -0.41211321, -0.42694297,\n",
       "        -0.90744413, -0.42450407, -0.40604749, -0.70853365, -0.41022528,\n",
       "        -0.43713534, -0.91432415, -0.40295338, -0.41232577, -0.67988852,\n",
       "        -0.43427262, -0.40885129, -0.69583922, -0.40156316, -0.4109462 ,\n",
       "        -0.88764452, -0.42665356, -0.39730079, -0.39245816, -0.38510011,\n",
       "        -0.36701012, -0.37364892, -0.37061532, -0.36757514, -0.37417789,\n",
       "        -0.37076292, -0.36742066, -0.36555988, -0.36794887, -0.38494672,\n",
       "        -0.39539903, -0.38476994, -0.38038886, -0.37154329, -0.3665483 ,\n",
       "        -0.36625645, -0.37407936, -0.36943097, -0.36829747, -0.36759907,\n",
       "        -0.3690376 , -0.41415742, -0.40079284, -0.40021986, -0.37600153,\n",
       "        -0.37666072, -0.38638143, -0.36810116, -0.36850085, -0.36958325,\n",
       "        -0.36940595, -0.36724122, -0.36835707, -0.39259921, -0.39082443,\n",
       "        -0.40326062, -0.37463624, -0.39435217, -0.38575658, -0.37466885,\n",
       "        -0.37474937, -0.37383047, -0.3731675 , -0.37524309, -0.37359282,\n",
       "        -0.3615673 , -0.35882422, -0.36659504, -0.33670703, -0.34718701,\n",
       "        -0.3440272 , -0.34448035, -0.34067927, -0.34839695, -0.34450805,\n",
       "        -0.34027581, -0.33552639, -0.35641296, -0.36426728, -0.35718062,\n",
       "        -0.33997383, -0.3481912 , -0.34236538, -0.34345141, -0.34355145,\n",
       "        -0.33702349, -0.34099074, -0.34205398, -0.34147953, -0.35991287,\n",
       "        -0.35691009, -0.3515565 , -0.34908889, -0.34815337, -0.34950657,\n",
       "        -0.336111  , -0.33414317, -0.34408918, -0.34611327, -0.33893105,\n",
       "        -0.33190228, -0.35658949, -0.35625862, -0.35820304, -0.34676437,\n",
       "        -0.34516297, -0.34884202, -0.34591244, -0.34418597, -0.34280444,\n",
       "        -0.33894042, -0.34393   , -0.33835231]),\n",
       " 'split2_test_neg_log_loss': array([-0.35877034, -0.34919489, -0.33828361, -0.32553735, -0.34981875,\n",
       "        -0.36404672, -0.35729236, -0.34874351, -0.35466921, -0.36741559,\n",
       "        -0.37701613, -0.34672615, -0.36016119, -0.36464448, -0.33451288,\n",
       "        -0.35064507, -0.33105102, -0.34015333, -0.33871534, -0.34967294,\n",
       "        -0.33901337, -0.35357092, -0.37578849, -0.37177244, -0.35416316,\n",
       "        -0.35801143, -0.34016349, -0.34623543, -0.32206628, -0.3400714 ,\n",
       "        -0.3324552 , -0.35433298, -0.34888443, -0.38039886, -0.35398991,\n",
       "        -0.3530234 , -0.34259676, -0.34245372, -0.3628094 , -0.33926021,\n",
       "        -0.34547984, -0.33396316, -0.32877271, -0.35879235, -0.34349079,\n",
       "        -0.35317204, -0.32344738, -0.35821055, -0.33955306, -0.34554878,\n",
       "        -0.35406803, -0.34306887, -0.34091358, -0.35516662, -0.37080965,\n",
       "        -0.36574006, -0.33044817, -0.38097769, -0.36744296, -0.35613131,\n",
       "        -0.35652832, -0.34003606, -0.33327587, -0.34807723, -0.35139507,\n",
       "        -0.33813023, -0.37710956, -0.3811095 , -0.36963012, -0.36507183,\n",
       "        -0.36251268, -0.35960536, -0.35051814, -0.35943394, -0.35331251,\n",
       "        -0.32860855, -0.34735821, -0.34872122, -0.36263835, -0.37480213,\n",
       "        -0.3554896 , -0.36290844, -0.41091852, -0.35110099, -0.33392259,\n",
       "        -0.32986966, -0.34239716, -0.34244043, -0.3398628 , -0.35299177,\n",
       "        -0.35049315, -0.35493495, -0.36849213, -0.34955261, -0.35309643,\n",
       "        -0.34647133, -0.50627688, -0.77361709, -0.51166017, -0.48649798,\n",
       "        -0.64636056, -0.49088014, -0.48508324, -0.64956175, -0.48113726,\n",
       "        -0.48372481, -0.68400271, -0.48273233, -0.49087151, -0.67904711,\n",
       "        -0.49784913, -0.48016125, -0.67891353, -0.49550958, -0.48674113,\n",
       "        -0.78821122, -0.48209872, -0.48248745, -0.69824233, -0.48383422,\n",
       "        -0.49394068, -0.79646614, -0.4908777 , -0.48578544, -0.75079936,\n",
       "        -0.4875871 , -0.48447794, -0.68839206, -0.48229747, -0.48204002,\n",
       "        -0.65554252, -0.48896758, -0.49168364, -0.66718788, -0.49631617,\n",
       "        -0.48846467, -0.73018182, -0.48456676, -0.48562347, -0.65166859,\n",
       "        -0.48411512, -0.48540036, -0.65031546, -0.486066  , -0.43696846,\n",
       "        -0.68467776, -0.42181776, -0.41801674, -0.68547721, -0.43544604,\n",
       "        -0.43288527, -0.74918571, -0.41634549, -0.42522391, -0.72872513,\n",
       "        -0.42461904, -0.47161559, -0.80929055, -0.43513956, -0.42791969,\n",
       "        -0.78215374, -0.42081789, -0.42182386, -0.74320996, -0.42970498,\n",
       "        -0.42072637, -0.75754415, -0.43011849, -0.43775853, -0.68176895,\n",
       "        -0.43482638, -0.4397529 , -0.75476234, -0.42112067, -0.43204662,\n",
       "        -0.81068652, -0.42636938, -0.42241933, -0.68675961, -0.42856506,\n",
       "        -0.44143014, -0.64231366, -0.46145454, -0.4253925 , -1.02948516,\n",
       "        -0.42264066, -0.41579424, -0.63446592, -0.40767111, -0.43894149,\n",
       "        -0.61226677, -0.4331465 , -0.4244028 , -0.40515808, -0.42233989,\n",
       "        -0.39277851, -0.39476583, -0.40476335, -0.38755824, -0.38785227,\n",
       "        -0.38898063, -0.38867039, -0.38837403, -0.38797872, -0.40898322,\n",
       "        -0.41041052, -0.43184766, -0.40629959, -0.39099997, -0.39429888,\n",
       "        -0.39501201, -0.38878747, -0.39001029, -0.38848616, -0.3884265 ,\n",
       "        -0.38717832, -0.42097677, -0.41936563, -0.40235231, -0.39181226,\n",
       "        -0.39431523, -0.39338172, -0.39880047, -0.39108796, -0.3904322 ,\n",
       "        -0.38971815, -0.39111152, -0.38995672, -0.41560414, -0.43063977,\n",
       "        -0.42087165, -0.40595915, -0.40422596, -0.41759379, -0.39709316,\n",
       "        -0.39867628, -0.39478391, -0.38983561, -0.39628267, -0.39074525,\n",
       "        -0.38185192, -0.38849036, -0.38403994, -0.35644907, -0.3609663 ,\n",
       "        -0.35859266, -0.35098773, -0.34989036, -0.35572897, -0.35194508,\n",
       "        -0.35253437, -0.34716274, -0.36534538, -0.36926532, -0.38436349,\n",
       "        -0.35378643, -0.36545688, -0.36908713, -0.35071705, -0.35274996,\n",
       "        -0.35115902, -0.3521829 , -0.34119631, -0.35382182, -0.38496226,\n",
       "        -0.37779027, -0.3721565 , -0.36592298, -0.35060952, -0.36051106,\n",
       "        -0.35305696, -0.3523759 , -0.36329685, -0.34850165, -0.34422452,\n",
       "        -0.35454166, -0.37067452, -0.3834725 , -0.38009671, -0.362942  ,\n",
       "        -0.35822978, -0.362381  , -0.35586019, -0.3532003 , -0.35334216,\n",
       "        -0.35105083, -0.3492823 , -0.35279541]),\n",
       " 'split3_test_neg_log_loss': array([-0.33028211, -0.3582886 , -0.3478587 , -0.34088402, -0.33829122,\n",
       "        -0.35862321, -0.37092376, -0.34459467, -0.35400052, -0.37680872,\n",
       "        -0.38436068, -0.39742716, -0.34424076, -0.3503535 , -0.34824326,\n",
       "        -0.33768904, -0.34050766, -0.34060398, -0.3605287 , -0.35723578,\n",
       "        -0.34360354, -0.33746852, -0.35559405, -0.35392814, -0.34645666,\n",
       "        -0.35275405, -0.33853831, -0.3561003 , -0.35749021, -0.3560443 ,\n",
       "        -0.35265709, -0.36051307, -0.35348304, -0.36802126, -0.33469205,\n",
       "        -0.35698478, -0.34389178, -0.34884353, -0.3562531 , -0.34769328,\n",
       "        -0.34703756, -0.33537362, -0.32123053, -0.33698939, -0.32323756,\n",
       "        -0.34217042, -0.35742088, -0.32905452, -0.33757292, -0.34828912,\n",
       "        -0.3508127 , -0.33093048, -0.34014072, -0.33872678, -0.3479882 ,\n",
       "        -0.33018706, -0.35499255, -0.36454504, -0.37147027, -0.37607845,\n",
       "        -0.34218861, -0.35185179, -0.34434751, -0.33044359, -0.36237865,\n",
       "        -0.33279055, -0.38939164, -0.36054089, -0.3277206 , -0.35699658,\n",
       "        -0.34123745, -0.36668484, -0.33718496, -0.34620382, -0.34500822,\n",
       "        -0.33993409, -0.3419792 , -0.34099469, -0.34658617, -0.35803149,\n",
       "        -0.35464936, -0.34578096, -0.37645188, -0.36868608, -0.33896899,\n",
       "        -0.33947325, -0.34622707, -0.34970909, -0.33568595, -0.34003372,\n",
       "        -0.34951319, -0.35452737, -0.34616205, -0.3483814 , -0.37477566,\n",
       "        -0.3473325 , -0.47015151, -0.68315538, -0.48849947, -0.46412145,\n",
       "        -0.85140429, -0.46453972, -0.46143912, -0.75706904, -0.46587281,\n",
       "        -0.46275314, -0.72219407, -0.45844422, -0.48064897, -0.66785802,\n",
       "        -0.49538985, -0.46818864, -0.71884076, -0.4684    , -0.46188516,\n",
       "        -0.74144537, -0.45908493, -0.45934287, -0.6658711 , -0.4595917 ,\n",
       "        -0.47327326, -0.71714805, -0.48997641, -0.46549008, -0.70454793,\n",
       "        -0.46513529, -0.46888298, -0.68406312, -0.46387426, -0.46123966,\n",
       "        -0.64047549, -0.46040148, -0.46807784, -0.6581886 , -0.47288991,\n",
       "        -0.46539046, -0.64631512, -0.47055986, -0.4612065 , -0.65230015,\n",
       "        -0.46788667, -0.46193659, -0.67407346, -0.45913612, -0.41450658,\n",
       "        -0.77854787, -0.43626237, -0.42346927, -0.68818256, -0.39979848,\n",
       "        -0.39533707, -0.73454233, -0.40027364, -0.3993844 , -0.65459833,\n",
       "        -0.4051513 , -0.41718438, -0.53030073, -0.44234175, -0.40392618,\n",
       "        -0.73238831, -0.41768065, -0.40756493, -0.8707771 , -0.42096033,\n",
       "        -0.40651286, -0.69558189, -0.40727521, -0.43650301, -0.75705042,\n",
       "        -0.42269711, -0.40297795, -0.85232276, -0.4150935 , -0.40259262,\n",
       "        -0.51364415, -0.40709606, -0.40894949, -0.79210964, -0.41879267,\n",
       "        -0.4139717 , -0.6229675 , -0.42724881, -0.39764372, -0.7624876 ,\n",
       "        -0.42631759, -0.42269796, -0.81165401, -0.3971491 , -0.40685161,\n",
       "        -0.71111359, -0.41076162, -0.39379125, -0.39094347, -0.4065763 ,\n",
       "        -0.37005286, -0.37642962, -0.37291215, -0.37191837, -0.367352  ,\n",
       "        -0.37138002, -0.36710978, -0.36936921, -0.36634707, -0.39917707,\n",
       "        -0.40799071, -0.38774754, -0.36719798, -0.37141911, -0.37025315,\n",
       "        -0.36883876, -0.36668538, -0.3677775 , -0.36779213, -0.37393834,\n",
       "        -0.36612073, -0.38497723, -0.40333449, -0.40025215, -0.37981309,\n",
       "        -0.37511833, -0.38340767, -0.37312406, -0.37178368, -0.36950319,\n",
       "        -0.37378192, -0.36738516, -0.36784363, -0.39614667, -0.40069506,\n",
       "        -0.39635199, -0.37749317, -0.38350528, -0.3748664 , -0.37534158,\n",
       "        -0.37251399, -0.37524198, -0.37138501, -0.37231771, -0.37135893,\n",
       "        -0.35329013, -0.36414359, -0.35960475, -0.34824153, -0.34227866,\n",
       "        -0.34758142, -0.33838044, -0.34174472, -0.33917818, -0.33904629,\n",
       "        -0.34268925, -0.33869511, -0.35609466, -0.35138769, -0.37061237,\n",
       "        -0.33846736, -0.34625403, -0.33788503, -0.33774148, -0.33833706,\n",
       "        -0.34281863, -0.33403261, -0.3434167 , -0.33771701, -0.36977015,\n",
       "        -0.35759473, -0.34848108, -0.35736295, -0.35122098, -0.34495473,\n",
       "        -0.33738214, -0.33753759, -0.34495541, -0.34878173, -0.33753152,\n",
       "        -0.34187558, -0.35878711, -0.3617346 , -0.36389547, -0.35395582,\n",
       "        -0.35219597, -0.34203856, -0.34509425, -0.34364352, -0.34036712,\n",
       "        -0.34112301, -0.34693221, -0.34196657]),\n",
       " 'split4_test_neg_log_loss': array([-0.34065796, -0.35082932, -0.35407905, -0.34303387, -0.3662968 ,\n",
       "        -0.33565058, -0.36342173, -0.40094812, -0.38848489, -0.43937688,\n",
       "        -0.37175804, -0.36493948, -0.34250958, -0.34659711, -0.3395058 ,\n",
       "        -0.34076708, -0.34494129, -0.34515159, -0.34127932, -0.37170114,\n",
       "        -0.37733556, -0.36479639, -0.37317353, -0.38273833, -0.33433187,\n",
       "        -0.36079737, -0.33322356, -0.32483474, -0.37517942, -0.35126777,\n",
       "        -0.34838193, -0.36254751, -0.35810816, -0.40005191, -0.38376871,\n",
       "        -0.36314197, -0.35716242, -0.33395678, -0.33427913, -0.32985026,\n",
       "        -0.34490944, -0.34811323, -0.35479993, -0.34445234, -0.34157022,\n",
       "        -0.36023162, -0.35666012, -0.35594616, -0.35163774, -0.33393637,\n",
       "        -0.33187956, -0.3482503 , -0.35690263, -0.36069399, -0.36085568,\n",
       "        -0.34936704, -0.38486307, -0.37444393, -0.39263636, -0.39504171,\n",
       "        -0.355759  , -0.34463457, -0.351875  , -0.38244597, -0.33565414,\n",
       "        -0.33861261, -0.36775519, -0.41650084, -0.3563205 , -0.32472085,\n",
       "        -0.37052606, -0.36351459, -0.34239814, -0.34242357, -0.33424518,\n",
       "        -0.38779318, -0.37998219, -0.36130938, -0.34212049, -0.38497189,\n",
       "        -0.35974545, -0.38964373, -0.37321129, -0.38416196, -0.34590318,\n",
       "        -0.36131188, -0.38031744, -0.37045494, -0.34637502, -0.33144156,\n",
       "        -0.36843719, -0.38174735, -0.36509475, -0.38285867, -0.3544094 ,\n",
       "        -0.37019387, -0.47781207, -0.74527843, -0.46986686, -0.46785991,\n",
       "        -0.68276719, -0.46597108, -0.45951962, -0.67950755, -0.46291944,\n",
       "        -0.46097881, -0.64843729, -0.46060815, -0.46666962, -0.74951671,\n",
       "        -0.46814047, -0.46819685, -0.64614664, -0.46505719, -0.4606831 ,\n",
       "        -0.67545586, -0.46175178, -0.45941982, -0.69871864, -0.46056113,\n",
       "        -0.48287594, -0.78458256, -0.47346263, -0.47306772, -0.68596902,\n",
       "        -0.46336573, -0.46004677, -0.66330702, -0.46243531, -0.46168247,\n",
       "        -0.63455527, -0.46591823, -0.48132248, -0.65227674, -0.47098037,\n",
       "        -0.46390399, -0.69473949, -0.46424374, -0.46398445, -0.68737656,\n",
       "        -0.46766565, -0.46328039, -0.67898681, -0.46324979, -0.41627895,\n",
       "        -1.02290303, -0.41754648, -0.42074162, -0.61390502, -0.41958102,\n",
       "        -0.41184042, -0.67586956, -0.41835598, -0.4150538 , -0.81916693,\n",
       "        -0.41349471, -0.41455185, -0.82828406, -0.41045254, -0.41891189,\n",
       "        -0.8309781 , -0.43752365, -0.41352698, -0.77903721, -0.41095323,\n",
       "        -0.40670801, -0.7757257 , -0.40578829, -0.41787557, -0.65353325,\n",
       "        -0.41746763, -0.40903246, -0.65937328, -0.41340591, -0.41583155,\n",
       "        -0.7288044 , -0.41184153, -0.42633368, -0.60517554, -0.41940998,\n",
       "        -0.41631706, -0.82053574, -0.42405558, -0.40664418, -0.82882614,\n",
       "        -0.41694595, -0.40499045, -0.78855727, -0.40879552, -0.4288387 ,\n",
       "        -0.83110659, -0.40784715, -0.3888944 , -0.40305847, -0.40925604,\n",
       "        -0.37904677, -0.38047173, -0.4013559 , -0.37654826, -0.378605  ,\n",
       "        -0.37852029, -0.37928002, -0.37722157, -0.3796286 , -0.39040609,\n",
       "        -0.39176715, -0.42528381, -0.38044124, -0.38214838, -0.3790708 ,\n",
       "        -0.38153424, -0.37779577, -0.37755509, -0.3778079 , -0.38112911,\n",
       "        -0.37895219, -0.40387242, -0.38520626, -0.39947846, -0.38011193,\n",
       "        -0.3800439 , -0.38593233, -0.37881061, -0.37834489, -0.37983786,\n",
       "        -0.38061142, -0.38623062, -0.3791668 , -0.39217692, -0.39108412,\n",
       "        -0.40738342, -0.38348331, -0.38190255, -0.38813719, -0.38384248,\n",
       "        -0.38459287, -0.37940023, -0.38006005, -0.38094517, -0.38081831,\n",
       "        -0.36869505, -0.3739595 , -0.36958028, -0.36298851, -0.35105946,\n",
       "        -0.36033988, -0.36058485, -0.35997024, -0.35825426, -0.35512041,\n",
       "        -0.3623551 , -0.35757908, -0.3612949 , -0.38379772, -0.3637088 ,\n",
       "        -0.3586816 , -0.3584389 , -0.3740613 , -0.3635955 , -0.36125553,\n",
       "        -0.35988067, -0.35728935, -0.35500357, -0.36176562, -0.36742312,\n",
       "        -0.35594971, -0.37435719, -0.36165909, -0.35761506, -0.36428154,\n",
       "        -0.36205411, -0.35475738, -0.35576801, -0.35865475, -0.35102871,\n",
       "        -0.35776027, -0.38865843, -0.36549686, -0.36627309, -0.37009641,\n",
       "        -0.34759181, -0.35392139, -0.35906132, -0.3612892 , -0.35382564,\n",
       "        -0.35838051, -0.36718056, -0.36070639]),\n",
       " 'mean_test_neg_log_loss': array([-0.34624425, -0.35357345, -0.34345793, -0.34260449, -0.34782491,\n",
       "        -0.35374516, -0.36225021, -0.35957023, -0.36545837, -0.39097271,\n",
       "        -0.37477428, -0.36955154, -0.35029605, -0.34996875, -0.34788818,\n",
       "        -0.34477263, -0.34556945, -0.34303889, -0.35007094, -0.35746442,\n",
       "        -0.35847268, -0.36482357, -0.37485494, -0.37443273, -0.34423865,\n",
       "        -0.35233809, -0.33904977, -0.35010427, -0.35323284, -0.34793069,\n",
       "        -0.35399593, -0.35791176, -0.35442755, -0.37941992, -0.37178341,\n",
       "        -0.3656055 , -0.34312506, -0.34574534, -0.34563801, -0.34258677,\n",
       "        -0.34096962, -0.34116129, -0.33484437, -0.34901429, -0.33718017,\n",
       "        -0.35822944, -0.34948086, -0.34507185, -0.3490284 , -0.34036888,\n",
       "        -0.34354159, -0.342866  , -0.34351153, -0.3524577 , -0.35800102,\n",
       "        -0.35595513, -0.35716759, -0.37252327, -0.37678974, -0.37569618,\n",
       "        -0.34993052, -0.34294346, -0.34810847, -0.35310186, -0.3499343 ,\n",
       "        -0.3412483 , -0.36671568, -0.38087223, -0.35274684, -0.35826874,\n",
       "        -0.36578742, -0.3654165 , -0.34804572, -0.34743082, -0.34886632,\n",
       "        -0.34904624, -0.35001169, -0.35394856, -0.35378594, -0.36090141,\n",
       "        -0.36449905, -0.37263277, -0.38099482, -0.36628556, -0.33978423,\n",
       "        -0.33786704, -0.35191544, -0.35319696, -0.34510262, -0.34700186,\n",
       "        -0.35872532, -0.35610875, -0.35696042, -0.36545   , -0.36617959,\n",
       "        -0.35610123, -0.48690644, -0.77096634, -0.4885986 , -0.47562227,\n",
       "        -0.74638096, -0.4752484 , -0.47213307, -0.71362122, -0.47098826,\n",
       "        -0.47047501, -0.69237317, -0.46989384, -0.48362367, -0.70415212,\n",
       "        -0.48333776, -0.4756344 , -0.67921561, -0.47587237, -0.47134091,\n",
       "        -0.7480422 , -0.47154669, -0.47096397, -0.69288892, -0.47121184,\n",
       "        -0.49212645, -0.73414887, -0.4855451 , -0.47658781, -0.69022701,\n",
       "        -0.47613758, -0.47319106, -0.68744504, -0.47124643, -0.47061792,\n",
       "        -0.65433455, -0.47210091, -0.4826788 , -0.6712611 , -0.48664198,\n",
       "        -0.47310538, -0.68843893, -0.47438715, -0.47162381, -0.70989509,\n",
       "        -0.47412581, -0.47362464, -0.66701465, -0.47205739, -0.42390856,\n",
       "        -0.78706417, -0.42641774, -0.42277306, -0.69445317, -0.4177824 ,\n",
       "        -0.41624151, -0.72188853, -0.41285105, -0.41378803, -0.73317107,\n",
       "        -0.41360944, -0.4296296 , -0.72592239, -0.43088589, -0.41643849,\n",
       "        -0.77728834, -0.42156887, -0.4151104 , -0.79226136, -0.41358393,\n",
       "        -0.41105085, -0.71684666, -0.41757917, -0.43067954, -0.69393255,\n",
       "        -0.42776892, -0.41906494, -0.72954499, -0.4152925 , -0.41898905,\n",
       "        -0.73363264, -0.41961253, -0.41799569, -0.68488857, -0.41970408,\n",
       "        -0.43236255, -0.74155626, -0.42787077, -0.41361142, -0.79250168,\n",
       "        -0.4233514 , -0.41661946, -0.71982062, -0.40630386, -0.42026407,\n",
       "        -0.74810908, -0.41924223, -0.40635928, -0.3987435 , -0.40762153,\n",
       "        -0.37952781, -0.38454892, -0.38955257, -0.37657677, -0.3785353 ,\n",
       "        -0.37821947, -0.37678391, -0.37617952, -0.37705669, -0.39925849,\n",
       "        -0.40028773, -0.40674492, -0.38519303, -0.38098916, -0.38018919,\n",
       "        -0.37904879, -0.37771408, -0.37900081, -0.37708125, -0.37953086,\n",
       "        -0.3764139 , -0.40521079, -0.40541375, -0.40572542, -0.38221889,\n",
       "        -0.38254599, -0.38901291, -0.38023184, -0.37883477, -0.37875715,\n",
       "        -0.37933702, -0.37931558, -0.37760696, -0.40304612, -0.40320376,\n",
       "        -0.40470165, -0.38589603, -0.3960968 , -0.39094574, -0.38343221,\n",
       "        -0.38343196, -0.38242627, -0.38021649, -0.38187938, -0.38134658,\n",
       "        -0.37021726, -0.37238543, -0.36994845, -0.35128684, -0.35206255,\n",
       "        -0.35298542, -0.34873793, -0.35022065, -0.34973092, -0.347245  ,\n",
       "        -0.35060046, -0.34430617, -0.36369013, -0.37031112, -0.37341826,\n",
       "        -0.35046108, -0.35545137, -0.35866822, -0.3490398 , -0.35001123,\n",
       "        -0.34745868, -0.34617541, -0.34576503, -0.34897833, -0.37566256,\n",
       "        -0.36800981, -0.36403483, -0.35892365, -0.35519819, -0.35641836,\n",
       "        -0.34764526, -0.34599022, -0.35093653, -0.35013406, -0.34412956,\n",
       "        -0.34640277, -0.37025499, -0.36892919, -0.37081971, -0.36016002,\n",
       "        -0.35511921, -0.35306362, -0.35091595, -0.350479  , -0.3478708 ,\n",
       "        -0.3477435 , -0.35138535, -0.34969588]),\n",
       " 'std_test_neg_log_loss': array([0.01821913, 0.00363218, 0.00644208, 0.013667  , 0.01446421,\n",
       "        0.0097708 , 0.01103901, 0.02219962, 0.02287821, 0.03470931,\n",
       "        0.01125978, 0.02026071, 0.00636754, 0.00944922, 0.01290565,\n",
       "        0.01295623, 0.0108989 , 0.00944717, 0.01868274, 0.00799067,\n",
       "        0.02103228, 0.02211211, 0.01518337, 0.01698083, 0.00929827,\n",
       "        0.00964453, 0.01093342, 0.01387286, 0.02227285, 0.00898633,\n",
       "        0.01729691, 0.01175002, 0.00461825, 0.01284748, 0.02590321,\n",
       "        0.01256304, 0.00919684, 0.00681004, 0.0135989 , 0.01111721,\n",
       "        0.00921397, 0.01016013, 0.01317241, 0.01001119, 0.00999219,\n",
       "        0.01408001, 0.01604446, 0.0148063 , 0.00903044, 0.01140456,\n",
       "        0.01375881, 0.00990844, 0.0084835 , 0.01102503, 0.00974225,\n",
       "        0.01923831, 0.01825395, 0.00939904, 0.01177933, 0.01745957,\n",
       "        0.01083861, 0.01099783, 0.00865234, 0.02347204, 0.01982075,\n",
       "        0.00758011, 0.01940024, 0.02732828, 0.01622691, 0.02112698,\n",
       "        0.0150011 , 0.01286488, 0.01364247, 0.00932133, 0.0083723 ,\n",
       "        0.02181713, 0.01605047, 0.01079801, 0.0177162 , 0.02126855,\n",
       "        0.01416938, 0.02058334, 0.01886134, 0.01076079, 0.01024774,\n",
       "        0.01316843, 0.01867644, 0.01605205, 0.01121204, 0.02285344,\n",
       "        0.01472526, 0.01382766, 0.02400528, 0.01978661, 0.02296023,\n",
       "        0.0196979 , 0.01232883, 0.09351481, 0.01533067, 0.01151993,\n",
       "        0.08220405, 0.01146074, 0.01154655, 0.06802524, 0.00793534,\n",
       "        0.00983041, 0.0262975 , 0.01190669, 0.01783961, 0.04596079,\n",
       "        0.01261312, 0.01123933, 0.02674777, 0.0126133 , 0.01064934,\n",
       "        0.03943364, 0.01048908, 0.01205645, 0.02649456, 0.01088051,\n",
       "        0.01410552, 0.048536  , 0.00868482, 0.00909354, 0.036054  ,\n",
       "        0.01321841, 0.01029083, 0.02016654, 0.01007479, 0.00979123,\n",
       "        0.02446473, 0.01097218, 0.00805518, 0.02745304, 0.01653419,\n",
       "        0.0108299 , 0.03111769, 0.00956722, 0.0108727 , 0.06183848,\n",
       "        0.00799761, 0.01197206, 0.01319397, 0.01173032, 0.01038136,\n",
       "        0.12230247, 0.00648411, 0.00411499, 0.05499031, 0.01234914,\n",
       "        0.01433546, 0.02561057, 0.00860304, 0.00826683, 0.08437471,\n",
       "        0.00900857, 0.02138174, 0.10889946, 0.01356777, 0.00769858,\n",
       "        0.04916001, 0.01566458, 0.00466732, 0.06341778, 0.0103862 ,\n",
       "        0.00516577, 0.04741841, 0.01030047, 0.00759879, 0.03874698,\n",
       "        0.00705493, 0.01312721, 0.06886313, 0.00309812, 0.01014596,\n",
       "        0.13057145, 0.0085002 , 0.00873456, 0.06515316, 0.00587548,\n",
       "        0.01499917, 0.1105886 , 0.01887631, 0.01094256, 0.13277097,\n",
       "        0.00656438, 0.00930867, 0.06876562, 0.00655887, 0.01191245,\n",
       "        0.0986409 , 0.00950893, 0.01620874, 0.00585445, 0.01248717,\n",
       "        0.01007316, 0.0097121 , 0.01469384, 0.00679193, 0.00733125,\n",
       "        0.00676358, 0.00837192, 0.00808126, 0.00852691, 0.01058851,\n",
       "        0.00745401, 0.01910374, 0.01308816, 0.00829372, 0.01092876,\n",
       "        0.01048453, 0.00734545, 0.00966377, 0.00811594, 0.00780714,\n",
       "        0.00776844, 0.01224468, 0.01262161, 0.01034394, 0.00533306,\n",
       "        0.00708449, 0.00480003, 0.01047722, 0.0082279 , 0.00823946,\n",
       "        0.00712111, 0.01003106, 0.00850741, 0.01164252, 0.01455572,\n",
       "        0.00919301, 0.01104346, 0.01300278, 0.0142133 , 0.00820718,\n",
       "        0.00936522, 0.00811114, 0.00723082, 0.00838141, 0.00808929,\n",
       "        0.01215062, 0.01030232, 0.00796171, 0.00878637, 0.00700656,\n",
       "        0.00627604, 0.00736321, 0.00814824, 0.00676322, 0.00568351,\n",
       "        0.00814586, 0.00769313, 0.00851914, 0.01212039, 0.01267061,\n",
       "        0.00950834, 0.00718804, 0.0152992 , 0.0086486 , 0.00811394,\n",
       "        0.00773974, 0.00817289, 0.00504859, 0.00861741, 0.01312065,\n",
       "        0.01441496, 0.01150714, 0.00562831, 0.00729961, 0.00773276,\n",
       "        0.00978958, 0.00843718, 0.00745412, 0.00437168, 0.00531468,\n",
       "        0.0092327 , 0.01180629, 0.01012156, 0.01032677, 0.00862278,\n",
       "        0.00972801, 0.00710836, 0.00556502, 0.00649237, 0.00545302,\n",
       "        0.00703701, 0.00815482, 0.00829322]),\n",
       " 'rank_test_neg_log_loss': array([ 31,  82,  16,  11,  39,  83, 109, 106, 116, 179, 136, 124,  64,\n",
       "         57,  41,  22,  25,  14,  60,  97, 102, 113, 137, 135,  20,  74,\n",
       "          4,  61,  81,  42,  86,  98,  87, 157, 130, 117,  15,  27,  26,\n",
       "         10,   7,   8,   1,  48,   2, 100,  52,  23,  49,   6,  18,  12,\n",
       "         17,  75,  99,  91,  96, 132, 144, 139,  55,  13,  44,  79,  56,\n",
       "          9, 121, 163,  76, 101, 118, 114,  43,  35,  46,  51,  59,  85,\n",
       "         84, 108, 112, 133, 165, 120,   5,   3,  72,  80,  24,  33, 104,\n",
       "         93,  95, 115, 119,  92, 254, 284, 255, 244, 281, 243, 237, 271,\n",
       "        229, 226, 265, 225, 251, 269, 250, 245, 260, 246, 232, 282, 233,\n",
       "        228, 266, 230, 256, 279, 252, 248, 264, 247, 239, 262, 231, 227,\n",
       "        257, 236, 249, 259, 253, 238, 263, 242, 234, 270, 241, 240, 258,\n",
       "        235, 217, 286, 218, 215, 268, 206, 202, 274, 195, 199, 277, 197,\n",
       "        221, 275, 223, 203, 285, 214, 200, 287, 196, 194, 272, 205, 222,\n",
       "        267, 219, 209, 276, 201, 208, 278, 211, 207, 261, 212, 224, 280,\n",
       "        220, 198, 288, 216, 204, 273, 190, 213, 283, 210, 191, 181, 193,\n",
       "        158, 173, 177, 142, 150, 149, 143, 140, 145, 182, 183, 192, 174,\n",
       "        164, 160, 154, 148, 153, 146, 159, 141, 187, 188, 189, 168, 170,\n",
       "        176, 162, 152, 151, 156, 155, 147, 184, 185, 186, 175, 180, 178,\n",
       "        172, 171, 169, 161, 167, 166, 126, 131, 125,  70,  73,  77,  45,\n",
       "         63,  54,  34,  67,  21, 110, 128, 134,  65,  90, 103,  50,  58,\n",
       "         36,  30,  28,  47, 138, 122, 111, 105,  89,  94,  37,  29,  69,\n",
       "         62,  19,  32, 127, 123, 129, 107,  88,  78,  68,  66,  40,  38,\n",
       "         71,  53])}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRIAL_5_MLP.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FIVE NEG LOG LOSS RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.1,\n",
       " 'classifier__hidden_layer_sizes': (15,),\n",
       " 'classifier__learning_rate': 'constant',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL FIVE NEG LOG LOSS RESULTS ---------------------------------------------\")\n",
    "TRIAL_5_MLP.cv_results_['rank_test_neg_log_loss']\n",
    "TRIAL_5_MLP.cv_results_['params'][ np.argmin(TRIAL_5_MLP.cv_results_['rank_test_neg_log_loss']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FIVE F1 RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.1,\n",
       " 'classifier__hidden_layer_sizes': (15,),\n",
       " 'classifier__learning_rate': 'constant',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL FIVE F1 RESULTS ---------------------------------------------\")\n",
    "TRIAL_5_MLP.cv_results_['rank_test_f1_micro']\n",
    "TRIAL_5_MLP.cv_results_['params'][ np.argmin(TRIAL_5_MLP.cv_results_['rank_test_f1_micro']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FIVE ROC AUC RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.1,\n",
       " 'classifier__hidden_layer_sizes': (15,),\n",
       " 'classifier__learning_rate': 'adaptive',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL FIVE ROC AUC RESULTS ---------------------------------------------\")\n",
    "TRIAL_5_MLP.cv_results_['rank_test_roc_auc_ovo']\n",
    "TRIAL_5_MLP.cv_results_['params'][ np.argmin(TRIAL_5_MLP.cv_results_['rank_test_roc_auc_ovo']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FIVE JACCARD SCORE RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'tanh',\n",
       " 'classifier__alpha': 0.1,\n",
       " 'classifier__hidden_layer_sizes': (20,),\n",
       " 'classifier__learning_rate': 'adaptive',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL FIVE JACCARD SCORE RESULTS ---------------------------------------------\")\n",
    "TRIAL_5_MLP.cv_results_['rank_test_jaccard']\n",
    "TRIAL_5_MLP.cv_results_['params'][ np.argmin(TRIAL_5_MLP.cv_results_['rank_test_jaccard']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 5 Multilayered Perceptrons using best NEG LOG LOSS hyperparameters :0.8059586400280406\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 5 Multilayered Perceptrons using best F1 hyperparameters :0.8138100245355766\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 5 Multilayered Perceptrons using best ROC_AUC hyperparameters :0.8119172800560813\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 5 Multilayered Perceptrons using best JACCARD hyperparameters :0.810515247108307\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best NEG LOG LOSS results to train on test set, \n",
    "bestMPLT5_1 = MLPClassifier(activation = 'logistic', alpha = .1, hidden_layer_sizes = (20,), \n",
    "                          learning_rate = 'constant',solver = 'lbfgs')\n",
    "bestMPLT5_1.fit(X_train,y_train)\n",
    "y_pred5_1 = bestMPLT5_1.predict(X_test)\n",
    "print('Accuracy of Trial 5 Multilayered Perceptrons using best NEG LOG LOSS hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred5_1)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best F1 results to train on test set, \n",
    "bestMPLT5_2 = MLPClassifier(activation = 'tanh', alpha = 0.1, hidden_layer_sizes = (15,), \n",
    "                          learning_rate = 'constant',solver = 'lbfgs')\n",
    "bestMPLT5_2.fit(X_train,y_train)\n",
    "y_pred5_2 = bestMPLT5_2.predict(X_test)\n",
    "print('Accuracy of Trial 5 Multilayered Perceptrons using best F1 hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred5_2)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best ROC_AUC results to train on test set,\n",
    "bestMPLT5_3 = MLPClassifier(activation = 'logistic', alpha = 0.1, hidden_layer_sizes = (15,), \n",
    "                          learning_rate = 'adaptive',solver = 'lbfgs')\n",
    "bestMPLT5_3.fit(X_train,y_train)\n",
    "y_pred5_3 = bestMPLT5_3.predict(X_test)\n",
    "print('Accuracy of Trial 5 Multilayered Perceptrons using best ROC_AUC hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred5_3)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best JACCARD results to train on test set, \n",
    "bestMPLT5_4 = MLPClassifier(activation = 'tanh', alpha = 0.1, hidden_layer_sizes = (20,), \n",
    "                          learning_rate = 'adaptive',solver = 'lbfgs')\n",
    "bestMPLT5_4.fit(X_train,y_train)\n",
    "y_pred5_4 = bestMPLT5_4.predict(X_test)\n",
    "print('Accuracy of Trial 5 Multilayered Perceptrons using best JACCARD hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred5_4)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
