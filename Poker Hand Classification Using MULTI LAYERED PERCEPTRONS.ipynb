{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries;\n",
    "import numpy as np\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# use seaborn plotting defaults\n",
    "import seaborn as sns; sns.set_style('white')\n",
    "\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from numpy import genfromtxt\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import load_digits, make_blobs\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, plot_confusion_matrix\n",
    "from sklearn.model_selection import KFold, train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, StandardScaler \n",
    "from sklearn.preprocessing import RobustScaler, Normalizer, QuantileTransformer, PowerTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import seaborn as sns\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>1.1</th>\n",
       "      <th>11</th>\n",
       "      <th>1.2</th>\n",
       "      <th>13</th>\n",
       "      <th>1.3</th>\n",
       "      <th>12</th>\n",
       "      <th>1.4</th>\n",
       "      <th>1.5</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  10  1.1  11  1.2  13  1.3  12  1.4  1.5  9\n",
       "0  2  11    2  13    2  10    2  12    2    1  9\n",
       "1  3  12    3  11    3  13    3  10    3    1  9\n",
       "2  4  10    4  11    4   1    4  13    4   12  9\n",
       "3  4   1    4  13    4  12    4  11    4   10  9\n",
       "4  1   2    1   4    1   5    1   3    1    6  8"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pokerData = pd.read_csv('poker-hand-training-true.data')\n",
    "pokerData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Go from multi variable classification to binary by setting 2 pair, 3 of a kind and 4 of a kind to a 1, else 0\n",
    "\n",
    "pokerData.loc[pokerData['9'] == 1, '9'] = 0\n",
    "pokerData.loc[pokerData['9'] == 2, '9'] = 1\n",
    "pokerData.loc[pokerData['9'] == 3, '9'] = 1\n",
    "pokerData.loc[pokerData['9'] == 4, '9'] = 0\n",
    "pokerData.loc[pokerData['9'] == 5, '9'] = 0\n",
    "pokerData.loc[pokerData['9'] == 6, '9'] = 0\n",
    "pokerData.loc[pokerData['9'] == 7, '9'] = 1\n",
    "pokerData.loc[pokerData['9'] == 8, '9'] = 0\n",
    "pokerData.loc[pokerData['9'] == 9, '9'] = 0\n",
    "\n",
    "X = pokerData.iloc[:,0:9].values\n",
    "y = pokerData.iloc[:,10].values\n",
    "pokerData.head()\n",
    "\n",
    "\n",
    "import plotly.express as px\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size= 20000, random_state=1738)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTI LAYERED PERCEPTRON TRIAL ONE ON POKER DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    6.3s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    8.7s\n",
      "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed:    9.3s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   10.4s\n",
      "[Parallel(n_jobs=-1)]: Done 189 tasks      | elapsed:   11.5s\n",
      "[Parallel(n_jobs=-1)]: Done 210 tasks      | elapsed:   12.2s\n",
      "[Parallel(n_jobs=-1)]: Done 233 tasks      | elapsed:   13.9s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:   14.9s\n",
      "[Parallel(n_jobs=-1)]: Done 281 tasks      | elapsed:   16.6s\n",
      "[Parallel(n_jobs=-1)]: Done 306 tasks      | elapsed:   18.4s\n",
      "[Parallel(n_jobs=-1)]: Done 333 tasks      | elapsed:   19.8s\n",
      "[Parallel(n_jobs=-1)]: Done 360 tasks      | elapsed:   21.8s\n",
      "[Parallel(n_jobs=-1)]: Done 389 tasks      | elapsed:   23.2s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:   25.4s\n",
      "[Parallel(n_jobs=-1)]: Done 449 tasks      | elapsed:   26.5s\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:   28.8s\n",
      "[Parallel(n_jobs=-1)]: Done 513 tasks      | elapsed:   31.3s\n",
      "[Parallel(n_jobs=-1)]: Done 546 tasks      | elapsed:   34.3s\n",
      "[Parallel(n_jobs=-1)]: Done 581 tasks      | elapsed:   36.6s\n",
      "[Parallel(n_jobs=-1)]: Done 616 tasks      | elapsed:   38.8s\n",
      "[Parallel(n_jobs=-1)]: Done 653 tasks      | elapsed:   41.4s\n",
      "[Parallel(n_jobs=-1)]: Done 690 tasks      | elapsed:   43.5s\n",
      "[Parallel(n_jobs=-1)]: Done 729 tasks      | elapsed:   46.9s\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:   49.8s\n",
      "[Parallel(n_jobs=-1)]: Done 809 tasks      | elapsed:   52.9s\n",
      "[Parallel(n_jobs=-1)]: Done 850 tasks      | elapsed:   56.6s\n",
      "[Parallel(n_jobs=-1)]: Done 893 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 936 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 981 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1026 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1073 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1120 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1169 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1269 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1320 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1373 tasks      | elapsed:  1.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL ONE RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1440 out of 1440 | elapsed:  1.8min finished\n"
     ]
    }
   ],
   "source": [
    "ITERATIONS = 1\n",
    "for i in range(1):\n",
    "    #For reach \"trial\", randomly select 5000 data points\n",
    "    random5000DataPoints = pokerData.sample(n = 5000, replace = True)\n",
    "    #From that 5000, create x set and y set\n",
    "    xSet = random5000DataPoints.iloc[:,0:9].values\n",
    "    ySet = random5000DataPoints.iloc[:,10].values\n",
    "    \n",
    "    pipe = Pipeline([('std', StandardScaler()),('classifier', MLPClassifier(max_iter = 250))])\n",
    "\n",
    "    search_space = [\n",
    "                {\n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                {\n",
    "                 \n",
    "                  \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['sgd'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                \n",
    "                {\n",
    "                \n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['adam'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                ]\n",
    "    \n",
    "    #'precision_micro','roc_auc_ovo','jaccard','neg_log_loss'] JACCARD DOES NOT WORK FOR NON BINARY?\n",
    "    runHyperParamSearch = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=5), \n",
    "                          scoring=['recall_micro','f1_micro','roc_auc_ovo','neg_log_loss'], \n",
    "                          refit=False, verbose=10, n_jobs = -1)\n",
    "    TRIAL_1_MLP = runHyperParamSearch.fit(xSet, ySet)\n",
    "    print(\"---------------------------------TRIAL ONE RESULTS ---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.33228664, 0.41075368, 0.34039283, 0.69279799, 0.66467299,\n",
       "        0.82531109, 1.07712784, 1.11455722, 1.09644246, 1.05580897,\n",
       "        0.80459213, 1.35386539, 0.37162004, 0.31627083, 0.34549704,\n",
       "        0.6653728 , 0.62974124, 0.69049616, 0.90117788, 1.08373156,\n",
       "        1.16240034, 1.07172227, 1.038094  , 0.79648542, 0.30376287,\n",
       "        0.27543778, 0.31447077, 0.51794543, 0.52405076, 0.47440891,\n",
       "        1.11596017, 0.99295359, 0.92479343, 1.3248394 , 0.83511739,\n",
       "        1.01737642, 0.42046227, 0.27924013, 0.23540573, 0.57779784,\n",
       "        0.5381639 , 0.48671956, 0.94731727, 1.05500846, 0.88646402,\n",
       "        1.07992802, 1.213942  , 0.79448242, 0.621034  , 0.59331002,\n",
       "        0.54606838, 0.70650735, 0.7456418 , 0.66797481, 1.12486777,\n",
       "        1.20043473, 1.35826979, 1.8235672 , 1.67904196, 1.66743379,\n",
       "        0.62764316, 0.47450862, 0.45158939, 0.60492048, 0.81580319,\n",
       "        0.73683529, 1.02918639, 1.14738727, 1.11065483, 1.65322123,\n",
       "        1.56854959, 1.32974319, 0.42386484, 0.46620121, 0.4012454 ,\n",
       "        0.64235325, 0.52004676, 0.78327451, 0.92679839, 1.22185178,\n",
       "        1.22645364, 1.63790908, 1.62129421, 1.41321321, 0.44037971,\n",
       "        0.32978296, 0.38863425, 0.6312438 , 0.51844625, 0.59181113,\n",
       "        1.05420694, 0.98164468, 1.00706611, 1.64781613, 1.47566819,\n",
       "        1.27219172, 0.72231884, 1.28050022, 1.1094543 , 0.50243216,\n",
       "        2.27675824, 1.08783555, 0.52615294, 2.35022092, 1.12216535,\n",
       "        0.47810864, 2.61484861, 1.14568548, 0.85083222, 0.9826457 ,\n",
       "        1.42062249, 0.5638844 , 1.10014629, 1.1840188 , 0.49512625,\n",
       "        1.89833198, 1.19472752, 0.46910377, 1.81235871, 1.02388039,\n",
       "        0.6494585 , 0.98274522, 1.09053826, 0.50583553, 1.14868813,\n",
       "        1.01517324, 0.44127994, 2.02574239, 1.01767612, 0.43187199,\n",
       "        2.06007123, 0.99925933, 0.60752201, 0.98804941, 1.09964604,\n",
       "        0.50553498, 1.341854  , 1.03398938, 0.45158825, 2.19008346,\n",
       "        1.05971189, 0.42206359, 1.91975121, 1.10374875, 0.96512985,\n",
       "        1.04149642, 1.38789363, 0.87225013, 1.5058948 , 1.33564873,\n",
       "        0.83762045, 1.63670831, 1.63120308, 0.90738053, 1.58135948,\n",
       "        1.40080481, 0.83141541, 0.92709756, 1.44684381, 0.78357425,\n",
       "        0.78337345, 1.29301248, 0.76635985, 1.58816657, 1.472367  ,\n",
       "        0.86674585, 1.88922496, 1.62940044, 0.93710628, 1.45965505,\n",
       "        1.46806245, 0.91758966, 1.68134656, 1.45435052, 0.95841899,\n",
       "        1.86019912, 1.68725119, 0.84162431, 2.70182328, 1.58085918,\n",
       "        1.00486417, 1.03979445, 1.43983788, 0.96114588, 1.39930329,\n",
       "        1.71607571, 0.92029138, 1.46606054, 1.60528116, 0.92329412,\n",
       "        1.79654531, 1.50629501, 0.74784327, 0.91929059, 0.8735518 ,\n",
       "        0.67217832, 0.63704791, 0.6055212 , 0.64465418, 0.56768804,\n",
       "        0.61192589, 0.4372757 , 0.4612967 , 0.45368958, 0.76515808,\n",
       "        0.79208155, 0.88636274, 0.65826626, 0.56348448, 0.65306182,\n",
       "        0.5537765 , 0.5618835 , 0.58620443, 0.60802231, 0.56338449,\n",
       "        0.53696156, 0.93260217, 0.90657992, 0.87114921, 0.56999049,\n",
       "        0.59250989, 0.5391645 , 0.61202683, 0.48481708, 0.49912963,\n",
       "        0.50263267, 0.48431683, 0.46539979, 0.91829104, 0.93230195,\n",
       "        0.79308209, 0.77606792, 0.5871057 , 0.71411419, 0.63334517,\n",
       "        0.53205781, 0.56989012, 0.54256673, 0.51634417, 0.47120514,\n",
       "        0.97083483, 1.23516207, 1.34495687, 1.47656941, 1.30221872,\n",
       "        1.91995158, 1.99051213, 2.51135964, 1.99921961, 3.33847132,\n",
       "        3.42624626, 3.55715923, 1.3359489 , 1.26428733, 1.14348383,\n",
       "        1.43233204, 2.0523654 , 1.48047299, 2.75777197, 2.654283  ,\n",
       "        2.97475796, 3.7371139 , 3.47919226, 3.78615646, 1.5568387 ,\n",
       "        1.33925204, 1.30622334, 1.36467342, 1.4825747 , 1.45495114,\n",
       "        2.52036748, 2.84694881, 3.16041837, 3.23498216, 3.00218234,\n",
       "        3.22987747, 1.18551941, 1.19712958, 1.17901382, 1.49458518,\n",
       "        1.10194826, 1.33064423, 1.31813388, 1.43723602, 1.91474724,\n",
       "        1.7527071 , 2.17426939, 1.60147147]),\n",
       " 'std_fit_time': array([0.04504402, 0.07731582, 0.09898534, 0.12880099, 0.02774637,\n",
       "        0.02952754, 0.08638669, 0.0861973 , 0.10034798, 0.50341497,\n",
       "        0.6062807 , 0.03880051, 0.08974848, 0.05067742, 0.05167735,\n",
       "        0.1303541 , 0.12430284, 0.13672719, 0.13483671, 0.07969166,\n",
       "        0.03745671, 0.50362624, 0.49248172, 0.60804356, 0.01968971,\n",
       "        0.04715737, 0.04371652, 0.10195907, 0.117586  , 0.07417263,\n",
       "        0.12103414, 0.20658648, 0.16014834, 0.05440225, 0.62504457,\n",
       "        0.49358412, 0.15804088, 0.04223662, 0.04256447, 0.1162949 ,\n",
       "        0.07901394, 0.04775186, 0.15661739, 0.17319115, 0.11377125,\n",
       "        0.50811162, 0.11697232, 0.5874157 , 0.11041513, 0.11396484,\n",
       "        0.11553198, 0.12317095, 0.10360869, 0.12866863, 0.09467482,\n",
       "        0.08534937, 0.05498956, 0.14350137, 0.14916752, 0.15639219,\n",
       "        0.12587535, 0.1466177 , 0.12471955, 0.14712161, 0.11105578,\n",
       "        0.17911155, 0.17712366, 0.19145986, 0.15005825, 0.19846418,\n",
       "        0.26003444, 0.13421505, 0.09294534, 0.17586817, 0.12751201,\n",
       "        0.11689689, 0.12978074, 0.1644679 , 0.21026595, 0.1493402 ,\n",
       "        0.18416827, 0.25064112, 0.30893627, 0.15913038, 0.16436178,\n",
       "        0.0222728 , 0.12946356, 0.15561573, 0.12900033, 0.1223525 ,\n",
       "        0.1959787 , 0.14466165, 0.26194211, 0.18382915, 0.18304822,\n",
       "        0.06574292, 0.07045291, 0.94342585, 0.04215486, 0.02702345,\n",
       "        0.25997423, 0.05828688, 0.05140241, 1.02626329, 0.12527369,\n",
       "        0.03402683, 0.88132619, 0.15083953, 0.13123124, 0.66772866,\n",
       "        0.19151064, 0.04772342, 0.73707044, 0.18253415, 0.07537327,\n",
       "        0.72433271, 0.12632601, 0.03887372, 0.82459686, 0.01971729,\n",
       "        0.02838895, 0.63695849, 0.05660263, 0.01126297, 0.15015862,\n",
       "        0.02292487, 0.01115545, 0.66244716, 0.02625343, 0.01036797,\n",
       "        0.76731731, 0.01066919, 0.03699373, 0.87937152, 0.04454892,\n",
       "        0.02003067, 0.90961958, 0.01679046, 0.01267398, 0.57799896,\n",
       "        0.02648697, 0.02416358, 0.73483463, 0.04422437, 0.11949217,\n",
       "        0.75951041, 0.1641353 , 0.07282434, 0.98464685, 0.0651534 ,\n",
       "        0.0802943 , 0.47224778, 0.039857  , 0.07332933, 0.65337978,\n",
       "        0.08743034, 0.09106882, 0.74013752, 0.06512549, 0.04750006,\n",
       "        0.29144152, 0.02807107, 0.0419838 , 0.94660477, 0.10928484,\n",
       "        0.13335354, 0.9274343 , 0.09277953, 0.12338501, 0.74375605,\n",
       "        0.11228528, 0.14700697, 0.79241117, 0.16489588, 0.10191128,\n",
       "        0.83851396, 0.14406673, 0.06903107, 0.82975725, 0.10679154,\n",
       "        0.07953861, 0.95654003, 0.07614314, 0.06301586, 0.67115547,\n",
       "        0.21550864, 0.16750354, 0.52110899, 0.1641743 , 0.11863005,\n",
       "        1.05271979, 0.15634711, 0.29145422, 0.09755458, 0.25894653,\n",
       "        0.08430142, 0.13932277, 0.09307681, 0.12191066, 0.10121599,\n",
       "        0.04064921, 0.03472081, 0.03123456, 0.04149003, 0.12979355,\n",
       "        0.19756607, 0.14089088, 0.07317062, 0.11955311, 0.08574134,\n",
       "        0.05247532, 0.04096259, 0.15188071, 0.14704755, 0.10305222,\n",
       "        0.01969426, 0.2122599 , 0.16868455, 0.20358973, 0.12588723,\n",
       "        0.10025279, 0.1020318 , 0.07214765, 0.05207101, 0.05069395,\n",
       "        0.07601426, 0.08238706, 0.02955   , 0.11059858, 0.15820307,\n",
       "        0.11228093, 0.07216687, 0.14351473, 0.08270917, 0.11704066,\n",
       "        0.04185573, 0.10839705, 0.05640422, 0.08120583, 0.07789466,\n",
       "        0.12774631, 0.24946569, 0.11399951, 0.20405674, 0.11524265,\n",
       "        0.48103863, 0.69619671, 0.72997699, 0.7139759 , 0.11013376,\n",
       "        0.20957064, 0.30610075, 0.17394628, 0.31283755, 0.21128274,\n",
       "        0.23302452, 0.63308045, 0.22265092, 0.95281629, 0.6892069 ,\n",
       "        0.70475669, 0.28872402, 0.25562634, 0.22379098, 0.32131297,\n",
       "        0.24237255, 0.06350963, 0.19728551, 0.24791603, 0.22681949,\n",
       "        0.5535669 , 0.93214237, 0.47285117, 0.79209471, 0.73702707,\n",
       "        0.06652706, 0.14110893, 0.09224921, 0.11148131, 0.35745629,\n",
       "        0.07840196, 0.09816378, 0.27443677, 0.23143774, 0.47107206,\n",
       "        0.24024737, 0.50003737, 0.23145503]),\n",
       " 'mean_score_time': array([0.00760808, 0.00780649, 0.00750642, 0.01000772, 0.00840712,\n",
       "        0.00921001, 0.00950708, 0.01000891, 0.00830765, 0.00970826,\n",
       "        0.00870686, 0.01150942, 0.00920892, 0.00840735, 0.00900717,\n",
       "        0.00800591, 0.00800686, 0.00890722, 0.01311083, 0.010009  ,\n",
       "        0.01060944, 0.01050882, 0.00900693, 0.00970716, 0.01080875,\n",
       "        0.00910802, 0.00790596, 0.00870757, 0.00910835, 0.00840845,\n",
       "        0.00940862, 0.01070924, 0.00980835, 0.00900855, 0.01020923,\n",
       "        0.00910859, 0.00840607, 0.00810637, 0.00770636, 0.00810661,\n",
       "        0.00900588, 0.00820761, 0.00930696, 0.01101065, 0.01020765,\n",
       "        0.00960808, 0.00850801, 0.0097075 , 0.009308  , 0.00720778,\n",
       "        0.00760641, 0.00900774, 0.00940876, 0.00940704, 0.01080976,\n",
       "        0.01371102, 0.01050782, 0.01031065, 0.01060882, 0.00850592,\n",
       "        0.00840402, 0.00840602, 0.00750637, 0.00850782, 0.00920544,\n",
       "        0.00860729, 0.01040931, 0.01131001, 0.01090965, 0.00940905,\n",
       "        0.00960798, 0.00890703, 0.00890784, 0.0078073 , 0.00760684,\n",
       "        0.00810699, 0.00990882, 0.01230979, 0.0098073 , 0.00990953,\n",
       "        0.01020956, 0.01010656, 0.00950875, 0.00850663, 0.00910711,\n",
       "        0.00940833, 0.00820684, 0.00870733, 0.00920725, 0.008605  ,\n",
       "        0.00960712, 0.00980768, 0.00991011, 0.00950813, 0.00940776,\n",
       "        0.00810614, 0.00640574, 0.00740647, 0.00690579, 0.00700622,\n",
       "        0.01030884, 0.00660615, 0.00830693, 0.0089077 , 0.00720592,\n",
       "        0.00740676, 0.01181049, 0.00780649, 0.00720563, 0.00690546,\n",
       "        0.00670576, 0.00660582, 0.00740623, 0.00700569, 0.00720568,\n",
       "        0.00702205, 0.00690618, 0.00770645, 0.0069056 , 0.00720625,\n",
       "        0.00650549, 0.00630565, 0.00640526, 0.00660558, 0.00680561,\n",
       "        0.00660534, 0.00710554, 0.00700684, 0.00690579, 0.00690565,\n",
       "        0.00720615, 0.00850711, 0.00670576, 0.00690627, 0.00620522,\n",
       "        0.00700603, 0.0075067 , 0.00670547, 0.00860748, 0.00730596,\n",
       "        0.00680561, 0.01271076, 0.00700593, 0.00690589, 0.00680594,\n",
       "        0.0081068 , 0.00790672, 0.00950837, 0.00700569, 0.00710588,\n",
       "        0.00680594, 0.00720615, 0.00700641, 0.00770664, 0.00710626,\n",
       "        0.00860767, 0.00990882, 0.00800691, 0.00680599, 0.00720611,\n",
       "        0.00680628, 0.0072062 , 0.00700588, 0.00970836, 0.00700631,\n",
       "        0.00790639, 0.00800719, 0.01000896, 0.00680599, 0.00770655,\n",
       "        0.00820704, 0.00790663, 0.00960784, 0.00780702, 0.00750675,\n",
       "        0.00720634, 0.00800719, 0.0076067 , 0.01291146, 0.00780706,\n",
       "        0.00700688, 0.00680561, 0.0075067 , 0.00738697, 0.00920806,\n",
       "        0.00700603, 0.00690575, 0.01120987, 0.00740643, 0.0079072 ,\n",
       "        0.01010852, 0.00870752, 0.00650563, 0.00806403, 0.00740652,\n",
       "        0.007306  , 0.00840774, 0.00920749, 0.00700612, 0.00680609,\n",
       "        0.00700612, 0.00710578, 0.00700626, 0.00680609, 0.00630522,\n",
       "        0.00650511, 0.00630531, 0.00650558, 0.00660591, 0.00650525,\n",
       "        0.00730653, 0.0098084 , 0.00790734, 0.00910788, 0.00740647,\n",
       "        0.00860763, 0.00640597, 0.00660553, 0.00650554, 0.00660548,\n",
       "        0.00660591, 0.00690575, 0.00700579, 0.00730624, 0.00690546,\n",
       "        0.00690613, 0.00680575, 0.00700598, 0.00731735, 0.00720654,\n",
       "        0.00720634, 0.00690579, 0.0080061 , 0.00730634, 0.00680547,\n",
       "        0.00700555, 0.00710616, 0.00720601, 0.00670619, 0.00690575,\n",
       "        0.00640526, 0.0067059 , 0.00640554, 0.00680642, 0.00690598,\n",
       "        0.00670567, 0.00950818, 0.0085072 , 0.00740609, 0.00850768,\n",
       "        0.00880818, 0.00790653, 0.00910759, 0.00670586, 0.00630469,\n",
       "        0.00690551, 0.00720606, 0.00750642, 0.00840755, 0.00850735,\n",
       "        0.00800705, 0.01010847, 0.0118103 , 0.00880752, 0.00700583,\n",
       "        0.00660567, 0.00670586, 0.00820751, 0.00710654, 0.00760651,\n",
       "        0.00890765, 0.00930781, 0.00750604, 0.00780697, 0.0079061 ,\n",
       "        0.00750666, 0.0063055 , 0.00630546, 0.00650606, 0.00670605,\n",
       "        0.00660553, 0.00670562, 0.0076066 , 0.00720606, 0.00750532,\n",
       "        0.00710626, 0.00570464, 0.0049047 ]),\n",
       " 'std_score_time': array([1.20072784e-03, 6.78262523e-04, 1.14099200e-03, 3.52424989e-03,\n",
       "        6.63471417e-04, 6.00890117e-04, 1.05010056e-03, 8.37343939e-04,\n",
       "        6.79456934e-04, 1.16736964e-03, 3.99809443e-04, 2.95215953e-03,\n",
       "        9.27465642e-04, 5.81802263e-04, 1.58215559e-03, 1.58203504e-03,\n",
       "        8.36545400e-04, 7.99615446e-04, 3.99418406e-03, 1.14132919e-03,\n",
       "        1.53056534e-03, 1.22553248e-03, 1.04702960e-03, 1.57173025e-03,\n",
       "        4.89033419e-03, 8.60769202e-04, 2.01226411e-04, 6.78965672e-04,\n",
       "        1.28072208e-03, 3.76180174e-04, 6.64932458e-04, 9.29455478e-04,\n",
       "        5.11180185e-04, 5.48732583e-04, 1.12387006e-03, 3.74988191e-04,\n",
       "        4.91517549e-04, 6.64494379e-04, 2.44175892e-04, 3.74832273e-04,\n",
       "        8.37941213e-04, 7.49083363e-04, 3.97832342e-04, 1.67260175e-03,\n",
       "        7.49994079e-04, 5.82900093e-04, 4.48781274e-04, 8.11852090e-04,\n",
       "        1.43607741e-03, 2.45278823e-04, 3.74128349e-04, 8.95108959e-04,\n",
       "        1.15747293e-03, 2.11013994e-03, 2.45267699e-04, 7.41380789e-03,\n",
       "        8.37006215e-04, 5.09661347e-04, 1.11338331e-03, 6.31393775e-04,\n",
       "        1.15742717e-03, 1.11533097e-03, 7.08274082e-04, 4.47024183e-04,\n",
       "        1.47210309e-03, 1.15930956e-03, 1.96111930e-03, 9.28036675e-04,\n",
       "        1.71802005e-03, 5.84600403e-04, 1.02049221e-03, 5.84371756e-04,\n",
       "        1.15845983e-03, 5.11036671e-04, 3.73883896e-04, 3.74854741e-04,\n",
       "        2.08649193e-03, 4.40337443e-03, 8.71505001e-04, 5.82729961e-04,\n",
       "        9.79885614e-04, 4.91634534e-04, 1.04871159e-03, 3.17262434e-04,\n",
       "        5.84953655e-04, 2.82051390e-03, 7.49794022e-04, 8.13231709e-04,\n",
       "        1.24958964e-03, 3.76076059e-04, 3.73706331e-04, 6.78375842e-04,\n",
       "        9.20211376e-04, 1.09588874e-03, 1.31994232e-03, 8.00145008e-04,\n",
       "        2.00223951e-04, 1.20051708e-03, 1.11467986e-03, 1.04986949e-03,\n",
       "        3.49050653e-03, 2.00247786e-04, 1.36485923e-03, 2.17914276e-03,\n",
       "        6.79379076e-04, 8.01420574e-04, 4.76423047e-03, 6.00997719e-04,\n",
       "        1.03049356e-03, 4.90154658e-04, 1.16742669e-03, 2.00297570e-04,\n",
       "        1.62655967e-03, 7.75156302e-04, 5.10894253e-04, 3.17432527e-04,\n",
       "        3.74712328e-04, 7.49430755e-04, 2.00296037e-04, 6.79105052e-04,\n",
       "        3.16280411e-04, 2.45145707e-04, 3.74138863e-04, 2.00057506e-04,\n",
       "        4.00579184e-04, 1.99461810e-04, 4.90475925e-04, 4.47181522e-04,\n",
       "        5.83232898e-04, 2.00319546e-04, 2.45126043e-04, 3.25821570e-03,\n",
       "        9.28389971e-04, 1.11455127e-03, 2.45554636e-04, 6.33239039e-04,\n",
       "        8.95535255e-04, 2.45126321e-04, 1.80181674e-03, 6.78655285e-04,\n",
       "        2.45243307e-04, 1.14232004e-02, 6.32824550e-04, 5.83617293e-04,\n",
       "        6.00671967e-04, 2.71083352e-03, 3.05824155e-03, 5.75833800e-03,\n",
       "        4.47874641e-04, 9.70331566e-04, 2.45126043e-04, 4.00209810e-04,\n",
       "        4.26496120e-07, 2.45087759e-04, 1.99937963e-04, 2.01202627e-03,\n",
       "        7.05906790e-03, 2.32593007e-03, 3.99935255e-04, 5.10819216e-04,\n",
       "        2.45301163e-04, 8.72690270e-04, 4.62310777e-07, 5.15815125e-03,\n",
       "        9.36836372e-07, 7.35611329e-04, 1.14107563e-03, 2.84903538e-03,\n",
       "        4.00233393e-04, 1.69265783e-03, 2.31738877e-03, 1.24207254e-03,\n",
       "        5.46702792e-03, 8.13683455e-04, 4.47288012e-04, 2.45068165e-04,\n",
       "        8.95348824e-04, 4.90281207e-04, 6.25385217e-03, 8.72509778e-04,\n",
       "        8.95162258e-04, 4.00758311e-04, 1.09610590e-03, 3.96343260e-04,\n",
       "        3.91000844e-03, 4.47341562e-04, 3.74278820e-04, 8.41325830e-03,\n",
       "        3.74151770e-04, 5.84059330e-04, 4.46907251e-03, 2.91051692e-03,\n",
       "        5.76164530e-07, 2.50053545e-03, 1.96092930e-03, 9.28086752e-04,\n",
       "        3.10684454e-03, 3.89720058e-03, 4.47607713e-04, 2.44562122e-04,\n",
       "        2.33601546e-07, 4.89745872e-04, 3.16582119e-04, 2.45437929e-04,\n",
       "        2.45184687e-04, 3.16732764e-04, 2.44970170e-04, 3.16129680e-04,\n",
       "        2.00248354e-04, 9.65521657e-07, 8.72542795e-04, 3.57482157e-03,\n",
       "        1.49851721e-03, 2.85519494e-03, 8.00848199e-04, 2.95848490e-03,\n",
       "        2.00224462e-04, 2.00796269e-04, 3.16204968e-04, 4.90300758e-04,\n",
       "        2.00367144e-04, 5.83470257e-04, 5.48510295e-04, 8.72394980e-04,\n",
       "        2.00463759e-04, 1.99962082e-04, 2.45067747e-04, 5.48161789e-04,\n",
       "        6.93035270e-04, 1.96655795e-03, 1.69322178e-03, 8.60985140e-04,\n",
       "        1.64404977e-03, 9.27952933e-04, 4.00448129e-04, 3.16129536e-04,\n",
       "        4.90641584e-04, 6.78858786e-04, 2.45028628e-04, 3.74406312e-04,\n",
       "        2.00105132e-04, 4.00114471e-04, 2.00367371e-04, 2.45223373e-04,\n",
       "        3.74406312e-04, 2.45456882e-04, 4.25802956e-03, 1.26627091e-03,\n",
       "        3.74635713e-04, 1.04936939e-03, 1.94130461e-03, 6.63772989e-04,\n",
       "        3.65505230e-03, 2.45106944e-04, 2.45048114e-04, 5.82938475e-04,\n",
       "        7.48876569e-04, 1.04991511e-03, 2.33453429e-03, 2.25990681e-03,\n",
       "        1.51821030e-03, 2.26922613e-03, 8.36564663e-03, 1.20952734e-03,\n",
       "        6.32711353e-04, 2.00249035e-04, 2.44911874e-04, 2.42300094e-03,\n",
       "        2.00152418e-04, 1.71610931e-03, 1.39381286e-03, 1.91464676e-03,\n",
       "        1.00033285e-03, 6.79034755e-04, 9.16813040e-04, 3.16506546e-04,\n",
       "        2.45028674e-04, 2.44989633e-04, 7.00804637e-07, 4.00042601e-04,\n",
       "        1.99843619e-04, 4.00495685e-04, 9.70351264e-04, 7.48621409e-04,\n",
       "        4.47927882e-04, 8.01063313e-04, 1.20909722e-03, 8.00752732e-04]),\n",
       " 'param_classifier': masked_array(data=[MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__activation': masked_array(data=['logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__alpha': masked_array(data=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__hidden_layer_sizes': masked_array(data=[(5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__learning_rate': masked_array(data=['constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'}],\n",
       " 'split0_test_recall_micro': array([0.925, 0.929, 0.929, 0.918, 0.923, 0.922, 0.922, 0.903, 0.912,\n",
       "        0.915, 0.891, 0.897, 0.93 , 0.93 , 0.924, 0.925, 0.926, 0.918,\n",
       "        0.911, 0.915, 0.911, 0.904, 0.905, 0.905, 0.93 , 0.929, 0.929,\n",
       "        0.925, 0.927, 0.922, 0.91 , 0.915, 0.919, 0.902, 0.93 , 0.897,\n",
       "        0.93 , 0.93 , 0.93 , 0.931, 0.927, 0.927, 0.916, 0.923, 0.92 ,\n",
       "        0.93 , 0.916, 0.91 , 0.927, 0.929, 0.929, 0.922, 0.913, 0.919,\n",
       "        0.922, 0.908, 0.902, 0.879, 0.905, 0.91 , 0.931, 0.928, 0.929,\n",
       "        0.926, 0.926, 0.917, 0.906, 0.916, 0.917, 0.913, 0.89 , 0.897,\n",
       "        0.931, 0.928, 0.931, 0.929, 0.924, 0.922, 0.919, 0.913, 0.917,\n",
       "        0.893, 0.904, 0.893, 0.928, 0.93 , 0.929, 0.922, 0.929, 0.927,\n",
       "        0.922, 0.911, 0.916, 0.912, 0.904, 0.899, 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.557, 0.93 , 0.93 , 0.882, 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.542, 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.925, 0.93 ,\n",
       "        0.93 , 0.724, 0.93 , 0.93 , 0.474, 0.93 , 0.93 , 0.495, 0.93 ,\n",
       "        0.93 , 0.759, 0.93 , 0.93 , 0.406, 0.93 , 0.93 , 0.716, 0.93 ,\n",
       "        0.93 , 0.769, 0.93 , 0.93 , 0.634, 0.93 , 0.93 , 0.748, 0.93 ,\n",
       "        0.93 , 0.754, 0.93 , 0.93 , 0.511, 0.93 , 0.93 , 0.51 , 0.93 ,\n",
       "        0.93 , 0.472, 0.93 , 0.93 , 0.798, 0.93 , 0.93 , 0.676, 0.93 ,\n",
       "        0.93 , 0.548, 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ]),\n",
       " 'split1_test_recall_micro': array([0.93 , 0.93 , 0.931, 0.932, 0.927, 0.929, 0.922, 0.921, 0.919,\n",
       "        0.911, 0.93 , 0.915, 0.931, 0.93 , 0.93 , 0.927, 0.923, 0.931,\n",
       "        0.912, 0.916, 0.917, 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.929,\n",
       "        0.926, 0.927, 0.929, 0.925, 0.92 , 0.909, 0.914, 0.93 , 0.906,\n",
       "        0.931, 0.931, 0.93 , 0.929, 0.929, 0.93 , 0.922, 0.925, 0.925,\n",
       "        0.921, 0.921, 0.93 , 0.929, 0.93 , 0.931, 0.925, 0.921, 0.921,\n",
       "        0.915, 0.907, 0.915, 0.897, 0.905, 0.897, 0.93 , 0.931, 0.929,\n",
       "        0.922, 0.926, 0.931, 0.914, 0.919, 0.915, 0.886, 0.888, 0.902,\n",
       "        0.929, 0.927, 0.929, 0.93 , 0.927, 0.926, 0.918, 0.913, 0.917,\n",
       "        0.882, 0.906, 0.897, 0.929, 0.929, 0.931, 0.932, 0.925, 0.926,\n",
       "        0.924, 0.915, 0.913, 0.908, 0.917, 0.906, 0.93 , 0.07 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.928, 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.206, 0.93 , 0.93 , 0.332, 0.93 , 0.93 , 0.598, 0.93 ,\n",
       "        0.93 , 0.658, 0.93 , 0.93 , 0.797, 0.93 , 0.93 , 0.704, 0.93 ,\n",
       "        0.93 , 0.672, 0.93 , 0.93 , 0.345, 0.93 , 0.93 , 0.417, 0.93 ,\n",
       "        0.93 , 0.634, 0.93 , 0.93 , 0.536, 0.93 , 0.93 , 0.503, 0.93 ,\n",
       "        0.93 , 0.359, 0.93 , 0.93 , 0.408, 0.93 , 0.93 , 0.648, 0.93 ,\n",
       "        0.93 , 0.798, 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.931, 0.933, 0.932,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.931, 0.93 , 0.93 ,\n",
       "        0.928, 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.932, 0.932, 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ]),\n",
       " 'split2_test_recall_micro': array([0.931, 0.931, 0.924, 0.915, 0.927, 0.922, 0.903, 0.908, 0.915,\n",
       "        0.901, 0.891, 0.894, 0.93 , 0.931, 0.93 , 0.923, 0.922, 0.923,\n",
       "        0.922, 0.914, 0.903, 0.892, 0.905, 0.894, 0.93 , 0.927, 0.93 ,\n",
       "        0.921, 0.932, 0.929, 0.914, 0.912, 0.905, 0.908, 0.909, 0.911,\n",
       "        0.93 , 0.928, 0.929, 0.927, 0.928, 0.926, 0.928, 0.921, 0.926,\n",
       "        0.916, 0.915, 0.912, 0.932, 0.931, 0.931, 0.921, 0.916, 0.926,\n",
       "        0.915, 0.914, 0.9  , 0.894, 0.917, 0.91 , 0.928, 0.929, 0.931,\n",
       "        0.924, 0.919, 0.923, 0.908, 0.921, 0.924, 0.903, 0.909, 0.895,\n",
       "        0.931, 0.932, 0.93 , 0.923, 0.916, 0.923, 0.914, 0.902, 0.914,\n",
       "        0.899, 0.905, 0.903, 0.93 , 0.93 , 0.929, 0.927, 0.92 , 0.928,\n",
       "        0.92 , 0.926, 0.906, 0.906, 0.908, 0.914, 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.916, 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.913, 0.93 ,\n",
       "        0.93 , 0.152, 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.927, 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.07 , 0.93 ,\n",
       "        0.93 , 0.917, 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.779, 0.93 , 0.93 , 0.489, 0.93 , 0.93 , 0.68 , 0.93 ,\n",
       "        0.93 , 0.435, 0.93 , 0.93 , 0.266, 0.93 , 0.93 , 0.766, 0.93 ,\n",
       "        0.93 , 0.585, 0.93 , 0.93 , 0.779, 0.93 , 0.93 , 0.3  , 0.93 ,\n",
       "        0.93 , 0.718, 0.93 , 0.93 , 0.614, 0.93 , 0.93 , 0.305, 0.93 ,\n",
       "        0.93 , 0.87 , 0.93 , 0.93 , 0.591, 0.93 , 0.93 , 0.628, 0.93 ,\n",
       "        0.93 , 0.903, 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.931, 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.931, 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ]),\n",
       " 'split3_test_recall_micro': array([0.93 , 0.928, 0.929, 0.913, 0.925, 0.923, 0.912, 0.917, 0.916,\n",
       "        0.912, 0.897, 0.901, 0.928, 0.93 , 0.93 , 0.929, 0.92 , 0.925,\n",
       "        0.909, 0.918, 0.921, 0.889, 0.905, 0.9  , 0.928, 0.927, 0.927,\n",
       "        0.925, 0.925, 0.924, 0.917, 0.918, 0.916, 0.916, 0.902, 0.93 ,\n",
       "        0.93 , 0.927, 0.93 , 0.929, 0.922, 0.926, 0.914, 0.916, 0.917,\n",
       "        0.915, 0.911, 0.913, 0.927, 0.928, 0.928, 0.926, 0.922, 0.927,\n",
       "        0.91 , 0.911, 0.906, 0.908, 0.903, 0.904, 0.928, 0.93 , 0.928,\n",
       "        0.927, 0.921, 0.923, 0.905, 0.921, 0.919, 0.906, 0.904, 0.901,\n",
       "        0.925, 0.93 , 0.926, 0.928, 0.924, 0.932, 0.91 , 0.908, 0.921,\n",
       "        0.905, 0.9  , 0.907, 0.929, 0.926, 0.927, 0.921, 0.927, 0.929,\n",
       "        0.918, 0.927, 0.918, 0.916, 0.912, 0.915, 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.921, 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.603, 0.93 , 0.93 , 0.77 , 0.93 , 0.93 , 0.524, 0.93 ,\n",
       "        0.93 , 0.616, 0.93 , 0.93 , 0.79 , 0.93 , 0.93 , 0.866, 0.93 ,\n",
       "        0.93 , 0.704, 0.93 , 0.93 , 0.726, 0.93 , 0.93 , 0.524, 0.93 ,\n",
       "        0.93 , 0.401, 0.93 , 0.93 , 0.693, 0.93 , 0.93 , 0.725, 0.93 ,\n",
       "        0.93 , 0.783, 0.93 , 0.93 , 0.445, 0.93 , 0.93 , 0.737, 0.93 ,\n",
       "        0.93 , 0.453, 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ]),\n",
       " 'split4_test_recall_micro': array([0.924, 0.93 , 0.929, 0.928, 0.921, 0.923, 0.923, 0.908, 0.918,\n",
       "        0.929, 0.929, 0.9  , 0.928, 0.929, 0.931, 0.925, 0.922, 0.924,\n",
       "        0.898, 0.908, 0.904, 0.898, 0.904, 0.929, 0.93 , 0.928, 0.928,\n",
       "        0.924, 0.92 , 0.922, 0.915, 0.915, 0.908, 0.902, 0.913, 0.901,\n",
       "        0.928, 0.929, 0.928, 0.926, 0.928, 0.927, 0.922, 0.92 , 0.915,\n",
       "        0.913, 0.906, 0.929, 0.927, 0.928, 0.93 , 0.922, 0.92 , 0.924,\n",
       "        0.908, 0.909, 0.902, 0.884, 0.9  , 0.903, 0.927, 0.928, 0.928,\n",
       "        0.922, 0.924, 0.921, 0.906, 0.927, 0.898, 0.897, 0.894, 0.901,\n",
       "        0.929, 0.929, 0.93 , 0.929, 0.919, 0.922, 0.914, 0.919, 0.902,\n",
       "        0.91 , 0.894, 0.9  , 0.929, 0.929, 0.927, 0.924, 0.924, 0.923,\n",
       "        0.913, 0.921, 0.918, 0.908, 0.909, 0.896, 0.929, 0.072, 0.929,\n",
       "        0.929, 0.071, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.928, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.071, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.66 , 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.496, 0.929, 0.929, 0.799, 0.929, 0.929, 0.789, 0.929,\n",
       "        0.929, 0.783, 0.929, 0.929, 0.671, 0.929, 0.929, 0.566, 0.929,\n",
       "        0.929, 0.276, 0.929, 0.929, 0.588, 0.929, 0.929, 0.376, 0.929,\n",
       "        0.929, 0.448, 0.929, 0.929, 0.474, 0.929, 0.929, 0.546, 0.929,\n",
       "        0.929, 0.807, 0.929, 0.929, 0.665, 0.929, 0.929, 0.61 , 0.929,\n",
       "        0.929, 0.566, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.928,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.928, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929]),\n",
       " 'mean_test_recall_micro': array([0.928 , 0.9296, 0.9284, 0.9212, 0.9246, 0.9238, 0.9164, 0.9114,\n",
       "        0.916 , 0.9136, 0.9076, 0.9014, 0.9294, 0.93  , 0.929 , 0.9258,\n",
       "        0.9226, 0.9242, 0.9104, 0.9142, 0.9112, 0.9026, 0.9098, 0.9116,\n",
       "        0.9296, 0.9282, 0.9286, 0.9242, 0.9262, 0.9252, 0.9162, 0.916 ,\n",
       "        0.9114, 0.9084, 0.9168, 0.909 , 0.9298, 0.929 , 0.9294, 0.9284,\n",
       "        0.9268, 0.9272, 0.9204, 0.921 , 0.9206, 0.919 , 0.9138, 0.9188,\n",
       "        0.9284, 0.9292, 0.9298, 0.9232, 0.9184, 0.9234, 0.914 , 0.9098,\n",
       "        0.905 , 0.8924, 0.906 , 0.9048, 0.9288, 0.9292, 0.929 , 0.9242,\n",
       "        0.9232, 0.923 , 0.9078, 0.9208, 0.9146, 0.901 , 0.897 , 0.8992,\n",
       "        0.929 , 0.9292, 0.9292, 0.9278, 0.922 , 0.925 , 0.915 , 0.911 ,\n",
       "        0.9142, 0.8978, 0.9018, 0.9   , 0.929 , 0.9288, 0.9286, 0.9252,\n",
       "        0.925 , 0.9266, 0.9194, 0.92  , 0.9142, 0.91  , 0.91  , 0.906 ,\n",
       "        0.9298, 0.5864, 0.9298, 0.9298, 0.6808, 0.9298, 0.9298, 0.9202,\n",
       "        0.9298, 0.9298, 0.9264, 0.9298, 0.9298, 0.7742, 0.9298, 0.9298,\n",
       "        0.9296, 0.9298, 0.9298, 0.9292, 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.7582, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9294,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.6802, 0.9298, 0.9298,\n",
       "        0.9254, 0.9298, 0.9298, 0.876 , 0.9298, 0.9298, 0.9288, 0.9298,\n",
       "        0.9298, 0.5616, 0.9298, 0.9298, 0.5728, 0.9298, 0.9298, 0.6172,\n",
       "        0.9298, 0.9298, 0.6502, 0.9298, 0.9298, 0.586 , 0.9298, 0.9298,\n",
       "        0.7236, 0.9298, 0.9298, 0.6012, 0.9298, 0.9298, 0.6144, 0.9298,\n",
       "        0.9298, 0.473 , 0.9298, 0.9298, 0.591 , 0.9298, 0.9298, 0.5656,\n",
       "        0.9298, 0.9298, 0.5178, 0.9298, 0.9298, 0.6582, 0.9298, 0.9298,\n",
       "        0.5814, 0.9298, 0.9298, 0.6598, 0.9298, 0.9298, 0.6536, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.93  , 0.9304, 0.93  , 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.9298, 0.93  , 0.93  , 0.9298, 0.9294, 0.9296, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.9302, 0.9302, 0.93  , 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298]),\n",
       " 'std_test_recall_micro': array([2.89827535e-03, 1.01980390e-03, 2.33238076e-03, 7.46726188e-03,\n",
       "        2.33238076e-03, 2.63818119e-03, 7.81280999e-03, 6.59090282e-03,\n",
       "        2.44948974e-03, 9.02441134e-03, 1.80177690e-02, 7.22772440e-03,\n",
       "        1.20000000e-03, 6.32455532e-04, 2.52982213e-03, 2.03960781e-03,\n",
       "        1.95959179e-03, 4.16653333e-03, 7.65767589e-03, 3.37045991e-03,\n",
       "        7.05407684e-03, 1.46369396e-02, 1.01074230e-02, 1.50279739e-02,\n",
       "        8.00000000e-04, 1.16619038e-03, 1.01980390e-03, 1.72046505e-03,\n",
       "        3.86781592e-03, 3.18747549e-03, 4.95580468e-03, 2.75680975e-03,\n",
       "        5.23832034e-03, 5.85149554e-03, 1.13384302e-02, 1.15065199e-02,\n",
       "        9.79795897e-04, 1.41421356e-03, 8.00000000e-04, 1.74355958e-03,\n",
       "        2.48193473e-03, 1.46969385e-03, 4.96386946e-03, 3.03315018e-03,\n",
       "        4.31740663e-03, 6.09918027e-03, 5.03587132e-03, 8.79545337e-03,\n",
       "        1.95959179e-03, 1.16619038e-03, 1.16619038e-03, 1.93907194e-03,\n",
       "        3.38230691e-03, 3.00665928e-03, 4.85798312e-03, 2.48193473e-03,\n",
       "        5.36656315e-03, 1.01705457e-02, 5.79655070e-03, 4.87442304e-03,\n",
       "        1.46969385e-03, 1.16619038e-03, 1.09544512e-03, 2.03960781e-03,\n",
       "        2.78567766e-03, 4.56070170e-03, 3.24961536e-03, 3.60000000e-03,\n",
       "        8.82269800e-03, 9.09945053e-03, 8.14861951e-03, 2.71293199e-03,\n",
       "        2.19089023e-03, 1.72046505e-03, 1.72046505e-03, 2.48193473e-03,\n",
       "        3.94968353e-03, 3.79473319e-03, 3.22490310e-03, 5.69209979e-03,\n",
       "        6.49307323e-03, 9.74474217e-03, 4.40000000e-03, 4.81663783e-03,\n",
       "        6.32455532e-04, 1.46969385e-03, 1.49666295e-03, 3.96988665e-03,\n",
       "        3.03315018e-03, 2.05912603e-03, 3.77359245e-03, 6.19677335e-03,\n",
       "        4.48998886e-03, 3.57770876e-03, 4.33589668e-03, 7.66811581e-03,\n",
       "        4.00000000e-04, 4.20822813e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        3.36660898e-01, 4.00000000e-04, 4.00000000e-04, 1.91039263e-02,\n",
       "        4.00000000e-04, 4.00000000e-04, 6.71118469e-03, 4.00000000e-04,\n",
       "        4.00000000e-04, 3.11100241e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        8.00000000e-04, 4.00000000e-04, 4.00000000e-04, 1.16619038e-03,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 3.43600000e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 8.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 3.40042586e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        5.38887743e-03, 4.00000000e-04, 4.00000000e-04, 1.08000000e-01,\n",
       "        4.00000000e-04, 4.00000000e-04, 1.93907194e-03, 4.00000000e-04,\n",
       "        4.00000000e-04, 2.03029653e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        1.81561450e-01, 4.00000000e-04, 4.00000000e-04, 1.07188432e-01,\n",
       "        4.00000000e-04, 4.00000000e-04, 1.24108662e-01, 4.00000000e-04,\n",
       "        4.00000000e-04, 2.13617415e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        9.73192684e-02, 4.00000000e-04, 4.00000000e-04, 1.73063457e-01,\n",
       "        4.00000000e-04, 4.00000000e-04, 1.50469399e-01, 4.00000000e-04,\n",
       "        4.00000000e-04, 1.55370525e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        1.42194233e-01, 4.00000000e-04, 4.00000000e-04, 7.85126741e-02,\n",
       "        4.00000000e-04, 4.00000000e-04, 1.33634427e-01, 4.00000000e-04,\n",
       "        4.00000000e-04, 2.03355256e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        1.43296336e-01, 4.00000000e-04, 4.00000000e-04, 4.43909901e-02,\n",
       "        4.00000000e-04, 4.00000000e-04, 1.68592527e-01, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 6.32455532e-04, 1.35646600e-03, 1.26491106e-03,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 6.32455532e-04, 6.32455532e-04,\n",
       "        4.00000000e-04, 8.00000000e-04, 8.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 9.79795897e-04, 9.79795897e-04, 6.32455532e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04]),\n",
       " 'rank_test_recall_micro': array([192, 167, 188, 216, 205, 209, 228, 241, 230, 239, 253, 260, 169,\n",
       "          4, 178, 199, 214, 206, 245, 234, 243, 258, 248, 240, 167, 191,\n",
       "        186, 206, 198, 201, 229, 230, 241, 251, 227, 250,  10, 178, 169,\n",
       "        188, 195, 194, 220, 217, 219, 224, 238, 225, 188, 174,  11, 212,\n",
       "        226, 210, 237, 248, 256, 266, 254, 257, 183, 173, 178, 206, 211,\n",
       "        213, 252, 218, 233, 261, 265, 263, 178, 174, 174, 193, 215, 203,\n",
       "        232, 244, 234, 264, 259, 262, 178, 183, 186, 201, 203, 196, 223,\n",
       "        222, 234, 246, 246, 254,  11, 281,  11,  11, 271,  11,  11, 221,\n",
       "         11,  11, 197,  11,  11, 268,  11,  11, 165,  11,  11, 174,  11,\n",
       "         11,  11,  11,  11, 269,  11,  11,  11,  11,  11, 169,  11,  11,\n",
       "         11,  11,  11, 272,  11,  11, 200,  11,  11, 267,  11,  11, 183,\n",
       "         11,  11, 286,  11,  11, 284,  11,  11, 277,  11,  11, 276,  11,\n",
       "         11, 282,  11,  11, 270,  11,  11, 279,  11,  11, 278,  11,  11,\n",
       "        288,  11,  11, 280,  11,  11, 285,  11,  11, 287,  11,  11, 274,\n",
       "         11,  11, 283,  11,  11, 273,  11,  11, 275,  11,  11,  11,  11,\n",
       "         11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,\n",
       "         11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,\n",
       "         11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,\n",
       "         11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,\n",
       "         11,  11,   4,   1,   4,  11,  11,  11,  11,  11,  11,   4,   4,\n",
       "         11, 169, 165,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,\n",
       "          2,   2,   4,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,\n",
       "         11,  11]),\n",
       " 'split0_test_f1_micro': array([0.925, 0.929, 0.929, 0.918, 0.923, 0.922, 0.922, 0.903, 0.912,\n",
       "        0.915, 0.891, 0.897, 0.93 , 0.93 , 0.924, 0.925, 0.926, 0.918,\n",
       "        0.911, 0.915, 0.911, 0.904, 0.905, 0.905, 0.93 , 0.929, 0.929,\n",
       "        0.925, 0.927, 0.922, 0.91 , 0.915, 0.919, 0.902, 0.93 , 0.897,\n",
       "        0.93 , 0.93 , 0.93 , 0.931, 0.927, 0.927, 0.916, 0.923, 0.92 ,\n",
       "        0.93 , 0.916, 0.91 , 0.927, 0.929, 0.929, 0.922, 0.913, 0.919,\n",
       "        0.922, 0.908, 0.902, 0.879, 0.905, 0.91 , 0.931, 0.928, 0.929,\n",
       "        0.926, 0.926, 0.917, 0.906, 0.916, 0.917, 0.913, 0.89 , 0.897,\n",
       "        0.931, 0.928, 0.931, 0.929, 0.924, 0.922, 0.919, 0.913, 0.917,\n",
       "        0.893, 0.904, 0.893, 0.928, 0.93 , 0.929, 0.922, 0.929, 0.927,\n",
       "        0.922, 0.911, 0.916, 0.912, 0.904, 0.899, 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.557, 0.93 , 0.93 , 0.882, 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.542, 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.925, 0.93 ,\n",
       "        0.93 , 0.724, 0.93 , 0.93 , 0.474, 0.93 , 0.93 , 0.495, 0.93 ,\n",
       "        0.93 , 0.759, 0.93 , 0.93 , 0.406, 0.93 , 0.93 , 0.716, 0.93 ,\n",
       "        0.93 , 0.769, 0.93 , 0.93 , 0.634, 0.93 , 0.93 , 0.748, 0.93 ,\n",
       "        0.93 , 0.754, 0.93 , 0.93 , 0.511, 0.93 , 0.93 , 0.51 , 0.93 ,\n",
       "        0.93 , 0.472, 0.93 , 0.93 , 0.798, 0.93 , 0.93 , 0.676, 0.93 ,\n",
       "        0.93 , 0.548, 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ]),\n",
       " 'split1_test_f1_micro': array([0.93 , 0.93 , 0.931, 0.932, 0.927, 0.929, 0.922, 0.921, 0.919,\n",
       "        0.911, 0.93 , 0.915, 0.931, 0.93 , 0.93 , 0.927, 0.923, 0.931,\n",
       "        0.912, 0.916, 0.917, 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.929,\n",
       "        0.926, 0.927, 0.929, 0.925, 0.92 , 0.909, 0.914, 0.93 , 0.906,\n",
       "        0.931, 0.931, 0.93 , 0.929, 0.929, 0.93 , 0.922, 0.925, 0.925,\n",
       "        0.921, 0.921, 0.93 , 0.929, 0.93 , 0.931, 0.925, 0.921, 0.921,\n",
       "        0.915, 0.907, 0.915, 0.897, 0.905, 0.897, 0.93 , 0.931, 0.929,\n",
       "        0.922, 0.926, 0.931, 0.914, 0.919, 0.915, 0.886, 0.888, 0.902,\n",
       "        0.929, 0.927, 0.929, 0.93 , 0.927, 0.926, 0.918, 0.913, 0.917,\n",
       "        0.882, 0.906, 0.897, 0.929, 0.929, 0.931, 0.932, 0.925, 0.926,\n",
       "        0.924, 0.915, 0.913, 0.908, 0.917, 0.906, 0.93 , 0.07 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.928, 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.206, 0.93 , 0.93 , 0.332, 0.93 , 0.93 , 0.598, 0.93 ,\n",
       "        0.93 , 0.658, 0.93 , 0.93 , 0.797, 0.93 , 0.93 , 0.704, 0.93 ,\n",
       "        0.93 , 0.672, 0.93 , 0.93 , 0.345, 0.93 , 0.93 , 0.417, 0.93 ,\n",
       "        0.93 , 0.634, 0.93 , 0.93 , 0.536, 0.93 , 0.93 , 0.503, 0.93 ,\n",
       "        0.93 , 0.359, 0.93 , 0.93 , 0.408, 0.93 , 0.93 , 0.648, 0.93 ,\n",
       "        0.93 , 0.798, 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.931, 0.933, 0.932,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.931, 0.93 , 0.93 ,\n",
       "        0.928, 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.932, 0.932, 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ]),\n",
       " 'split2_test_f1_micro': array([0.931, 0.931, 0.924, 0.915, 0.927, 0.922, 0.903, 0.908, 0.915,\n",
       "        0.901, 0.891, 0.894, 0.93 , 0.931, 0.93 , 0.923, 0.922, 0.923,\n",
       "        0.922, 0.914, 0.903, 0.892, 0.905, 0.894, 0.93 , 0.927, 0.93 ,\n",
       "        0.921, 0.932, 0.929, 0.914, 0.912, 0.905, 0.908, 0.909, 0.911,\n",
       "        0.93 , 0.928, 0.929, 0.927, 0.928, 0.926, 0.928, 0.921, 0.926,\n",
       "        0.916, 0.915, 0.912, 0.932, 0.931, 0.931, 0.921, 0.916, 0.926,\n",
       "        0.915, 0.914, 0.9  , 0.894, 0.917, 0.91 , 0.928, 0.929, 0.931,\n",
       "        0.924, 0.919, 0.923, 0.908, 0.921, 0.924, 0.903, 0.909, 0.895,\n",
       "        0.931, 0.932, 0.93 , 0.923, 0.916, 0.923, 0.914, 0.902, 0.914,\n",
       "        0.899, 0.905, 0.903, 0.93 , 0.93 , 0.929, 0.927, 0.92 , 0.928,\n",
       "        0.92 , 0.926, 0.906, 0.906, 0.908, 0.914, 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.916, 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.913, 0.93 ,\n",
       "        0.93 , 0.152, 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.927, 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.07 , 0.93 ,\n",
       "        0.93 , 0.917, 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.779, 0.93 , 0.93 , 0.489, 0.93 , 0.93 , 0.68 , 0.93 ,\n",
       "        0.93 , 0.435, 0.93 , 0.93 , 0.266, 0.93 , 0.93 , 0.766, 0.93 ,\n",
       "        0.93 , 0.585, 0.93 , 0.93 , 0.779, 0.93 , 0.93 , 0.3  , 0.93 ,\n",
       "        0.93 , 0.718, 0.93 , 0.93 , 0.614, 0.93 , 0.93 , 0.305, 0.93 ,\n",
       "        0.93 , 0.87 , 0.93 , 0.93 , 0.591, 0.93 , 0.93 , 0.628, 0.93 ,\n",
       "        0.93 , 0.903, 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.931, 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.931, 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ]),\n",
       " 'split3_test_f1_micro': array([0.93 , 0.928, 0.929, 0.913, 0.925, 0.923, 0.912, 0.917, 0.916,\n",
       "        0.912, 0.897, 0.901, 0.928, 0.93 , 0.93 , 0.929, 0.92 , 0.925,\n",
       "        0.909, 0.918, 0.921, 0.889, 0.905, 0.9  , 0.928, 0.927, 0.927,\n",
       "        0.925, 0.925, 0.924, 0.917, 0.918, 0.916, 0.916, 0.902, 0.93 ,\n",
       "        0.93 , 0.927, 0.93 , 0.929, 0.922, 0.926, 0.914, 0.916, 0.917,\n",
       "        0.915, 0.911, 0.913, 0.927, 0.928, 0.928, 0.926, 0.922, 0.927,\n",
       "        0.91 , 0.911, 0.906, 0.908, 0.903, 0.904, 0.928, 0.93 , 0.928,\n",
       "        0.927, 0.921, 0.923, 0.905, 0.921, 0.919, 0.906, 0.904, 0.901,\n",
       "        0.925, 0.93 , 0.926, 0.928, 0.924, 0.932, 0.91 , 0.908, 0.921,\n",
       "        0.905, 0.9  , 0.907, 0.929, 0.926, 0.927, 0.921, 0.927, 0.929,\n",
       "        0.918, 0.927, 0.918, 0.916, 0.912, 0.915, 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.921, 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.603, 0.93 , 0.93 , 0.77 , 0.93 , 0.93 , 0.524, 0.93 ,\n",
       "        0.93 , 0.616, 0.93 , 0.93 , 0.79 , 0.93 , 0.93 , 0.866, 0.93 ,\n",
       "        0.93 , 0.704, 0.93 , 0.93 , 0.726, 0.93 , 0.93 , 0.524, 0.93 ,\n",
       "        0.93 , 0.401, 0.93 , 0.93 , 0.693, 0.93 , 0.93 , 0.725, 0.93 ,\n",
       "        0.93 , 0.783, 0.93 , 0.93 , 0.445, 0.93 , 0.93 , 0.737, 0.93 ,\n",
       "        0.93 , 0.453, 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ]),\n",
       " 'split4_test_f1_micro': array([0.924, 0.93 , 0.929, 0.928, 0.921, 0.923, 0.923, 0.908, 0.918,\n",
       "        0.929, 0.929, 0.9  , 0.928, 0.929, 0.931, 0.925, 0.922, 0.924,\n",
       "        0.898, 0.908, 0.904, 0.898, 0.904, 0.929, 0.93 , 0.928, 0.928,\n",
       "        0.924, 0.92 , 0.922, 0.915, 0.915, 0.908, 0.902, 0.913, 0.901,\n",
       "        0.928, 0.929, 0.928, 0.926, 0.928, 0.927, 0.922, 0.92 , 0.915,\n",
       "        0.913, 0.906, 0.929, 0.927, 0.928, 0.93 , 0.922, 0.92 , 0.924,\n",
       "        0.908, 0.909, 0.902, 0.884, 0.9  , 0.903, 0.927, 0.928, 0.928,\n",
       "        0.922, 0.924, 0.921, 0.906, 0.927, 0.898, 0.897, 0.894, 0.901,\n",
       "        0.929, 0.929, 0.93 , 0.929, 0.919, 0.922, 0.914, 0.919, 0.902,\n",
       "        0.91 , 0.894, 0.9  , 0.929, 0.929, 0.927, 0.924, 0.924, 0.923,\n",
       "        0.913, 0.921, 0.918, 0.908, 0.909, 0.896, 0.929, 0.072, 0.929,\n",
       "        0.929, 0.071, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.928, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.071, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.66 , 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.496, 0.929, 0.929, 0.799, 0.929, 0.929, 0.789, 0.929,\n",
       "        0.929, 0.783, 0.929, 0.929, 0.671, 0.929, 0.929, 0.566, 0.929,\n",
       "        0.929, 0.276, 0.929, 0.929, 0.588, 0.929, 0.929, 0.376, 0.929,\n",
       "        0.929, 0.448, 0.929, 0.929, 0.474, 0.929, 0.929, 0.546, 0.929,\n",
       "        0.929, 0.807, 0.929, 0.929, 0.665, 0.929, 0.929, 0.61 , 0.929,\n",
       "        0.929, 0.566, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.928,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.928, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929]),\n",
       " 'mean_test_f1_micro': array([0.928 , 0.9296, 0.9284, 0.9212, 0.9246, 0.9238, 0.9164, 0.9114,\n",
       "        0.916 , 0.9136, 0.9076, 0.9014, 0.9294, 0.93  , 0.929 , 0.9258,\n",
       "        0.9226, 0.9242, 0.9104, 0.9142, 0.9112, 0.9026, 0.9098, 0.9116,\n",
       "        0.9296, 0.9282, 0.9286, 0.9242, 0.9262, 0.9252, 0.9162, 0.916 ,\n",
       "        0.9114, 0.9084, 0.9168, 0.909 , 0.9298, 0.929 , 0.9294, 0.9284,\n",
       "        0.9268, 0.9272, 0.9204, 0.921 , 0.9206, 0.919 , 0.9138, 0.9188,\n",
       "        0.9284, 0.9292, 0.9298, 0.9232, 0.9184, 0.9234, 0.914 , 0.9098,\n",
       "        0.905 , 0.8924, 0.906 , 0.9048, 0.9288, 0.9292, 0.929 , 0.9242,\n",
       "        0.9232, 0.923 , 0.9078, 0.9208, 0.9146, 0.901 , 0.897 , 0.8992,\n",
       "        0.929 , 0.9292, 0.9292, 0.9278, 0.922 , 0.925 , 0.915 , 0.911 ,\n",
       "        0.9142, 0.8978, 0.9018, 0.9   , 0.929 , 0.9288, 0.9286, 0.9252,\n",
       "        0.925 , 0.9266, 0.9194, 0.92  , 0.9142, 0.91  , 0.91  , 0.906 ,\n",
       "        0.9298, 0.5864, 0.9298, 0.9298, 0.6808, 0.9298, 0.9298, 0.9202,\n",
       "        0.9298, 0.9298, 0.9264, 0.9298, 0.9298, 0.7742, 0.9298, 0.9298,\n",
       "        0.9296, 0.9298, 0.9298, 0.9292, 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.7582, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9294,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.6802, 0.9298, 0.9298,\n",
       "        0.9254, 0.9298, 0.9298, 0.876 , 0.9298, 0.9298, 0.9288, 0.9298,\n",
       "        0.9298, 0.5616, 0.9298, 0.9298, 0.5728, 0.9298, 0.9298, 0.6172,\n",
       "        0.9298, 0.9298, 0.6502, 0.9298, 0.9298, 0.586 , 0.9298, 0.9298,\n",
       "        0.7236, 0.9298, 0.9298, 0.6012, 0.9298, 0.9298, 0.6144, 0.9298,\n",
       "        0.9298, 0.473 , 0.9298, 0.9298, 0.591 , 0.9298, 0.9298, 0.5656,\n",
       "        0.9298, 0.9298, 0.5178, 0.9298, 0.9298, 0.6582, 0.9298, 0.9298,\n",
       "        0.5814, 0.9298, 0.9298, 0.6598, 0.9298, 0.9298, 0.6536, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.93  , 0.9304, 0.93  , 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.9298, 0.93  , 0.93  , 0.9298, 0.9294, 0.9296, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.9302, 0.9302, 0.93  , 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298]),\n",
       " 'std_test_f1_micro': array([2.89827535e-03, 1.01980390e-03, 2.33238076e-03, 7.46726188e-03,\n",
       "        2.33238076e-03, 2.63818119e-03, 7.81280999e-03, 6.59090282e-03,\n",
       "        2.44948974e-03, 9.02441134e-03, 1.80177690e-02, 7.22772440e-03,\n",
       "        1.20000000e-03, 6.32455532e-04, 2.52982213e-03, 2.03960781e-03,\n",
       "        1.95959179e-03, 4.16653333e-03, 7.65767589e-03, 3.37045991e-03,\n",
       "        7.05407684e-03, 1.46369396e-02, 1.01074230e-02, 1.50279739e-02,\n",
       "        8.00000000e-04, 1.16619038e-03, 1.01980390e-03, 1.72046505e-03,\n",
       "        3.86781592e-03, 3.18747549e-03, 4.95580468e-03, 2.75680975e-03,\n",
       "        5.23832034e-03, 5.85149554e-03, 1.13384302e-02, 1.15065199e-02,\n",
       "        9.79795897e-04, 1.41421356e-03, 8.00000000e-04, 1.74355958e-03,\n",
       "        2.48193473e-03, 1.46969385e-03, 4.96386946e-03, 3.03315018e-03,\n",
       "        4.31740663e-03, 6.09918027e-03, 5.03587132e-03, 8.79545337e-03,\n",
       "        1.95959179e-03, 1.16619038e-03, 1.16619038e-03, 1.93907194e-03,\n",
       "        3.38230691e-03, 3.00665928e-03, 4.85798312e-03, 2.48193473e-03,\n",
       "        5.36656315e-03, 1.01705457e-02, 5.79655070e-03, 4.87442304e-03,\n",
       "        1.46969385e-03, 1.16619038e-03, 1.09544512e-03, 2.03960781e-03,\n",
       "        2.78567766e-03, 4.56070170e-03, 3.24961536e-03, 3.60000000e-03,\n",
       "        8.82269800e-03, 9.09945053e-03, 8.14861951e-03, 2.71293199e-03,\n",
       "        2.19089023e-03, 1.72046505e-03, 1.72046505e-03, 2.48193473e-03,\n",
       "        3.94968353e-03, 3.79473319e-03, 3.22490310e-03, 5.69209979e-03,\n",
       "        6.49307323e-03, 9.74474217e-03, 4.40000000e-03, 4.81663783e-03,\n",
       "        6.32455532e-04, 1.46969385e-03, 1.49666295e-03, 3.96988665e-03,\n",
       "        3.03315018e-03, 2.05912603e-03, 3.77359245e-03, 6.19677335e-03,\n",
       "        4.48998886e-03, 3.57770876e-03, 4.33589668e-03, 7.66811581e-03,\n",
       "        4.00000000e-04, 4.20822813e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        3.36660898e-01, 4.00000000e-04, 4.00000000e-04, 1.91039263e-02,\n",
       "        4.00000000e-04, 4.00000000e-04, 6.71118469e-03, 4.00000000e-04,\n",
       "        4.00000000e-04, 3.11100241e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        8.00000000e-04, 4.00000000e-04, 4.00000000e-04, 1.16619038e-03,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 3.43600000e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 8.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 3.40042586e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        5.38887743e-03, 4.00000000e-04, 4.00000000e-04, 1.08000000e-01,\n",
       "        4.00000000e-04, 4.00000000e-04, 1.93907194e-03, 4.00000000e-04,\n",
       "        4.00000000e-04, 2.03029653e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        1.81561450e-01, 4.00000000e-04, 4.00000000e-04, 1.07188432e-01,\n",
       "        4.00000000e-04, 4.00000000e-04, 1.24108662e-01, 4.00000000e-04,\n",
       "        4.00000000e-04, 2.13617415e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        9.73192684e-02, 4.00000000e-04, 4.00000000e-04, 1.73063457e-01,\n",
       "        4.00000000e-04, 4.00000000e-04, 1.50469399e-01, 4.00000000e-04,\n",
       "        4.00000000e-04, 1.55370525e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        1.42194233e-01, 4.00000000e-04, 4.00000000e-04, 7.85126741e-02,\n",
       "        4.00000000e-04, 4.00000000e-04, 1.33634427e-01, 4.00000000e-04,\n",
       "        4.00000000e-04, 2.03355256e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        1.43296336e-01, 4.00000000e-04, 4.00000000e-04, 4.43909901e-02,\n",
       "        4.00000000e-04, 4.00000000e-04, 1.68592527e-01, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 6.32455532e-04, 1.35646600e-03, 1.26491106e-03,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 6.32455532e-04, 6.32455532e-04,\n",
       "        4.00000000e-04, 8.00000000e-04, 8.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 9.79795897e-04, 9.79795897e-04, 6.32455532e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04]),\n",
       " 'rank_test_f1_micro': array([192, 167, 188, 216, 205, 209, 228, 241, 230, 239, 253, 260, 169,\n",
       "          4, 178, 199, 214, 206, 245, 234, 243, 258, 248, 240, 167, 191,\n",
       "        186, 206, 198, 201, 229, 230, 241, 251, 227, 250,  10, 178, 169,\n",
       "        188, 195, 194, 220, 217, 219, 224, 238, 225, 188, 174,  11, 211,\n",
       "        226, 210, 237, 248, 256, 266, 254, 257, 183, 173, 178, 206, 211,\n",
       "        213, 252, 218, 233, 261, 265, 263, 178, 174, 174, 193, 215, 203,\n",
       "        232, 244, 236, 264, 259, 262, 178, 183, 186, 201, 203, 196, 223,\n",
       "        222, 234, 246, 246, 254,  11, 281,  11,  11, 271,  11,  11, 221,\n",
       "         11,  11, 197,  11,  11, 268,  11,  11, 165,  11,  11, 174,  11,\n",
       "         11,  11,  11,  11, 269,  11,  11,  11,  11,  11, 169,  11,  11,\n",
       "         11,  11,  11, 272,  11,  11, 200,  11,  11, 267,  11,  11, 183,\n",
       "         11,  11, 286,  11,  11, 284,  11,  11, 277,  11,  11, 276,  11,\n",
       "         11, 282,  11,  11, 270,  11,  11, 279,  11,  11, 278,  11,  11,\n",
       "        288,  11,  11, 280,  11,  11, 285,  11,  11, 287,  11,  11, 274,\n",
       "         11,  11, 283,  11,  11, 273,  11,  11, 275,  11,  11,  11,  11,\n",
       "         11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,\n",
       "         11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,\n",
       "         11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,\n",
       "         11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,\n",
       "         11,  11,   4,   1,   4,  11,  11,  11,  11,  11,  11,   4,   4,\n",
       "         11, 169, 165,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,\n",
       "          2,   2,   4,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,\n",
       "         11,  11]),\n",
       " 'split0_test_roc_auc_ovo': array([0.61841014, 0.59056836, 0.561298  , 0.62456221, 0.61414747,\n",
       "        0.64652842, 0.66162826, 0.53834869, 0.65889401, 0.61241167,\n",
       "        0.54866359, 0.5640553 , 0.56476959, 0.57698925, 0.63886329,\n",
       "        0.61543779, 0.5993702 , 0.55231951, 0.6050384 , 0.62414747,\n",
       "        0.61403994, 0.63932412, 0.59456221, 0.61623656, 0.5909831 ,\n",
       "        0.57230415, 0.6527957 , 0.59273425, 0.60356375, 0.60970814,\n",
       "        0.63127496, 0.61190476, 0.64039939, 0.63648233, 0.54301075,\n",
       "        0.59043011, 0.56850998, 0.57926267, 0.5874808 , 0.71130568,\n",
       "        0.62238095, 0.59453149, 0.62147465, 0.60211982, 0.61231951,\n",
       "        0.54041475, 0.58737327, 0.66089094, 0.53441628, 0.62036866,\n",
       "        0.57470814, 0.59304147, 0.59616743, 0.57079877, 0.59406298,\n",
       "        0.58084485, 0.58189708, 0.53439324, 0.61668203, 0.6397235 ,\n",
       "        0.5952381 , 0.59913978, 0.57582949, 0.55430876, 0.59193548,\n",
       "        0.60582181, 0.63336406, 0.61500768, 0.62230415, 0.61769585,\n",
       "        0.60096774, 0.55797235, 0.63946237, 0.59599846, 0.55093702,\n",
       "        0.65540707, 0.6546851 , 0.60854071, 0.63221198, 0.61474654,\n",
       "        0.58451613, 0.58852535, 0.57536098, 0.55898618, 0.56321045,\n",
       "        0.58864823, 0.55714286, 0.59827957, 0.64198157, 0.59904762,\n",
       "        0.59966206, 0.57545315, 0.59941628, 0.67211982, 0.58165899,\n",
       "        0.60291859, 0.44764977, 0.53032258, 0.5781874 , 0.5602765 ,\n",
       "        0.46961598, 0.47245776, 0.5409063 , 0.51826421, 0.43210445,\n",
       "        0.54663594, 0.49832565, 0.44761905, 0.46003072, 0.46875576,\n",
       "        0.44264209, 0.49709677, 0.50634409, 0.51073733, 0.44453149,\n",
       "        0.53248848, 0.49978495, 0.51933948, 0.55457757, 0.49800307,\n",
       "        0.45940092, 0.44264209, 0.5290937 , 0.48167435, 0.52969278,\n",
       "        0.46894009, 0.4809831 , 0.49221198, 0.49198157, 0.44844854,\n",
       "        0.5053149 , 0.48228879, 0.48391705, 0.48946237, 0.4930722 ,\n",
       "        0.47012289, 0.52513057, 0.45380952, 0.47368664, 0.47396313,\n",
       "        0.54187404, 0.45872504, 0.50271889, 0.50858679, 0.45826421,\n",
       "        0.52199693, 0.50615975, 0.56858679, 0.54715822, 0.51987711,\n",
       "        0.56694316, 0.48006144, 0.49115207, 0.50043011, 0.55950845,\n",
       "        0.53795699, 0.50549923, 0.5225192 , 0.5102765 , 0.54774194,\n",
       "        0.52889401, 0.5890169 , 0.49794163, 0.46387097, 0.48760369,\n",
       "        0.54298003, 0.55857143, 0.50860215, 0.49946237, 0.51746544,\n",
       "        0.50026114, 0.49298003, 0.53758833, 0.49328725, 0.51834101,\n",
       "        0.52789555, 0.47577573, 0.47064516, 0.51557604, 0.48477727,\n",
       "        0.49399386, 0.49635945, 0.56457757, 0.5244086 , 0.52305684,\n",
       "        0.54256528, 0.52144393, 0.49357911, 0.52202765, 0.52732719,\n",
       "        0.44721966, 0.53408602, 0.54761905, 0.5440553 , 0.54516129,\n",
       "        0.54543779, 0.54373272, 0.54599078, 0.54889401, 0.54682028,\n",
       "        0.5425192 , 0.54371736, 0.5435023 , 0.54729647, 0.55299539,\n",
       "        0.54975422, 0.54453149, 0.54698925, 0.54508449, 0.54714286,\n",
       "        0.5435023 , 0.54612903, 0.54281106, 0.54173579, 0.54897081,\n",
       "        0.54545315, 0.54147465, 0.5530722 , 0.57382488, 0.54700461,\n",
       "        0.55290323, 0.54993856, 0.54883257, 0.54789555, 0.54451613,\n",
       "        0.54695853, 0.54812596, 0.54505376, 0.54221198, 0.54792627,\n",
       "        0.5455914 , 0.54735791, 0.54596006, 0.55136713, 0.54384025,\n",
       "        0.54534562, 0.54497696, 0.54645161, 0.54502304, 0.54477727,\n",
       "        0.63482335, 0.64021505, 0.64248848, 0.65265745, 0.63159754,\n",
       "        0.61127496, 0.66374808, 0.61164363, 0.63658986, 0.63099846,\n",
       "        0.646298  , 0.5984639 , 0.65004608, 0.6609831 , 0.64552995,\n",
       "        0.66749616, 0.6327957 , 0.65717358, 0.65347158, 0.61714286,\n",
       "        0.65271889, 0.58855607, 0.64010753, 0.63632873, 0.50800307,\n",
       "        0.65179724, 0.66795699, 0.6824424 , 0.66990783, 0.63714286,\n",
       "        0.60674347, 0.65826421, 0.67582181, 0.62278034, 0.62829493,\n",
       "        0.61745008, 0.66061444, 0.64611367, 0.64795699, 0.67860215,\n",
       "        0.65116743, 0.63952381, 0.66700461, 0.66620584, 0.65499232,\n",
       "        0.63175115, 0.62360983, 0.65317972]),\n",
       " 'split1_test_roc_auc_ovo': array([0.61614439, 0.60420891, 0.62345622, 0.49484639, 0.65564516,\n",
       "        0.53142857, 0.67665131, 0.58015361, 0.64622888, 0.61353303,\n",
       "        0.5687404 , 0.59572965, 0.59971582, 0.59791859, 0.63430108,\n",
       "        0.57792627, 0.62645161, 0.63066052, 0.61354839, 0.57190476,\n",
       "        0.6250384 , 0.5724424 , 0.57253456, 0.56834101, 0.57591398,\n",
       "        0.55777266, 0.56485407, 0.6221659 , 0.60019969, 0.61218126,\n",
       "        0.63310292, 0.58737327, 0.60427035, 0.55019969, 0.56689708,\n",
       "        0.6159447 , 0.60345622, 0.55920123, 0.60070661, 0.56385561,\n",
       "        0.58422427, 0.63095238, 0.63632873, 0.55348694, 0.5971275 ,\n",
       "        0.65540707, 0.63835637, 0.57026114, 0.56709677, 0.56546851,\n",
       "        0.62803379, 0.64600614, 0.58431644, 0.57857143, 0.70178187,\n",
       "        0.63170507, 0.60110599, 0.59385561, 0.64316436, 0.58514593,\n",
       "        0.57841782, 0.62996928, 0.57734255, 0.62514593, 0.60445469,\n",
       "        0.59798771, 0.62      , 0.59718894, 0.60923195, 0.57658986,\n",
       "        0.59477727, 0.63858679, 0.58904762, 0.56437788, 0.49454685,\n",
       "        0.59953917, 0.58599078, 0.61596006, 0.5840553 , 0.66345622,\n",
       "        0.64568356, 0.61084485, 0.59562212, 0.52347158, 0.57204301,\n",
       "        0.62176651, 0.61448541, 0.61210445, 0.56023041, 0.57642089,\n",
       "        0.66605223, 0.6106298 , 0.62655914, 0.60324117, 0.5812596 ,\n",
       "        0.65761905, 0.51817204, 0.46560676, 0.51144393, 0.51211982,\n",
       "        0.50671275, 0.54302611, 0.56354839, 0.4343318 , 0.49792627,\n",
       "        0.52536098, 0.49890937, 0.47267281, 0.55015361, 0.4455914 ,\n",
       "        0.40304147, 0.52591398, 0.48913978, 0.52419355, 0.41963134,\n",
       "        0.47092166, 0.4449616 , 0.48281106, 0.50182796, 0.5041321 ,\n",
       "        0.4430722 , 0.50894009, 0.54112135, 0.44351767, 0.47863287,\n",
       "        0.53198157, 0.46995392, 0.43774194, 0.47457757, 0.50946237,\n",
       "        0.52572965, 0.53847926, 0.44883257, 0.5462212 , 0.54717358,\n",
       "        0.48898618, 0.51815668, 0.53556068, 0.5671275 , 0.45185868,\n",
       "        0.42732719, 0.53615975, 0.51387097, 0.50522273, 0.5381874 ,\n",
       "        0.4558679 , 0.53434716, 0.50938556, 0.5074808 , 0.57763441,\n",
       "        0.51079877, 0.47267281, 0.52304147, 0.57844854, 0.52235023,\n",
       "        0.52620584, 0.57081413, 0.50792627, 0.49305684, 0.5009063 ,\n",
       "        0.54907834, 0.48763441, 0.51847926, 0.55760369, 0.49572965,\n",
       "        0.50362519, 0.48850998, 0.46987711, 0.45207373, 0.54718894,\n",
       "        0.46391705, 0.45410138, 0.54236559, 0.48603687, 0.46735791,\n",
       "        0.549447  , 0.57158218, 0.49795699, 0.51294931, 0.54130568,\n",
       "        0.48009217, 0.47039939, 0.5677573 , 0.45764977, 0.4155914 ,\n",
       "        0.49460829, 0.49764977, 0.59860215, 0.53771121, 0.53956989,\n",
       "        0.50615975, 0.56520737, 0.57642089, 0.56431644, 0.57534562,\n",
       "        0.58159754, 0.57172043, 0.57635945, 0.57362519, 0.56766513,\n",
       "        0.57149002, 0.56983103, 0.57368664, 0.57574501, 0.57202765,\n",
       "        0.5762212 , 0.58356375, 0.57788018, 0.56852535, 0.57149002,\n",
       "        0.57758833, 0.57176651, 0.57261137, 0.56784946, 0.57265745,\n",
       "        0.57380952, 0.57534562, 0.58245776, 0.57044547, 0.57247312,\n",
       "        0.57207373, 0.57003072, 0.57175115, 0.57589862, 0.56758833,\n",
       "        0.5716129 , 0.5758679 , 0.5728725 , 0.573149  , 0.57357911,\n",
       "        0.58278034, 0.57185868, 0.56683564, 0.56364055, 0.57376344,\n",
       "        0.57302611, 0.57480799, 0.57551459, 0.57112135, 0.57694316,\n",
       "        0.6040553 , 0.60242704, 0.61050691, 0.60949309, 0.58894009,\n",
       "        0.59192012, 0.61626728, 0.58533026, 0.59824885, 0.59101382,\n",
       "        0.59219662, 0.6040553 , 0.61474654, 0.5956682 , 0.6203533 ,\n",
       "        0.6037788 , 0.60829493, 0.59866359, 0.5981874 , 0.5715361 ,\n",
       "        0.60623656, 0.59812596, 0.59514593, 0.60221198, 0.59044547,\n",
       "        0.60669739, 0.58227343, 0.59539171, 0.593149  , 0.60417819,\n",
       "        0.59784946, 0.61651306, 0.60213518, 0.57104455, 0.61385561,\n",
       "        0.60316436, 0.56545315, 0.60485407, 0.6000768 , 0.5918126 ,\n",
       "        0.60652842, 0.59344086, 0.60897081, 0.60276498, 0.5841321 ,\n",
       "        0.60903226, 0.61102919, 0.60046083]),\n",
       " 'split2_test_roc_auc_ovo': array([0.58209677, 0.6275576 , 0.57439324, 0.58690476, 0.54317972,\n",
       "        0.59293395, 0.54608295, 0.58227343, 0.60749616, 0.63233487,\n",
       "        0.5915361 , 0.58620584, 0.61888633, 0.57703533, 0.54539171,\n",
       "        0.63803379, 0.59072197, 0.58304147, 0.5287404 , 0.58190476,\n",
       "        0.58880184, 0.62354839, 0.63502304, 0.59605223, 0.56319508,\n",
       "        0.60133641, 0.56265745, 0.60738863, 0.54150538, 0.61926267,\n",
       "        0.56583717, 0.56543779, 0.58875576, 0.59052227, 0.63743472,\n",
       "        0.59245776, 0.59517665, 0.60408602, 0.57983103, 0.57589862,\n",
       "        0.56167435, 0.55597542, 0.55150538, 0.60941628, 0.58474654,\n",
       "        0.61387097, 0.57571429, 0.63165899, 0.55439324, 0.60534562,\n",
       "        0.56671275, 0.60443932, 0.57021505, 0.58348694, 0.56695084,\n",
       "        0.59423963, 0.57725038, 0.58095238, 0.60984639, 0.61325653,\n",
       "        0.54419355, 0.59706605, 0.61522273, 0.64471582, 0.56932412,\n",
       "        0.61357911, 0.62136713, 0.59745008, 0.55658986, 0.60359447,\n",
       "        0.61465438, 0.61678955, 0.56824885, 0.55824885, 0.56772657,\n",
       "        0.53259601, 0.54477727, 0.57245776, 0.5665745 , 0.56222734,\n",
       "        0.60926267, 0.5753149 , 0.68195084, 0.56337942, 0.63101382,\n",
       "        0.58477727, 0.57505376, 0.58861751, 0.5516129 , 0.57313364,\n",
       "        0.53766513, 0.53792627, 0.54674347, 0.61399386, 0.65580645,\n",
       "        0.66348694, 0.53623656, 0.49205837, 0.48717358, 0.52102919,\n",
       "        0.49357911, 0.47907834, 0.51831029, 0.48379416, 0.48682028,\n",
       "        0.54052227, 0.4724424 , 0.45700461, 0.44924731, 0.52382488,\n",
       "        0.51270353, 0.46637481, 0.49265745, 0.50228879, 0.50654378,\n",
       "        0.51396313, 0.46987711, 0.52422427, 0.52043011, 0.44571429,\n",
       "        0.56428571, 0.52794163, 0.47235023, 0.50557604, 0.45834101,\n",
       "        0.48557604, 0.48003072, 0.53012289, 0.52070661, 0.5287404 ,\n",
       "        0.4606298 , 0.49597542, 0.51059908, 0.48179724, 0.49930876,\n",
       "        0.54393241, 0.4937788 , 0.45493088, 0.52473118, 0.47568356,\n",
       "        0.54047619, 0.53195084, 0.55989247, 0.50525346, 0.54652842,\n",
       "        0.51743472, 0.56268817, 0.54771121, 0.48929339, 0.54291859,\n",
       "        0.49866359, 0.48442396, 0.52423963, 0.54721966, 0.47227343,\n",
       "        0.54563748, 0.46416283, 0.48778802, 0.51614439, 0.52923195,\n",
       "        0.51291859, 0.55534562, 0.50668203, 0.48058372, 0.54665131,\n",
       "        0.59010753, 0.4850384 , 0.53026114, 0.53906298, 0.43149002,\n",
       "        0.53477727, 0.52431644, 0.51330261, 0.52265745, 0.53663594,\n",
       "        0.46238095, 0.53482335, 0.50545315, 0.4988172 , 0.56453149,\n",
       "        0.53975422, 0.49004608, 0.46923195, 0.53069124, 0.51703533,\n",
       "        0.50843318, 0.55579109, 0.51089094, 0.54419355, 0.49854071,\n",
       "        0.49723502, 0.50245776, 0.55301075, 0.55013825, 0.55015361,\n",
       "        0.55036866, 0.55235023, 0.54950845, 0.55205837, 0.55310292,\n",
       "        0.55178187, 0.55170507, 0.55023041, 0.54976959, 0.5512596 ,\n",
       "        0.5590937 , 0.55086022, 0.55339478, 0.55801843, 0.55155146,\n",
       "        0.55073733, 0.55453149, 0.55213518, 0.55337942, 0.55129032,\n",
       "        0.54824885, 0.56196621, 0.55525346, 0.54668203, 0.55124424,\n",
       "        0.54961598, 0.55758833, 0.55321045, 0.55053763, 0.55305684,\n",
       "        0.54952381, 0.54906298, 0.55096774, 0.553702  , 0.55038402,\n",
       "        0.54990783, 0.54940092, 0.55006144, 0.55284178, 0.5516129 ,\n",
       "        0.55290323, 0.55270353, 0.55121352, 0.55112135, 0.54938556,\n",
       "        0.58903226, 0.58820276, 0.58983103, 0.56294931, 0.54434716,\n",
       "        0.59019969, 0.59211982, 0.5916129 , 0.56981567, 0.5918894 ,\n",
       "        0.60795699, 0.56973886, 0.5878341 , 0.5827957 , 0.59812596,\n",
       "        0.58439324, 0.58284178, 0.58652842, 0.54880184, 0.55345622,\n",
       "        0.58502304, 0.58093702, 0.57760369, 0.54439324, 0.58935484,\n",
       "        0.59036866, 0.5956682 , 0.59308756, 0.57382488, 0.60514593,\n",
       "        0.53969278, 0.56542243, 0.56989247, 0.57801843, 0.57187404,\n",
       "        0.56870968, 0.59201229, 0.55012289, 0.59980031, 0.60170507,\n",
       "        0.57718894, 0.58603687, 0.57688172, 0.58574501, 0.59179724,\n",
       "        0.5840553 , 0.57225806, 0.59803379]),\n",
       " 'split3_test_roc_auc_ovo': array([0.56506912, 0.57714286, 0.5853533 , 0.58304147, 0.57567588,\n",
       "        0.57247312, 0.59869432, 0.60423195, 0.6350384 , 0.60847158,\n",
       "        0.55388633, 0.61238095, 0.54626728, 0.52543779, 0.52499232,\n",
       "        0.57675883, 0.63519201, 0.56368664, 0.62328725, 0.59069124,\n",
       "        0.63003072, 0.58970814, 0.58678955, 0.60605223, 0.62439324,\n",
       "        0.61600614, 0.60436252, 0.57167435, 0.56052227, 0.56961598,\n",
       "        0.5883871 , 0.66073733, 0.57486943, 0.62211982, 0.58669739,\n",
       "        0.48628264, 0.63529954, 0.58682028, 0.59943164, 0.59129032,\n",
       "        0.62549923, 0.59466974, 0.54824885, 0.59952381, 0.6074808 ,\n",
       "        0.59761905, 0.61276498, 0.59903226, 0.53522273, 0.57672811,\n",
       "        0.61211982, 0.57449309, 0.59256528, 0.59816436, 0.63095238,\n",
       "        0.6334639 , 0.57549923, 0.60400922, 0.58142857, 0.64453917,\n",
       "        0.61367127, 0.63969278, 0.58540707, 0.59687404, 0.58794163,\n",
       "        0.62574501, 0.57669739, 0.67502304, 0.58452381, 0.5918894 ,\n",
       "        0.66023041, 0.58745008, 0.55528418, 0.58176651, 0.57411674,\n",
       "        0.59059908, 0.61582181, 0.59798771, 0.63447005, 0.59883257,\n",
       "        0.63588326, 0.63643625, 0.65500768, 0.60009217, 0.5849616 ,\n",
       "        0.61231951, 0.61136713, 0.59777266, 0.58651306, 0.55261137,\n",
       "        0.59675883, 0.53639017, 0.63571429, 0.62574501, 0.61602151,\n",
       "        0.60437788, 0.48933948, 0.50104455, 0.46443932, 0.45070661,\n",
       "        0.46124424, 0.54497696, 0.51172043, 0.46420891, 0.51367127,\n",
       "        0.53969278, 0.46235023, 0.45473118, 0.4877573 , 0.51577573,\n",
       "        0.47960061, 0.49665131, 0.47437788, 0.51482335, 0.48416283,\n",
       "        0.51082949, 0.51542243, 0.4606298 , 0.52672811, 0.44086022,\n",
       "        0.53574501, 0.47568356, 0.4425192 , 0.56239631, 0.49949309,\n",
       "        0.47831029, 0.46493088, 0.4981106 , 0.51311828, 0.48669739,\n",
       "        0.5412596 , 0.52219662, 0.52820276, 0.48723502, 0.53038402,\n",
       "        0.50414747, 0.50339478, 0.51788018, 0.48895545, 0.45695853,\n",
       "        0.55376344, 0.47780338, 0.5128725 , 0.50056836, 0.5072043 ,\n",
       "        0.54582181, 0.47800307, 0.47533026, 0.51345622, 0.50463902,\n",
       "        0.51262673, 0.48304147, 0.466298  , 0.47382488, 0.45774194,\n",
       "        0.51520737, 0.5328725 , 0.50142857, 0.4511828 , 0.5025192 ,\n",
       "        0.43534562, 0.44912442, 0.49486943, 0.47142857, 0.4940553 ,\n",
       "        0.53330261, 0.52728111, 0.49301075, 0.46800307, 0.5083871 ,\n",
       "        0.50648233, 0.48513057, 0.47337942, 0.54006144, 0.51253456,\n",
       "        0.46867896, 0.49642089, 0.5075576 , 0.46207373, 0.48238095,\n",
       "        0.45892473, 0.51669739, 0.49786482, 0.51557604, 0.51568356,\n",
       "        0.4540553 , 0.48345622, 0.456298  , 0.46906298, 0.45463902,\n",
       "        0.45339478, 0.51705069, 0.48708141, 0.49548387, 0.48639017,\n",
       "        0.4874808 , 0.48892473, 0.49      , 0.48703533, 0.48781874,\n",
       "        0.4888172 , 0.48658986, 0.48562212, 0.48829493, 0.48930876,\n",
       "        0.48780338, 0.48926267, 0.48844854, 0.48632873, 0.4877573 ,\n",
       "        0.48877112, 0.48924731, 0.48540707, 0.48582181, 0.48456221,\n",
       "        0.48556068, 0.48869432, 0.48910906, 0.48763441, 0.48402458,\n",
       "        0.48427035, 0.49043011, 0.48774194, 0.48574501, 0.4862212 ,\n",
       "        0.48379416, 0.486298  , 0.48454685, 0.48889401, 0.48884793,\n",
       "        0.48758833, 0.48709677, 0.4875576 , 0.49058372, 0.48568356,\n",
       "        0.48797235, 0.48640553, 0.48485407, 0.48574501, 0.48526882,\n",
       "        0.63416283, 0.60585253, 0.61010753, 0.60963134, 0.59740399,\n",
       "        0.60419355, 0.61149002, 0.63892473, 0.61296467, 0.6       ,\n",
       "        0.63536098, 0.61829493, 0.59860215, 0.60382488, 0.62067588,\n",
       "        0.60373272, 0.62849462, 0.60450077, 0.6127957 , 0.61076805,\n",
       "        0.62195084, 0.62717358, 0.62221198, 0.61843318, 0.62254992,\n",
       "        0.62519201, 0.60718894, 0.61376344, 0.6030722 , 0.60365591,\n",
       "        0.60552995, 0.62201229, 0.61427035, 0.63976959, 0.62298003,\n",
       "        0.62138249, 0.6043318 , 0.60715822, 0.58285714, 0.61371736,\n",
       "        0.60463902, 0.61373272, 0.60299539, 0.61265745, 0.62576037,\n",
       "        0.60778802, 0.60568356, 0.60680492]),\n",
       " 'split4_test_roc_auc_ovo': array([0.52546279, 0.58838066, 0.55435953, 0.61333556, 0.54829515,\n",
       "        0.6088934 , 0.50931639, 0.59963765, 0.60409497, 0.50313073,\n",
       "        0.49917373, 0.55310117, 0.57111994, 0.58765294, 0.51989872,\n",
       "        0.57309844, 0.57786655, 0.51553238, 0.57362907, 0.54662745,\n",
       "        0.55088767, 0.60822632, 0.57805607, 0.50166012, 0.58255886,\n",
       "        0.59070028, 0.51935293, 0.5679134 , 0.59853849, 0.53880441,\n",
       "        0.54254916, 0.55962037, 0.51765491, 0.55874104, 0.60387513,\n",
       "        0.60793826, 0.56858048, 0.51269728, 0.54958383, 0.56545733,\n",
       "        0.54367107, 0.54774936, 0.59562759, 0.55695205, 0.59659789,\n",
       "        0.54562683, 0.52077806, 0.50414652, 0.51365242, 0.52044452,\n",
       "        0.49395837, 0.483702  , 0.58081535, 0.59435407, 0.51820828,\n",
       "        0.568535  , 0.53739444, 0.50326718, 0.51995937, 0.58346852,\n",
       "        0.61862672, 0.56908079, 0.55842266, 0.50695129, 0.58501493,\n",
       "        0.55249473, 0.57033915, 0.50451038, 0.53859216, 0.5273579 ,\n",
       "        0.55370761, 0.53134523, 0.54767355, 0.57253749, 0.51873133,\n",
       "        0.59972104, 0.46838187, 0.5078003 , 0.5366364 , 0.56036326,\n",
       "        0.54682454, 0.54135145, 0.5146682 , 0.53198199, 0.52588729,\n",
       "        0.59825043, 0.52414379, 0.57011174, 0.54884095, 0.53298261,\n",
       "        0.58340787, 0.5916251 , 0.58186146, 0.53986567, 0.56630634,\n",
       "        0.50078079, 0.4745827 , 0.53490805, 0.46706287, 0.53014752,\n",
       "        0.52846465, 0.54039631, 0.50314589, 0.48370958, 0.46772995,\n",
       "        0.50557164, 0.49952243, 0.49010749, 0.47703877, 0.49480738,\n",
       "        0.48715111, 0.53750057, 0.49028942, 0.47785746, 0.53537804,\n",
       "        0.45205355, 0.51774587, 0.48696918, 0.52587213, 0.49949211,\n",
       "        0.50601131, 0.52308252, 0.50554132, 0.478191  , 0.47744811,\n",
       "        0.50971058, 0.4851347 , 0.49398869, 0.48766658, 0.4876969 ,\n",
       "        0.52995042, 0.52535666, 0.46381843, 0.50978638, 0.52291575,\n",
       "        0.47656878, 0.49553511, 0.47388529, 0.49374612, 0.53487773,\n",
       "        0.50543519, 0.5366364 , 0.52710017, 0.46880638, 0.54332237,\n",
       "        0.46909444, 0.50431329, 0.51456208, 0.50269107, 0.52012614,\n",
       "        0.51422854, 0.46741157, 0.47059537, 0.52297639, 0.49124456,\n",
       "        0.46207493, 0.53125426, 0.49429191, 0.49118392, 0.46713868,\n",
       "        0.48725724, 0.50989251, 0.50234236, 0.48337604, 0.4977486 ,\n",
       "        0.54377719, 0.43969739, 0.45338771, 0.48903106, 0.52684243,\n",
       "        0.45422156, 0.51374339, 0.50867963, 0.49824891, 0.46877606,\n",
       "        0.49662669, 0.50749708, 0.51239406, 0.49697539, 0.4857563 ,\n",
       "        0.55708849, 0.51697266, 0.53126942, 0.51137828, 0.53690929,\n",
       "        0.50129626, 0.50123562, 0.53133007, 0.50660259, 0.50958929,\n",
       "        0.49803666, 0.5455207 , 0.50457102, 0.50335815, 0.50681484,\n",
       "        0.51066572, 0.50491972, 0.50413135, 0.50420716, 0.50103852,\n",
       "        0.50209979, 0.50119013, 0.50200882, 0.50169044, 0.50373717,\n",
       "        0.49950727, 0.50078079, 0.50617808, 0.50211495, 0.50535939,\n",
       "        0.50191786, 0.50519262, 0.50372201, 0.50247881, 0.50394942,\n",
       "        0.50401007, 0.50326718, 0.50021983, 0.50405555, 0.50654194,\n",
       "        0.50187238, 0.5025243 , 0.50244849, 0.50331266, 0.50288816,\n",
       "        0.50408587, 0.50496521, 0.50241817, 0.49859761, 0.50510165,\n",
       "        0.50695129, 0.50223624, 0.5045407 , 0.5003108 , 0.50452554,\n",
       "        0.50237269, 0.50209979, 0.50278203, 0.50188754, 0.50300945,\n",
       "        0.55899877, 0.55901393, 0.57375036, 0.57052108, 0.58089116,\n",
       "        0.56462348, 0.56283449, 0.53413484, 0.57278006, 0.5500993 ,\n",
       "        0.52132385, 0.56786792, 0.58954805, 0.56292545, 0.5825437 ,\n",
       "        0.5857275 , 0.58092148, 0.57124881, 0.5497506 , 0.56841371,\n",
       "        0.55942328, 0.56568474, 0.52535666, 0.55889265, 0.57922346,\n",
       "        0.59421762, 0.55346503, 0.57167331, 0.58105793, 0.59085189,\n",
       "        0.57658545, 0.55264634, 0.55625464, 0.56497218, 0.55176701,\n",
       "        0.55545111, 0.56979336, 0.59392956, 0.5802544 , 0.5783138 ,\n",
       "        0.56247063, 0.57801058, 0.57876863, 0.56281933, 0.58691005,\n",
       "        0.584833  , 0.56374414, 0.57688867]),\n",
       " 'mean_test_roc_auc_ovo': array([0.58143664, 0.59757168, 0.57977206, 0.58053808, 0.58738868,\n",
       "        0.59045149, 0.59847465, 0.58092907, 0.63035048, 0.59397638,\n",
       "        0.55240003, 0.58229458, 0.58015179, 0.57300678, 0.57268942,\n",
       "        0.59625102, 0.60592047, 0.5690481 , 0.5888487 , 0.58305514,\n",
       "        0.60175972, 0.60664987, 0.59339309, 0.57766843, 0.58740885,\n",
       "        0.58762393, 0.58080453, 0.59237531, 0.58086592, 0.58991449,\n",
       "        0.59223026, 0.5970147 , 0.58518997, 0.59161303, 0.58758301,\n",
       "        0.57861069, 0.59420458, 0.5684135 , 0.58340678, 0.60156151,\n",
       "        0.58748997, 0.58477568, 0.59063704, 0.58429978, 0.59965445,\n",
       "        0.59058773, 0.58699739, 0.59319797, 0.54095629, 0.57767108,\n",
       "        0.57510657, 0.58033641, 0.58481591, 0.58507511, 0.60239127,\n",
       "        0.60175769, 0.57462943, 0.56329553, 0.59421614, 0.61322673,\n",
       "        0.59002949, 0.60698974, 0.5824449 , 0.58559917, 0.58773417,\n",
       "        0.59912567, 0.60435354, 0.59783602, 0.58224839, 0.5834255 ,\n",
       "        0.60486748, 0.5864288 , 0.57994331, 0.57458584, 0.5412117 ,\n",
       "        0.59557247, 0.57393137, 0.58054931, 0.59078964, 0.59992519,\n",
       "        0.60443403, 0.59049456, 0.60452197, 0.55558227, 0.57542323,\n",
       "        0.60115239, 0.57643859, 0.59337719, 0.57783578, 0.56683923,\n",
       "        0.59670922, 0.5704049 , 0.59805893, 0.6109931 , 0.60021058,\n",
       "        0.60583665, 0.49319611, 0.50478806, 0.50166142, 0.51485592,\n",
       "        0.49192335, 0.5159871 , 0.52752626, 0.47686173, 0.47965045,\n",
       "        0.53155672, 0.48631002, 0.46442703, 0.48484554, 0.48975103,\n",
       "        0.46502776, 0.50470749, 0.49056172, 0.50598009, 0.47804949,\n",
       "        0.49605126, 0.48955839, 0.49479476, 0.52588718, 0.47764036,\n",
       "        0.50170303, 0.49565798, 0.49812516, 0.49427107, 0.48872157,\n",
       "        0.49490371, 0.47620666, 0.49043522, 0.49761012, 0.49220912,\n",
       "        0.51257687, 0.51285935, 0.48707398, 0.50290044, 0.51857086,\n",
       "        0.49675154, 0.50719919, 0.48721331, 0.50964938, 0.47866833,\n",
       "        0.51377521, 0.50825508, 0.523291  , 0.49768754, 0.51870134,\n",
       "        0.50204316, 0.51710229, 0.52311518, 0.51201594, 0.53303905,\n",
       "        0.52065216, 0.47752225, 0.49506531, 0.52457992, 0.50062372,\n",
       "        0.51741652, 0.52092059, 0.50279079, 0.49236889, 0.50950761,\n",
       "        0.50269876, 0.51820277, 0.50406294, 0.4913726 , 0.50435771,\n",
       "        0.54275851, 0.49981966, 0.49102777, 0.48952664, 0.50627478,\n",
       "        0.49193187, 0.49405436, 0.51506312, 0.50805838, 0.5007291 ,\n",
       "        0.50100583, 0.51721985, 0.49880139, 0.49727834, 0.51175034,\n",
       "        0.50597069, 0.49809499, 0.52614021, 0.50794079, 0.50165528,\n",
       "        0.50019166, 0.51191533, 0.51814005, 0.5159196 , 0.50593322,\n",
       "        0.48040918, 0.53286451, 0.53374063, 0.5314704 , 0.53277311,\n",
       "        0.5351101 , 0.53232957, 0.53319801, 0.53316401, 0.53128912,\n",
       "        0.53134162, 0.53060669, 0.53101006, 0.53255929, 0.53386571,\n",
       "        0.53447595, 0.53379978, 0.53457817, 0.53201439, 0.5326602 ,\n",
       "        0.53250339, 0.53337339, 0.53133734, 0.53025306, 0.53228604,\n",
       "        0.53141645, 0.5341496 , 0.53602246, 0.53652847, 0.5322577 ,\n",
       "        0.53214713, 0.5341024 , 0.53279692, 0.53267789, 0.53085413,\n",
       "        0.53119505, 0.53286401, 0.53117181, 0.53131092, 0.5331678 ,\n",
       "        0.53456384, 0.5315901 , 0.53099109, 0.5317488 , 0.53188514,\n",
       "        0.532324  , 0.53219876, 0.53216317, 0.53097966, 0.53187685,\n",
       "        0.6042145 , 0.59914226, 0.60533686, 0.60105045, 0.58863599,\n",
       "        0.59244236, 0.60929194, 0.59232927, 0.59807982, 0.5928002 ,\n",
       "        0.60062729, 0.59168418, 0.60815539, 0.60123947, 0.61344576,\n",
       "        0.60902568, 0.6066697 , 0.60362303, 0.59260143, 0.58426339,\n",
       "        0.60507052, 0.59209547, 0.59208516, 0.59205195, 0.57791535,\n",
       "        0.61365458, 0.60131052, 0.61127168, 0.60420237, 0.60819496,\n",
       "        0.58528022, 0.60297166, 0.60367489, 0.59531702, 0.59775432,\n",
       "        0.59323154, 0.59844101, 0.60043568, 0.60218913, 0.61283019,\n",
       "        0.60039889, 0.60214897, 0.60692423, 0.60603852, 0.60871842,\n",
       "        0.60349195, 0.59526496, 0.60707359]),\n",
       " 'std_test_roc_auc_ovo': array([0.03456319, 0.01728622, 0.02431245, 0.04561505, 0.0424144 ,\n",
       "        0.03820423, 0.06448892, 0.02327459, 0.02144997, 0.04616652,\n",
       "        0.03048885, 0.02138184, 0.02588228, 0.02502002, 0.05288102,\n",
       "        0.025945  , 0.02162995, 0.03785348, 0.03435939, 0.02529632,\n",
       "        0.02914729, 0.02371845, 0.02212839, 0.04121982, 0.02060379,\n",
       "        0.02063415, 0.04493938, 0.02068779, 0.02515719, 0.0308972 ,\n",
       "        0.03570613, 0.03679859, 0.04023022, 0.03388433, 0.03214344,\n",
       "        0.04713776, 0.02486821, 0.03136305, 0.01859815, 0.05573708,\n",
       "        0.03243237, 0.03008574, 0.03575371, 0.02398955, 0.0095878 ,\n",
       "        0.04320163, 0.03953268, 0.05394791, 0.01835769, 0.03468036,\n",
       "        0.04653368, 0.05371636, 0.00914712, 0.01006043, 0.06181648,\n",
       "        0.02645655, 0.0207263 , 0.03831979, 0.04200605, 0.02590996,\n",
       "        0.0269801 , 0.02527741, 0.01860122, 0.04969995, 0.01134543,\n",
       "        0.02505222, 0.02568161, 0.05470456, 0.03131128, 0.03112293,\n",
       "        0.03433713, 0.03869688, 0.03289856, 0.01330683, 0.03021039,\n",
       "        0.03899716, 0.06389376, 0.03924138, 0.03791592, 0.03806135,\n",
       "        0.03585936, 0.03216868, 0.05924071, 0.02698734, 0.03404609,\n",
       "        0.01401832, 0.03398986, 0.01384196, 0.03472252, 0.02243885,\n",
       "        0.04118326, 0.02934515, 0.03202394, 0.04261995, 0.03222944,\n",
       "        0.05839923, 0.03134351, 0.02557522, 0.04181953, 0.0359377 ,\n",
       "        0.02447677, 0.03293749, 0.02193527, 0.0274882 , 0.0280911 ,\n",
       "        0.01474836, 0.01577384, 0.01522556, 0.03526351, 0.02917106,\n",
       "        0.03825757, 0.02496926, 0.01017338, 0.01572942, 0.04164612,\n",
       "        0.02977985, 0.02808662, 0.0238354 , 0.01692614, 0.02816382,\n",
       "        0.04543358, 0.03218825, 0.03638432, 0.03940052, 0.02427557,\n",
       "        0.02293673, 0.00752845, 0.02975362, 0.01694109, 0.02682512,\n",
       "        0.02845617, 0.02058822, 0.02918287, 0.02364503, 0.01998877,\n",
       "        0.02629625, 0.01242765, 0.0335068 , 0.03318333, 0.02959938,\n",
       "        0.0461397 , 0.03324512, 0.01987295, 0.01466449, 0.03330188,\n",
       "        0.0339688 , 0.02893961, 0.03230383, 0.01929191, 0.02542791,\n",
       "        0.02378703, 0.00648515, 0.02480103, 0.03626636, 0.03652399,\n",
       "        0.02954034, 0.03520869, 0.0119562 , 0.0227338 , 0.02745491,\n",
       "        0.03928508, 0.04933464, 0.00824186, 0.03382604, 0.02141826,\n",
       "        0.02779873, 0.0404153 , 0.02726233, 0.02972809, 0.03954605,\n",
       "        0.02940652, 0.02441508, 0.02463408, 0.02018261, 0.02783337,\n",
       "        0.03354307, 0.03319605, 0.01482676, 0.01909036, 0.03442431,\n",
       "        0.03682614, 0.01753431, 0.03814124, 0.02603103, 0.04368341,\n",
       "        0.02837042, 0.02507333, 0.04717702, 0.02681025, 0.02928562,\n",
       "        0.02485257, 0.02182833, 0.03290441, 0.02709634, 0.03191607,\n",
       "        0.0327617 , 0.03071735, 0.03163395, 0.03227349, 0.03112891,\n",
       "        0.03104206, 0.03149094, 0.03239724, 0.03252877, 0.03167926,\n",
       "        0.03459267, 0.03451518, 0.03261187, 0.03212863, 0.0310998 ,\n",
       "        0.03266218, 0.03106661, 0.03206225, 0.03106675, 0.03271062,\n",
       "        0.03203558, 0.03329997, 0.03549495, 0.03489871, 0.03219619,\n",
       "        0.0332832 , 0.03161538, 0.0320695 , 0.03311674, 0.03099421,\n",
       "        0.03224836, 0.03253123, 0.03263187, 0.03237275, 0.03130254,\n",
       "        0.03361853, 0.03171344, 0.02986192, 0.03009913, 0.03215287,\n",
       "        0.03198087, 0.03287018, 0.03331013, 0.03195868, 0.03319513,\n",
       "        0.02866633, 0.02635431, 0.02309922, 0.032224  , 0.02808611,\n",
       "        0.01594961, 0.03308212, 0.03458402, 0.02507409, 0.02583924,\n",
       "        0.04405923, 0.01977854, 0.02301619, 0.03290946, 0.0215361 ,\n",
       "        0.03041021, 0.02187362, 0.02909757, 0.03974616, 0.0250832 ,\n",
       "        0.03175608, 0.02050076, 0.03971284, 0.03501577, 0.037867  ,\n",
       "        0.02261153, 0.0378411 , 0.03800617, 0.03434856, 0.01539348,\n",
       "        0.02522904, 0.03884781, 0.04173666, 0.03013127, 0.03037413,\n",
       "        0.02648153, 0.03420658, 0.03074053, 0.02432991, 0.03488159,\n",
       "        0.03036528, 0.02213212, 0.03263301, 0.03450901, 0.02756128,\n",
       "        0.01774076, 0.02316622, 0.02514925]),\n",
       " 'rank_test_roc_auc_ovo': array([113,  57, 122, 118,  96,  86,  51, 114,   1,  66, 142, 111, 120,\n",
       "        134, 135,  60,  19, 137,  89, 109,  36,  17,  67, 127,  95,  92,\n",
       "        116,  74, 115,  88,  76,  58, 101,  81,  93, 123,  65, 138, 108,\n",
       "         38,  94, 104,  83, 105,  48,  84,  97,  70, 145, 126, 130, 119,\n",
       "        103, 102,  33,  37, 131, 140,  64,   4,  87,  14, 110,  99,  91,\n",
       "         50,  26,  55, 112, 107,  23,  98, 121, 132, 144,  61, 133, 117,\n",
       "         82,  47,  25,  85,  24, 141, 129,  41, 128,  68, 125, 139,  59,\n",
       "        136,  54,   7,  46,  20, 262, 232, 241, 215, 266, 212, 197, 285,\n",
       "        280, 182, 277, 288, 278, 271, 287, 233, 269, 229, 282, 255, 272,\n",
       "        259, 199, 283, 240, 256, 249, 260, 274, 258, 286, 270, 252, 264,\n",
       "        218, 217, 276, 236, 206, 254, 227, 275, 222, 281, 216, 224, 201,\n",
       "        251, 205, 239, 211, 202, 219, 161, 204, 284, 257, 200, 245, 209,\n",
       "        203, 237, 263, 223, 238, 207, 235, 267, 234, 143, 247, 268, 273,\n",
       "        228, 265, 261, 214, 225, 244, 243, 210, 248, 253, 221, 230, 250,\n",
       "        198, 226, 242, 246, 220, 208, 213, 231, 279, 162, 156, 183, 165,\n",
       "        148, 170, 158, 160, 188, 185, 195, 191, 168, 154, 151, 155, 149,\n",
       "        177, 167, 169, 157, 186, 196, 172, 184, 152, 147, 146, 173, 176,\n",
       "        153, 164, 166, 194, 189, 163, 190, 187, 159, 150, 181, 192, 180,\n",
       "        178, 171, 174, 175, 193, 179,  27,  49,  21,  42,  90,  73,   8,\n",
       "         75,  53,  71,  43,  80,  12,  40,   3,   9,  16,  30,  72, 106,\n",
       "         22,  77,  78,  79, 124,   2,  39,   6,  28,  11, 100,  32,  29,\n",
       "         62,  56,  69,  52,  44,  34,   5,  45,  35,  15,  18,  10,  31,\n",
       "         63,  13]),\n",
       " 'split0_test_neg_log_loss': array([-0.25581663, -0.26569904, -0.27528583, -0.28862888, -0.2850945 ,\n",
       "        -0.27036598, -0.29866412, -0.37071218, -0.29886237, -0.33787083,\n",
       "        -0.42727806, -0.50509984, -0.26149021, -0.25721958, -0.25307517,\n",
       "        -0.27491192, -0.36874687, -0.30637889, -0.32215244, -0.30326439,\n",
       "        -0.31619161, -0.36961215, -0.36581113, -0.36575588, -0.26181966,\n",
       "        -0.26224953, -0.24533975, -0.27263112, -0.27886602, -0.27772693,\n",
       "        -0.30034631, -0.30595079, -0.29605261, -0.3341839 , -0.25328029,\n",
       "        -0.36073565, -0.26010264, -0.25871715, -0.25479555, -0.23654598,\n",
       "        -0.26312792, -0.27591907, -0.28848837, -0.28430179, -0.28983197,\n",
       "        -0.2534113 , -0.32065411, -0.3032087 , -0.42797894, -0.2541513 ,\n",
       "        -0.26145288, -0.29151365, -0.293278  , -0.30526017, -0.48933675,\n",
       "        -0.51863602, -0.35566725, -0.48351064, -0.33746838, -0.33830117,\n",
       "        -0.25725859, -0.38750727, -0.39065689, -0.30555116, -0.28987627,\n",
       "        -0.29332705, -0.4875546 , -0.31582866, -0.30839546, -0.37364626,\n",
       "        -0.41365883, -0.39802272, -0.24670466, -0.29668016, -0.26806682,\n",
       "        -0.26618975, -0.31971521, -0.34109551, -0.32904021, -0.35640732,\n",
       "        -0.31501688, -0.4043641 , -0.38544668, -0.42063615, -0.26738554,\n",
       "        -0.25724466, -0.2671747 , -0.28412844, -0.25727997, -0.2749885 ,\n",
       "        -0.29462845, -0.3128147 , -0.31266689, -0.30655469, -0.35824444,\n",
       "        -0.36764694, -0.25701368, -0.54063154, -0.25175457, -0.2522963 ,\n",
       "        -0.68769756, -0.25610464, -0.25314457, -0.62903   , -0.25691017,\n",
       "        -0.2528378 , -0.53617714, -0.25537416, -0.25739066, -0.53558039,\n",
       "        -0.25892411, -0.25566188, -0.4691862 , -0.25431846, -0.25606655,\n",
       "        -0.47194488, -0.25388068, -0.25370942, -0.38933208, -0.25432049,\n",
       "        -0.2582062 , -0.56009279, -0.25356247, -0.25662477, -0.49258505,\n",
       "        -0.25581573, -0.25489707, -0.50045178, -0.25477883, -0.25608464,\n",
       "        -0.51334198, -0.25428175, -0.25601163, -0.68769013, -0.25671465,\n",
       "        -0.25593961, -0.39777461, -0.25534384, -0.25566016, -0.5518161 ,\n",
       "        -0.25331999, -0.25484156, -0.59120882, -0.25422276, -0.25985915,\n",
       "        -0.5279153 , -0.25737135, -0.25243715, -0.79478437, -0.25497281,\n",
       "        -0.25183082, -0.72094963, -0.25702877, -0.25561358, -0.53498326,\n",
       "        -0.25511301, -0.25767765, -0.7660554 , -0.25895942, -0.25355523,\n",
       "        -0.56732993, -0.25115152, -0.2561622 , -0.50696383, -0.25699893,\n",
       "        -0.25328935, -0.64209388, -0.25641441, -0.25641507, -0.54573555,\n",
       "        -0.25825024, -0.25596986, -0.53180838, -0.25715673, -0.25464548,\n",
       "        -0.76973666, -0.25771051, -0.25659079, -0.74948074, -0.25779057,\n",
       "        -0.25734447, -0.77029328, -0.25151036, -0.25481253, -0.47713982,\n",
       "        -0.25449639, -0.25548908, -0.61610491, -0.25531807, -0.25425305,\n",
       "        -0.7080589 , -0.25294879, -0.25325346, -0.2534038 , -0.25344651,\n",
       "        -0.25319655, -0.25321656, -0.25314291, -0.25309417, -0.25310839,\n",
       "        -0.25326609, -0.25322632, -0.25317676, -0.25308438, -0.25302546,\n",
       "        -0.25310622, -0.2533804 , -0.25314968, -0.25324264, -0.25323531,\n",
       "        -0.25328403, -0.25316054, -0.25317425, -0.25327265, -0.25300882,\n",
       "        -0.2531689 , -0.25353126, -0.25308074, -0.25230597, -0.25305028,\n",
       "        -0.2529148 , -0.25297388, -0.25307363, -0.25310085, -0.25320973,\n",
       "        -0.25312577, -0.25307237, -0.2531509 , -0.25347023, -0.25326328,\n",
       "        -0.25331459, -0.25314151, -0.25310319, -0.2530092 , -0.25320738,\n",
       "        -0.25313548, -0.25313648, -0.25311727, -0.25314393, -0.25313715,\n",
       "        -0.24597684, -0.24475335, -0.24369243, -0.24294455, -0.24503152,\n",
       "        -0.248428  , -0.24227778, -0.24881228, -0.24455599, -0.24628352,\n",
       "        -0.2450169 , -0.25410041, -0.24258377, -0.24376077, -0.24332114,\n",
       "        -0.24253504, -0.24526763, -0.24190704, -0.24133233, -0.24827583,\n",
       "        -0.24372657, -0.25513821, -0.24592699, -0.24804637, -0.25544355,\n",
       "        -0.2438916 , -0.24263108, -0.24092547, -0.24200409, -0.24462113,\n",
       "        -0.2488695 , -0.24144663, -0.2401832 , -0.2478465 , -0.24669715,\n",
       "        -0.24947629, -0.24336731, -0.2438692 , -0.24365912, -0.24206074,\n",
       "        -0.24449187, -0.24464009, -0.24304236, -0.2411575 , -0.24343661,\n",
       "        -0.24429835, -0.24718959, -0.2421489 ]),\n",
       " 'split1_test_neg_log_loss': array([-0.25667011, -0.26454166, -0.25458236, -0.31641824, -0.26342377,\n",
       "        -0.30218098, -0.27802827, -0.47689998, -0.30112237, -0.35790894,\n",
       "        -0.25182259, -0.34458303, -0.2511919 , -0.25293191, -0.25192538,\n",
       "        -0.35587374, -0.2768023 , -0.27040694, -0.31639217, -0.31872226,\n",
       "        -0.31129676, -0.25167676, -0.25175201, -0.25181172, -0.25955744,\n",
       "        -0.26210726, -0.26824139, -0.26809554, -0.27413307, -0.27032223,\n",
       "        -0.29176131, -0.31323161, -0.31216373, -0.37568093, -0.25185524,\n",
       "        -0.33834614, -0.25255004, -0.26038223, -0.25930591, -0.27137375,\n",
       "        -0.2682507 , -0.25727368, -0.28217568, -0.31011646, -0.2823766 ,\n",
       "        -0.29623143, -0.29502956, -0.25179655, -0.26952592, -0.26283905,\n",
       "        -0.25178708, -0.26323445, -0.29466537, -0.41534161, -0.31931751,\n",
       "        -0.50754432, -0.32430258, -0.43553823, -0.38781316, -0.39565243,\n",
       "        -0.26289588, -0.25459769, -0.25776796, -0.28191826, -0.39632346,\n",
       "        -0.27203398, -0.3154803 , -0.32898127, -0.31448239, -0.4096402 ,\n",
       "        -0.41364698, -0.35742245, -0.30227389, -0.26780148, -0.27760773,\n",
       "        -0.27742095, -0.27424048, -0.27019709, -0.31271784, -0.29968061,\n",
       "        -0.2976963 , -0.40422778, -0.35234761, -0.43519248, -0.26194277,\n",
       "        -0.25303611, -0.24870714, -0.26560154, -0.28227719, -0.27867913,\n",
       "        -0.27098508, -0.30709478, -0.29577143, -0.33287316, -0.34376295,\n",
       "        -0.32122263, -0.25430876, -0.80592534, -0.25429876, -0.25366009,\n",
       "        -0.55238853, -0.25292836, -0.25249442, -0.57424188, -0.25435137,\n",
       "        -0.25324946, -0.39942586, -0.25513609, -0.25306057, -0.51925634,\n",
       "        -0.25820694, -0.25399573, -0.50714105, -0.25356459, -0.25798748,\n",
       "        -0.52195636, -0.25532217, -0.25487392, -0.44606751, -0.25458691,\n",
       "        -0.25942131, -0.59259158, -0.25343459, -0.25577039, -0.51065143,\n",
       "        -0.25308612, -0.25500959, -0.60345553, -0.25511119, -0.25369541,\n",
       "        -0.55022004, -0.25286079, -0.25898866, -0.36059578, -0.2535897 ,\n",
       "        -0.25533217, -0.48488232, -0.25351337, -0.25287034, -0.52987402,\n",
       "        -0.25629941, -0.25346972, -0.40607586, -0.25389467, -0.25446516,\n",
       "        -1.09445685, -0.2539036 , -0.25663934, -0.93560495, -0.25188346,\n",
       "        -0.25598327, -0.6768441 , -0.25456817, -0.25033546, -0.61114009,\n",
       "        -0.25449408, -0.25328716, -0.4978882 , -0.25570063, -0.25721327,\n",
       "        -0.57635132, -0.25620944, -0.255148  , -0.6134605 , -0.25793188,\n",
       "        -0.2564116 , -0.83448389, -0.25692573, -0.25928485, -0.83041918,\n",
       "        -0.26188461, -0.260077  , -0.63304571, -0.25665653, -0.25804042,\n",
       "        -0.70267277, -0.25180711, -0.25616924, -0.77180049, -0.25339739,\n",
       "        -0.25846127, -0.96431099, -0.25182083, -0.25972272, -0.81453535,\n",
       "        -0.25765881, -0.25728327, -0.64619127, -0.25409058, -0.25310911,\n",
       "        -0.46991243, -0.25148831, -0.25172967, -0.25230383, -0.25202411,\n",
       "        -0.25156581, -0.25192958, -0.25188222, -0.25191801, -0.25194493,\n",
       "        -0.25193863, -0.25194469, -0.25177574, -0.25173252, -0.2520997 ,\n",
       "        -0.25200876, -0.25195748, -0.2518107 , -0.25197228, -0.25194002,\n",
       "        -0.25169142, -0.2518868 , -0.25188753, -0.2520144 , -0.25190061,\n",
       "        -0.25177576, -0.25196275, -0.25195293, -0.25206388, -0.25193368,\n",
       "        -0.25201645, -0.25194786, -0.25191811, -0.25172306, -0.25197794,\n",
       "        -0.25193596, -0.25181816, -0.251849  , -0.25250469, -0.25203088,\n",
       "        -0.25167893, -0.2519816 , -0.25216162, -0.2522354 , -0.25193169,\n",
       "        -0.25188313, -0.25181603, -0.25187362, -0.25200592, -0.25188016,\n",
       "        -0.24543965, -0.2452924 , -0.24516339, -0.24411872, -0.2478659 ,\n",
       "        -0.24698466, -0.24381879, -0.2523097 , -0.2457897 , -0.2477233 ,\n",
       "        -0.25365947, -0.25031076, -0.24487517, -0.24650094, -0.24542567,\n",
       "        -0.24554493, -0.24587393, -0.2463731 , -0.24777189, -0.2533052 ,\n",
       "        -0.24572781, -0.25238405, -0.25095651, -0.25095087, -0.24625409,\n",
       "        -0.24450068, -0.24623988, -0.24700939, -0.24684107, -0.24607083,\n",
       "        -0.24695891, -0.24463831, -0.24918089, -0.25398495, -0.24627081,\n",
       "        -0.25038437, -0.25212089, -0.24508553, -0.2463171 , -0.24543226,\n",
       "        -0.24471305, -0.24592346, -0.24506045, -0.24540905, -0.24881814,\n",
       "        -0.24503235, -0.24482323, -0.24591056]),\n",
       " 'split2_test_neg_log_loss': array([-0.26127886, -0.2532268 , -0.26189564, -0.30301889, -0.29482449,\n",
       "        -0.29279151, -0.37092458, -0.450413  , -0.3169684 , -0.34943702,\n",
       "        -0.40817328, -0.41679364, -0.25159146, -0.25569115, -0.2692869 ,\n",
       "        -0.27223381, -0.28208691, -0.29524071, -0.34764277, -0.32943588,\n",
       "        -0.34451653, -0.36804297, -0.36267283, -0.40249962, -0.26424959,\n",
       "        -0.25855003, -0.26927819, -0.28000352, -0.29247898, -0.26813808,\n",
       "        -0.32127053, -0.32475628, -0.31352531, -0.3492987 , -0.33824999,\n",
       "        -0.3508227 , -0.25933806, -0.25524379, -0.26275149, -0.28114045,\n",
       "        -0.2737758 , -0.2849478 , -0.3028509 , -0.29056787, -0.28573788,\n",
       "        -0.31146722, -0.3284266 , -0.30491416, -0.26496829, -0.25678223,\n",
       "        -0.26608801, -0.29163328, -0.30107711, -0.30685046, -0.68504393,\n",
       "        -0.34320492, -0.50363381, -0.42094577, -0.37004173, -0.46551686,\n",
       "        -0.27187094, -0.2600012 , -0.26051655, -0.26786981, -0.30299585,\n",
       "        -0.28182729, -0.32132836, -0.31550548, -0.5310939 , -0.55089509,\n",
       "        -0.4274131 , -0.41950919, -0.26153745, -0.26406249, -0.26330208,\n",
       "        -0.30129261, -0.29946938, -0.29327823, -0.35207429, -0.35921388,\n",
       "        -0.31799439, -0.38672373, -0.32184482, -0.422542  , -0.24847611,\n",
       "        -0.25020808, -0.26187358, -0.27989045, -0.29308641, -0.27655092,\n",
       "        -0.31661395, -0.32516431, -0.34167514, -0.3454563 , -0.31248105,\n",
       "        -0.30035125, -0.25430659, -0.51329286, -0.25606675, -0.25368017,\n",
       "        -0.61472569, -0.25555702, -0.25441144, -0.58886352, -0.25409217,\n",
       "        -0.25322388, -0.62295471, -0.2558217 , -0.25717711, -0.73706787,\n",
       "        -0.25442952, -0.25605034, -0.39890423, -0.25457913, -0.25380615,\n",
       "        -0.62358512, -0.25460629, -0.25389562, -0.38523512, -0.25525538,\n",
       "        -0.25324259, -0.50103442, -0.25556003, -0.25422479, -0.50500125,\n",
       "        -0.25502544, -0.25548074, -0.41353145, -0.25380337, -0.25330243,\n",
       "        -0.57617357, -0.25418843, -0.25524277, -0.90546831, -0.25485712,\n",
       "        -0.25318398, -0.60020661, -0.25512723, -0.25411053, -0.46966151,\n",
       "        -0.25334013, -0.25340624, -0.54567872, -0.25418328, -0.25690592,\n",
       "        -0.44582367, -0.25331298, -0.25243361, -0.8501923 , -0.25381147,\n",
       "        -0.25620446, -0.60190879, -0.25549906, -0.25350344, -0.82822305,\n",
       "        -0.25459886, -0.25988372, -1.28824242, -0.25629073, -0.2565034 ,\n",
       "        -0.49179622, -0.25427155, -0.25568011, -0.66731942, -0.25424133,\n",
       "        -0.25032211, -0.5061572 , -0.25430713, -0.25497262, -1.08255073,\n",
       "        -0.25504482, -0.25514586, -0.56682873, -0.25507937, -0.25547301,\n",
       "        -0.67444824, -0.25404012, -0.25667682, -0.88671399, -0.25293095,\n",
       "        -0.25332505, -0.4293904 , -0.25936112, -0.25530505, -0.66907055,\n",
       "        -0.25639396, -0.25183202, -0.63028364, -0.25319722, -0.25707068,\n",
       "        -0.44550097, -0.25576669, -0.25261446, -0.25262247, -0.25273997,\n",
       "        -0.25263658, -0.25256562, -0.25264748, -0.25252862, -0.25247151,\n",
       "        -0.25250339, -0.25249086, -0.25254923, -0.25254477, -0.2527583 ,\n",
       "        -0.2524133 , -0.25272246, -0.25256548, -0.25235887, -0.2525434 ,\n",
       "        -0.25256026, -0.25239598, -0.25250399, -0.25245161, -0.25250113,\n",
       "        -0.25262455, -0.25233235, -0.2524946 , -0.25262125, -0.25254024,\n",
       "        -0.25252246, -0.25232792, -0.25244576, -0.25252909, -0.25243451,\n",
       "        -0.25258071, -0.25254125, -0.25249595, -0.25259329, -0.25280821,\n",
       "        -0.25261976, -0.25265805, -0.25258329, -0.25252719, -0.25260359,\n",
       "        -0.25251954, -0.25252782, -0.25258497, -0.25253298, -0.25258328,\n",
       "        -0.24924118, -0.24996118, -0.24964345, -0.25336777, -0.25476335,\n",
       "        -0.24999064, -0.25302872, -0.24990808, -0.252639  , -0.25173316,\n",
       "        -0.25075384, -0.25701253, -0.24988091, -0.24943313, -0.24921798,\n",
       "        -0.25135248, -0.25146504, -0.25058059, -0.25553714, -0.25856738,\n",
       "        -0.25237107, -0.25467716, -0.25588096, -0.25980545, -0.24989191,\n",
       "        -0.24845939, -0.24971651, -0.25081731, -0.25128512, -0.24800663,\n",
       "        -0.2598931 , -0.25831213, -0.25575579, -0.25244499, -0.25201143,\n",
       "        -0.25542024, -0.24868121, -0.25149664, -0.24846138, -0.24878597,\n",
       "        -0.25217458, -0.24913869, -0.25070364, -0.2500716 , -0.24887404,\n",
       "        -0.24976455, -0.25114281, -0.2489653 ]),\n",
       " 'split3_test_neg_log_loss': array([-0.26007989, -0.26681143, -0.26296136, -0.3118556 , -0.28815845,\n",
       "        -0.29741093, -0.31259652, -0.31337898, -0.31189679, -0.34418765,\n",
       "        -0.42245077, -0.40859461, -0.27408544, -0.2742581 , -0.26276469,\n",
       "        -0.27816832, -0.27901945, -0.29509165, -0.30397392, -0.31318797,\n",
       "        -0.2893481 , -0.42028727, -0.38717895, -0.41124358, -0.2537823 ,\n",
       "        -0.25396018, -0.2541071 , -0.28742642, -0.28370939, -0.29439871,\n",
       "        -0.31965037, -0.28484661, -0.31041897, -0.33497284, -0.35213004,\n",
       "        -0.25648297, -0.24918456, -0.26082186, -0.25736872, -0.27381958,\n",
       "        -0.27235636, -0.27247011, -0.31571796, -0.29980868, -0.29473945,\n",
       "        -0.31881825, -0.31045113, -0.32324388, -0.27876365, -0.32879058,\n",
       "        -0.2564343 , -0.29409748, -0.28633872, -0.28359279, -0.32223407,\n",
       "        -0.30753439, -0.35912522, -0.36671923, -0.39570609, -0.35267885,\n",
       "        -0.26538105, -0.25209116, -0.26100165, -0.31803803, -0.29289462,\n",
       "        -0.28106005, -0.33535862, -0.27854961, -0.40521654, -0.37779685,\n",
       "        -0.34885999, -0.41575273, -0.27308985, -0.26431299, -0.27052831,\n",
       "        -0.29972772, -0.27307208, -0.26865761, -0.30072784, -0.32822232,\n",
       "        -0.28543671, -0.35560347, -0.33729957, -0.36044467, -0.259887  ,\n",
       "        -0.25468522, -0.25591623, -0.28076641, -0.27283218, -0.28163571,\n",
       "        -0.30192381, -0.3096465 , -0.28603975, -0.3076331 , -0.32121111,\n",
       "        -0.33314049, -0.25536252, -0.465799  , -0.25638435, -0.25668536,\n",
       "        -0.55899621, -0.25320415, -0.25406033, -0.37140809, -0.2540041 ,\n",
       "        -0.25311754, -0.60238265, -0.25453949, -0.25612035, -0.46885168,\n",
       "        -0.25571378, -0.25537505, -0.40581449, -0.25487814, -0.25452631,\n",
       "        -0.39238002, -0.254054  , -0.25574299, -0.53068649, -0.25440119,\n",
       "        -0.2540709 , -0.49415174, -0.2581355 , -0.25252938, -0.53163055,\n",
       "        -0.25653693, -0.25492058, -0.51682132, -0.25403913, -0.25410956,\n",
       "        -0.41490813, -0.25408132, -0.2541545 , -0.46859744, -0.25382444,\n",
       "        -0.25447824, -0.62534123, -0.25420695, -0.2547959 , -0.45717855,\n",
       "        -0.25281778, -0.25456417, -0.39414712, -0.25452882, -0.25730692,\n",
       "        -0.65877432, -0.25919477, -0.25951378, -0.49080771, -0.25643235,\n",
       "        -0.25562318, -0.72377743, -0.26103459, -0.25727289, -0.65280169,\n",
       "        -0.25558944, -0.25509896, -0.48088247, -0.2618267 , -0.2574494 ,\n",
       "        -0.50719741, -0.26078788, -0.25554692, -0.5801153 , -0.25856632,\n",
       "        -0.25454208, -0.56301122, -0.25705155, -0.25775696, -0.71308872,\n",
       "        -0.25954515, -0.25833546, -0.93280298, -0.25539781, -0.25693471,\n",
       "        -0.56322478, -0.25681726, -0.25665766, -0.57186192, -0.25761119,\n",
       "        -0.26102717, -0.47716598, -0.25792153, -0.2553914 , -0.7809873 ,\n",
       "        -0.25940171, -0.25834216, -0.52551715, -0.26008562, -0.26049257,\n",
       "        -0.75405018, -0.25595262, -0.25536696, -0.25536511, -0.25581913,\n",
       "        -0.25582477, -0.25562772, -0.25562041, -0.25572638, -0.25585205,\n",
       "        -0.25577701, -0.25596493, -0.25596194, -0.25586157, -0.25543149,\n",
       "        -0.25570369, -0.25550161, -0.25560151, -0.25571962, -0.25572095,\n",
       "        -0.25577246, -0.25568308, -0.25586801, -0.25578553, -0.25595406,\n",
       "        -0.25582366, -0.25553921, -0.25573432, -0.25529494, -0.2557834 ,\n",
       "        -0.25590443, -0.25544952, -0.25567441, -0.25572787, -0.2558131 ,\n",
       "        -0.25597758, -0.25570473, -0.25592666, -0.25544492, -0.25532288,\n",
       "        -0.25534256, -0.25543695, -0.25555836, -0.25517128, -0.25564589,\n",
       "        -0.25552401, -0.25555536, -0.25557274, -0.25557863, -0.25563823,\n",
       "        -0.24526462, -0.2466502 , -0.24692946, -0.24641141, -0.24859403,\n",
       "        -0.24858247, -0.24836241, -0.24748074, -0.24706143, -0.25353416,\n",
       "        -0.24790034, -0.24729578, -0.24877135, -0.2473654 , -0.24625119,\n",
       "        -0.24829227, -0.24709297, -0.24874841, -0.24656848, -0.24720061,\n",
       "        -0.24776253, -0.2469273 , -0.24792649, -0.24931604, -0.24611047,\n",
       "        -0.2453531 , -0.24706933, -0.24626289, -0.24760572, -0.24849639,\n",
       "        -0.24874327, -0.24782186, -0.24850627, -0.24489796, -0.24833085,\n",
       "        -0.24750675, -0.24734716, -0.24673246, -0.25006798, -0.2458499 ,\n",
       "        -0.24630075, -0.24641095, -0.24760779, -0.24709405, -0.24474392,\n",
       "        -0.24692938, -0.2470186 , -0.24736857]),\n",
       " 'split4_test_neg_log_loss': array([-0.28281983, -0.26169672, -0.27790211, -0.27831529, -0.30605367,\n",
       "        -0.29135468, -0.35724361, -0.33873978, -0.3269721 , -0.25810538,\n",
       "        -0.25825318, -0.40886577, -0.26896785, -0.26524424, -0.27360646,\n",
       "        -0.29154465, -0.29259612, -0.31958061, -0.36514758, -0.35718141,\n",
       "        -0.36777596, -0.38778779, -0.41325487, -0.25814389, -0.26615895,\n",
       "        -0.25957939, -0.27465914, -0.2922329 , -0.28518002, -0.29937124,\n",
       "        -0.34347576, -0.33038877, -0.35452887, -0.38845071, -0.34879123,\n",
       "        -0.35988983, -0.26439063, -0.27199647, -0.27067692, -0.2872618 ,\n",
       "        -0.2834583 , -0.28951096, -0.30004287, -0.31219676, -0.30439295,\n",
       "        -0.33868813, -0.3637261 , -0.25810961, -0.28555098, -0.27682992,\n",
       "        -0.28789619, -0.33536122, -0.30198051, -0.28967629, -0.3835458 ,\n",
       "        -0.35163688, -0.72218894, -0.49247855, -0.46878617, -0.42843441,\n",
       "        -0.26391304, -0.26821425, -0.27412686, -0.31056907, -0.29450704,\n",
       "        -0.3109156 , -0.37295059, -0.35100814, -0.37281738, -0.47495124,\n",
       "        -0.43429522, -0.42531342, -0.27048836, -0.26469574, -0.28323033,\n",
       "        -0.27753664, -0.32636013, -0.31268277, -0.36010757, -0.32717051,\n",
       "        -0.35773829, -0.40936549, -0.4297479 , -0.40838093, -0.27294278,\n",
       "        -0.2601976 , -0.27535793, -0.29084832, -0.29692332, -0.29958054,\n",
       "        -0.31766083, -0.3196455 , -0.31506872, -0.39702214, -0.36277979,\n",
       "        -0.43596712, -0.25738927, -0.79833803, -0.25912152, -0.25671138,\n",
       "        -0.83013393, -0.25598813, -0.25779251, -0.49203112, -0.25778757,\n",
       "        -0.25644588, -0.52680356, -0.2573654 , -0.25870941, -0.5504552 ,\n",
       "        -0.25760391, -0.25605841, -0.60769065, -0.25780237, -0.25581759,\n",
       "        -0.49184615, -0.2563141 , -0.25718063, -0.499677  , -0.25629164,\n",
       "        -0.25703942, -0.90520279, -0.2577358 , -0.25818283, -0.51855467,\n",
       "        -0.25715666, -0.25751892, -0.55662179, -0.2568498 , -0.25671916,\n",
       "        -0.39423171, -0.25619056, -0.25865691, -0.45099207, -0.25632563,\n",
       "        -0.25876406, -0.4319496 , -0.25757647, -0.25723486, -0.6768056 ,\n",
       "        -0.25671763, -0.25617576, -0.4352086 , -0.25705326, -0.25794311,\n",
       "        -0.7382619 , -0.26064636, -0.25805737, -0.49738623, -0.2610264 ,\n",
       "        -0.25800512, -0.50755267, -0.26365469, -0.25711601, -0.53660061,\n",
       "        -0.26100023, -0.25873579, -0.60281838, -0.26287039, -0.25914455,\n",
       "        -0.71095165, -0.25954929, -0.25876246, -1.05928349, -0.25869915,\n",
       "        -0.25598638, -0.6868775 , -0.2623246 , -0.26041352, -0.87892739,\n",
       "        -0.26280001, -0.2602321 , -0.79326016, -0.26088916, -0.26115275,\n",
       "        -0.8023423 , -0.2592838 , -0.25791953, -0.75325336, -0.26197786,\n",
       "        -0.25613316, -0.49403564, -0.2590978 , -0.26003991, -0.60583015,\n",
       "        -0.26190088, -0.25994254, -0.6562924 , -0.25989166, -0.25800035,\n",
       "        -0.67577329, -0.25663791, -0.25739585, -0.25760914, -0.25730445,\n",
       "        -0.25736749, -0.25758231, -0.25764016, -0.2576337 , -0.25759959,\n",
       "        -0.2578262 , -0.25790076, -0.25771767, -0.25763688, -0.2575226 ,\n",
       "        -0.25770725, -0.25744962, -0.2575981 , -0.25755072, -0.25775154,\n",
       "        -0.25782917, -0.25759179, -0.25760886, -0.25782879, -0.25761591,\n",
       "        -0.25771265, -0.25754461, -0.25753664, -0.25749753, -0.25761281,\n",
       "        -0.25767925, -0.25756858, -0.25767005, -0.25784286, -0.25765995,\n",
       "        -0.25768476, -0.25757995, -0.25775661, -0.25748206, -0.25732116,\n",
       "        -0.25723861, -0.25750487, -0.25740178, -0.2575462 , -0.25752275,\n",
       "        -0.2575308 , -0.25744919, -0.25753994, -0.25761716, -0.25758244,\n",
       "        -0.25456896, -0.25748454, -0.25448622, -0.25432101, -0.25319588,\n",
       "        -0.26062901, -0.25613418, -0.26217625, -0.25975009, -0.26356991,\n",
       "        -0.26951187, -0.26052151, -0.25253217, -0.25521912, -0.25350211,\n",
       "        -0.2533566 , -0.25544026, -0.25595073, -0.26248727, -0.25523228,\n",
       "        -0.26296464, -0.26302646, -0.26958309, -0.26276152, -0.25246221,\n",
       "        -0.2519919 , -0.25629851, -0.25479383, -0.25633198, -0.25250143,\n",
       "        -0.25420908, -0.26391933, -0.26265759, -0.26179518, -0.26475485,\n",
       "        -0.26322215, -0.2553626 , -0.25246429, -0.25444963, -0.25358312,\n",
       "        -0.25507051, -0.25414611, -0.2543866 , -0.25606213, -0.2525203 ,\n",
       "        -0.25351021, -0.25850528, -0.25450266]),\n",
       " 'mean_test_neg_log_loss': array([-0.26333306, -0.26239513, -0.26652546, -0.29964738, -0.28751098,\n",
       "        -0.29082082, -0.32349142, -0.39002879, -0.31116441, -0.32950196,\n",
       "        -0.35359557, -0.41678738, -0.26146537, -0.261069  , -0.26213172,\n",
       "        -0.29454649, -0.29985033, -0.29733976, -0.33106178, -0.32435838,\n",
       "        -0.32582579, -0.35948139, -0.35613396, -0.33789094, -0.26111359,\n",
       "        -0.25928928, -0.26232511, -0.2800779 , -0.2828735 , -0.28199144,\n",
       "        -0.31530086, -0.31183481, -0.3173379 , -0.35651742, -0.30886136,\n",
       "        -0.33325546, -0.25711319, -0.2614323 , -0.26097972, -0.27002831,\n",
       "        -0.27219381, -0.27602432, -0.29785516, -0.29939831, -0.29141577,\n",
       "        -0.30372327, -0.3236575 , -0.28825458, -0.30535756, -0.27587862,\n",
       "        -0.26473169, -0.29516801, -0.29546794, -0.32014427, -0.43989561,\n",
       "        -0.40571131, -0.45298356, -0.43983848, -0.39196311, -0.39611675,\n",
       "        -0.2642639 , -0.28448231, -0.28881398, -0.29678927, -0.31531945,\n",
       "        -0.28783279, -0.36653449, -0.31797463, -0.38640113, -0.43738593,\n",
       "        -0.40757482, -0.4032041 , -0.27081884, -0.27151057, -0.27254706,\n",
       "        -0.28443353, -0.29857146, -0.29718224, -0.33093355, -0.33413893,\n",
       "        -0.31477652, -0.39205691, -0.36533732, -0.40943925, -0.26212684,\n",
       "        -0.25507434, -0.26180592, -0.28024703, -0.28047981, -0.28228696,\n",
       "        -0.30036242, -0.31487316, -0.31024439, -0.33790788, -0.33969587,\n",
       "        -0.35166569, -0.25567616, -0.62479735, -0.25552519, -0.25460666,\n",
       "        -0.64878838, -0.25475646, -0.25438065, -0.53111492, -0.25542908,\n",
       "        -0.25377491, -0.53754878, -0.25564737, -0.25649162, -0.5622423 ,\n",
       "        -0.25697565, -0.25542828, -0.47774732, -0.25502854, -0.25564082,\n",
       "        -0.50034251, -0.25483545, -0.25508052, -0.45019964, -0.25497112,\n",
       "        -0.25639608, -0.61061466, -0.25568568, -0.25546643, -0.51168459,\n",
       "        -0.25552418, -0.25556538, -0.51817637, -0.25491647, -0.25478224,\n",
       "        -0.48977508, -0.25432057, -0.25661089, -0.57466875, -0.25506231,\n",
       "        -0.25553961, -0.50803088, -0.25515357, -0.25493436, -0.53706715,\n",
       "        -0.25449899, -0.25449149, -0.47446382, -0.25477656, -0.25729605,\n",
       "        -0.69304641, -0.25688581, -0.25581625, -0.71375511, -0.2556253 ,\n",
       "        -0.25552937, -0.64620652, -0.25835706, -0.25476828, -0.63274974,\n",
       "        -0.25615913, -0.25693665, -0.72717737, -0.25912957, -0.25677317,\n",
       "        -0.57072531, -0.25639393, -0.25625994, -0.68542851, -0.25728752,\n",
       "        -0.2541103 , -0.64652474, -0.25740469, -0.25776861, -0.81014431,\n",
       "        -0.25950496, -0.25795206, -0.69154919, -0.25703592, -0.25724927,\n",
       "        -0.70248495, -0.25593176, -0.25680281, -0.7466221 , -0.25674159,\n",
       "        -0.25725822, -0.62703926, -0.25594233, -0.25705432, -0.66951263,\n",
       "        -0.25797035, -0.25657781, -0.61487787, -0.25651663, -0.25658515,\n",
       "        -0.61065915, -0.25455887, -0.25407208, -0.25426087, -0.25426684,\n",
       "        -0.25411824, -0.25418436, -0.25418664, -0.25418018, -0.25419529,\n",
       "        -0.25426227, -0.25430551, -0.25423627, -0.25417202, -0.25416751,\n",
       "        -0.25418785, -0.25420232, -0.2541451 , -0.25416883, -0.25423824,\n",
       "        -0.25422747, -0.25414364, -0.25420853, -0.2542706 , -0.2541961 ,\n",
       "        -0.2542211 , -0.25418203, -0.25415984, -0.25395671, -0.25418408,\n",
       "        -0.25420748, -0.25405355, -0.25415639, -0.25418474, -0.25421905,\n",
       "        -0.25426096, -0.2541433 , -0.25423582, -0.25429904, -0.25414928,\n",
       "        -0.25403889, -0.2541446 , -0.25416165, -0.25409785, -0.25418226,\n",
       "        -0.25411859, -0.25409698, -0.25413771, -0.25417572, -0.25416425,\n",
       "        -0.24809825, -0.24882833, -0.24798299, -0.24823269, -0.24989013,\n",
       "        -0.25092295, -0.24872438, -0.25213741, -0.24995924, -0.25256881,\n",
       "        -0.25336848, -0.2538482 , -0.24772867, -0.24845587, -0.24754362,\n",
       "        -0.24821626, -0.24902797, -0.24871197, -0.25073942, -0.25251626,\n",
       "        -0.25051052, -0.25443064, -0.25405481, -0.25417605, -0.25003245,\n",
       "        -0.24683933, -0.24839106, -0.24796178, -0.2488136 , -0.24793928,\n",
       "        -0.25173477, -0.25122765, -0.25125675, -0.25219392, -0.25161302,\n",
       "        -0.25320196, -0.24937584, -0.24792963, -0.24859104, -0.2471424 ,\n",
       "        -0.24855015, -0.24805186, -0.24816017, -0.24795887, -0.2476786 ,\n",
       "        -0.24790697, -0.2497359 , -0.2477792 ]),\n",
       " 'std_test_neg_log_loss': array([0.00995418, 0.00489026, 0.00875155, 0.01426979, 0.01402691,\n",
       "        0.01090919, 0.03518837, 0.06335889, 0.01035653, 0.0362975 ,\n",
       "        0.08074253, 0.05126877, 0.00914978, 0.0077632 , 0.00859602,\n",
       "        0.03137406, 0.03487129, 0.01619381, 0.02220404, 0.0184642 ,\n",
       "        0.02736309, 0.05709007, 0.05524235, 0.06942704, 0.00428854,\n",
       "        0.00302437, 0.01087671, 0.00895301, 0.00617639, 0.01266704,\n",
       "        0.01803402, 0.01597592, 0.01962351, 0.02191821, 0.04619354,\n",
       "        0.03922522, 0.00548696, 0.00563465, 0.00550018, 0.01765063,\n",
       "        0.00674027, 0.01118703, 0.01168251, 0.01081415, 0.00769295,\n",
       "        0.02862792, 0.02294003, 0.02815431, 0.06172639, 0.02759478,\n",
       "        0.0125347 , 0.02307484, 0.00570366, 0.04842541, 0.13718655,\n",
       "        0.08898511, 0.1482401 , 0.0456072 , 0.04332029, 0.04709674,\n",
       "        0.00469434, 0.05180809, 0.05123635, 0.01884487, 0.0407355 ,\n",
       "        0.01337566, 0.0637324 , 0.02356859, 0.08090949, 0.06734822,\n",
       "        0.03042648, 0.02463718, 0.01823362, 0.01265691, 0.00706201,\n",
       "        0.0137675 , 0.02219139, 0.02729738, 0.02256473, 0.0218887 ,\n",
       "        0.02453033, 0.0197812 , 0.03846961, 0.02593018, 0.00819794,\n",
       "        0.00343349, 0.00915152, 0.00827454, 0.01434758, 0.00892948,\n",
       "        0.0170979 , 0.00664442, 0.01903873, 0.03309504, 0.01987859,\n",
       "        0.04746634, 0.00130901, 0.14677965, 0.00243667, 0.00178006,\n",
       "        0.10287465, 0.00139478, 0.00183467, 0.09145055, 0.00159596,\n",
       "        0.00134344, 0.07834907, 0.00095346, 0.00190298, 0.09163337,\n",
       "        0.00166026, 0.00076074, 0.07643706, 0.00145369, 0.00143718,\n",
       "        0.07508717, 0.00089421, 0.00127874, 0.0580835 , 0.00073761,\n",
       "        0.00237459, 0.15180968, 0.00198976, 0.00194838, 0.01308102,\n",
       "        0.00141199, 0.00099948, 0.06327265, 0.00107734, 0.00136172,\n",
       "        0.07267363, 0.00106809, 0.00190288, 0.19726959, 0.00127037,\n",
       "        0.0018588 , 0.09026488, 0.00137795, 0.00146814, 0.07838599,\n",
       "        0.0016567 , 0.00101847, 0.0792081 , 0.00115595, 0.00174094,\n",
       "        0.22482737, 0.00287637, 0.00290628, 0.18488874, 0.00308351,\n",
       "        0.00202382, 0.08213357, 0.00345028, 0.00259804, 0.10759351,\n",
       "        0.00245202, 0.00241525, 0.29833047, 0.00286754, 0.0018277 ,\n",
       "        0.07742377, 0.00350098, 0.00129253, 0.19401824, 0.00163742,\n",
       "        0.0021921 , 0.112831  , 0.00265086, 0.0019465 , 0.17814557,\n",
       "        0.00275641, 0.00208158, 0.15036189, 0.00207449, 0.00227582,\n",
       "        0.08327517, 0.00267489, 0.00058828, 0.10084935, 0.00331602,\n",
       "        0.0025454 , 0.20663541, 0.0035268 , 0.00231883, 0.12208288,\n",
       "        0.00253568, 0.00277992, 0.04673302, 0.00291442, 0.00264681,\n",
       "        0.12757305, 0.00198718, 0.00204968, 0.00198389, 0.00198347,\n",
       "        0.00214614, 0.00211047, 0.00213313, 0.00216065, 0.0021703 ,\n",
       "        0.00221275, 0.00226762, 0.00224147, 0.00222   , 0.00202091,\n",
       "        0.00217951, 0.00200689, 0.00214382, 0.00213545, 0.00217724,\n",
       "        0.00225657, 0.00216212, 0.00217616, 0.00220684, 0.00220623,\n",
       "        0.00220884, 0.00209375, 0.00213024, 0.00211752, 0.00216176,\n",
       "        0.00220199, 0.00213923, 0.00217985, 0.00226887, 0.00217286,\n",
       "        0.0021994 , 0.00216187, 0.00224303, 0.00191107, 0.00192402,\n",
       "        0.00200228, 0.00204236, 0.00200304, 0.00200819, 0.00208848,\n",
       "        0.00210424, 0.00209532, 0.00210774, 0.00211106, 0.00212735,\n",
       "        0.00354329, 0.00469182, 0.00380836, 0.00472531, 0.00357938,\n",
       "        0.0049455 , 0.00527612, 0.00526344, 0.00562173, 0.00609325,\n",
       "        0.00856947, 0.00469164, 0.00356105, 0.00384061, 0.00352966,\n",
       "        0.00389042, 0.00387012, 0.00464048, 0.00742598, 0.00426269,\n",
       "        0.00685587, 0.00519496, 0.00845789, 0.0059497 , 0.0036017 ,\n",
       "        0.00301797, 0.00455712, 0.00465046, 0.00478234, 0.00266943,\n",
       "        0.00474654, 0.0085083 , 0.0075461 , 0.00578941, 0.00687523,\n",
       "        0.00564942, 0.00410066, 0.00344371, 0.00363362, 0.00386271,\n",
       "        0.00428729, 0.00338183, 0.00403344, 0.00497262, 0.00324889,\n",
       "        0.003379  , 0.00483494, 0.00404901]),\n",
       " 'rank_test_neg_log_loss': array([174, 173, 177, 208, 193, 197, 224, 244, 215, 228, 237, 252, 168,\n",
       "        165, 171, 199, 209, 204, 230, 226, 227, 240, 238, 233, 166, 162,\n",
       "        172, 185, 190, 188, 219, 216, 221, 239, 213, 231, 151, 167, 164,\n",
       "        178, 181, 184, 205, 207, 198, 211, 225, 195, 212, 183, 176, 200,\n",
       "        201, 223, 255, 249, 257, 254, 245, 247, 175, 192, 196, 202, 220,\n",
       "        194, 242, 222, 243, 253, 250, 248, 179, 180, 182, 191, 206, 203,\n",
       "        229, 232, 217, 246, 241, 251, 170, 115, 169, 186, 187, 189, 210,\n",
       "        218, 214, 234, 235, 236, 129, 274, 122, 104, 279, 105,  99, 265,\n",
       "        119,  45, 267, 128, 138, 268, 148, 118, 259, 113, 127, 261, 109,\n",
       "        116, 256, 112, 137, 271, 130, 120, 263, 121, 125, 264, 110, 108,\n",
       "        260,  98, 142, 270, 114, 124, 262, 117, 111, 266, 102, 101, 258,\n",
       "        107, 155, 283, 146, 131, 285, 126, 123, 277, 160, 106, 276, 134,\n",
       "        147, 286, 161, 144, 269, 136, 135, 281, 154,  54, 278, 156, 157,\n",
       "        288, 163, 158, 282, 149, 152, 284, 132, 145, 287, 143, 153, 275,\n",
       "        133, 150, 280, 159, 140, 273, 139, 141, 272, 103,  51,  91,  94,\n",
       "         55,  76,  78,  72,  80,  93,  97,  89,  69,  67,  79,  82,  61,\n",
       "         68,  90,  87,  59,  84,  95,  81,  86,  73,  64,  47,  75,  83,\n",
       "         49,  63,  77,  85,  92,  58,  88,  96,  62,  48,  60,  65,  53,\n",
       "         74,  56,  52,  57,  70,  66,  14,  25,  12,  17,  29,  34,  23,\n",
       "         39,  30,  42,  44,  46,   5,  19,   3,  16,  26,  22,  33,  41,\n",
       "         32, 100,  50,  71,  31,   1,  18,  11,  24,   9,  38,  35,  36,\n",
       "         40,  37,  43,  27,   8,  21,   2,  20,  13,  15,  10,   4,   7,\n",
       "         28,   6])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRIAL_1_MLP.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL ONE NEG LOG LOSS RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'tanh',\n",
       " 'classifier__alpha': 0.01,\n",
       " 'classifier__hidden_layer_sizes': (5,),\n",
       " 'classifier__learning_rate': 'invscaling',\n",
       " 'classifier__solver': 'adam'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL ONE NEG LOG LOSS RESULTS ---------------------------------------------\")\n",
    "TRIAL_1_MLP.cv_results_['rank_test_neg_log_loss']\n",
    "TRIAL_1_MLP.cv_results_['params'][ np.argmin(TRIAL_1_MLP.cv_results_['rank_test_neg_log_loss']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL ONE F1 RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'tanh',\n",
       " 'classifier__alpha': 0.0001,\n",
       " 'classifier__hidden_layer_sizes': (20,),\n",
       " 'classifier__learning_rate': 'invscaling',\n",
       " 'classifier__solver': 'adam'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL ONE F1 RESULTS ---------------------------------------------\")\n",
    "TRIAL_1_MLP.cv_results_['rank_test_f1_micro']\n",
    "TRIAL_1_MLP.cv_results_['params'][ np.argmin(TRIAL_1_MLP.cv_results_['rank_test_f1_micro']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL ONE ROC AUC RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.0001,\n",
       " 'classifier__hidden_layer_sizes': (15,),\n",
       " 'classifier__learning_rate': 'adaptive',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL ONE ROC AUC RESULTS ---------------------------------------------\")\n",
    "TRIAL_1_MLP.cv_results_['rank_test_roc_auc_ovo']\n",
    "TRIAL_1_MLP.cv_results_['params'][ np.argmin(TRIAL_1_MLP.cv_results_['rank_test_roc_auc_ovo']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 1 MLP using best NEG LOG LOSS hyperparameters :0.9319\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 1 MLP using best F1 hyperparameters :0.9319\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 1 MLP using best ROC_AUC hyperparameters :0.9195\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best NEG LOG LOSS results to train on test set, \n",
    "p = TRIAL_1_MLP.cv_results_['params'][ np.argmin(TRIAL_1_MLP.cv_results_['rank_test_neg_log_loss']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train__neg_log_loss = pipe.predict(X_train)\n",
    "y_pred_test_neg_log_loss = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 1 MLP using best NEG LOG LOSS hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_neg_log_loss)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best F1 results to train on test set, LR_Trial 1\n",
    "p = TRIAL_1_MLP.cv_results_['params'][ np.argmin(TRIAL_1_MLP.cv_results_['rank_test_f1_micro']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train_roc_auc = pipe.predict(X_train)\n",
    "y_pred_test_f1 = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 1 MLP using best F1 hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_f1)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best ROC_AUC results to train on test set, LR_Trial 1\n",
    "p  = TRIAL_1_MLP.cv_results_['params'][ np.argmin(TRIAL_1_MLP.cv_results_['rank_test_roc_auc_ovo']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train_roc_auc = pipe.predict(X_train)\n",
    "y_pred_test_roc_auc = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 1 MLP using best ROC_AUC hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_roc_auc)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTI LAYERED PERCEPTRON TRIAL TWO ON POKER DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=-1)]: Done 189 tasks      | elapsed:   10.9s\n",
      "[Parallel(n_jobs=-1)]: Done 210 tasks      | elapsed:   11.9s\n",
      "[Parallel(n_jobs=-1)]: Done 233 tasks      | elapsed:   13.6s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:   14.4s\n",
      "[Parallel(n_jobs=-1)]: Done 281 tasks      | elapsed:   16.4s\n",
      "[Parallel(n_jobs=-1)]: Done 306 tasks      | elapsed:   18.2s\n",
      "[Parallel(n_jobs=-1)]: Done 333 tasks      | elapsed:   19.7s\n",
      "[Parallel(n_jobs=-1)]: Done 360 tasks      | elapsed:   22.0s\n",
      "[Parallel(n_jobs=-1)]: Done 389 tasks      | elapsed:   23.2s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:   25.7s\n",
      "[Parallel(n_jobs=-1)]: Done 449 tasks      | elapsed:   27.1s\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:   30.0s\n",
      "[Parallel(n_jobs=-1)]: Done 513 tasks      | elapsed:   32.4s\n",
      "[Parallel(n_jobs=-1)]: Done 546 tasks      | elapsed:   34.7s\n",
      "[Parallel(n_jobs=-1)]: Done 581 tasks      | elapsed:   37.0s\n",
      "[Parallel(n_jobs=-1)]: Done 616 tasks      | elapsed:   39.6s\n",
      "[Parallel(n_jobs=-1)]: Done 653 tasks      | elapsed:   42.8s\n",
      "[Parallel(n_jobs=-1)]: Done 690 tasks      | elapsed:   45.4s\n",
      "[Parallel(n_jobs=-1)]: Done 729 tasks      | elapsed:   48.3s\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:   51.6s\n",
      "[Parallel(n_jobs=-1)]: Done 809 tasks      | elapsed:   54.5s\n",
      "[Parallel(n_jobs=-1)]: Done 850 tasks      | elapsed:   57.6s\n",
      "[Parallel(n_jobs=-1)]: Done 893 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 936 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 981 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1026 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1073 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1120 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1169 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1269 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1320 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1373 tasks      | elapsed:  1.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL TWO RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1440 out of 1440 | elapsed:  1.8min finished\n"
     ]
    }
   ],
   "source": [
    "ITERATIONS = 1\n",
    "for i in range(1):\n",
    "    #For reach \"trial\", randomly select 5000 data points\n",
    "    random5000DataPoints = pokerData.sample(n = 5000, replace = True)\n",
    "    #From that 5000, create x set and y set\n",
    "    xSet = random5000DataPoints.iloc[:,0:9].values\n",
    "    ySet = random5000DataPoints.iloc[:,10].values\n",
    "    pipe = Pipeline([('std', StandardScaler()),('classifier', MLPClassifier(max_iter = 250))])\n",
    "\n",
    "    search_space = [\n",
    "                {\n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                {\n",
    "                 \n",
    "                  \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['sgd'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                \n",
    "                {\n",
    "                \n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['adam'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                ]\n",
    "    \n",
    "    #'precision_micro','roc_auc_ovo','jaccard','neg_log_loss'] JACCARD DOES NOT WORK FOR NON BINARY?\n",
    "    runHyperParamSearch = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=5), \n",
    "                          scoring=['recall_micro','f1_micro','roc_auc_ovo','neg_log_loss'], \n",
    "                          refit=False, verbose=10, n_jobs = -1)\n",
    "    TRIAL_2_MLP = runHyperParamSearch.fit(xSet, ySet)\n",
    "    print(\"---------------------------------TRIAL TWO RESULTS ---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.36431441, 0.41075478, 0.4355762 , 0.70260544, 0.70960984,\n",
       "        0.71511531, 1.19442711, 1.21574726, 1.29061055, 1.50649638,\n",
       "        1.19492574, 1.56034117, 0.42005916, 0.33448668, 0.34579844,\n",
       "        0.55557852, 0.65936804, 0.75635195, 0.99936051, 1.08984046,\n",
       "        1.06141458, 1.47777157, 1.45595102, 1.48437681, 0.42036233,\n",
       "        0.37271929, 0.32858353, 0.57309494, 0.62553706, 0.62273531,\n",
       "        1.14008207, 1.013973  , 1.09654379, 1.54352756, 1.49338078,\n",
       "        1.21354175, 0.38663149, 0.32227468, 0.28514385, 0.62253704,\n",
       "        0.44077973, 0.57429528, 0.94941998, 1.0364944 , 0.97233834,\n",
       "        1.40651054, 1.34705625, 1.11015077, 0.59641166, 0.47360711,\n",
       "        0.50563593, 0.84733   , 0.68869362, 0.94461303, 1.25588078,\n",
       "        1.38529162, 1.26668911, 1.78914065, 1.66903553, 1.6389081 ,\n",
       "        0.5934104 , 0.61522694, 0.43186989, 0.58620405, 0.63755059,\n",
       "        0.85834012, 1.10304813, 1.17881403, 1.26148496, 1.78393397,\n",
       "        1.64671521, 1.77702894, 0.40625029, 0.50393338, 0.37101846,\n",
       "        0.69669981, 0.79058003, 0.65526204, 1.16800389, 1.09984727,\n",
       "        1.21144233, 1.60608091, 1.60097713, 1.92825756, 0.43277216,\n",
       "        0.34119296, 0.33658996, 0.65686641, 0.6390501 , 0.70920954,\n",
       "        1.19562793, 1.29711647, 1.27679958, 1.82066388, 1.57355237,\n",
       "        1.47196584, 0.84923019, 1.25367799, 1.16720357, 0.58210106,\n",
       "        1.79144058, 1.06431537, 0.54647055, 1.97569866, 1.23966637,\n",
       "        0.49502583, 1.60067682, 1.16460261, 0.71821756, 1.70526662,\n",
       "        1.2385653 , 0.56678772, 0.59180956, 1.11976299, 0.54847174,\n",
       "        2.1568552 , 1.16690426, 0.50443358, 2.43059206, 1.06971998,\n",
       "        0.69109445, 0.63694773, 1.18271699, 0.51854572, 2.48553762,\n",
       "        1.11996331, 0.5311564 , 2.44560375, 1.12496767, 0.46329856,\n",
       "        1.60998468, 1.13877892, 0.69249563, 1.3066237 , 1.24637175,\n",
       "        0.54286699, 1.7597136 , 1.08983693, 0.50283213, 2.21580572,\n",
       "        1.01527252, 0.39724178, 1.83327656, 1.06651745, 0.94731522,\n",
       "        0.48491678, 1.53241811, 0.79818616, 1.55233531, 1.46175699,\n",
       "        0.85343399, 2.12823057, 1.54592953, 0.86314235, 1.8738111 ,\n",
       "        1.43713598, 0.90617919, 0.78017073, 1.35016236, 0.85543604,\n",
       "        0.62643914, 1.34395638, 0.77026272, 1.47857161, 1.45455117,\n",
       "        0.7784698 , 1.81766315, 1.3961009 , 0.81289887, 1.03949351,\n",
       "        1.29451346, 0.70901008, 2.17947469, 1.44774613, 0.84042306,\n",
       "        1.40480847, 1.39570074, 0.74624214, 1.33224621, 1.43473396,\n",
       "        0.78637638, 1.19042406, 1.37027831, 0.80969658, 0.74784365,\n",
       "        1.45064783, 0.85303373, 1.40811186, 1.49858875, 0.78417425,\n",
       "        2.13203349, 1.71847811, 0.8032907 , 1.0272831 , 0.7879776 ,\n",
       "        0.71741686, 0.66427121, 0.64465437, 0.49252381, 0.51514335,\n",
       "        0.46409874, 0.48131428, 0.48521738, 0.45769367, 0.80188985,\n",
       "        0.85823851, 0.80989623, 0.64265265, 0.77346492, 0.65896659,\n",
       "        0.58480287, 0.53205752, 0.56188359, 0.4774107 , 0.50093079,\n",
       "        0.48872042, 0.64755692, 0.87064877, 0.78727717, 0.66617346,\n",
       "        0.66507206, 0.58400254, 0.61893239, 0.60471911, 0.62253571,\n",
       "        0.54466825, 0.56068196, 0.48301549, 0.81580157, 0.87255068,\n",
       "        0.78617573, 0.60151725, 0.64475436, 0.6172308 , 0.54807138,\n",
       "        0.54707012, 0.51063881, 0.49792852, 0.53405919, 0.49272356,\n",
       "        1.22755547, 1.41501675, 1.36537404, 1.35216336, 2.2102015 ,\n",
       "        2.05146494, 3.03901401, 2.94283094, 3.31975493, 3.79726563,\n",
       "        3.50051022, 3.57267251, 1.49158239, 1.48157487, 1.5027926 ,\n",
       "        1.83547888, 2.55259557, 1.71517553, 3.23197899, 3.65083966,\n",
       "        2.94773493, 3.45036755, 3.4539701 , 3.38380995, 1.20253396,\n",
       "        1.45955553, 1.34245486, 1.84478698, 1.43503404, 1.94467273,\n",
       "        2.48430719, 3.02400045, 2.44680471, 3.19294605, 3.18624043,\n",
       "        3.17342916, 1.08823619, 0.97083549, 1.07652545, 1.31963534,\n",
       "        1.45044785, 1.29091024, 1.95518208, 1.86009979, 1.87271118,\n",
       "        2.24272814, 2.54769025, 1.79003844]),\n",
       " 'std_fit_time': array([0.10798604, 0.09303858, 0.03258055, 0.0847485 , 0.09800205,\n",
       "        0.15856553, 0.06993633, 0.12493631, 0.08390995, 0.04715555,\n",
       "        0.54977592, 0.0621073 , 0.06111767, 0.1042016 , 0.11904785,\n",
       "        0.0395941 , 0.12651918, 0.06003018, 0.05706685, 0.07945616,\n",
       "        0.09280044, 0.01542958, 0.07558704, 0.04504718, 0.09969171,\n",
       "        0.07595332, 0.09380757, 0.10935516, 0.08749618, 0.10318503,\n",
       "        0.1047643 , 0.13598005, 0.21001135, 0.08008472, 0.07099856,\n",
       "        0.57780999, 0.03775884, 0.11069487, 0.04010674, 0.12924555,\n",
       "        0.09930237, 0.17960766, 0.16338108, 0.12005051, 0.07413182,\n",
       "        0.07512815, 0.14157053, 0.52225507, 0.15262197, 0.13115185,\n",
       "        0.14108844, 0.16964045, 0.17843497, 0.13271214, 0.13248534,\n",
       "        0.15977522, 0.12303708, 0.19717606, 0.04763396, 0.05383146,\n",
       "        0.17419731, 0.09830888, 0.18662023, 0.08857135, 0.08210402,\n",
       "        0.1004606 , 0.19834134, 0.16295757, 0.2466048 , 0.23956746,\n",
       "        0.19053264, 0.16541418, 0.10237739, 0.17650709, 0.14401269,\n",
       "        0.16184444, 0.1803066 , 0.20800568, 0.17708317, 0.27196081,\n",
       "        0.22636899, 0.17847618, 0.25438416, 0.20093626, 0.16673456,\n",
       "        0.11171928, 0.05346641, 0.15569034, 0.16509343, 0.11067899,\n",
       "        0.22625617, 0.27919742, 0.20731612, 0.16717202, 0.18535292,\n",
       "        0.27902489, 0.08125667, 0.52693584, 0.0761309 , 0.05171401,\n",
       "        0.9718245 , 0.12862294, 0.04011797, 0.71004461, 0.0927176 ,\n",
       "        0.06018552, 1.01776562, 0.07643273, 0.02585592, 1.0586735 ,\n",
       "        0.12710641, 0.05748824, 0.36480407, 0.08311951, 0.02865134,\n",
       "        0.50294599, 0.10225017, 0.0536514 , 0.86434455, 0.07076539,\n",
       "        0.06095111, 0.1952022 , 0.10810668, 0.03576041, 0.47467105,\n",
       "        0.13023354, 0.04095191, 0.71902173, 0.06976589, 0.04375645,\n",
       "        0.95269419, 0.05260626, 0.05949375, 1.0699975 , 0.13419076,\n",
       "        0.06926613, 0.46420918, 0.0585619 , 0.05949441, 0.73167587,\n",
       "        0.02390623, 0.02235426, 0.94003881, 0.05985221, 0.11902952,\n",
       "        0.08708827, 0.12448862, 0.10149681, 0.75947494, 0.08948739,\n",
       "        0.14977296, 1.00589345, 0.11680724, 0.12074384, 1.19053418,\n",
       "        0.06230775, 0.13979127, 0.88065611, 0.05708264, 0.10746691,\n",
       "        0.31778357, 0.0793382 , 0.05656608, 1.04590521, 0.08231448,\n",
       "        0.04507972, 0.89883921, 0.05854444, 0.10620185, 0.80195287,\n",
       "        0.06391682, 0.0425801 , 0.85432846, 0.09091007, 0.10245737,\n",
       "        0.45917351, 0.06357567, 0.03013078, 0.76903396, 0.03059889,\n",
       "        0.12696111, 0.62092406, 0.14048876, 0.12752987, 0.48066606,\n",
       "        0.10132744, 0.10669449, 0.8496924 , 0.11806183, 0.06628371,\n",
       "        0.55084309, 0.19305677, 0.12083191, 0.22087871, 0.25237502,\n",
       "        0.19753641, 0.1314416 , 0.12049998, 0.10336373, 0.00620643,\n",
       "        0.07362337, 0.08368569, 0.04462946, 0.09690733, 0.17593144,\n",
       "        0.05813144, 0.17559072, 0.0653755 , 0.08230157, 0.06260931,\n",
       "        0.04740393, 0.08529742, 0.04007979, 0.07228016, 0.06735046,\n",
       "        0.0464647 , 0.09930192, 0.20526516, 0.17413917, 0.10517805,\n",
       "        0.11902647, 0.11100919, 0.08042401, 0.13034194, 0.16927169,\n",
       "        0.0450273 , 0.05564939, 0.03866136, 0.13162292, 0.23018744,\n",
       "        0.19681184, 0.09334446, 0.23065933, 0.0934543 , 0.09683382,\n",
       "        0.10760916, 0.05886098, 0.04333727, 0.07909013, 0.05457255,\n",
       "        0.14748483, 0.12512559, 0.34659403, 0.17307011, 0.68731357,\n",
       "        0.65426256, 0.49413029, 0.54094148, 0.49681157, 0.30355678,\n",
       "        0.34278587, 0.24656297, 0.22550179, 0.24927052, 0.32000764,\n",
       "        0.67305612, 0.54661277, 0.39631838, 1.03233856, 0.32348236,\n",
       "        0.75446406, 0.14871054, 0.1160505 , 0.18534047, 0.21276705,\n",
       "        0.30090043, 0.24209005, 0.42120156, 0.200512  , 0.48835321,\n",
       "        0.6697082 , 0.0277477 , 0.70612372, 0.0093338 , 0.02595468,\n",
       "        0.02107685, 0.23721909, 0.08412942, 0.08298182, 0.1307683 ,\n",
       "        0.19080461, 0.11455634, 0.59995504, 0.27805369, 0.33533523,\n",
       "        0.63146254, 0.24618556, 0.41181933]),\n",
       " 'mean_score_time': array([0.00730648, 0.00770693, 0.01010876, 0.00870738, 0.00930891,\n",
       "        0.01000867, 0.01130881, 0.01190996, 0.01671419, 0.0118093 ,\n",
       "        0.01521373, 0.01150913, 0.00960922, 0.0090085 , 0.00770574,\n",
       "        0.00850601, 0.00830674, 0.00940681, 0.01000853, 0.00960741,\n",
       "        0.01090999, 0.01010904, 0.01040869, 0.0125104 , 0.00870667,\n",
       "        0.00980797, 0.00730572, 0.00900636, 0.01120968, 0.0128109 ,\n",
       "        0.01060929, 0.01201072, 0.01040864, 0.00990858, 0.00970845,\n",
       "        0.02482138, 0.01090879, 0.01411195, 0.00730629, 0.00890627,\n",
       "        0.00780764, 0.00910902, 0.01010737, 0.0097075 , 0.00971227,\n",
       "        0.0104095 , 0.00970874, 0.008108  , 0.00910845, 0.00800624,\n",
       "        0.00900698, 0.01080928, 0.00830641, 0.01030746, 0.01421399,\n",
       "        0.0139112 , 0.01050987, 0.01061039, 0.01060915, 0.01411362,\n",
       "        0.01110864, 0.01421289, 0.00880809, 0.00860724, 0.00820532,\n",
       "        0.00920796, 0.01130929, 0.00960851, 0.01391163, 0.01251292,\n",
       "        0.00950785, 0.00920749, 0.00840726, 0.01050911, 0.01041055,\n",
       "        0.01010909, 0.0096076 , 0.00930877, 0.01101103, 0.01200967,\n",
       "        0.01120944, 0.01120987, 0.00970731, 0.01461163, 0.00780687,\n",
       "        0.0075067 , 0.01060915, 0.0090075 , 0.00820541, 0.01040812,\n",
       "        0.01221166, 0.01140914, 0.01280975, 0.01060858, 0.01010847,\n",
       "        0.00850692, 0.00680494, 0.01040869, 0.0063055 , 0.00760641,\n",
       "        0.00680671, 0.00720611, 0.00760674, 0.00870819, 0.00770674,\n",
       "        0.00720601, 0.0072063 , 0.00720572, 0.00680561, 0.00730615,\n",
       "        0.00840755, 0.00730639, 0.00710592, 0.00680609, 0.00710626,\n",
       "        0.00710616, 0.00760589, 0.0076067 , 0.0075047 , 0.00730629,\n",
       "        0.00710607, 0.0076067 , 0.00730629, 0.00680676, 0.00970864,\n",
       "        0.00660596, 0.00720682, 0.00750628, 0.00690565, 0.00790687,\n",
       "        0.00980854, 0.0071064 , 0.01040874, 0.00790658, 0.00660629,\n",
       "        0.00640602, 0.00690594, 0.00660615, 0.00780745, 0.0077064 ,\n",
       "        0.00670633, 0.0068059 , 0.00700603, 0.00760598, 0.00680561,\n",
       "        0.00650587, 0.00640521, 0.00670571, 0.00740685, 0.00710635,\n",
       "        0.00710621, 0.00790668, 0.00780673, 0.00770655, 0.00780706,\n",
       "        0.00760636, 0.00660572, 0.00740623, 0.00650449, 0.00700603,\n",
       "        0.00680609, 0.00770588, 0.00690603, 0.00720611, 0.0068058 ,\n",
       "        0.00730643, 0.00740628, 0.00710597, 0.00640554, 0.00750642,\n",
       "        0.0063055 , 0.00710611, 0.00770669, 0.00790577, 0.00810661,\n",
       "        0.0070055 , 0.00690613, 0.00760651, 0.00880752, 0.00720615,\n",
       "        0.00650568, 0.00690613, 0.008708  , 0.00660548, 0.00670643,\n",
       "        0.00720606, 0.00710607, 0.00750618, 0.008007  , 0.00730639,\n",
       "        0.00930791, 0.00720544, 0.01150975, 0.00670609, 0.00670552,\n",
       "        0.00850716, 0.00660553, 0.006706  , 0.0070065 , 0.00690589,\n",
       "        0.00660586, 0.00710573, 0.00690603, 0.00700631, 0.00710602,\n",
       "        0.00650554, 0.00650582, 0.00700612, 0.00670609, 0.00660567,\n",
       "        0.00690646, 0.00690637, 0.0068058 , 0.00700603, 0.00680609,\n",
       "        0.00690589, 0.00620556, 0.00710597, 0.00620522, 0.00730658,\n",
       "        0.0081069 , 0.00680585, 0.00780711, 0.00830727, 0.00850735,\n",
       "        0.0095078 , 0.00860734, 0.00730581, 0.0069057 , 0.00640588,\n",
       "        0.0106092 , 0.00670586, 0.01090956, 0.00680599, 0.00730624,\n",
       "        0.00740623, 0.00670609, 0.00690584, 0.00720673, 0.0068058 ,\n",
       "        0.00630493, 0.00650592, 0.00640531, 0.0066052 , 0.00820694,\n",
       "        0.00700593, 0.00870738, 0.00930786, 0.00810738, 0.00910811,\n",
       "        0.01541367, 0.00860729, 0.00830722, 0.00690522, 0.00960865,\n",
       "        0.00800657, 0.00890741, 0.00740628, 0.00840759, 0.00870752,\n",
       "        0.01151004, 0.00770645, 0.01741495, 0.00730624, 0.00680642,\n",
       "        0.00660558, 0.00680556, 0.00770655, 0.00760679, 0.00910797,\n",
       "        0.00730643, 0.00760665, 0.00910745, 0.00750618, 0.00750642,\n",
       "        0.01040936, 0.00670562, 0.00660558, 0.00620551, 0.0070056 ,\n",
       "        0.00670581, 0.00710616, 0.00710568, 0.00950875, 0.00790606,\n",
       "        0.00640564, 0.00460415, 0.00500484]),\n",
       " 'std_score_time': array([1.12360136e-03, 1.07806649e-03, 3.40017468e-03, 7.48497267e-04,\n",
       "        1.47225464e-03, 1.26666659e-03, 1.20961488e-03, 3.29490416e-03,\n",
       "        5.55121366e-03, 2.67926310e-03, 6.02538268e-03, 2.58953792e-03,\n",
       "        2.74895866e-03, 1.41593266e-03, 6.79773815e-04, 1.30180836e-03,\n",
       "        4.01081223e-04, 8.02190522e-04, 1.81869043e-03, 3.75224266e-04,\n",
       "        1.53092561e-03, 1.06901753e-03, 2.63585882e-03, 4.72622679e-03,\n",
       "        1.60011996e-03, 3.79171870e-03, 2.47015667e-04, 1.54785142e-03,\n",
       "        7.66559875e-03, 1.03708939e-02, 1.56282042e-03, 2.91900258e-03,\n",
       "        1.15915618e-03, 1.59410214e-03, 1.96657518e-03, 1.84598203e-02,\n",
       "        2.65640835e-03, 1.05659434e-02, 8.12761759e-04, 1.15844989e-03,\n",
       "        2.44622556e-04, 5.84092957e-04, 5.82812890e-04, 4.00226883e-04,\n",
       "        7.48971836e-04, 1.06792402e-03, 1.40137692e-03, 9.71236734e-04,\n",
       "        1.59575860e-03, 8.95136535e-04, 1.64315030e-03, 2.62257942e-03,\n",
       "        6.78883473e-04, 3.38857936e-03, 9.42372879e-03, 4.25165452e-03,\n",
       "        2.02744499e-03, 9.14563345e-04, 3.75760479e-04, 6.71003238e-03,\n",
       "        1.74445109e-03, 1.05382490e-02, 1.43801108e-03, 4.90350214e-04,\n",
       "        7.49088845e-04, 8.14038264e-04, 2.50433236e-03, 6.62789636e-04,\n",
       "        4.03416957e-03, 3.96405863e-03, 9.50072764e-04, 1.56968765e-03,\n",
       "        9.69269926e-04, 4.15104636e-03, 2.78201006e-03, 2.08327367e-03,\n",
       "        1.39429953e-03, 9.27660611e-04, 9.48992114e-04, 4.08957008e-03,\n",
       "        1.28924228e-03, 2.38079200e-03, 5.10838801e-04, 8.84738187e-03,\n",
       "        3.99822103e-04, 3.15909695e-04, 1.88339769e-03, 1.67384033e-03,\n",
       "        7.49094284e-04, 1.06859577e-03, 4.24250470e-03, 1.82970189e-03,\n",
       "        1.72148925e-03, 2.35489512e-03, 1.93613985e-03, 1.41461693e-03,\n",
       "        6.78662579e-04, 6.08279599e-03, 2.44542830e-04, 1.24219928e-03,\n",
       "        6.00886674e-04, 1.16764745e-03, 8.61400829e-04, 1.02976637e-03,\n",
       "        1.50478775e-03, 6.78859054e-04, 5.10744075e-04, 2.44991164e-04,\n",
       "        8.71858876e-04, 1.12356295e-03, 3.32514406e-03, 1.36447132e-03,\n",
       "        1.24216471e-03, 4.00400176e-04, 3.74368840e-04, 4.90105832e-04,\n",
       "        7.35767068e-04, 8.60962883e-04, 7.10668877e-04, 9.27865422e-04,\n",
       "        1.02100181e-03, 1.77322994e-03, 1.63307199e-03, 2.43846830e-04,\n",
       "        4.50511549e-03, 2.00105076e-04, 5.10351475e-04, 1.09606226e-03,\n",
       "        2.00439031e-04, 1.35812891e-03, 6.35824328e-03, 2.00701355e-04,\n",
       "        6.16467596e-03, 2.08515291e-03, 3.74304334e-04, 3.74725186e-04,\n",
       "        3.74648458e-04, 2.00964173e-04, 1.63336447e-03, 1.07792007e-03,\n",
       "        2.45690761e-04, 2.44893231e-04, 3.16054258e-04, 1.24142696e-03,\n",
       "        2.45145661e-04, 5.76164530e-07, 1.99842937e-04, 2.45126275e-04,\n",
       "        8.60752273e-04, 5.83715361e-04, 3.74189665e-04, 6.63435158e-04,\n",
       "        7.49207711e-04, 8.72493350e-04, 6.78500578e-04, 3.74521170e-04,\n",
       "        7.35605026e-04, 1.56331655e-03, 3.16362873e-04, 7.75033060e-04,\n",
       "        2.45534903e-04, 1.66352929e-03, 3.74610537e-04, 2.45359737e-04,\n",
       "        4.00435984e-04, 2.45359506e-04, 3.74431999e-04, 1.99962252e-04,\n",
       "        2.00248013e-04, 1.76173220e-03, 2.45320804e-04, 5.83470140e-04,\n",
       "        1.91484587e-03, 1.24204176e-03, 1.49726817e-03, 3.16733511e-04,\n",
       "        2.00319886e-04, 4.89882014e-04, 2.85893840e-03, 2.45028767e-04,\n",
       "        4.62310777e-07, 8.60857678e-04, 4.26528984e-03, 2.00105473e-04,\n",
       "        2.45906101e-04, 6.78598796e-04, 3.74865070e-04, 6.32786829e-04,\n",
       "        1.76211134e-03, 2.45419029e-04, 2.73396529e-03, 2.45126832e-04,\n",
       "        6.66136363e-03, 6.00584429e-04, 6.78591750e-04, 3.51040963e-03,\n",
       "        1.99961911e-04, 4.00185596e-04, 3.16506913e-04, 2.00558672e-04,\n",
       "        2.00629652e-04, 2.00319773e-04, 3.74419083e-04, 3.15978747e-04,\n",
       "        1.71609822e-03, 4.62310777e-07, 3.16054330e-04, 3.16581860e-04,\n",
       "        5.10416807e-04, 3.74151406e-04, 2.00009765e-04, 3.74763114e-04,\n",
       "        2.44620000e-04, 3.16280541e-04, 4.00281446e-04, 1.99962252e-04,\n",
       "        2.45087341e-04, 1.06847462e-03, 2.45067933e-04, 1.40101584e-03,\n",
       "        2.48008591e-03, 4.00221656e-04, 1.36500253e-03, 1.83466644e-03,\n",
       "        1.58292428e-03, 3.16477777e-03, 2.78421147e-03, 6.78753373e-04,\n",
       "        5.83543636e-04, 3.73731246e-04, 8.45962609e-03, 6.79020493e-04,\n",
       "        5.33743629e-03, 2.44581608e-04, 6.78563513e-04, 5.83682710e-04,\n",
       "        2.44814745e-04, 2.00415707e-04, 7.49112244e-04, 2.45009242e-04,\n",
       "        2.45048161e-04, 3.16506546e-04, 3.74470102e-04, 2.00129322e-04,\n",
       "        2.01654715e-03, 4.47714467e-04, 1.36514947e-03, 2.25156657e-03,\n",
       "        5.83200723e-04, 1.02003862e-03, 1.28849612e-02, 2.22466848e-03,\n",
       "        2.91060703e-03, 1.06892633e-03, 6.21353825e-03, 1.26621424e-03,\n",
       "        2.08488728e-03, 8.61129496e-04, 1.46398049e-03, 7.49092979e-04,\n",
       "        7.52310056e-03, 2.45242890e-04, 1.33954372e-02, 4.00138224e-04,\n",
       "        4.00865272e-04, 1.99342138e-04, 2.45203818e-04, 1.03070185e-03,\n",
       "        1.49759317e-03, 4.20372489e-03, 4.00591138e-04, 3.74572406e-04,\n",
       "        2.97530771e-03, 3.16657515e-04, 3.16582722e-04, 3.10748491e-03,\n",
       "        4.00137940e-04, 3.74814129e-04, 2.45612724e-04, 5.48205448e-04,\n",
       "        2.45242983e-04, 7.35637234e-04, 3.74394020e-04, 3.88982989e-03,\n",
       "        1.24169974e-03, 9.70390462e-04, 4.90358950e-04, 1.00173954e-03]),\n",
       " 'param_classifier': masked_array(data=[MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__activation': masked_array(data=['logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__alpha': masked_array(data=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__hidden_layer_sizes': masked_array(data=[(5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__learning_rate': masked_array(data=['constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'}],\n",
       " 'split0_test_recall_micro': array([0.925, 0.916, 0.912, 0.914, 0.924, 0.919, 0.902, 0.917, 0.904,\n",
       "        0.896, 0.909, 0.884, 0.921, 0.924, 0.926, 0.916, 0.909, 0.918,\n",
       "        0.903, 0.905, 0.906, 0.893, 0.905, 0.906, 0.922, 0.923, 0.925,\n",
       "        0.923, 0.922, 0.914, 0.914, 0.908, 0.907, 0.909, 0.897, 0.904,\n",
       "        0.918, 0.925, 0.925, 0.917, 0.921, 0.914, 0.911, 0.915, 0.911,\n",
       "        0.895, 0.911, 0.903, 0.921, 0.92 , 0.922, 0.912, 0.916, 0.909,\n",
       "        0.9  , 0.908, 0.897, 0.893, 0.882, 0.889, 0.914, 0.918, 0.921,\n",
       "        0.915, 0.918, 0.918, 0.901, 0.913, 0.909, 0.895, 0.894, 0.9  ,\n",
       "        0.92 , 0.926, 0.925, 0.911, 0.917, 0.917, 0.913, 0.896, 0.898,\n",
       "        0.901, 0.904, 0.89 , 0.924, 0.925, 0.924, 0.915, 0.924, 0.92 ,\n",
       "        0.92 , 0.915, 0.903, 0.897, 0.899, 0.897, 0.925, 0.925, 0.925,\n",
       "        0.925, 0.603, 0.925, 0.925, 0.846, 0.925, 0.925, 0.919, 0.925,\n",
       "        0.925, 0.075, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925,\n",
       "        0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.108, 0.925,\n",
       "        0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925,\n",
       "        0.925, 0.924, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925,\n",
       "        0.925, 0.702, 0.925, 0.925, 0.662, 0.925, 0.925, 0.338, 0.925,\n",
       "        0.925, 0.516, 0.925, 0.925, 0.854, 0.925, 0.925, 0.864, 0.925,\n",
       "        0.925, 0.694, 0.925, 0.925, 0.496, 0.925, 0.925, 0.807, 0.925,\n",
       "        0.925, 0.266, 0.925, 0.925, 0.682, 0.925, 0.925, 0.719, 0.925,\n",
       "        0.925, 0.676, 0.925, 0.925, 0.704, 0.925, 0.925, 0.33 , 0.925,\n",
       "        0.925, 0.668, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925,\n",
       "        0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925,\n",
       "        0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925,\n",
       "        0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925,\n",
       "        0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925,\n",
       "        0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925,\n",
       "        0.925, 0.925, 0.925, 0.925, 0.925, 0.924, 0.925, 0.925, 0.925,\n",
       "        0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.924, 0.923, 0.925,\n",
       "        0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925,\n",
       "        0.925, 0.925, 0.925, 0.925, 0.925, 0.924, 0.925, 0.925, 0.925,\n",
       "        0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925]),\n",
       " 'split1_test_recall_micro': array([0.924, 0.926, 0.924, 0.914, 0.919, 0.907, 0.906, 0.911, 0.917,\n",
       "        0.897, 0.893, 0.887, 0.924, 0.924, 0.924, 0.914, 0.919, 0.921,\n",
       "        0.907, 0.913, 0.903, 0.905, 0.895, 0.905, 0.926, 0.924, 0.923,\n",
       "        0.922, 0.919, 0.923, 0.907, 0.915, 0.906, 0.899, 0.904, 0.9  ,\n",
       "        0.924, 0.924, 0.923, 0.918, 0.919, 0.922, 0.918, 0.91 , 0.914,\n",
       "        0.907, 0.906, 0.9  , 0.92 , 0.925, 0.924, 0.924, 0.915, 0.919,\n",
       "        0.891, 0.899, 0.903, 0.887, 0.875, 0.882, 0.924, 0.919, 0.922,\n",
       "        0.91 , 0.914, 0.92 , 0.902, 0.909, 0.903, 0.896, 0.887, 0.892,\n",
       "        0.921, 0.924, 0.926, 0.919, 0.915, 0.92 , 0.917, 0.919, 0.894,\n",
       "        0.9  , 0.888, 0.897, 0.924, 0.923, 0.924, 0.917, 0.927, 0.924,\n",
       "        0.91 , 0.92 , 0.915, 0.89 , 0.895, 0.896, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.116, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.77 , 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.75 , 0.924, 0.924, 0.699, 0.924, 0.924, 0.335, 0.924,\n",
       "        0.924, 0.431, 0.924, 0.924, 0.74 , 0.924, 0.924, 0.83 , 0.924,\n",
       "        0.924, 0.374, 0.924, 0.924, 0.652, 0.924, 0.924, 0.517, 0.924,\n",
       "        0.924, 0.698, 0.924, 0.924, 0.711, 0.924, 0.924, 0.734, 0.924,\n",
       "        0.924, 0.502, 0.924, 0.924, 0.717, 0.924, 0.924, 0.645, 0.924,\n",
       "        0.924, 0.753, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924]),\n",
       " 'split2_test_recall_micro': array([0.923, 0.924, 0.925, 0.912, 0.921, 0.917, 0.908, 0.916, 0.916,\n",
       "        0.885, 0.896, 0.898, 0.922, 0.924, 0.923, 0.914, 0.918, 0.92 ,\n",
       "        0.9  , 0.913, 0.909, 0.894, 0.892, 0.898, 0.922, 0.924, 0.923,\n",
       "        0.924, 0.925, 0.915, 0.912, 0.907, 0.915, 0.9  , 0.902, 0.896,\n",
       "        0.927, 0.923, 0.925, 0.918, 0.915, 0.923, 0.921, 0.917, 0.91 ,\n",
       "        0.913, 0.906, 0.903, 0.92 , 0.923, 0.92 , 0.921, 0.921, 0.917,\n",
       "        0.904, 0.906, 0.897, 0.897, 0.913, 0.908, 0.922, 0.922, 0.924,\n",
       "        0.916, 0.912, 0.918, 0.901, 0.907, 0.909, 0.887, 0.876, 0.886,\n",
       "        0.916, 0.924, 0.922, 0.918, 0.918, 0.914, 0.907, 0.912, 0.918,\n",
       "        0.888, 0.9  , 0.886, 0.924, 0.924, 0.925, 0.92 , 0.919, 0.912,\n",
       "        0.91 , 0.907, 0.913, 0.897, 0.913, 0.899, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.077, 0.924, 0.924, 0.924, 0.924, 0.924, 0.923, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.198, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.089, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.7  , 0.924, 0.924, 0.57 , 0.924, 0.924, 0.366, 0.924,\n",
       "        0.924, 0.684, 0.924, 0.924, 0.289, 0.924, 0.924, 0.646, 0.924,\n",
       "        0.924, 0.377, 0.924, 0.924, 0.386, 0.924, 0.924, 0.567, 0.924,\n",
       "        0.924, 0.353, 0.924, 0.924, 0.569, 0.924, 0.924, 0.576, 0.924,\n",
       "        0.924, 0.185, 0.924, 0.924, 0.82 , 0.924, 0.924, 0.543, 0.924,\n",
       "        0.924, 0.567, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.923, 0.924, 0.923,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.923, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.922, 0.922, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.923, 0.924, 0.924, 0.924, 0.924, 0.924]),\n",
       " 'split3_test_recall_micro': array([0.924, 0.923, 0.92 , 0.909, 0.925, 0.918, 0.916, 0.908, 0.903,\n",
       "        0.891, 0.902, 0.904, 0.924, 0.924, 0.924, 0.917, 0.922, 0.918,\n",
       "        0.908, 0.907, 0.925, 0.911, 0.9  , 0.885, 0.919, 0.924, 0.924,\n",
       "        0.916, 0.919, 0.919, 0.908, 0.92 , 0.921, 0.9  , 0.905, 0.924,\n",
       "        0.924, 0.923, 0.924, 0.92 , 0.922, 0.924, 0.916, 0.918, 0.923,\n",
       "        0.912, 0.913, 0.924, 0.924, 0.923, 0.921, 0.916, 0.921, 0.915,\n",
       "        0.9  , 0.905, 0.905, 0.9  , 0.894, 0.891, 0.921, 0.923, 0.925,\n",
       "        0.926, 0.921, 0.922, 0.908, 0.912, 0.898, 0.909, 0.884, 0.893,\n",
       "        0.923, 0.924, 0.923, 0.909, 0.919, 0.927, 0.907, 0.907, 0.914,\n",
       "        0.898, 0.91 , 0.883, 0.923, 0.923, 0.923, 0.919, 0.92 , 0.918,\n",
       "        0.914, 0.912, 0.917, 0.902, 0.903, 0.909, 0.924, 0.076, 0.924,\n",
       "        0.924, 0.552, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.602, 0.924,\n",
       "        0.924, 0.643, 0.924, 0.924, 0.444, 0.924, 0.924, 0.774, 0.924,\n",
       "        0.924, 0.739, 0.924, 0.924, 0.918, 0.924, 0.924, 0.674, 0.924,\n",
       "        0.924, 0.722, 0.924, 0.924, 0.873, 0.924, 0.924, 0.327, 0.924,\n",
       "        0.924, 0.218, 0.924, 0.924, 0.507, 0.924, 0.924, 0.77 , 0.924,\n",
       "        0.924, 0.542, 0.924, 0.924, 0.457, 0.924, 0.924, 0.759, 0.924,\n",
       "        0.924, 0.566, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924]),\n",
       " 'split4_test_recall_micro': array([0.923, 0.924, 0.922, 0.916, 0.923, 0.906, 0.908, 0.906, 0.902,\n",
       "        0.906, 0.924, 0.897, 0.925, 0.923, 0.924, 0.918, 0.922, 0.907,\n",
       "        0.912, 0.898, 0.905, 0.897, 0.906, 0.903, 0.924, 0.918, 0.915,\n",
       "        0.916, 0.914, 0.911, 0.907, 0.905, 0.903, 0.901, 0.908, 0.91 ,\n",
       "        0.924, 0.92 , 0.924, 0.919, 0.924, 0.914, 0.913, 0.915, 0.91 ,\n",
       "        0.915, 0.899, 0.901, 0.92 , 0.918, 0.924, 0.916, 0.916, 0.914,\n",
       "        0.901, 0.895, 0.91 , 0.892, 0.901, 0.899, 0.92 , 0.922, 0.922,\n",
       "        0.906, 0.911, 0.913, 0.904, 0.915, 0.912, 0.893, 0.891, 0.902,\n",
       "        0.923, 0.922, 0.921, 0.907, 0.907, 0.913, 0.903, 0.907, 0.915,\n",
       "        0.907, 0.888, 0.893, 0.922, 0.923, 0.922, 0.917, 0.913, 0.921,\n",
       "        0.906, 0.9  , 0.902, 0.899, 0.896, 0.903, 0.924, 0.826, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.791, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.076, 0.924,\n",
       "        0.924, 0.921, 0.924, 0.924, 0.914, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.721, 0.924, 0.924, 0.549, 0.924, 0.924, 0.605, 0.924,\n",
       "        0.924, 0.891, 0.924, 0.924, 0.836, 0.924, 0.924, 0.711, 0.924,\n",
       "        0.924, 0.765, 0.924, 0.924, 0.612, 0.924, 0.924, 0.742, 0.924,\n",
       "        0.924, 0.316, 0.924, 0.924, 0.553, 0.924, 0.924, 0.779, 0.924,\n",
       "        0.924, 0.425, 0.924, 0.924, 0.763, 0.924, 0.924, 0.806, 0.924,\n",
       "        0.924, 0.491, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.923, 0.925, 0.923, 0.923, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.922, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.923, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924]),\n",
       " 'mean_test_recall_micro': array([0.9238, 0.9226, 0.9206, 0.913 , 0.9224, 0.9134, 0.908 , 0.9116,\n",
       "        0.9084, 0.895 , 0.9048, 0.894 , 0.9232, 0.9238, 0.9242, 0.9158,\n",
       "        0.918 , 0.9168, 0.906 , 0.9072, 0.9096, 0.9   , 0.8996, 0.8994,\n",
       "        0.9226, 0.9226, 0.922 , 0.9202, 0.9198, 0.9164, 0.9096, 0.911 ,\n",
       "        0.9104, 0.9018, 0.9032, 0.9068, 0.9234, 0.923 , 0.9242, 0.9184,\n",
       "        0.9202, 0.9194, 0.9158, 0.915 , 0.9136, 0.9084, 0.907 , 0.9062,\n",
       "        0.921 , 0.9218, 0.9222, 0.9178, 0.9178, 0.9148, 0.8992, 0.9026,\n",
       "        0.9024, 0.8938, 0.893 , 0.8938, 0.9202, 0.9208, 0.9228, 0.9146,\n",
       "        0.9152, 0.9182, 0.9032, 0.9112, 0.9062, 0.896 , 0.8864, 0.8946,\n",
       "        0.9206, 0.924 , 0.9234, 0.9128, 0.9152, 0.9182, 0.9094, 0.9082,\n",
       "        0.9078, 0.8988, 0.898 , 0.8898, 0.9234, 0.9236, 0.9236, 0.9176,\n",
       "        0.9206, 0.919 , 0.912 , 0.9108, 0.91  , 0.897 , 0.9012, 0.9008,\n",
       "        0.9242, 0.735 , 0.9242, 0.9242, 0.7854, 0.9242, 0.9242, 0.9084,\n",
       "        0.9242, 0.9242, 0.923 , 0.9242, 0.9242, 0.4232, 0.9242, 0.9242,\n",
       "        0.9242, 0.9242, 0.9242, 0.924 , 0.9242, 0.9242, 0.9242, 0.9242,\n",
       "        0.9242, 0.9242, 0.9242, 0.9242, 0.5582, 0.9242, 0.9242, 0.9242,\n",
       "        0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.5876, 0.9242, 0.9242,\n",
       "        0.9234, 0.9242, 0.9242, 0.9222, 0.9242, 0.9242, 0.8598, 0.9242,\n",
       "        0.9242, 0.7032, 0.9242, 0.9242, 0.5848, 0.9242, 0.9242, 0.4836,\n",
       "        0.9242, 0.9242, 0.6522, 0.9242, 0.9242, 0.7274, 0.9242, 0.9242,\n",
       "        0.745 , 0.9242, 0.9242, 0.5864, 0.9242, 0.9242, 0.6038, 0.9242,\n",
       "        0.9242, 0.592 , 0.9242, 0.9242, 0.3702, 0.9242, 0.9242, 0.6044,\n",
       "        0.9242, 0.9242, 0.7156, 0.9242, 0.9242, 0.466 , 0.9242, 0.9242,\n",
       "        0.6922, 0.9242, 0.9242, 0.6166, 0.9242, 0.9242, 0.609 , 0.9242,\n",
       "        0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242,\n",
       "        0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242,\n",
       "        0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242,\n",
       "        0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242,\n",
       "        0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242,\n",
       "        0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242,\n",
       "        0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.924 ,\n",
       "        0.9242, 0.9238, 0.924 , 0.924 , 0.9242, 0.9242, 0.9242, 0.9242,\n",
       "        0.9242, 0.9242, 0.924 , 0.9238, 0.9242, 0.9238, 0.9242, 0.924 ,\n",
       "        0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242,\n",
       "        0.9242, 0.9236, 0.9238, 0.924 , 0.9242, 0.9242, 0.9242, 0.9242,\n",
       "        0.9242, 0.9242, 0.924 , 0.9242, 0.9242, 0.9242, 0.9242, 0.9242]),\n",
       " 'std_test_recall_micro': array([7.48331477e-04, 3.44093011e-03, 4.63033476e-03, 2.36643191e-03,\n",
       "        2.15406592e-03, 5.67802783e-03, 4.56070170e-03, 4.31740663e-03,\n",
       "        6.65131566e-03, 6.95701085e-03, 1.10526015e-02, 7.40270221e-03,\n",
       "        1.46969385e-03, 4.00000000e-04, 9.79795897e-04, 1.60000000e-03,\n",
       "        4.77493455e-03, 5.03587132e-03, 4.14728827e-03, 5.60000000e-03,\n",
       "        7.93977330e-03, 6.92820323e-03, 5.46260011e-03, 7.70973411e-03,\n",
       "        2.33238076e-03, 2.33238076e-03, 3.57770876e-03, 3.48711915e-03,\n",
       "        3.65513338e-03, 4.17612260e-03, 2.87054002e-03, 5.62138773e-03,\n",
       "        6.62117814e-03, 3.65513338e-03, 3.65513338e-03, 9.76524449e-03,\n",
       "        2.93938769e-03, 1.67332005e-03, 7.48331477e-04, 1.01980390e-03,\n",
       "        3.05941171e-03, 4.45421149e-03, 3.54400903e-03, 2.75680975e-03,\n",
       "        4.92341345e-03, 7.20000000e-03, 4.85798312e-03, 8.97552227e-03,\n",
       "        1.54919334e-03, 2.48193473e-03, 1.60000000e-03, 4.21426150e-03,\n",
       "        2.63818119e-03, 3.37045991e-03, 4.35430821e-03, 4.84148737e-03,\n",
       "        4.96386946e-03, 4.44522215e-03, 1.34907376e-02, 8.93084542e-03,\n",
       "        3.37045991e-03, 1.93907194e-03, 1.46969385e-03, 6.74091982e-03,\n",
       "        3.76297754e-03, 2.99332591e-03, 2.63818119e-03, 2.85657137e-03,\n",
       "        5.03587132e-03, 7.21110255e-03, 6.21610811e-03, 5.78273292e-03,\n",
       "        2.57681975e-03, 1.26491106e-03, 1.85472370e-03, 4.83321839e-03,\n",
       "        4.30813185e-03, 5.03587132e-03, 4.96386946e-03, 7.52063827e-03,\n",
       "        9.80612054e-03, 6.17737808e-03, 8.76356092e-03, 4.95580468e-03,\n",
       "        8.00000000e-04, 8.00000000e-04, 1.01980390e-03, 1.74355958e-03,\n",
       "        4.75815090e-03, 4.00000000e-03, 4.73286383e-03, 6.85273668e-03,\n",
       "        6.26099034e-03, 3.94968353e-03, 6.52380257e-03, 4.74973683e-03,\n",
       "        4.00000000e-04, 3.31693835e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        1.70514046e-01, 4.00000000e-04, 4.00000000e-04, 3.12000000e-02,\n",
       "        4.00000000e-04, 4.00000000e-04, 2.00000000e-03, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.09162755e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 6.32455532e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        3.36240628e-01, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.12433073e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        1.20000000e-03, 4.00000000e-04, 4.00000000e-04, 4.11825206e-03,\n",
       "        4.00000000e-04, 4.00000000e-04, 1.28900582e-01, 4.00000000e-04,\n",
       "        4.00000000e-04, 3.50508202e-02, 4.00000000e-04, 4.00000000e-04,\n",
       "        8.98073494e-02, 4.00000000e-04, 4.00000000e-04, 1.76737772e-01,\n",
       "        4.00000000e-04, 4.00000000e-04, 1.63126209e-01, 4.00000000e-04,\n",
       "        4.00000000e-04, 2.26496446e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        8.64684914e-02, 4.00000000e-04, 4.00000000e-04, 1.73680857e-01,\n",
       "        4.00000000e-04, 4.00000000e-04, 1.63656225e-01, 4.00000000e-04,\n",
       "        4.00000000e-04, 1.70411267e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        1.70122779e-01, 4.00000000e-04, 4.00000000e-04, 7.84438653e-02,\n",
       "        4.00000000e-04, 4.00000000e-04, 7.32355105e-02, 4.00000000e-04,\n",
       "        4.00000000e-04, 1.62366253e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        1.24433757e-01, 4.00000000e-04, 4.00000000e-04, 1.70007765e-01,\n",
       "        4.00000000e-04, 4.00000000e-04, 9.13827117e-02, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 6.32455532e-04,\n",
       "        4.00000000e-04, 7.48331477e-04, 6.32455532e-04, 6.32455532e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 0.00000000e+00, 4.00000000e-04,\n",
       "        4.00000000e-04, 9.79795897e-04, 4.00000000e-04, 6.32455532e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 1.01980390e-03, 9.79795897e-04, 0.00000000e+00,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 6.32455532e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04]),\n",
       " 'rank_test_recall_micro': array([166, 182, 192, 219, 185, 218, 235, 222, 231, 259, 243, 261, 178,\n",
       "        166,   1, 210, 204, 208, 242, 237, 228, 251, 252, 253, 182, 184,\n",
       "        188, 195, 198, 209, 228, 224, 226, 248, 244, 239, 174, 179,   1,\n",
       "        201, 195, 199, 211, 214, 217, 231, 238, 241, 190, 189, 186, 205,\n",
       "        205, 215, 254, 246, 247, 262, 264, 263, 195, 191, 181, 216, 212,\n",
       "        202, 245, 223, 240, 258, 266, 260, 192, 156, 174, 220, 212, 202,\n",
       "        230, 234, 236, 255, 256, 265, 174, 171, 171, 207, 192, 200, 221,\n",
       "        225, 227, 257, 249, 250,   1, 270,   1,   1, 268,   1,   1, 231,\n",
       "          1,   1, 179,   1,   1, 287,   1,   1,   1,   1,   1, 156,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1, 284,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1, 281,   1,   1, 174,   1,   1, 187,   1,   1, 267,\n",
       "          1,   1, 273,   1,   1, 283,   1,   1, 285,   1,   1, 275,   1,\n",
       "          1, 271,   1,   1, 269,   1,   1, 282,   1,   1, 279,   1,   1,\n",
       "        280,   1,   1, 288,   1,   1, 278,   1,   1, 272,   1,   1, 286,\n",
       "          1,   1, 274,   1,   1, 276,   1,   1, 277,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "        156,   1, 166, 156, 156,   1,   1,   1,   1,   1,   1, 156, 166,\n",
       "          1, 166,   1, 156,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "        171, 165, 156,   1,   1,   1,   1,   1,   1, 156,   1,   1,   1,\n",
       "          1,   1]),\n",
       " 'split0_test_f1_micro': array([0.925, 0.916, 0.912, 0.914, 0.924, 0.919, 0.902, 0.917, 0.904,\n",
       "        0.896, 0.909, 0.884, 0.921, 0.924, 0.926, 0.916, 0.909, 0.918,\n",
       "        0.903, 0.905, 0.906, 0.893, 0.905, 0.906, 0.922, 0.923, 0.925,\n",
       "        0.923, 0.922, 0.914, 0.914, 0.908, 0.907, 0.909, 0.897, 0.904,\n",
       "        0.918, 0.925, 0.925, 0.917, 0.921, 0.914, 0.911, 0.915, 0.911,\n",
       "        0.895, 0.911, 0.903, 0.921, 0.92 , 0.922, 0.912, 0.916, 0.909,\n",
       "        0.9  , 0.908, 0.897, 0.893, 0.882, 0.889, 0.914, 0.918, 0.921,\n",
       "        0.915, 0.918, 0.918, 0.901, 0.913, 0.909, 0.895, 0.894, 0.9  ,\n",
       "        0.92 , 0.926, 0.925, 0.911, 0.917, 0.917, 0.913, 0.896, 0.898,\n",
       "        0.901, 0.904, 0.89 , 0.924, 0.925, 0.924, 0.915, 0.924, 0.92 ,\n",
       "        0.92 , 0.915, 0.903, 0.897, 0.899, 0.897, 0.925, 0.925, 0.925,\n",
       "        0.925, 0.603, 0.925, 0.925, 0.846, 0.925, 0.925, 0.919, 0.925,\n",
       "        0.925, 0.075, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925,\n",
       "        0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.108, 0.925,\n",
       "        0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925,\n",
       "        0.925, 0.924, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925,\n",
       "        0.925, 0.702, 0.925, 0.925, 0.662, 0.925, 0.925, 0.338, 0.925,\n",
       "        0.925, 0.516, 0.925, 0.925, 0.854, 0.925, 0.925, 0.864, 0.925,\n",
       "        0.925, 0.694, 0.925, 0.925, 0.496, 0.925, 0.925, 0.807, 0.925,\n",
       "        0.925, 0.266, 0.925, 0.925, 0.682, 0.925, 0.925, 0.719, 0.925,\n",
       "        0.925, 0.676, 0.925, 0.925, 0.704, 0.925, 0.925, 0.33 , 0.925,\n",
       "        0.925, 0.668, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925,\n",
       "        0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925,\n",
       "        0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925,\n",
       "        0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925,\n",
       "        0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925,\n",
       "        0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925,\n",
       "        0.925, 0.925, 0.925, 0.925, 0.925, 0.924, 0.925, 0.925, 0.925,\n",
       "        0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.924, 0.923, 0.925,\n",
       "        0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925,\n",
       "        0.925, 0.925, 0.925, 0.925, 0.925, 0.924, 0.925, 0.925, 0.925,\n",
       "        0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925]),\n",
       " 'split1_test_f1_micro': array([0.924, 0.926, 0.924, 0.914, 0.919, 0.907, 0.906, 0.911, 0.917,\n",
       "        0.897, 0.893, 0.887, 0.924, 0.924, 0.924, 0.914, 0.919, 0.921,\n",
       "        0.907, 0.913, 0.903, 0.905, 0.895, 0.905, 0.926, 0.924, 0.923,\n",
       "        0.922, 0.919, 0.923, 0.907, 0.915, 0.906, 0.899, 0.904, 0.9  ,\n",
       "        0.924, 0.924, 0.923, 0.918, 0.919, 0.922, 0.918, 0.91 , 0.914,\n",
       "        0.907, 0.906, 0.9  , 0.92 , 0.925, 0.924, 0.924, 0.915, 0.919,\n",
       "        0.891, 0.899, 0.903, 0.887, 0.875, 0.882, 0.924, 0.919, 0.922,\n",
       "        0.91 , 0.914, 0.92 , 0.902, 0.909, 0.903, 0.896, 0.887, 0.892,\n",
       "        0.921, 0.924, 0.926, 0.919, 0.915, 0.92 , 0.917, 0.919, 0.894,\n",
       "        0.9  , 0.888, 0.897, 0.924, 0.923, 0.924, 0.917, 0.927, 0.924,\n",
       "        0.91 , 0.92 , 0.915, 0.89 , 0.895, 0.896, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.116, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.77 , 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.75 , 0.924, 0.924, 0.699, 0.924, 0.924, 0.335, 0.924,\n",
       "        0.924, 0.431, 0.924, 0.924, 0.74 , 0.924, 0.924, 0.83 , 0.924,\n",
       "        0.924, 0.374, 0.924, 0.924, 0.652, 0.924, 0.924, 0.517, 0.924,\n",
       "        0.924, 0.698, 0.924, 0.924, 0.711, 0.924, 0.924, 0.734, 0.924,\n",
       "        0.924, 0.502, 0.924, 0.924, 0.717, 0.924, 0.924, 0.645, 0.924,\n",
       "        0.924, 0.753, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924]),\n",
       " 'split2_test_f1_micro': array([0.923, 0.924, 0.925, 0.912, 0.921, 0.917, 0.908, 0.916, 0.916,\n",
       "        0.885, 0.896, 0.898, 0.922, 0.924, 0.923, 0.914, 0.918, 0.92 ,\n",
       "        0.9  , 0.913, 0.909, 0.894, 0.892, 0.898, 0.922, 0.924, 0.923,\n",
       "        0.924, 0.925, 0.915, 0.912, 0.907, 0.915, 0.9  , 0.902, 0.896,\n",
       "        0.927, 0.923, 0.925, 0.918, 0.915, 0.923, 0.921, 0.917, 0.91 ,\n",
       "        0.913, 0.906, 0.903, 0.92 , 0.923, 0.92 , 0.921, 0.921, 0.917,\n",
       "        0.904, 0.906, 0.897, 0.897, 0.913, 0.908, 0.922, 0.922, 0.924,\n",
       "        0.916, 0.912, 0.918, 0.901, 0.907, 0.909, 0.887, 0.876, 0.886,\n",
       "        0.916, 0.924, 0.922, 0.918, 0.918, 0.914, 0.907, 0.912, 0.918,\n",
       "        0.888, 0.9  , 0.886, 0.924, 0.924, 0.925, 0.92 , 0.919, 0.912,\n",
       "        0.91 , 0.907, 0.913, 0.897, 0.913, 0.899, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.077, 0.924, 0.924, 0.924, 0.924, 0.924, 0.923, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.198, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.089, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.7  , 0.924, 0.924, 0.57 , 0.924, 0.924, 0.366, 0.924,\n",
       "        0.924, 0.684, 0.924, 0.924, 0.289, 0.924, 0.924, 0.646, 0.924,\n",
       "        0.924, 0.377, 0.924, 0.924, 0.386, 0.924, 0.924, 0.567, 0.924,\n",
       "        0.924, 0.353, 0.924, 0.924, 0.569, 0.924, 0.924, 0.576, 0.924,\n",
       "        0.924, 0.185, 0.924, 0.924, 0.82 , 0.924, 0.924, 0.543, 0.924,\n",
       "        0.924, 0.567, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.923, 0.924, 0.923,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.923, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.922, 0.922, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.923, 0.924, 0.924, 0.924, 0.924, 0.924]),\n",
       " 'split3_test_f1_micro': array([0.924, 0.923, 0.92 , 0.909, 0.925, 0.918, 0.916, 0.908, 0.903,\n",
       "        0.891, 0.902, 0.904, 0.924, 0.924, 0.924, 0.917, 0.922, 0.918,\n",
       "        0.908, 0.907, 0.925, 0.911, 0.9  , 0.885, 0.919, 0.924, 0.924,\n",
       "        0.916, 0.919, 0.919, 0.908, 0.92 , 0.921, 0.9  , 0.905, 0.924,\n",
       "        0.924, 0.923, 0.924, 0.92 , 0.922, 0.924, 0.916, 0.918, 0.923,\n",
       "        0.912, 0.913, 0.924, 0.924, 0.923, 0.921, 0.916, 0.921, 0.915,\n",
       "        0.9  , 0.905, 0.905, 0.9  , 0.894, 0.891, 0.921, 0.923, 0.925,\n",
       "        0.926, 0.921, 0.922, 0.908, 0.912, 0.898, 0.909, 0.884, 0.893,\n",
       "        0.923, 0.924, 0.923, 0.909, 0.919, 0.927, 0.907, 0.907, 0.914,\n",
       "        0.898, 0.91 , 0.883, 0.923, 0.923, 0.923, 0.919, 0.92 , 0.918,\n",
       "        0.914, 0.912, 0.917, 0.902, 0.903, 0.909, 0.924, 0.076, 0.924,\n",
       "        0.924, 0.552, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.602, 0.924,\n",
       "        0.924, 0.643, 0.924, 0.924, 0.444, 0.924, 0.924, 0.774, 0.924,\n",
       "        0.924, 0.739, 0.924, 0.924, 0.918, 0.924, 0.924, 0.674, 0.924,\n",
       "        0.924, 0.722, 0.924, 0.924, 0.873, 0.924, 0.924, 0.327, 0.924,\n",
       "        0.924, 0.218, 0.924, 0.924, 0.507, 0.924, 0.924, 0.77 , 0.924,\n",
       "        0.924, 0.542, 0.924, 0.924, 0.457, 0.924, 0.924, 0.759, 0.924,\n",
       "        0.924, 0.566, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924]),\n",
       " 'split4_test_f1_micro': array([0.923, 0.924, 0.922, 0.916, 0.923, 0.906, 0.908, 0.906, 0.902,\n",
       "        0.906, 0.924, 0.897, 0.925, 0.923, 0.924, 0.918, 0.922, 0.907,\n",
       "        0.912, 0.898, 0.905, 0.897, 0.906, 0.903, 0.924, 0.918, 0.915,\n",
       "        0.916, 0.914, 0.911, 0.907, 0.905, 0.903, 0.901, 0.908, 0.91 ,\n",
       "        0.924, 0.92 , 0.924, 0.919, 0.924, 0.914, 0.913, 0.915, 0.91 ,\n",
       "        0.915, 0.899, 0.901, 0.92 , 0.918, 0.924, 0.916, 0.916, 0.914,\n",
       "        0.901, 0.895, 0.91 , 0.892, 0.901, 0.899, 0.92 , 0.922, 0.922,\n",
       "        0.906, 0.911, 0.913, 0.904, 0.915, 0.912, 0.893, 0.891, 0.902,\n",
       "        0.923, 0.922, 0.921, 0.907, 0.907, 0.913, 0.903, 0.907, 0.915,\n",
       "        0.907, 0.888, 0.893, 0.922, 0.923, 0.922, 0.917, 0.913, 0.921,\n",
       "        0.906, 0.9  , 0.902, 0.899, 0.896, 0.903, 0.924, 0.826, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.791, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.076, 0.924,\n",
       "        0.924, 0.921, 0.924, 0.924, 0.914, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.721, 0.924, 0.924, 0.549, 0.924, 0.924, 0.605, 0.924,\n",
       "        0.924, 0.891, 0.924, 0.924, 0.836, 0.924, 0.924, 0.711, 0.924,\n",
       "        0.924, 0.765, 0.924, 0.924, 0.612, 0.924, 0.924, 0.742, 0.924,\n",
       "        0.924, 0.316, 0.924, 0.924, 0.553, 0.924, 0.924, 0.779, 0.924,\n",
       "        0.924, 0.425, 0.924, 0.924, 0.763, 0.924, 0.924, 0.806, 0.924,\n",
       "        0.924, 0.491, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.923, 0.925, 0.923, 0.923, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.922, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.923, 0.924, 0.924, 0.924, 0.924, 0.924,\n",
       "        0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924, 0.924]),\n",
       " 'mean_test_f1_micro': array([0.9238, 0.9226, 0.9206, 0.913 , 0.9224, 0.9134, 0.908 , 0.9116,\n",
       "        0.9084, 0.895 , 0.9048, 0.894 , 0.9232, 0.9238, 0.9242, 0.9158,\n",
       "        0.918 , 0.9168, 0.906 , 0.9072, 0.9096, 0.9   , 0.8996, 0.8994,\n",
       "        0.9226, 0.9226, 0.922 , 0.9202, 0.9198, 0.9164, 0.9096, 0.911 ,\n",
       "        0.9104, 0.9018, 0.9032, 0.9068, 0.9234, 0.923 , 0.9242, 0.9184,\n",
       "        0.9202, 0.9194, 0.9158, 0.915 , 0.9136, 0.9084, 0.907 , 0.9062,\n",
       "        0.921 , 0.9218, 0.9222, 0.9178, 0.9178, 0.9148, 0.8992, 0.9026,\n",
       "        0.9024, 0.8938, 0.893 , 0.8938, 0.9202, 0.9208, 0.9228, 0.9146,\n",
       "        0.9152, 0.9182, 0.9032, 0.9112, 0.9062, 0.896 , 0.8864, 0.8946,\n",
       "        0.9206, 0.924 , 0.9234, 0.9128, 0.9152, 0.9182, 0.9094, 0.9082,\n",
       "        0.9078, 0.8988, 0.898 , 0.8898, 0.9234, 0.9236, 0.9236, 0.9176,\n",
       "        0.9206, 0.919 , 0.912 , 0.9108, 0.91  , 0.897 , 0.9012, 0.9008,\n",
       "        0.9242, 0.735 , 0.9242, 0.9242, 0.7854, 0.9242, 0.9242, 0.9084,\n",
       "        0.9242, 0.9242, 0.923 , 0.9242, 0.9242, 0.4232, 0.9242, 0.9242,\n",
       "        0.9242, 0.9242, 0.9242, 0.924 , 0.9242, 0.9242, 0.9242, 0.9242,\n",
       "        0.9242, 0.9242, 0.9242, 0.9242, 0.5582, 0.9242, 0.9242, 0.9242,\n",
       "        0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.5876, 0.9242, 0.9242,\n",
       "        0.9234, 0.9242, 0.9242, 0.9222, 0.9242, 0.9242, 0.8598, 0.9242,\n",
       "        0.9242, 0.7032, 0.9242, 0.9242, 0.5848, 0.9242, 0.9242, 0.4836,\n",
       "        0.9242, 0.9242, 0.6522, 0.9242, 0.9242, 0.7274, 0.9242, 0.9242,\n",
       "        0.745 , 0.9242, 0.9242, 0.5864, 0.9242, 0.9242, 0.6038, 0.9242,\n",
       "        0.9242, 0.592 , 0.9242, 0.9242, 0.3702, 0.9242, 0.9242, 0.6044,\n",
       "        0.9242, 0.9242, 0.7156, 0.9242, 0.9242, 0.466 , 0.9242, 0.9242,\n",
       "        0.6922, 0.9242, 0.9242, 0.6166, 0.9242, 0.9242, 0.609 , 0.9242,\n",
       "        0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242,\n",
       "        0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242,\n",
       "        0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242,\n",
       "        0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242,\n",
       "        0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242,\n",
       "        0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242,\n",
       "        0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.924 ,\n",
       "        0.9242, 0.9238, 0.924 , 0.924 , 0.9242, 0.9242, 0.9242, 0.9242,\n",
       "        0.9242, 0.9242, 0.924 , 0.9238, 0.9242, 0.9238, 0.9242, 0.924 ,\n",
       "        0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242, 0.9242,\n",
       "        0.9242, 0.9236, 0.9238, 0.924 , 0.9242, 0.9242, 0.9242, 0.9242,\n",
       "        0.9242, 0.9242, 0.924 , 0.9242, 0.9242, 0.9242, 0.9242, 0.9242]),\n",
       " 'std_test_f1_micro': array([7.48331477e-04, 3.44093011e-03, 4.63033476e-03, 2.36643191e-03,\n",
       "        2.15406592e-03, 5.67802783e-03, 4.56070170e-03, 4.31740663e-03,\n",
       "        6.65131566e-03, 6.95701085e-03, 1.10526015e-02, 7.40270221e-03,\n",
       "        1.46969385e-03, 4.00000000e-04, 9.79795897e-04, 1.60000000e-03,\n",
       "        4.77493455e-03, 5.03587132e-03, 4.14728827e-03, 5.60000000e-03,\n",
       "        7.93977330e-03, 6.92820323e-03, 5.46260011e-03, 7.70973411e-03,\n",
       "        2.33238076e-03, 2.33238076e-03, 3.57770876e-03, 3.48711915e-03,\n",
       "        3.65513338e-03, 4.17612260e-03, 2.87054002e-03, 5.62138773e-03,\n",
       "        6.62117814e-03, 3.65513338e-03, 3.65513338e-03, 9.76524449e-03,\n",
       "        2.93938769e-03, 1.67332005e-03, 7.48331477e-04, 1.01980390e-03,\n",
       "        3.05941171e-03, 4.45421149e-03, 3.54400903e-03, 2.75680975e-03,\n",
       "        4.92341345e-03, 7.20000000e-03, 4.85798312e-03, 8.97552227e-03,\n",
       "        1.54919334e-03, 2.48193473e-03, 1.60000000e-03, 4.21426150e-03,\n",
       "        2.63818119e-03, 3.37045991e-03, 4.35430821e-03, 4.84148737e-03,\n",
       "        4.96386946e-03, 4.44522215e-03, 1.34907376e-02, 8.93084542e-03,\n",
       "        3.37045991e-03, 1.93907194e-03, 1.46969385e-03, 6.74091982e-03,\n",
       "        3.76297754e-03, 2.99332591e-03, 2.63818119e-03, 2.85657137e-03,\n",
       "        5.03587132e-03, 7.21110255e-03, 6.21610811e-03, 5.78273292e-03,\n",
       "        2.57681975e-03, 1.26491106e-03, 1.85472370e-03, 4.83321839e-03,\n",
       "        4.30813185e-03, 5.03587132e-03, 4.96386946e-03, 7.52063827e-03,\n",
       "        9.80612054e-03, 6.17737808e-03, 8.76356092e-03, 4.95580468e-03,\n",
       "        8.00000000e-04, 8.00000000e-04, 1.01980390e-03, 1.74355958e-03,\n",
       "        4.75815090e-03, 4.00000000e-03, 4.73286383e-03, 6.85273668e-03,\n",
       "        6.26099034e-03, 3.94968353e-03, 6.52380257e-03, 4.74973683e-03,\n",
       "        4.00000000e-04, 3.31693835e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        1.70514046e-01, 4.00000000e-04, 4.00000000e-04, 3.12000000e-02,\n",
       "        4.00000000e-04, 4.00000000e-04, 2.00000000e-03, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.09162755e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 6.32455532e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        3.36240628e-01, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.12433073e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        1.20000000e-03, 4.00000000e-04, 4.00000000e-04, 4.11825206e-03,\n",
       "        4.00000000e-04, 4.00000000e-04, 1.28900582e-01, 4.00000000e-04,\n",
       "        4.00000000e-04, 3.50508202e-02, 4.00000000e-04, 4.00000000e-04,\n",
       "        8.98073494e-02, 4.00000000e-04, 4.00000000e-04, 1.76737772e-01,\n",
       "        4.00000000e-04, 4.00000000e-04, 1.63126209e-01, 4.00000000e-04,\n",
       "        4.00000000e-04, 2.26496446e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        8.64684914e-02, 4.00000000e-04, 4.00000000e-04, 1.73680857e-01,\n",
       "        4.00000000e-04, 4.00000000e-04, 1.63656225e-01, 4.00000000e-04,\n",
       "        4.00000000e-04, 1.70411267e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        1.70122779e-01, 4.00000000e-04, 4.00000000e-04, 7.84438653e-02,\n",
       "        4.00000000e-04, 4.00000000e-04, 7.32355105e-02, 4.00000000e-04,\n",
       "        4.00000000e-04, 1.62366253e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        1.24433757e-01, 4.00000000e-04, 4.00000000e-04, 1.70007765e-01,\n",
       "        4.00000000e-04, 4.00000000e-04, 9.13827117e-02, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 6.32455532e-04,\n",
       "        4.00000000e-04, 7.48331477e-04, 6.32455532e-04, 6.32455532e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 0.00000000e+00, 4.00000000e-04,\n",
       "        4.00000000e-04, 9.79795897e-04, 4.00000000e-04, 6.32455532e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 1.01980390e-03, 9.79795897e-04, 0.00000000e+00,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 6.32455532e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04]),\n",
       " 'rank_test_f1_micro': array([166, 182, 192, 219, 185, 218, 235, 222, 231, 259, 243, 261, 178,\n",
       "        166,   1, 210, 204, 208, 242, 237, 228, 251, 252, 253, 182, 184,\n",
       "        188, 195, 198, 209, 228, 224, 226, 248, 244, 239, 174, 179,   1,\n",
       "        201, 195, 199, 211, 214, 217, 231, 238, 241, 190, 189, 186, 205,\n",
       "        206, 215, 254, 246, 247, 262, 264, 263, 195, 191, 181, 216, 212,\n",
       "        202, 245, 223, 240, 258, 266, 260, 192, 156, 174, 220, 212, 202,\n",
       "        230, 234, 236, 255, 256, 265, 174, 171, 171, 207, 192, 200, 221,\n",
       "        225, 227, 257, 249, 250,   1, 270,   1,   1, 268,   1,   1, 231,\n",
       "          1,   1, 179,   1,   1, 287,   1,   1,   1,   1,   1, 156,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1, 284,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1, 281,   1,   1, 174,   1,   1, 186,   1,   1, 267,\n",
       "          1,   1, 273,   1,   1, 283,   1,   1, 285,   1,   1, 275,   1,\n",
       "          1, 271,   1,   1, 269,   1,   1, 282,   1,   1, 279,   1,   1,\n",
       "        280,   1,   1, 288,   1,   1, 278,   1,   1, 272,   1,   1, 286,\n",
       "          1,   1, 274,   1,   1, 276,   1,   1, 277,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "        156,   1, 166, 156, 156,   1,   1,   1,   1,   1,   1, 156, 166,\n",
       "          1, 166,   1, 156,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "        171, 165, 156,   1,   1,   1,   1,   1,   1, 156,   1,   1,   1,\n",
       "          1,   1]),\n",
       " 'split0_test_roc_auc_ovo': array([0.56805045, 0.57001081, 0.57151712, 0.57252613, 0.58843243,\n",
       "        0.58129009, 0.62652973, 0.66812252, 0.61757117, 0.60859099,\n",
       "        0.6532036 , 0.63731892, 0.59351351, 0.59386667, 0.56015856,\n",
       "        0.60514595, 0.61045045, 0.58797838, 0.6384    , 0.62888649,\n",
       "        0.57137297, 0.64518919, 0.61862342, 0.67331171, 0.62013694,\n",
       "        0.56266667, 0.5981982 , 0.58567207, 0.58743063, 0.65321802,\n",
       "        0.68112432, 0.60207568, 0.57020541, 0.61513514, 0.58091532,\n",
       "        0.61581261, 0.57455856, 0.63538739, 0.52905225, 0.54208288,\n",
       "        0.63120721, 0.62672432, 0.59334054, 0.60131171, 0.5838991 ,\n",
       "        0.5957045 , 0.62235676, 0.57187748, 0.54552793, 0.55887568,\n",
       "        0.58640721, 0.56700541, 0.57772973, 0.54561441, 0.61022703,\n",
       "        0.58617658, 0.61693694, 0.56710631, 0.55419099, 0.54575856,\n",
       "        0.53858018, 0.62975135, 0.57852973, 0.59127207, 0.60797117,\n",
       "        0.62368288, 0.62536937, 0.6035027 , 0.58698378, 0.58009369,\n",
       "        0.64606847, 0.57045045, 0.58384144, 0.55974054, 0.59922162,\n",
       "        0.5757982 , 0.63901982, 0.54323604, 0.55831351, 0.57340541,\n",
       "        0.54228468, 0.63145225, 0.59576216, 0.58437477, 0.57461622,\n",
       "        0.59871712, 0.59243243, 0.61349189, 0.56974414, 0.5530955 ,\n",
       "        0.60915315, 0.62564324, 0.55051532, 0.57200721, 0.63122162,\n",
       "        0.65706667, 0.51779459, 0.46744505, 0.52074955, 0.52096577,\n",
       "        0.46630631, 0.49246847, 0.50918919, 0.49311712, 0.54013694,\n",
       "        0.44951351, 0.45052252, 0.45615856, 0.54299099, 0.46427387,\n",
       "        0.52841802, 0.45852252, 0.50139099, 0.48723604, 0.50905946,\n",
       "        0.54918919, 0.47656937, 0.50776216, 0.55234595, 0.51293694,\n",
       "        0.47795315, 0.50431712, 0.47182703, 0.51656937, 0.48563604,\n",
       "        0.50721441, 0.43313874, 0.4894991 , 0.51620901, 0.54668108,\n",
       "        0.53617297, 0.46856937, 0.49295856, 0.52      , 0.50224144,\n",
       "        0.47110631, 0.48841802, 0.52784144, 0.4706018 , 0.46558559,\n",
       "        0.45868108, 0.4738018 , 0.46025225, 0.51485405, 0.51982703,\n",
       "        0.47619459, 0.51685766, 0.51770811, 0.47551712, 0.52357477,\n",
       "        0.47910631, 0.47952432, 0.48379099, 0.51364324, 0.48468468,\n",
       "        0.51306667, 0.48795676, 0.48985946, 0.4282955 , 0.45497658,\n",
       "        0.5220036 , 0.46897297, 0.51878919, 0.5229982 , 0.50388468,\n",
       "        0.48900901, 0.50382703, 0.46731532, 0.45747027, 0.46332252,\n",
       "        0.46521081, 0.46660901, 0.53481802, 0.4638991 , 0.46248649,\n",
       "        0.50316396, 0.48243604, 0.51909189, 0.45833514, 0.50404324,\n",
       "        0.47440721, 0.53082523, 0.4781982 , 0.50245766, 0.51135135,\n",
       "        0.4882018 , 0.51129369, 0.55089009, 0.50064144, 0.53863784,\n",
       "        0.4734991 , 0.42401441, 0.47874595, 0.48082162, 0.47974054,\n",
       "        0.48367568, 0.48727928, 0.48516036, 0.4826955 , 0.48475676,\n",
       "        0.48703423, 0.48403604, 0.48367568, 0.48226306, 0.48240721,\n",
       "        0.47940901, 0.48923964, 0.48090811, 0.48017297, 0.48464144,\n",
       "        0.48112432, 0.48314234, 0.48234955, 0.47981261, 0.48348829,\n",
       "        0.48570811, 0.48304144, 0.48165766, 0.48435315, 0.48171532,\n",
       "        0.48485766, 0.48468468, 0.48158559, 0.48304144, 0.48311351,\n",
       "        0.48232072, 0.48258018, 0.4818018 , 0.47975495, 0.48971532,\n",
       "        0.48292613, 0.48348829, 0.48216216, 0.48498739, 0.48527568,\n",
       "        0.4826955 , 0.48374775, 0.48080721, 0.48015856, 0.48226306,\n",
       "        0.55876036, 0.58077117, 0.60638559, 0.61359279, 0.60723604,\n",
       "        0.56056216, 0.5805982 , 0.61902703, 0.58030991, 0.60665946,\n",
       "        0.60347387, 0.59884685, 0.59217297, 0.61349189, 0.52890811,\n",
       "        0.60618378, 0.57343423, 0.57896937, 0.59022703, 0.57636036,\n",
       "        0.56521802, 0.56812973, 0.58591712, 0.56801441, 0.58365405,\n",
       "        0.57448649, 0.58144865, 0.61728288, 0.59407568, 0.57582703,\n",
       "        0.5972036 , 0.55086126, 0.58402883, 0.60572252, 0.59161081,\n",
       "        0.57441441, 0.56406486, 0.5845045 , 0.5765045 , 0.59932252,\n",
       "        0.57236757, 0.56713514, 0.57999279, 0.58365405, 0.58854054,\n",
       "        0.58561441, 0.57034955, 0.59871712]),\n",
       " 'split1_test_roc_auc_ovo': array([0.58170284, 0.56258544, 0.60207337, 0.62003731, 0.54057018,\n",
       "        0.61360788, 0.6778167 , 0.57024664, 0.61802945, 0.63513898,\n",
       "        0.62722146, 0.57689679, 0.59553714, 0.61058897, 0.58250028,\n",
       "        0.6010908 , 0.613437  , 0.63025461, 0.67415698, 0.57933185,\n",
       "        0.57095865, 0.67684837, 0.585626  , 0.60463659, 0.59452609,\n",
       "        0.61877706, 0.64164673, 0.64193153, 0.62632433, 0.62742082,\n",
       "        0.61917578, 0.66246582, 0.63335897, 0.62189565, 0.59948166,\n",
       "        0.63076726, 0.64312771, 0.59557986, 0.63717532, 0.6521702 ,\n",
       "        0.6467447 , 0.64267202, 0.64329859, 0.57436204, 0.63243336,\n",
       "        0.64161825, 0.67004158, 0.61930394, 0.59458305, 0.61412053,\n",
       "        0.55812116, 0.5961637 , 0.605669  , 0.62314878, 0.64672334,\n",
       "        0.64905161, 0.65869931, 0.63971719, 0.57551549, 0.63809381,\n",
       "        0.62597545, 0.63701868, 0.59632035, 0.63202039, 0.61967419,\n",
       "        0.56205144, 0.62195261, 0.61345124, 0.61544486, 0.63896958,\n",
       "        0.65513215, 0.61508886, 0.538662  , 0.66881693, 0.59525234,\n",
       "        0.65103099, 0.57992994, 0.5898126 , 0.63864206, 0.64546309,\n",
       "        0.63274664, 0.67209216, 0.60906528, 0.65278252, 0.60738494,\n",
       "        0.64093472, 0.56916439, 0.59859877, 0.58440134, 0.61544486,\n",
       "        0.55275974, 0.61451925, 0.61216963, 0.63971007, 0.64257234,\n",
       "        0.60587548, 0.46917008, 0.5180565 , 0.50222146, 0.44714058,\n",
       "        0.4864149 , 0.50665015, 0.47455286, 0.45386193, 0.4958846 ,\n",
       "        0.50320403, 0.49767886, 0.51730178, 0.49877535, 0.4473257 ,\n",
       "        0.51092219, 0.53320802, 0.5312144 , 0.45569891, 0.51751538,\n",
       "        0.44473399, 0.45960071, 0.48894965, 0.52772556, 0.50304739,\n",
       "        0.54999715, 0.49974368, 0.49524379, 0.54608111, 0.47661768,\n",
       "        0.49446058, 0.50103953, 0.48316815, 0.44890636, 0.49112839,\n",
       "        0.47955115, 0.4846064 , 0.50800296, 0.48241342, 0.42780246,\n",
       "        0.54317612, 0.46280474, 0.47731545, 0.53354978, 0.5375655 ,\n",
       "        0.55962349, 0.47238836, 0.52492026, 0.50692071, 0.52208647,\n",
       "        0.48046252, 0.437144  , 0.57701071, 0.55099396, 0.48590226,\n",
       "        0.57894737, 0.51159148, 0.519153  , 0.48370927, 0.47280132,\n",
       "        0.48809524, 0.54363181, 0.47768569, 0.45878902, 0.5041154 ,\n",
       "        0.51097915, 0.49357769, 0.48997494, 0.48802404, 0.47009569,\n",
       "        0.49894623, 0.53063055, 0.43999203, 0.52691388, 0.47490886,\n",
       "        0.49715197, 0.46234905, 0.4995728 , 0.48407952, 0.46798815,\n",
       "        0.50017088, 0.48729779, 0.54126794, 0.49951584, 0.53933128,\n",
       "        0.49820574, 0.47089314, 0.45487298, 0.50516917, 0.5197938 ,\n",
       "        0.56336865, 0.51421167, 0.44225621, 0.54156699, 0.48917749,\n",
       "        0.49763614, 0.52426521, 0.48399408, 0.4744247 , 0.46932673,\n",
       "        0.46578093, 0.47085042, 0.46156585, 0.46034119, 0.46014183,\n",
       "        0.45927318, 0.45858966, 0.45925894, 0.46331738, 0.4703093 ,\n",
       "        0.46353099, 0.45985703, 0.46747551, 0.45830485, 0.46431419,\n",
       "        0.45745044, 0.45720836, 0.46187913, 0.45948679, 0.46555309,\n",
       "        0.45901686, 0.46303258, 0.46491228, 0.46602301, 0.46748975,\n",
       "        0.46656414, 0.46280474, 0.45934438, 0.46408635, 0.45988551,\n",
       "        0.46233481, 0.46351675, 0.46089656, 0.47019537, 0.4627905 ,\n",
       "        0.46317498, 0.46402939, 0.4651686 , 0.47154819, 0.46044087,\n",
       "        0.4612668 , 0.45723684, 0.46259114, 0.46230633, 0.46129528,\n",
       "        0.60604637, 0.63385737, 0.63387161, 0.62450159, 0.62538448,\n",
       "        0.6331596 , 0.6369902 , 0.60232969, 0.65023354, 0.62609649,\n",
       "        0.63946799, 0.62981317, 0.61456197, 0.61582935, 0.61303828,\n",
       "        0.65282524, 0.63344441, 0.63187799, 0.64616086, 0.65442014,\n",
       "        0.65311005, 0.65497551, 0.64391091, 0.63182103, 0.61037537,\n",
       "        0.6154591 , 0.5781072 , 0.64701527, 0.61924698, 0.61063169,\n",
       "        0.63568011, 0.65424926, 0.65595808, 0.62608225, 0.63297448,\n",
       "        0.63814365, 0.62263614, 0.61899066, 0.6093928 , 0.63422761,\n",
       "        0.62639553, 0.64049328, 0.61440533, 0.63126566, 0.64328435,\n",
       "        0.61863465, 0.64564821, 0.63441274]),\n",
       " 'split2_test_roc_auc_ovo': array([0.5796309 , 0.62646674, 0.57682559, 0.62759883, 0.58452239,\n",
       "        0.63440562, 0.56996184, 0.66824732, 0.62894452, 0.57063112,\n",
       "        0.60873775, 0.55780075, 0.56000797, 0.57564365, 0.54854466,\n",
       "        0.65093131, 0.59125085, 0.62001595, 0.65615744, 0.64285714,\n",
       "        0.66326327, 0.60707166, 0.68049385, 0.63367225, 0.53851959,\n",
       "        0.64574789, 0.566829  , 0.5680679 , 0.6292578 , 0.61271075,\n",
       "        0.62394623, 0.62208077, 0.58383174, 0.58408806, 0.60947824,\n",
       "        0.53562884, 0.60633117, 0.58770506, 0.6461039 , 0.63217703,\n",
       "        0.6169116 , 0.60831055, 0.70139838, 0.63862782, 0.64100592,\n",
       "        0.62529904, 0.61044657, 0.65410686, 0.5843159 , 0.55631978,\n",
       "        0.5685663 , 0.57923217, 0.60647357, 0.61050353, 0.67254785,\n",
       "        0.59111557, 0.58131123, 0.57649806, 0.61856345, 0.65231972,\n",
       "        0.59258943, 0.54029961, 0.62625313, 0.57573622, 0.61627079,\n",
       "        0.58927147, 0.58593928, 0.5588118 , 0.62760595, 0.5862668 ,\n",
       "        0.56551891, 0.57762303, 0.56157439, 0.57355747, 0.55889012,\n",
       "        0.6523838 , 0.61600023, 0.57471805, 0.57421964, 0.60040727,\n",
       "        0.59603554, 0.56326897, 0.61664103, 0.56460754, 0.60099111,\n",
       "        0.63221975, 0.53357826, 0.60096263, 0.60279961, 0.59849909,\n",
       "        0.59899749, 0.62740658, 0.6325188 , 0.59267487, 0.61359364,\n",
       "        0.60307018, 0.48440704, 0.50677831, 0.5601931 , 0.41558442,\n",
       "        0.51597744, 0.49662509, 0.55995101, 0.49141319, 0.51227501,\n",
       "        0.44642857, 0.46592333, 0.53457507, 0.4859877 , 0.44933356,\n",
       "        0.49998576, 0.49728013, 0.56742709, 0.49927375, 0.53869048,\n",
       "        0.50129585, 0.49104295, 0.59268911, 0.44205685, 0.43391148,\n",
       "        0.50983994, 0.53924584, 0.50566758, 0.5197226 , 0.52907838,\n",
       "        0.57698223, 0.54027113, 0.5413961 , 0.46877136, 0.49349225,\n",
       "        0.49950159, 0.54038505, 0.51977956, 0.49409034, 0.54076954,\n",
       "        0.52754044, 0.49622636, 0.47113522, 0.47503702, 0.53702438,\n",
       "        0.45594099, 0.44121668, 0.52210071, 0.45567043, 0.51012474,\n",
       "        0.46383003, 0.53926008, 0.50364548, 0.49780702, 0.49787822,\n",
       "        0.4742823 , 0.40954659, 0.5525319 , 0.43208874, 0.51620529,\n",
       "        0.50478469, 0.47821258, 0.45266576, 0.47695944, 0.51107883,\n",
       "        0.48943381, 0.48961893, 0.44170084, 0.4887218 , 0.45088574,\n",
       "        0.46962577, 0.57100137, 0.50068353, 0.40705457, 0.47243108,\n",
       "        0.49491627, 0.49629756, 0.53161312, 0.49068694, 0.5425638 ,\n",
       "        0.45569891, 0.43806961, 0.47422534, 0.51302973, 0.44627193,\n",
       "        0.47614776, 0.48087548, 0.49984336, 0.52120358, 0.47046594,\n",
       "        0.38812941, 0.54205115, 0.437144  , 0.48548929, 0.49690989,\n",
       "        0.51117851, 0.47083618, 0.47624744, 0.48454944, 0.47891034,\n",
       "        0.48517601, 0.48561745, 0.47778537, 0.47960811, 0.48094668,\n",
       "        0.48000684, 0.48318239, 0.47734393, 0.48526145, 0.48484848,\n",
       "        0.49194008, 0.48951925, 0.48573137, 0.47663192, 0.4822995 ,\n",
       "        0.48379471, 0.47985019, 0.48600194, 0.47871098, 0.48086124,\n",
       "        0.48158749, 0.47684552, 0.48444976, 0.48638642, 0.48671394,\n",
       "        0.48457792, 0.47980747, 0.48133117, 0.47841194, 0.48071884,\n",
       "        0.48368079, 0.48275518, 0.47913819, 0.48427888, 0.48117453,\n",
       "        0.48386591, 0.48741171, 0.48373775, 0.47989291, 0.47774265,\n",
       "        0.48060492, 0.47883914, 0.47824106, 0.48342447, 0.47903851,\n",
       "        0.58384598, 0.5726817 , 0.56986216, 0.58462919, 0.60420939,\n",
       "        0.59360048, 0.57332251, 0.57649806, 0.57296651, 0.61877706,\n",
       "        0.60440875, 0.59962406, 0.58982684, 0.59784404, 0.57844896,\n",
       "        0.59105149, 0.59694691, 0.57822112, 0.6035401 , 0.58680793,\n",
       "        0.60875199, 0.57251082, 0.59418432, 0.60459387, 0.57820688,\n",
       "        0.57256778, 0.578036  , 0.60593244, 0.59458305, 0.58071315,\n",
       "        0.56971975, 0.59532354, 0.58912907, 0.57273866, 0.61406357,\n",
       "        0.55536569, 0.5800581 , 0.59107997, 0.57672591, 0.59881237,\n",
       "        0.60768398, 0.5824362 , 0.59710355, 0.58421622, 0.57696799,\n",
       "        0.58544087, 0.61283892, 0.61064593]),\n",
       " 'split3_test_roc_auc_ovo': array([0.62616769, 0.60021503, 0.64611814, 0.64445204, 0.66121269,\n",
       "        0.64090624, 0.66114149, 0.65822226, 0.66047932, 0.64401059,\n",
       "        0.62754899, 0.63592219, 0.63941103, 0.61259683, 0.66522129,\n",
       "        0.60571884, 0.63034005, 0.64218786, 0.72500854, 0.62042891,\n",
       "        0.66461609, 0.68469469, 0.66861757, 0.64207393, 0.62467248,\n",
       "        0.61000513, 0.66707963, 0.67689109, 0.72227444, 0.66948622,\n",
       "        0.64459444, 0.59841365, 0.72207507, 0.65709729, 0.63264696,\n",
       "        0.51204716, 0.68262987, 0.61708248, 0.63533835, 0.70107086,\n",
       "        0.61795113, 0.68956482, 0.64903737, 0.68990658, 0.63088118,\n",
       "        0.63839998, 0.70243791, 0.5139411 , 0.63375769, 0.59933214,\n",
       "        0.64789815, 0.67237697, 0.67189992, 0.62479352, 0.68457365,\n",
       "        0.67489747, 0.65417806, 0.65910515, 0.68399692, 0.63314536,\n",
       "        0.65170027, 0.6388699 , 0.60569036, 0.58435862, 0.6851219 ,\n",
       "        0.6582721 , 0.66013044, 0.64499316, 0.67162224, 0.69493336,\n",
       "        0.61795113, 0.64153281, 0.61500342, 0.63391433, 0.71274778,\n",
       "        0.59441217, 0.63718956, 0.61953178, 0.66279335, 0.6503617 ,\n",
       "        0.67985304, 0.66007348, 0.66071429, 0.61147186, 0.58653737,\n",
       "        0.63674812, 0.64741399, 0.63011221, 0.61571542, 0.68301435,\n",
       "        0.66319207, 0.73519025, 0.67616484, 0.59800068, 0.70831909,\n",
       "        0.64043632, 0.51750114, 0.53006095, 0.54385965, 0.56359649,\n",
       "        0.4917692 , 0.4802774 , 0.4958846 , 0.48141661, 0.49332137,\n",
       "        0.49084359, 0.47515095, 0.5804853 , 0.53068751, 0.48323935,\n",
       "        0.45968615, 0.52957678, 0.52261335, 0.51439679, 0.51295853,\n",
       "        0.47957963, 0.45396161, 0.45997095, 0.56328321, 0.4823707 ,\n",
       "        0.47617624, 0.49323593, 0.5348314 , 0.49458874, 0.53451811,\n",
       "        0.50740488, 0.56566131, 0.54895762, 0.56766917, 0.55321542,\n",
       "        0.51122123, 0.44318182, 0.49374858, 0.49030246, 0.51881123,\n",
       "        0.52126054, 0.48081852, 0.5585982 , 0.46679198, 0.48923445,\n",
       "        0.52302632, 0.57161369, 0.43647471, 0.47938027, 0.48325359,\n",
       "        0.54232171, 0.52315448, 0.50464229, 0.49280873, 0.5686375 ,\n",
       "        0.51883971, 0.42887047, 0.51335726, 0.51169116, 0.47926635,\n",
       "        0.52238551, 0.56645876, 0.48852244, 0.5160344 , 0.4903167 ,\n",
       "        0.4359051 , 0.50505525, 0.46160857, 0.52667179, 0.531072  ,\n",
       "        0.55421224, 0.58495671, 0.61706824, 0.47751481, 0.43036569,\n",
       "        0.511919  , 0.56116143, 0.49755069, 0.50995386, 0.59492481,\n",
       "        0.50239234, 0.55206197, 0.58472887, 0.49107143, 0.48906357,\n",
       "        0.51019594, 0.51196172, 0.42335954, 0.53129984, 0.51767202,\n",
       "        0.50743336, 0.55855548, 0.51429711, 0.49900319, 0.54835954,\n",
       "        0.49315049, 0.57353611, 0.51636193, 0.53879016, 0.51381294,\n",
       "        0.51624801, 0.52295512, 0.52200103, 0.51760082, 0.523126  ,\n",
       "        0.51489519, 0.51157724, 0.51552176, 0.50899977, 0.51866883,\n",
       "        0.52889326, 0.52765436, 0.52079061, 0.52286967, 0.51291581,\n",
       "        0.52234279, 0.51411198, 0.52137446, 0.51787138, 0.51426863,\n",
       "        0.51315789, 0.51150604, 0.52630155, 0.52265607, 0.51523696,\n",
       "        0.515892  , 0.51899635, 0.52043461, 0.5211039 , 0.51164844,\n",
       "        0.5177717 , 0.51881123, 0.51452495, 0.5190818 , 0.5197226 ,\n",
       "        0.52133174, 0.51471007, 0.50148098, 0.52466393, 0.50890009,\n",
       "        0.51461039, 0.51251709, 0.51550752, 0.51451071, 0.51976532,\n",
       "        0.62618193, 0.62165357, 0.63515322, 0.64584757, 0.64317043,\n",
       "        0.67247665, 0.64949305, 0.66522841, 0.66396104, 0.67787366,\n",
       "        0.6854779 , 0.69577352, 0.60859535, 0.6312087 , 0.67089599,\n",
       "        0.66383288, 0.65919059, 0.67539588, 0.68197482, 0.69608681,\n",
       "        0.64962121, 0.67521075, 0.6894509 , 0.71251994, 0.62683698,\n",
       "        0.64019424, 0.62306334, 0.63136535, 0.6505753 , 0.67670597,\n",
       "        0.63234792, 0.64731431, 0.65829346, 0.67257633, 0.69715482,\n",
       "        0.65799442, 0.65081738, 0.63098086, 0.62572625, 0.66957166,\n",
       "        0.64188881, 0.63291752, 0.64020848, 0.64440932, 0.65449134,\n",
       "        0.67553828, 0.67135167, 0.66216678]),\n",
       " 'split4_test_roc_auc_ovo': array([0.53790015, 0.55868364, 0.62767715, 0.62874516, 0.60539132,\n",
       "        0.52808157, 0.64155417, 0.67461979, 0.56983368, 0.60063511,\n",
       "        0.50844441, 0.56635908, 0.60991969, 0.51171964, 0.53634085,\n",
       "        0.58760538, 0.59138614, 0.55604921, 0.5723969 , 0.58813226,\n",
       "        0.63296024, 0.60269993, 0.57961666, 0.63659148, 0.53236785,\n",
       "        0.64355491, 0.57698223, 0.60067783, 0.55177717, 0.5629272 ,\n",
       "        0.57259626, 0.59073821, 0.62378959, 0.65246924, 0.58175268,\n",
       "        0.64029392, 0.5563625 , 0.56902199, 0.5747038 , 0.5797733 ,\n",
       "        0.59878389, 0.60099111, 0.63360105, 0.60447995, 0.62608225,\n",
       "        0.62786227, 0.57954545, 0.64033664, 0.56692869, 0.56892231,\n",
       "        0.56372465, 0.60957792, 0.5878617 , 0.63029733, 0.59986614,\n",
       "        0.65844298, 0.63075302, 0.56165983, 0.58423046, 0.62232285,\n",
       "        0.60805423, 0.60153224, 0.60579004, 0.58824618, 0.61325188,\n",
       "        0.63176407, 0.57961666, 0.62012987, 0.60241513, 0.67147984,\n",
       "        0.65090283, 0.64467988, 0.60849567, 0.5934296 , 0.56156015,\n",
       "        0.60234393, 0.59163534, 0.52032069, 0.52907838, 0.58235076,\n",
       "        0.60455115, 0.58317669, 0.60842447, 0.6312087 , 0.58163876,\n",
       "        0.55335783, 0.55315846, 0.59387104, 0.60086295, 0.5974026 ,\n",
       "        0.57451868, 0.62693666, 0.60052119, 0.62327694, 0.55990829,\n",
       "        0.5918347 , 0.51629073, 0.54945603, 0.49023126, 0.50720551,\n",
       "        0.49661085, 0.49770734, 0.48564593, 0.52059125, 0.50085441,\n",
       "        0.51006778, 0.53000399, 0.48603042, 0.50266291, 0.49765038,\n",
       "        0.55189109, 0.48557473, 0.51284461, 0.5196514 , 0.51452495,\n",
       "        0.50018512, 0.44630041, 0.47039474, 0.49339257, 0.48722659,\n",
       "        0.52497722, 0.4536056 , 0.48791012, 0.46804511, 0.48275518,\n",
       "        0.51250285, 0.51865459, 0.49464571, 0.51495215, 0.5017373 ,\n",
       "        0.49165527, 0.53152768, 0.50430052, 0.5156784 , 0.49924527,\n",
       "        0.46361643, 0.52321144, 0.47005297, 0.52654363, 0.52987583,\n",
       "        0.44978925, 0.52698508, 0.50032752, 0.50793176, 0.49703805,\n",
       "        0.51117851, 0.47245956, 0.55177717, 0.48153053, 0.48086124,\n",
       "        0.5156072 , 0.52242823, 0.50200786, 0.50441445, 0.45447425,\n",
       "        0.49982912, 0.46492652, 0.50793176, 0.47977899, 0.51107883,\n",
       "        0.5312144 , 0.51664673, 0.50659319, 0.48153053, 0.53322226,\n",
       "        0.48513329, 0.48687059, 0.49557131, 0.4945745 , 0.49689565,\n",
       "        0.51893939, 0.49322169, 0.51552176, 0.46008487, 0.50625142,\n",
       "        0.52413705, 0.52670027, 0.55444008, 0.51095067, 0.49165527,\n",
       "        0.51443951, 0.52385224, 0.52728412, 0.46514012, 0.4702381 ,\n",
       "        0.51082251, 0.56314081, 0.51055195, 0.56665812, 0.54121098,\n",
       "        0.51740146, 0.55233254, 0.50615174, 0.52247095, 0.51391262,\n",
       "        0.5161056 , 0.51349966, 0.50805992, 0.51159148, 0.5100393 ,\n",
       "        0.51063739, 0.51050923, 0.50835897, 0.50993962, 0.5177005 ,\n",
       "        0.53410515, 0.51349966, 0.51426863, 0.5133003 , 0.51643313,\n",
       "        0.50452837, 0.50660743, 0.50796024, 0.50907097, 0.50981146,\n",
       "        0.50790328, 0.52516234, 0.52436489, 0.51167692, 0.50810264,\n",
       "        0.50743336, 0.51471007, 0.51613409, 0.51181932, 0.51325758,\n",
       "        0.50852985, 0.50998234, 0.50764696, 0.51849795, 0.51073707,\n",
       "        0.5113494 , 0.50992538, 0.51169116, 0.51270221, 0.50712007,\n",
       "        0.51040955, 0.50831625, 0.50674983, 0.50971178, 0.50951242,\n",
       "        0.60453691, 0.58861643, 0.61191331, 0.59898325, 0.58514183,\n",
       "        0.61437685, 0.58695033, 0.60056391, 0.60920768, 0.58002962,\n",
       "        0.61330884, 0.63019765, 0.60237241, 0.59688995, 0.60593244,\n",
       "        0.60020791, 0.59854181, 0.60897984, 0.60587548, 0.60064935,\n",
       "        0.60524892, 0.59528082, 0.5902398 , 0.61533094, 0.60383914,\n",
       "        0.60714286, 0.59372864, 0.60999089, 0.60789758, 0.61711096,\n",
       "        0.59674755, 0.58341877, 0.60792607, 0.59425553, 0.59284575,\n",
       "        0.59908293, 0.60802575, 0.60049271, 0.6090368 , 0.62099852,\n",
       "        0.6030417 , 0.59906869, 0.60121896, 0.6128674 , 0.59510993,\n",
       "        0.60066359, 0.6036113 , 0.61296708]),\n",
       " 'mean_test_roc_auc_ovo': array([0.5786904 , 0.58359233, 0.60484227, 0.61867189, 0.5960258 ,\n",
       "        0.59965828, 0.63540079, 0.64789171, 0.61897163, 0.61180136,\n",
       "        0.60503124, 0.59485955, 0.59967787, 0.58088315, 0.57855313,\n",
       "        0.61009845, 0.6073729 , 0.6072972 , 0.65322397, 0.61192733,\n",
       "        0.62063424, 0.64330077, 0.6265955 , 0.63805719, 0.58204459,\n",
       "        0.61615033, 0.61014716, 0.61464808, 0.62341287, 0.6251526 ,\n",
       "        0.62828741, 0.61515483, 0.62665215, 0.62613707, 0.60085497,\n",
       "        0.58690996, 0.61260196, 0.60095535, 0.60447472, 0.62145485,\n",
       "        0.62231971, 0.63365257, 0.64413518, 0.62173762, 0.62286036,\n",
       "        0.62577681, 0.63696566, 0.5999132 , 0.58502265, 0.57951408,\n",
       "        0.58494349, 0.60487123, 0.60992678, 0.60687152, 0.6427876 ,\n",
       "        0.63193684, 0.62837571, 0.60081731, 0.60329947, 0.61832806,\n",
       "        0.60337991, 0.60949436, 0.60251672, 0.5943267 , 0.62845798,\n",
       "        0.61300839, 0.61460167, 0.60817776, 0.62081439, 0.63434865,\n",
       "        0.6271147 , 0.60987501, 0.58151538, 0.60589177, 0.6055344 ,\n",
       "        0.61519382, 0.61275498, 0.56952383, 0.59260939, 0.61039765,\n",
       "        0.61109421, 0.62201271, 0.61812145, 0.60888908, 0.59023368,\n",
       "        0.61239551, 0.57914951, 0.60740731, 0.5947047 , 0.60949128,\n",
       "        0.59972423, 0.6459392 , 0.61437795, 0.60513395, 0.631123  ,\n",
       "        0.61965667, 0.50103272, 0.51435937, 0.523451  , 0.49089855,\n",
       "        0.49141574, 0.49474569, 0.50504472, 0.48808002, 0.50849446,\n",
       "        0.4800115 , 0.48385593, 0.51491023, 0.51222089, 0.46836457,\n",
       "        0.51018064, 0.50083244, 0.52709809, 0.49525138, 0.51854976,\n",
       "        0.49499676, 0.46549501, 0.50395332, 0.51576083, 0.48389862,\n",
       "        0.50778874, 0.49802963, 0.49909598, 0.50900139, 0.50172108,\n",
       "        0.51971299, 0.51175306, 0.51153334, 0.50330161, 0.51725089,\n",
       "        0.50362045, 0.49365407, 0.50375804, 0.50049692, 0.49777399,\n",
       "        0.50533997, 0.49029582, 0.50098866, 0.49450484, 0.51185715,\n",
       "        0.48941222, 0.49720112, 0.48881509, 0.49295144, 0.50646598,\n",
       "        0.49479747, 0.49777515, 0.53095675, 0.49973147, 0.5113708 ,\n",
       "        0.51335658, 0.47039222, 0.5141682 , 0.48910937, 0.48148638,\n",
       "        0.50563224, 0.50823728, 0.48333302, 0.47197147, 0.49431327,\n",
       "        0.49790721, 0.49477432, 0.48373334, 0.50158927, 0.49783207,\n",
       "        0.49938531, 0.53545725, 0.50412608, 0.47270561, 0.46758476,\n",
       "        0.49762749, 0.49592775, 0.51581528, 0.48174086, 0.51484293,\n",
       "        0.49711263, 0.49731314, 0.53475082, 0.49458056, 0.49407306,\n",
       "        0.49467923, 0.50368156, 0.47671164, 0.50505407, 0.49790424,\n",
       "        0.49159115, 0.53785056, 0.49102787, 0.51867181, 0.52285915,\n",
       "        0.49857314, 0.50899689, 0.49230023, 0.50021137, 0.49114063,\n",
       "        0.49339724, 0.49604039, 0.49091451, 0.49036742, 0.49180211,\n",
       "        0.49036937, 0.48957891, 0.48883185, 0.48995626, 0.49478686,\n",
       "        0.4995757 , 0.49595399, 0.49383485, 0.49025594, 0.49212082,\n",
       "        0.48984813, 0.48818406, 0.49191306, 0.48899055, 0.49079654,\n",
       "        0.48947473, 0.49191758, 0.49633723, 0.49421912, 0.49185172,\n",
       "        0.49186502, 0.49220066, 0.49176597, 0.49169259, 0.48972478,\n",
       "        0.49092758, 0.49152914, 0.48880169, 0.49436179, 0.492828  ,\n",
       "        0.49252963, 0.49191297, 0.48884813, 0.49475893, 0.48789587,\n",
       "        0.48991743, 0.48813141, 0.48877935, 0.49002237, 0.49037492,\n",
       "        0.59587431, 0.59951605, 0.61143718, 0.61351088, 0.61302843,\n",
       "        0.61483515, 0.60547086, 0.61272942, 0.61533573, 0.62188726,\n",
       "        0.62922747, 0.63085105, 0.60150591, 0.61105279, 0.59944476,\n",
       "        0.62282026, 0.61231159, 0.61468884, 0.62555566, 0.62286492,\n",
       "        0.61639004, 0.61322153, 0.62074061, 0.62645604, 0.60058249,\n",
       "        0.60197009, 0.59087677, 0.62231736, 0.61327572, 0.61219776,\n",
       "        0.60633979, 0.60623343, 0.6190671 , 0.61427506, 0.62572989,\n",
       "        0.60500022, 0.60512045, 0.60520974, 0.59947725, 0.62458654,\n",
       "        0.61027552, 0.60441017, 0.60658582, 0.61128253, 0.61167883,\n",
       "        0.61317836, 0.62075993, 0.62378193]),\n",
       " 'std_test_roc_auc_ovo': array([0.02842923, 0.02592573, 0.02873429, 0.02440112, 0.03899322,\n",
       "        0.04138594, 0.03704189, 0.03917526, 0.02911953, 0.02612301,\n",
       "        0.05032452, 0.03463302, 0.02573894, 0.03706159, 0.04592972,\n",
       "        0.02143878, 0.01475962, 0.03132328, 0.0497043 , 0.0242702 ,\n",
       "        0.04194736, 0.03406887, 0.04152096, 0.0218967 , 0.03946177,\n",
       "        0.03010712, 0.03834757, 0.0395552 , 0.05699587, 0.0368377 ,\n",
       "        0.03538652, 0.0258217 , 0.05325135, 0.02667984, 0.0192326 ,\n",
       "        0.05261723, 0.04579351, 0.02310781, 0.04545269, 0.05555964,\n",
       "        0.01598646, 0.03151038, 0.0346262 , 0.03972852, 0.02006787,\n",
       "        0.01624325, 0.04380501, 0.05123321, 0.0295101 , 0.02309315,\n",
       "        0.03287346, 0.03673195, 0.03284583, 0.03130668, 0.0333157 ,\n",
       "        0.03633589, 0.02806009, 0.04042731, 0.04537023, 0.03754454,\n",
       "        0.03790173, 0.03710031, 0.01547885, 0.01955549, 0.02859167,\n",
       "        0.0336885 , 0.02928594, 0.0282308 , 0.02877303, 0.04544458,\n",
       "        0.03343085, 0.03109748, 0.02859668, 0.04018853, 0.05612433,\n",
       "        0.03103678, 0.02375266, 0.0347891 , 0.05019039, 0.03187969,\n",
       "        0.04516842, 0.04243758, 0.02232633, 0.03160151, 0.01217644,\n",
       "        0.03308499, 0.03921339, 0.01307585, 0.01596302, 0.04216287,\n",
       "        0.03730572, 0.04487637, 0.04104152, 0.02378306, 0.04789394,\n",
       "        0.02478062, 0.02037948, 0.02738762, 0.02579937, 0.0529942 ,\n",
       "        0.01603673, 0.0085853 , 0.02973837, 0.02148833, 0.01710638,\n",
       "        0.02689442, 0.02767377, 0.04237527, 0.0212044 , 0.01949597,\n",
       "        0.03074587, 0.02796412, 0.02248333, 0.02283301, 0.01043307,\n",
       "        0.03396307, 0.01619588, 0.04727427, 0.04398322, 0.02728378,\n",
       "        0.02818054, 0.02734125, 0.02099412, 0.02620025, 0.02478984,\n",
       "        0.0292477 , 0.04485979, 0.02781239, 0.04146493, 0.02700783,\n",
       "        0.01927122, 0.0370857 , 0.00991787, 0.01471619, 0.03797189,\n",
       "        0.0319083 , 0.01983791, 0.03591266, 0.02922104, 0.02922953,\n",
       "        0.04403234, 0.04630718, 0.03492679, 0.02224107, 0.01458496,\n",
       "        0.02841631, 0.03754003, 0.02887331, 0.02682193, 0.03221783,\n",
       "        0.03751576, 0.04453036, 0.02266243, 0.03041909, 0.02012782,\n",
       "        0.01164374, 0.03957554, 0.01814936, 0.02866617, 0.02107889,\n",
       "        0.033991  , 0.01598636, 0.02844883, 0.01918028, 0.03276357,\n",
       "        0.02899173, 0.03767761, 0.0605131 , 0.03995519, 0.02163184,\n",
       "        0.01853031, 0.03535537, 0.01554462, 0.01826902, 0.04938003,\n",
       "        0.02244666, 0.03922364, 0.03697697, 0.0197927 , 0.02987842,\n",
       "        0.01672151, 0.02369554, 0.03582802, 0.02258108, 0.02266724,\n",
       "        0.05742586, 0.02167948, 0.0442454 , 0.03046278, 0.02467324,\n",
       "        0.01531269, 0.05468944, 0.0160077 , 0.02557351, 0.01891024,\n",
       "        0.01981204, 0.019231  , 0.0215739 , 0.02129936, 0.02228736,\n",
       "        0.0204831 , 0.01976855, 0.02062414, 0.01762298, 0.019731  ,\n",
       "        0.02762615, 0.02324755, 0.02035551, 0.02409502, 0.019744  ,\n",
       "        0.02206251, 0.02032754, 0.02075536, 0.02143689, 0.01844917,\n",
       "        0.01951708, 0.02293128, 0.02460788, 0.02033295, 0.01751349,\n",
       "        0.01767489, 0.02144241, 0.023149  , 0.02137023, 0.02024666,\n",
       "        0.01987253, 0.02013253, 0.01968709, 0.02045812, 0.02045384,\n",
       "        0.02103677, 0.01850884, 0.01620451, 0.02035431, 0.01830015,\n",
       "        0.01994696, 0.02030825, 0.01948213, 0.01947489, 0.02130719,\n",
       "        0.0228875 , 0.02391506, 0.02374861, 0.02103173, 0.01974944,\n",
       "        0.0375633 , 0.0313898 , 0.02954545, 0.03644665, 0.03208057,\n",
       "        0.03098903, 0.03525855, 0.00943373, 0.01272857, 0.04636463,\n",
       "        0.02959621, 0.03027   , 0.03638455, 0.03384613, 0.04541693,\n",
       "        0.03241515, 0.04381452, 0.04026457, 0.04786005, 0.01779351,\n",
       "        0.02565011, 0.01709354, 0.01508381, 0.02085136, 0.03606109,\n",
       "        0.0247135 , 0.0392397 , 0.03208657, 0.0338752 , 0.03882161,\n",
       "        0.03831701, 0.03071525, 0.01734746, 0.01961675, 0.02619948,\n",
       "        0.02347472, 0.02833893, 0.02008748, 0.02447505, 0.03113381,\n",
       "        0.03347921, 0.03486252, 0.02237933]),\n",
       " 'rank_test_roc_auc_ovo': array([142, 136, 106,  47, 125, 121,   9,   2,  46,  74, 103, 127, 120,\n",
       "        139, 143,  83,  91,  92,   1,  73,  43,   5,  21,   7, 137,  51,\n",
       "         82,  57,  30,  27,  18,  54,  20,  23, 115, 133,  69, 114, 107,\n",
       "         39,  34,  11,   4,  38,  32,  24,   8, 118, 134, 140, 135, 105,\n",
       "         84,  93,   6,  12,  17, 116, 110,  48, 109,  86, 111, 129,  16,\n",
       "         66,  58,  89,  40,  10,  19,  85, 138,  97,  98,  53,  67, 144,\n",
       "        130,  80,  78,  36,  49,  88, 132,  70, 141,  90, 128,  87, 119,\n",
       "          3,  59, 101,  13,  44, 187, 160, 150, 249, 244, 217, 178, 273,\n",
       "        171, 281, 276, 158, 163, 286, 168, 189, 149, 211, 154, 212, 288,\n",
       "        180, 157, 275, 173, 197, 195, 169, 185, 152, 165, 166, 184, 155,\n",
       "        183, 226, 181, 190, 202, 176, 254, 188, 220, 164, 263, 205, 268,\n",
       "        228, 174, 213, 201, 148, 192, 167, 162, 285, 161, 264, 280, 175,\n",
       "        172, 278, 284, 222, 198, 215, 277, 186, 200, 194, 146, 179, 283,\n",
       "        287, 203, 210, 156, 279, 159, 206, 204, 147, 219, 224, 218, 182,\n",
       "        282, 177, 199, 242, 145, 246, 153, 151, 196, 170, 231, 191, 245,\n",
       "        227, 208, 248, 253, 239, 252, 261, 267, 257, 214, 193, 209, 225,\n",
       "        255, 233, 259, 271, 235, 265, 250, 262, 234, 207, 223, 238, 237,\n",
       "        232, 240, 241, 260, 247, 243, 269, 221, 229, 230, 236, 266, 216,\n",
       "        274, 258, 272, 270, 256, 251, 126, 122,  76,  61,  65,  55,  99,\n",
       "         68,  52,  37,  15,  14, 113,  79, 124,  33,  71,  56,  26,  31,\n",
       "         50,  63,  42,  22, 117, 112, 131,  35,  62,  72,  95,  96,  45,\n",
       "         60,  25, 104, 102, 100, 123,  28,  81, 108,  94,  77,  75,  64,\n",
       "         41,  29]),\n",
       " 'split0_test_neg_log_loss': array([-0.28201528, -0.28459325, -0.28768994, -0.32083855, -0.30625041,\n",
       "        -0.30735964, -0.33601593, -0.29907589, -0.33529702, -0.40592021,\n",
       "        -0.35319773, -0.42266678, -0.27604284, -0.2744758 , -0.28679232,\n",
       "        -0.30090273, -0.30292399, -0.30575065, -0.30902742, -0.34453951,\n",
       "        -0.34348561, -0.39575765, -0.36913384, -0.36345635, -0.2695309 ,\n",
       "        -0.27539385, -0.27479429, -0.29607826, -0.28884625, -0.279628  ,\n",
       "        -0.28824205, -0.32624199, -0.34395931, -0.36046617, -0.39371172,\n",
       "        -0.353372  , -0.27723146, -0.25892363, -0.28450796, -0.30175366,\n",
       "        -0.28311035, -0.28914963, -0.32754581, -0.31384504, -0.3149995 ,\n",
       "        -0.3439306 , -0.32165031, -0.33764305, -0.28449125, -0.28412028,\n",
       "        -0.27916195, -0.3169448 , -0.32584819, -0.33881171, -0.50056143,\n",
       "        -0.63642273, -0.35469409, -0.43365764, -0.46202467, -0.4763997 ,\n",
       "        -0.28931611, -0.27421342, -0.28859896, -0.30429452, -0.29195419,\n",
       "        -0.29423143, -0.33393354, -0.33619344, -0.33725013, -0.41785165,\n",
       "        -0.40262894, -0.40212033, -0.27963849, -0.28002422, -0.276914  ,\n",
       "        -0.30794545, -0.29671347, -0.32854122, -0.34412252, -0.34947571,\n",
       "        -0.36635656, -0.3838246 , -0.40350344, -0.43128639, -0.27850796,\n",
       "        -0.2763451 , -0.27002275, -0.28733389, -0.29366221, -0.29993562,\n",
       "        -0.31088678, -0.31225535, -0.35072851, -0.37813428, -0.38007758,\n",
       "        -0.33442169, -0.26727183, -0.51618687, -0.26678287, -0.26660055,\n",
       "        -0.67310543, -0.26724074, -0.26735003, -0.6485432 , -0.26572464,\n",
       "        -0.26817807, -0.57070563, -0.26850559, -0.26658324, -0.98763699,\n",
       "        -0.2665327 , -0.26979197, -0.4206322 , -0.26735322, -0.26642229,\n",
       "        -0.50369955, -0.26762886, -0.2668054 , -0.49849237, -0.26637695,\n",
       "        -0.26872961, -0.56138601, -0.26807983, -0.26784605, -0.78802271,\n",
       "        -0.2669834 , -0.26875113, -0.48701286, -0.26678447, -0.26613759,\n",
       "        -0.39233214, -0.26790076, -0.26753671, -0.46676737, -0.26746467,\n",
       "        -0.2681271 , -0.58272217, -0.26740329, -0.26896724, -0.49155863,\n",
       "        -0.26810598, -0.26767364, -0.38395519, -0.26655716, -0.27023227,\n",
       "        -0.54183614, -0.26847712, -0.26751314, -0.63493645, -0.26950495,\n",
       "        -0.27039286, -0.91704829, -0.27098902, -0.26755894, -0.76807378,\n",
       "        -0.26889929, -0.27398659, -0.57251695, -0.27752983, -0.27380709,\n",
       "        -0.41966197, -0.27334799, -0.27169604, -0.56507175, -0.26927285,\n",
       "        -0.27017938, -0.75330125, -0.27137664, -0.27388826, -0.4767376 ,\n",
       "        -0.2752521 , -0.27180366, -1.08850341, -0.27210808, -0.27093418,\n",
       "        -0.584777  , -0.27084205, -0.26620059, -0.56697727, -0.26971988,\n",
       "        -0.27133459, -0.58359791, -0.27254398, -0.26869584, -0.58140031,\n",
       "        -0.27068571, -0.26777893, -0.78795087, -0.26851736, -0.26522282,\n",
       "        -0.60479021, -0.27415812, -0.26792046, -0.26789615, -0.26787522,\n",
       "        -0.26759976, -0.26759273, -0.26763261, -0.26763903, -0.26766015,\n",
       "        -0.26760127, -0.2675699 , -0.26769772, -0.26768718, -0.26735992,\n",
       "        -0.26773549, -0.2671447 , -0.26779743, -0.26777723, -0.26752292,\n",
       "        -0.26783506, -0.26765219, -0.26763791, -0.26775104, -0.26773532,\n",
       "        -0.26754465, -0.26736254, -0.26761178, -0.26765102, -0.26777339,\n",
       "        -0.2674613 , -0.26765245, -0.26768693, -0.26759968, -0.26760667,\n",
       "        -0.26771982, -0.26752731, -0.26770533, -0.26754967, -0.26713714,\n",
       "        -0.2673887 , -0.26749397, -0.26761358, -0.26739333, -0.2675307 ,\n",
       "        -0.26749952, -0.26752464, -0.2676237 , -0.2675558 , -0.2675395 ,\n",
       "        -0.26546795, -0.26358194, -0.26133441, -0.26244242, -0.26077339,\n",
       "        -0.26906092, -0.26955604, -0.26337784, -0.27088782, -0.26813084,\n",
       "        -0.2674775 , -0.26461372, -0.26204927, -0.26073657, -0.26702813,\n",
       "        -0.26193507, -0.27063224, -0.26500066, -0.26922758, -0.27235611,\n",
       "        -0.26936103, -0.27129428, -0.27194608, -0.27008726, -0.26346564,\n",
       "        -0.26401803, -0.26267683, -0.26155599, -0.26330732, -0.26636761,\n",
       "        -0.2637024 , -0.27287593, -0.26843501, -0.26488969, -0.26747708,\n",
       "        -0.27100629, -0.26405694, -0.26269692, -0.26409317, -0.26148904,\n",
       "        -0.26445695, -0.26642038, -0.26409687, -0.26381479, -0.26382041,\n",
       "        -0.26612712, -0.26830403, -0.2620947 ]),\n",
       " 'split1_test_neg_log_loss': array([-0.2820448 , -0.27703528, -0.27333648, -0.2859042 , -0.33033101,\n",
       "        -0.30857788, -0.31693704, -0.33458165, -0.32674468, -0.3985434 ,\n",
       "        -0.42584969, -0.43052512, -0.27213724, -0.27266448, -0.27402384,\n",
       "        -0.30107308, -0.29414281, -0.2900573 , -0.31152083, -0.34697634,\n",
       "        -0.35367439, -0.32998107, -0.39525522, -0.42226845, -0.26889011,\n",
       "        -0.26746755, -0.26321549, -0.28276914, -0.28725391, -0.28440176,\n",
       "        -0.30562966, -0.29208277, -0.31857734, -0.35456281, -0.3600893 ,\n",
       "        -0.35513654, -0.26230737, -0.26839301, -0.26584605, -0.27255941,\n",
       "        -0.27702371, -0.27529216, -0.28915545, -0.31790227, -0.29760511,\n",
       "        -0.31612167, -0.30350301, -0.34656221, -0.28058765, -0.26962287,\n",
       "        -0.28978566, -0.34975253, -0.30922656, -0.29359761, -0.35851728,\n",
       "        -0.42480292, -0.33497741, -0.39742169, -0.49042704, -0.54877156,\n",
       "        -0.27026285, -0.2651756 , -0.27396747, -0.29806417, -0.3077714 ,\n",
       "        -0.31924593, -0.34612961, -0.33701092, -0.3394471 , -0.37469655,\n",
       "        -0.38440677, -0.38633519, -0.29174635, -0.2560801 , -0.26791055,\n",
       "        -0.28993758, -0.31761145, -0.29563551, -0.30976763, -0.30766945,\n",
       "        -0.36278439, -0.34667846, -0.42531206, -0.34443201, -0.27045686,\n",
       "        -0.26459365, -0.27913414, -0.28663646, -0.28659624, -0.28220936,\n",
       "        -0.33840469, -0.29949139, -0.30869675, -0.35318418, -0.36152605,\n",
       "        -0.36934714, -0.27068526, -0.5993148 , -0.26967286, -0.27205043,\n",
       "        -0.40615047, -0.26953207, -0.26977919, -0.46843492, -0.26911713,\n",
       "        -0.26904428, -0.34968686, -0.26905881, -0.27064752, -0.76326146,\n",
       "        -0.27009062, -0.26888913, -0.34364347, -0.2712759 , -0.26896402,\n",
       "        -0.52648851, -0.27066438, -0.2698174 , -0.402311  , -0.26944003,\n",
       "        -0.26859582, -0.50535948, -0.27030765, -0.26827855, -0.64777945,\n",
       "        -0.26957546, -0.26985573, -0.54774338, -0.27097609, -0.26988556,\n",
       "        -0.52075369, -0.27017256, -0.27033275, -0.49359393, -0.2731898 ,\n",
       "        -0.2681232 , -0.55575428, -0.27197335, -0.26858363, -0.53602452,\n",
       "        -0.26797876, -0.26999797, -0.52216226, -0.26918616, -0.2709284 ,\n",
       "        -0.53289288, -0.27847607, -0.26746774, -0.57735854, -0.27103526,\n",
       "        -0.2667842 , -0.94687048, -0.2705419 , -0.27166404, -0.79657809,\n",
       "        -0.27198696, -0.26879641, -0.57914154, -0.27424283, -0.2720338 ,\n",
       "        -0.46074726, -0.27119308, -0.27145414, -0.87892914, -0.27230391,\n",
       "        -0.27159717, -0.61432481, -0.27233484, -0.26924041, -0.69701796,\n",
       "        -0.27167566, -0.27740689, -0.56474538, -0.27266619, -0.27307167,\n",
       "        -0.58814822, -0.27265782, -0.26863629, -0.54541365, -0.26851123,\n",
       "        -0.27285394, -0.83711268, -0.27650794, -0.27204353, -0.55931198,\n",
       "        -0.2678012 , -0.27097057, -0.62990145, -0.269075  , -0.27237559,\n",
       "        -0.56722181, -0.26953105, -0.26997488, -0.27028929, -0.27034458,\n",
       "        -0.27085479, -0.27074136, -0.2710775 , -0.2713402 , -0.27139192,\n",
       "        -0.27127929, -0.27119433, -0.27129673, -0.2712114 , -0.27061598,\n",
       "        -0.27079844, -0.27090566, -0.27079218, -0.27145398, -0.27117459,\n",
       "        -0.27134009, -0.27130965, -0.27117589, -0.27120528, -0.27116151,\n",
       "        -0.27135065, -0.27108915, -0.27089805, -0.27097235, -0.27096687,\n",
       "        -0.27067917, -0.27119362, -0.27125261, -0.27108198, -0.27128325,\n",
       "        -0.27120607, -0.27120526, -0.27102099, -0.27041341, -0.27091556,\n",
       "        -0.27063842, -0.27082731, -0.27081063, -0.2707871 , -0.27100859,\n",
       "        -0.27104145, -0.27115917, -0.27089593, -0.27098053, -0.27097277,\n",
       "        -0.26188564, -0.26138746, -0.2598802 , -0.26066613, -0.26428996,\n",
       "        -0.2600349 , -0.2617861 , -0.2675001 , -0.2572054 , -0.26176547,\n",
       "        -0.2628025 , -0.26123333, -0.26200084, -0.26005936, -0.26158849,\n",
       "        -0.25740217, -0.26078789, -0.2601408 , -0.26030446, -0.25916307,\n",
       "        -0.25798628, -0.25897573, -0.26124749, -0.26361038, -0.26158824,\n",
       "        -0.26170014, -0.26449834, -0.25814853, -0.26107555, -0.26193146,\n",
       "        -0.26170359, -0.25935306, -0.25867443, -0.26229004, -0.26205609,\n",
       "        -0.26125256, -0.25985362, -0.26075367, -0.26257345, -0.26050809,\n",
       "        -0.26102063, -0.2597758 , -0.26078107, -0.26097026, -0.25939038,\n",
       "        -0.26029568, -0.2608523 , -0.2595391 ]),\n",
       " 'split2_test_neg_log_loss': array([-0.28031868, -0.2624086 , -0.28230719, -0.30280313, -0.3042027 ,\n",
       "        -0.29192955, -0.34104076, -0.31953682, -0.40187602, -0.43920151,\n",
       "        -0.40190825, -0.42993551, -0.28446429, -0.28137935, -0.28899233,\n",
       "        -0.28170144, -0.2950815 , -0.29103936, -0.31419574, -0.31245408,\n",
       "        -0.3037727 , -0.37667901, -0.34753523, -0.37463846, -0.28567036,\n",
       "        -0.26485449, -0.28465758, -0.29870403, -0.28025501, -0.29365867,\n",
       "        -0.3108471 , -0.3132945 , -0.31463382, -0.37198115, -0.3492675 ,\n",
       "        -0.40617196, -0.26590088, -0.27713413, -0.26381419, -0.2779622 ,\n",
       "        -0.28470789, -0.28585805, -0.2695665 , -0.29856464, -0.29374583,\n",
       "        -0.31440366, -0.34126461, -0.32728443, -0.28498007, -0.29163034,\n",
       "        -0.2929034 , -0.30310299, -0.29457097, -0.30065946, -0.31857194,\n",
       "        -0.42053951, -0.37096891, -0.41333336, -0.51904397, -0.50233947,\n",
       "        -0.27652056, -0.28797421, -0.26751216, -0.31294466, -0.29744967,\n",
       "        -0.30933548, -0.35349394, -0.46991284, -0.34404043, -0.41541913,\n",
       "        -0.44483433, -0.69146885, -0.29213804, -0.28006198, -0.2839021 ,\n",
       "        -0.28158783, -0.29543531, -0.30928683, -0.35248031, -0.32156471,\n",
       "        -0.31713361, -0.43498808, -0.38062721, -0.42292104, -0.27592292,\n",
       "        -0.27066439, -0.29210174, -0.28945887, -0.28805073, -0.30563946,\n",
       "        -0.32169227, -0.31752792, -0.30586167, -0.37394252, -0.33394912,\n",
       "        -0.35835716, -0.27220295, -0.57693116, -0.26832922, -0.27390143,\n",
       "        -0.50388104, -0.26966108, -0.26777636, -0.44020373, -0.26888941,\n",
       "        -0.27215033, -0.50829958, -0.26853908, -0.27304836, -0.81714457,\n",
       "        -0.27056121, -0.2699839 , -0.44262765, -0.27047382, -0.26824094,\n",
       "        -0.55987292, -0.27009666, -0.26627248, -0.56950122, -0.27155239,\n",
       "        -0.26965934, -0.45752126, -0.26987581, -0.26973334, -0.75379206,\n",
       "        -0.26734303, -0.26786938, -0.50800172, -0.27030197, -0.26956916,\n",
       "        -0.38648655, -0.26793366, -0.27033203, -0.81591196, -0.26840706,\n",
       "        -0.26877623, -0.45827615, -0.27086786, -0.27077685, -0.55231403,\n",
       "        -0.2717367 , -0.27102914, -0.39377379, -0.27228298, -0.27415492,\n",
       "        -0.56221912, -0.26908579, -0.27389277, -0.67781081, -0.27255895,\n",
       "        -0.27478398, -0.95850385, -0.26800875, -0.27601543, -0.5868264 ,\n",
       "        -0.27124413, -0.27672033, -1.00703546, -0.27334261, -0.2709909 ,\n",
       "        -0.62497187, -0.27301978, -0.27267369, -0.91277779, -0.27632134,\n",
       "        -0.27367755, -0.91928996, -0.26869174, -0.27885831, -0.71605848,\n",
       "        -0.27486794, -0.27175038, -0.87212488, -0.27172032, -0.26885953,\n",
       "        -0.67763981, -0.27491071, -0.27296092, -0.69118538, -0.27280318,\n",
       "        -0.27172302, -1.15019327, -0.27573688, -0.27091768, -0.48234984,\n",
       "        -0.27980804, -0.26838774, -0.72112957, -0.26913982, -0.27014878,\n",
       "        -0.6782838 , -0.27259729, -0.27037277, -0.26983815, -0.26992004,\n",
       "        -0.26963809, -0.26982821, -0.27006914, -0.27009957, -0.27002348,\n",
       "        -0.27001526, -0.2699026 , -0.27025122, -0.26986993, -0.2699214 ,\n",
       "        -0.26901533, -0.26958966, -0.26957626, -0.27022812, -0.26983095,\n",
       "        -0.26985189, -0.26998565, -0.26974624, -0.27021539, -0.27009359,\n",
       "        -0.26992264, -0.27001553, -0.26984428, -0.26943814, -0.26990733,\n",
       "        -0.2698989 , -0.26997574, -0.27006929, -0.27024081, -0.27005188,\n",
       "        -0.26989785, -0.26995019, -0.27007734, -0.26981105, -0.26995512,\n",
       "        -0.26997836, -0.2697156 , -0.26975117, -0.2700497 , -0.27006155,\n",
       "        -0.2699882 , -0.26992798, -0.27007777, -0.26994112, -0.27003382,\n",
       "        -0.26174319, -0.26543669, -0.26473763, -0.26345755, -0.26205963,\n",
       "        -0.26700278, -0.26901525, -0.26402113, -0.26682039, -0.26402359,\n",
       "        -0.26499815, -0.26824629, -0.26233015, -0.26135222, -0.26181712,\n",
       "        -0.26726054, -0.26475938, -0.26640421, -0.26568849, -0.26878089,\n",
       "        -0.26250848, -0.27150467, -0.26752465, -0.26850999, -0.26305669,\n",
       "        -0.26326709, -0.26222752, -0.26141928, -0.26266006, -0.26306062,\n",
       "        -0.27004127, -0.26850864, -0.2669879 , -0.2723026 , -0.26653306,\n",
       "        -0.27437348, -0.26265129, -0.26138274, -0.26467313, -0.26148056,\n",
       "        -0.26132865, -0.26409356, -0.26465956, -0.26434183, -0.26463268,\n",
       "        -0.26521071, -0.26130918, -0.26489905]),\n",
       " 'split3_test_neg_log_loss': array([-0.26884235, -0.27290725, -0.26848556, -0.28881427, -0.27027887,\n",
       "        -0.28348096, -0.31037143, -0.30808487, -0.34270018, -0.39816563,\n",
       "        -0.37364954, -0.35766404, -0.26608594, -0.2671348 , -0.25921275,\n",
       "        -0.29745747, -0.28361918, -0.28272814, -0.27114635, -0.32105149,\n",
       "        -0.2856352 , -0.31430441, -0.36152155, -0.36690414, -0.27205711,\n",
       "        -0.26784643, -0.25372946, -0.27122333, -0.25793499, -0.27585222,\n",
       "        -0.3051324 , -0.30693519, -0.26316304, -0.33317672, -0.34264675,\n",
       "        -0.26911138, -0.25252392, -0.26368832, -0.26080017, -0.25746993,\n",
       "        -0.28047415, -0.26272606, -0.29702936, -0.27291609, -0.28942167,\n",
       "        -0.31466935, -0.29695048, -0.2690741 , -0.26588293, -0.27157435,\n",
       "        -0.26568935, -0.28034273, -0.27349191, -0.29684028, -0.31110443,\n",
       "        -0.30728426, -0.31207924, -0.36865229, -0.35701406, -0.42379194,\n",
       "        -0.2652742 , -0.26619861, -0.27213242, -0.29966396, -0.27045289,\n",
       "        -0.36237001, -0.31281583, -0.32211661, -0.32212996, -0.31345802,\n",
       "        -0.43965081, -0.38409156, -0.27210161, -0.27283388, -0.25291007,\n",
       "        -0.30481478, -0.28213497, -0.28311751, -0.30761754, -0.30566517,\n",
       "        -0.28365376, -0.36952823, -0.34769007, -0.40950053, -0.27448813,\n",
       "        -0.26491158, -0.261648  , -0.28239615, -0.28430049, -0.26430169,\n",
       "        -0.28630662, -0.26240636, -0.28639728, -0.37770245, -0.30443191,\n",
       "        -0.32662965, -0.27080731, -0.94602521, -0.26843843, -0.26766177,\n",
       "        -0.68871456, -0.26999343, -0.2696753 , -0.52606321, -0.27000939,\n",
       "        -0.26996533, -0.41637698, -0.26703218, -0.26911984, -0.51256503,\n",
       "        -0.27139662, -0.26929516, -0.40100037, -0.27010748, -0.26870466,\n",
       "        -0.47731854, -0.27139786, -0.27032789, -0.48764152, -0.26973312,\n",
       "        -0.27074285, -0.58694196, -0.26851556, -0.27002851, -0.54786896,\n",
       "        -0.27053816, -0.26741718, -0.53027409, -0.26745456, -0.26805882,\n",
       "        -0.50535059, -0.27083219, -0.2706125 , -0.46167019, -0.26946937,\n",
       "        -0.26994735, -0.5498208 , -0.26786136, -0.27024534, -0.43365106,\n",
       "        -0.2691355 , -0.26746357, -0.68278647, -0.27058193, -0.27432053,\n",
       "        -0.61977488, -0.26991204, -0.27045719, -0.81755925, -0.26671533,\n",
       "        -0.27099651, -0.51670117, -0.27142013, -0.26936751, -0.56282653,\n",
       "        -0.26855897, -0.26756703, -0.36161769, -0.27241996, -0.27169553,\n",
       "        -0.5990072 , -0.27004177, -0.27378312, -0.55161341, -0.26986614,\n",
       "        -0.26754507, -0.43999167, -0.26382245, -0.27529395, -1.08928846,\n",
       "        -0.27317025, -0.2668188 , -0.98722622, -0.27420974, -0.26623792,\n",
       "        -0.75281024, -0.26792586, -0.26606712, -0.5373954 , -0.27265423,\n",
       "        -0.27078077, -0.71533247, -0.2760332 , -0.26928279, -0.76325832,\n",
       "        -0.27064224, -0.26693207, -0.51159072, -0.27066724, -0.26881634,\n",
       "        -0.70441188, -0.26694352, -0.2688535 , -0.26829495, -0.26891173,\n",
       "        -0.26894624, -0.26863217, -0.26887266, -0.26881773, -0.26874354,\n",
       "        -0.26897488, -0.26908104, -0.26898286, -0.26909486, -0.26892211,\n",
       "        -0.2683221 , -0.26864038, -0.268817  , -0.26868546, -0.26899731,\n",
       "        -0.26866451, -0.26895246, -0.26874663, -0.26888014, -0.26900907,\n",
       "        -0.26899794, -0.26883411, -0.26867055, -0.26854686, -0.26897438,\n",
       "        -0.26880991, -0.26874955, -0.26873926, -0.2685888 , -0.2689256 ,\n",
       "        -0.26888576, -0.26882647, -0.26899521, -0.26891468, -0.26902415,\n",
       "        -0.26882619, -0.2689447 , -0.26933021, -0.26862151, -0.26913058,\n",
       "        -0.26894806, -0.26904526, -0.26892528, -0.26899169, -0.26882479,\n",
       "        -0.25857334, -0.2604005 , -0.2578702 , -0.25693845, -0.25659733,\n",
       "        -0.25314446, -0.25642441, -0.25290149, -0.25331091, -0.25224823,\n",
       "        -0.25194558, -0.25021798, -0.25924878, -0.25762904, -0.25579517,\n",
       "        -0.25519089, -0.25585502, -0.25331147, -0.2532184 , -0.25279019,\n",
       "        -0.2572232 , -0.25166143, -0.2529729 , -0.24994957, -0.25838941,\n",
       "        -0.25772163, -0.2577492 , -0.25800844, -0.25414861, -0.25499745,\n",
       "        -0.25731129, -0.25596836, -0.2553765 , -0.25431382, -0.25214608,\n",
       "        -0.25541284, -0.25695564, -0.2568075 , -0.25896755, -0.25524221,\n",
       "        -0.25694614, -0.25673296, -0.25632946, -0.25795925, -0.2563296 ,\n",
       "        -0.2527347 , -0.25417718, -0.25474009]),\n",
       " 'split4_test_neg_log_loss': array([-0.50530554, -0.49589934, -0.27708496, -0.31421961, -0.30759974,\n",
       "        -0.55950968, -0.31879956, -0.31247801, -0.51942709, -0.50620182,\n",
       "        -0.26990984, -0.43515306, -0.26916359, -0.49641004, -0.28830911,\n",
       "        -0.30299051, -0.30856215, -0.32422531, -0.33517572, -0.34845158,\n",
       "        -0.3336483 , -0.41396027, -0.4056988 , -0.364191  , -0.29147254,\n",
       "        -0.26703145, -0.28058718, -0.29191485, -0.30938647, -0.33964838,\n",
       "        -0.33236777, -0.37723008, -0.36429852, -0.39722316, -0.38090644,\n",
       "        -0.34722238, -0.28511814, -0.27884226, -0.27476569, -0.29912619,\n",
       "        -0.28473007, -0.29850067, -0.29969603, -0.32186198, -0.30736823,\n",
       "        -0.32489015, -0.34959757, -0.32658888, -0.28854325, -0.29408336,\n",
       "        -0.28339679, -0.30290697, -0.30815424, -0.29654778, -0.41562402,\n",
       "        -0.3384698 , -0.33290244, -0.45342969, -0.43156953, -0.40778678,\n",
       "        -0.28308342, -0.28080134, -0.27581872, -0.49199817, -0.31169959,\n",
       "        -0.39280439, -0.34146796, -0.33121995, -0.54055971, -0.37245559,\n",
       "        -0.38638623, -0.36164893, -0.27102641, -0.27645894, -0.38336353,\n",
       "        -0.31695946, -0.30565158, -0.33692878, -0.3745499 , -0.33337742,\n",
       "        -0.32163777, -0.40370112, -0.40872913, -0.37155636, -0.27870962,\n",
       "        -0.28294035, -0.28794816, -0.30965254, -0.30418352, -0.29339131,\n",
       "        -0.33014131, -0.33275057, -0.3323357 , -0.36722791, -0.38121921,\n",
       "        -0.36973749, -0.27008384, -0.61516433, -0.27155674, -0.27025325,\n",
       "        -0.56062647, -0.2703447 , -0.27012932, -0.47914532, -0.27023553,\n",
       "        -0.26906432, -0.38802774, -0.26971025, -0.27017134, -0.42053944,\n",
       "        -0.26847594, -0.27062392, -0.51872174, -0.26915779, -0.26913474,\n",
       "        -0.48809793, -0.27221564, -0.27060713, -0.44900886, -0.27047357,\n",
       "        -0.26924984, -0.48047268, -0.27081862, -0.27075439, -0.66744909,\n",
       "        -0.26894866, -0.26941384, -0.4346594 , -0.26903987, -0.26953563,\n",
       "        -0.42016671, -0.2683357 , -0.27035648, -0.86758554, -0.26987192,\n",
       "        -0.27235271, -0.57187305, -0.27157658, -0.26848118, -0.620075  ,\n",
       "        -0.27083514, -0.26866389, -0.4455783 , -0.26933517, -0.27328733,\n",
       "        -0.52929757, -0.27622516, -0.26853534, -0.72603531, -0.27405973,\n",
       "        -0.2708673 , -0.65088835, -0.2723389 , -0.27079379, -0.46312153,\n",
       "        -0.27166986, -0.27432799, -0.49177502, -0.27712234, -0.26996931,\n",
       "        -0.55578832, -0.27146004, -0.27165879, -0.53664337, -0.2683458 ,\n",
       "        -0.27260549, -0.65476731, -0.27111865, -0.27366831, -0.52073097,\n",
       "        -0.27187538, -0.27168381, -0.95213642, -0.27480701, -0.27291373,\n",
       "        -0.68606741, -0.26979513, -0.26636472, -0.49971518, -0.27121474,\n",
       "        -0.27365102, -0.74327433, -0.2699771 , -0.27324148, -0.54170147,\n",
       "        -0.27211938, -0.26821245, -0.49257377, -0.26671308, -0.26827745,\n",
       "        -0.7754054 , -0.26846892, -0.26973492, -0.26915162, -0.26940563,\n",
       "        -0.26951787, -0.26945334, -0.26975484, -0.26962176, -0.26965991,\n",
       "        -0.26955139, -0.26972889, -0.2697531 , -0.26972797, -0.26918195,\n",
       "        -0.26900301, -0.2694162 , -0.2695393 , -0.26957328, -0.2694717 ,\n",
       "        -0.26988653, -0.26979316, -0.26966668, -0.26967013, -0.26971998,\n",
       "        -0.26980156, -0.26914908, -0.26905384, -0.26964238, -0.26975201,\n",
       "        -0.26966142, -0.26940055, -0.26940787, -0.26951045, -0.26954544,\n",
       "        -0.26962344, -0.26968796, -0.26983446, -0.26916841, -0.26950059,\n",
       "        -0.26949142, -0.26963632, -0.26951956, -0.26948957, -0.26973116,\n",
       "        -0.2695231 , -0.26971492, -0.26978944, -0.26956665, -0.26964555,\n",
       "        -0.26477245, -0.26519936, -0.26375396, -0.2659501 , -0.27033108,\n",
       "        -0.26489243, -0.26973622, -0.26884197, -0.26655201, -0.27400646,\n",
       "        -0.26761933, -0.26186938, -0.26506517, -0.26417772, -0.26380835,\n",
       "        -0.26523465, -0.26501301, -0.26358066, -0.26445177, -0.26907854,\n",
       "        -0.2650982 , -0.27133952, -0.27217805, -0.26878146, -0.26493337,\n",
       "        -0.26428731, -0.26561801, -0.26401202, -0.26390344, -0.26327145,\n",
       "        -0.26897456, -0.27063691, -0.26519313, -0.27190819, -0.27239716,\n",
       "        -0.27351221, -0.2630391 , -0.2645717 , -0.26312373, -0.26289814,\n",
       "        -0.26443108, -0.26661448, -0.264928  , -0.26341975, -0.26536822,\n",
       "        -0.26445477, -0.26637929, -0.26334374]),\n",
       " 'mean_test_neg_log_loss': array([-0.32370533, -0.31856874, -0.27778083, -0.30251595, -0.30373255,\n",
       "        -0.35017154, -0.32463294, -0.31475145, -0.385209  , -0.42960652,\n",
       "        -0.36490301, -0.4151889 , -0.27357878, -0.31841289, -0.27946607,\n",
       "        -0.29682505, -0.29686593, -0.29876015, -0.30821321, -0.3346946 ,\n",
       "        -0.32404324, -0.36613648, -0.37582893, -0.37829168, -0.2775242 ,\n",
       "        -0.26851875, -0.2713968 , -0.28813792, -0.28473533, -0.29463781,\n",
       "        -0.3084438 , -0.32315691, -0.32092641, -0.363482  , -0.36532434,\n",
       "        -0.34620285, -0.26861635, -0.26939627, -0.26994681, -0.28177428,\n",
       "        -0.28200923, -0.28230531, -0.29659863, -0.305018  , -0.30062807,\n",
       "        -0.32280309, -0.3225932 , -0.32143054, -0.28089703, -0.28220624,\n",
       "        -0.28218743, -0.31061   , -0.30225837, -0.30529137, -0.38087582,\n",
       "        -0.42550384, -0.34112442, -0.41329893, -0.45201585, -0.47181789,\n",
       "        -0.27689143, -0.27487264, -0.27560594, -0.3413931 , -0.29586555,\n",
       "        -0.33559745, -0.33756818, -0.35929075, -0.37668546, -0.37877619,\n",
       "        -0.41158142, -0.44513297, -0.28133018, -0.27309182, -0.29300005,\n",
       "        -0.30024902, -0.29950936, -0.31070197, -0.33770758, -0.32355049,\n",
       "        -0.33031322, -0.3877441 , -0.39317238, -0.39593927, -0.2756171 ,\n",
       "        -0.27189101, -0.27817096, -0.29109558, -0.29135864, -0.28909549,\n",
       "        -0.31748633, -0.30488632, -0.31680398, -0.37003827, -0.35224077,\n",
       "        -0.35169863, -0.27021024, -0.65072447, -0.26895602, -0.27009349,\n",
       "        -0.56649559, -0.2693544 , -0.26894204, -0.51247808, -0.26879522,\n",
       "        -0.26968046, -0.44661936, -0.26856918, -0.26991406, -0.7002295 ,\n",
       "        -0.26941142, -0.26971682, -0.42532508, -0.26967364, -0.26829333,\n",
       "        -0.51109549, -0.27040068, -0.26876606, -0.481391  , -0.26951521,\n",
       "        -0.26939549, -0.51833628, -0.26951949, -0.26932817, -0.68098246,\n",
       "        -0.26867774, -0.26866145, -0.50153829, -0.26891139, -0.26863735,\n",
       "        -0.44501793, -0.26903497, -0.26983409, -0.6211058 , -0.26968056,\n",
       "        -0.26946532, -0.54368929, -0.26993649, -0.26941085, -0.52672465,\n",
       "        -0.26955842, -0.26896564, -0.4856512 , -0.26958868, -0.27258469,\n",
       "        -0.55720412, -0.27243524, -0.26957324, -0.68674007, -0.27077484,\n",
       "        -0.27076497, -0.79800243, -0.27065974, -0.27107994, -0.63548527,\n",
       "        -0.27047184, -0.27227967, -0.60241733, -0.27493152, -0.27169932,\n",
       "        -0.53203532, -0.27181253, -0.27225316, -0.68900709, -0.27122201,\n",
       "        -0.27112093, -0.676335  , -0.26946886, -0.27418985, -0.6999667 ,\n",
       "        -0.27336827, -0.27189271, -0.89294726, -0.27310227, -0.2704034 ,\n",
       "        -0.65788854, -0.27122631, -0.26804593, -0.56813738, -0.27098065,\n",
       "        -0.27206867, -0.80590213, -0.27415982, -0.27083627, -0.58560438,\n",
       "        -0.27221132, -0.26845635, -0.62862928, -0.2688225 , -0.26896819,\n",
       "        -0.66602262, -0.27033978, -0.26937131, -0.26909403, -0.26929144,\n",
       "        -0.26931135, -0.26924956, -0.26948135, -0.26950366, -0.2694958 ,\n",
       "        -0.26948442, -0.26949535, -0.26959633, -0.26951827, -0.26920027,\n",
       "        -0.26897487, -0.26913932, -0.26930443, -0.26954361, -0.2693995 ,\n",
       "        -0.26951562, -0.26953862, -0.26939467, -0.2695444 , -0.2695439 ,\n",
       "        -0.26952349, -0.26929008, -0.2692157 , -0.26925015, -0.2694748 ,\n",
       "        -0.26930214, -0.26939438, -0.26943119, -0.26940434, -0.26948257,\n",
       "        -0.26946659, -0.26943944, -0.26952667, -0.26917145, -0.26930651,\n",
       "        -0.26926462, -0.26932358, -0.26940503, -0.26926824, -0.26949252,\n",
       "        -0.26940007, -0.26947439, -0.26946242, -0.26940716, -0.26940329,\n",
       "        -0.26248851, -0.26320119, -0.26151528, -0.26189093, -0.26281028,\n",
       "        -0.2628271 , -0.2653036 , -0.26332851, -0.2629553 , -0.26403492,\n",
       "        -0.26296861, -0.26123614, -0.26213884, -0.26079098, -0.26200745,\n",
       "        -0.26140466, -0.26340951, -0.26168756, -0.26257814, -0.26443376,\n",
       "        -0.26243544, -0.26495513, -0.26517383, -0.26418773, -0.26228667,\n",
       "        -0.26219884, -0.26255398, -0.26062885, -0.261019  , -0.26192572,\n",
       "        -0.26434662, -0.26546858, -0.26293339, -0.26514087, -0.26412189,\n",
       "        -0.26711148, -0.26131132, -0.26124251, -0.26268621, -0.26032361,\n",
       "        -0.26163669, -0.26272743, -0.26215899, -0.26210118, -0.26190826,\n",
       "        -0.2617646 , -0.2622044 , -0.26092334]),\n",
       " 'std_test_neg_log_loss': array([0.09093365, 0.08895441, 0.00671292, 0.01368548, 0.01922572,\n",
       "        0.1050953 , 0.01179351, 0.01192872, 0.07212291, 0.04116647,\n",
       "        0.05351841, 0.0290388 , 0.00636051, 0.08911487, 0.01151033,\n",
       "        0.0077693 , 0.0084803 , 0.0147629 , 0.02072836, 0.01495188,\n",
       "        0.0254336 , 0.03812991, 0.02153903, 0.02234231, 0.00926548,\n",
       "        0.00359063, 0.01140985, 0.01003876, 0.01654843, 0.02328037,\n",
       "        0.01417835, 0.02918712, 0.03403932, 0.02106067, 0.01922575,\n",
       "        0.04398099, 0.01142517, 0.00764676, 0.00864101, 0.0166758 ,\n",
       "        0.00293633, 0.01228241, 0.01872776, 0.01788753, 0.00931724,\n",
       "        0.01124346, 0.02049747, 0.02719014, 0.00791926, 0.01004909,\n",
       "        0.00954325, 0.02282134, 0.01747227, 0.01690966, 0.07040143,\n",
       "        0.11494582, 0.02011884, 0.02922453, 0.05569876, 0.05153814,\n",
       "        0.00862263, 0.00867763, 0.00705866, 0.07548009, 0.01453325,\n",
       "        0.03649004, 0.01391421, 0.05556439, 0.08226617, 0.03792797,\n",
       "        0.02587297, 0.12384206, 0.00915972, 0.00891565, 0.04635468,\n",
       "        0.01275841, 0.01176178, 0.01997993, 0.02570133, 0.01639952,\n",
       "        0.03091786, 0.03006722, 0.02686159, 0.03289032, 0.00303041,\n",
       "        0.00700565, 0.01122488, 0.00955776, 0.00711652, 0.01464115,\n",
       "        0.01806181, 0.02376628, 0.02237023, 0.00928964, 0.02939771,\n",
       "        0.01793406, 0.00162501, 0.1514304 , 0.00159129, 0.00270072,\n",
       "        0.10580049, 0.00109382, 0.0011438 , 0.07344757, 0.001618  ,\n",
       "        0.00135816, 0.08117308, 0.0008842 , 0.0021049 , 0.20673157,\n",
       "        0.00172549, 0.00059375, 0.05711027, 0.00134508, 0.00098291,\n",
       "        0.02946405, 0.00155778, 0.00184371, 0.05549268, 0.00173034,\n",
       "        0.00077319, 0.04870428, 0.00105037, 0.00109427, 0.08457381,\n",
       "        0.0013411 , 0.00091389, 0.03920767, 0.0016037 , 0.00140119,\n",
       "        0.05691263, 0.00122576, 0.00115355, 0.18121868, 0.00194566,\n",
       "        0.00158974, 0.04426388, 0.00191987, 0.00092819, 0.06221726,\n",
       "        0.0014939 , 0.00136677, 0.11007702, 0.00187948, 0.00168817,\n",
       "        0.03330536, 0.00410139, 0.00241636, 0.08171345, 0.0025351 ,\n",
       "        0.00253778, 0.18048117, 0.00145243, 0.00283238, 0.12719029,\n",
       "        0.00144632, 0.00349773, 0.21696904, 0.00204244, 0.00126881,\n",
       "        0.07924191, 0.00122136, 0.00087404, 0.16946705, 0.00286649,\n",
       "        0.00212792, 0.15818945, 0.00306794, 0.0030943 , 0.21623864,\n",
       "        0.00147856, 0.00335328, 0.17820445, 0.00120175, 0.00258604,\n",
       "        0.06388004, 0.00239637, 0.00263261, 0.06524598, 0.00166522,\n",
       "        0.00104284, 0.19029175, 0.00251461, 0.00168783, 0.09471943,\n",
       "        0.00404864, 0.00135409, 0.1150221 , 0.0012736 , 0.00234658,\n",
       "        0.0736202 , 0.0026595 , 0.00088006, 0.00090101, 0.0008564 ,\n",
       "        0.00105777, 0.00107034, 0.00116244, 0.00124011, 0.00125189,\n",
       "        0.00120944, 0.00118203, 0.00121045, 0.00114599, 0.00109443,\n",
       "        0.00102859, 0.00123506, 0.000985  , 0.00126297, 0.00118538,\n",
       "        0.00119399, 0.00120858, 0.00117273, 0.0011732 , 0.00114077,\n",
       "        0.00124637, 0.00124146, 0.00110647, 0.00111418, 0.00106164,\n",
       "        0.00109658, 0.00118536, 0.00120387, 0.00122044, 0.00121653,\n",
       "        0.00115108, 0.00122252, 0.0011163 , 0.00096412, 0.00125166,\n",
       "        0.00110991, 0.00109578, 0.00103185, 0.00117475, 0.00115403,\n",
       "        0.00117231, 0.00119061, 0.00111445, 0.00113015, 0.00115999,\n",
       "        0.00246337, 0.00201338, 0.00250552, 0.00300874, 0.00451808,\n",
       "        0.00569407, 0.00534267, 0.00560366, 0.00658598, 0.00721505,\n",
       "        0.00578987, 0.00603906, 0.0018428 , 0.00211287, 0.00366755,\n",
       "        0.0045579 , 0.00490926, 0.00467672, 0.00548041, 0.00730189,\n",
       "        0.00451712, 0.00820143, 0.00727637, 0.00745181, 0.00222101,\n",
       "        0.00241273, 0.00269727, 0.0022781 , 0.00356244, 0.00376378,\n",
       "        0.00470624, 0.00661024, 0.005041  , 0.00667087, 0.00682992,\n",
       "        0.00748412, 0.00258519, 0.00257398, 0.001998  , 0.00265253,\n",
       "        0.00276543, 0.00387967, 0.00327285, 0.00237193, 0.00347883,\n",
       "        0.00493642, 0.00493358, 0.00355487]),\n",
       " 'rank_test_neg_log_loss': array([220, 213, 174, 200, 201, 231, 222, 209, 245, 254, 236, 251, 165,\n",
       "        212, 176, 193, 194, 195, 205, 224, 221, 238, 240, 242, 173,  52,\n",
       "        151, 185, 184, 190, 206, 218, 214, 235, 237, 230,  54,  90, 135,\n",
       "        179, 180, 183, 192, 203, 198, 217, 216, 215, 177, 182, 181, 207,\n",
       "        199, 204, 244, 253, 228, 250, 258, 259, 172, 168, 170, 229, 191,\n",
       "        225, 226, 234, 241, 243, 249, 256, 178, 162, 189, 197, 196, 208,\n",
       "        227, 219, 223, 246, 247, 248, 171, 154, 175, 187, 188, 186, 211,\n",
       "        202, 210, 239, 233, 232, 137, 277,  63, 136, 270,  85,  62, 264,\n",
       "         59, 129, 257,  53, 133, 285,  98, 131, 252, 128,  50, 263, 139,\n",
       "         58, 260, 114,  89, 265, 117,  84, 281,  57,  56, 262,  61,  55,\n",
       "        255,  67, 132, 274, 130, 102, 268, 134,  97, 266, 124,  64, 261,\n",
       "        126, 161, 269, 160, 125, 282, 144, 143, 286, 142, 147, 276, 141,\n",
       "        159, 273, 169, 152, 267, 153, 158, 283, 149, 148, 280, 104, 167,\n",
       "        284, 164, 155, 288, 163, 140, 278, 150,  49, 271, 146, 156, 287,\n",
       "        166, 145, 272, 157,  51, 275,  60,  65, 279, 138,  86,  68,  78,\n",
       "         82,  73, 107, 113, 112, 109, 111, 127, 116,  71,  66,  69,  80,\n",
       "        121,  91, 115, 120,  88, 123, 122, 118,  77,  72,  74, 106,  79,\n",
       "         87,  99,  94, 108, 103, 100, 119,  70,  81,  75,  83,  95,  76,\n",
       "        110,  92, 105, 101,  96,  93,  25,  35,  10,  14,  30,  31,  46,\n",
       "         36,  33,  38,  34,   6,  19,   3,  17,   9,  37,  12,  27,  42,\n",
       "         24,  43,  45,  40,  23,  21,  26,   2,   5,  16,  41,  47,  32,\n",
       "         44,  39,  48,   8,   7,  28,   1,  11,  29,  20,  18,  15,  13,\n",
       "         22,   4])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRIAL_2_MLP.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL TWO NEG LOG LOSS RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'tanh',\n",
       " 'classifier__alpha': 0.1,\n",
       " 'classifier__hidden_layer_sizes': (10,),\n",
       " 'classifier__learning_rate': 'constant',\n",
       " 'classifier__solver': 'adam'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL TWO NEG LOG LOSS RESULTS ---------------------------------------------\")\n",
    "TRIAL_2_MLP.cv_results_['rank_test_neg_log_loss']\n",
    "TRIAL_2_MLP.cv_results_['params'][ np.argmin(TRIAL_2_MLP.cv_results_['rank_test_neg_log_loss']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL TWO F1 RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.001,\n",
       " 'classifier__hidden_layer_sizes': (5,),\n",
       " 'classifier__learning_rate': 'adaptive',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL TWO F1 RESULTS ---------------------------------------------\")\n",
    "TRIAL_2_MLP.cv_results_['rank_test_f1_micro']\n",
    "TRIAL_2_MLP.cv_results_['params'][ np.argmin(TRIAL_2_MLP.cv_results_['rank_test_f1_micro']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL TWO ROC AUC RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.001,\n",
       " 'classifier__hidden_layer_sizes': (15,),\n",
       " 'classifier__learning_rate': 'constant',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL TWO ROC AUC RESULTS ---------------------------------------------\")\n",
    "TRIAL_2_MLP.cv_results_['rank_test_roc_auc_ovo']\n",
    "TRIAL_2_MLP.cv_results_['params'][ np.argmin(TRIAL_2_MLP.cv_results_['rank_test_roc_auc_ovo']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 2 MLP using best NEG LOG LOSS hyperparameters :0.9319\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 2 MLP using best F1 hyperparameters :0.93085\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 2 MLP using best ROC_AUC hyperparameters :0.9204\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best NEG LOG LOSS results to train on test set, \n",
    "p = TRIAL_2_MLP.cv_results_['params'][ np.argmin(TRIAL_2_MLP.cv_results_['rank_test_neg_log_loss']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train__neg_log_loss = pipe.predict(X_train)\n",
    "y_pred_test_neg_log_loss = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 2 MLP using best NEG LOG LOSS hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_neg_log_loss)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best F1 results to train on test set, LR_Trial 1\n",
    "p = TRIAL_2_MLP.cv_results_['params'][ np.argmin(TRIAL_2_MLP.cv_results_['rank_test_f1_micro']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train_roc_auc = pipe.predict(X_train)\n",
    "y_pred_test_f1 = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 2 MLP using best F1 hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_f1)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best ROC_AUC results to train on test set, LR_Trial 1\n",
    "p  = TRIAL_2_MLP.cv_results_['params'][ np.argmin(TRIAL_2_MLP.cv_results_['rank_test_roc_auc_ovo']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train_roc_auc = pipe.predict(X_train)\n",
    "y_pred_test_roc_auc = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 2 MLP using best ROC_AUC hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_roc_auc)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTI LAYERED PERCEPTRON TRIAL THREE ON POKER DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1902s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too slow (2.0558s.) Setting batch_size=1.\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done  74 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=-1)]: Done  95 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=-1)]: Done 128 tasks      | elapsed:    6.9s\n",
      "[Parallel(n_jobs=-1)]: Done 145 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=-1)]: Done 162 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=-1)]: Done 181 tasks      | elapsed:    9.5s\n",
      "[Parallel(n_jobs=-1)]: Done 200 tasks      | elapsed:   10.1s\n",
      "[Parallel(n_jobs=-1)]: Done 221 tasks      | elapsed:   11.2s\n",
      "[Parallel(n_jobs=-1)]: Done 242 tasks      | elapsed:   12.5s\n",
      "[Parallel(n_jobs=-1)]: Done 265 tasks      | elapsed:   13.6s\n",
      "[Parallel(n_jobs=-1)]: Done 288 tasks      | elapsed:   15.9s\n",
      "[Parallel(n_jobs=-1)]: Done 313 tasks      | elapsed:   16.9s\n",
      "[Parallel(n_jobs=-1)]: Done 338 tasks      | elapsed:   18.4s\n",
      "[Parallel(n_jobs=-1)]: Done 365 tasks      | elapsed:   20.3s\n",
      "[Parallel(n_jobs=-1)]: Done 392 tasks      | elapsed:   21.8s\n",
      "[Parallel(n_jobs=-1)]: Done 421 tasks      | elapsed:   23.7s\n",
      "[Parallel(n_jobs=-1)]: Done 450 tasks      | elapsed:   25.0s\n",
      "[Parallel(n_jobs=-1)]: Done 481 tasks      | elapsed:   27.8s\n",
      "[Parallel(n_jobs=-1)]: Done 512 tasks      | elapsed:   30.1s\n",
      "[Parallel(n_jobs=-1)]: Done 545 tasks      | elapsed:   32.4s\n",
      "[Parallel(n_jobs=-1)]: Done 578 tasks      | elapsed:   34.6s\n",
      "[Parallel(n_jobs=-1)]: Done 613 tasks      | elapsed:   37.5s\n",
      "[Parallel(n_jobs=-1)]: Done 648 tasks      | elapsed:   40.3s\n",
      "[Parallel(n_jobs=-1)]: Done 685 tasks      | elapsed:   43.0s\n",
      "[Parallel(n_jobs=-1)]: Done 722 tasks      | elapsed:   46.3s\n",
      "[Parallel(n_jobs=-1)]: Done 761 tasks      | elapsed:   49.7s\n",
      "[Parallel(n_jobs=-1)]: Done 800 tasks      | elapsed:   52.9s\n",
      "[Parallel(n_jobs=-1)]: Done 841 tasks      | elapsed:   56.2s\n",
      "[Parallel(n_jobs=-1)]: Done 882 tasks      | elapsed:   59.3s\n",
      "[Parallel(n_jobs=-1)]: Done 925 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 968 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1013 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1058 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1105 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1152 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1201 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1250 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1301 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1352 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1405 tasks      | elapsed:  1.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL THREE RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1440 out of 1440 | elapsed:  1.9min finished\n"
     ]
    }
   ],
   "source": [
    "ITERATIONS = 1\n",
    "for i in range(1):\n",
    "    #For reach \"trial\", randomly select 5000 data points\n",
    "    random5000DataPoints = pokerData.sample(n = 5000, replace = True)\n",
    "    #From that 5000, create x set and y set\n",
    "    xSet = random5000DataPoints.iloc[:,0:9].values\n",
    "    ySet = random5000DataPoints.iloc[:,10].values\n",
    "    # Run GridSearch to find best hyperparameters per 5000 data points \n",
    "    \n",
    "    pipe = Pipeline([('std', StandardScaler()),('classifier', MLPClassifier(max_iter = 250))])\n",
    "\n",
    "    search_space = [\n",
    "                {\n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                {\n",
    "                 \n",
    "                  \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['sgd'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                \n",
    "                {\n",
    "                \n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['adam'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                ]\n",
    "    \n",
    "    #'precision_micro','roc_auc_ovo','jaccard','neg_log_loss'] JACCARD DOES NOT WORK FOR NON BINARY?\n",
    "    runHyperParamSearch = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=5), \n",
    "                          scoring=['recall_micro','f1_micro','roc_auc_ovo','neg_log_loss'], \n",
    "                          refit=False, verbose=10, n_jobs = -1)\n",
    "    TRIAL_3_MLP = runHyperParamSearch.fit(xSet, ySet)\n",
    "    print(\"---------------------------------TRIAL THREE RESULTS ---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.2474133 , 0.29675632, 0.2919518 , 0.67328048, 0.67368169,\n",
       "        0.66567345, 1.14308391, 1.27049313, 1.31713305, 1.45214815,\n",
       "        1.55113297, 1.47266703, 0.40414839, 0.39473815, 0.37592282,\n",
       "        0.59621248, 0.61112747, 0.61452785, 1.05650916, 0.98444753,\n",
       "        0.92609935, 1.1975204 , 1.18952346, 1.37878575, 0.32457843,\n",
       "        0.25902276, 0.26873159, 0.48992276, 0.57139325, 0.46820416,\n",
       "        0.90267806, 0.87505131, 0.92639613, 1.13787942, 1.00156269,\n",
       "        1.29040723, 0.38002491, 0.26973152, 0.26412501, 0.51394234,\n",
       "        0.50633588, 0.59050989, 1.00556569, 0.82541127, 0.98234267,\n",
       "        1.14098158, 1.09574347, 1.35736775, 0.5792963 , 0.40805182,\n",
       "        0.3932364 , 0.71241322, 0.77686925, 0.78797784, 1.13257527,\n",
       "        1.24146862, 1.16960769, 1.80685229, 1.7975441 , 1.74490066,\n",
       "        0.48361626, 0.43677616, 0.31537137, 0.61232562, 0.73002934,\n",
       "        0.55778117, 1.0070682 , 1.2176465 , 1.19883084, 1.65782552,\n",
       "        1.77922988, 1.32083549, 0.41908607, 0.37792406, 0.29825726,\n",
       "        0.59301057, 0.5819005 , 0.5831028 , 1.14658499, 1.12436738,\n",
       "        1.22405343, 1.67143912, 1.46555963, 1.68845253, 0.28024006,\n",
       "        0.33558831, 0.28764811, 0.58159924, 0.6663722 , 0.45078826,\n",
       "        1.18682098, 1.26999207, 1.26759038, 1.68915253, 1.717977  ,\n",
       "        1.52200985, 0.86954727, 1.4540503 , 1.26058407, 0.56478605,\n",
       "        1.88432102, 1.09424095, 0.52785392, 2.25844193, 1.10114722,\n",
       "        0.50203209, 1.53802276, 1.03258801, 0.65045972, 0.94651384,\n",
       "        1.18802204, 0.53626199, 1.91024294, 1.02868485, 0.44218078,\n",
       "        1.72568431, 1.09654317, 0.4930243 , 2.628861  , 1.2345612 ,\n",
       "        0.72982769, 0.95181818, 1.22935929, 0.60492024, 1.956883  ,\n",
       "        1.13927951, 0.52795458, 2.34151354, 1.11666045, 0.49172297,\n",
       "        1.75611072, 1.04329715, 0.69309649, 1.86940727, 1.13867927,\n",
       "        0.61683054, 1.65892701, 1.16930594, 0.54126611, 2.3374105 ,\n",
       "        1.20553699, 0.49752789, 2.7366539 , 1.17941446, 0.98734941,\n",
       "        1.45495138, 1.33464775, 0.78197212, 2.00712624, 1.27789879,\n",
       "        0.78287296, 1.41361613, 1.49718828, 0.95552177, 2.51866665,\n",
       "        1.63911014, 0.93040066, 1.0404954 , 1.52120786, 0.90768075,\n",
       "        1.52921515, 1.45415068, 0.88406076, 1.72438316, 1.54372759,\n",
       "        0.95852437, 1.08643446, 1.40210614, 0.83051424, 1.04139614,\n",
       "        1.28790798, 0.77086339, 0.85393438, 1.32844243, 0.74143763,\n",
       "        1.83737993, 1.49268427, 0.83411803, 1.87901597, 1.38509121,\n",
       "        0.78147221, 0.65526381, 1.28960934, 0.70800862, 1.54422817,\n",
       "        1.32523909, 0.81560121, 1.88702316, 1.43893752, 0.74694324,\n",
       "        1.76231542, 1.39820237, 0.84782934, 0.73593302, 0.66126866,\n",
       "        0.52695313, 0.64335341, 0.56548681, 0.56058211, 0.44688487,\n",
       "        0.59321046, 0.5097383 , 0.45609217, 0.50813694, 0.70800905,\n",
       "        0.80198979, 0.75164666, 0.58029885, 0.52875519, 0.57899833,\n",
       "        0.55828013, 0.55447702, 0.53065643, 0.46059618, 0.51524382,\n",
       "        0.44458241, 0.88596163, 0.80349078, 0.72752562, 0.53586111,\n",
       "        0.55097389, 0.63424559, 0.52435112, 0.50423369, 0.55017366,\n",
       "        0.47450824, 0.48111362, 0.48371625, 0.66797404, 0.77506647,\n",
       "        0.8376205 , 0.53846345, 0.58760562, 0.60992517, 0.64555559,\n",
       "        0.53165755, 0.55797954, 0.56458511, 0.51103911, 0.54526906,\n",
       "        1.12416677, 1.57005033, 1.12867088, 2.18698144, 1.98931069,\n",
       "        2.51456256, 3.00388365, 3.21746716, 3.32656116, 3.34497628,\n",
       "        3.21396389, 3.2968348 , 0.9400085 , 1.36006961, 1.35036149,\n",
       "        2.34851975, 1.86360264, 2.16165924, 3.00158186, 3.0806499 ,\n",
       "        3.06083241, 3.40983233, 3.45917511, 3.62161403, 1.51980705,\n",
       "        1.27829909, 1.15349288, 2.67760291, 2.41707873, 2.15405259,\n",
       "        3.39792209, 3.44456239, 3.52953444, 3.65834618, 3.49300423,\n",
       "        3.47949152, 1.43983765, 1.30532246, 1.24807301, 1.97639966,\n",
       "        1.39379878, 1.63530674, 2.37324147, 2.09720387, 2.48673897,\n",
       "        2.49764805, 2.09550209, 2.41988125]),\n",
       " 'std_fit_time': array([0.05830875, 0.06112891, 0.08165684, 0.06527873, 0.08914948,\n",
       "        0.07660536, 0.04345955, 0.08127421, 0.15737466, 0.06552711,\n",
       "        0.11787465, 0.11531934, 0.05747116, 0.10758376, 0.10588623,\n",
       "        0.07158032, 0.0897064 , 0.07300146, 0.13663688, 0.10390752,\n",
       "        0.44149508, 0.55852446, 0.56468318, 0.06417765, 0.06405533,\n",
       "        0.03064569, 0.07601996, 0.07819467, 0.0927593 , 0.05232018,\n",
       "        0.09304829, 0.2070309 , 0.10994693, 0.53722025, 0.47369826,\n",
       "        0.17204537, 0.05354939, 0.05653605, 0.05044037, 0.03223858,\n",
       "        0.07334102, 0.08115575, 0.14180607, 0.45809626, 0.16634426,\n",
       "        0.52961223, 0.50256684, 0.17554569, 0.16641026, 0.10633902,\n",
       "        0.04748641, 0.13941042, 0.14823479, 0.08147216, 0.14467864,\n",
       "        0.13424876, 0.15153456, 0.24133822, 0.23374736, 0.14134663,\n",
       "        0.17189542, 0.15875191, 0.05506876, 0.13704452, 0.15870145,\n",
       "        0.06956826, 0.11473383, 0.24888852, 0.15491988, 0.21956766,\n",
       "        0.27383765, 0.1682928 , 0.09310629, 0.14366486, 0.0652264 ,\n",
       "        0.04719812, 0.17212481, 0.08391371, 0.21860548, 0.19259122,\n",
       "        0.18417858, 0.19796301, 0.31171994, 0.06663316, 0.07888638,\n",
       "        0.12573406, 0.05323869, 0.06528037, 0.19778227, 0.07577388,\n",
       "        0.249091  , 0.29911001, 0.22826651, 0.29146246, 0.30877495,\n",
       "        0.18322623, 0.09434219, 0.80420603, 0.124357  , 0.05977712,\n",
       "        1.14496829, 0.0582958 , 0.05979859, 0.67414007, 0.07932308,\n",
       "        0.03873397, 0.83781928, 0.02093099, 0.08869682, 0.56747384,\n",
       "        0.06672345, 0.05581491, 0.71321935, 0.0330866 , 0.0195913 ,\n",
       "        0.82122281, 0.08401448, 0.06387472, 0.62249874, 0.03320101,\n",
       "        0.06624967, 0.80288599, 0.12776919, 0.02123937, 0.70836842,\n",
       "        0.0803318 , 0.06112679, 0.60020779, 0.11178274, 0.0629372 ,\n",
       "        0.753778  , 0.08161454, 0.05470645, 0.81487033, 0.05816778,\n",
       "        0.05580776, 0.58031827, 0.10441025, 0.0532749 , 0.5013215 ,\n",
       "        0.11376476, 0.01552923, 0.33217256, 0.08424201, 0.1643809 ,\n",
       "        0.55420019, 0.03695195, 0.06436947, 0.79915538, 0.05318136,\n",
       "        0.05230468, 1.03417003, 0.12154582, 0.04659826, 0.84463436,\n",
       "        0.05066827, 0.06912524, 0.5513376 , 0.14417028, 0.10545227,\n",
       "        0.95132136, 0.17179841, 0.14715054, 0.69588515, 0.11193416,\n",
       "        0.11267498, 1.00543249, 0.02720902, 0.03776891, 0.7169369 ,\n",
       "        0.06432086, 0.08910147, 0.63254685, 0.0887597 , 0.04249768,\n",
       "        0.62329002, 0.06636182, 0.05852806, 0.78195378, 0.06783772,\n",
       "        0.06953862, 0.3271471 , 0.06972012, 0.07485663, 0.8889573 ,\n",
       "        0.04968105, 0.06162469, 0.83349193, 0.10340607, 0.03860758,\n",
       "        1.00801155, 0.02277909, 0.07944205, 0.11002394, 0.20103235,\n",
       "        0.07665495, 0.07659457, 0.03752386, 0.05588754, 0.13007559,\n",
       "        0.12943491, 0.06402462, 0.04045096, 0.05250187, 0.15644351,\n",
       "        0.15245272, 0.10626946, 0.12470818, 0.08665106, 0.11123845,\n",
       "        0.09052995, 0.09425457, 0.05302929, 0.02427914, 0.07259775,\n",
       "        0.06045964, 0.16561471, 0.1143846 , 0.10983107, 0.08900094,\n",
       "        0.08396608, 0.06507737, 0.09744064, 0.0783727 , 0.06062376,\n",
       "        0.04856658, 0.05141596, 0.06311819, 0.04718748, 0.23986939,\n",
       "        0.10365541, 0.12502251, 0.09359102, 0.06930348, 0.06756514,\n",
       "        0.09302115, 0.12265089, 0.08434298, 0.03544741, 0.04089692,\n",
       "        0.11535187, 0.36188164, 0.19553098, 0.52452044, 0.65584358,\n",
       "        0.34098959, 0.03077623, 0.11413818, 0.14629535, 0.16242658,\n",
       "        0.02563583, 0.08740727, 0.30831743, 0.38030868, 0.12909841,\n",
       "        0.34858833, 0.62900706, 0.55301525, 0.03694393, 0.01773486,\n",
       "        0.03479544, 0.1814883 , 0.16082517, 0.27433881, 0.60044312,\n",
       "        0.36117323, 0.35600539, 0.58532715, 0.562569  , 0.59938009,\n",
       "        0.89552331, 0.30398091, 0.20749267, 0.26683528, 0.24348353,\n",
       "        0.20583713, 0.21280789, 0.22570371, 0.08275091, 0.62605298,\n",
       "        0.26850271, 0.31153554, 0.65248274, 0.65877406, 0.6996098 ,\n",
       "        0.51493007, 0.6598547 , 0.3071375 ]),\n",
       " 'mean_score_time': array([0.00780768, 0.00740647, 0.0070056 , 0.00790715, 0.00850625,\n",
       "        0.00830665, 0.01090946, 0.01301031, 0.01190906, 0.01230993,\n",
       "        0.00950785, 0.01020846, 0.0106091 , 0.01151109, 0.01391268,\n",
       "        0.00950851, 0.00870681, 0.00910916, 0.0097105 , 0.00960765,\n",
       "        0.01471105, 0.0093081 , 0.01100917, 0.00960793, 0.00790749,\n",
       "        0.00850697, 0.00760679, 0.00780635, 0.00820656, 0.00910645,\n",
       "        0.00940776, 0.01040916, 0.00970788, 0.00960765, 0.00950718,\n",
       "        0.00850787, 0.0086091 , 0.00780625, 0.00960822, 0.00790734,\n",
       "        0.01060939, 0.01190939, 0.01070929, 0.00910788, 0.01151085,\n",
       "        0.00900822, 0.01060891, 0.00850677, 0.01291089, 0.00790758,\n",
       "        0.00780683, 0.00850749, 0.00870919, 0.01030841, 0.01010942,\n",
       "        0.01110888, 0.01070805, 0.01301255, 0.0107089 , 0.01000757,\n",
       "        0.00960808, 0.00770636, 0.00800667, 0.00830822, 0.00910816,\n",
       "        0.00950937, 0.01330976, 0.01191158, 0.0117105 , 0.0138123 ,\n",
       "        0.01051002, 0.00940952, 0.00978241, 0.00950837, 0.007306  ,\n",
       "        0.00800633, 0.00830774, 0.01080894, 0.01100969, 0.01020842,\n",
       "        0.01040764, 0.0112083 , 0.00970745, 0.01120882, 0.00840697,\n",
       "        0.00950909, 0.0105083 , 0.00910807, 0.00830741, 0.01070895,\n",
       "        0.00930781, 0.01110883, 0.01341295, 0.0099082 , 0.00930786,\n",
       "        0.00860705, 0.00790763, 0.00670543, 0.00660539, 0.00710559,\n",
       "        0.00730596, 0.00640545, 0.00870757, 0.00690594, 0.00710564,\n",
       "        0.00860782, 0.00670605, 0.00700603, 0.0064054 , 0.00710621,\n",
       "        0.00630503, 0.00660515, 0.00820718, 0.00770655, 0.01050863,\n",
       "        0.00710616, 0.00730619, 0.00730591, 0.01231031, 0.00860786,\n",
       "        0.0076066 , 0.00610542, 0.00900488, 0.00700598, 0.00730615,\n",
       "        0.00690589, 0.00750613, 0.00750623, 0.00710602, 0.00740666,\n",
       "        0.00740614, 0.00700612, 0.00650544, 0.01050916, 0.00650592,\n",
       "        0.00650606, 0.00680599, 0.00780625, 0.0085072 , 0.00730629,\n",
       "        0.00750618, 0.00710583, 0.00700555, 0.00800705, 0.00730605,\n",
       "        0.00660563, 0.00640569, 0.00710611, 0.0082068 , 0.00680656,\n",
       "        0.00690613, 0.01090922, 0.00710592, 0.00800695, 0.00830636,\n",
       "        0.00830731, 0.0065053 , 0.00810671, 0.00680571, 0.00810652,\n",
       "        0.00790677, 0.0068059 , 0.009308  , 0.00750647, 0.00720677,\n",
       "        0.00730624, 0.00710597, 0.00700569, 0.00650597, 0.00650525,\n",
       "        0.00630541, 0.00700583, 0.00690632, 0.00680566, 0.00710611,\n",
       "        0.00740638, 0.00700598, 0.01000872, 0.00750618, 0.00740771,\n",
       "        0.00640545, 0.00660591, 0.00630584, 0.00670619, 0.00700622,\n",
       "        0.01571417, 0.00740623, 0.00770617, 0.00720611, 0.00720601,\n",
       "        0.00740576, 0.00740623, 0.00650539, 0.00650549, 0.00670576,\n",
       "        0.01010842, 0.00680585, 0.00650549, 0.00720601, 0.00700579,\n",
       "        0.00740671, 0.00730643, 0.00690627, 0.00710635, 0.00630536,\n",
       "        0.0063055 , 0.00640554, 0.00650616, 0.00640535, 0.00640554,\n",
       "        0.00700612, 0.00690584, 0.00710607, 0.00710597, 0.00690589,\n",
       "        0.00730648, 0.00650573, 0.00650582, 0.00620508, 0.00670567,\n",
       "        0.00690608, 0.00670567, 0.0067059 , 0.0071064 , 0.00670581,\n",
       "        0.00710573, 0.00730643, 0.00680552, 0.00630579, 0.00630541,\n",
       "        0.00630598, 0.008007  , 0.00660605, 0.00670581, 0.00760665,\n",
       "        0.00710597, 0.00710602, 0.00860753, 0.00730653, 0.00670571,\n",
       "        0.00630569, 0.00630527, 0.00640545, 0.00680523, 0.00720625,\n",
       "        0.00840721, 0.01030855, 0.00740595, 0.00750651, 0.00770659,\n",
       "        0.00750685, 0.00820699, 0.00720615, 0.00690579, 0.00670557,\n",
       "        0.0069056 , 0.00660558, 0.00670581, 0.00750594, 0.00750623,\n",
       "        0.00730605, 0.01421261, 0.00950823, 0.00860791, 0.00810733,\n",
       "        0.00650516, 0.00850682, 0.00710607, 0.00690608, 0.00740628,\n",
       "        0.00810709, 0.0085073 , 0.00890794, 0.00760646, 0.00820718,\n",
       "        0.0132113 , 0.00650563, 0.00660567, 0.0064054 , 0.007306  ,\n",
       "        0.00730643, 0.00690546, 0.00730639, 0.00810676, 0.0092082 ,\n",
       "        0.00690627, 0.0058053 , 0.00492964]),\n",
       " 'std_score_time': array([1.86289776e-03, 8.60541785e-04, 3.16734631e-04, 4.91636569e-04,\n",
       "        4.48304418e-04, 4.00914055e-04, 2.10907215e-03, 1.95082049e-03,\n",
       "        2.71304775e-03, 3.98633891e-03, 1.00011999e-03, 2.46354112e-03,\n",
       "        1.98519160e-03, 4.50934310e-03, 1.08368934e-02, 2.32565399e-03,\n",
       "        9.79717602e-04, 3.72611448e-04, 1.07844069e-03, 9.18036546e-04,\n",
       "        9.42450491e-03, 6.78769898e-04, 2.12272961e-03, 1.28162704e-03,\n",
       "        7.34820165e-04, 1.51816345e-03, 3.75176476e-04, 3.99918165e-04,\n",
       "        5.09164641e-04, 1.06797030e-03, 5.83154280e-04, 1.20147544e-03,\n",
       "        4.00926384e-04, 1.02094693e-03, 6.31053455e-04, 6.34030981e-04,\n",
       "        3.75046214e-04, 2.45340313e-04, 2.94108568e-03, 3.73705723e-04,\n",
       "        5.00749222e-03, 5.59464635e-03, 1.39968828e-03, 7.99742952e-04,\n",
       "        1.92555577e-03, 3.16055546e-04, 5.01731844e-03, 8.37256707e-04,\n",
       "        1.49796618e-03, 8.00276662e-04, 6.78783538e-04, 7.07461905e-04,\n",
       "        8.76410651e-04, 1.80732910e-03, 9.71174522e-04, 1.24084734e-03,\n",
       "        8.13960406e-04, 5.07337276e-03, 1.50485313e-03, 2.25842928e-03,\n",
       "        2.06146905e-03, 4.00925165e-04, 7.73247840e-04, 5.10520697e-04,\n",
       "        1.46511816e-03, 1.97583393e-03, 7.10953363e-03, 4.46715059e-03,\n",
       "        1.80701184e-03, 5.45315006e-03, 1.89976789e-03, 1.11392027e-03,\n",
       "        2.96919752e-03, 2.12349352e-03, 5.10838178e-04, 5.47989283e-04,\n",
       "        6.79064673e-04, 1.72133267e-03, 1.92591575e-03, 1.28941255e-03,\n",
       "        1.11439810e-03, 2.11204154e-03, 8.72936762e-04, 1.21102457e-03,\n",
       "        7.99936744e-04, 2.30429513e-03, 4.33972324e-03, 1.82884853e-03,\n",
       "        6.78473116e-04, 4.66889271e-03, 2.45446312e-04, 1.46284312e-03,\n",
       "        3.32456479e-03, 1.01938945e-03, 1.20878185e-03, 1.31964222e-03,\n",
       "        1.20141217e-03, 4.00114130e-04, 3.74483327e-04, 8.00490456e-04,\n",
       "        1.36500249e-03, 2.00200216e-04, 3.41796809e-03, 1.99985845e-04,\n",
       "        1.99771518e-04, 1.80186440e-03, 2.45145846e-04, 3.16581903e-04,\n",
       "        2.00295640e-04, 1.71690740e-03, 2.45320943e-04, 2.00272061e-04,\n",
       "        1.77934499e-03, 1.12374127e-03, 7.25833238e-03, 5.83339174e-04,\n",
       "        1.12321049e-03, 8.72181625e-04, 8.46033919e-03, 1.82921370e-03,\n",
       "        1.49739882e-03, 2.00272004e-04, 4.80566951e-03, 4.47554373e-04,\n",
       "        9.27731799e-04, 3.74635955e-04, 1.00100072e-03, 1.14109667e-03,\n",
       "        4.90621814e-04, 8.01348734e-04, 6.63420723e-04, 3.16355973e-04,\n",
       "        3.16355721e-04, 8.76013825e-03, 5.48292371e-04, 4.42200589e-07,\n",
       "        4.00472095e-04, 1.96630813e-03, 1.70467741e-03, 9.28302648e-04,\n",
       "        6.32635897e-04, 4.90514600e-04, 3.16808060e-04, 1.58250201e-03,\n",
       "        1.60124302e-03, 2.00272799e-04, 3.74635895e-04, 8.00514292e-04,\n",
       "        1.50480669e-03, 2.44951550e-04, 1.99961740e-04, 6.81329294e-03,\n",
       "        3.74393534e-04, 1.26568646e-03, 6.00648066e-04, 1.20899455e-03,\n",
       "        3.16130823e-04, 2.99230550e-03, 6.78816683e-04, 1.46441725e-03,\n",
       "        1.39396697e-03, 2.45574516e-04, 4.35849787e-03, 4.47554530e-04,\n",
       "        2.45106572e-04, 2.45301395e-04, 2.00319602e-04, 4.26496120e-07,\n",
       "        3.01578299e-07, 3.16582923e-04, 2.45632257e-04, 6.32975305e-04,\n",
       "        1.99938133e-04, 2.45281994e-04, 4.90066902e-04, 3.74712177e-04,\n",
       "        3.16506618e-04, 4.75771364e-03, 4.62310777e-07, 3.75324776e-04,\n",
       "        2.00438521e-04, 3.73896508e-04, 2.45009428e-04, 2.45515595e-04,\n",
       "        5.48553566e-04, 1.67190191e-02, 5.83560008e-04, 1.43648826e-03,\n",
       "        6.78830671e-04, 4.00221627e-04, 4.90125302e-04, 5.83682632e-04,\n",
       "        3.16581903e-04, 3.16506532e-04, 7.49252225e-04, 6.97314660e-03,\n",
       "        2.45048903e-04, 6.39744180e-07, 6.78507281e-04, 4.67203091e-07,\n",
       "        8.61095996e-04, 6.79041606e-04, 2.00153043e-04, 4.90115546e-04,\n",
       "        4.00078470e-04, 2.45320618e-04, 2.00605421e-04, 2.78041453e-07,\n",
       "        2.00748557e-04, 2.00128584e-04, 3.16732793e-04, 5.83241093e-04,\n",
       "        1.99914398e-04, 1.99962082e-04, 3.74317096e-04, 8.72723184e-04,\n",
       "        5.48379359e-04, 5.56082906e-07, 2.45184872e-04, 2.44775966e-04,\n",
       "        5.83690840e-04, 2.45164915e-04, 2.45262229e-04, 2.00343664e-04,\n",
       "        2.45535089e-04, 2.00558106e-04, 4.00173738e-04, 4.00412277e-04,\n",
       "        2.45262276e-04, 4.00221372e-04, 2.45321823e-04, 1.89888779e-03,\n",
       "        3.74559379e-04, 5.10201797e-04, 1.46398697e-03, 2.00677053e-04,\n",
       "        5.83609263e-04, 3.71020490e-03, 6.78401864e-04, 4.00328675e-04,\n",
       "        2.44989957e-04, 2.44931406e-04, 1.99963276e-04, 2.44737033e-04,\n",
       "        7.49048410e-04, 2.80232433e-03, 4.52740453e-03, 2.00248354e-04,\n",
       "        5.48249162e-04, 4.00447931e-04, 5.47944448e-04, 1.20921159e-03,\n",
       "        1.20983517e-03, 8.61206772e-04, 2.45048949e-04, 3.74495603e-04,\n",
       "        2.00176296e-04, 2.45340313e-04, 7.75187033e-04, 3.16431509e-04,\n",
       "        2.44853369e-04, 5.63225733e-03, 1.70449535e-03, 8.00931572e-04,\n",
       "        3.71006330e-03, 6.67572021e-07, 3.01867030e-03, 5.84051030e-04,\n",
       "        3.74431939e-04, 8.00943495e-04, 7.35598381e-04, 1.48406450e-03,\n",
       "        9.70818692e-04, 4.90524343e-04, 1.40018470e-03, 6.21495291e-03,\n",
       "        3.23406696e-07, 3.74661227e-04, 3.74814129e-04, 1.12340998e-03,\n",
       "        6.00171153e-04, 3.74329974e-04, 2.45126043e-04, 1.20097002e-03,\n",
       "        3.15915993e-03, 1.77410150e-03, 1.66309874e-03, 5.68664200e-04]),\n",
       " 'param_classifier': masked_array(data=[MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__activation': masked_array(data=['logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__alpha': masked_array(data=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__hidden_layer_sizes': masked_array(data=[(5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__learning_rate': masked_array(data=['constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'}],\n",
       " 'split0_test_recall_micro': array([0.927, 0.927, 0.927, 0.919, 0.925, 0.915, 0.904, 0.907, 0.903,\n",
       "        0.905, 0.893, 0.896, 0.928, 0.928, 0.927, 0.924, 0.922, 0.923,\n",
       "        0.918, 0.914, 0.909, 0.906, 0.9  , 0.905, 0.927, 0.924, 0.927,\n",
       "        0.924, 0.926, 0.926, 0.921, 0.913, 0.915, 0.9  , 0.911, 0.917,\n",
       "        0.927, 0.927, 0.926, 0.926, 0.927, 0.925, 0.924, 0.927, 0.917,\n",
       "        0.913, 0.927, 0.904, 0.925, 0.925, 0.927, 0.914, 0.919, 0.918,\n",
       "        0.9  , 0.909, 0.91 , 0.908, 0.9  , 0.893, 0.927, 0.924, 0.928,\n",
       "        0.921, 0.918, 0.923, 0.912, 0.91 , 0.908, 0.901, 0.9  , 0.892,\n",
       "        0.924, 0.926, 0.927, 0.919, 0.922, 0.924, 0.912, 0.919, 0.914,\n",
       "        0.899, 0.904, 0.906, 0.927, 0.927, 0.927, 0.924, 0.926, 0.922,\n",
       "        0.923, 0.911, 0.914, 0.903, 0.914, 0.901, 0.927, 0.87 , 0.927,\n",
       "        0.927, 0.818, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927,\n",
       "        0.927, 0.927, 0.927, 0.927, 0.926, 0.927, 0.927, 0.927, 0.927,\n",
       "        0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.27 , 0.927,\n",
       "        0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927,\n",
       "        0.927, 0.845, 0.927, 0.927, 0.927, 0.927, 0.927, 0.91 , 0.927,\n",
       "        0.927, 0.319, 0.927, 0.927, 0.473, 0.927, 0.927, 0.403, 0.927,\n",
       "        0.927, 0.53 , 0.927, 0.927, 0.586, 0.927, 0.927, 0.78 , 0.927,\n",
       "        0.927, 0.577, 0.927, 0.927, 0.331, 0.927, 0.927, 0.223, 0.927,\n",
       "        0.927, 0.855, 0.927, 0.927, 0.564, 0.927, 0.927, 0.661, 0.927,\n",
       "        0.927, 0.421, 0.927, 0.927, 0.571, 0.927, 0.927, 0.334, 0.927,\n",
       "        0.927, 0.758, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927,\n",
       "        0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927,\n",
       "        0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927,\n",
       "        0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927,\n",
       "        0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927,\n",
       "        0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927,\n",
       "        0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.926, 0.927,\n",
       "        0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927,\n",
       "        0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927,\n",
       "        0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927,\n",
       "        0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927]),\n",
       " 'split1_test_recall_micro': array([0.926, 0.924, 0.923, 0.92 , 0.922, 0.912, 0.908, 0.91 , 0.913,\n",
       "        0.91 , 0.912, 0.892, 0.924, 0.926, 0.926, 0.917, 0.923, 0.923,\n",
       "        0.913, 0.916, 0.901, 0.905, 0.906, 0.903, 0.925, 0.927, 0.921,\n",
       "        0.919, 0.924, 0.921, 0.912, 0.915, 0.914, 0.909, 0.899, 0.911,\n",
       "        0.926, 0.925, 0.926, 0.925, 0.923, 0.925, 0.915, 0.918, 0.921,\n",
       "        0.915, 0.908, 0.914, 0.924, 0.924, 0.925, 0.921, 0.915, 0.913,\n",
       "        0.897, 0.909, 0.908, 0.895, 0.882, 0.885, 0.921, 0.926, 0.925,\n",
       "        0.917, 0.914, 0.919, 0.916, 0.917, 0.907, 0.897, 0.896, 0.887,\n",
       "        0.925, 0.926, 0.925, 0.924, 0.917, 0.92 , 0.918, 0.896, 0.916,\n",
       "        0.903, 0.899, 0.904, 0.926, 0.926, 0.926, 0.92 , 0.926, 0.925,\n",
       "        0.916, 0.916, 0.914, 0.904, 0.908, 0.909, 0.926, 0.463, 0.926,\n",
       "        0.926, 0.301, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.854, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.721, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.499, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.655, 0.926, 0.926, 0.52 , 0.926, 0.926, 0.845, 0.926,\n",
       "        0.926, 0.519, 0.926, 0.926, 0.927, 0.926, 0.926, 0.385, 0.926,\n",
       "        0.926, 0.426, 0.926, 0.926, 0.88 , 0.926, 0.926, 0.531, 0.926,\n",
       "        0.926, 0.542, 0.926, 0.926, 0.679, 0.926, 0.926, 0.498, 0.926,\n",
       "        0.926, 0.729, 0.926, 0.926, 0.736, 0.926, 0.926, 0.732, 0.926,\n",
       "        0.926, 0.878, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.927, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.925, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926]),\n",
       " 'split2_test_recall_micro': array([0.923, 0.919, 0.926, 0.918, 0.921, 0.92 , 0.897, 0.913, 0.914,\n",
       "        0.908, 0.9  , 0.892, 0.926, 0.926, 0.926, 0.923, 0.918, 0.922,\n",
       "        0.907, 0.914, 0.919, 0.896, 0.894, 0.897, 0.926, 0.926, 0.926,\n",
       "        0.916, 0.923, 0.925, 0.914, 0.915, 0.911, 0.926, 0.914, 0.91 ,\n",
       "        0.926, 0.926, 0.927, 0.92 , 0.921, 0.926, 0.917, 0.916, 0.909,\n",
       "        0.926, 0.91 , 0.908, 0.926, 0.923, 0.927, 0.916, 0.921, 0.925,\n",
       "        0.915, 0.897, 0.913, 0.898, 0.909, 0.892, 0.926, 0.926, 0.925,\n",
       "        0.923, 0.916, 0.915, 0.915, 0.904, 0.916, 0.9  , 0.901, 0.898,\n",
       "        0.926, 0.925, 0.926, 0.92 , 0.916, 0.918, 0.916, 0.904, 0.915,\n",
       "        0.901, 0.909, 0.905, 0.926, 0.926, 0.926, 0.919, 0.919, 0.922,\n",
       "        0.918, 0.911, 0.911, 0.89 , 0.909, 0.902, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.855, 0.926, 0.926, 0.626, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.074, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.847, 0.926,\n",
       "        0.926, 0.246, 0.926, 0.926, 0.61 , 0.926, 0.926, 0.69 , 0.926,\n",
       "        0.926, 0.439, 0.926, 0.926, 0.556, 0.926, 0.926, 0.703, 0.926,\n",
       "        0.926, 0.76 , 0.926, 0.926, 0.812, 0.926, 0.926, 0.558, 0.926,\n",
       "        0.926, 0.504, 0.926, 0.926, 0.377, 0.926, 0.926, 0.721, 0.926,\n",
       "        0.926, 0.684, 0.926, 0.926, 0.404, 0.926, 0.926, 0.407, 0.926,\n",
       "        0.926, 0.401, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926]),\n",
       " 'split3_test_recall_micro': array([0.926, 0.925, 0.926, 0.913, 0.924, 0.92 , 0.911, 0.905, 0.91 ,\n",
       "        0.899, 0.891, 0.898, 0.926, 0.926, 0.926, 0.918, 0.923, 0.923,\n",
       "        0.906, 0.906, 0.926, 0.926, 0.905, 0.895, 0.926, 0.926, 0.925,\n",
       "        0.918, 0.92 , 0.918, 0.905, 0.921, 0.909, 0.897, 0.926, 0.895,\n",
       "        0.924, 0.926, 0.926, 0.922, 0.924, 0.921, 0.922, 0.918, 0.918,\n",
       "        0.908, 0.905, 0.904, 0.926, 0.924, 0.921, 0.913, 0.914, 0.918,\n",
       "        0.897, 0.898, 0.906, 0.901, 0.906, 0.895, 0.925, 0.926, 0.926,\n",
       "        0.922, 0.916, 0.902, 0.905, 0.909, 0.906, 0.896, 0.89 , 0.902,\n",
       "        0.926, 0.925, 0.926, 0.919, 0.921, 0.923, 0.916, 0.91 , 0.907,\n",
       "        0.904, 0.891, 0.897, 0.926, 0.926, 0.926, 0.925, 0.919, 0.923,\n",
       "        0.904, 0.914, 0.91 , 0.892, 0.902, 0.901, 0.926, 0.395, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.653, 0.926, 0.926, 0.191, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.077, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.36 , 0.926, 0.926, 0.623, 0.926, 0.926, 0.436, 0.926,\n",
       "        0.926, 0.324, 0.926, 0.926, 0.388, 0.926, 0.926, 0.761, 0.926,\n",
       "        0.926, 0.755, 0.926, 0.926, 0.812, 0.926, 0.926, 0.68 , 0.926,\n",
       "        0.926, 0.652, 0.926, 0.926, 0.641, 0.926, 0.926, 0.722, 0.926,\n",
       "        0.926, 0.888, 0.926, 0.926, 0.778, 0.926, 0.926, 0.65 , 0.926,\n",
       "        0.926, 0.462, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.925,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.927, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926]),\n",
       " 'split4_test_recall_micro': array([0.925, 0.926, 0.926, 0.919, 0.915, 0.918, 0.921, 0.905, 0.91 ,\n",
       "        0.91 , 0.9  , 0.894, 0.926, 0.926, 0.926, 0.921, 0.921, 0.922,\n",
       "        0.913, 0.901, 0.909, 0.897, 0.926, 0.898, 0.926, 0.926, 0.925,\n",
       "        0.92 , 0.922, 0.92 , 0.92 , 0.911, 0.917, 0.918, 0.897, 0.91 ,\n",
       "        0.926, 0.926, 0.925, 0.922, 0.924, 0.925, 0.921, 0.914, 0.923,\n",
       "        0.903, 0.909, 0.911, 0.926, 0.925, 0.924, 0.915, 0.917, 0.922,\n",
       "        0.902, 0.906, 0.912, 0.907, 0.898, 0.9  , 0.925, 0.925, 0.926,\n",
       "        0.923, 0.92 , 0.912, 0.919, 0.912, 0.901, 0.895, 0.906, 0.897,\n",
       "        0.925, 0.926, 0.926, 0.92 , 0.924, 0.919, 0.907, 0.911, 0.904,\n",
       "        0.893, 0.903, 0.897, 0.926, 0.926, 0.925, 0.922, 0.923, 0.915,\n",
       "        0.918, 0.914, 0.917, 0.91 , 0.901, 0.912, 0.926, 0.878, 0.926,\n",
       "        0.926, 0.487, 0.926, 0.926, 0.537, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.084, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.176, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.382, 0.926, 0.926, 0.224, 0.926, 0.926, 0.828, 0.926,\n",
       "        0.926, 0.759, 0.926, 0.926, 0.505, 0.926, 0.926, 0.217, 0.926,\n",
       "        0.926, 0.433, 0.926, 0.926, 0.763, 0.926, 0.926, 0.804, 0.926,\n",
       "        0.926, 0.87 , 0.926, 0.926, 0.49 , 0.926, 0.926, 0.437, 0.926,\n",
       "        0.926, 0.578, 0.926, 0.926, 0.263, 0.926, 0.926, 0.485, 0.926,\n",
       "        0.926, 0.593, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.925,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.925, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926]),\n",
       " 'mean_test_recall_micro': array([0.9254, 0.9242, 0.9256, 0.9178, 0.9214, 0.917 , 0.9082, 0.908 ,\n",
       "        0.91  , 0.9064, 0.8992, 0.8944, 0.926 , 0.9264, 0.9262, 0.9206,\n",
       "        0.9214, 0.9226, 0.9114, 0.9102, 0.9128, 0.906 , 0.9062, 0.8996,\n",
       "        0.926 , 0.9258, 0.9248, 0.9194, 0.923 , 0.922 , 0.9144, 0.915 ,\n",
       "        0.9132, 0.91  , 0.9094, 0.9086, 0.9258, 0.926 , 0.926 , 0.923 ,\n",
       "        0.9238, 0.9244, 0.9198, 0.9186, 0.9176, 0.913 , 0.9118, 0.9082,\n",
       "        0.9254, 0.9242, 0.9248, 0.9158, 0.9172, 0.9192, 0.9022, 0.9038,\n",
       "        0.9098, 0.9018, 0.899 , 0.893 , 0.9248, 0.9254, 0.926 , 0.9212,\n",
       "        0.9168, 0.9142, 0.9134, 0.9104, 0.9076, 0.8978, 0.8986, 0.8952,\n",
       "        0.9252, 0.9256, 0.926 , 0.9204, 0.92  , 0.9208, 0.9138, 0.908 ,\n",
       "        0.9112, 0.9   , 0.9012, 0.9018, 0.9262, 0.9262, 0.926 , 0.922 ,\n",
       "        0.9226, 0.9214, 0.9158, 0.9132, 0.9132, 0.8998, 0.9068, 0.905 ,\n",
       "        0.9262, 0.7064, 0.9262, 0.9262, 0.6916, 0.9262, 0.9262, 0.8484,\n",
       "        0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.843 , 0.9262, 0.9262,\n",
       "        0.719 , 0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262,\n",
       "        0.9262, 0.7168, 0.9262, 0.9262, 0.7948, 0.9262, 0.9262, 0.9262,\n",
       "        0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.3506, 0.9262, 0.9262,\n",
       "        0.9098, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.907 , 0.9262,\n",
       "        0.9262, 0.3924, 0.9262, 0.9262, 0.49  , 0.9262, 0.9262, 0.6404,\n",
       "        0.9262, 0.9262, 0.5142, 0.9262, 0.9262, 0.5924, 0.9262, 0.9262,\n",
       "        0.5692, 0.9262, 0.9262, 0.5902, 0.9262, 0.9262, 0.7196, 0.9262,\n",
       "        0.9262, 0.5592, 0.9262, 0.9262, 0.6846, 0.9262, 0.9262, 0.5502,\n",
       "        0.9262, 0.9262, 0.6078, 0.9262, 0.9262, 0.66  , 0.9262, 0.9262,\n",
       "        0.5504, 0.9262, 0.9262, 0.5216, 0.9262, 0.9262, 0.6184, 0.9262,\n",
       "        0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262,\n",
       "        0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262,\n",
       "        0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262,\n",
       "        0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262,\n",
       "        0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262,\n",
       "        0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262,\n",
       "        0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262,\n",
       "        0.9262, 0.9262, 0.926 , 0.9258, 0.9262, 0.9262, 0.9262, 0.9264,\n",
       "        0.9262, 0.9262, 0.9262, 0.9264, 0.9262, 0.9262, 0.9262, 0.9262,\n",
       "        0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.926 , 0.9262,\n",
       "        0.9262, 0.926 , 0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262,\n",
       "        0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262]),\n",
       " 'std_test_recall_micro': array([1.35646600e-03, 2.78567766e-03, 1.35646600e-03, 2.48193473e-03,\n",
       "        3.49857114e-03, 3.09838668e-03, 7.93473377e-03, 3.09838668e-03,\n",
       "        3.84707681e-03, 4.12795349e-03, 7.35934780e-03, 2.33238076e-03,\n",
       "        1.26491106e-03, 8.00000000e-04, 4.00000000e-04, 2.72763634e-03,\n",
       "        1.85472370e-03, 4.89897949e-04, 4.40908154e-03, 5.74108004e-03,\n",
       "        8.72696969e-03, 1.07888832e-02, 1.07777549e-02, 3.77359245e-03,\n",
       "        6.32455532e-04, 9.79795897e-04, 2.03960781e-03, 2.65329983e-03,\n",
       "        2.00000000e-03, 3.03315018e-03, 5.81721583e-03, 3.34664011e-03,\n",
       "        2.85657137e-03, 1.08627805e-02, 1.05943381e-02, 7.28285658e-03,\n",
       "        9.79795897e-04, 6.32455532e-04, 6.32455532e-04, 2.19089023e-03,\n",
       "        1.93907194e-03, 1.74355958e-03, 3.31058907e-03, 4.45421149e-03,\n",
       "        4.80000000e-03, 7.72010363e-03, 7.78203058e-03, 3.91918359e-03,\n",
       "        8.00000000e-04, 7.48331477e-04, 2.22710575e-03, 2.78567766e-03,\n",
       "        2.56124969e-03, 4.06939799e-03, 6.67532771e-03, 5.26877595e-03,\n",
       "        2.56124969e-03, 5.03587132e-03, 9.38083152e-03, 4.85798312e-03,\n",
       "        2.03960781e-03, 8.00000000e-04, 1.09544512e-03, 2.22710575e-03,\n",
       "        2.03960781e-03, 7.13862732e-03, 4.75815090e-03, 4.22374242e-03,\n",
       "        4.84148737e-03, 2.31516738e-03, 5.35163526e-03, 5.19230199e-03,\n",
       "        7.48331477e-04, 4.89897949e-04, 6.32455532e-04, 1.85472370e-03,\n",
       "        3.03315018e-03, 2.31516738e-03, 3.91918359e-03, 7.66811581e-03,\n",
       "        4.79165942e-03, 3.89871774e-03, 6.01331855e-03, 3.96988665e-03,\n",
       "        4.00000000e-04, 4.00000000e-04, 6.32455532e-04, 2.28035085e-03,\n",
       "        3.13687743e-03, 3.38230691e-03, 6.33719181e-03, 1.93907194e-03,\n",
       "        2.48193473e-03, 7.60000000e-03, 4.79165942e-03, 4.60434577e-03,\n",
       "        4.00000000e-04, 2.28319601e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        2.53098084e-01, 4.00000000e-04, 4.00000000e-04, 1.55700482e-01,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 1.00309521e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        2.88437168e-01, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 3.26241260e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        2.62400000e-01, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 3.27447461e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        3.24000000e-02, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 3.06333152e-02, 4.00000000e-04,\n",
       "        4.00000000e-04, 1.39238788e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        1.44245624e-01, 4.00000000e-04, 4.00000000e-04, 1.88495729e-01,\n",
       "        4.00000000e-04, 4.00000000e-04, 1.42850131e-01, 4.00000000e-04,\n",
       "        4.00000000e-04, 1.80411308e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        2.26760138e-01, 4.00000000e-04, 4.00000000e-04, 1.46859661e-01,\n",
       "        4.00000000e-04, 4.00000000e-04, 1.97836903e-01, 4.00000000e-04,\n",
       "        4.00000000e-04, 1.94085960e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        1.53246990e-01, 4.00000000e-04, 4.00000000e-04, 1.08302170e-01,\n",
       "        4.00000000e-04, 4.00000000e-04, 1.18249567e-01, 4.00000000e-04,\n",
       "        4.00000000e-04, 1.55670164e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        1.95297312e-01, 4.00000000e-04, 4.00000000e-04, 1.48607671e-01,\n",
       "        4.00000000e-04, 4.00000000e-04, 1.78465235e-01, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 1.11022302e-16, 7.48331477e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.89897949e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.89897949e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 6.32455532e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 6.32455532e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04]),\n",
       " 'rank_test_recall_micro': array([182, 190, 180, 211, 201, 214, 239, 241, 234, 246, 258, 263, 170,\n",
       "          1,   4, 204, 199, 195, 229, 232, 227, 248, 247, 257, 170, 177,\n",
       "        187, 208, 193, 197, 219, 218, 223, 233, 237, 238, 177, 170, 170,\n",
       "        193, 192, 189, 207, 210, 212, 226, 228, 239, 182, 190, 186, 216,\n",
       "        213, 209, 251, 250, 235, 252, 259, 264, 187, 182, 167, 202, 215,\n",
       "        220, 222, 231, 243, 261, 260, 262, 185, 180, 170, 205, 206, 203,\n",
       "        221, 242, 230, 255, 254, 252,   4,   4, 167, 198, 196, 199, 216,\n",
       "        223, 223, 256, 245, 249,   4, 271,   4,   4, 272,   4,   4, 265,\n",
       "          4,   4,   4,   4,   4, 266,   4,   4, 269,   4,   4,   4,   4,\n",
       "          4,   4,   4,   4, 270,   4,   4, 267,   4,   4,   4,   4,   4,\n",
       "          4,   4,   4, 288,   4,   4, 235,   4,   4,   4,   4,   4, 244,\n",
       "          4,   4, 287,   4,   4, 286,   4,   4, 275,   4,   4, 285,   4,\n",
       "          4, 278,   4,   4, 280,   4,   4, 279,   4,   4, 268,   4,   4,\n",
       "        281,   4,   4, 273,   4,   4, 283,   4,   4, 277,   4,   4, 274,\n",
       "          4,   4, 282,   4,   4, 284,   4,   4, 276,   4,   4,   4,   4,\n",
       "          4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,\n",
       "          4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,\n",
       "          4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,\n",
       "          4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,\n",
       "          4,   4,   4, 170, 177,   4,   4,   4,   1,   4,   4,   4,   1,\n",
       "          4,   4,   4,   4,   4,   4,   4,   4,   4,   4, 170,   4,   4,\n",
       "        167,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,\n",
       "          4,   4]),\n",
       " 'split0_test_f1_micro': array([0.927, 0.927, 0.927, 0.919, 0.925, 0.915, 0.904, 0.907, 0.903,\n",
       "        0.905, 0.893, 0.896, 0.928, 0.928, 0.927, 0.924, 0.922, 0.923,\n",
       "        0.918, 0.914, 0.909, 0.906, 0.9  , 0.905, 0.927, 0.924, 0.927,\n",
       "        0.924, 0.926, 0.926, 0.921, 0.913, 0.915, 0.9  , 0.911, 0.917,\n",
       "        0.927, 0.927, 0.926, 0.926, 0.927, 0.925, 0.924, 0.927, 0.917,\n",
       "        0.913, 0.927, 0.904, 0.925, 0.925, 0.927, 0.914, 0.919, 0.918,\n",
       "        0.9  , 0.909, 0.91 , 0.908, 0.9  , 0.893, 0.927, 0.924, 0.928,\n",
       "        0.921, 0.918, 0.923, 0.912, 0.91 , 0.908, 0.901, 0.9  , 0.892,\n",
       "        0.924, 0.926, 0.927, 0.919, 0.922, 0.924, 0.912, 0.919, 0.914,\n",
       "        0.899, 0.904, 0.906, 0.927, 0.927, 0.927, 0.924, 0.926, 0.922,\n",
       "        0.923, 0.911, 0.914, 0.903, 0.914, 0.901, 0.927, 0.87 , 0.927,\n",
       "        0.927, 0.818, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927,\n",
       "        0.927, 0.927, 0.927, 0.927, 0.926, 0.927, 0.927, 0.927, 0.927,\n",
       "        0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.27 , 0.927,\n",
       "        0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927,\n",
       "        0.927, 0.845, 0.927, 0.927, 0.927, 0.927, 0.927, 0.91 , 0.927,\n",
       "        0.927, 0.319, 0.927, 0.927, 0.473, 0.927, 0.927, 0.403, 0.927,\n",
       "        0.927, 0.53 , 0.927, 0.927, 0.586, 0.927, 0.927, 0.78 , 0.927,\n",
       "        0.927, 0.577, 0.927, 0.927, 0.331, 0.927, 0.927, 0.223, 0.927,\n",
       "        0.927, 0.855, 0.927, 0.927, 0.564, 0.927, 0.927, 0.661, 0.927,\n",
       "        0.927, 0.421, 0.927, 0.927, 0.571, 0.927, 0.927, 0.334, 0.927,\n",
       "        0.927, 0.758, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927,\n",
       "        0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927,\n",
       "        0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927,\n",
       "        0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927,\n",
       "        0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927,\n",
       "        0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927,\n",
       "        0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.926, 0.927,\n",
       "        0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927,\n",
       "        0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927,\n",
       "        0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927,\n",
       "        0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927, 0.927]),\n",
       " 'split1_test_f1_micro': array([0.926, 0.924, 0.923, 0.92 , 0.922, 0.912, 0.908, 0.91 , 0.913,\n",
       "        0.91 , 0.912, 0.892, 0.924, 0.926, 0.926, 0.917, 0.923, 0.923,\n",
       "        0.913, 0.916, 0.901, 0.905, 0.906, 0.903, 0.925, 0.927, 0.921,\n",
       "        0.919, 0.924, 0.921, 0.912, 0.915, 0.914, 0.909, 0.899, 0.911,\n",
       "        0.926, 0.925, 0.926, 0.925, 0.923, 0.925, 0.915, 0.918, 0.921,\n",
       "        0.915, 0.908, 0.914, 0.924, 0.924, 0.925, 0.921, 0.915, 0.913,\n",
       "        0.897, 0.909, 0.908, 0.895, 0.882, 0.885, 0.921, 0.926, 0.925,\n",
       "        0.917, 0.914, 0.919, 0.916, 0.917, 0.907, 0.897, 0.896, 0.887,\n",
       "        0.925, 0.926, 0.925, 0.924, 0.917, 0.92 , 0.918, 0.896, 0.916,\n",
       "        0.903, 0.899, 0.904, 0.926, 0.926, 0.926, 0.92 , 0.926, 0.925,\n",
       "        0.916, 0.916, 0.914, 0.904, 0.908, 0.909, 0.926, 0.463, 0.926,\n",
       "        0.926, 0.301, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.854, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.721, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.499, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.655, 0.926, 0.926, 0.52 , 0.926, 0.926, 0.845, 0.926,\n",
       "        0.926, 0.519, 0.926, 0.926, 0.927, 0.926, 0.926, 0.385, 0.926,\n",
       "        0.926, 0.426, 0.926, 0.926, 0.88 , 0.926, 0.926, 0.531, 0.926,\n",
       "        0.926, 0.542, 0.926, 0.926, 0.679, 0.926, 0.926, 0.498, 0.926,\n",
       "        0.926, 0.729, 0.926, 0.926, 0.736, 0.926, 0.926, 0.732, 0.926,\n",
       "        0.926, 0.878, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.927, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.925, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926]),\n",
       " 'split2_test_f1_micro': array([0.923, 0.919, 0.926, 0.918, 0.921, 0.92 , 0.897, 0.913, 0.914,\n",
       "        0.908, 0.9  , 0.892, 0.926, 0.926, 0.926, 0.923, 0.918, 0.922,\n",
       "        0.907, 0.914, 0.919, 0.896, 0.894, 0.897, 0.926, 0.926, 0.926,\n",
       "        0.916, 0.923, 0.925, 0.914, 0.915, 0.911, 0.926, 0.914, 0.91 ,\n",
       "        0.926, 0.926, 0.927, 0.92 , 0.921, 0.926, 0.917, 0.916, 0.909,\n",
       "        0.926, 0.91 , 0.908, 0.926, 0.923, 0.927, 0.916, 0.921, 0.925,\n",
       "        0.915, 0.897, 0.913, 0.898, 0.909, 0.892, 0.926, 0.926, 0.925,\n",
       "        0.923, 0.916, 0.915, 0.915, 0.904, 0.916, 0.9  , 0.901, 0.898,\n",
       "        0.926, 0.925, 0.926, 0.92 , 0.916, 0.918, 0.916, 0.904, 0.915,\n",
       "        0.901, 0.909, 0.905, 0.926, 0.926, 0.926, 0.919, 0.919, 0.922,\n",
       "        0.918, 0.911, 0.911, 0.89 , 0.909, 0.902, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.855, 0.926, 0.926, 0.626, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.074, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.847, 0.926,\n",
       "        0.926, 0.246, 0.926, 0.926, 0.61 , 0.926, 0.926, 0.69 , 0.926,\n",
       "        0.926, 0.439, 0.926, 0.926, 0.556, 0.926, 0.926, 0.703, 0.926,\n",
       "        0.926, 0.76 , 0.926, 0.926, 0.812, 0.926, 0.926, 0.558, 0.926,\n",
       "        0.926, 0.504, 0.926, 0.926, 0.377, 0.926, 0.926, 0.721, 0.926,\n",
       "        0.926, 0.684, 0.926, 0.926, 0.404, 0.926, 0.926, 0.407, 0.926,\n",
       "        0.926, 0.401, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926]),\n",
       " 'split3_test_f1_micro': array([0.926, 0.925, 0.926, 0.913, 0.924, 0.92 , 0.911, 0.905, 0.91 ,\n",
       "        0.899, 0.891, 0.898, 0.926, 0.926, 0.926, 0.918, 0.923, 0.923,\n",
       "        0.906, 0.906, 0.926, 0.926, 0.905, 0.895, 0.926, 0.926, 0.925,\n",
       "        0.918, 0.92 , 0.918, 0.905, 0.921, 0.909, 0.897, 0.926, 0.895,\n",
       "        0.924, 0.926, 0.926, 0.922, 0.924, 0.921, 0.922, 0.918, 0.918,\n",
       "        0.908, 0.905, 0.904, 0.926, 0.924, 0.921, 0.913, 0.914, 0.918,\n",
       "        0.897, 0.898, 0.906, 0.901, 0.906, 0.895, 0.925, 0.926, 0.926,\n",
       "        0.922, 0.916, 0.902, 0.905, 0.909, 0.906, 0.896, 0.89 , 0.902,\n",
       "        0.926, 0.925, 0.926, 0.919, 0.921, 0.923, 0.916, 0.91 , 0.907,\n",
       "        0.904, 0.891, 0.897, 0.926, 0.926, 0.926, 0.925, 0.919, 0.923,\n",
       "        0.904, 0.914, 0.91 , 0.892, 0.902, 0.901, 0.926, 0.395, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.653, 0.926, 0.926, 0.191, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.077, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.36 , 0.926, 0.926, 0.623, 0.926, 0.926, 0.436, 0.926,\n",
       "        0.926, 0.324, 0.926, 0.926, 0.388, 0.926, 0.926, 0.761, 0.926,\n",
       "        0.926, 0.755, 0.926, 0.926, 0.812, 0.926, 0.926, 0.68 , 0.926,\n",
       "        0.926, 0.652, 0.926, 0.926, 0.641, 0.926, 0.926, 0.722, 0.926,\n",
       "        0.926, 0.888, 0.926, 0.926, 0.778, 0.926, 0.926, 0.65 , 0.926,\n",
       "        0.926, 0.462, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.925,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.927, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926]),\n",
       " 'split4_test_f1_micro': array([0.925, 0.926, 0.926, 0.919, 0.915, 0.918, 0.921, 0.905, 0.91 ,\n",
       "        0.91 , 0.9  , 0.894, 0.926, 0.926, 0.926, 0.921, 0.921, 0.922,\n",
       "        0.913, 0.901, 0.909, 0.897, 0.926, 0.898, 0.926, 0.926, 0.925,\n",
       "        0.92 , 0.922, 0.92 , 0.92 , 0.911, 0.917, 0.918, 0.897, 0.91 ,\n",
       "        0.926, 0.926, 0.925, 0.922, 0.924, 0.925, 0.921, 0.914, 0.923,\n",
       "        0.903, 0.909, 0.911, 0.926, 0.925, 0.924, 0.915, 0.917, 0.922,\n",
       "        0.902, 0.906, 0.912, 0.907, 0.898, 0.9  , 0.925, 0.925, 0.926,\n",
       "        0.923, 0.92 , 0.912, 0.919, 0.912, 0.901, 0.895, 0.906, 0.897,\n",
       "        0.925, 0.926, 0.926, 0.92 , 0.924, 0.919, 0.907, 0.911, 0.904,\n",
       "        0.893, 0.903, 0.897, 0.926, 0.926, 0.925, 0.922, 0.923, 0.915,\n",
       "        0.918, 0.914, 0.917, 0.91 , 0.901, 0.912, 0.926, 0.878, 0.926,\n",
       "        0.926, 0.487, 0.926, 0.926, 0.537, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.084, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.176, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.382, 0.926, 0.926, 0.224, 0.926, 0.926, 0.828, 0.926,\n",
       "        0.926, 0.759, 0.926, 0.926, 0.505, 0.926, 0.926, 0.217, 0.926,\n",
       "        0.926, 0.433, 0.926, 0.926, 0.763, 0.926, 0.926, 0.804, 0.926,\n",
       "        0.926, 0.87 , 0.926, 0.926, 0.49 , 0.926, 0.926, 0.437, 0.926,\n",
       "        0.926, 0.578, 0.926, 0.926, 0.263, 0.926, 0.926, 0.485, 0.926,\n",
       "        0.926, 0.593, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.925,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.925, 0.926, 0.926, 0.926, 0.926, 0.926,\n",
       "        0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926, 0.926]),\n",
       " 'mean_test_f1_micro': array([0.9254, 0.9242, 0.9256, 0.9178, 0.9214, 0.917 , 0.9082, 0.908 ,\n",
       "        0.91  , 0.9064, 0.8992, 0.8944, 0.926 , 0.9264, 0.9262, 0.9206,\n",
       "        0.9214, 0.9226, 0.9114, 0.9102, 0.9128, 0.906 , 0.9062, 0.8996,\n",
       "        0.926 , 0.9258, 0.9248, 0.9194, 0.923 , 0.922 , 0.9144, 0.915 ,\n",
       "        0.9132, 0.91  , 0.9094, 0.9086, 0.9258, 0.926 , 0.926 , 0.923 ,\n",
       "        0.9238, 0.9244, 0.9198, 0.9186, 0.9176, 0.913 , 0.9118, 0.9082,\n",
       "        0.9254, 0.9242, 0.9248, 0.9158, 0.9172, 0.9192, 0.9022, 0.9038,\n",
       "        0.9098, 0.9018, 0.899 , 0.893 , 0.9248, 0.9254, 0.926 , 0.9212,\n",
       "        0.9168, 0.9142, 0.9134, 0.9104, 0.9076, 0.8978, 0.8986, 0.8952,\n",
       "        0.9252, 0.9256, 0.926 , 0.9204, 0.92  , 0.9208, 0.9138, 0.908 ,\n",
       "        0.9112, 0.9   , 0.9012, 0.9018, 0.9262, 0.9262, 0.926 , 0.922 ,\n",
       "        0.9226, 0.9214, 0.9158, 0.9132, 0.9132, 0.8998, 0.9068, 0.905 ,\n",
       "        0.9262, 0.7064, 0.9262, 0.9262, 0.6916, 0.9262, 0.9262, 0.8484,\n",
       "        0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.843 , 0.9262, 0.9262,\n",
       "        0.719 , 0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262,\n",
       "        0.9262, 0.7168, 0.9262, 0.9262, 0.7948, 0.9262, 0.9262, 0.9262,\n",
       "        0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.3506, 0.9262, 0.9262,\n",
       "        0.9098, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.907 , 0.9262,\n",
       "        0.9262, 0.3924, 0.9262, 0.9262, 0.49  , 0.9262, 0.9262, 0.6404,\n",
       "        0.9262, 0.9262, 0.5142, 0.9262, 0.9262, 0.5924, 0.9262, 0.9262,\n",
       "        0.5692, 0.9262, 0.9262, 0.5902, 0.9262, 0.9262, 0.7196, 0.9262,\n",
       "        0.9262, 0.5592, 0.9262, 0.9262, 0.6846, 0.9262, 0.9262, 0.5502,\n",
       "        0.9262, 0.9262, 0.6078, 0.9262, 0.9262, 0.66  , 0.9262, 0.9262,\n",
       "        0.5504, 0.9262, 0.9262, 0.5216, 0.9262, 0.9262, 0.6184, 0.9262,\n",
       "        0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262,\n",
       "        0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262,\n",
       "        0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262,\n",
       "        0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262,\n",
       "        0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262,\n",
       "        0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262,\n",
       "        0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262,\n",
       "        0.9262, 0.9262, 0.926 , 0.9258, 0.9262, 0.9262, 0.9262, 0.9264,\n",
       "        0.9262, 0.9262, 0.9262, 0.9264, 0.9262, 0.9262, 0.9262, 0.9262,\n",
       "        0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.926 , 0.9262,\n",
       "        0.9262, 0.926 , 0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262,\n",
       "        0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262, 0.9262]),\n",
       " 'std_test_f1_micro': array([1.35646600e-03, 2.78567766e-03, 1.35646600e-03, 2.48193473e-03,\n",
       "        3.49857114e-03, 3.09838668e-03, 7.93473377e-03, 3.09838668e-03,\n",
       "        3.84707681e-03, 4.12795349e-03, 7.35934780e-03, 2.33238076e-03,\n",
       "        1.26491106e-03, 8.00000000e-04, 4.00000000e-04, 2.72763634e-03,\n",
       "        1.85472370e-03, 4.89897949e-04, 4.40908154e-03, 5.74108004e-03,\n",
       "        8.72696969e-03, 1.07888832e-02, 1.07777549e-02, 3.77359245e-03,\n",
       "        6.32455532e-04, 9.79795897e-04, 2.03960781e-03, 2.65329983e-03,\n",
       "        2.00000000e-03, 3.03315018e-03, 5.81721583e-03, 3.34664011e-03,\n",
       "        2.85657137e-03, 1.08627805e-02, 1.05943381e-02, 7.28285658e-03,\n",
       "        9.79795897e-04, 6.32455532e-04, 6.32455532e-04, 2.19089023e-03,\n",
       "        1.93907194e-03, 1.74355958e-03, 3.31058907e-03, 4.45421149e-03,\n",
       "        4.80000000e-03, 7.72010363e-03, 7.78203058e-03, 3.91918359e-03,\n",
       "        8.00000000e-04, 7.48331477e-04, 2.22710575e-03, 2.78567766e-03,\n",
       "        2.56124969e-03, 4.06939799e-03, 6.67532771e-03, 5.26877595e-03,\n",
       "        2.56124969e-03, 5.03587132e-03, 9.38083152e-03, 4.85798312e-03,\n",
       "        2.03960781e-03, 8.00000000e-04, 1.09544512e-03, 2.22710575e-03,\n",
       "        2.03960781e-03, 7.13862732e-03, 4.75815090e-03, 4.22374242e-03,\n",
       "        4.84148737e-03, 2.31516738e-03, 5.35163526e-03, 5.19230199e-03,\n",
       "        7.48331477e-04, 4.89897949e-04, 6.32455532e-04, 1.85472370e-03,\n",
       "        3.03315018e-03, 2.31516738e-03, 3.91918359e-03, 7.66811581e-03,\n",
       "        4.79165942e-03, 3.89871774e-03, 6.01331855e-03, 3.96988665e-03,\n",
       "        4.00000000e-04, 4.00000000e-04, 6.32455532e-04, 2.28035085e-03,\n",
       "        3.13687743e-03, 3.38230691e-03, 6.33719181e-03, 1.93907194e-03,\n",
       "        2.48193473e-03, 7.60000000e-03, 4.79165942e-03, 4.60434577e-03,\n",
       "        4.00000000e-04, 2.28319601e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        2.53098084e-01, 4.00000000e-04, 4.00000000e-04, 1.55700482e-01,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 1.00309521e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        2.88437168e-01, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 3.26241260e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        2.62400000e-01, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 3.27447461e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        3.24000000e-02, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 3.06333152e-02, 4.00000000e-04,\n",
       "        4.00000000e-04, 1.39238788e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        1.44245624e-01, 4.00000000e-04, 4.00000000e-04, 1.88495729e-01,\n",
       "        4.00000000e-04, 4.00000000e-04, 1.42850131e-01, 4.00000000e-04,\n",
       "        4.00000000e-04, 1.80411308e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        2.26760138e-01, 4.00000000e-04, 4.00000000e-04, 1.46859661e-01,\n",
       "        4.00000000e-04, 4.00000000e-04, 1.97836903e-01, 4.00000000e-04,\n",
       "        4.00000000e-04, 1.94085960e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        1.53246990e-01, 4.00000000e-04, 4.00000000e-04, 1.08302170e-01,\n",
       "        4.00000000e-04, 4.00000000e-04, 1.18249567e-01, 4.00000000e-04,\n",
       "        4.00000000e-04, 1.55670164e-01, 4.00000000e-04, 4.00000000e-04,\n",
       "        1.95297312e-01, 4.00000000e-04, 4.00000000e-04, 1.48607671e-01,\n",
       "        4.00000000e-04, 4.00000000e-04, 1.78465235e-01, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 1.11022302e-16, 7.48331477e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.89897949e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.89897949e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 6.32455532e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 6.32455532e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04,\n",
       "        4.00000000e-04, 4.00000000e-04, 4.00000000e-04, 4.00000000e-04]),\n",
       " 'rank_test_f1_micro': array([182, 190, 180, 211, 201, 214, 239, 241, 233, 246, 258, 263, 170,\n",
       "          1,   4, 204, 199, 195, 229, 232, 227, 248, 247, 257, 170, 177,\n",
       "        186, 208, 193, 197, 219, 218, 223, 233, 237, 238, 177, 170, 170,\n",
       "        193, 192, 189, 207, 210, 212, 226, 228, 239, 182, 190, 186, 216,\n",
       "        213, 209, 251, 250, 235, 252, 259, 264, 186, 182, 167, 202, 215,\n",
       "        220, 222, 231, 243, 261, 260, 262, 185, 180, 170, 205, 206, 203,\n",
       "        221, 241, 230, 255, 254, 252,   4,   4, 167, 198, 196, 199, 217,\n",
       "        223, 223, 256, 245, 249,   4, 271,   4,   4, 272,   4,   4, 265,\n",
       "          4,   4,   4,   4,   4, 266,   4,   4, 269,   4,   4,   4,   4,\n",
       "          4,   4,   4,   4, 270,   4,   4, 267,   4,   4,   4,   4,   4,\n",
       "          4,   4,   4, 288,   4,   4, 235,   4,   4,   4,   4,   4, 244,\n",
       "          4,   4, 287,   4,   4, 286,   4,   4, 275,   4,   4, 285,   4,\n",
       "          4, 278,   4,   4, 280,   4,   4, 279,   4,   4, 268,   4,   4,\n",
       "        281,   4,   4, 273,   4,   4, 283,   4,   4, 277,   4,   4, 274,\n",
       "          4,   4, 282,   4,   4, 284,   4,   4, 276,   4,   4,   4,   4,\n",
       "          4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,\n",
       "          4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,\n",
       "          4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,\n",
       "          4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,\n",
       "          4,   4,   4, 170, 177,   4,   4,   4,   1,   4,   4,   4,   1,\n",
       "          4,   4,   4,   4,   4,   4,   4,   4,   4,   4, 170,   4,   4,\n",
       "        167,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,\n",
       "          4,   4]),\n",
       " 'split0_test_roc_auc_ovo': array([0.49554462, 0.56538251, 0.58181496, 0.58961002, 0.56789467,\n",
       "        0.58219178, 0.51939531, 0.53356682, 0.57053243, 0.57161857,\n",
       "        0.58601173, 0.51327747, 0.5622571 , 0.58474827, 0.58011556,\n",
       "        0.56853009, 0.58197751, 0.6139262 , 0.58859777, 0.53876845,\n",
       "        0.62177299, 0.57351007, 0.59626723, 0.60535532, 0.59408018,\n",
       "        0.53405447, 0.61476851, 0.53406925, 0.58669149, 0.5736874 ,\n",
       "        0.55262963, 0.53692128, 0.58512509, 0.55929423, 0.59192268,\n",
       "        0.59515893, 0.56684547, 0.59390285, 0.54634925, 0.56994872,\n",
       "        0.5733623 , 0.53669962, 0.59502593, 0.5132627 , 0.57430805,\n",
       "        0.56250092, 0.5137208 , 0.59320832, 0.5017659 , 0.58909282,\n",
       "        0.60126938, 0.5523932 , 0.53327127, 0.55094501, 0.52601558,\n",
       "        0.54543305, 0.57392384, 0.56378656, 0.61779788, 0.54791565,\n",
       "        0.56078675, 0.57620694, 0.58870122, 0.56359445, 0.61298045,\n",
       "        0.55381182, 0.50528291, 0.53711339, 0.57959835, 0.59007551,\n",
       "        0.55502357, 0.53058179, 0.58598218, 0.54651919, 0.54704379,\n",
       "        0.51413456, 0.54574338, 0.55231931, 0.57287464, 0.63616616,\n",
       "        0.57167768, 0.62047258, 0.5477531 , 0.61317256, 0.58078054,\n",
       "        0.58803623, 0.5764951 , 0.61278834, 0.57285987, 0.57800239,\n",
       "        0.52780364, 0.6103353 , 0.52851295, 0.56910641, 0.56952018,\n",
       "        0.55610232, 0.50090881, 0.4834715 , 0.49347579, 0.51982385,\n",
       "        0.55028003, 0.59391763, 0.57612567, 0.47945205, 0.51329225,\n",
       "        0.49241182, 0.44416367, 0.53930044, 0.48493446, 0.54535916,\n",
       "        0.47819598, 0.42909075, 0.54441341, 0.46520666, 0.55673775,\n",
       "        0.47364454, 0.49798289, 0.45598558, 0.41285041, 0.46572387,\n",
       "        0.50950924, 0.45616291, 0.45821696, 0.47362977, 0.43953835,\n",
       "        0.55230453, 0.45619246, 0.44206529, 0.43002172, 0.46248762,\n",
       "        0.50795762, 0.51061755, 0.44544931, 0.51076532, 0.50050982,\n",
       "        0.58018945, 0.5092137 , 0.58094309, 0.46507367, 0.55398915,\n",
       "        0.48115145, 0.45331087, 0.54247758, 0.46028579, 0.4393758 ,\n",
       "        0.46319694, 0.49292902, 0.44586307, 0.49697803, 0.5210356 ,\n",
       "        0.54988104, 0.5634319 , 0.49830799, 0.48293952, 0.42269214,\n",
       "        0.5671558 , 0.51960219, 0.41496357, 0.53949254, 0.49015088,\n",
       "        0.54422131, 0.46551699, 0.48890958, 0.52078438, 0.52020807,\n",
       "        0.54463507, 0.48363405, 0.50936147, 0.51675016, 0.51091309,\n",
       "        0.50217966, 0.54744277, 0.45298577, 0.47812209, 0.5437041 ,\n",
       "        0.51657283, 0.52504027, 0.5152872 , 0.50040638, 0.48035347,\n",
       "        0.52712388, 0.44237561, 0.53219252, 0.5542847 , 0.46649229,\n",
       "        0.48604277, 0.51219873, 0.45277889, 0.47589071, 0.51042544,\n",
       "        0.51921798, 0.46368459, 0.51440056, 0.51732648, 0.51750381,\n",
       "        0.50810539, 0.51873033, 0.51577485, 0.51635117, 0.50602178,\n",
       "        0.51057321, 0.51376513, 0.5177698 , 0.51230217, 0.52840951,\n",
       "        0.51738559, 0.52040017, 0.51278982, 0.51915887, 0.51271593,\n",
       "        0.51828701, 0.51892243, 0.51262727, 0.51525764, 0.51314448,\n",
       "        0.51881899, 0.52170058, 0.5221439 , 0.51703093, 0.51403112,\n",
       "        0.5094058 , 0.51849389, 0.51581918, 0.51048455, 0.51881899,\n",
       "        0.51512465, 0.51443011, 0.52150847, 0.51596696, 0.51880421,\n",
       "        0.51389812, 0.51765158, 0.51630684, 0.51509509, 0.51604084,\n",
       "        0.5090807 , 0.51420845, 0.51354347, 0.51379468, 0.51042544,\n",
       "        0.58735647, 0.56888475, 0.49060898, 0.57512081, 0.5548758 ,\n",
       "        0.54660046, 0.60244418, 0.57430805, 0.59699133, 0.51116431,\n",
       "        0.58555363, 0.56850054, 0.56585539, 0.57238699, 0.59425751,\n",
       "        0.56084586, 0.57384995, 0.58292326, 0.52653278, 0.56046165,\n",
       "        0.58818401, 0.54577293, 0.56394911, 0.5947895 , 0.47761966,\n",
       "        0.59783364, 0.52903016, 0.57599267, 0.57684976, 0.58069188,\n",
       "        0.57528336, 0.5386059 , 0.5233852 , 0.55927946, 0.56272258,\n",
       "        0.56356489, 0.58800668, 0.57940624, 0.58397245, 0.58797712,\n",
       "        0.54476807, 0.54971849, 0.60643407, 0.56483575, 0.56640215,\n",
       "        0.58513987, 0.57062848, 0.58797712]),\n",
       " 'split1_test_roc_auc_ovo': array([0.62126408, 0.60388331, 0.58944311, 0.5308505 , 0.57251474,\n",
       "        0.61048684, 0.49309731, 0.58710817, 0.56494075, 0.59571829,\n",
       "        0.62621855, 0.58678711, 0.55117915, 0.5350826 , 0.54570661,\n",
       "        0.57054463, 0.60155566, 0.53533069, 0.58826105, 0.5530617 ,\n",
       "        0.57937365, 0.59621447, 0.62785302, 0.60339443, 0.55561555,\n",
       "        0.6105598 , 0.58332847, 0.55846127, 0.54915066, 0.55679762,\n",
       "        0.58232152, 0.56375868, 0.57124511, 0.59265367, 0.61717063,\n",
       "        0.55478373, 0.55037651, 0.55930769, 0.57581285, 0.61699551,\n",
       "        0.57524371, 0.59466756, 0.55869476, 0.58242368, 0.63643395,\n",
       "        0.52909929, 0.56565583, 0.61687876, 0.5799209 , 0.54090538,\n",
       "        0.57136186, 0.57444107, 0.55865828, 0.61302609, 0.52234254,\n",
       "        0.6312095 , 0.50990164, 0.61512755, 0.52602008, 0.57034032,\n",
       "        0.54770591, 0.59065437, 0.54178828, 0.53062431, 0.53429455,\n",
       "        0.59289446, 0.54678653, 0.57449945, 0.5885967 , 0.55402487,\n",
       "        0.66138871, 0.58824645, 0.58521102, 0.61976826, 0.55208394,\n",
       "        0.57738894, 0.58486078, 0.54172261, 0.54456833, 0.5546086 ,\n",
       "        0.53696515, 0.60892534, 0.55996439, 0.57845426, 0.59447785,\n",
       "        0.56842858, 0.53627926, 0.59760084, 0.53502423, 0.545298  ,\n",
       "        0.56422567, 0.55294495, 0.57934446, 0.54499154, 0.58798377,\n",
       "        0.58737085, 0.55275524, 0.43254918, 0.49557819, 0.54283171,\n",
       "        0.51027377, 0.49248439, 0.51622789, 0.44267702, 0.47773043,\n",
       "        0.53822019, 0.52339326, 0.49613274, 0.49665811, 0.52133559,\n",
       "        0.46620162, 0.55196719, 0.53635223, 0.56333547, 0.5173224 ,\n",
       "        0.56403596, 0.46583679, 0.46828848, 0.45299457, 0.52648707,\n",
       "        0.50935439, 0.53369622, 0.52047458, 0.50185337, 0.46436285,\n",
       "        0.48781449, 0.47872278, 0.47484093, 0.4620425 , 0.49527173,\n",
       "        0.49090829, 0.5626058 , 0.47034616, 0.49563657, 0.46414395,\n",
       "        0.52907011, 0.49896387, 0.56289767, 0.49969354, 0.56631253,\n",
       "        0.49036834, 0.46865332, 0.43149845, 0.55139805, 0.55834452,\n",
       "        0.4931265 , 0.4591384 , 0.522649  , 0.55271146, 0.54532718,\n",
       "        0.54772051, 0.52167124, 0.49192983, 0.51361567, 0.5199784 ,\n",
       "        0.50319596, 0.4951258 , 0.52362676, 0.53854124, 0.52152531,\n",
       "        0.49004728, 0.4904559 , 0.49937248, 0.49535929, 0.57452863,\n",
       "        0.50274356, 0.50167824, 0.44203491, 0.5139805 , 0.52467749,\n",
       "        0.4875664 , 0.58054112, 0.48416613, 0.54653844, 0.49277625,\n",
       "        0.43463604, 0.51651976, 0.53613333, 0.47095908, 0.4968916 ,\n",
       "        0.50637733, 0.46164847, 0.50713619, 0.55936606, 0.48226899,\n",
       "        0.51190824, 0.46820092, 0.50907711, 0.49721266, 0.52321814,\n",
       "        0.50707781, 0.52978518, 0.53598739, 0.5355204 , 0.53779698,\n",
       "        0.53582686, 0.53566634, 0.54154749, 0.53629385, 0.53750511,\n",
       "        0.53398809, 0.54073026, 0.54185395, 0.53791372, 0.54064269,\n",
       "        0.53785535, 0.53724243, 0.5359728 , 0.5388477 , 0.54056973,\n",
       "        0.53911038, 0.53769482, 0.53867258, 0.53890608, 0.53697974,\n",
       "        0.53849746, 0.52667678, 0.53074835, 0.53387134, 0.53741755,\n",
       "        0.53598739, 0.53776779, 0.53833693, 0.5386434 , 0.53792832,\n",
       "        0.53878933, 0.53800128, 0.53655653, 0.53919795, 0.53693596,\n",
       "        0.53384216, 0.53144884, 0.5359582 , 0.53692137, 0.54032164,\n",
       "        0.53690678, 0.5379721 , 0.53606036, 0.53960656, 0.53946063,\n",
       "        0.5463925 , 0.55471076, 0.54255443, 0.5805849 , 0.57973849,\n",
       "        0.58116864, 0.56688168, 0.54181017, 0.54748701, 0.55446267,\n",
       "        0.58912206, 0.54414512, 0.53514097, 0.56051894, 0.55853424,\n",
       "        0.54201448, 0.53778238, 0.53137587, 0.60847294, 0.53527231,\n",
       "        0.59421517, 0.56457592, 0.52307221, 0.55078513, 0.52682272,\n",
       "        0.55948281, 0.5626058 , 0.54122643, 0.54901932, 0.6065758 ,\n",
       "        0.52647248, 0.59328119, 0.57899422, 0.5439554 , 0.58750219,\n",
       "        0.57635281, 0.55380597, 0.56396299, 0.51757049, 0.58366412,\n",
       "        0.55007005, 0.54010274, 0.53749051, 0.57911097, 0.56644387,\n",
       "        0.57384274, 0.55382056, 0.54970521]),\n",
       " 'split2_test_roc_auc_ovo': array([0.56548071, 0.59273393, 0.61017308, 0.57055922, 0.50988705,\n",
       "        0.60084058, 0.60893264, 0.56430594, 0.62273072, 0.55366003,\n",
       "        0.60917343, 0.56898313, 0.59844726, 0.57644767, 0.50307921,\n",
       "        0.56508668, 0.59821376, 0.61905318, 0.54097834, 0.54097834,\n",
       "        0.63110735, 0.53525772, 0.54471426, 0.60298582, 0.59727979,\n",
       "        0.54437861, 0.49983947, 0.54148911, 0.57055922, 0.59541183,\n",
       "        0.57388652, 0.60537914, 0.56640009, 0.504451  , 0.63493083,\n",
       "        0.62861187, 0.57153698, 0.52337867, 0.62175296, 0.56422567,\n",
       "        0.61107057, 0.62170918, 0.62732765, 0.54836262, 0.55808184,\n",
       "        0.50684432, 0.59602475, 0.59660849, 0.6223294 , 0.56469266,\n",
       "        0.54650196, 0.56872045, 0.59334686, 0.60964771, 0.59018738,\n",
       "        0.61333256, 0.60373008, 0.60530617, 0.59426624, 0.5917051 ,\n",
       "        0.53623548, 0.5437365 , 0.54586714, 0.5768125 , 0.50242251,\n",
       "        0.59878291, 0.58954527, 0.59115055, 0.5566152 , 0.60574397,\n",
       "        0.5905814 , 0.56178857, 0.56553178, 0.60748059, 0.50907711,\n",
       "        0.57591501, 0.55900123, 0.61898021, 0.59811161, 0.63205592,\n",
       "        0.62832   , 0.60921721, 0.60895453, 0.61108517, 0.57639659,\n",
       "        0.59409842, 0.52763995, 0.59917693, 0.61543401, 0.6343325 ,\n",
       "        0.57895044, 0.49715428, 0.5599498 , 0.56729029, 0.56534937,\n",
       "        0.59672523, 0.5470638 , 0.53735917, 0.4800216 , 0.45738719,\n",
       "        0.43681046, 0.51046349, 0.52024108, 0.52785885, 0.43056447,\n",
       "        0.47254976, 0.51430156, 0.55907419, 0.49938708, 0.51935088,\n",
       "        0.49479015, 0.47663592, 0.50316677, 0.4895657 , 0.5270854 ,\n",
       "        0.50048158, 0.49490689, 0.48272138, 0.50758858, 0.49871578,\n",
       "        0.50271438, 0.46609947, 0.57486428, 0.53122993, 0.47672348,\n",
       "        0.54270037, 0.50761777, 0.47682564, 0.48251707, 0.44810577,\n",
       "        0.45820442, 0.50823069, 0.51161637, 0.49344755, 0.50515148,\n",
       "        0.47403829, 0.43646022, 0.52966844, 0.44416555, 0.50367754,\n",
       "        0.51831475, 0.47977351, 0.52982897, 0.47437394, 0.5177602 ,\n",
       "        0.44488063, 0.58853832, 0.52564065, 0.45747475, 0.5439554 ,\n",
       "        0.53055864, 0.44527465, 0.50858094, 0.5872541 , 0.51236063,\n",
       "        0.52695406, 0.57079271, 0.4911126 , 0.56567042, 0.49206118,\n",
       "        0.50696106, 0.47272488, 0.48625299, 0.50077345, 0.47853307,\n",
       "        0.58273014, 0.47640243, 0.47281245, 0.57556477, 0.51167474,\n",
       "        0.48075127, 0.49699375, 0.4651363 , 0.45696398, 0.53204717,\n",
       "        0.45754772, 0.5896912 , 0.51993462, 0.49706672, 0.5794904 ,\n",
       "        0.53102563, 0.51055105, 0.59265367, 0.49737318, 0.4868951 ,\n",
       "        0.50684432, 0.52696865, 0.48117448, 0.54213122, 0.5564036 ,\n",
       "        0.55201097, 0.56259121, 0.50671298, 0.50722375, 0.50853715,\n",
       "        0.51002568, 0.50398401, 0.50506392, 0.5075448 , 0.50598331,\n",
       "        0.50823069, 0.50731131, 0.50542875, 0.50696106, 0.50783667,\n",
       "        0.51489989, 0.50430506, 0.50694647, 0.50589574, 0.50738427,\n",
       "        0.50628977, 0.50588115, 0.50655245, 0.50585196, 0.50779289,\n",
       "        0.50538497, 0.51495826, 0.50591034, 0.51228767, 0.50518067,\n",
       "        0.51173312, 0.50545794, 0.50639192, 0.5091063 , 0.50804098,\n",
       "        0.50623139, 0.5068735 , 0.50720915, 0.50922305, 0.50734049,\n",
       "        0.50523904, 0.505779  , 0.50650867, 0.50360458, 0.50513689,\n",
       "        0.50649408, 0.50560388, 0.50412994, 0.50412994, 0.50542875,\n",
       "        0.57088028, 0.61665986, 0.6183381 , 0.60327768, 0.6563102 ,\n",
       "        0.59816998, 0.66273131, 0.62830541, 0.6147919 , 0.59530967,\n",
       "        0.5961269 , 0.60762653, 0.51984706, 0.62137353, 0.54156208,\n",
       "        0.66426362, 0.60489755, 0.66477439, 0.60610881, 0.63115113,\n",
       "        0.64939291, 0.62043955, 0.62270154, 0.60694063, 0.62270154,\n",
       "        0.5293182 , 0.54936956, 0.58359115, 0.62294962, 0.61992878,\n",
       "        0.6281011 , 0.64362851, 0.59961473, 0.60680929, 0.62468624,\n",
       "        0.59408382, 0.62586831, 0.62169459, 0.51069698, 0.614573  ,\n",
       "        0.6272109 , 0.62557644, 0.60943611, 0.62547429, 0.63491623,\n",
       "        0.65082307, 0.6296626 , 0.61614909]),\n",
       " 'split3_test_roc_auc_ovo': array([0.58227774, 0.58498482, 0.59876102, 0.62985231, 0.60527698,\n",
       "        0.59677631, 0.62862647, 0.58466377, 0.54671356, 0.61671823,\n",
       "        0.62684607, 0.6056856 , 0.56110268, 0.58008873, 0.57450674,\n",
       "        0.59322281, 0.59099002, 0.56161345, 0.57017979, 0.65595996,\n",
       "        0.48057615, 0.48228358, 0.69145117, 0.6292102 , 0.60120542,\n",
       "        0.56615201, 0.52817991, 0.59272664, 0.58596988, 0.53677544,\n",
       "        0.55628685, 0.58141673, 0.61371198, 0.61426653, 0.48048859,\n",
       "        0.61997256, 0.57817699, 0.56586014, 0.60059249, 0.58270095,\n",
       "        0.56972739, 0.60229992, 0.63351526, 0.62683148, 0.63078629,\n",
       "        0.625664  , 0.60850213, 0.60873563, 0.57287958, 0.60788921,\n",
       "        0.59661578, 0.6170028 , 0.59688576, 0.53851205, 0.57857831,\n",
       "        0.62977205, 0.61449273, 0.64899889, 0.61528807, 0.63703228,\n",
       "        0.57463079, 0.57951958, 0.63373417, 0.60495593, 0.64186271,\n",
       "        0.58397058, 0.58331388, 0.55754188, 0.55425836, 0.61654311,\n",
       "        0.61471893, 0.63072792, 0.46993754, 0.53861421, 0.57534586,\n",
       "        0.62006012, 0.60537914, 0.5699463 , 0.64028661, 0.56578717,\n",
       "        0.67078688, 0.58167941, 0.6312241 , 0.6021394 , 0.54680112,\n",
       "        0.58983714, 0.58982254, 0.62150487, 0.63201214, 0.60610881,\n",
       "        0.57994279, 0.62043955, 0.55028895, 0.59511996, 0.6039052 ,\n",
       "        0.56229934, 0.54637791, 0.51923414, 0.52402078, 0.52955169,\n",
       "        0.50696106, 0.49613274, 0.54236472, 0.47800771, 0.46878466,\n",
       "        0.5521715 , 0.50868309, 0.46522386, 0.51507501, 0.51075536,\n",
       "        0.47723425, 0.48771233, 0.49185687, 0.49566575, 0.45903625,\n",
       "        0.46199872, 0.53271846, 0.45257136, 0.54611523, 0.50340027,\n",
       "        0.48650108, 0.51022999, 0.5053266 , 0.4757895 , 0.50274356,\n",
       "        0.44741988, 0.49295137, 0.5699463 , 0.56596229, 0.45454147,\n",
       "        0.55291577, 0.5179937 , 0.52365595, 0.5159798 , 0.50685891,\n",
       "        0.47894168, 0.44723017, 0.47076937, 0.51848987, 0.49033915,\n",
       "        0.47198062, 0.51320705, 0.52029946, 0.46608488, 0.55160236,\n",
       "        0.55888448, 0.49252817, 0.52572821, 0.47310431, 0.54216041,\n",
       "        0.57495184, 0.52151071, 0.54296305, 0.55653494, 0.49784017,\n",
       "        0.54216041, 0.53006246, 0.53457183, 0.51247738, 0.53058782,\n",
       "        0.46020372, 0.50036484, 0.53992762, 0.52270737, 0.48559629,\n",
       "        0.48302784, 0.50055455, 0.55869476, 0.49724184, 0.45098068,\n",
       "        0.52377269, 0.49436694, 0.56098593, 0.51428697, 0.51739537,\n",
       "        0.46392505, 0.50598331, 0.51615492, 0.47815364, 0.50151772,\n",
       "        0.54977818, 0.48165606, 0.50182418, 0.56326251, 0.45906544,\n",
       "        0.52062051, 0.511091  , 0.53581227, 0.60167241, 0.57366762,\n",
       "        0.47160119, 0.53972331, 0.47613975, 0.48169984, 0.47945245,\n",
       "        0.47428638, 0.48057615, 0.47774502, 0.47870819, 0.47898546,\n",
       "        0.47746775, 0.47866441, 0.4802405 , 0.4809118 , 0.48066371,\n",
       "        0.47967136, 0.48117448, 0.4791314 , 0.48171444, 0.47765746,\n",
       "        0.47843091, 0.47838713, 0.48037184, 0.48064912, 0.4808972 ,\n",
       "        0.47684023, 0.47975892, 0.47466581, 0.48130582, 0.47740937,\n",
       "        0.48266301, 0.47945245, 0.47945245, 0.47840173, 0.47920437,\n",
       "        0.47504524, 0.47930652, 0.47843091, 0.47742397, 0.47692779,\n",
       "        0.47851848, 0.48034265, 0.47828498, 0.47618353, 0.47822661,\n",
       "        0.47806608, 0.47539548, 0.47897087, 0.47930652, 0.47812445,\n",
       "        0.56885179, 0.59148619, 0.56552449, 0.55145642, 0.58805674,\n",
       "        0.55919094, 0.57684899, 0.60806433, 0.6076849 , 0.60765571,\n",
       "        0.62045415, 0.60397817, 0.51503123, 0.57139105, 0.56518884,\n",
       "        0.56276633, 0.60375927, 0.58280311, 0.60804973, 0.57379896,\n",
       "        0.62325609, 0.57941743, 0.63167649, 0.58604285, 0.5677281 ,\n",
       "        0.51596521, 0.54186854, 0.59193859, 0.61397467, 0.57314226,\n",
       "        0.57236881, 0.61663067, 0.62432141, 0.5896912 , 0.61593019,\n",
       "        0.61180025, 0.56495534, 0.56635631, 0.53986924, 0.56762594,\n",
       "        0.57639659, 0.56374409, 0.60997607, 0.58891775, 0.59402545,\n",
       "        0.58084759, 0.58464188, 0.57846886]),\n",
       " 'split4_test_roc_auc_ovo': array([0.57160995, 0.56430594, 0.5426347 , 0.54805616, 0.57843237,\n",
       "        0.62386901, 0.57849804, 0.55107699, 0.57849804, 0.57630903,\n",
       "        0.57070515, 0.56587473, 0.57981875, 0.56670656, 0.56389732,\n",
       "        0.51473936, 0.59355846, 0.58153348, 0.5377532 , 0.54788103,\n",
       "        0.64012609, 0.59007063, 0.53457183, 0.58049734, 0.59027494,\n",
       "        0.57858561, 0.56067947, 0.51545444, 0.54127021, 0.55392271,\n",
       "        0.54700543, 0.59535345, 0.58246746, 0.55923472, 0.55746892,\n",
       "        0.56723192, 0.57654253, 0.53006246, 0.5786002 , 0.61282179,\n",
       "        0.56063569, 0.54970521, 0.57235421, 0.54802697, 0.54073026,\n",
       "        0.53493666, 0.52555309, 0.57266067, 0.60330687, 0.59316444,\n",
       "        0.5879254 , 0.56512317, 0.55546232, 0.63267614, 0.56778647,\n",
       "        0.61495243, 0.59856401, 0.57401786, 0.57489347, 0.59219398,\n",
       "        0.59050114, 0.47066721, 0.57714815, 0.57346331, 0.52295546,\n",
       "        0.56648036, 0.59511266, 0.58027844, 0.54001518, 0.49747534,\n",
       "        0.5781624 , 0.56588932, 0.5171035 , 0.62294962, 0.54469967,\n",
       "        0.55509019, 0.61096842, 0.52533419, 0.57645496, 0.62852431,\n",
       "        0.58677252, 0.5326309 , 0.56070866, 0.53822019, 0.58000117,\n",
       "        0.54170801, 0.60558345, 0.57635281, 0.5408616 , 0.54268577,\n",
       "        0.55901582, 0.57986983, 0.54140155, 0.59837429, 0.56107349,\n",
       "        0.53096725, 0.47765746, 0.54261281, 0.49964976, 0.52822369,\n",
       "        0.44237056, 0.55225906, 0.48476446, 0.50300625, 0.4851147 ,\n",
       "        0.53744673, 0.45594244, 0.47214115, 0.44079447, 0.48003619,\n",
       "        0.49257195, 0.53947522, 0.5348637 , 0.52466289, 0.52799019,\n",
       "        0.47819742, 0.45813146, 0.50442181, 0.5266476 , 0.5432841 ,\n",
       "        0.57489347, 0.5224155 , 0.54143074, 0.54243769, 0.50729671,\n",
       "        0.4331621 , 0.52083941, 0.49054346, 0.53273306, 0.5199784 ,\n",
       "        0.46367696, 0.47403829, 0.49798611, 0.55476913, 0.49681863,\n",
       "        0.49290759, 0.44098418, 0.48810636, 0.48685132, 0.47501605,\n",
       "        0.46945596, 0.48390345, 0.48669079, 0.51955519, 0.50469908,\n",
       "        0.49432316, 0.52832584, 0.51682622, 0.53468858, 0.59074193,\n",
       "        0.45840873, 0.44566867, 0.53051486, 0.53925632, 0.41836437,\n",
       "        0.53938766, 0.4911126 , 0.46564707, 0.50824529, 0.49997081,\n",
       "        0.45206059, 0.610122  , 0.53992762, 0.54488938, 0.54235013,\n",
       "        0.54278793, 0.5141994 , 0.51484152, 0.53315627, 0.52622439,\n",
       "        0.47730722, 0.55479832, 0.45813146, 0.51403888, 0.52377269,\n",
       "        0.47109042, 0.52537797, 0.54786644, 0.45353453, 0.58156266,\n",
       "        0.5361917 , 0.52782967, 0.54458292, 0.53266009, 0.55710408,\n",
       "        0.55468157, 0.51962816, 0.45334481, 0.53903742, 0.52763995,\n",
       "        0.521321  , 0.50007297, 0.53957737, 0.54685949, 0.54650925,\n",
       "        0.54049676, 0.5404092 , 0.53892067, 0.54135777, 0.54026327,\n",
       "        0.54220419, 0.53722783, 0.5366295 , 0.54255443, 0.54261281,\n",
       "        0.53722783, 0.53800128, 0.54056973, 0.54404296, 0.54119724,\n",
       "        0.54083241, 0.53657113, 0.53652735, 0.53998599, 0.53868718,\n",
       "        0.53579768, 0.53919795, 0.53889148, 0.53890608, 0.53776779,\n",
       "        0.53517016, 0.5410659 , 0.54100753, 0.53948981, 0.53989843,\n",
       "        0.53871636, 0.54008814, 0.53962115, 0.53927091, 0.53897904,\n",
       "        0.53992762, 0.53967953, 0.53792832, 0.53966493, 0.54096375,\n",
       "        0.53610414, 0.53935847, 0.54052595, 0.53735917, 0.5386288 ,\n",
       "        0.58595529, 0.59838889, 0.57744732, 0.54993871, 0.58741463,\n",
       "        0.53785535, 0.57264608, 0.5919094 , 0.53941685, 0.54055513,\n",
       "        0.55142724, 0.58713735, 0.53309789, 0.56304361, 0.58446676,\n",
       "        0.56889557, 0.58376627, 0.57871695, 0.56923122, 0.54248147,\n",
       "        0.56517425, 0.57918394, 0.5490631 , 0.60162863, 0.58602825,\n",
       "        0.58887397, 0.59910396, 0.5444078 , 0.59408382, 0.58324091,\n",
       "        0.53887689, 0.59335415, 0.59952717, 0.57546261, 0.55999358,\n",
       "        0.57048625, 0.57137645, 0.53490748, 0.59202615, 0.55201097,\n",
       "        0.57684899, 0.58274473, 0.59341253, 0.56418189, 0.58529858,\n",
       "        0.57617769, 0.53439671, 0.54332789]),\n",
       " 'mean_test_roc_auc_ovo': array([0.56723542, 0.5822581 , 0.58456537, 0.57378564, 0.56680116,\n",
       "        0.6028329 , 0.56570995, 0.56414434, 0.5766831 , 0.58280483,\n",
       "        0.60379099, 0.56812161, 0.57056099, 0.56861476, 0.55346109,\n",
       "        0.56242472, 0.59325908, 0.5822914 , 0.56515403, 0.5673299 ,\n",
       "        0.59059124, 0.55546729, 0.5989715 , 0.60428862, 0.58769118,\n",
       "        0.5667461 , 0.55735917, 0.54844014, 0.56672829, 0.563319  ,\n",
       "        0.56242599, 0.57656586, 0.58378995, 0.56598003, 0.57639633,\n",
       "        0.5931518 , 0.5686957 , 0.55450236, 0.58462155, 0.58933853,\n",
       "        0.57800793, 0.5810163 , 0.59738357, 0.56378149, 0.58806808,\n",
       "        0.55180904, 0.56189132, 0.59761837, 0.57604053, 0.5791489 ,\n",
       "        0.58073487, 0.57553614, 0.5675249 , 0.5889614 , 0.55698205,\n",
       "        0.60693992, 0.58012246, 0.60144741, 0.58565315, 0.58783746,\n",
       "        0.56197201, 0.55215692, 0.57744779, 0.5698901 , 0.56290314,\n",
       "        0.57918803, 0.56400825, 0.56811674, 0.56381676, 0.57277256,\n",
       "        0.599975  , 0.57544681, 0.5447532 , 0.58706637, 0.54565007,\n",
       "        0.56851777, 0.58119059, 0.56166052, 0.58645923, 0.60342843,\n",
       "        0.59890445, 0.59058509, 0.58172095, 0.58861431, 0.57569145,\n",
       "        0.57642168, 0.56716406, 0.60148476, 0.57923837, 0.58128549,\n",
       "        0.56198767, 0.57214878, 0.55189954, 0.5749765 , 0.5775664 ,\n",
       "        0.566693  , 0.52495264, 0.50304536, 0.49854922, 0.51556363,\n",
       "        0.48933918, 0.52905146, 0.52794476, 0.48620038, 0.4750973 ,\n",
       "        0.51856   , 0.48929681, 0.50637448, 0.48736982, 0.51536744,\n",
       "        0.48179879, 0.49697628, 0.5221306 , 0.5076873 , 0.5176344 ,\n",
       "        0.49567164, 0.4899153 , 0.47279772, 0.48923928, 0.50752222,\n",
       "        0.51659451, 0.49772082, 0.52006263, 0.50498805, 0.47813299,\n",
       "        0.49268027, 0.49126476, 0.49084432, 0.49465533, 0.476077  ,\n",
       "        0.49473261, 0.51469721, 0.48981078, 0.51411967, 0.49469656,\n",
       "        0.51102943, 0.46657043, 0.52647698, 0.48285479, 0.51786689,\n",
       "        0.48625422, 0.47976964, 0.50215905, 0.49433957, 0.51435639,\n",
       "        0.49088234, 0.51229195, 0.50734143, 0.50299143, 0.54864411,\n",
       "        0.53230415, 0.49951143, 0.51445933, 0.53592011, 0.47424714,\n",
       "        0.53577078, 0.52133915, 0.48598437, 0.53288537, 0.5068592 ,\n",
       "        0.49069879, 0.50783692, 0.51087806, 0.51690278, 0.52024324,\n",
       "        0.53118491, 0.49529374, 0.49954902, 0.52733871, 0.50489408,\n",
       "        0.49431545, 0.53482858, 0.48428112, 0.50199007, 0.52193912,\n",
       "        0.46875441, 0.5325225 , 0.5270753 , 0.48002407, 0.52796317,\n",
       "        0.53009934, 0.48481217, 0.53567789, 0.54138931, 0.49036518,\n",
       "        0.51601948, 0.50761749, 0.48643751, 0.53118888, 0.53827095,\n",
       "        0.51424579, 0.51917145, 0.51456361, 0.51772599, 0.51795993,\n",
       "        0.51374822, 0.5158732 , 0.51581039, 0.51605116, 0.51375179,\n",
       "        0.51449279, 0.51553979, 0.5163845 , 0.51612864, 0.52003308,\n",
       "        0.517408  , 0.51622468, 0.51508204, 0.51793194, 0.51590493,\n",
       "        0.5165901 , 0.51549133, 0.5149503 , 0.51613016, 0.5155003 ,\n",
       "        0.51506787, 0.5164585 , 0.51447198, 0.51668037, 0.5143613 ,\n",
       "        0.5149919 , 0.5164476 , 0.5162016 , 0.51522516, 0.51677822,\n",
       "        0.51478139, 0.51573991, 0.51666525, 0.51621657, 0.5157975 ,\n",
       "        0.51428508, 0.51498032, 0.5149974 , 0.5142939 , 0.51613795,\n",
       "        0.51333035, 0.51450768, 0.51464612, 0.51483938, 0.51441362,\n",
       "        0.57188727, 0.58602609, 0.55889466, 0.5720757 , 0.59327917,\n",
       "        0.56459708, 0.59631045, 0.58887947, 0.5812744 , 0.5618295 ,\n",
       "        0.5885368 , 0.58227754, 0.53379451, 0.57774282, 0.56880188,\n",
       "        0.57975717, 0.58081109, 0.58811871, 0.5836791 , 0.5686331 ,\n",
       "        0.60404448, 0.57787795, 0.57809249, 0.58803735, 0.55618005,\n",
       "        0.55829476, 0.55639561, 0.56743133, 0.59137544, 0.59271592,\n",
       "        0.56822053, 0.59710008, 0.58516855, 0.57503959, 0.59016696,\n",
       "        0.58325761, 0.58080255, 0.57326552, 0.54882706, 0.58117023,\n",
       "        0.57505892, 0.5723773 , 0.59134986, 0.58450413, 0.58941726,\n",
       "        0.59336619, 0.57463005, 0.57512563]),\n",
       " 'std_test_roc_auc_ovo': array([0.04076065, 0.01543952, 0.02300787, 0.03437943, 0.03126893,\n",
       "        0.01391264, 0.05178391, 0.02026895, 0.02528859, 0.02160381,\n",
       "        0.02224535, 0.030897  , 0.01671778, 0.01778395, 0.02778763,\n",
       "        0.02581748, 0.00672152, 0.03157736, 0.02210914, 0.04460333,\n",
       "        0.05881618, 0.04230259, 0.0574197 , 0.01543208, 0.01643726,\n",
       "        0.02694147, 0.04035147, 0.02609449, 0.0186576 , 0.0198637 ,\n",
       "        0.01340617, 0.02425871, 0.01648417, 0.03719971, 0.05456453,\n",
       "        0.02872194, 0.00998555, 0.02557531, 0.02534731, 0.02175798,\n",
       "        0.01727844, 0.03237163, 0.02943127, 0.03836907, 0.03871301,\n",
       "        0.0409661 , 0.03739705, 0.01508583, 0.04105577, 0.02363418,\n",
       "        0.01992646, 0.02195979, 0.02419201, 0.03717094, 0.02772921,\n",
       "        0.03161797, 0.03754494, 0.03044306, 0.03363799, 0.02950922,\n",
       "        0.01919089, 0.04363633, 0.03335813, 0.02396555, 0.05443257,\n",
       "        0.01673401, 0.03387502, 0.01893271, 0.01773609, 0.04318221,\n",
       "        0.03625931, 0.03319818, 0.04501178, 0.03678437, 0.02127751,\n",
       "        0.03443215, 0.02543285, 0.03212034, 0.03185165, 0.03555653,\n",
       "        0.04638387, 0.03166949, 0.03243735, 0.02804913, 0.01570063,\n",
       "        0.01947443, 0.03030693, 0.01536621, 0.03889357, 0.03528739,\n",
       "        0.01893415, 0.04436842, 0.01719165, 0.01972739, 0.01605159,\n",
       "        0.02339788, 0.03011933, 0.04088507, 0.01433772, 0.03000713,\n",
       "        0.04342325, 0.03876214, 0.03030018, 0.02837185, 0.02679238,\n",
       "        0.0305733 , 0.032598  , 0.03696367, 0.02519309, 0.02107396,\n",
       "        0.01060116, 0.04458286, 0.02067383, 0.03365235, 0.03212313,\n",
       "        0.03639017, 0.02650321, 0.01902971, 0.04923391, 0.02638266,\n",
       "        0.03033039, 0.03094352, 0.03873506, 0.02806006, 0.02504936,\n",
       "        0.04831491, 0.02250464, 0.04263623, 0.04883243, 0.02730059,\n",
       "        0.03427409, 0.02834681, 0.02840643, 0.02207284, 0.01567793,\n",
       "        0.03958521, 0.0309939 , 0.04213989, 0.02600923, 0.03590851,\n",
       "        0.01764516, 0.01979287, 0.039881  , 0.03535   , 0.04254211,\n",
       "        0.03879125, 0.04395776, 0.03090892, 0.03599433, 0.02284505,\n",
       "        0.03957023, 0.04669373, 0.01935583, 0.03570913, 0.04445542,\n",
       "        0.02086999, 0.02872474, 0.04303847, 0.02085245, 0.01627109,\n",
       "        0.03332388, 0.05261939, 0.02412111, 0.01764249, 0.0357093 ,\n",
       "        0.03492982, 0.01355501, 0.03962595, 0.02666825, 0.02769558,\n",
       "        0.01701994, 0.03381172, 0.03978283, 0.03123451, 0.01703154,\n",
       "        0.02685157, 0.02944726, 0.01283373, 0.01728154, 0.04349586,\n",
       "        0.01411962, 0.03117811, 0.03255269, 0.0244193 , 0.0348748 ,\n",
       "        0.02243041, 0.02052044, 0.03226809, 0.04327016, 0.02322367,\n",
       "        0.0259651 , 0.03424573, 0.02290292, 0.02268807, 0.02358133,\n",
       "        0.02368918, 0.02185947, 0.02349869, 0.02243719, 0.02278543,\n",
       "        0.02268664, 0.02251924, 0.02230197, 0.0224135 , 0.0232528 ,\n",
       "        0.02117199, 0.02146623, 0.02213566, 0.02270391, 0.02363765,\n",
       "        0.02306492, 0.02199183, 0.02144717, 0.02215017, 0.02126653,\n",
       "        0.02257367, 0.01999042, 0.02270649, 0.02029921, 0.0224913 ,\n",
       "        0.01967208, 0.02260182, 0.02259284, 0.02259675, 0.02224486,\n",
       "        0.02367043, 0.02234335, 0.02234894, 0.02286256, 0.02269641,\n",
       "        0.0219146 , 0.02084625, 0.02184837, 0.02333665, 0.02349244,\n",
       "        0.02183051, 0.02356808, 0.02241814, 0.02235586, 0.02292276,\n",
       "        0.01481497, 0.0218886 , 0.04207521, 0.01985167, 0.0337424 ,\n",
       "        0.02222603, 0.03536709, 0.02952792, 0.03150111, 0.03548181,\n",
       "        0.02219319, 0.02358387, 0.01775832, 0.02229528, 0.01872892,\n",
       "        0.04319757, 0.02455784, 0.04298717, 0.0322062 , 0.03405529,\n",
       "        0.0292733 , 0.0245896 , 0.0422639 , 0.01989651, 0.04996593,\n",
       "        0.03203515, 0.02396597, 0.02074523, 0.02653309, 0.01760749,\n",
       "        0.03537435, 0.03463098, 0.03406791, 0.02208516, 0.0265558 ,\n",
       "        0.01749558, 0.02510627, 0.02824428, 0.03350367, 0.02098575,\n",
       "        0.02921233, 0.03022328, 0.02759226, 0.02248092, 0.02516278,\n",
       "        0.02899066, 0.03223845, 0.02652002]),\n",
       " 'rank_test_roc_auc_ovo': array([105,  51,  43,  85, 107,   6, 112, 115,  72,  48,   4, 100,  92,\n",
       "         97, 136, 122,  18,  49, 113, 104,  23, 134,  10,   2,  36, 108,\n",
       "        130, 142, 109, 119, 121,  73,  45, 111,  75,  19,  95, 135,  42,\n",
       "         27,  67,  57,  13, 118,  33, 139, 125,  12,  76,  65,  60,  78,\n",
       "        102,  28, 131,   1,  61,   8,  40,  35, 124, 137,  71,  93, 120,\n",
       "         64, 116, 101, 117,  87,   9,  79, 144,  37, 143,  98,  55, 127,\n",
       "         38,   5,  11,  24,  52,  30,  77,  74, 106,   7,  63,  53, 123,\n",
       "         89, 138,  83,  70, 110, 164, 244, 250, 201, 268, 158, 160, 274,\n",
       "        284, 172, 269, 241, 271, 205, 279, 252, 165, 236, 177, 253, 266,\n",
       "        286, 270, 238, 183, 251, 169, 242, 282, 260, 261, 263, 257, 283,\n",
       "        255, 215, 267, 228, 256, 233, 288, 163, 278, 175, 273, 281, 246,\n",
       "        258, 224, 262, 232, 239, 245, 141, 154, 249, 221, 147, 285, 148,\n",
       "        167, 275, 152, 240, 264, 235, 234, 179, 168, 156, 254, 248, 161,\n",
       "        243, 259, 150, 277, 247, 166, 287, 153, 162, 280, 159, 157, 276,\n",
       "        149, 145, 265, 195, 237, 272, 155, 146, 227, 171, 217, 176, 173,\n",
       "        230, 197, 198, 194, 229, 219, 202, 187, 193, 170, 178, 188, 207,\n",
       "        174, 196, 184, 204, 212, 192, 203, 208, 185, 220, 181, 223, 210,\n",
       "        186, 190, 206, 180, 214, 200, 182, 189, 199, 226, 211, 209, 225,\n",
       "        191, 231, 218, 216, 213, 222,  91,  39, 128,  90,  17, 114,  15,\n",
       "         29,  54, 126,  31,  50, 151,  69,  94,  62,  58,  32,  46,  96,\n",
       "          3,  68,  66,  34, 133, 129, 132, 103,  21,  20,  99,  14,  41,\n",
       "         82,  25,  47,  59,  86, 140,  56,  81,  88,  22,  44,  26,  16,\n",
       "         84,  80]),\n",
       " 'split0_test_neg_log_loss': array([-0.28129647, -0.27866172, -0.26542702, -0.29957349, -0.28862479,\n",
       "        -0.29564165, -0.37109576, -0.36552344, -0.35602663, -0.38479527,\n",
       "        -0.4196557 , -0.45136212, -0.27132897, -0.26672065, -0.26605373,\n",
       "        -0.29860528, -0.29693971, -0.29069478, -0.32205489, -0.33489341,\n",
       "        -0.32114003, -0.35961819, -0.37434049, -0.34788995, -0.26443748,\n",
       "        -0.28119954, -0.26075892, -0.30177893, -0.29479489, -0.28763551,\n",
       "        -0.31999257, -0.33694857, -0.31195943, -0.3782205 , -0.35276409,\n",
       "        -0.35402889, -0.2704481 , -0.26397076, -0.27390236, -0.28546535,\n",
       "        -0.28129421, -0.29370864, -0.29710226, -0.26197964, -0.3022553 ,\n",
       "        -0.32960299, -0.26192539, -0.33165566, -0.29285127, -0.27479633,\n",
       "        -0.26290199, -0.55832831, -0.33034483, -0.31550325, -0.38213317,\n",
       "        -0.35338795, -0.33750049, -0.38833057, -0.38476625, -0.64419061,\n",
       "        -0.27438017, -0.2803048 , -0.26957355, -0.30442603, -0.29619014,\n",
       "        -0.31825332, -0.3719137 , -0.35866707, -0.3567142 , -0.80261502,\n",
       "        -0.45264832, -0.41187881, -0.27235153, -0.2780197 , -0.2754608 ,\n",
       "        -0.31203725, -0.3004276 , -0.29165879, -0.33809002, -0.30683067,\n",
       "        -0.33089975, -0.35777428, -0.39889625, -0.35524628, -0.26708453,\n",
       "        -0.26882327, -0.26896773, -0.27809824, -0.29301922, -0.29494478,\n",
       "        -0.32357754, -0.30345478, -0.33688829, -0.35438383, -0.33787439,\n",
       "        -0.38704161, -0.26317965, -0.59964405, -0.26320799, -0.26154229,\n",
       "        -0.64317618, -0.25942654, -0.26012835, -0.41901333, -0.26200767,\n",
       "        -0.26185793, -0.4645287 , -0.26092078, -0.26323651, -0.48412584,\n",
       "        -0.26455532, -0.26509508, -0.57541077, -0.26359496, -0.26047339,\n",
       "        -0.38558011, -0.26233566, -0.26307683, -0.42872262, -0.26298311,\n",
       "        -0.26278018, -0.50436279, -0.26259702, -0.26344843, -0.72947271,\n",
       "        -0.26037503, -0.26267835, -0.53613879, -0.26491674, -0.26362144,\n",
       "        -0.42295815, -0.26179224, -0.26640155, -0.45478993, -0.26309712,\n",
       "        -0.25983622, -0.64369218, -0.25980418, -0.26403033, -0.5826557 ,\n",
       "        -0.26279075, -0.26296269, -0.6192664 , -0.26322723, -0.26813655,\n",
       "        -0.94378648, -0.26471219, -0.26756553, -0.81149008, -0.26439546,\n",
       "        -0.2610309 , -0.89413058, -0.26319171, -0.26417689, -0.74496759,\n",
       "        -0.2593735 , -0.26410256, -0.69135329, -0.26381458, -0.26616757,\n",
       "        -0.52257003, -0.26668594, -0.26397988, -0.68890955, -0.26271657,\n",
       "        -0.26176758, -0.87430961, -0.26370603, -0.26482922, -1.17352028,\n",
       "        -0.26377065, -0.26203318, -0.46539584, -0.2672878 , -0.26189601,\n",
       "        -0.69547374, -0.26177818, -0.26333883, -0.61224038, -0.26418637,\n",
       "        -0.26214751, -0.8173984 , -0.26200907, -0.26046855, -0.68983691,\n",
       "        -0.26423763, -0.2623777 , -0.86706764, -0.26571939, -0.2629654 ,\n",
       "        -0.49575128, -0.26556334, -0.26167866, -0.2616475 , -0.26155071,\n",
       "        -0.26185909, -0.26150372, -0.26161764, -0.26160952, -0.26202406,\n",
       "        -0.26187868, -0.26172703, -0.26155897, -0.26182324, -0.26127706,\n",
       "        -0.26167676, -0.26149112, -0.26174924, -0.26144885, -0.26180997,\n",
       "        -0.26147353, -0.26148075, -0.26167331, -0.2616475 , -0.26176077,\n",
       "        -0.2615474 , -0.26145434, -0.26139317, -0.26165999, -0.26176386,\n",
       "        -0.26184794, -0.26154346, -0.26163366, -0.26172486, -0.26156414,\n",
       "        -0.26167034, -0.26171363, -0.26147111, -0.26156025, -0.26158847,\n",
       "        -0.26165181, -0.26149494, -0.26154764, -0.26155197, -0.26152592,\n",
       "        -0.26183286, -0.2616163 , -0.26163664, -0.2615162 , -0.26170673,\n",
       "        -0.2571596 , -0.25942883, -0.26373376, -0.26239565, -0.26426282,\n",
       "        -0.26825307, -0.25793319, -0.26421418, -0.25894701, -0.27622405,\n",
       "        -0.26139565, -0.26609602, -0.25891653, -0.25728691, -0.25543972,\n",
       "        -0.26353197, -0.25823643, -0.26055745, -0.27029027, -0.26753874,\n",
       "        -0.26147919, -0.27153347, -0.26547821, -0.26207129, -0.26452223,\n",
       "        -0.25592783, -0.26125257, -0.25905868, -0.2584489 , -0.26207074,\n",
       "        -0.26148223, -0.26905658, -0.27042693, -0.26357498, -0.26679381,\n",
       "        -0.26630716, -0.2560659 , -0.25620626, -0.25724836, -0.25718937,\n",
       "        -0.26015534, -0.2596582 , -0.25480734, -0.25959828, -0.26003131,\n",
       "        -0.25777945, -0.26402689, -0.25804451]),\n",
       " 'split1_test_neg_log_loss': array([-0.26316591, -0.27023542, -0.27267549, -0.32151747, -0.30933201,\n",
       "        -0.30482037, -0.40439508, -0.34121773, -0.33267628, -0.38712305,\n",
       "        -0.35198258, -0.39829988, -0.2825278 , -0.28545713, -0.28162968,\n",
       "        -0.31857597, -0.28952514, -0.31267719, -0.32338823, -0.34859911,\n",
       "        -0.33783027, -0.37077966, -0.3833657 , -0.3577319 , -0.27782262,\n",
       "        -0.26484445, -0.27519266, -0.29550498, -0.3028536 , -0.30922269,\n",
       "        -0.32719455, -0.32413489, -0.32789023, -0.35287967, -0.33474428,\n",
       "        -0.37922099, -0.27936913, -0.27207645, -0.27279127, -0.27476055,\n",
       "        -0.28518467, -0.27663452, -0.31620417, -0.30851012, -0.27818288,\n",
       "        -0.34421597, -0.34998656, -0.32359564, -0.40687969, -0.28355419,\n",
       "        -0.27787075, -0.30659708, -0.31664629, -0.31232564, -0.39249914,\n",
       "        -0.31916175, -0.37905838, -0.48508615, -0.4613672 , -0.44808826,\n",
       "        -0.28464085, -0.2844223 , -0.27897247, -0.32573497, -0.32747941,\n",
       "        -0.42517581, -0.35652864, -0.33638775, -0.34814308, -0.51953678,\n",
       "        -0.37423826, -0.44201589, -0.2790345 , -0.26467647, -0.27961579,\n",
       "        -0.29446766, -0.30664529, -0.32887558, -0.37164885, -0.35814839,\n",
       "        -0.34704891, -0.36674734, -0.42102971, -0.38856949, -0.26981626,\n",
       "        -0.2748426 , -0.2763518 , -0.29130523, -0.30726591, -0.29840952,\n",
       "        -0.32863145, -0.32807879, -0.3188509 , -0.38064692, -0.36369777,\n",
       "        -0.36144655, -0.26335189, -0.70169335, -0.26559647, -0.26330678,\n",
       "        -0.71974062, -0.26486224, -0.26409703, -0.58283814, -0.26454498,\n",
       "        -0.26374993, -0.40113131, -0.26442404, -0.26590932, -0.6455604 ,\n",
       "        -0.26701171, -0.26278187, -0.49289836, -0.26271222, -0.2639181 ,\n",
       "        -0.46429121, -0.26631939, -0.26567395, -0.54165274, -0.26357358,\n",
       "        -0.26566298, -0.65793601, -0.26471124, -0.2645285 , -0.47038695,\n",
       "        -0.26469956, -0.26572522, -0.49005053, -0.26598621, -0.26564898,\n",
       "        -0.39908349, -0.26255856, -0.26613959, -0.69460535, -0.26760873,\n",
       "        -0.26428693, -0.52967135, -0.26258365, -0.26451345, -0.55436266,\n",
       "        -0.2650546 , -0.26566735, -0.48364873, -0.26295545, -0.26326488,\n",
       "        -0.63424306, -0.2685014 , -0.26546036, -0.78786715, -0.26357128,\n",
       "        -0.2635607 , -0.45987218, -0.26834676, -0.26489181, -0.73803119,\n",
       "        -0.2664832 , -0.2707369 , -0.42088619, -0.26624441, -0.26599801,\n",
       "        -1.03593713, -0.2668924 , -0.26719505, -0.80060644, -0.26188474,\n",
       "        -0.26639406, -0.38388863, -0.26832342, -0.26847523, -0.73423069,\n",
       "        -0.26742599, -0.26165726, -0.75694528, -0.26397972, -0.26578294,\n",
       "        -0.60917039, -0.26538042, -0.26413217, -0.767094  , -0.26612086,\n",
       "        -0.26760833, -0.54317318, -0.26540128, -0.26393083, -0.57003607,\n",
       "        -0.26535069, -0.26963952, -0.54543206, -0.26625372, -0.26528774,\n",
       "        -0.48484541, -0.26460907, -0.26317825, -0.26312513, -0.26309687,\n",
       "        -0.26310143, -0.26312022, -0.26297381, -0.26310911, -0.2630048 ,\n",
       "        -0.26317384, -0.26293367, -0.26282773, -0.26301039, -0.26293035,\n",
       "        -0.26304587, -0.26308819, -0.26309923, -0.26305945, -0.2628577 ,\n",
       "        -0.26296338, -0.26305039, -0.2629762 , -0.26297781, -0.26304489,\n",
       "        -0.26298778, -0.2634956 , -0.26325604, -0.26326176, -0.26294131,\n",
       "        -0.2630716 , -0.26305004, -0.26293582, -0.26300301, -0.26297916,\n",
       "        -0.26302764, -0.26305329, -0.26306268, -0.26305173, -0.26310451,\n",
       "        -0.26325279, -0.2632337 , -0.26303482, -0.26305821, -0.26294961,\n",
       "        -0.26298486, -0.26301533, -0.26307406, -0.26302031, -0.26297854,\n",
       "        -0.26442828, -0.26199803, -0.2630668 , -0.26112983, -0.26548735,\n",
       "        -0.26311971, -0.26751841, -0.27314498, -0.26929946, -0.26961139,\n",
       "        -0.26618998, -0.27132554, -0.26338045, -0.262201  , -0.26145001,\n",
       "        -0.26893718, -0.26294144, -0.26575548, -0.2605809 , -0.27317394,\n",
       "        -0.26377275, -0.2713998 , -0.27356199, -0.26814208, -0.26520023,\n",
       "        -0.26136311, -0.26116148, -0.26421974, -0.26691936, -0.25888746,\n",
       "        -0.27602359, -0.26484318, -0.2654373 , -0.27255038, -0.26456734,\n",
       "        -0.26814762, -0.2621641 , -0.2616632 , -0.26408941, -0.26106314,\n",
       "        -0.26469131, -0.26319967, -0.26439036, -0.26090428, -0.26684627,\n",
       "        -0.26231734, -0.26365006, -0.26621886]),\n",
       " 'split2_test_neg_log_loss': array([-0.27385031, -0.27491195, -0.26470893, -0.30192534, -0.32428253,\n",
       "        -0.29668661, -0.35404192, -0.33924052, -0.31858691, -0.41860703,\n",
       "        -0.39524182, -0.37822404, -0.27075227, -0.27894578, -0.28452737,\n",
       "        -0.3008036 , -0.29771258, -0.28878501, -0.34990942, -0.35388561,\n",
       "        -0.30610465, -0.40039864, -0.39169176, -0.38982298, -0.26912568,\n",
       "        -0.27707254, -0.28330405, -0.30582035, -0.2916137 , -0.28825036,\n",
       "        -0.32285262, -0.31143141, -0.32783494, -0.26527364, -0.31925628,\n",
       "        -0.32624053, -0.27529614, -0.27973045, -0.26532326, -0.30069888,\n",
       "        -0.28101167, -0.27215523, -0.28574323, -0.32492484, -0.33939245,\n",
       "        -0.26514758, -0.34375651, -0.32786238, -0.27139642, -0.28243749,\n",
       "        -0.27897177, -0.37575373, -0.29899598, -0.2907936 , -0.329841  ,\n",
       "        -0.34162473, -0.38661981, -0.40049756, -0.37744291, -0.41029794,\n",
       "        -0.28106238, -0.28805281, -0.28039817, -0.30896968, -0.33816549,\n",
       "        -0.29904471, -0.33625158, -0.35852689, -0.35219487, -0.39573877,\n",
       "        -0.41437366, -0.42315989, -0.27548987, -0.26606425, -0.29391584,\n",
       "        -0.29306739, -0.29724336, -0.28383144, -0.33291415, -0.32190401,\n",
       "        -0.32036179, -0.35518868, -0.3662562 , -0.36246233, -0.26658511,\n",
       "        -0.26971331, -0.28328125, -0.28899382, -0.28426728, -0.26908444,\n",
       "        -0.32081975, -0.36097804, -0.33093698, -0.37907309, -0.36027778,\n",
       "        -0.33839529, -0.26321084, -0.47788837, -0.26524278, -0.26620115,\n",
       "        -0.47149435, -0.26427981, -0.26364994, -0.52434338, -0.26734629,\n",
       "        -0.2656753 , -0.54773059, -0.2627477 , -0.2675437 , -0.64353705,\n",
       "        -0.26571225, -0.26596883, -0.67799147, -0.26581668, -0.26379599,\n",
       "        -0.5703664 , -0.26478832, -0.26457724, -0.4862686 , -0.26421886,\n",
       "        -0.26537816, -0.46080896, -0.26177286, -0.26376456, -0.54842746,\n",
       "        -0.26336937, -0.26504596, -0.54832071, -0.26482849, -0.2662795 ,\n",
       "        -0.40762443, -0.26440431, -0.26549262, -1.02615524, -0.26578783,\n",
       "        -0.26587422, -0.49685224, -0.26391939, -0.26495989, -0.46279644,\n",
       "        -0.26397857, -0.26558461, -0.63931192, -0.26433785, -0.26666328,\n",
       "        -0.95313297, -0.26199951, -0.26445777, -0.67146678, -0.26362889,\n",
       "        -0.26487496, -0.58142795, -0.26863937, -0.26158827, -0.82382717,\n",
       "        -0.26554984, -0.26300713, -0.69275688, -0.26218985, -0.26778037,\n",
       "        -0.58847025, -0.26805386, -0.26848681, -0.56429701, -0.26943828,\n",
       "        -0.26091316, -0.48457428, -0.26914856, -0.26112998, -0.75414773,\n",
       "        -0.26856378, -0.26542197, -0.80557212, -0.268606  , -0.26486762,\n",
       "        -0.92072971, -0.26039168, -0.2649876 , -0.55062432, -0.26129513,\n",
       "        -0.26604057, -0.61082385, -0.26238926, -0.26784089, -0.85357063,\n",
       "        -0.26611423, -0.26488869, -0.92541276, -0.26445265, -0.26256067,\n",
       "        -0.85423791, -0.26259659, -0.26467391, -0.2646358 , -0.26455132,\n",
       "        -0.26458181, -0.26482389, -0.26474369, -0.26471422, -0.26478711,\n",
       "        -0.26483098, -0.2647472 , -0.26482843, -0.26483448, -0.26456471,\n",
       "        -0.26430252, -0.26470736, -0.26472938, -0.26482058, -0.2646249 ,\n",
       "        -0.26481279, -0.26484061, -0.26467448, -0.26487051, -0.26476373,\n",
       "        -0.26487807, -0.26426846, -0.26471635, -0.264498  , -0.26481504,\n",
       "        -0.26453974, -0.26473796, -0.26469673, -0.26473555, -0.26464504,\n",
       "        -0.26475811, -0.26477721, -0.26478688, -0.26434517, -0.26445797,\n",
       "        -0.26450574, -0.26473713, -0.26471092, -0.26479   , -0.26475417,\n",
       "        -0.26457982, -0.26462173, -0.26470861, -0.26474363, -0.26471063,\n",
       "        -0.26334259, -0.25675328, -0.25744903, -0.25936897, -0.25408057,\n",
       "        -0.26138539, -0.2526065 , -0.25738286, -0.25876637, -0.26410355,\n",
       "        -0.26600404, -0.2632253 , -0.26357584, -0.25618091, -0.26387803,\n",
       "        -0.25262333, -0.25861904, -0.25318289, -0.2612116 , -0.25840392,\n",
       "        -0.25644591, -0.26159517, -0.26186929, -0.26125752, -0.25678903,\n",
       "        -0.26483959, -0.26141476, -0.26441099, -0.25911692, -0.25637609,\n",
       "        -0.25511373, -0.25608057, -0.26150951, -0.26311181, -0.25954   ,\n",
       "        -0.26510374, -0.25604719, -0.25630086, -0.26552006, -0.25800314,\n",
       "        -0.25555601, -0.25681018, -0.26041567, -0.25578077, -0.25722111,\n",
       "        -0.25493784, -0.25611146, -0.26009372]),\n",
       " 'split3_test_neg_log_loss': array([-0.27477972, -0.27667595, -0.27446929, -0.29155714, -0.28443908,\n",
       "        -0.29493261, -0.32352933, -0.34605726, -0.35894119, -0.39789869,\n",
       "        -0.37199992, -0.36755191, -0.27726107, -0.27012896, -0.26903297,\n",
       "        -0.29830549, -0.29804128, -0.30234082, -0.34344756, -0.30898871,\n",
       "        -0.26555498, -0.26532165, -0.32273212, -0.34651045, -0.26650891,\n",
       "        -0.27307743, -0.285345  , -0.2886257 , -0.28972082, -0.3078401 ,\n",
       "        -0.343822  , -0.31813687, -0.31297634, -0.33737264, -0.2654932 ,\n",
       "        -0.37321762, -0.27506067, -0.27146819, -0.26707097, -0.28708713,\n",
       "        -0.28695378, -0.28862053, -0.27827493, -0.28951197, -0.28871115,\n",
       "        -0.31520628, -0.32724219, -0.32787872, -0.27176988, -0.27174519,\n",
       "        -0.27464512, -0.43194187, -0.29716041, -0.46803289, -0.54563765,\n",
       "        -0.33912544, -0.33965203, -0.37005614, -0.38499782, -0.37106694,\n",
       "        -0.30446545, -0.27601135, -0.26255926, -0.29283705, -0.32063118,\n",
       "        -0.3144089 , -0.33515172, -0.38529728, -0.35919601, -0.3639583 ,\n",
       "        -0.38922806, -0.36084757, -0.28807781, -0.31746093, -0.27705212,\n",
       "        -0.28222338, -0.30726092, -0.29743098, -0.29967185, -0.35453425,\n",
       "        -0.30175747, -0.4249742 , -0.37078695, -0.38668886, -0.2797832 ,\n",
       "        -0.26729634, -0.26991446, -0.27078837, -0.28065001, -0.27495908,\n",
       "        -0.32822606, -0.29816093, -0.34039247, -0.36121539, -0.3434343 ,\n",
       "        -0.35875506, -0.26346953, -0.71446812, -0.2642477 , -0.26403109,\n",
       "        -0.3724742 , -0.26520955, -0.26354081, -0.50680448, -0.26535599,\n",
       "        -0.26334201, -0.42739938, -0.26524581, -0.26461652, -0.66837482,\n",
       "        -0.26602492, -0.26524655, -0.73138451, -0.26547121, -0.2656058 ,\n",
       "        -0.4629247 , -0.2636256 , -0.26494291, -0.45761757, -0.26422858,\n",
       "        -0.26770777, -0.35534329, -0.26448505, -0.26497464, -0.5820348 ,\n",
       "        -0.26774499, -0.26449126, -0.45135672, -0.26281316, -0.26497206,\n",
       "        -0.47553331, -0.26478521, -0.26414805, -0.80271846, -0.26484544,\n",
       "        -0.26506952, -0.59871454, -0.26571367, -0.26407397, -0.58098414,\n",
       "        -0.26523028, -0.26409334, -0.47305384, -0.26602956, -0.26332982,\n",
       "        -0.84860073, -0.26677362, -0.26580086, -0.63877922, -0.26367304,\n",
       "        -0.26101385, -0.77963808, -0.26429494, -0.26227472, -0.87763823,\n",
       "        -0.26392227, -0.26453219, -0.94446317, -0.26554685, -0.26452858,\n",
       "        -0.57349734, -0.2661801 , -0.26353761, -0.51974809, -0.26795226,\n",
       "        -0.26681395, -0.48979635, -0.26240072, -0.26780251, -0.61122829,\n",
       "        -0.26859302, -0.2683239 , -0.59305988, -0.26551121, -0.26509142,\n",
       "        -0.64101643, -0.26695343, -0.26501344, -0.56163701, -0.26618875,\n",
       "        -0.26397597, -0.39396856, -0.26595038, -0.26274155, -0.52398575,\n",
       "        -0.26577574, -0.26449416, -0.60900376, -0.25958195, -0.26205068,\n",
       "        -0.80707299, -0.26270752, -0.26502885, -0.26467147, -0.2650454 ,\n",
       "        -0.26524756, -0.26489966, -0.26512245, -0.26511971, -0.26544485,\n",
       "        -0.26529582, -0.26524127, -0.26517066, -0.26513792, -0.26498916,\n",
       "        -0.26497953, -0.26503688, -0.26505707, -0.26485974, -0.26513807,\n",
       "        -0.26514706, -0.26512446, -0.26510701, -0.26516223, -0.26515437,\n",
       "        -0.26524032, -0.26488682, -0.26519353, -0.26486006, -0.26515119,\n",
       "        -0.26503132, -0.26500365, -0.26505754, -0.26506326, -0.26531383,\n",
       "        -0.26522023, -0.26512538, -0.26519221, -0.26500346, -0.26495582,\n",
       "        -0.26504977, -0.26500927, -0.2649131 , -0.26510692, -0.26501338,\n",
       "        -0.26494886, -0.26504625, -0.26504582, -0.26504697, -0.26511387,\n",
       "        -0.26386737, -0.26085474, -0.26239445, -0.26680724, -0.26195103,\n",
       "        -0.26646681, -0.26578442, -0.26282363, -0.26286337, -0.26358499,\n",
       "        -0.26242199, -0.26537771, -0.26491963, -0.26378751, -0.26198912,\n",
       "        -0.26816196, -0.26205717, -0.26359366, -0.26144594, -0.26688294,\n",
       "        -0.25910604, -0.26482872, -0.25960904, -0.26588933, -0.2621921 ,\n",
       "        -0.26437817, -0.26471707, -0.26351512, -0.25924785, -0.26348076,\n",
       "        -0.26273819, -0.26178958, -0.25926086, -0.26628591, -0.26316548,\n",
       "        -0.2625191 , -0.26274576, -0.26430669, -0.26313924, -0.26397981,\n",
       "        -0.26238437, -0.26324048, -0.26021281, -0.26161934, -0.26058682,\n",
       "        -0.26216739, -0.26177456, -0.2642386 ]),\n",
       " 'split4_test_neg_log_loss': array([-0.27959615, -0.27523481, -0.28625875, -0.31618271, -0.31010881,\n",
       "        -0.27859695, -0.32655591, -0.36542042, -0.34726217, -0.4232826 ,\n",
       "        -0.62876095, -0.42345917, -0.27045335, -0.27684832, -0.27782008,\n",
       "        -0.3201592 , -0.29266407, -0.29594151, -0.35693656, -0.37112964,\n",
       "        -0.31175159, -0.40331664, -0.26386727, -0.39717022, -0.27138736,\n",
       "        -0.27324579, -0.27391482, -0.31918142, -0.29975373, -0.30083712,\n",
       "        -0.322138  , -0.32220398, -0.31361446, -0.36179332, -0.37736301,\n",
       "        -0.34516128, -0.27215466, -0.27887216, -0.26897777, -0.28190649,\n",
       "        -0.29041677, -0.29147663, -0.3079452 , -0.31059837, -0.32731001,\n",
       "        -0.37493322, -0.36344825, -0.34247695, -0.2693321 , -0.27126455,\n",
       "        -0.28114973, -0.31564846, -0.3250179 , -0.293005  , -0.36133107,\n",
       "        -0.34550798, -0.3369089 , -0.40235964, -0.42164362, -0.3908474 ,\n",
       "        -0.27188512, -0.30228975, -0.27882995, -0.50950559, -0.32317687,\n",
       "        -0.31623704, -0.36368405, -0.34641867, -0.37325491, -0.49518745,\n",
       "        -0.39299566, -0.42005036, -0.28903982, -0.26585909, -0.27615652,\n",
       "        -0.3088119 , -0.2844674 , -0.31445177, -0.35058635, -0.31551737,\n",
       "        -0.34085119, -0.43368652, -0.37830025, -0.4104319 , -0.27414615,\n",
       "        -0.27697381, -0.26761571, -0.29161826, -0.30204826, -0.30898718,\n",
       "        -0.31860772, -0.33138789, -0.32533424, -0.34314781, -0.37046661,\n",
       "        -0.39744923, -0.26780025, -0.60576045, -0.26628404, -0.26372944,\n",
       "        -0.69529438, -0.26326804, -0.26477372, -0.69134975, -0.2654474 ,\n",
       "        -0.26322747, -0.34258325, -0.26486146, -0.26884159, -0.37834695,\n",
       "        -0.26528292, -0.2635317 , -0.51385905, -0.26390642, -0.26365274,\n",
       "        -0.56072009, -0.26514433, -0.2641939 , -0.5183298 , -0.26334083,\n",
       "        -0.26253737, -0.82721158, -0.26379281, -0.26369666, -0.53069932,\n",
       "        -0.26658951, -0.26392147, -0.53743975, -0.26341295, -0.26363638,\n",
       "        -0.46552254, -0.26532392, -0.26515608, -0.74204722, -0.26599143,\n",
       "        -0.2649748 , -0.48343968, -0.2654924 , -0.26507222, -0.47989554,\n",
       "        -0.26533875, -0.26472826, -0.44655871, -0.26395199, -0.26949637,\n",
       "        -0.89237637, -0.26579224, -0.26569555, -0.93784972, -0.26035703,\n",
       "        -0.26807349, -0.50907086, -0.26554576, -0.2637801 , -0.54962431,\n",
       "        -0.26411177, -0.27007298, -0.7809069 , -0.26746614, -0.26765292,\n",
       "        -1.10824521, -0.25880415, -0.2628019 , -0.82089043, -0.26377115,\n",
       "        -0.26345735, -0.54159146, -0.26456675, -0.26586579, -0.4613405 ,\n",
       "        -0.26933488, -0.26315568, -0.41251204, -0.26645806, -0.26522797,\n",
       "        -0.72418467, -0.26527912, -0.26355806, -0.78665919, -0.26110799,\n",
       "        -0.26496312, -0.6658218 , -0.26449693, -0.26422505, -1.09196829,\n",
       "        -0.26330027, -0.26545864, -0.73012944, -0.26421837, -0.26457153,\n",
       "        -0.65839494, -0.26713119, -0.26372425, -0.26341099, -0.26333598,\n",
       "        -0.2636008 , -0.2636811 , -0.263651  , -0.26359037, -0.26356348,\n",
       "        -0.2635041 , -0.26376282, -0.26371676, -0.26352558, -0.26363146,\n",
       "        -0.26372158, -0.26372617, -0.26356065, -0.26347312, -0.26352977,\n",
       "        -0.26358894, -0.26375434, -0.26367997, -0.26361909, -0.26366531,\n",
       "        -0.2637446 , -0.26385547, -0.26372881, -0.26361711, -0.26370151,\n",
       "        -0.26380127, -0.2635734 , -0.26356779, -0.2636224 , -0.26365498,\n",
       "        -0.26364912, -0.26357856, -0.26367013, -0.26363532, -0.26385597,\n",
       "        -0.26373491, -0.26362497, -0.2636828 , -0.26362851, -0.26354919,\n",
       "        -0.26369335, -0.26359239, -0.26357784, -0.26365334, -0.26361821,\n",
       "        -0.26057405, -0.25982596, -0.26061779, -0.26713883, -0.26056617,\n",
       "        -0.2737966 , -0.26465235, -0.26269763, -0.27194593, -0.27717854,\n",
       "        -0.27194442, -0.26668655, -0.26491592, -0.26240452, -0.25996993,\n",
       "        -0.263427  , -0.26152264, -0.26306902, -0.26759291, -0.27785769,\n",
       "        -0.26867487, -0.26536581, -0.27312573, -0.26507165, -0.26082795,\n",
       "        -0.26152977, -0.25949017, -0.27099941, -0.26049201, -0.26074501,\n",
       "        -0.27360708, -0.26375018, -0.26328058, -0.26669033, -0.27241632,\n",
       "        -0.26794103, -0.26000499, -0.26388177, -0.2602631 , -0.26431659,\n",
       "        -0.26115811, -0.26205916, -0.26170235, -0.26510458, -0.26007793,\n",
       "        -0.26261516, -0.26903591, -0.27088185]),\n",
       " 'mean_test_neg_log_loss': array([-0.27453771, -0.27514397, -0.27270789, -0.30615123, -0.30335745,\n",
       "        -0.29413564, -0.3559236 , -0.35149187, -0.34269864, -0.40234133,\n",
       "        -0.43352819, -0.40377943, -0.27446469, -0.27562017, -0.27581277,\n",
       "        -0.30728991, -0.29497656, -0.29808786, -0.33914733, -0.3434993 ,\n",
       "        -0.3084763 , -0.35988696, -0.34719947, -0.3678251 , -0.26985641,\n",
       "        -0.27388795, -0.27570309, -0.30218227, -0.29574735, -0.29875716,\n",
       "        -0.32719995, -0.32257115, -0.31885508, -0.33910795, -0.32992417,\n",
       "        -0.35557386, -0.27446574, -0.2732236 , -0.26961313, -0.28598368,\n",
       "        -0.28497222, -0.28451911, -0.29705396, -0.29910499, -0.30717036,\n",
       "        -0.32582121, -0.32927178, -0.33069387, -0.30244587, -0.27675955,\n",
       "        -0.27510787, -0.39765389, -0.31363308, -0.33593208, -0.40228841,\n",
       "        -0.33976157, -0.35594792, -0.40926601, -0.40604356, -0.45289823,\n",
       "        -0.28328679, -0.2862162 , -0.27406668, -0.34829466, -0.32112862,\n",
       "        -0.33462396, -0.35270594, -0.35705953, -0.35790061, -0.51540726,\n",
       "        -0.40469679, -0.41159051, -0.28079871, -0.27841609, -0.28044021,\n",
       "        -0.29812151, -0.29920891, -0.30324971, -0.33858224, -0.33138694,\n",
       "        -0.32818382, -0.3876742 , -0.38705387, -0.38067977, -0.27148305,\n",
       "        -0.27152986, -0.27322619, -0.28416078, -0.29345014, -0.289277  ,\n",
       "        -0.3239725 , -0.32441209, -0.33048058, -0.36369341, -0.35515017,\n",
       "        -0.36861755, -0.26420243, -0.61989087, -0.2649158 , -0.26376215,\n",
       "        -0.58043594, -0.26340924, -0.26323797, -0.54486981, -0.26494047,\n",
       "        -0.26357053, -0.43667465, -0.26363996, -0.26602953, -0.56398901,\n",
       "        -0.26571742, -0.26452481, -0.59830883, -0.2643003 , -0.2634892 ,\n",
       "        -0.4887765 , -0.26444266, -0.26449297, -0.48651827, -0.26366899,\n",
       "        -0.26481329, -0.56113253, -0.2634718 , -0.26408256, -0.57220425,\n",
       "        -0.26455569, -0.26437245, -0.5126613 , -0.26439151, -0.26483167,\n",
       "        -0.43414439, -0.26377285, -0.26546758, -0.74406324, -0.26546611,\n",
       "        -0.26400834, -0.550474  , -0.26350266, -0.26452997, -0.5321389 ,\n",
       "        -0.26447859, -0.26460725, -0.53236792, -0.26410042, -0.26617818,\n",
       "        -0.85442792, -0.26555579, -0.26579602, -0.76949059, -0.26312514,\n",
       "        -0.26371078, -0.64482793, -0.26600371, -0.26334236, -0.7468177 ,\n",
       "        -0.26388812, -0.26649035, -0.70607328, -0.26505237, -0.26642549,\n",
       "        -0.76574399, -0.26532329, -0.26520025, -0.6788903 , -0.2651526 ,\n",
       "        -0.26386922, -0.55483207, -0.2656291 , -0.26562055, -0.7468935 ,\n",
       "        -0.26753766, -0.2641184 , -0.60669703, -0.26636856, -0.26457319,\n",
       "        -0.71811499, -0.26395657, -0.26420602, -0.65565098, -0.26377982,\n",
       "        -0.2649471 , -0.60623716, -0.26404938, -0.26384137, -0.74587953,\n",
       "        -0.26495571, -0.26537174, -0.73540913, -0.26404522, -0.2634872 ,\n",
       "        -0.6600605 , -0.26452154, -0.26365678, -0.26349818, -0.26351606,\n",
       "        -0.26367814, -0.26360572, -0.26362172, -0.26362859, -0.26376486,\n",
       "        -0.26373668, -0.2636824 , -0.26362051, -0.26366632, -0.26347855,\n",
       "        -0.26354525, -0.26360994, -0.26363911, -0.26353235, -0.26359208,\n",
       "        -0.26359714, -0.26365011, -0.26362219, -0.26365543, -0.26367781,\n",
       "        -0.26367963, -0.26359214, -0.26365758, -0.26357938, -0.26367458,\n",
       "        -0.26365837, -0.2635817 , -0.26357831, -0.26362981, -0.26363143,\n",
       "        -0.26366509, -0.26364961, -0.2636366 , -0.26351918, -0.26359255,\n",
       "        -0.26363901, -0.26362   , -0.26357786, -0.26362712, -0.26355845,\n",
       "        -0.26360795, -0.2635784 , -0.26360859, -0.26359609, -0.2636256 ,\n",
       "        -0.26187438, -0.25977217, -0.26145236, -0.2633681 , -0.26126959,\n",
       "        -0.26660432, -0.26169898, -0.26405266, -0.26436443, -0.27014051,\n",
       "        -0.26559121, -0.26654223, -0.26314168, -0.26037217, -0.26054536,\n",
       "        -0.26333629, -0.26067534, -0.2612317 , -0.26422432, -0.26877145,\n",
       "        -0.26189575, -0.26694459, -0.26672885, -0.26448638, -0.26190631,\n",
       "        -0.26160769, -0.26160721, -0.26444079, -0.26084501, -0.26031201,\n",
       "        -0.26579297, -0.26310402, -0.26398304, -0.26644268, -0.26529659,\n",
       "        -0.26600373, -0.25940559, -0.26047175, -0.26205203, -0.26091041,\n",
       "        -0.26078903, -0.26099354, -0.26030571, -0.26060145, -0.26095269,\n",
       "        -0.25996343, -0.26291978, -0.26389551]),\n",
       " 'std_test_neg_log_loss': array([0.0063422 , 0.00278846, 0.00779389, 0.01105318, 0.01479251,\n",
       "        0.0085443 , 0.02998356, 0.01162822, 0.01512683, 0.01588904,\n",
       "        0.10020932, 0.03048587, 0.00474424, 0.00661345, 0.00714163,\n",
       "        0.00991169, 0.0033406 , 0.00868037, 0.01408028, 0.02079597,\n",
       "        0.02400257, 0.05017699, 0.04809803, 0.02144127, 0.00462404,\n",
       "        0.00541212, 0.0086894 , 0.01030278, 0.00491657, 0.00927833,\n",
       "        0.00863437, 0.00840294, 0.00737354, 0.03921376, 0.0375779 ,\n",
       "        0.0191924 , 0.00305186, 0.00573163, 0.00327925, 0.00849242,\n",
       "        0.00354481, 0.00854059, 0.01389919, 0.02171422, 0.02301757,\n",
       "        0.03621774, 0.03562751, 0.00642018, 0.05291519, 0.00524593,\n",
       "        0.00645391, 0.09218219, 0.01344299, 0.06679138, 0.07480267,\n",
       "        0.01137478, 0.02210523, 0.03962066, 0.03167873, 0.09897208,\n",
       "        0.01152947, 0.00898926, 0.00692029, 0.08129497, 0.01383444,\n",
       "        0.04578263, 0.01471711, 0.01638693, 0.00856277, 0.15503002,\n",
       "        0.02719158, 0.02722518, 0.00668659, 0.02011805, 0.00688325,\n",
       "        0.01095059, 0.00828004, 0.01629385, 0.02359613, 0.02096069,\n",
       "        0.0160204 , 0.03433852, 0.02033903, 0.01981052, 0.00494165,\n",
       "        0.00371911, 0.00585582, 0.00831374, 0.01013072, 0.01494497,\n",
       "        0.00396674, 0.02248336, 0.00776205, 0.01441448, 0.01240694,\n",
       "        0.02113191, 0.0018019 , 0.08533559, 0.00107752, 0.0014938 ,\n",
       "        0.13538903, 0.00209724, 0.00161417, 0.09012006, 0.00173063,\n",
       "        0.00123005, 0.06828151, 0.00160472, 0.00200062, 0.11377107,\n",
       "        0.00081352, 0.00117944, 0.09254827, 0.00117005, 0.00166613,\n",
       "        0.06892043, 0.00135948, 0.00086013, 0.0405646 , 0.00049045,\n",
       "        0.00193558, 0.16480644, 0.00112406, 0.00057391, 0.08658495,\n",
       "        0.00257737, 0.00103644, 0.0366231 , 0.00113654, 0.00106558,\n",
       "        0.03083938, 0.00135844, 0.00079533, 0.1839269 , 0.00148105,\n",
       "        0.00214597, 0.06134887, 0.00217001, 0.00043283, 0.05093024,\n",
       "        0.00097319, 0.00100622, 0.08030355, 0.00108383, 0.0025172 ,\n",
       "        0.11634694, 0.00217138, 0.00100448, 0.10690237, 0.00141627,\n",
       "        0.00264069, 0.16551964, 0.00216673, 0.0012252 , 0.11136456,\n",
       "        0.00244658, 0.00324152, 0.16979723, 0.00185569, 0.00119915,\n",
       "        0.25212288, 0.00331687, 0.0022265 , 0.12127416, 0.00299087,\n",
       "        0.00238206, 0.16770729, 0.00264179, 0.00259853, 0.23755797,\n",
       "        0.00197988, 0.00247862, 0.15491007, 0.00156849, 0.00137221,\n",
       "        0.10900945, 0.00245811, 0.00069859, 0.10132759, 0.00222536,\n",
       "        0.0018471 , 0.13942415, 0.00158497, 0.00239737, 0.20720683,\n",
       "        0.0010418 , 0.00237442, 0.14528098, 0.00235784, 0.00123386,\n",
       "        0.15298246, 0.00172623, 0.00118858, 0.00111737, 0.00122323,\n",
       "        0.00117705, 0.00125044, 0.00126079, 0.00124499, 0.00122605,\n",
       "        0.00122077, 0.0012613 , 0.00132285, 0.00121467, 0.00131394,\n",
       "        0.00113158, 0.00126615, 0.001189  , 0.00126409, 0.00119799,\n",
       "        0.00132601, 0.00131664, 0.00122725, 0.00128368, 0.00121944,\n",
       "        0.00133426, 0.00116469, 0.00132469, 0.00111985, 0.00123857,\n",
       "        0.00112287, 0.00124868, 0.0012356 , 0.00120824, 0.00130825,\n",
       "        0.00126481, 0.00122948, 0.00132385, 0.00117938, 0.0011774 ,\n",
       "        0.00117037, 0.00125245, 0.00122429, 0.00127844, 0.00126874,\n",
       "        0.00112152, 0.00121739, 0.00122073, 0.00127143, 0.00122418,\n",
       "        0.00270444, 0.00175276, 0.00225486, 0.00309821, 0.00398472,\n",
       "        0.00433234, 0.00558808, 0.00510939, 0.00538096, 0.00576616,\n",
       "        0.0037024 , 0.00266301, 0.00220936, 0.00304056, 0.00284282,\n",
       "        0.00582187, 0.00189416, 0.00435102, 0.00395509, 0.00655001,\n",
       "        0.0041746 , 0.00391133, 0.00571809, 0.00252718, 0.00300413,\n",
       "        0.0031764 , 0.00170379, 0.00381809, 0.00310808, 0.00248364,\n",
       "        0.00784457, 0.00424103, 0.00380918, 0.00336774, 0.00426952,\n",
       "        0.00206853, 0.00288298, 0.00355936, 0.00295417, 0.00294432,\n",
       "        0.00302308, 0.00246313, 0.00312691, 0.00302163, 0.00317512,\n",
       "        0.00308079, 0.00416727, 0.004539  ]),\n",
       " 'rank_test_neg_log_loss': array([172, 174, 165, 203, 202, 190, 235, 231, 227, 248, 254, 249, 170,\n",
       "        175, 177, 205, 191, 194, 225, 228, 206, 239, 229, 241, 161, 168,\n",
       "        176, 199, 192, 196, 214, 210, 208, 224, 217, 234, 171, 166, 160,\n",
       "        186, 185, 184, 193, 197, 204, 213, 216, 219, 200, 178, 173, 246,\n",
       "        207, 222, 247, 226, 236, 252, 251, 257, 182, 187, 169, 230, 209,\n",
       "        221, 232, 237, 238, 261, 250, 253, 181, 179, 180, 195, 198, 201,\n",
       "        223, 220, 215, 245, 244, 243, 163, 164, 167, 183, 189, 188, 211,\n",
       "        212, 218, 240, 233, 242, 107, 274, 127,  90, 270,  34,  30, 264,\n",
       "        128,  46, 256,  73, 148, 268, 143, 120, 271, 110,  38, 259, 115,\n",
       "        118, 258,  82, 125, 267,  35, 104, 269, 122, 112, 260, 113, 126,\n",
       "        255,  92, 138, 282, 137, 100, 265,  40, 121, 262, 116, 124, 263,\n",
       "        105, 149, 288, 139, 145, 287,  28,  88, 275, 146,  32, 284,  96,\n",
       "        153, 279, 131, 151, 286, 135, 133, 278, 132,  95, 266, 142, 141,\n",
       "        285, 158, 106, 273, 150, 123, 280,  98, 108, 276,  93, 129, 272,\n",
       "        102,  94, 283, 130, 136, 281, 101,  37, 277, 119,  77,  39,  41,\n",
       "         85,  57,  63,  67,  91,  89,  87,  62,  81,  36,  44,  60,  72,\n",
       "         43,  52,  56,  75,  64,  76,  84,  86,  53,  78,  50,  83,  79,\n",
       "         51,  48,  68,  69,  80,  74,  70,  42,  54,  71,  61,  47,  66,\n",
       "         45,  58,  49,  59,  55,  65,  22,   2,  18,  33,  17, 155,  21,\n",
       "        103, 111, 162, 140, 154,  29,   6,   8,  31,  10,  16, 109, 159,\n",
       "         23, 157, 156, 117,  24,  20,  19, 114,  12,   5, 144,  27,  99,\n",
       "        152, 134, 147,   1,   7,  25,  13,  11,  15,   4,   9,  14,   3,\n",
       "         26,  97])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRIAL_3_MLP.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL THREE NEG LOG LOSS RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'tanh',\n",
       " 'classifier__alpha': 0.1,\n",
       " 'classifier__hidden_layer_sizes': (5,),\n",
       " 'classifier__learning_rate': 'constant',\n",
       " 'classifier__solver': 'adam'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL THREE NEG LOG LOSS RESULTS ---------------------------------------------\")\n",
    "TRIAL_3_MLP.cv_results_['rank_test_neg_log_loss']\n",
    "TRIAL_3_MLP.cv_results_['params'][ np.argmin(TRIAL_3_MLP.cv_results_['rank_test_neg_log_loss']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL THREE F1 RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.001,\n",
       " 'classifier__hidden_layer_sizes': (5,),\n",
       " 'classifier__learning_rate': 'invscaling',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL THREE F1 RESULTS ---------------------------------------------\")\n",
    "TRIAL_3_MLP.cv_results_['rank_test_f1_micro']\n",
    "TRIAL_3_MLP.cv_results_['params'][ np.argmin(TRIAL_3_MLP.cv_results_['rank_test_f1_micro']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL THREE ROC AUC RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'tanh',\n",
       " 'classifier__alpha': 0.0001,\n",
       " 'classifier__hidden_layer_sizes': (15,),\n",
       " 'classifier__learning_rate': 'invscaling',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL THREE ROC AUC RESULTS ---------------------------------------------\")\n",
    "TRIAL_3_MLP.cv_results_['rank_test_roc_auc_ovo']\n",
    "TRIAL_3_MLP.cv_results_['params'][ np.argmin(TRIAL_3_MLP.cv_results_['rank_test_roc_auc_ovo']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 3 MLP using best NEG LOG LOSS hyperparameters :0.9319\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 3 MLP using best F1 hyperparameters :0.9292\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 3 MLP using best ROC_AUC hyperparameters :0.9219\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best NEG LOG LOSS results to train on test set, \n",
    "p = TRIAL_3_MLP.cv_results_['params'][ np.argmin(TRIAL_3_MLP.cv_results_['rank_test_neg_log_loss']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train__neg_log_loss = pipe.predict(X_train)\n",
    "y_pred_test_neg_log_loss = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 3 MLP using best NEG LOG LOSS hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_neg_log_loss)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best F1 results to train on test set, LR_Trial 1\n",
    "p = TRIAL_3_MLP.cv_results_['params'][ np.argmin(TRIAL_3_MLP.cv_results_['rank_test_f1_micro']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train_roc_auc = pipe.predict(X_train)\n",
    "y_pred_test_f1 = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 3 MLP using best F1 hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_f1)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best ROC_AUC results to train on test set, LR_Trial 1\n",
    "p  = TRIAL_3_MLP.cv_results_['params'][ np.argmin(TRIAL_3_MLP.cv_results_['rank_test_roc_auc_ovo']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train_roc_auc = pipe.predict(X_train)\n",
    "y_pred_test_roc_auc = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 3 MLP using best ROC_AUC hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_roc_auc)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTI LAYERED PERCEPTRON TRIAL FOUR ON POKER DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    8.8s\n",
      "[Parallel(n_jobs=-1)]: Done 189 tasks      | elapsed:    9.6s\n",
      "[Parallel(n_jobs=-1)]: Done 210 tasks      | elapsed:   10.4s\n",
      "[Parallel(n_jobs=-1)]: Done 233 tasks      | elapsed:   11.9s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:   13.0s\n",
      "[Parallel(n_jobs=-1)]: Done 281 tasks      | elapsed:   14.8s\n",
      "[Parallel(n_jobs=-1)]: Done 306 tasks      | elapsed:   16.6s\n",
      "[Parallel(n_jobs=-1)]: Done 333 tasks      | elapsed:   18.3s\n",
      "[Parallel(n_jobs=-1)]: Done 360 tasks      | elapsed:   20.0s\n",
      "[Parallel(n_jobs=-1)]: Done 389 tasks      | elapsed:   21.2s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:   23.2s\n",
      "[Parallel(n_jobs=-1)]: Done 449 tasks      | elapsed:   24.5s\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:   26.8s\n",
      "[Parallel(n_jobs=-1)]: Done 513 tasks      | elapsed:   28.8s\n",
      "[Parallel(n_jobs=-1)]: Done 546 tasks      | elapsed:   31.8s\n",
      "[Parallel(n_jobs=-1)]: Done 581 tasks      | elapsed:   33.8s\n",
      "[Parallel(n_jobs=-1)]: Done 616 tasks      | elapsed:   36.5s\n",
      "[Parallel(n_jobs=-1)]: Done 653 tasks      | elapsed:   39.3s\n",
      "[Parallel(n_jobs=-1)]: Done 690 tasks      | elapsed:   41.8s\n",
      "[Parallel(n_jobs=-1)]: Done 729 tasks      | elapsed:   44.7s\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:   47.6s\n",
      "[Parallel(n_jobs=-1)]: Done 809 tasks      | elapsed:   51.0s\n",
      "[Parallel(n_jobs=-1)]: Done 850 tasks      | elapsed:   54.4s\n",
      "[Parallel(n_jobs=-1)]: Done 893 tasks      | elapsed:   57.9s\n",
      "[Parallel(n_jobs=-1)]: Done 936 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 981 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1026 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1073 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1120 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1169 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1269 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1320 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1373 tasks      | elapsed:  1.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FOUR RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1440 out of 1440 | elapsed:  1.7min finished\n"
     ]
    }
   ],
   "source": [
    "ITERATIONS = 1\n",
    "for i in range(1):\n",
    "    #For reach \"trial\", randomly select 5000 data points\n",
    "    random5000DataPoints = pokerData.sample(n = 5000, replace = True)\n",
    "    #From that 5000, create x set and y set\n",
    "    xSet = random5000DataPoints.iloc[:,0:9].values\n",
    "    ySet = random5000DataPoints.iloc[:,10].values\n",
    "    # Run GridSearch to find best hyperparameters per 5000 data points \n",
    "    \n",
    "    pipe = Pipeline([('std', StandardScaler()),('classifier', MLPClassifier(max_iter = 250))])\n",
    "\n",
    "    search_space = [\n",
    "                {\n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                {\n",
    "                 \n",
    "                  \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['sgd'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                \n",
    "                {\n",
    "                \n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['adam'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                ]\n",
    "    \n",
    "    #'precision_micro','roc_auc_ovo','jaccard','neg_log_loss'] JACCARD DOES NOT WORK FOR NON BINARY?\n",
    "    runHyperParamSearch = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=5), \n",
    "                          scoring=['recall_micro','f1_micro','roc_auc_ovo','neg_log_loss'], \n",
    "                          refit=False, verbose=10, n_jobs = -1)\n",
    "    TRIAL_4_MLP = runHyperParamSearch.fit(xSet, ySet)\n",
    "    print(\"---------------------------------TRIAL FOUR RESULTS ---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.30736609, 0.33458924, 0.2959558 , 0.5821012 , 0.68619223,\n",
       "        0.76826181, 1.11826224, 1.17621202, 1.25608163, 1.43193178,\n",
       "        1.38388968, 0.8518311 , 0.31306891, 0.29635463, 0.34909973,\n",
       "        0.49893231, 0.5278554 , 0.53235755, 0.99265485, 1.02938967,\n",
       "        1.1647018 , 1.27629623, 0.80949712, 1.33775015, 0.27573729,\n",
       "        0.26412706, 0.30335979, 0.52745352, 0.56498718, 0.57569723,\n",
       "        1.09013772, 0.95201774, 1.15779629, 0.88045926, 1.4051105 ,\n",
       "        1.37858577, 0.28464484, 0.2403069 , 0.24551101, 0.63474574,\n",
       "        0.56708708, 0.53666282, 0.94841757, 1.09324079, 1.01216908,\n",
       "        1.23866458, 0.60842347, 1.03208904, 0.39173679, 0.49452419,\n",
       "        0.53676195, 0.71791816, 0.71301346, 0.74193821, 1.2981163 ,\n",
       "        1.34095354, 1.29791431, 1.598175  , 1.59447136, 1.90243573,\n",
       "        0.42966957, 0.53365784, 0.47400684, 0.70610781, 0.57169347,\n",
       "        0.79048038, 1.09504242, 1.21904888, 1.18722267, 1.5746552 ,\n",
       "        1.47046466, 1.33064351, 0.50613565, 0.31546941, 0.3491991 ,\n",
       "        0.68919215, 0.64945831, 0.54236703, 1.02337995, 1.343256  ,\n",
       "        1.01547413, 1.37428122, 1.40340738, 1.56144099, 0.27984185,\n",
       "        0.31597199, 0.22649488, 0.65536437, 0.58390231, 0.61893263,\n",
       "        1.00977025, 1.06511564, 1.14348507, 1.36377263, 1.27049303,\n",
       "        1.34005127, 0.69179368, 1.33785124, 1.11946254, 0.49032197,\n",
       "        1.35056167, 1.12746959, 0.49242344, 1.7817327 , 1.15739608,\n",
       "        0.43357301, 2.8057127 , 1.14868765, 0.78027163, 1.21584582,\n",
       "        1.22465243, 0.61572962, 1.27009192, 1.07062106, 0.49342461,\n",
       "        2.11361752, 1.05110455, 0.44938688, 2.57561502, 1.04139571,\n",
       "        0.66096873, 1.39610109, 1.1124567 , 0.49262347, 1.29351239,\n",
       "        1.03148746, 0.47270675, 1.63120322, 1.0434978 , 0.42036109,\n",
       "        2.38164811, 1.06671734, 0.65736532, 1.4625576 , 1.17440968,\n",
       "        0.51424198, 1.68224683, 1.07422414, 0.49432502, 2.09400101,\n",
       "        1.12086453, 0.46229739, 1.78563662, 1.08183069, 0.88586216,\n",
       "        0.44428186, 1.34375529, 0.81790366, 1.13177352, 1.31402969,\n",
       "        0.8321156 , 2.09880528, 1.46786203, 0.83371749, 1.63230424,\n",
       "        1.41021242, 0.87475238, 1.60387888, 1.41942081, 0.77036295,\n",
       "        1.291611  , 1.36097045, 0.82200685, 1.53612084, 1.37558331,\n",
       "        0.85103245, 1.91434641, 1.44624429, 0.84252439, 1.04399767,\n",
       "        1.3838902 , 0.81660204, 1.72448349, 1.39600086, 0.83371711,\n",
       "        1.36977811, 1.46696172, 0.82390904, 1.93526459, 1.40150509,\n",
       "        0.89556999, 0.93290262, 1.35758305, 0.81880417, 1.44464273,\n",
       "        1.43243251, 0.90788102, 1.77322564, 1.4344337 , 0.81820393,\n",
       "        2.49674721, 1.48677816, 0.8269114 , 0.83421788, 0.85113149,\n",
       "        0.65456309, 0.61512899, 0.60502043, 0.57759724, 0.58280172,\n",
       "        0.61873212, 0.47781062, 0.51774468, 0.49902964, 0.84923072,\n",
       "        0.94861627, 0.74303894, 0.6149291 , 0.76425762, 0.59911528,\n",
       "        0.55517788, 0.54857163, 0.53145733, 0.47300692, 0.47530861,\n",
       "        0.45989571, 0.76215539, 0.75705113, 0.7065073 , 0.63564701,\n",
       "        0.69749985, 0.66196923, 0.45669298, 0.62173491, 0.53305888,\n",
       "        0.53706212, 0.44818578, 0.49652715, 0.68849168, 0.75745144,\n",
       "        0.85103178, 0.58970718, 0.65796623, 0.5495728 , 0.53996458,\n",
       "        0.47691045, 0.53966408, 0.49202323, 0.4472846 , 0.48751926,\n",
       "        1.1847188 , 1.25207715, 1.21804719, 1.74670224, 1.77692823,\n",
       "        1.86760678, 2.9608469 , 2.08629436, 2.12172437, 3.30944629,\n",
       "        3.31294947, 3.51051989, 1.39489918, 1.05730929, 1.10775247,\n",
       "        1.62399645, 1.83527813, 1.25578022, 3.00118079, 2.97385764,\n",
       "        2.86226201, 3.20335503, 3.19454756, 3.21116114, 1.05400677,\n",
       "        1.12046328, 1.15309138, 1.36707611, 1.33094468, 1.73509264,\n",
       "        3.15951724, 3.12228484, 2.88878436, 3.52593193, 3.42154274,\n",
       "        3.527033  , 1.28450537, 1.30342116, 1.29081044, 1.23085861,\n",
       "        1.14928794, 1.24587183, 1.88121839, 1.56354809, 1.49678779,\n",
       "        2.03164744, 2.1038094 , 1.75500908]),\n",
       " 'std_fit_time': array([0.08219458, 0.03435189, 0.06418672, 0.1044489 , 0.0739937 ,\n",
       "        0.13566421, 0.07849954, 0.04985295, 0.04269136, 0.0418698 ,\n",
       "        0.03315794, 0.62957202, 0.06436861, 0.04654425, 0.12971314,\n",
       "        0.05177415, 0.04389948, 0.07080873, 0.1616213 , 0.07164061,\n",
       "        0.08754989, 0.11571306, 0.62144226, 0.09406886, 0.04643423,\n",
       "        0.04545941, 0.09082532, 0.06359592, 0.10397486, 0.09643725,\n",
       "        0.16849809, 0.06343315, 0.12144162, 0.65173657, 0.11593975,\n",
       "        0.04009033, 0.0566436 , 0.04439401, 0.0225238 , 0.18208235,\n",
       "        0.06175432, 0.0793771 , 0.18573127, 0.14778589, 0.21232842,\n",
       "        0.25228159, 0.65274732, 0.50161348, 0.05411857, 0.12200896,\n",
       "        0.18499636, 0.15066546, 0.17184355, 0.1014114 , 0.20797645,\n",
       "        0.12511518, 0.20470201, 0.17414659, 0.154041  , 0.18868929,\n",
       "        0.1381211 , 0.19462745, 0.15289836, 0.18050179, 0.06907191,\n",
       "        0.12822981, 0.14478948, 0.13538846, 0.17086991, 0.080981  ,\n",
       "        0.14489175, 0.12843632, 0.12737957, 0.04699822, 0.15125605,\n",
       "        0.16566982, 0.2471352 , 0.10571742, 0.12378935, 0.14706919,\n",
       "        0.16037055, 0.19118912, 0.34296018, 0.21716101, 0.0447817 ,\n",
       "        0.12217516, 0.03670475, 0.18900106, 0.16071189, 0.14448648,\n",
       "        0.20267273, 0.09806011, 0.25026424, 0.16126111, 0.17329161,\n",
       "        0.16094578, 0.02843904, 0.56284733, 0.02129044, 0.01461885,\n",
       "        0.67533043, 0.03508304, 0.04088989, 0.79979332, 0.03406676,\n",
       "        0.02522244, 0.42567777, 0.14255699, 0.07401626, 0.78958161,\n",
       "        0.07445057, 0.04720426, 0.6130384 , 0.02641715, 0.03271458,\n",
       "        0.89571914, 0.02050558, 0.01256114, 0.22161598, 0.01624015,\n",
       "        0.04793179, 0.70845359, 0.03661975, 0.00796056, 0.826618  ,\n",
       "        0.01400626, 0.03501389, 0.93924265, 0.00880233, 0.01160773,\n",
       "        0.38534324, 0.01578568, 0.02715002, 0.98992124, 0.02800507,\n",
       "        0.00472232, 0.66401713, 0.02509222, 0.0297685 , 0.81195505,\n",
       "        0.07596845, 0.01948335, 0.87576187, 0.01151813, 0.1149078 ,\n",
       "        0.44767845, 0.11158523, 0.07873701, 0.81959561, 0.0450769 ,\n",
       "        0.07593271, 0.63862386, 0.07302618, 0.06018836, 0.72976826,\n",
       "        0.06091437, 0.05779035, 0.73312876, 0.12268877, 0.06866112,\n",
       "        0.80001461, 0.11721985, 0.04063637, 0.7540621 , 0.04547813,\n",
       "        0.09215819, 0.72825848, 0.05653485, 0.08894967, 0.6760785 ,\n",
       "        0.09108136, 0.11251438, 0.73534623, 0.17428392, 0.05285264,\n",
       "        0.8348558 , 0.11639056, 0.02870375, 0.74768181, 0.04264213,\n",
       "        0.14421531, 0.81674316, 0.06979822, 0.0538172 , 0.78923748,\n",
       "        0.07322049, 0.15442355, 0.76956003, 0.0601005 , 0.04773835,\n",
       "        0.89819839, 0.08736858, 0.1711235 , 0.24936977, 0.10830684,\n",
       "        0.07132125, 0.10806033, 0.080406  , 0.08565635, 0.11187007,\n",
       "        0.06158365, 0.1035088 , 0.05582718, 0.06866199, 0.18967734,\n",
       "        0.1541052 , 0.1017654 , 0.06693716, 0.16454914, 0.06538795,\n",
       "        0.05819773, 0.06320467, 0.08077642, 0.03538215, 0.05499032,\n",
       "        0.04148678, 0.17794193, 0.14043418, 0.11013091, 0.06107496,\n",
       "        0.06589327, 0.04758139, 0.0765936 , 0.06951842, 0.07541452,\n",
       "        0.07258974, 0.06079838, 0.09259472, 0.10612625, 0.14362388,\n",
       "        0.08359804, 0.09418857, 0.07373493, 0.12239943, 0.0480403 ,\n",
       "        0.04635051, 0.07008458, 0.05339673, 0.04968782, 0.07181352,\n",
       "        0.24058253, 0.44397217, 0.23595377, 0.44404357, 0.2858746 ,\n",
       "        0.41678727, 0.28201155, 0.57178853, 0.73249737, 0.14761273,\n",
       "        0.12800214, 0.12031025, 0.26074954, 0.30544578, 0.25725675,\n",
       "        0.50507213, 0.41584013, 0.14546768, 0.09565059, 0.18584002,\n",
       "        0.37480056, 0.0237548 , 0.04378563, 0.04168885, 0.19982151,\n",
       "        0.215652  , 0.17832247, 0.38248193, 0.23381784, 0.35751034,\n",
       "        0.22964933, 0.43494547, 0.66617526, 0.14834164, 0.30586283,\n",
       "        0.20503484, 0.34119508, 0.17467805, 0.18977049, 0.06467819,\n",
       "        0.25841918, 0.17778764, 0.66189741, 0.46873757, 0.13972687,\n",
       "        0.64551968, 0.67274285, 0.34979638]),\n",
       " 'mean_score_time': array([0.00700622, 0.00730624, 0.00690603, 0.00780706, 0.01040802,\n",
       "        0.00970864, 0.01110926, 0.01121006, 0.01010809, 0.01030779,\n",
       "        0.0091084 , 0.00890808, 0.00950813, 0.00820689, 0.00860672,\n",
       "        0.00780396, 0.00910692, 0.00820794, 0.00940809, 0.01060548,\n",
       "        0.00960822, 0.00870781, 0.00910697, 0.01020918, 0.00910954,\n",
       "        0.0090075 , 0.00800686, 0.00750661, 0.00800662, 0.01120892,\n",
       "        0.0102097 , 0.00860763, 0.01091037, 0.02161727, 0.00970993,\n",
       "        0.00880814, 0.01691394, 0.00890932, 0.00800691, 0.00960803,\n",
       "        0.00930796, 0.00830641, 0.01051059, 0.01040983, 0.01060944,\n",
       "        0.00970817, 0.01201   , 0.00940647, 0.01201048, 0.00850735,\n",
       "        0.00940704, 0.00930843, 0.01351109, 0.00910854, 0.0130116 ,\n",
       "        0.010009  , 0.01071091, 0.01151009, 0.00950756, 0.01271091,\n",
       "        0.01010857, 0.00770764, 0.00840793, 0.00920682, 0.00840583,\n",
       "        0.00910792, 0.00940757, 0.01010914, 0.01060781, 0.00950847,\n",
       "        0.00880876, 0.00810585, 0.00820584, 0.00730572, 0.00740757,\n",
       "        0.00840931, 0.00900712, 0.0090075 , 0.0105093 , 0.01080914,\n",
       "        0.00990739, 0.01231222, 0.01090865, 0.01032252, 0.00970736,\n",
       "        0.00970683, 0.00810671, 0.0090065 , 0.00900793, 0.0086091 ,\n",
       "        0.00970731, 0.0098105 , 0.01080909, 0.00960655, 0.00850739,\n",
       "        0.00770721, 0.0068059 , 0.00880737, 0.00650558, 0.00640569,\n",
       "        0.0071063 , 0.00690632, 0.0068059 , 0.00900741, 0.00700583,\n",
       "        0.00700598, 0.00680561, 0.00940819, 0.00630507, 0.00690603,\n",
       "        0.00790691, 0.00650568, 0.00650582, 0.0065053 , 0.00820704,\n",
       "        0.00700631, 0.00720601, 0.00820661, 0.00780711, 0.00690618,\n",
       "        0.00650516, 0.00670562, 0.00640554, 0.00670624, 0.00660553,\n",
       "        0.00650573, 0.01110945, 0.007406  , 0.00660548, 0.00780673,\n",
       "        0.00680623, 0.00720654, 0.01000872, 0.00740647, 0.00650592,\n",
       "        0.00710621, 0.0118103 , 0.00840712, 0.00820746, 0.00710588,\n",
       "        0.00700607, 0.00710607, 0.00800653, 0.00810685, 0.00660553,\n",
       "        0.00660586, 0.00640583, 0.00710568, 0.00690598, 0.00670576,\n",
       "        0.0072062 , 0.00780644, 0.0075068 , 0.00790668, 0.00720582,\n",
       "        0.0071064 , 0.00690608, 0.00670581, 0.00680547, 0.00720615,\n",
       "        0.00770669, 0.00690627, 0.00710645, 0.00750651, 0.00700612,\n",
       "        0.00750637, 0.00710635, 0.0070056 , 0.00640545, 0.00670581,\n",
       "        0.00670552, 0.00670543, 0.00710583, 0.00670619, 0.00740657,\n",
       "        0.00750608, 0.00710616, 0.00750613, 0.00740623, 0.00720596,\n",
       "        0.006706  , 0.00680604, 0.00668998, 0.00660572, 0.00700626,\n",
       "        0.00690522, 0.00850792, 0.00720615, 0.0071063 , 0.00780702,\n",
       "        0.0081069 , 0.00730643, 0.00640531, 0.00660548, 0.0067059 ,\n",
       "        0.00740609, 0.00750656, 0.00690608, 0.00700617, 0.00700588,\n",
       "        0.00690594, 0.00700612, 0.00690613, 0.00710602, 0.00640545,\n",
       "        0.00660567, 0.00620546, 0.00640569, 0.00680566, 0.00630531,\n",
       "        0.0077064 , 0.0068058 , 0.00710635, 0.00690589, 0.00690622,\n",
       "        0.00670547, 0.00630527, 0.00640526, 0.00630531, 0.00660563,\n",
       "        0.00640559, 0.00650582, 0.0068059 , 0.00690646, 0.00760641,\n",
       "        0.00700583, 0.00700612, 0.00690579, 0.00630541, 0.00630541,\n",
       "        0.00660572, 0.00690589, 0.00660558, 0.0065052 , 0.00680575,\n",
       "        0.00690594, 0.00730648, 0.00720663, 0.00690579, 0.00710578,\n",
       "        0.00640559, 0.00660591, 0.00700645, 0.00670609, 0.00700626,\n",
       "        0.00710578, 0.0081068 , 0.0072063 , 0.00780735, 0.00790739,\n",
       "        0.00810661, 0.01150947, 0.00620561, 0.00630527, 0.00640559,\n",
       "        0.00660515, 0.006706  , 0.00670586, 0.00710626, 0.00730634,\n",
       "        0.00740604, 0.00780659, 0.00760593, 0.0089076 , 0.00650573,\n",
       "        0.0071064 , 0.00680594, 0.00700574, 0.00990825, 0.00750637,\n",
       "        0.00760627, 0.00850725, 0.00770683, 0.00830736, 0.00780687,\n",
       "        0.00740681, 0.00810699, 0.00740652, 0.00660601, 0.00680633,\n",
       "        0.00700636, 0.00850773, 0.00710583, 0.0072032 , 0.01191053,\n",
       "        0.006005  , 0.00490394, 0.0048038 ]),\n",
       " 'std_score_time': array([5.48727627e-04, 6.00028310e-04, 1.99201370e-04, 4.00747616e-04,\n",
       "        3.84392854e-03, 1.75014970e-03, 2.03763194e-03, 1.93841303e-03,\n",
       "        6.62780161e-04, 1.07803370e-03, 8.01195333e-04, 7.35984049e-04,\n",
       "        1.54965120e-03, 9.27608930e-04, 1.59548131e-03, 2.49693373e-04,\n",
       "        7.98547636e-04, 4.00115693e-04, 1.59332100e-03, 1.02250193e-03,\n",
       "        5.81286820e-04, 4.00819609e-04, 7.36353018e-04, 1.32788132e-03,\n",
       "        1.77388446e-03, 1.30427708e-03, 9.50049370e-04, 1.36545382e-06,\n",
       "        5.48426508e-04, 2.84441351e-03, 6.00370224e-04, 1.06924727e-03,\n",
       "        2.49958237e-03, 1.11061532e-02, 1.02802683e-03, 1.16748792e-03,\n",
       "        1.02103465e-02, 1.35919429e-03, 1.00095304e-03, 2.31336349e-03,\n",
       "        2.87590728e-03, 6.79161804e-04, 1.87330716e-03, 1.93620206e-03,\n",
       "        2.51952594e-03, 1.53732253e-03, 1.55042004e-03, 2.35582994e-03,\n",
       "        7.77914747e-03, 1.51799039e-03, 2.57713645e-03, 3.37347960e-03,\n",
       "        5.73238343e-03, 2.22304467e-03, 5.54641956e-03, 6.32788752e-04,\n",
       "        1.40289599e-03, 2.66563317e-03, 8.37340477e-04, 2.67845536e-03,\n",
       "        1.74517029e-03, 4.00638751e-04, 1.82929722e-03, 2.67861060e-03,\n",
       "        3.73765685e-04, 7.99943992e-04, 3.74561139e-04, 1.20036287e-03,\n",
       "        5.82726352e-04, 7.06585767e-04, 6.01341921e-04, 2.00585539e-04,\n",
       "        1.16772102e-03, 4.00235921e-04, 2.00345480e-04, 8.61049752e-04,\n",
       "        1.81869013e-03, 1.05032455e-03, 5.48078306e-04, 1.88704154e-03,\n",
       "        5.83013812e-04, 3.66952993e-03, 2.10878431e-03, 8.68041302e-04,\n",
       "        1.88688192e-03, 3.17278185e-03, 5.82866382e-04, 1.41507259e-03,\n",
       "        1.76276150e-03, 1.11311636e-03, 5.11496034e-04, 6.00955213e-04,\n",
       "        1.40096998e-03, 1.24094785e-03, 7.08404496e-04, 2.45593987e-04,\n",
       "        8.72307396e-04, 4.36984048e-03, 3.16431509e-04, 3.74444666e-04,\n",
       "        5.83355708e-04, 3.73731002e-04, 2.45087249e-04, 4.26979335e-03,\n",
       "        7.75156326e-04, 2.78041453e-07, 2.45146263e-04, 4.33248173e-03,\n",
       "        2.44873435e-04, 1.35732388e-03, 3.31037214e-03, 3.16297988e-07,\n",
       "        3.16432192e-04, 4.62310777e-07, 1.94081264e-03, 5.47900889e-04,\n",
       "        2.45048346e-04, 2.65914921e-03, 1.86163130e-03, 2.00343664e-04,\n",
       "        3.16355980e-04, 2.45399165e-04, 2.00248070e-04, 5.10454501e-04,\n",
       "        2.00200329e-04, 3.16581846e-04, 8.71010480e-03, 9.17213571e-04,\n",
       "        2.00822198e-04, 9.27685428e-04, 2.45360710e-04, 6.78879890e-04,\n",
       "        6.50928666e-03, 1.82934415e-03, 3.16130428e-04, 4.90096205e-04,\n",
       "        1.03612218e-02, 2.85585675e-03, 1.86190050e-03, 7.35254501e-04,\n",
       "        4.47767804e-04, 2.00033216e-04, 8.37654007e-04, 1.49806468e-03,\n",
       "        4.90680070e-04, 2.00033216e-04, 1.99914967e-04, 5.83748221e-04,\n",
       "        5.83797156e-04, 2.45184316e-04, 2.44892442e-04, 9.27855280e-04,\n",
       "        8.37368850e-04, 4.90174057e-04, 2.45203771e-04, 2.00224178e-04,\n",
       "        8.00943580e-04, 2.44950714e-04, 8.72755969e-04, 7.49201262e-04,\n",
       "        1.40080456e-03, 2.00272288e-04, 3.74380781e-04, 5.48335875e-04,\n",
       "        7.13664510e-07, 4.47767712e-04, 2.00606328e-04, 4.42200589e-07,\n",
       "        3.74444575e-04, 2.45242797e-04, 5.09986692e-04, 2.45262461e-04,\n",
       "        3.74801499e-04, 2.45223976e-04, 3.74253644e-04, 8.37026906e-04,\n",
       "        2.00104962e-04, 4.47554947e-04, 5.83641728e-04, 2.45379169e-04,\n",
       "        4.00543298e-04, 3.99840499e-04, 6.84204006e-04, 2.00224065e-04,\n",
       "        3.16883482e-04, 3.73820096e-04, 2.55156935e-03, 4.00686505e-04,\n",
       "        2.00033898e-04, 6.78943311e-04, 7.35656934e-04, 4.00114216e-04,\n",
       "        5.83388572e-04, 4.90543908e-04, 5.10632078e-04, 9.70272306e-04,\n",
       "        6.32522835e-04, 3.74240788e-04, 3.16883460e-04, 5.48422881e-04,\n",
       "        2.00105132e-04, 3.16808089e-04, 2.00081269e-04, 2.00176977e-04,\n",
       "        2.00319546e-04, 7.36033292e-04, 2.45750584e-04, 1.99962366e-04,\n",
       "        2.45281669e-04, 2.45067515e-04, 1.66292093e-03, 2.45594125e-04,\n",
       "        3.74597469e-04, 1.99962082e-04, 3.74279002e-04, 2.45320804e-04,\n",
       "        4.00925250e-04, 2.00224746e-04, 2.45164961e-04, 3.74737734e-04,\n",
       "        2.00152418e-04, 3.16206378e-04, 4.00543326e-04, 3.74342571e-04,\n",
       "        1.32007594e-03, 1.16800773e-07, 5.47813688e-04, 3.74673989e-04,\n",
       "        2.45145568e-04, 4.00281077e-04, 3.74648397e-04, 5.84189715e-04,\n",
       "        2.00414970e-04, 4.86280395e-07, 6.00647971e-04, 1.99986243e-04,\n",
       "        4.00197771e-04, 4.00447931e-04, 1.99914171e-04, 1.99938133e-04,\n",
       "        5.83846181e-04, 4.90164186e-04, 7.75587070e-04, 2.45399072e-04,\n",
       "        4.47661256e-04, 7.35526988e-04, 1.24154997e-03, 2.45203864e-04,\n",
       "        4.00054571e-04, 6.64010236e-04, 9.70444507e-04, 8.00678731e-03,\n",
       "        2.45340591e-04, 2.45126368e-04, 2.00272344e-04, 2.00272061e-04,\n",
       "        2.45281623e-04, 2.45203818e-04, 2.00176409e-04, 2.45184409e-04,\n",
       "        3.74559227e-04, 2.45321035e-04, 4.90485479e-04, 1.85602399e-03,\n",
       "        3.16657967e-04, 4.90154518e-04, 6.00473242e-04, 3.16883618e-04,\n",
       "        6.05818816e-03, 1.26544142e-03, 5.83355591e-04, 1.34259205e-03,\n",
       "        5.10145798e-04, 1.40098514e-03, 6.00370072e-04, 3.74189817e-04,\n",
       "        2.17936392e-03, 1.15895398e-03, 3.75145724e-04, 2.45438021e-04,\n",
       "        7.75494806e-04, 2.30419126e-03, 1.99795168e-04, 3.98152991e-04,\n",
       "        6.34915512e-03, 8.37397442e-04, 8.00061245e-04, 6.00147456e-04]),\n",
       " 'param_classifier': masked_array(data=[MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__activation': masked_array(data=['logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__alpha': masked_array(data=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__hidden_layer_sizes': masked_array(data=[(5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__learning_rate': masked_array(data=['constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'}],\n",
       " 'split0_test_recall_micro': array([0.929, 0.93 , 0.93 , 0.91 , 0.918, 0.918, 0.909, 0.909, 0.908,\n",
       "        0.906, 0.898, 0.885, 0.93 , 0.93 , 0.93 , 0.918, 0.926, 0.925,\n",
       "        0.912, 0.91 , 0.91 , 0.906, 0.93 , 0.89 , 0.93 , 0.93 , 0.925,\n",
       "        0.917, 0.925, 0.925, 0.911, 0.914, 0.916, 0.93 , 0.905, 0.907,\n",
       "        0.93 , 0.929, 0.93 , 0.926, 0.929, 0.927, 0.925, 0.919, 0.908,\n",
       "        0.902, 0.903, 0.912, 0.927, 0.926, 0.927, 0.925, 0.918, 0.917,\n",
       "        0.911, 0.913, 0.903, 0.902, 0.893, 0.884, 0.929, 0.926, 0.93 ,\n",
       "        0.923, 0.914, 0.919, 0.905, 0.91 , 0.922, 0.895, 0.897, 0.898,\n",
       "        0.927, 0.93 , 0.928, 0.928, 0.927, 0.922, 0.906, 0.914, 0.907,\n",
       "        0.891, 0.903, 0.893, 0.929, 0.927, 0.929, 0.929, 0.918, 0.929,\n",
       "        0.91 , 0.925, 0.914, 0.904, 0.896, 0.911, 0.93 , 0.853, 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.78 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.08 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.156, 0.93 , 0.93 , 0.82 , 0.93 , 0.93 , 0.38 , 0.93 ,\n",
       "        0.93 , 0.716, 0.93 , 0.93 , 0.609, 0.93 , 0.93 , 0.802, 0.93 ,\n",
       "        0.93 , 0.309, 0.93 , 0.93 , 0.686, 0.93 , 0.93 , 0.725, 0.93 ,\n",
       "        0.93 , 0.383, 0.93 , 0.93 , 0.578, 0.93 , 0.93 , 0.403, 0.93 ,\n",
       "        0.93 , 0.321, 0.93 , 0.93 , 0.636, 0.93 , 0.93 , 0.419, 0.93 ,\n",
       "        0.93 , 0.556, 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ]),\n",
       " 'split1_test_recall_micro': array([0.911, 0.93 , 0.93 , 0.923, 0.921, 0.926, 0.903, 0.902, 0.921,\n",
       "        0.896, 0.898, 0.93 , 0.93 , 0.93 , 0.927, 0.927, 0.92 , 0.929,\n",
       "        0.917, 0.912, 0.916, 0.897, 0.93 , 0.898, 0.93 , 0.93 , 0.93 ,\n",
       "        0.925, 0.927, 0.925, 0.915, 0.917, 0.908, 0.908, 0.907, 0.916,\n",
       "        0.93 , 0.93 , 0.93 , 0.928, 0.927, 0.928, 0.926, 0.92 , 0.923,\n",
       "        0.913, 0.93 , 0.914, 0.929, 0.927, 0.93 , 0.923, 0.925, 0.922,\n",
       "        0.912, 0.912, 0.903, 0.903, 0.897, 0.894, 0.929, 0.93 , 0.927,\n",
       "        0.929, 0.927, 0.928, 0.918, 0.909, 0.909, 0.904, 0.897, 0.898,\n",
       "        0.931, 0.931, 0.93 , 0.923, 0.926, 0.929, 0.911, 0.91 , 0.909,\n",
       "        0.907, 0.911, 0.909, 0.929, 0.93 , 0.93 , 0.927, 0.926, 0.919,\n",
       "        0.923, 0.918, 0.917, 0.906, 0.912, 0.912, 0.93 , 0.889, 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.899, 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.919, 0.93 , 0.93 , 0.903, 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.85 , 0.93 , 0.93 , 0.728, 0.93 , 0.93 , 0.512, 0.93 ,\n",
       "        0.93 , 0.836, 0.93 , 0.93 , 0.32 , 0.93 , 0.93 , 0.525, 0.93 ,\n",
       "        0.93 , 0.573, 0.93 , 0.93 , 0.682, 0.93 , 0.93 , 0.142, 0.93 ,\n",
       "        0.93 , 0.586, 0.93 , 0.93 , 0.787, 0.93 , 0.93 , 0.605, 0.93 ,\n",
       "        0.93 , 0.839, 0.93 , 0.93 , 0.268, 0.93 , 0.93 , 0.685, 0.93 ,\n",
       "        0.93 , 0.85 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.929, 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.929, 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ]),\n",
       " 'split2_test_recall_micro': array([0.93 , 0.93 , 0.927, 0.929, 0.924, 0.913, 0.907, 0.905, 0.908,\n",
       "        0.895, 0.901, 0.93 , 0.929, 0.927, 0.93 , 0.918, 0.926, 0.927,\n",
       "        0.916, 0.91 , 0.912, 0.906, 0.904, 0.909, 0.929, 0.927, 0.929,\n",
       "        0.926, 0.928, 0.927, 0.913, 0.914, 0.917, 0.93 , 0.91 , 0.903,\n",
       "        0.929, 0.929, 0.93 , 0.928, 0.926, 0.93 , 0.921, 0.921, 0.922,\n",
       "        0.915, 0.916, 0.93 , 0.925, 0.927, 0.928, 0.92 , 0.923, 0.921,\n",
       "        0.915, 0.917, 0.92 , 0.896, 0.909, 0.901, 0.928, 0.93 , 0.93 ,\n",
       "        0.925, 0.927, 0.928, 0.914, 0.909, 0.91 , 0.9  , 0.91 , 0.9  ,\n",
       "        0.929, 0.928, 0.93 , 0.923, 0.925, 0.926, 0.915, 0.921, 0.922,\n",
       "        0.918, 0.909, 0.894, 0.929, 0.93 , 0.932, 0.929, 0.927, 0.929,\n",
       "        0.918, 0.918, 0.919, 0.899, 0.901, 0.913, 0.93 , 0.378, 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.584, 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.259, 0.93 , 0.93 , 0.07 , 0.93 ,\n",
       "        0.93 , 0.79 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.91 , 0.93 , 0.93 , 0.396, 0.93 , 0.93 , 0.692, 0.93 ,\n",
       "        0.93 , 0.667, 0.93 , 0.93 , 0.457, 0.93 , 0.93 , 0.803, 0.93 ,\n",
       "        0.93 , 0.699, 0.93 , 0.93 , 0.707, 0.93 , 0.93 , 0.713, 0.93 ,\n",
       "        0.93 , 0.309, 0.93 , 0.93 , 0.713, 0.93 , 0.93 , 0.555, 0.93 ,\n",
       "        0.93 , 0.437, 0.93 , 0.93 , 0.652, 0.93 , 0.93 , 0.621, 0.93 ,\n",
       "        0.93 , 0.484, 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.929, 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ]),\n",
       " 'split3_test_recall_micro': array([0.93 , 0.93 , 0.926, 0.925, 0.923, 0.921, 0.915, 0.922, 0.913,\n",
       "        0.901, 0.904, 0.907, 0.93 , 0.93 , 0.93 , 0.927, 0.926, 0.922,\n",
       "        0.905, 0.913, 0.905, 0.911, 0.907, 0.896, 0.93 , 0.93 , 0.93 ,\n",
       "        0.923, 0.924, 0.926, 0.912, 0.928, 0.923, 0.908, 0.911, 0.906,\n",
       "        0.93 , 0.929, 0.93 , 0.923, 0.926, 0.929, 0.924, 0.924, 0.924,\n",
       "        0.91 , 0.93 , 0.909, 0.929, 0.927, 0.932, 0.921, 0.923, 0.923,\n",
       "        0.912, 0.916, 0.915, 0.895, 0.907, 0.913, 0.922, 0.929, 0.93 ,\n",
       "        0.926, 0.921, 0.927, 0.921, 0.916, 0.906, 0.906, 0.903, 0.89 ,\n",
       "        0.929, 0.93 , 0.931, 0.924, 0.921, 0.923, 0.916, 0.903, 0.911,\n",
       "        0.903, 0.888, 0.905, 0.93 , 0.929, 0.93 , 0.923, 0.928, 0.929,\n",
       "        0.918, 0.921, 0.914, 0.903, 0.908, 0.915, 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.916, 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.105, 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.179, 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.916, 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.823, 0.93 , 0.93 , 0.753, 0.93 , 0.93 , 0.564, 0.93 ,\n",
       "        0.93 , 0.445, 0.93 , 0.93 , 0.1  , 0.93 , 0.93 , 0.368, 0.93 ,\n",
       "        0.93 , 0.652, 0.93 , 0.93 , 0.462, 0.93 , 0.93 , 0.234, 0.93 ,\n",
       "        0.93 , 0.757, 0.93 , 0.93 , 0.588, 0.93 , 0.93 , 0.787, 0.93 ,\n",
       "        0.93 , 0.882, 0.93 , 0.93 , 0.701, 0.93 , 0.93 , 0.514, 0.93 ,\n",
       "        0.93 , 0.377, 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ]),\n",
       " 'split4_test_recall_micro': array([0.929, 0.928, 0.928, 0.925, 0.923, 0.924, 0.91 , 0.919, 0.91 ,\n",
       "        0.912, 0.907, 0.911, 0.929, 0.927, 0.926, 0.926, 0.921, 0.919,\n",
       "        0.912, 0.922, 0.911, 0.906, 0.904, 0.906, 0.929, 0.929, 0.929,\n",
       "        0.927, 0.925, 0.924, 0.916, 0.916, 0.916, 0.905, 0.907, 0.897,\n",
       "        0.929, 0.929, 0.928, 0.927, 0.927, 0.927, 0.913, 0.926, 0.919,\n",
       "        0.919, 0.929, 0.906, 0.931, 0.929, 0.925, 0.922, 0.921, 0.93 ,\n",
       "        0.911, 0.915, 0.916, 0.897, 0.898, 0.89 , 0.925, 0.929, 0.925,\n",
       "        0.919, 0.918, 0.921, 0.917, 0.911, 0.913, 0.901, 0.899, 0.881,\n",
       "        0.928, 0.92 , 0.927, 0.925, 0.921, 0.92 , 0.927, 0.921, 0.924,\n",
       "        0.906, 0.902, 0.894, 0.928, 0.929, 0.929, 0.927, 0.927, 0.926,\n",
       "        0.921, 0.918, 0.917, 0.897, 0.915, 0.889, 0.929, 0.079, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.071, 0.929, 0.929, 0.928, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.543, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.831, 0.929, 0.929, 0.64 , 0.929, 0.929, 0.43 , 0.929,\n",
       "        0.929, 0.612, 0.929, 0.929, 0.65 , 0.929, 0.929, 0.433, 0.929,\n",
       "        0.929, 0.815, 0.929, 0.929, 0.447, 0.929, 0.929, 0.604, 0.929,\n",
       "        0.929, 0.519, 0.929, 0.929, 0.854, 0.929, 0.929, 0.662, 0.929,\n",
       "        0.929, 0.68 , 0.929, 0.929, 0.531, 0.929, 0.929, 0.722, 0.929,\n",
       "        0.929, 0.499, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929]),\n",
       " 'mean_test_recall_micro': array([0.9258, 0.9296, 0.9282, 0.9224, 0.9218, 0.9204, 0.9088, 0.9114,\n",
       "        0.912 , 0.902 , 0.9016, 0.9126, 0.9296, 0.9288, 0.9286, 0.9232,\n",
       "        0.9238, 0.9244, 0.9124, 0.9134, 0.9108, 0.9052, 0.915 , 0.8998,\n",
       "        0.9296, 0.9292, 0.9286, 0.9236, 0.9258, 0.9254, 0.9134, 0.9178,\n",
       "        0.916 , 0.9162, 0.908 , 0.9058, 0.9296, 0.9292, 0.9296, 0.9264,\n",
       "        0.927 , 0.9282, 0.9218, 0.922 , 0.9192, 0.9118, 0.9216, 0.9142,\n",
       "        0.9282, 0.9272, 0.9284, 0.9222, 0.922 , 0.9226, 0.9122, 0.9146,\n",
       "        0.9114, 0.8986, 0.9008, 0.8964, 0.9266, 0.9288, 0.9284, 0.9244,\n",
       "        0.9214, 0.9246, 0.915 , 0.911 , 0.912 , 0.9012, 0.9012, 0.8934,\n",
       "        0.9288, 0.9278, 0.9292, 0.9246, 0.924 , 0.924 , 0.915 , 0.9138,\n",
       "        0.9146, 0.905 , 0.9026, 0.899 , 0.929 , 0.929 , 0.93  , 0.927 ,\n",
       "        0.9252, 0.9264, 0.918 , 0.92  , 0.9162, 0.9018, 0.9064, 0.908 ,\n",
       "        0.9298, 0.6258, 0.9298, 0.9298, 0.927 , 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.6956, 0.9298, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.578 , 0.9298, 0.9298, 0.9296, 0.9298, 0.9298, 0.9208,\n",
       "        0.9298, 0.9298, 0.7956, 0.9298, 0.9298, 0.5106, 0.9298, 0.9298,\n",
       "        0.8996, 0.9298, 0.9298, 0.9244, 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.714 , 0.9298, 0.9298, 0.6674, 0.9298, 0.9298, 0.5156,\n",
       "        0.9298, 0.9298, 0.6552, 0.9298, 0.9298, 0.4272, 0.9298, 0.9298,\n",
       "        0.5862, 0.9298, 0.9298, 0.6096, 0.9298, 0.9298, 0.5968, 0.9298,\n",
       "        0.9298, 0.4836, 0.9298, 0.9298, 0.5108, 0.9298, 0.9298, 0.704 ,\n",
       "        0.9298, 0.9298, 0.6024, 0.9298, 0.9298, 0.6318, 0.9298, 0.9298,\n",
       "        0.5576, 0.9298, 0.9298, 0.5922, 0.9298, 0.9298, 0.5532, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9296, 0.9298, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.9296, 0.9296, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298]),\n",
       " 'std_test_recall_micro': array([0.0074135 , 0.0008    , 0.0016    , 0.00649923, 0.00213542,\n",
       "        0.00458694, 0.00391918, 0.00781281, 0.00485798, 0.0063561 ,\n",
       "        0.00349857, 0.01674037, 0.0004899 , 0.00146969, 0.00174356,\n",
       "        0.00426146, 0.00271293, 0.00355528, 0.00422374, 0.00445421,\n",
       "        0.00354401, 0.00453431, 0.01229634, 0.00688186, 0.0004899 ,\n",
       "        0.00116619, 0.00185472, 0.00355528, 0.00146969, 0.0010198 ,\n",
       "        0.00185472, 0.00523068, 0.00477493, 0.01132078, 0.00219089,\n",
       "        0.00617738, 0.0004899 , 0.0004    , 0.0008    , 0.00185472,\n",
       "        0.00109545, 0.00116619, 0.00470744, 0.00260768, 0.00584466,\n",
       "        0.00570614, 0.01070701, 0.00835225, 0.00203961, 0.0009798 ,\n",
       "        0.00241661, 0.00172047, 0.00236643, 0.00422374, 0.00146969,\n",
       "        0.00185472, 0.00705975, 0.0032619 , 0.00614492, 0.00997196,\n",
       "        0.00272764, 0.00146969, 0.00205913, 0.00332265, 0.00508331,\n",
       "        0.00382623, 0.00547723, 0.00260768, 0.00547723, 0.00376298,\n",
       "        0.00491528, 0.00708802, 0.00132665, 0.00401995, 0.00146969,\n",
       "        0.00185472, 0.00252982, 0.00316228, 0.00695701, 0.00685274,\n",
       "        0.00700286, 0.0086487 , 0.00806474, 0.00666333, 0.00063246,\n",
       "        0.00109545, 0.00109545, 0.00219089, 0.00365513, 0.00387814,\n",
       "        0.00442719, 0.00275681, 0.00193907, 0.00331059, 0.00700286,\n",
       "        0.00959166, 0.0004    , 0.33877036, 0.0004    , 0.0004    ,\n",
       "        0.00551362, 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.32422992,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.37546291, 0.0004    , 0.0004    , 0.0008    ,\n",
       "        0.0004    , 0.0004    , 0.01212271, 0.0004    , 0.0004    ,\n",
       "        0.26830028, 0.0004    , 0.0004    , 0.38272371, 0.0004    ,\n",
       "        0.0004    , 0.05495671, 0.0004    , 0.0004    , 0.01070701,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.28065851, 0.0004    , 0.0004    , 0.14744165, 0.0004    ,\n",
       "        0.0004    , 0.1088    , 0.0004    , 0.0004    , 0.12851054,\n",
       "        0.0004    , 0.0004    , 0.20106258, 0.0004    , 0.0004    ,\n",
       "        0.18352046, 0.0004    , 0.0004    , 0.16951649, 0.0004    ,\n",
       "        0.0004    , 0.116594  , 0.0004    , 0.0004    , 0.24673273,\n",
       "        0.0004    , 0.0004    , 0.15709411, 0.0004    , 0.0004    ,\n",
       "        0.10844538, 0.0004    , 0.0004    , 0.12620872, 0.0004    ,\n",
       "        0.0004    , 0.2201812 , 0.0004    , 0.0004    , 0.15504657,\n",
       "        0.0004    , 0.0004    , 0.11171643, 0.0004    , 0.0004    ,\n",
       "        0.15929269, 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004899 , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004899 , 0.0004899 ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    ]),\n",
       " 'rank_test_recall_micro': array([197, 165, 186, 212, 216, 221, 248, 244, 241, 256, 258, 238, 165,\n",
       "        179, 182, 210, 208, 204, 239, 236, 247, 253, 229, 262, 165, 176,\n",
       "        183, 209, 197, 199, 236, 225, 228, 226, 249, 252, 165, 174, 165,\n",
       "        196, 191, 186, 216, 215, 223, 243, 218, 234, 186, 190, 184, 213,\n",
       "        214, 211, 240, 232, 244, 265, 261, 266, 194, 179, 184, 204, 219,\n",
       "        201, 229, 246, 241, 259, 259, 267, 179, 189, 174, 201, 206, 207,\n",
       "        229, 235, 232, 254, 255, 264, 177, 177,   1, 191, 200, 195, 224,\n",
       "        222, 226, 257, 251, 249,   2, 275,   2,   2, 191,   2,   2,   2,\n",
       "          2,   2,   2,   2,   2, 271,   2,   2,   2,   2,   2,   2,   2,\n",
       "          2,   2,   2,   2, 281,   2,   2, 165,   2,   2, 220,   2,   2,\n",
       "        268,   2,   2, 286,   2,   2, 263,   2,   2, 203,   2,   2,   2,\n",
       "          2,   2, 269,   2,   2, 272,   2,   2, 284,   2,   2, 273,   2,\n",
       "          2, 288,   2,   2, 280,   2,   2, 276,   2,   2, 278,   2,   2,\n",
       "        287,   2,   2, 285,   2,   2, 270,   2,   2, 277,   2,   2, 274,\n",
       "          2,   2, 282,   2,   2, 279,   2,   2, 283,   2,   2,   2,   2,\n",
       "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
       "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
       "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
       "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2, 165,   2,\n",
       "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
       "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
       "        165, 165,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
       "          2,   2]),\n",
       " 'split0_test_f1_micro': array([0.929, 0.93 , 0.93 , 0.91 , 0.918, 0.918, 0.909, 0.909, 0.908,\n",
       "        0.906, 0.898, 0.885, 0.93 , 0.93 , 0.93 , 0.918, 0.926, 0.925,\n",
       "        0.912, 0.91 , 0.91 , 0.906, 0.93 , 0.89 , 0.93 , 0.93 , 0.925,\n",
       "        0.917, 0.925, 0.925, 0.911, 0.914, 0.916, 0.93 , 0.905, 0.907,\n",
       "        0.93 , 0.929, 0.93 , 0.926, 0.929, 0.927, 0.925, 0.919, 0.908,\n",
       "        0.902, 0.903, 0.912, 0.927, 0.926, 0.927, 0.925, 0.918, 0.917,\n",
       "        0.911, 0.913, 0.903, 0.902, 0.893, 0.884, 0.929, 0.926, 0.93 ,\n",
       "        0.923, 0.914, 0.919, 0.905, 0.91 , 0.922, 0.895, 0.897, 0.898,\n",
       "        0.927, 0.93 , 0.928, 0.928, 0.927, 0.922, 0.906, 0.914, 0.907,\n",
       "        0.891, 0.903, 0.893, 0.929, 0.927, 0.929, 0.929, 0.918, 0.929,\n",
       "        0.91 , 0.925, 0.914, 0.904, 0.896, 0.911, 0.93 , 0.853, 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.78 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.08 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.156, 0.93 , 0.93 , 0.82 , 0.93 , 0.93 , 0.38 , 0.93 ,\n",
       "        0.93 , 0.716, 0.93 , 0.93 , 0.609, 0.93 , 0.93 , 0.802, 0.93 ,\n",
       "        0.93 , 0.309, 0.93 , 0.93 , 0.686, 0.93 , 0.93 , 0.725, 0.93 ,\n",
       "        0.93 , 0.383, 0.93 , 0.93 , 0.578, 0.93 , 0.93 , 0.403, 0.93 ,\n",
       "        0.93 , 0.321, 0.93 , 0.93 , 0.636, 0.93 , 0.93 , 0.419, 0.93 ,\n",
       "        0.93 , 0.556, 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ]),\n",
       " 'split1_test_f1_micro': array([0.911, 0.93 , 0.93 , 0.923, 0.921, 0.926, 0.903, 0.902, 0.921,\n",
       "        0.896, 0.898, 0.93 , 0.93 , 0.93 , 0.927, 0.927, 0.92 , 0.929,\n",
       "        0.917, 0.912, 0.916, 0.897, 0.93 , 0.898, 0.93 , 0.93 , 0.93 ,\n",
       "        0.925, 0.927, 0.925, 0.915, 0.917, 0.908, 0.908, 0.907, 0.916,\n",
       "        0.93 , 0.93 , 0.93 , 0.928, 0.927, 0.928, 0.926, 0.92 , 0.923,\n",
       "        0.913, 0.93 , 0.914, 0.929, 0.927, 0.93 , 0.923, 0.925, 0.922,\n",
       "        0.912, 0.912, 0.903, 0.903, 0.897, 0.894, 0.929, 0.93 , 0.927,\n",
       "        0.929, 0.927, 0.928, 0.918, 0.909, 0.909, 0.904, 0.897, 0.898,\n",
       "        0.931, 0.931, 0.93 , 0.923, 0.926, 0.929, 0.911, 0.91 , 0.909,\n",
       "        0.907, 0.911, 0.909, 0.929, 0.93 , 0.93 , 0.927, 0.926, 0.919,\n",
       "        0.923, 0.918, 0.917, 0.906, 0.912, 0.912, 0.93 , 0.889, 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.899, 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.919, 0.93 , 0.93 , 0.903, 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.85 , 0.93 , 0.93 , 0.728, 0.93 , 0.93 , 0.512, 0.93 ,\n",
       "        0.93 , 0.836, 0.93 , 0.93 , 0.32 , 0.93 , 0.93 , 0.525, 0.93 ,\n",
       "        0.93 , 0.573, 0.93 , 0.93 , 0.682, 0.93 , 0.93 , 0.142, 0.93 ,\n",
       "        0.93 , 0.586, 0.93 , 0.93 , 0.787, 0.93 , 0.93 , 0.605, 0.93 ,\n",
       "        0.93 , 0.839, 0.93 , 0.93 , 0.268, 0.93 , 0.93 , 0.685, 0.93 ,\n",
       "        0.93 , 0.85 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.929, 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.929, 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ]),\n",
       " 'split2_test_f1_micro': array([0.93 , 0.93 , 0.927, 0.929, 0.924, 0.913, 0.907, 0.905, 0.908,\n",
       "        0.895, 0.901, 0.93 , 0.929, 0.927, 0.93 , 0.918, 0.926, 0.927,\n",
       "        0.916, 0.91 , 0.912, 0.906, 0.904, 0.909, 0.929, 0.927, 0.929,\n",
       "        0.926, 0.928, 0.927, 0.913, 0.914, 0.917, 0.93 , 0.91 , 0.903,\n",
       "        0.929, 0.929, 0.93 , 0.928, 0.926, 0.93 , 0.921, 0.921, 0.922,\n",
       "        0.915, 0.916, 0.93 , 0.925, 0.927, 0.928, 0.92 , 0.923, 0.921,\n",
       "        0.915, 0.917, 0.92 , 0.896, 0.909, 0.901, 0.928, 0.93 , 0.93 ,\n",
       "        0.925, 0.927, 0.928, 0.914, 0.909, 0.91 , 0.9  , 0.91 , 0.9  ,\n",
       "        0.929, 0.928, 0.93 , 0.923, 0.925, 0.926, 0.915, 0.921, 0.922,\n",
       "        0.918, 0.909, 0.894, 0.929, 0.93 , 0.932, 0.929, 0.927, 0.929,\n",
       "        0.918, 0.918, 0.919, 0.899, 0.901, 0.913, 0.93 , 0.378, 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.584, 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.259, 0.93 , 0.93 , 0.07 , 0.93 ,\n",
       "        0.93 , 0.79 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.91 , 0.93 , 0.93 , 0.396, 0.93 , 0.93 , 0.692, 0.93 ,\n",
       "        0.93 , 0.667, 0.93 , 0.93 , 0.457, 0.93 , 0.93 , 0.803, 0.93 ,\n",
       "        0.93 , 0.699, 0.93 , 0.93 , 0.707, 0.93 , 0.93 , 0.713, 0.93 ,\n",
       "        0.93 , 0.309, 0.93 , 0.93 , 0.713, 0.93 , 0.93 , 0.555, 0.93 ,\n",
       "        0.93 , 0.437, 0.93 , 0.93 , 0.652, 0.93 , 0.93 , 0.621, 0.93 ,\n",
       "        0.93 , 0.484, 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.929, 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ]),\n",
       " 'split3_test_f1_micro': array([0.93 , 0.93 , 0.926, 0.925, 0.923, 0.921, 0.915, 0.922, 0.913,\n",
       "        0.901, 0.904, 0.907, 0.93 , 0.93 , 0.93 , 0.927, 0.926, 0.922,\n",
       "        0.905, 0.913, 0.905, 0.911, 0.907, 0.896, 0.93 , 0.93 , 0.93 ,\n",
       "        0.923, 0.924, 0.926, 0.912, 0.928, 0.923, 0.908, 0.911, 0.906,\n",
       "        0.93 , 0.929, 0.93 , 0.923, 0.926, 0.929, 0.924, 0.924, 0.924,\n",
       "        0.91 , 0.93 , 0.909, 0.929, 0.927, 0.932, 0.921, 0.923, 0.923,\n",
       "        0.912, 0.916, 0.915, 0.895, 0.907, 0.913, 0.922, 0.929, 0.93 ,\n",
       "        0.926, 0.921, 0.927, 0.921, 0.916, 0.906, 0.906, 0.903, 0.89 ,\n",
       "        0.929, 0.93 , 0.931, 0.924, 0.921, 0.923, 0.916, 0.903, 0.911,\n",
       "        0.903, 0.888, 0.905, 0.93 , 0.929, 0.93 , 0.923, 0.928, 0.929,\n",
       "        0.918, 0.921, 0.914, 0.903, 0.908, 0.915, 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.916, 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.105, 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.179, 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.916, 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.823, 0.93 , 0.93 , 0.753, 0.93 , 0.93 , 0.564, 0.93 ,\n",
       "        0.93 , 0.445, 0.93 , 0.93 , 0.1  , 0.93 , 0.93 , 0.368, 0.93 ,\n",
       "        0.93 , 0.652, 0.93 , 0.93 , 0.462, 0.93 , 0.93 , 0.234, 0.93 ,\n",
       "        0.93 , 0.757, 0.93 , 0.93 , 0.588, 0.93 , 0.93 , 0.787, 0.93 ,\n",
       "        0.93 , 0.882, 0.93 , 0.93 , 0.701, 0.93 , 0.93 , 0.514, 0.93 ,\n",
       "        0.93 , 0.377, 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ,\n",
       "        0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 , 0.93 ]),\n",
       " 'split4_test_f1_micro': array([0.929, 0.928, 0.928, 0.925, 0.923, 0.924, 0.91 , 0.919, 0.91 ,\n",
       "        0.912, 0.907, 0.911, 0.929, 0.927, 0.926, 0.926, 0.921, 0.919,\n",
       "        0.912, 0.922, 0.911, 0.906, 0.904, 0.906, 0.929, 0.929, 0.929,\n",
       "        0.927, 0.925, 0.924, 0.916, 0.916, 0.916, 0.905, 0.907, 0.897,\n",
       "        0.929, 0.929, 0.928, 0.927, 0.927, 0.927, 0.913, 0.926, 0.919,\n",
       "        0.919, 0.929, 0.906, 0.931, 0.929, 0.925, 0.922, 0.921, 0.93 ,\n",
       "        0.911, 0.915, 0.916, 0.897, 0.898, 0.89 , 0.925, 0.929, 0.925,\n",
       "        0.919, 0.918, 0.921, 0.917, 0.911, 0.913, 0.901, 0.899, 0.881,\n",
       "        0.928, 0.92 , 0.927, 0.925, 0.921, 0.92 , 0.927, 0.921, 0.924,\n",
       "        0.906, 0.902, 0.894, 0.928, 0.929, 0.929, 0.927, 0.927, 0.926,\n",
       "        0.921, 0.918, 0.917, 0.897, 0.915, 0.889, 0.929, 0.079, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.071, 0.929, 0.929, 0.928, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.543, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.831, 0.929, 0.929, 0.64 , 0.929, 0.929, 0.43 , 0.929,\n",
       "        0.929, 0.612, 0.929, 0.929, 0.65 , 0.929, 0.929, 0.433, 0.929,\n",
       "        0.929, 0.815, 0.929, 0.929, 0.447, 0.929, 0.929, 0.604, 0.929,\n",
       "        0.929, 0.519, 0.929, 0.929, 0.854, 0.929, 0.929, 0.662, 0.929,\n",
       "        0.929, 0.68 , 0.929, 0.929, 0.531, 0.929, 0.929, 0.722, 0.929,\n",
       "        0.929, 0.499, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929,\n",
       "        0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929, 0.929]),\n",
       " 'mean_test_f1_micro': array([0.9258, 0.9296, 0.9282, 0.9224, 0.9218, 0.9204, 0.9088, 0.9114,\n",
       "        0.912 , 0.902 , 0.9016, 0.9126, 0.9296, 0.9288, 0.9286, 0.9232,\n",
       "        0.9238, 0.9244, 0.9124, 0.9134, 0.9108, 0.9052, 0.915 , 0.8998,\n",
       "        0.9296, 0.9292, 0.9286, 0.9236, 0.9258, 0.9254, 0.9134, 0.9178,\n",
       "        0.916 , 0.9162, 0.908 , 0.9058, 0.9296, 0.9292, 0.9296, 0.9264,\n",
       "        0.927 , 0.9282, 0.9218, 0.922 , 0.9192, 0.9118, 0.9216, 0.9142,\n",
       "        0.9282, 0.9272, 0.9284, 0.9222, 0.922 , 0.9226, 0.9122, 0.9146,\n",
       "        0.9114, 0.8986, 0.9008, 0.8964, 0.9266, 0.9288, 0.9284, 0.9244,\n",
       "        0.9214, 0.9246, 0.915 , 0.911 , 0.912 , 0.9012, 0.9012, 0.8934,\n",
       "        0.9288, 0.9278, 0.9292, 0.9246, 0.924 , 0.924 , 0.915 , 0.9138,\n",
       "        0.9146, 0.905 , 0.9026, 0.899 , 0.929 , 0.929 , 0.93  , 0.927 ,\n",
       "        0.9252, 0.9264, 0.918 , 0.92  , 0.9162, 0.9018, 0.9064, 0.908 ,\n",
       "        0.9298, 0.6258, 0.9298, 0.9298, 0.927 , 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.6956, 0.9298, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.578 , 0.9298, 0.9298, 0.9296, 0.9298, 0.9298, 0.9208,\n",
       "        0.9298, 0.9298, 0.7956, 0.9298, 0.9298, 0.5106, 0.9298, 0.9298,\n",
       "        0.8996, 0.9298, 0.9298, 0.9244, 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.714 , 0.9298, 0.9298, 0.6674, 0.9298, 0.9298, 0.5156,\n",
       "        0.9298, 0.9298, 0.6552, 0.9298, 0.9298, 0.4272, 0.9298, 0.9298,\n",
       "        0.5862, 0.9298, 0.9298, 0.6096, 0.9298, 0.9298, 0.5968, 0.9298,\n",
       "        0.9298, 0.4836, 0.9298, 0.9298, 0.5108, 0.9298, 0.9298, 0.704 ,\n",
       "        0.9298, 0.9298, 0.6024, 0.9298, 0.9298, 0.6318, 0.9298, 0.9298,\n",
       "        0.5576, 0.9298, 0.9298, 0.5922, 0.9298, 0.9298, 0.5532, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9296, 0.9298, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.9296, 0.9296, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298,\n",
       "        0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298, 0.9298]),\n",
       " 'std_test_f1_micro': array([0.0074135 , 0.0008    , 0.0016    , 0.00649923, 0.00213542,\n",
       "        0.00458694, 0.00391918, 0.00781281, 0.00485798, 0.0063561 ,\n",
       "        0.00349857, 0.01674037, 0.0004899 , 0.00146969, 0.00174356,\n",
       "        0.00426146, 0.00271293, 0.00355528, 0.00422374, 0.00445421,\n",
       "        0.00354401, 0.00453431, 0.01229634, 0.00688186, 0.0004899 ,\n",
       "        0.00116619, 0.00185472, 0.00355528, 0.00146969, 0.0010198 ,\n",
       "        0.00185472, 0.00523068, 0.00477493, 0.01132078, 0.00219089,\n",
       "        0.00617738, 0.0004899 , 0.0004    , 0.0008    , 0.00185472,\n",
       "        0.00109545, 0.00116619, 0.00470744, 0.00260768, 0.00584466,\n",
       "        0.00570614, 0.01070701, 0.00835225, 0.00203961, 0.0009798 ,\n",
       "        0.00241661, 0.00172047, 0.00236643, 0.00422374, 0.00146969,\n",
       "        0.00185472, 0.00705975, 0.0032619 , 0.00614492, 0.00997196,\n",
       "        0.00272764, 0.00146969, 0.00205913, 0.00332265, 0.00508331,\n",
       "        0.00382623, 0.00547723, 0.00260768, 0.00547723, 0.00376298,\n",
       "        0.00491528, 0.00708802, 0.00132665, 0.00401995, 0.00146969,\n",
       "        0.00185472, 0.00252982, 0.00316228, 0.00695701, 0.00685274,\n",
       "        0.00700286, 0.0086487 , 0.00806474, 0.00666333, 0.00063246,\n",
       "        0.00109545, 0.00109545, 0.00219089, 0.00365513, 0.00387814,\n",
       "        0.00442719, 0.00275681, 0.00193907, 0.00331059, 0.00700286,\n",
       "        0.00959166, 0.0004    , 0.33877036, 0.0004    , 0.0004    ,\n",
       "        0.00551362, 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.32422992,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.37546291, 0.0004    , 0.0004    , 0.0008    ,\n",
       "        0.0004    , 0.0004    , 0.01212271, 0.0004    , 0.0004    ,\n",
       "        0.26830028, 0.0004    , 0.0004    , 0.38272371, 0.0004    ,\n",
       "        0.0004    , 0.05495671, 0.0004    , 0.0004    , 0.01070701,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.28065851, 0.0004    , 0.0004    , 0.14744165, 0.0004    ,\n",
       "        0.0004    , 0.1088    , 0.0004    , 0.0004    , 0.12851054,\n",
       "        0.0004    , 0.0004    , 0.20106258, 0.0004    , 0.0004    ,\n",
       "        0.18352046, 0.0004    , 0.0004    , 0.16951649, 0.0004    ,\n",
       "        0.0004    , 0.116594  , 0.0004    , 0.0004    , 0.24673273,\n",
       "        0.0004    , 0.0004    , 0.15709411, 0.0004    , 0.0004    ,\n",
       "        0.10844538, 0.0004    , 0.0004    , 0.12620872, 0.0004    ,\n",
       "        0.0004    , 0.2201812 , 0.0004    , 0.0004    , 0.15504657,\n",
       "        0.0004    , 0.0004    , 0.11171643, 0.0004    , 0.0004    ,\n",
       "        0.15929269, 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004899 , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004899 , 0.0004899 ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    ]),\n",
       " 'rank_test_f1_micro': array([197, 165, 186, 212, 216, 221, 248, 244, 241, 256, 258, 238, 165,\n",
       "        179, 182, 210, 208, 204, 239, 236, 247, 253, 229, 262, 165, 176,\n",
       "        183, 209, 197, 199, 236, 225, 228, 226, 249, 252, 165, 174, 165,\n",
       "        196, 191, 186, 216, 215, 223, 243, 218, 234, 186, 190, 184, 213,\n",
       "        214, 211, 240, 232, 244, 265, 261, 266, 194, 179, 184, 204, 219,\n",
       "        201, 229, 246, 241, 259, 259, 267, 179, 189, 174, 201, 206, 206,\n",
       "        229, 235, 232, 254, 255, 264, 177, 177,   1, 191, 200, 195, 224,\n",
       "        222, 226, 257, 251, 249,   2, 275,   2,   2, 191,   2,   2,   2,\n",
       "          2,   2,   2,   2,   2, 271,   2,   2,   2,   2,   2,   2,   2,\n",
       "          2,   2,   2,   2, 281,   2,   2, 165,   2,   2, 220,   2,   2,\n",
       "        268,   2,   2, 286,   2,   2, 263,   2,   2, 203,   2,   2,   2,\n",
       "          2,   2, 269,   2,   2, 272,   2,   2, 284,   2,   2, 273,   2,\n",
       "          2, 288,   2,   2, 280,   2,   2, 276,   2,   2, 278,   2,   2,\n",
       "        287,   2,   2, 285,   2,   2, 270,   2,   2, 277,   2,   2, 274,\n",
       "          2,   2, 282,   2,   2, 279,   2,   2, 283,   2,   2,   2,   2,\n",
       "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
       "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
       "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
       "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2, 165,   2,\n",
       "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
       "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
       "        165, 165,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
       "          2,   2]),\n",
       " 'split0_test_roc_auc_ovo': array([0.54886329, 0.55973118, 0.61343318, 0.56609831, 0.60142857,\n",
       "        0.57294931, 0.56494624, 0.60560676, 0.58233487, 0.59705069,\n",
       "        0.57569892, 0.58019969, 0.5509063 , 0.5352381 , 0.50927035,\n",
       "        0.56606759, 0.57675883, 0.58445469, 0.56384025, 0.63571429,\n",
       "        0.58115207, 0.55591398, 0.4922427 , 0.55543779, 0.54135177,\n",
       "        0.52394777, 0.55960061, 0.51752688, 0.58691244, 0.57278034,\n",
       "        0.56029186, 0.57826421, 0.58904762, 0.49373272, 0.53857143,\n",
       "        0.58210445, 0.53774194, 0.49133641, 0.57082949, 0.60376344,\n",
       "        0.60239631, 0.59450077, 0.58807988, 0.56225806, 0.56403994,\n",
       "        0.61195084, 0.58072197, 0.59365591, 0.579447  , 0.64473118,\n",
       "        0.61021505, 0.53408602, 0.59221198, 0.54458525, 0.5622427 ,\n",
       "        0.51995392, 0.54072197, 0.59356375, 0.58235791, 0.60815668,\n",
       "        0.59176651, 0.55210445, 0.55884793, 0.58619816, 0.57931644,\n",
       "        0.64923963, 0.52380952, 0.5321659 , 0.57345622, 0.55009217,\n",
       "        0.60502304, 0.58536098, 0.563149  , 0.61771121, 0.55835637,\n",
       "        0.61391705, 0.52990783, 0.56729647, 0.60308756, 0.52817204,\n",
       "        0.61230415, 0.60112135, 0.5881106 , 0.57986175, 0.59089094,\n",
       "        0.53967742, 0.58677419, 0.64603687, 0.56288786, 0.5193702 ,\n",
       "        0.63235023, 0.5759447 , 0.60502304, 0.61471582, 0.61104455,\n",
       "        0.60082949, 0.43411674, 0.47124424, 0.47325653, 0.51311828,\n",
       "        0.54422427, 0.43655914, 0.52453149, 0.46308756, 0.45454685,\n",
       "        0.48526882, 0.52399386, 0.48290323, 0.54293395, 0.446298  ,\n",
       "        0.45486943, 0.44095238, 0.51442396, 0.56215054, 0.49897081,\n",
       "        0.51256528, 0.50092166, 0.5737788 , 0.51374808, 0.49953917,\n",
       "        0.45516129, 0.46018433, 0.5247619 , 0.5500768 , 0.48907834,\n",
       "        0.54738863, 0.55324117, 0.52781874, 0.53208909, 0.52018433,\n",
       "        0.47625192, 0.48682028, 0.49536098, 0.493702  , 0.44639017,\n",
       "        0.53611367, 0.51010753, 0.5616129 , 0.43579109, 0.4525192 ,\n",
       "        0.44832565, 0.49568356, 0.55205837, 0.48248848, 0.48402458,\n",
       "        0.45943164, 0.55700461, 0.5281874 , 0.46178187, 0.43923195,\n",
       "        0.5164977 , 0.48319508, 0.46001536, 0.51572965, 0.45109063,\n",
       "        0.54119816, 0.50697389, 0.47958525, 0.50485407, 0.49373272,\n",
       "        0.47012289, 0.43414747, 0.50184332, 0.46878648, 0.5115361 ,\n",
       "        0.47185868, 0.50918587, 0.49574501, 0.51969278, 0.44993856,\n",
       "        0.53150538, 0.44379416, 0.45256528, 0.51763441, 0.49324117,\n",
       "        0.59371736, 0.53764977, 0.47864823, 0.46004608, 0.45554531,\n",
       "        0.47164363, 0.49906298, 0.47221198, 0.53393241, 0.49239631,\n",
       "        0.56222734, 0.52938556, 0.46875576, 0.47850998, 0.47563748,\n",
       "        0.51138249, 0.46039939, 0.48814132, 0.49093702, 0.4944086 ,\n",
       "        0.49305684, 0.486851  , 0.48861751, 0.48639017, 0.48824885,\n",
       "        0.48958525, 0.48691244, 0.48774194, 0.49284178, 0.48854071,\n",
       "        0.49362519, 0.4859447 , 0.48763441, 0.49360983, 0.48795699,\n",
       "        0.49221198, 0.49021505, 0.48688172, 0.48860215, 0.49047619,\n",
       "        0.48887865, 0.48663594, 0.4883871 , 0.48953917, 0.49915515,\n",
       "        0.4865745 , 0.4888172 , 0.48843318, 0.4888172 , 0.49019969,\n",
       "        0.486851  , 0.48987711, 0.48961598, 0.48534562, 0.48357911,\n",
       "        0.48792627, 0.48941628, 0.48970814, 0.49099846, 0.48938556,\n",
       "        0.48947773, 0.48860215, 0.48837174, 0.48735791, 0.48920123,\n",
       "        0.59149002, 0.56843318, 0.51238095, 0.61156682, 0.61580645,\n",
       "        0.60574501, 0.55376344, 0.57533026, 0.54520737, 0.55634409,\n",
       "        0.58356375, 0.59735791, 0.59302611, 0.52356375, 0.56439324,\n",
       "        0.58963134, 0.58463902, 0.59986175, 0.54505376, 0.53675883,\n",
       "        0.58316436, 0.55672811, 0.56947773, 0.58516129, 0.61093702,\n",
       "        0.52537634, 0.61921659, 0.60076805, 0.58115207, 0.59800307,\n",
       "        0.60545315, 0.60554531, 0.60771121, 0.59082949, 0.57241167,\n",
       "        0.56552995, 0.6156682 , 0.61634409, 0.58047619, 0.57654378,\n",
       "        0.58843318, 0.58654378, 0.60556068, 0.57889401, 0.60490015,\n",
       "        0.58311828, 0.58642089, 0.56715822]),\n",
       " 'split1_test_roc_auc_ovo': array([0.51871736, 0.53711982, 0.53827957, 0.63375576, 0.59849462,\n",
       "        0.60612903, 0.59807988, 0.55367896, 0.62785714, 0.56066052,\n",
       "        0.58247312, 0.47347158, 0.55366359, 0.49723502, 0.57952381,\n",
       "        0.58907834, 0.5711828 , 0.55139785, 0.56852535, 0.57348694,\n",
       "        0.51903226, 0.60347158, 0.47394777, 0.53698925, 0.57104455,\n",
       "        0.52437788, 0.5650384 , 0.59333333, 0.56470046, 0.55360983,\n",
       "        0.65797235, 0.57840246, 0.60451613, 0.61437788, 0.59348694,\n",
       "        0.62863287, 0.57915515, 0.57416283, 0.48407066, 0.58274962,\n",
       "        0.60261137, 0.60697389, 0.62483871, 0.55545315, 0.586851  ,\n",
       "        0.59248848, 0.47743472, 0.59202765, 0.62119816, 0.54076805,\n",
       "        0.62060676, 0.5359447 , 0.57397849, 0.56994624, 0.53324885,\n",
       "        0.5825192 , 0.5803533 , 0.61068356, 0.62043779, 0.58149002,\n",
       "        0.57563748, 0.56729647, 0.56241167, 0.61678187, 0.60145929,\n",
       "        0.56546851, 0.61628264, 0.60258065, 0.54800307, 0.60403994,\n",
       "        0.57070661, 0.59192012, 0.52726575, 0.55230415, 0.55554531,\n",
       "        0.6175576 , 0.54738863, 0.57983103, 0.6103533 , 0.59242704,\n",
       "        0.56422427, 0.59718894, 0.64373272, 0.58870968, 0.58695853,\n",
       "        0.59192012, 0.57012289, 0.57109063, 0.60645161, 0.58262673,\n",
       "        0.57832565, 0.53367127, 0.58437788, 0.61674347, 0.58322581,\n",
       "        0.589447  , 0.49454685, 0.4622427 , 0.5018894 , 0.47534562,\n",
       "        0.45940092, 0.59723502, 0.44081413, 0.49382488, 0.51858679,\n",
       "        0.54843318, 0.52081413, 0.53093702, 0.54167435, 0.4841321 ,\n",
       "        0.54508449, 0.52841782, 0.5290937 , 0.44867896, 0.44293395,\n",
       "        0.43364055, 0.45549923, 0.54456221, 0.46860215, 0.5271275 ,\n",
       "        0.52781874, 0.48195084, 0.54362519, 0.47170507, 0.44149002,\n",
       "        0.50490015, 0.44502304, 0.50136713, 0.50929339, 0.48319508,\n",
       "        0.50442396, 0.46734255, 0.44884793, 0.45456221, 0.51296467,\n",
       "        0.51981567, 0.49637481, 0.55631336, 0.49167435, 0.4396467 ,\n",
       "        0.53138249, 0.54708141, 0.49142857, 0.44556068, 0.53754224,\n",
       "        0.47310292, 0.46691244, 0.48969278, 0.52746544, 0.50261137,\n",
       "        0.43877112, 0.46026114, 0.47709677, 0.5352381 , 0.52989247,\n",
       "        0.51368664, 0.46345622, 0.5546851 , 0.53490015, 0.5196467 ,\n",
       "        0.47617512, 0.49955453, 0.54852535, 0.49680492, 0.53806452,\n",
       "        0.49241167, 0.51935484, 0.52698925, 0.56242704, 0.49904762,\n",
       "        0.48984639, 0.53333333, 0.4528725 , 0.48652842, 0.526298  ,\n",
       "        0.46192012, 0.49274962, 0.49142857, 0.49880184, 0.4877573 ,\n",
       "        0.5264977 , 0.53812596, 0.50139785, 0.47437788, 0.53963134,\n",
       "        0.4652381 , 0.53840246, 0.50362519, 0.53242704, 0.52185868,\n",
       "        0.58654378, 0.46646697, 0.47505376, 0.48766513, 0.47347158,\n",
       "        0.47654378, 0.47579109, 0.47477727, 0.47746544, 0.47883257,\n",
       "        0.47179724, 0.47691244, 0.47431644, 0.47689708, 0.47086022,\n",
       "        0.47327189, 0.47705069, 0.47460829, 0.47723502, 0.47030722,\n",
       "        0.4753149 , 0.47897081, 0.47834101, 0.47718894, 0.47301075,\n",
       "        0.47678955, 0.46152074, 0.46138249, 0.47259601, 0.47219662,\n",
       "        0.47436252, 0.47333333, 0.47866359, 0.47537634, 0.47562212,\n",
       "        0.47396313, 0.47167435, 0.47582181, 0.46419355, 0.47271889,\n",
       "        0.46582181, 0.473149  , 0.47018433, 0.47142857, 0.47660522,\n",
       "        0.47849462, 0.47597542, 0.47336406, 0.47662058, 0.47509985,\n",
       "        0.54884793, 0.50434716, 0.58058372, 0.59572965, 0.62033794,\n",
       "        0.56010753, 0.59021505, 0.60983103, 0.60158218, 0.62064516,\n",
       "        0.62357911, 0.60749616, 0.56276498, 0.59439324, 0.49953917,\n",
       "        0.56493088, 0.56357911, 0.62351767, 0.5621659 , 0.62456221,\n",
       "        0.60867896, 0.5958679 , 0.65219662, 0.60353303, 0.51956989,\n",
       "        0.55367127, 0.58795699, 0.5934255 , 0.5872043 , 0.60262673,\n",
       "        0.61463902, 0.58      , 0.61522273, 0.57860215, 0.59728111,\n",
       "        0.61089094, 0.54932412, 0.59738863, 0.58857143, 0.59493088,\n",
       "        0.56711214, 0.59958525, 0.59705069, 0.5897235 , 0.59632873,\n",
       "        0.58915515, 0.59821813, 0.60794163]),\n",
       " 'split2_test_roc_auc_ovo': array([0.54235791, 0.55272657, 0.58522273, 0.610553  , 0.57695084,\n",
       "        0.57589862, 0.6090553 , 0.57605223, 0.58354839, 0.58821813,\n",
       "        0.61033794, 0.4875576 , 0.5447235 , 0.60572965, 0.51354839,\n",
       "        0.58943164, 0.57473118, 0.56514593, 0.58236559, 0.57900154,\n",
       "        0.53176651, 0.60397849, 0.61030722, 0.52740399, 0.61890937,\n",
       "        0.5559447 , 0.54239631, 0.60353303, 0.52215054, 0.6102765 ,\n",
       "        0.58864823, 0.62073733, 0.57917051, 0.48751152, 0.56632873,\n",
       "        0.51655914, 0.60058372, 0.58391705, 0.62953917, 0.63360983,\n",
       "        0.526851  , 0.56640553, 0.58705069, 0.64193548, 0.6164977 ,\n",
       "        0.59940092, 0.61519201, 0.49178187, 0.6237788 , 0.52577573,\n",
       "        0.4850384 , 0.59115207, 0.50584485, 0.54380184, 0.60046851,\n",
       "        0.5859831 , 0.6252765 , 0.56260369, 0.6365745 , 0.58583717,\n",
       "        0.49499232, 0.56523041, 0.51403226, 0.55353303, 0.59150538,\n",
       "        0.57686636, 0.53129032, 0.56542243, 0.6018126 , 0.61752688,\n",
       "        0.59620584, 0.58490015, 0.57149002, 0.50877112, 0.54592934,\n",
       "        0.52542243, 0.54282642, 0.57136713, 0.55113671, 0.56213518,\n",
       "        0.63608295, 0.54192012, 0.57745008, 0.5743318 , 0.57708141,\n",
       "        0.54652842, 0.563702  , 0.51359447, 0.55298003, 0.56261137,\n",
       "        0.54190476, 0.51680492, 0.4665745 , 0.55995392, 0.60466974,\n",
       "        0.59903226, 0.50079877, 0.54745008, 0.46036866, 0.45132104,\n",
       "        0.51199693, 0.42912442, 0.55849462, 0.50491551, 0.50030722,\n",
       "        0.50337942, 0.52293395, 0.52605223, 0.49218126, 0.50142857,\n",
       "        0.54447005, 0.43890937, 0.50637481, 0.4772043 , 0.45419355,\n",
       "        0.45416283, 0.44056836, 0.48844854, 0.47955453, 0.47168971,\n",
       "        0.55533026, 0.43334869, 0.54866359, 0.45199693, 0.5230722 ,\n",
       "        0.46823349, 0.46493088, 0.54327189, 0.5381874 , 0.43943164,\n",
       "        0.47645161, 0.5103533 , 0.57844854, 0.49892473, 0.49058372,\n",
       "        0.55336406, 0.50281106, 0.49187404, 0.56540707, 0.50780338,\n",
       "        0.46632873, 0.55416283, 0.49752688, 0.45648233, 0.48198157,\n",
       "        0.58317972, 0.50517665, 0.42695853, 0.48912442, 0.49850998,\n",
       "        0.50376344, 0.50480799, 0.51545315, 0.5383871 , 0.58053763,\n",
       "        0.48129032, 0.50026114, 0.5200768 , 0.51470046, 0.54333333,\n",
       "        0.47165899, 0.50745008, 0.47089094, 0.53831029, 0.53064516,\n",
       "        0.51993856, 0.46784946, 0.51414747, 0.50803379, 0.46133641,\n",
       "        0.43886329, 0.4734255 , 0.53663594, 0.52219662, 0.5064977 ,\n",
       "        0.43482335, 0.42187404, 0.46164363, 0.57230415, 0.47726575,\n",
       "        0.47554531, 0.43580645, 0.51150538, 0.44745008, 0.52632873,\n",
       "        0.43311828, 0.44253456, 0.43996928, 0.52459293, 0.5077573 ,\n",
       "        0.51956989, 0.55254992, 0.48431644, 0.47734255, 0.4878341 ,\n",
       "        0.4815361 , 0.48511521, 0.48606759, 0.48665131, 0.48308756,\n",
       "        0.49026114, 0.48847926, 0.49133641, 0.48746544, 0.47640553,\n",
       "        0.48003072, 0.48130568, 0.48678955, 0.4890937 , 0.48708141,\n",
       "        0.48890937, 0.48149002, 0.49109063, 0.48966206, 0.48784946,\n",
       "        0.48709677, 0.49235023, 0.48385561, 0.48516129, 0.48741935,\n",
       "        0.48543779, 0.48568356, 0.49069124, 0.48585253, 0.48740399,\n",
       "        0.4912596 , 0.48797235, 0.48774194, 0.47947773, 0.48330261,\n",
       "        0.4918894 , 0.48325653, 0.4875576 , 0.48298003, 0.4875576 ,\n",
       "        0.48924731, 0.49070661, 0.49142857, 0.48906298, 0.48669739,\n",
       "        0.54760369, 0.55152074, 0.56273425, 0.50798771, 0.55571429,\n",
       "        0.57565284, 0.5655914 , 0.54276498, 0.53554531, 0.54278034,\n",
       "        0.58039939, 0.54674347, 0.57104455, 0.50390169, 0.53219662,\n",
       "        0.54044547, 0.56400922, 0.56155146, 0.53508449, 0.59316436,\n",
       "        0.53639017, 0.55138249, 0.5574808 , 0.57789555, 0.54274962,\n",
       "        0.54205837, 0.5590169 , 0.57084485, 0.57165899, 0.55219662,\n",
       "        0.53176651, 0.52402458, 0.54459293, 0.55129032, 0.55428571,\n",
       "        0.55339478, 0.55582181, 0.55299539, 0.57041475, 0.54030722,\n",
       "        0.55533026, 0.56780338, 0.56801843, 0.55917051, 0.57333333,\n",
       "        0.57428571, 0.58554531, 0.55330261]),\n",
       " 'split3_test_roc_auc_ovo': array([0.5797619 , 0.59386329, 0.60067588, 0.59379416, 0.57646697,\n",
       "        0.53858679, 0.5890937 , 0.58596006, 0.6028341 , 0.60499232,\n",
       "        0.55791091, 0.60695853, 0.56348694, 0.58014593, 0.59970814,\n",
       "        0.61967742, 0.57941628, 0.5896467 , 0.57940092, 0.59691244,\n",
       "        0.57376344, 0.63569892, 0.6346851 , 0.59660522, 0.58745008,\n",
       "        0.5781106 , 0.5993702 , 0.5927957 , 0.60388633, 0.6212596 ,\n",
       "        0.61264209, 0.60875576, 0.60758833, 0.62176651, 0.58717358,\n",
       "        0.56425499, 0.63184332, 0.63494624, 0.54428571, 0.57534562,\n",
       "        0.6003533 , 0.57711214, 0.62431644, 0.64877112, 0.65824885,\n",
       "        0.64904762, 0.51658986, 0.59030722, 0.57823349, 0.56293395,\n",
       "        0.57354839, 0.60789555, 0.55897081, 0.59261905, 0.55745776,\n",
       "        0.58471582, 0.53993088, 0.59980799, 0.62946237, 0.62910906,\n",
       "        0.59549155, 0.53305684, 0.50757296, 0.56737327, 0.59620584,\n",
       "        0.57104455, 0.56886329, 0.59728111, 0.57858679, 0.58356375,\n",
       "        0.58781874, 0.60950845, 0.59205837, 0.61605223, 0.54665131,\n",
       "        0.59016897, 0.63686636, 0.61612903, 0.58930876, 0.63364055,\n",
       "        0.59562212, 0.60016897, 0.58752688, 0.51637481, 0.56087558,\n",
       "        0.60015361, 0.50927803, 0.5987404 , 0.61603687, 0.52626728,\n",
       "        0.57030722, 0.6312596 , 0.57129032, 0.58290323, 0.51652842,\n",
       "        0.63620584, 0.48608295, 0.57115207, 0.54190476, 0.52829493,\n",
       "        0.46907834, 0.4918894 , 0.51906298, 0.49238095, 0.53457757,\n",
       "        0.51145929, 0.513149  , 0.48311828, 0.43290323, 0.4599232 ,\n",
       "        0.5121659 , 0.54172043, 0.53708141, 0.50894009, 0.51511521,\n",
       "        0.47471582, 0.5155914 , 0.51583717, 0.54044547, 0.48033794,\n",
       "        0.5512596 , 0.53337942, 0.446298  , 0.46669739, 0.49285714,\n",
       "        0.45778802, 0.42565284, 0.50761905, 0.48546851, 0.46946237,\n",
       "        0.49705069, 0.52912442, 0.53843318, 0.4730722 , 0.43975422,\n",
       "        0.45448541, 0.47113671, 0.48033794, 0.47703533, 0.50130568,\n",
       "        0.49178187, 0.52480799, 0.52855607, 0.48351767, 0.46536098,\n",
       "        0.55897081, 0.50924731, 0.53995392, 0.50823349, 0.45855607,\n",
       "        0.48614439, 0.50211982, 0.48834101, 0.56376344, 0.50215054,\n",
       "        0.52056836, 0.47445469, 0.48714286, 0.49497696, 0.51520737,\n",
       "        0.49872504, 0.5609831 , 0.51980031, 0.50250384, 0.47384025,\n",
       "        0.48308756, 0.55265745, 0.49285714, 0.57591398, 0.52138249,\n",
       "        0.52010753, 0.49548387, 0.5000768 , 0.43895545, 0.5099232 ,\n",
       "        0.50018433, 0.56648233, 0.46291859, 0.44187404, 0.49764977,\n",
       "        0.50265745, 0.55955453, 0.46943164, 0.49694316, 0.51337942,\n",
       "        0.47953917, 0.49317972, 0.55387097, 0.53156682, 0.49301075,\n",
       "        0.52709677, 0.49635945, 0.51866359, 0.51508449, 0.5088172 ,\n",
       "        0.51708141, 0.51557604, 0.50697389, 0.51654378, 0.5130722 ,\n",
       "        0.51299539, 0.51198157, 0.51410138, 0.51345622, 0.51390169,\n",
       "        0.51714286, 0.52082949, 0.50932412, 0.51645161, 0.51536098,\n",
       "        0.5141321 , 0.51806452, 0.51526882, 0.51365591, 0.51419355,\n",
       "        0.51499232, 0.5153149 , 0.51626728, 0.51222734, 0.51552995,\n",
       "        0.51827957, 0.51497696, 0.51563748, 0.51239631, 0.51872504,\n",
       "        0.51764977, 0.51502304, 0.51195084, 0.51803379, 0.51774194,\n",
       "        0.51447005, 0.51348694, 0.5118894 , 0.51345622, 0.51218126,\n",
       "        0.51156682, 0.51351767, 0.51658986, 0.51447005, 0.51380952,\n",
       "        0.57729647, 0.58101382, 0.5881874 , 0.56754224, 0.60399386,\n",
       "        0.57195084, 0.57070661, 0.55288786, 0.55721966, 0.57771121,\n",
       "        0.5915361 , 0.59341014, 0.55903226, 0.57250384, 0.55923195,\n",
       "        0.55831029, 0.57917051, 0.56434716, 0.5724424 , 0.55087558,\n",
       "        0.58047619, 0.57920123, 0.58070661, 0.59102919, 0.56357911,\n",
       "        0.56804916, 0.56367127, 0.56337942, 0.5628725 , 0.55900154,\n",
       "        0.56764977, 0.5528725 , 0.57261137, 0.5681874 , 0.56969278,\n",
       "        0.58927803, 0.53780338, 0.57115207, 0.54192012, 0.57317972,\n",
       "        0.56631336, 0.56380952, 0.56778802, 0.53675883, 0.53551459,\n",
       "        0.59036866, 0.55892473, 0.54769585]),\n",
       " 'split4_test_roc_auc_ovo': array([0.58518928, 0.56836065, 0.56474477, 0.55827105, 0.52403008,\n",
       "        0.53210328, 0.5939523 , 0.55688382, 0.61094013, 0.57538774,\n",
       "        0.61438166, 0.65010082, 0.5955821 , 0.61294895, 0.57657029,\n",
       "        0.57696448, 0.58020892, 0.57064237, 0.63803272, 0.6169287 ,\n",
       "        0.59949362, 0.57256781, 0.63838142, 0.58980579, 0.6244182 ,\n",
       "        0.56620022, 0.51477433, 0.58672812, 0.55288892, 0.58492397,\n",
       "        0.53005655, 0.6292697 , 0.64077685, 0.62755651, 0.6459164 ,\n",
       "        0.59211025, 0.54256432, 0.60877212, 0.61056111, 0.55649722,\n",
       "        0.6386998 , 0.56674601, 0.58293789, 0.60790794, 0.58797132,\n",
       "        0.65504328, 0.51360694, 0.60352643, 0.62196213, 0.56927788,\n",
       "        0.56169742, 0.59386892, 0.55066784, 0.6276399 , 0.60766537,\n",
       "        0.57410664, 0.57268909, 0.59659031, 0.55086493, 0.56206128,\n",
       "        0.60831729, 0.62537334, 0.57459179, 0.56827726, 0.63230188,\n",
       "        0.5977956 , 0.61535196, 0.61147076, 0.62923938, 0.6246911 ,\n",
       "        0.61677709, 0.58362013, 0.59664337, 0.56615473, 0.57370488,\n",
       "        0.6319077 , 0.61157689, 0.54085113, 0.61639807, 0.59844752,\n",
       "        0.6088176 , 0.6045877 , 0.60228324, 0.59456632, 0.61444231,\n",
       "        0.55294956, 0.56242514, 0.57046044, 0.63174093, 0.55766461,\n",
       "        0.60877212, 0.61426037, 0.60849922, 0.5801786 , 0.5796328 ,\n",
       "        0.60040328, 0.50839158, 0.48105641, 0.52505344, 0.49262421,\n",
       "        0.48043482, 0.47320305, 0.57676739, 0.44805106, 0.52811595,\n",
       "        0.51962583, 0.5370609 , 0.52073258, 0.46122591, 0.56462348,\n",
       "        0.51439531, 0.45702633, 0.4788126 , 0.5038433 , 0.51783684,\n",
       "        0.52059613, 0.43925772, 0.56371382, 0.56433542, 0.48190543,\n",
       "        0.50311557, 0.45930047, 0.47241468, 0.47059537, 0.49618703,\n",
       "        0.46616838, 0.50146303, 0.54599069, 0.55100896, 0.52381025,\n",
       "        0.54723389, 0.54307979, 0.56595764, 0.57972377, 0.51183311,\n",
       "        0.52544763, 0.55120605, 0.43601328, 0.5374096 , 0.48252702,\n",
       "        0.45015843, 0.48992556, 0.52258221, 0.48748465, 0.51745781,\n",
       "        0.52167255, 0.53166361, 0.49773344, 0.53622705, 0.51639655,\n",
       "        0.51544141, 0.52455313, 0.49665701, 0.59141285, 0.54674874,\n",
       "        0.56080292, 0.50090208, 0.44215346, 0.52849497, 0.56356221,\n",
       "        0.55131218, 0.44257797, 0.51958035, 0.50969542, 0.54341333,\n",
       "        0.55816492, 0.47787262, 0.57059689, 0.48370958, 0.47682651,\n",
       "        0.50234236, 0.45699601, 0.56036326, 0.57452357, 0.50658743,\n",
       "        0.46516776, 0.54307979, 0.50927091, 0.50817932, 0.49450416,\n",
       "        0.42900893, 0.48458891, 0.51577495, 0.48431601, 0.4365894 ,\n",
       "        0.52822208, 0.52629664, 0.43352689, 0.58819873, 0.47643233,\n",
       "        0.45235677, 0.55158508, 0.5058597 , 0.51216665, 0.50091724,\n",
       "        0.51763975, 0.51301566, 0.51830683, 0.51028669, 0.51789748,\n",
       "        0.51604785, 0.5179278 , 0.51152989, 0.51844328, 0.50872512,\n",
       "        0.51560818, 0.52691824, 0.50763353, 0.51814006, 0.52362831,\n",
       "        0.51589624, 0.51483497, 0.51430434, 0.51727588, 0.516533  ,\n",
       "        0.5179278 , 0.50525326, 0.50961961, 0.50185721, 0.51718492,\n",
       "        0.51283373, 0.50701193, 0.51468336, 0.51077184, 0.51325824,\n",
       "        0.51689686, 0.51680589, 0.51366758, 0.51313695, 0.50548068,\n",
       "        0.51163602, 0.51224245, 0.50528359, 0.51644203, 0.51530496,\n",
       "        0.51488046, 0.51721524, 0.51539593, 0.51480465, 0.51475917,\n",
       "        0.59591564, 0.53653027, 0.59098834, 0.56377447, 0.57329553,\n",
       "        0.5860762 , 0.58345336, 0.59734077, 0.58848679, 0.58011795,\n",
       "        0.60604315, 0.56265256, 0.59984233, 0.58349884, 0.58210403,\n",
       "        0.5975227 , 0.58885065, 0.57041495, 0.61847511, 0.61078852,\n",
       "        0.59168574, 0.61447263, 0.56953562, 0.5731136 , 0.59694659,\n",
       "        0.58792583, 0.59893267, 0.59400537, 0.60725602, 0.60504253,\n",
       "        0.58463591, 0.6117285 , 0.62432723, 0.58236177, 0.59433891,\n",
       "        0.59430859, 0.61882381, 0.60129778, 0.53982019, 0.57194621,\n",
       "        0.60102488, 0.59142801, 0.58498461, 0.58839582, 0.61459391,\n",
       "        0.56898983, 0.60513349, 0.59438439]),\n",
       " 'mean_test_roc_auc_ovo': array([0.55497795, 0.5623603 , 0.58047123, 0.59249446, 0.57547422,\n",
       "        0.5651334 , 0.59102548, 0.57563636, 0.60150293, 0.58526188,\n",
       "        0.58816051, 0.55965764, 0.56167249, 0.56625953, 0.5557242 ,\n",
       "        0.58824389, 0.5764596 , 0.57225751, 0.58643296, 0.60040878,\n",
       "        0.56104158, 0.59432616, 0.56991284, 0.56124841, 0.58863479,\n",
       "        0.54971623, 0.55623597, 0.57878341, 0.56610774, 0.58857005,\n",
       "        0.58992222, 0.60308589, 0.60421989, 0.56898903, 0.58629542,\n",
       "        0.57673234, 0.57837769, 0.57862693, 0.56785723, 0.59039315,\n",
       "        0.59418236, 0.58234767, 0.60144472, 0.60326515, 0.60272176,\n",
       "        0.62158623, 0.5407091 , 0.57425982, 0.60492392, 0.56869736,\n",
       "        0.5702212 , 0.57258945, 0.5563348 , 0.57571846, 0.57221664,\n",
       "        0.56945574, 0.57179435, 0.59264986, 0.6039395 , 0.59333084,\n",
       "        0.57324103, 0.5686123 , 0.54349132, 0.57843272, 0.60015777,\n",
       "        0.59208293, 0.57111955, 0.58178417, 0.58621961, 0.59598277,\n",
       "        0.59530626, 0.59106197, 0.5701213 , 0.57219869, 0.55603744,\n",
       "        0.59579475, 0.57371323, 0.57509496, 0.59405688, 0.58296447,\n",
       "        0.60341022, 0.58899742, 0.5998207 , 0.57076887, 0.58604975,\n",
       "        0.56624583, 0.55846045, 0.57998456, 0.59401946, 0.54970804,\n",
       "        0.586332  , 0.57438817, 0.56715299, 0.59089901, 0.57902026,\n",
       "        0.60518357, 0.48478738, 0.5066291 , 0.50049456, 0.49214082,\n",
       "        0.49302706, 0.48560221, 0.52393412, 0.48045199, 0.50722688,\n",
       "        0.51363331, 0.52359037, 0.50874867, 0.49418374, 0.49128107,\n",
       "        0.51419703, 0.48140527, 0.5131573 , 0.50016344, 0.48581007,\n",
       "        0.47913612, 0.47036767, 0.53726811, 0.51333713, 0.49211995,\n",
       "        0.51853709, 0.47363275, 0.50715268, 0.48221431, 0.48853694,\n",
       "        0.48889573, 0.47806219, 0.5252135 , 0.52320947, 0.48721673,\n",
       "        0.50028242, 0.50734407, 0.52540965, 0.49999698, 0.48030518,\n",
       "        0.51784529, 0.50632723, 0.50523031, 0.50146349, 0.4767604 ,\n",
       "        0.47759543, 0.52233227, 0.51843042, 0.47110676, 0.49727344,\n",
       "        0.51927153, 0.51400092, 0.49650521, 0.50456645, 0.48306118,\n",
       "        0.49212361, 0.49498743, 0.48751266, 0.54890623, 0.522084  ,\n",
       "        0.52350928, 0.4892096 , 0.49672869, 0.51558532, 0.52709647,\n",
       "        0.49359884, 0.48894263, 0.51212805, 0.50322019, 0.51949987,\n",
       "        0.50509228, 0.50538405, 0.52006715, 0.52995543, 0.48170632,\n",
       "        0.49653299, 0.48060658, 0.50050276, 0.50796769, 0.5085095 ,\n",
       "        0.49116258, 0.51236711, 0.48078199, 0.49624109, 0.48254446,\n",
       "        0.4810706 , 0.50342777, 0.49406436, 0.48740391, 0.50166504,\n",
       "        0.49366899, 0.50595979, 0.47994962, 0.5310591 , 0.49493931,\n",
       "        0.51938994, 0.50547216, 0.49440696, 0.49663917, 0.49308975,\n",
       "        0.49717157, 0.4952698 , 0.49494862, 0.49546748, 0.49622773,\n",
       "        0.49613737, 0.4964427 , 0.49580521, 0.49782076, 0.49168665,\n",
       "        0.49593577, 0.49840976, 0.49319798, 0.49890605, 0.49686698,\n",
       "        0.49729292, 0.49671507, 0.4971773 , 0.49727699, 0.49641259,\n",
       "        0.49713702, 0.49221502, 0.49190242, 0.4922762 , 0.4982972 ,\n",
       "        0.49549762, 0.4939646 , 0.49762177, 0.49464285, 0.49704182,\n",
       "        0.49732407, 0.49627055, 0.49575963, 0.49203753, 0.49256465,\n",
       "        0.49434871, 0.49431024, 0.49292461, 0.49506106, 0.49620692,\n",
       "        0.49673339, 0.49720342, 0.49703003, 0.49646323, 0.49591343,\n",
       "        0.57223075, 0.54836903, 0.56697493, 0.56932018, 0.59382961,\n",
       "        0.57990648, 0.57274597, 0.57563098, 0.56560826, 0.57551975,\n",
       "        0.5970243 , 0.58153205, 0.57714204, 0.55557227, 0.547493  ,\n",
       "        0.57016814, 0.5760497 , 0.5839386 , 0.56664433, 0.5832299 ,\n",
       "        0.58007908, 0.57953047, 0.58587947, 0.58614653, 0.56675645,\n",
       "        0.5554162 , 0.58575888, 0.58448464, 0.58202878, 0.5833741 ,\n",
       "        0.58082887, 0.57483418, 0.5928931 , 0.57425423, 0.57760204,\n",
       "        0.58268046, 0.57548827, 0.58783559, 0.56424054, 0.57138156,\n",
       "        0.57564276, 0.58183399, 0.58468049, 0.57058853, 0.58493414,\n",
       "        0.58118353, 0.58684851, 0.57409654]),\n",
       " 'std_test_roc_auc_ovo': array([0.02465139, 0.0187896 , 0.02662613, 0.02792237, 0.02776052,\n",
       "        0.02703084, 0.01461002, 0.01917687, 0.01717648, 0.01574631,\n",
       "        0.02136382, 0.0685066 , 0.01800264, 0.04394846, 0.03707378,\n",
       "        0.01793983, 0.00327783, 0.0137074 , 0.02668198, 0.02330093,\n",
       "        0.03055045, 0.02770355, 0.07177331, 0.02768878, 0.030801  ,\n",
       "        0.02201273, 0.02778478, 0.03110006, 0.02817449, 0.02459513,\n",
       "        0.04384174, 0.02123535, 0.02100245, 0.06415272, 0.03544968,\n",
       "        0.03670287, 0.03545743, 0.04847297, 0.05140608, 0.02638269,\n",
       "        0.03658391, 0.01599452, 0.01896684, 0.03887239, 0.03236904,\n",
       "        0.02571074, 0.04993725, 0.04149354, 0.02131718, 0.041078  ,\n",
       "        0.04791252, 0.0312066 , 0.02893707, 0.03163141, 0.02789471,\n",
       "        0.02509478, 0.03135337, 0.01609713, 0.03246946, 0.02312588,\n",
       "        0.04049006, 0.03089234, 0.02727273, 0.02180091, 0.01766228,\n",
       "        0.03060068, 0.03956238, 0.02926985, 0.02747912, 0.02688276,\n",
       "        0.01560402, 0.00966291, 0.02478605, 0.04110937, 0.01007999,\n",
       "        0.0376566 , 0.0423978 , 0.02430464, 0.02328493, 0.03559045,\n",
       "        0.02354588, 0.02365681, 0.02333683, 0.02807946, 0.01757595,\n",
       "        0.02482078, 0.02607598, 0.0431095 , 0.03071041, 0.02359363,\n",
       "        0.03135574, 0.0442725 , 0.05210278, 0.02177905, 0.03348654,\n",
       "        0.01606031, 0.02637409, 0.04405853, 0.03056759, 0.02718728,\n",
       "        0.03112502, 0.06041542, 0.04673072, 0.0213117 , 0.02876292,\n",
       "        0.02079112, 0.00773018, 0.02126149, 0.04353727, 0.04132791,\n",
       "        0.03284637, 0.04446295, 0.02026572, 0.03771139, 0.03129114,\n",
       "        0.03331514, 0.03179821, 0.03139505, 0.03599539, 0.0197034 ,\n",
       "        0.03678305, 0.03360933, 0.04067802, 0.03465437, 0.02639537,\n",
       "        0.03343265, 0.04518901, 0.01812622, 0.02321222, 0.03175139,\n",
       "        0.02599003, 0.0275021 , 0.04773153, 0.0428635 , 0.03149892,\n",
       "        0.03367928, 0.02599302, 0.04770219, 0.04557703, 0.02670002,\n",
       "        0.03108295, 0.02604681, 0.02198483, 0.01684212, 0.02629359,\n",
       "        0.0477041 , 0.02993018, 0.03943656, 0.02686701, 0.02916719,\n",
       "        0.02882895, 0.02175509, 0.01859876, 0.0261752 , 0.04364298,\n",
       "        0.02680032, 0.01706086, 0.03812695, 0.01469945, 0.0240972 ,\n",
       "        0.03063714, 0.04646969, 0.0254772 , 0.02236333, 0.02525447,\n",
       "        0.03094057, 0.0303649 , 0.02817149, 0.03432536, 0.02577928,\n",
       "        0.03219652, 0.03151389, 0.04348726, 0.04460476, 0.01057632,\n",
       "        0.0553242 , 0.05115619, 0.01797164, 0.04516752, 0.01519998,\n",
       "        0.03273966, 0.04312951, 0.01956331, 0.02839514, 0.0360769 ,\n",
       "        0.04597504, 0.03521639, 0.0445055 , 0.03484333, 0.01793177,\n",
       "        0.04272384, 0.03994676, 0.01571733, 0.01460643, 0.01202378,\n",
       "        0.01733351, 0.01600419, 0.0155952 , 0.01515266, 0.01607627,\n",
       "        0.01643486, 0.01574001, 0.01502511, 0.01574534, 0.01709367,\n",
       "        0.01793714, 0.021069  , 0.01331191, 0.01594841, 0.01969144,\n",
       "        0.01554847, 0.01657131, 0.01495654, 0.01552414, 0.01659643,\n",
       "        0.0163346 , 0.01831329, 0.01957611, 0.01367675, 0.01705296,\n",
       "        0.01701219, 0.01504973, 0.01488314, 0.01454466, 0.01631956,\n",
       "        0.01725355, 0.01725147, 0.01471322, 0.02048809, 0.01649908,\n",
       "        0.01769067, 0.01602026, 0.0146197 , 0.01741539, 0.01500322,\n",
       "        0.01407612, 0.01570634, 0.01665121, 0.01544056, 0.01573929,\n",
       "        0.02054655, 0.02666934, 0.02901826, 0.03543396, 0.02515318,\n",
       "        0.0153563 , 0.01292672, 0.02544984, 0.02532941, 0.02646441,\n",
       "        0.01596366, 0.02294499, 0.01636735, 0.03540631, 0.0288267 ,\n",
       "        0.02087373, 0.01046775, 0.02403309, 0.02899062, 0.03397909,\n",
       "        0.02396274, 0.02366529, 0.03396276, 0.0106305 , 0.0336938 ,\n",
       "        0.02144918, 0.02236423, 0.01460963, 0.01508949, 0.02289213,\n",
       "        0.02947805, 0.03284808, 0.02983471, 0.01359061, 0.01612784,\n",
       "        0.02062214, 0.03459445, 0.0227042 , 0.01994123, 0.01760901,\n",
       "        0.01661477, 0.01379184, 0.01517944, 0.02013478, 0.02823137,\n",
       "        0.00834174, 0.01578117, 0.02338637]),\n",
       " 'rank_test_roc_auc_ovo': array([138, 126,  65,  27,  86, 124,  30,  82,  10,  49,  38, 130, 127,\n",
       "        120, 135,  37,  78,  97,  41,  12, 129,  19, 109, 128,  35, 139,\n",
       "        133,  71, 122,  36,  33,   8,   4, 112,  43,  77,  74,  72, 115,\n",
       "         32,  20,  58,  11,   7,   9,   1, 145,  90,   3, 113, 106,  96,\n",
       "        132,  80,  99, 110, 101,  26,   5,  24,  94, 114, 144,  73,  13,\n",
       "         28, 103,  61,  44,  16,  18,  29, 108, 100, 134,  17,  93,  87,\n",
       "         21,  56,   6,  34,  14, 104,  46, 121, 131,  67,  22, 140,  42,\n",
       "         89, 116,  31,  70,   2, 270, 179, 192, 253, 248, 269, 152, 279,\n",
       "        177, 168, 153, 173, 241, 259, 166, 275, 170, 194, 268, 282, 288,\n",
       "        146, 169, 255, 162, 286, 178, 273, 264, 263, 283, 151, 155, 267,\n",
       "        193, 176, 150, 195, 280, 164, 180, 184, 190, 285, 284, 156, 163,\n",
       "        287, 204, 161, 167, 217, 186, 271, 254, 234, 265, 141, 157, 154,\n",
       "        261, 213, 165, 149, 245, 262, 172, 188, 159, 185, 183, 158, 148,\n",
       "        274, 216, 278, 191, 175, 174, 260, 171, 277, 222, 272, 276, 187,\n",
       "        242, 266, 189, 244, 181, 281, 147, 236, 160, 182, 238, 215, 247,\n",
       "        207, 232, 235, 231, 223, 225, 219, 228, 199, 258, 226, 197, 246,\n",
       "        196, 211, 202, 214, 206, 203, 220, 208, 252, 257, 251, 198, 230,\n",
       "        243, 200, 237, 209, 201, 221, 229, 256, 250, 239, 240, 249, 233,\n",
       "        224, 212, 205, 210, 218, 227,  98, 142, 117, 111,  23,  68,  95,\n",
       "         83, 123,  84,  15,  62,  76, 136, 143, 107,  79,  53, 119,  55,\n",
       "         66,  69,  47,  45, 118, 137,  48,  52,  59,  54,  64,  88,  25,\n",
       "         91,  75,  57,  85,  39, 125, 102,  81,  60,  51, 105,  50,  63,\n",
       "         40,  92]),\n",
       " 'split0_test_neg_log_loss': array([-0.27027983, -0.27212638, -0.25379062, -0.32007937, -0.32452018,\n",
       "        -0.29768317, -0.3399739 , -0.32087177, -0.32739269, -0.37290626,\n",
       "        -0.39307623, -0.40944481, -0.27071443, -0.27464321, -0.27443582,\n",
       "        -0.30621717, -0.29499594, -0.29414231, -0.34547218, -0.30366611,\n",
       "        -0.32919345, -0.37239954, -0.25555385, -0.39410167, -0.26819366,\n",
       "        -0.27116723, -0.26639349, -0.31590547, -0.27912622, -0.28239585,\n",
       "        -0.32729455, -0.31689778, -0.31178519, -0.25540946, -0.38981612,\n",
       "        -0.34580229, -0.26884766, -0.27453693, -0.25980144, -0.27426939,\n",
       "        -0.27183382, -0.27758436, -0.29239977, -0.30593275, -0.31740172,\n",
       "        -0.32350572, -0.33510729, -0.31939974, -0.26545973, -0.47561062,\n",
       "        -0.25991139, -0.30536655, -0.28838651, -0.314181  , -0.34577596,\n",
       "        -0.34034154, -0.57146638, -0.40124302, -0.41060041, -0.3992506 ,\n",
       "        -0.26146134, -0.31462432, -0.26910113, -0.28771418, -0.30622029,\n",
       "        -0.280619  , -0.47603797, -0.38463715, -0.36024554, -0.42964276,\n",
       "        -0.41054541, -0.39839219, -0.27437317, -0.25306354, -0.26856547,\n",
       "        -0.26604628, -0.29667326, -0.29622311, -0.32386956, -0.340877  ,\n",
       "        -0.31992955, -0.36460504, -0.37575194, -0.38902998, -0.25738285,\n",
       "        -0.2698598 , -0.25878447, -0.26276071, -0.29113192, -0.29156014,\n",
       "        -0.29744683, -0.29996647, -0.30461237, -0.34611113, -0.34408176,\n",
       "        -0.33872422, -0.25841292, -0.63811766, -0.255556  , -0.25472767,\n",
       "        -0.54353389, -0.25714736, -0.25335553, -0.44642567, -0.25496786,\n",
       "        -0.25446676, -0.45297903, -0.25445008, -0.25363271, -0.5338966 ,\n",
       "        -0.256329  , -0.25583444, -0.42581996, -0.25243422, -0.25447845,\n",
       "        -0.3557936 , -0.2544166 , -0.25209914, -0.45750001, -0.25407066,\n",
       "        -0.25763314, -0.6379895 , -0.25422619, -0.25309555, -0.56617561,\n",
       "        -0.25297709, -0.25277482, -0.41586187, -0.25351472, -0.25377885,\n",
       "        -0.45709202, -0.25481754, -0.25525133, -0.76133137, -0.2554438 ,\n",
       "        -0.25342365, -0.51252722, -0.25264975, -0.25501194, -0.52109929,\n",
       "        -0.25620508, -0.25450949, -0.4184146 , -0.25436881, -0.25725559,\n",
       "        -0.96129629, -0.2551479 , -0.25477445, -0.4779929 , -0.26387647,\n",
       "        -0.2560131 , -0.88976689, -0.25888633, -0.25611251, -0.58251131,\n",
       "        -0.25402915, -0.25653335, -0.66328771, -0.25657067, -0.25630709,\n",
       "        -0.49097554, -0.26215884, -0.25663065, -1.06703205, -0.2556397 ,\n",
       "        -0.25988958, -0.59199351, -0.25568945, -0.2574562 , -0.53872772,\n",
       "        -0.2569107 , -0.26204418, -0.82546539, -0.25867945, -0.25725849,\n",
       "        -0.67631919, -0.25502834, -0.25685387, -0.81770269, -0.25880665,\n",
       "        -0.25965844, -0.95570837, -0.25953377, -0.2550177 , -0.62351107,\n",
       "        -0.25292702, -0.25475098, -0.78571301, -0.25828652, -0.25764356,\n",
       "        -0.71122291, -0.26010608, -0.25536607, -0.25516159, -0.25514066,\n",
       "        -0.25510459, -0.25540356, -0.25534958, -0.25544459, -0.25544042,\n",
       "        -0.25526391, -0.25536148, -0.25535106, -0.25520837, -0.25519666,\n",
       "        -0.25527716, -0.25525443, -0.25523274, -0.25510563, -0.25522016,\n",
       "        -0.2551054 , -0.25520065, -0.25536765, -0.25523676, -0.25527115,\n",
       "        -0.25527029, -0.25529292, -0.25517124, -0.25530679, -0.2548843 ,\n",
       "        -0.2553536 , -0.25520455, -0.25528731, -0.25527613, -0.25522067,\n",
       "        -0.25536762, -0.25529679, -0.25530326, -0.25528991, -0.25537095,\n",
       "        -0.25520654, -0.2550689 , -0.25520162, -0.25517385, -0.25502607,\n",
       "        -0.25503729, -0.25506141, -0.25514601, -0.25512284, -0.25508805,\n",
       "        -0.25092694, -0.25187385, -0.25689726, -0.2488804 , -0.24837619,\n",
       "        -0.24930241, -0.25822376, -0.25664079, -0.26212664, -0.25924699,\n",
       "        -0.25599541, -0.25290695, -0.25032631, -0.25449471, -0.25331908,\n",
       "        -0.25065251, -0.25165811, -0.25024263, -0.25980281, -0.26117442,\n",
       "        -0.25377426, -0.26137777, -0.25767086, -0.25684422, -0.24969102,\n",
       "        -0.25479315, -0.24824577, -0.25087229, -0.25119072, -0.25042362,\n",
       "        -0.2516282 , -0.25260302, -0.25091042, -0.25480664, -0.25645216,\n",
       "        -0.25889908, -0.24936337, -0.24860786, -0.25126943, -0.25138742,\n",
       "        -0.25105112, -0.2501553 , -0.24923736, -0.25154851, -0.24914653,\n",
       "        -0.25371245, -0.25282852, -0.25455786]),\n",
       " 'split1_test_neg_log_loss': array([-0.29400679, -0.27767934, -0.26860729, -0.2759675 , -0.29239407,\n",
       "        -0.28096834, -0.33954926, -0.42238972, -0.31159916, -0.44102186,\n",
       "        -0.40117968, -0.25608215, -0.26281586, -0.27102173, -0.27355046,\n",
       "        -0.3227677 , -0.29531936, -0.28758868, -0.40785689, -0.33129715,\n",
       "        -0.37512551, -0.36818374, -0.25630317, -0.40587314, -0.26063777,\n",
       "        -0.27260459, -0.26636035, -0.28166238, -0.29296391, -0.28949436,\n",
       "        -0.2800914 , -0.30515487, -0.3146458 , -0.34457764, -0.36410461,\n",
       "        -0.32924964, -0.2610694 , -0.26339009, -0.27200011, -0.2776771 ,\n",
       "        -0.27185883, -0.26857793, -0.28133585, -0.31757927, -0.29854403,\n",
       "        -0.32737518, -0.25606091, -0.33066617, -0.25871692, -0.27103805,\n",
       "        -0.25284415, -0.3135569 , -0.29289904, -0.2944669 , -0.58095278,\n",
       "        -0.40194115, -0.34782923, -0.38732264, -0.59109121, -0.43804412,\n",
       "        -0.26154773, -0.266697  , -0.26763537, -0.26682695, -0.28018956,\n",
       "        -0.31877301, -0.31568263, -0.3293583 , -0.37702609, -0.4033265 ,\n",
       "        -0.42942777, -0.37974761, -0.27117526, -0.25773507, -0.26935219,\n",
       "        -0.27528011, -0.29730176, -0.29383648, -0.31337291, -0.33152064,\n",
       "        -0.35003105, -0.36505475, -0.34273269, -0.40465988, -0.2568033 ,\n",
       "        -0.25490343, -0.26094241, -0.28442297, -0.27135573, -0.2956071 ,\n",
       "        -0.30635026, -0.33377475, -0.31479005, -0.33905128, -0.3528462 ,\n",
       "        -0.353358  , -0.25551352, -0.59797753, -0.25506102, -0.25569306,\n",
       "        -0.44955914, -0.25165368, -0.25544745, -0.5580225 , -0.25405218,\n",
       "        -0.25288763, -0.56707907, -0.25345923, -0.25411674, -0.57830013,\n",
       "        -0.25366583, -0.25350462, -0.54293473, -0.25748883, -0.2554732 ,\n",
       "        -0.52918439, -0.25530138, -0.25290919, -0.53388328, -0.25338159,\n",
       "        -0.25403535, -0.48691184, -0.25354009, -0.25568674, -0.38693097,\n",
       "        -0.25396277, -0.25590224, -0.62514276, -0.2545121 , -0.25442757,\n",
       "        -0.45947788, -0.25508289, -0.25560868, -0.41369796, -0.25461859,\n",
       "        -0.25386828, -0.55852153, -0.2526171 , -0.254445  , -0.62616818,\n",
       "        -0.25352869, -0.25313122, -0.51986964, -0.25552873, -0.25431873,\n",
       "        -0.4393525 , -0.26025869, -0.25701673, -0.58131332, -0.25680199,\n",
       "        -0.259222  , -0.75275497, -0.25725497, -0.2536653 , -0.46773291,\n",
       "        -0.2556341 , -0.25952484, -0.92968524, -0.25565605, -0.25818467,\n",
       "        -0.69656748, -0.25653087, -0.25284067, -0.69683823, -0.25380632,\n",
       "        -0.25671294, -0.61292657, -0.25403007, -0.25205089, -0.94305625,\n",
       "        -0.25855021, -0.25469759, -0.71644678, -0.2589066 , -0.25451322,\n",
       "        -0.49802178, -0.25636538, -0.25745011, -0.64004156, -0.25789882,\n",
       "        -0.25488326, -0.44450677, -0.25674989, -0.25894752, -1.01138732,\n",
       "        -0.25823715, -0.25487621, -0.57936768, -0.25456889, -0.25591409,\n",
       "        -0.4526047 , -0.25838441, -0.25543576, -0.25468455, -0.2555487 ,\n",
       "        -0.25569561, -0.25569749, -0.25554827, -0.2555437 , -0.25534454,\n",
       "        -0.25588517, -0.25573858, -0.25576254, -0.25573847, -0.25571211,\n",
       "        -0.25579237, -0.25572479, -0.25583375, -0.25562764, -0.25561651,\n",
       "        -0.25566264, -0.25550923, -0.25544665, -0.25555973, -0.25590934,\n",
       "        -0.25563492, -0.25596567, -0.25589547, -0.25562955, -0.25573285,\n",
       "        -0.25572692, -0.25586489, -0.25542163, -0.25565028, -0.25575884,\n",
       "        -0.25574043, -0.25581829, -0.25574195, -0.25555035, -0.25548157,\n",
       "        -0.25560899, -0.25555076, -0.25573888, -0.25556809, -0.25547689,\n",
       "        -0.25549922, -0.25545884, -0.25558997, -0.25529815, -0.25547835,\n",
       "        -0.25190476, -0.2560842 , -0.25124818, -0.25041822, -0.24852416,\n",
       "        -0.25450553, -0.25265125, -0.24881144, -0.24934818, -0.25053019,\n",
       "        -0.24829859, -0.25079074, -0.25306183, -0.25015941, -0.2550057 ,\n",
       "        -0.25747904, -0.25720833, -0.24781559, -0.25618453, -0.2498996 ,\n",
       "        -0.25028991, -0.25526655, -0.24436752, -0.25218399, -0.25457514,\n",
       "        -0.25199647, -0.25032434, -0.25029166, -0.24972936, -0.25124798,\n",
       "        -0.24922549, -0.25588275, -0.24885774, -0.25626878, -0.25256116,\n",
       "        -0.25342857, -0.2527826 , -0.25027023, -0.24970014, -0.25064094,\n",
       "        -0.2518951 , -0.24957739, -0.250427  , -0.2499722 , -0.25053911,\n",
       "        -0.25008916, -0.25167816, -0.250367  ]),\n",
       " 'split2_test_neg_log_loss': array([-0.27343701, -0.27010592, -0.26616664, -0.27314295, -0.29618995,\n",
       "        -0.30935805, -0.32486661, -0.35256966, -0.33164634, -0.39472114,\n",
       "        -0.37782705, -0.25565427, -0.2660613 , -0.26491363, -0.27188722,\n",
       "        -0.29119994, -0.32033211, -0.28650341, -0.31844838, -0.33554531,\n",
       "        -0.33174389, -0.42976896, -0.3680052 , -0.36850804, -0.25337392,\n",
       "        -0.27010735, -0.269932  , -0.2745566 , -0.28646229, -0.2752755 ,\n",
       "        -0.30912507, -0.29615906, -0.31203517, -0.25553936, -0.34021175,\n",
       "        -0.3783274 , -0.26268805, -0.2636303 , -0.2524354 , -0.26380251,\n",
       "        -0.28928011, -0.27671432, -0.2935598 , -0.27184713, -0.27817655,\n",
       "        -0.31040492, -0.30813932, -0.25531087, -0.25892168, -0.28089197,\n",
       "        -0.27995206, -0.42758077, -0.3275199 , -0.32100134, -0.32393888,\n",
       "        -0.54348344, -0.3980864 , -0.42017984, -0.41730688, -0.41802388,\n",
       "        -0.28384663, -0.26332566, -0.28432097, -0.29687062, -0.28774636,\n",
       "        -0.28914089, -0.36884426, -0.33825751, -0.34217498, -0.41079638,\n",
       "        -0.3692604 , -0.40164898, -0.26420771, -0.2785042 , -0.26848056,\n",
       "        -0.30424547, -0.29581407, -0.28176243, -0.33807574, -0.3252628 ,\n",
       "        -0.28548296, -0.35329573, -0.3628146 , -0.39310344, -0.26335311,\n",
       "        -0.27019588, -0.26196912, -0.29614234, -0.28399055, -0.29092382,\n",
       "        -0.31775389, -0.32759288, -0.34667932, -0.35916787, -0.33230808,\n",
       "        -0.33118273, -0.255175  , -0.70756386, -0.2571831 , -0.25611784,\n",
       "        -0.5001979 , -0.25765447, -0.25272767, -0.54048545, -0.25385513,\n",
       "        -0.25425915, -0.48402538, -0.25338567, -0.25576682, -0.68474999,\n",
       "        -0.2534687 , -0.25808686, -0.48027244, -0.25473123, -0.25505383,\n",
       "        -0.54275051, -0.25579449, -0.25474365, -0.46793639, -0.25418515,\n",
       "        -0.25337427, -0.55885804, -0.25327638, -0.2596845 , -0.42677579,\n",
       "        -0.25634047, -0.25614884, -0.46997938, -0.25336139, -0.25501727,\n",
       "        -0.72726221, -0.25377605, -0.25197779, -0.83871877, -0.25551408,\n",
       "        -0.25301231, -0.63463301, -0.25561081, -0.25258183, -0.54470835,\n",
       "        -0.25511601, -0.2528651 , -0.39213642, -0.25528811, -0.25680707,\n",
       "        -0.36663805, -0.25582832, -0.26069788, -0.84457983, -0.25729657,\n",
       "        -0.25654771, -0.57705421, -0.25448046, -0.25404652, -0.63199887,\n",
       "        -0.25891833, -0.25744079, -0.79933742, -0.25806803, -0.25384983,\n",
       "        -0.50599383, -0.25579332, -0.25786118, -0.58275572, -0.25473015,\n",
       "        -0.25509502, -0.57635972, -0.25622532, -0.25694175, -0.5588468 ,\n",
       "        -0.26011619, -0.26091446, -0.9291261 , -0.25534579, -0.25685942,\n",
       "        -0.58431864, -0.26141892, -0.25787904, -0.68378212, -0.25756613,\n",
       "        -0.25819267, -0.85029734, -0.25702221, -0.25896543, -0.63955567,\n",
       "        -0.26137412, -0.26097547, -0.62923359, -0.25490053, -0.25526349,\n",
       "        -0.7380825 , -0.25374911, -0.25492972, -0.25532345, -0.25516191,\n",
       "        -0.25515854, -0.25517643, -0.25530521, -0.25497553, -0.25509615,\n",
       "        -0.25503635, -0.25499673, -0.25491392, -0.25513994, -0.25548916,\n",
       "        -0.25538475, -0.25503402, -0.25511872, -0.25509882, -0.25503664,\n",
       "        -0.25508428, -0.25526247, -0.25498403, -0.25508722, -0.25502348,\n",
       "        -0.25510818, -0.25507553, -0.25520031, -0.25495599, -0.25502077,\n",
       "        -0.25510409, -0.25522443, -0.25490885, -0.2550662 , -0.25502963,\n",
       "        -0.25493952, -0.25506938, -0.25501203, -0.2552594 , -0.25498514,\n",
       "        -0.25486798, -0.25496909, -0.25473541, -0.25500918, -0.25483649,\n",
       "        -0.254965  , -0.25487925, -0.2547155 , -0.25485925, -0.2548847 ,\n",
       "        -0.25603497, -0.25418568, -0.25372394, -0.26326225, -0.25732325,\n",
       "        -0.25463247, -0.25963857, -0.25744233, -0.258141  , -0.26473173,\n",
       "        -0.25520345, -0.26020773, -0.25348865, -0.25588039, -0.25663417,\n",
       "        -0.25718368, -0.25641743, -0.25601437, -0.2628908 , -0.25203537,\n",
       "        -0.26338935, -0.26034742, -0.26007737, -0.25772562, -0.25469519,\n",
       "        -0.25463277, -0.25369571, -0.25443002, -0.25478039, -0.25640131,\n",
       "        -0.26152688, -0.260215  , -0.25920716, -0.26072524, -0.26059087,\n",
       "        -0.25979922, -0.25515201, -0.25406177, -0.25414683, -0.25590945,\n",
       "        -0.25494458, -0.25385625, -0.25296271, -0.25503433, -0.253773  ,\n",
       "        -0.25301301, -0.25383387, -0.25463589]),\n",
       " 'split3_test_neg_log_loss': array([-0.28308541, -0.26531603, -0.2651146 , -0.28345651, -0.29013001,\n",
       "        -0.30241766, -0.33379604, -0.31469804, -0.32661437, -0.37349527,\n",
       "        -0.40330937, -0.38976856, -0.26777558, -0.2679862 , -0.25936263,\n",
       "        -0.26848159, -0.27787169, -0.2836731 , -0.32518005, -0.33105473,\n",
       "        -0.3386261 , -0.34801853, -0.3577853 , -0.39439523, -0.25818202,\n",
       "        -0.26233069, -0.25667178, -0.2822665 , -0.2756507 , -0.26944576,\n",
       "        -0.30701187, -0.30384862, -0.29094413, -0.34295641, -0.37167487,\n",
       "        -0.36456583, -0.2515021 , -0.24892355, -0.26895341, -0.28865809,\n",
       "        -0.27047362, -0.27546575, -0.28428803, -0.27122585, -0.26794894,\n",
       "        -0.30745688, -0.25415972, -0.31785662, -0.26756953, -0.26768148,\n",
       "        -0.25982158, -0.2929441 , -0.30751546, -0.29290668, -0.56264957,\n",
       "        -0.32527156, -0.36252628, -0.40729839, -0.40290611, -0.37324828,\n",
       "        -0.2678698 , -0.27480087, -0.280023  , -0.30230442, -0.29362343,\n",
       "        -0.30161102, -0.32018516, -0.34035825, -0.34168915, -0.38624385,\n",
       "        -0.38180773, -0.41536677, -0.26165972, -0.25412511, -0.26995703,\n",
       "        -0.28836133, -0.26985357, -0.28049185, -0.32726998, -0.30694712,\n",
       "        -0.32198342, -0.36967203, -0.41352091, -0.41776176, -0.26045481,\n",
       "        -0.25562181, -0.27388336, -0.27675799, -0.27084252, -0.29222492,\n",
       "        -0.30838553, -0.29841932, -0.31708753, -0.34934062, -0.39300794,\n",
       "        -0.31132805, -0.25605604, -0.53239293, -0.25364508, -0.25417653,\n",
       "        -0.60014133, -0.25449313, -0.25391265, -0.47348696, -0.25320631,\n",
       "        -0.25433445, -0.50617042, -0.25513924, -0.25674051, -0.83495578,\n",
       "        -0.25530833, -0.25310466, -0.57212661, -0.25434483, -0.25401539,\n",
       "        -0.54651647, -0.25376836, -0.25360927, -0.5026663 , -0.25524182,\n",
       "        -0.25333621, -0.74045386, -0.25730557, -0.25515216, -0.51488909,\n",
       "        -0.25607743, -0.25598367, -0.62394736, -0.2555408 , -0.25537612,\n",
       "        -0.44028692, -0.25335174, -0.25384758, -0.41455793, -0.25960291,\n",
       "        -0.25623895, -0.44792717, -0.25530487, -0.25429776, -0.46083034,\n",
       "        -0.25521653, -0.25355825, -0.49554781, -0.25472236, -0.26104006,\n",
       "        -0.4963723 , -0.25839009, -0.25383275, -0.52828664, -0.26019416,\n",
       "        -0.25829532, -0.72065449, -0.25801746, -0.25265613, -0.75219859,\n",
       "        -0.25531318, -0.25791682, -1.20379599, -0.25780154, -0.2557863 ,\n",
       "        -0.80166839, -0.25349422, -0.25455648, -0.624123  , -0.25858742,\n",
       "        -0.25894853, -0.78845883, -0.25681151, -0.25236755, -0.98140032,\n",
       "        -0.25503163, -0.25780529, -0.54822257, -0.25887705, -0.25664031,\n",
       "        -0.70041044, -0.25211322, -0.2569271 , -0.53567648, -0.25523656,\n",
       "        -0.25816323, -0.42985126, -0.26012956, -0.25802867, -0.55309409,\n",
       "        -0.25575999, -0.25709246, -0.76663791, -0.25377015, -0.25692988,\n",
       "        -0.85014748, -0.25585109, -0.25384682, -0.25408619, -0.25415245,\n",
       "        -0.25383384, -0.25406612, -0.25429177, -0.25393406, -0.25407806,\n",
       "        -0.25408652, -0.25412862, -0.25409956, -0.25402417, -0.25410646,\n",
       "        -0.2539205 , -0.25387454, -0.25416577, -0.25389113, -0.25404112,\n",
       "        -0.25405608, -0.25395097, -0.25406218, -0.25411597, -0.25403898,\n",
       "        -0.2540316 , -0.25390659, -0.25391115, -0.25393615, -0.25403592,\n",
       "        -0.25381473, -0.25403898, -0.25397158, -0.25410921, -0.2538627 ,\n",
       "        -0.25393827, -0.25408236, -0.25414327, -0.25383531, -0.25395783,\n",
       "        -0.25394198, -0.25393187, -0.25400416, -0.25389757, -0.25400392,\n",
       "        -0.25400909, -0.25407446, -0.25389278, -0.2539421 , -0.25395325,\n",
       "        -0.2498762 , -0.24928186, -0.24802651, -0.2501739 , -0.24931675,\n",
       "        -0.25041735, -0.25613729, -0.25547231, -0.25137337, -0.25411356,\n",
       "        -0.25520919, -0.25157597, -0.25132707, -0.24995474, -0.25016518,\n",
       "        -0.25173001, -0.24849813, -0.25169966, -0.25596154, -0.25544032,\n",
       "        -0.25186278, -0.25221968, -0.2565975 , -0.25305595, -0.25031047,\n",
       "        -0.25197457, -0.25170457, -0.24998349, -0.24992424, -0.25072946,\n",
       "        -0.25367136, -0.25739151, -0.25155234, -0.25334738, -0.25646371,\n",
       "        -0.25399199, -0.25279839, -0.24893054, -0.25283289, -0.24895572,\n",
       "        -0.25029586, -0.25049023, -0.251132  , -0.25612075, -0.25409408,\n",
       "        -0.25144574, -0.25065987, -0.25388333]),\n",
       " 'split4_test_neg_log_loss': array([-0.26181604, -0.27546331, -0.27185864, -0.30383439, -0.30203487,\n",
       "        -0.3103262 , -0.33775691, -0.33788002, -0.33883241, -0.38947756,\n",
       "        -0.36427097, -0.33805404, -0.26197895, -0.27063103, -0.31594865,\n",
       "        -0.28488248, -0.29549221, -0.29368999, -0.30552727, -0.30279894,\n",
       "        -0.31640173, -0.37025645, -0.34608819, -0.36653482, -0.25925238,\n",
       "        -0.26368766, -0.27580668, -0.28195128, -0.28917591, -0.27882627,\n",
       "        -0.33225094, -0.30631128, -0.29274675, -0.34335646, -0.32493127,\n",
       "        -0.36788615, -0.26441788, -0.25564108, -0.25409781, -0.28689158,\n",
       "        -0.26288988, -0.27939663, -0.30761738, -0.29567995, -0.29698644,\n",
       "        -0.28916587, -0.25657586, -0.32816821, -0.25688965, -0.26765435,\n",
       "        -0.35559707, -0.2857894 , -0.31126346, -0.28012043, -0.35425308,\n",
       "        -0.45847127, -0.34920569, -0.41218626, -0.42747577, -0.42589288,\n",
       "        -0.26761218, -0.25682975, -0.26646987, -0.31202034, -0.28317581,\n",
       "        -0.29315697, -0.332564  , -0.33640902, -0.30944532, -0.3707894 ,\n",
       "        -0.41003209, -0.46038422, -0.25992655, -0.29176522, -0.26470622,\n",
       "        -0.27626894, -0.28276364, -0.30307889, -0.28881461, -0.32158327,\n",
       "        -0.30136767, -0.38454054, -0.38378261, -0.42231673, -0.25702523,\n",
       "        -0.26255579, -0.26794063, -0.28893324, -0.26505793, -0.31135607,\n",
       "        -0.30552641, -0.29832688, -0.30952335, -0.37255709, -0.3531222 ,\n",
       "        -0.36308295, -0.25802854, -0.83317998, -0.25664464, -0.25752087,\n",
       "        -0.4943409 , -0.25828992, -0.25386706, -0.39358057, -0.25593531,\n",
       "        -0.25641721, -0.59290107, -0.25586737, -0.25930632, -0.36776427,\n",
       "        -0.25849218, -0.25929788, -0.49972497, -0.25759201, -0.25646217,\n",
       "        -0.51281528, -0.25898031, -0.25472245, -0.47259073, -0.25760726,\n",
       "        -0.25751035, -0.90869074, -0.2580736 , -0.25858554, -0.61368532,\n",
       "        -0.25864392, -0.25698059, -0.36851811, -0.25546678, -0.25626821,\n",
       "        -0.55809095, -0.2557253 , -0.25515789, -0.68905668, -0.25731121,\n",
       "        -0.25639707, -0.55716103, -0.25875676, -0.2554088 , -0.40550433,\n",
       "        -0.25995228, -0.25702018, -0.4050654 , -0.25704458, -0.25926781,\n",
       "        -0.49791016, -0.25805071, -0.25881243, -0.63926161, -0.25916198,\n",
       "        -0.2575107 , -0.7793473 , -0.25916559, -0.25194343, -0.66885987,\n",
       "        -0.2562937 , -0.25900993, -0.63191194, -0.25912218, -0.25491086,\n",
       "        -0.91253347, -0.26697511, -0.25665418, -0.48059153, -0.2571598 ,\n",
       "        -0.25557025, -0.79261935, -0.25509674, -0.26174245, -0.6501908 ,\n",
       "        -0.26014533, -0.26281642, -0.75640329, -0.25460571, -0.25897165,\n",
       "        -0.46189186, -0.25565299, -0.25839443, -0.63784528, -0.25889275,\n",
       "        -0.26661374, -0.61804642, -0.25880237, -0.25907963, -0.77351558,\n",
       "        -0.25810059, -0.25765619, -0.56122022, -0.25315837, -0.25966005,\n",
       "        -0.79663121, -0.25559583, -0.2567182 , -0.25652675, -0.25689907,\n",
       "        -0.25629201, -0.2564945 , -0.25625352, -0.25652951, -0.25628286,\n",
       "        -0.25633844, -0.2563116 , -0.25653589, -0.25619265, -0.25649327,\n",
       "        -0.25652679, -0.25593349, -0.2565907 , -0.25628367, -0.25614044,\n",
       "        -0.25640157, -0.2563366 , -0.25641809, -0.25632891, -0.25635446,\n",
       "        -0.25630209, -0.25658058, -0.25657085, -0.25678008, -0.25631694,\n",
       "        -0.25641822, -0.25668822, -0.25637403, -0.25650671, -0.25638373,\n",
       "        -0.25628045, -0.25625538, -0.25642462, -0.25641038, -0.2566758 ,\n",
       "        -0.25657553, -0.25640558, -0.25653696, -0.25624497, -0.25631519,\n",
       "        -0.25631724, -0.25623899, -0.25633   , -0.25635195, -0.25635845,\n",
       "        -0.25208485, -0.25576013, -0.25203475, -0.25585058, -0.25499001,\n",
       "        -0.25359195, -0.25499041, -0.25168121, -0.25358757, -0.25573487,\n",
       "        -0.25330266, -0.25889046, -0.25101491, -0.25201458, -0.25375086,\n",
       "        -0.25238882, -0.25325536, -0.25404462, -0.24951791, -0.25206785,\n",
       "        -0.25776755, -0.25118814, -0.26203949, -0.25917409, -0.25178013,\n",
       "        -0.25326583, -0.25204389, -0.25252059, -0.25108127, -0.25043302,\n",
       "        -0.25442986, -0.25122231, -0.24964091, -0.25783219, -0.25399681,\n",
       "        -0.25432252, -0.2501266 , -0.25064452, -0.25500455, -0.25398289,\n",
       "        -0.25104861, -0.25237355, -0.25277494, -0.25280305, -0.2507503 ,\n",
       "        -0.25361493, -0.25085225, -0.2519888 ]),\n",
       " 'mean_test_neg_log_loss': array([-0.27652502, -0.2721382 , -0.26510756, -0.29129614, -0.30105381,\n",
       "        -0.30015068, -0.33518855, -0.34968184, -0.327217  , -0.39432442,\n",
       "        -0.38793266, -0.32980077, -0.26586922, -0.26983916, -0.27903696,\n",
       "        -0.29470978, -0.29680226, -0.2891195 , -0.34049695, -0.32087245,\n",
       "        -0.33821814, -0.37772545, -0.31674714, -0.38588258, -0.25992795,\n",
       "        -0.2679795 , -0.26703286, -0.28726844, -0.28467581, -0.27908755,\n",
       "        -0.31115476, -0.30567432, -0.30443141, -0.30836787, -0.35814772,\n",
       "        -0.35716626, -0.26170502, -0.26122439, -0.26145764, -0.27825973,\n",
       "        -0.27326725, -0.2755478 , -0.29184017, -0.29245299, -0.29181154,\n",
       "        -0.31158171, -0.28200862, -0.31028032, -0.2615115 , -0.3125753 ,\n",
       "        -0.28162525, -0.32504754, -0.30551688, -0.30053527, -0.43351406,\n",
       "        -0.41390179, -0.4058228 , -0.40564603, -0.44987608, -0.41089195,\n",
       "        -0.26846753, -0.27525552, -0.27351007, -0.2931473 , -0.29019109,\n",
       "        -0.29666018, -0.36266281, -0.34580405, -0.34611622, -0.40015978,\n",
       "        -0.40021468, -0.41110795, -0.26626848, -0.26703863, -0.26821229,\n",
       "        -0.28204043, -0.28848126, -0.29107855, -0.31828056, -0.32523817,\n",
       "        -0.31575893, -0.36743362, -0.37572055, -0.40537436, -0.25900386,\n",
       "        -0.26262734, -0.264704  , -0.28180345, -0.27647573, -0.29633441,\n",
       "        -0.30709259, -0.31161606, -0.31853852, -0.3532456 , -0.35507324,\n",
       "        -0.33953519, -0.25663721, -0.66184639, -0.25561797, -0.25564719,\n",
       "        -0.51755463, -0.25584771, -0.25386207, -0.48240023, -0.25440336,\n",
       "        -0.25447304, -0.52063099, -0.25446032, -0.25591262, -0.59993335,\n",
       "        -0.25545281, -0.25596569, -0.50417574, -0.25531822, -0.25509661,\n",
       "        -0.49741205, -0.25565223, -0.25361674, -0.48691534, -0.2548973 ,\n",
       "        -0.25517786, -0.6665808 , -0.25528437, -0.2564409 , -0.50169135,\n",
       "        -0.25560033, -0.25555803, -0.5006899 , -0.25447916, -0.2549736 ,\n",
       "        -0.52844199, -0.2545507 , -0.25436866, -0.62347254, -0.25649812,\n",
       "        -0.25458805, -0.54215399, -0.25498786, -0.25434907, -0.5116621 ,\n",
       "        -0.25600372, -0.25421685, -0.44620677, -0.25539052, -0.25773785,\n",
       "        -0.55231386, -0.25753514, -0.25702685, -0.61428686, -0.25946623,\n",
       "        -0.25751776, -0.74391557, -0.25756096, -0.25368478, -0.62066031,\n",
       "        -0.25603769, -0.25808515, -0.84560366, -0.25744369, -0.25580775,\n",
       "        -0.68154774, -0.25899047, -0.25570863, -0.69026811, -0.25598468,\n",
       "        -0.25724326, -0.6724716 , -0.25557062, -0.25611177, -0.73444438,\n",
       "        -0.25815081, -0.25965559, -0.75513282, -0.25728292, -0.25684862,\n",
       "        -0.58419238, -0.25611577, -0.25750091, -0.66300962, -0.25768018,\n",
       "        -0.25950227, -0.65968203, -0.25844756, -0.25800779, -0.72021275,\n",
       "        -0.25727977, -0.25707026, -0.66443448, -0.25493689, -0.25708222,\n",
       "        -0.70973776, -0.2567373 , -0.25525932, -0.25515651, -0.25538056,\n",
       "        -0.25521692, -0.25536762, -0.25534967, -0.25528548, -0.25524841,\n",
       "        -0.25532208, -0.2553074 , -0.25533259, -0.25526072, -0.25539953,\n",
       "        -0.25538031, -0.25516425, -0.25538834, -0.25520138, -0.25521097,\n",
       "        -0.25526199, -0.25525198, -0.25525572, -0.25526572, -0.25531948,\n",
       "        -0.25526941, -0.25536426, -0.2553498 , -0.25532171, -0.25519816,\n",
       "        -0.25528351, -0.25540421, -0.25519268, -0.25532171, -0.25525111,\n",
       "        -0.25525326, -0.25530444, -0.25532503, -0.25526907, -0.25529426,\n",
       "        -0.2552402 , -0.25518524, -0.2552434 , -0.25517873, -0.25513171,\n",
       "        -0.25516557, -0.25514259, -0.25513485, -0.25511486, -0.25515256,\n",
       "        -0.25216554, -0.25343715, -0.25238613, -0.25371707, -0.25170607,\n",
       "        -0.25248994, -0.25632826, -0.25400962, -0.25491535, -0.25687147,\n",
       "        -0.25360186, -0.25487437, -0.25184375, -0.25250077, -0.253775  ,\n",
       "        -0.25388681, -0.25340747, -0.25196337, -0.25687152, -0.25412351,\n",
       "        -0.25541677, -0.25607991, -0.25615055, -0.25579678, -0.25221039,\n",
       "        -0.25333256, -0.25120286, -0.25161961, -0.25134119, -0.25184708,\n",
       "        -0.25409636, -0.25546292, -0.25203371, -0.25659605, -0.25601294,\n",
       "        -0.25608828, -0.25204459, -0.25050299, -0.25259077, -0.25217528,\n",
       "        -0.25184705, -0.25129054, -0.2513068 , -0.25309577, -0.25166061,\n",
       "        -0.25237506, -0.25197053, -0.25308658]),\n",
       " 'std_test_neg_log_loss': array([0.01107739, 0.0042996 , 0.00611588, 0.01795354, 0.01240654,\n",
       "        0.0106537 , 0.00560345, 0.03869869, 0.00893138, 0.02488731,\n",
       "        0.01483497, 0.06471409, 0.00321261, 0.00324958, 0.01924548,\n",
       "        0.01852825, 0.01355875, 0.00412234, 0.03607143, 0.01449383,\n",
       "        0.01980726, 0.02744531, 0.05014082, 0.01559392, 0.00480179,\n",
       "        0.00415715, 0.00622251, 0.01460404, 0.00639352, 0.00673514,\n",
       "        0.01839433, 0.00664101, 0.01034086, 0.04319065, 0.02300603,\n",
       "        0.01746964, 0.00572445, 0.00860449, 0.00781845, 0.00903226,\n",
       "        0.00867115, 0.00371235, 0.00916042, 0.01843171, 0.01721801,\n",
       "        0.01350914, 0.03346028, 0.02792047, 0.0041992 , 0.08166154,\n",
       "        0.03807853, 0.05216181, 0.01395571, 0.01494558, 0.1134911 ,\n",
       "        0.08023382, 0.08478324, 0.01106228, 0.07106757, 0.02265259,\n",
       "        0.00818017, 0.0205192 , 0.00724995, 0.01533745, 0.00920801,\n",
       "        0.01295855, 0.05968176, 0.01976535, 0.02251642, 0.02024196,\n",
       "        0.02169559, 0.02713272, 0.0055762 , 0.01543685, 0.00183482,\n",
       "        0.01317454, 0.01075511, 0.00868273, 0.01671061, 0.01124246,\n",
       "        0.02170004, 0.01011135, 0.02344623, 0.01310508, 0.00254583,\n",
       "        0.00660806, 0.00550284, 0.01141387, 0.00958739, 0.00768291,\n",
       "        0.00650199, 0.01570182, 0.01471918, 0.01162441, 0.02042975,\n",
       "        0.01795345, 0.00132872, 0.10280639, 0.00124177, 0.00116126,\n",
       "        0.05090973, 0.00246344, 0.00090105, 0.06058906, 0.00095103,\n",
       "        0.00112768, 0.05197356, 0.00095895, 0.00203209, 0.15566678,\n",
       "        0.00185237, 0.00244387, 0.05069593, 0.0019744 , 0.00084375,\n",
       "        0.07179046, 0.00180528, 0.0010292 , 0.02787377, 0.00147989,\n",
       "        0.00197073, 0.14749715, 0.00200297, 0.0023892 , 0.08444801,\n",
       "        0.00197999, 0.00144351, 0.106103  , 0.00092554, 0.00084446,\n",
       "        0.10772811, 0.00086853, 0.00133622, 0.17736289, 0.00178401,\n",
       "        0.00143909, 0.06132196, 0.0022707 , 0.00096927, 0.07502812,\n",
       "        0.00215245, 0.0015089 , 0.05147694, 0.00092259, 0.00228187,\n",
       "        0.21004363, 0.00184535, 0.00252747, 0.12705741, 0.002525  ,\n",
       "        0.0011588 , 0.10098509, 0.00168065, 0.00142236, 0.09444174,\n",
       "        0.00161775, 0.00107513, 0.2080458 , 0.00120773, 0.00145172,\n",
       "        0.16440554, 0.00490455, 0.00178495, 0.20094407, 0.00170841,\n",
       "        0.00187649, 0.09710648, 0.00095712, 0.00359752, 0.19012856,\n",
       "        0.00196304, 0.00300953, 0.12611932, 0.00189989, 0.00142638,\n",
       "        0.09420912, 0.00302002, 0.0005817 , 0.09136295, 0.00132449,\n",
       "        0.00388435, 0.21204806, 0.00134526, 0.00154208, 0.16211678,\n",
       "        0.00281461, 0.00227101, 0.09410916, 0.0017822 , 0.00152681,\n",
       "        0.13725043, 0.00223961, 0.00092462, 0.00080917, 0.00088857,\n",
       "        0.00081412, 0.00078872, 0.00062889, 0.00084404, 0.00070839,\n",
       "        0.00076963, 0.00073257, 0.00081594, 0.00072706, 0.00077656,\n",
       "        0.00085158, 0.00072033, 0.00080444, 0.00078606, 0.00069676,\n",
       "        0.00077065, 0.00076654, 0.00076137, 0.00071714, 0.00079357,\n",
       "        0.00074272, 0.00090049, 0.00088461, 0.0009247 , 0.00077696,\n",
       "        0.00085774, 0.00087143, 0.00077828, 0.00078099, 0.00083884,\n",
       "        0.0007916 , 0.00073717, 0.00075867, 0.0008292 , 0.00087535,\n",
       "        0.00086536, 0.00080672, 0.00086189, 0.00076924, 0.00076026,\n",
       "        0.00075257, 0.00070978, 0.00081905, 0.00077521, 0.00078416,\n",
       "        0.00208913, 0.00255632, 0.00291769, 0.00533839, 0.00372179,\n",
       "        0.0022056 , 0.00244532, 0.00326462, 0.00464132, 0.00482776,\n",
       "        0.00279614, 0.00389871, 0.00122032, 0.00234939, 0.00214143,\n",
       "        0.00286809, 0.00318407, 0.00286276, 0.0044766 , 0.00394656,\n",
       "        0.00470535, 0.00414177, 0.0061879 , 0.0027125 , 0.00209324,\n",
       "        0.00122128, 0.001827  , 0.00165584, 0.00181791, 0.00229673,\n",
       "        0.0041309 , 0.00324468, 0.00370848, 0.0025472 , 0.00273251,\n",
       "        0.00269288, 0.00208022, 0.00193924, 0.00191875, 0.00247192,\n",
       "        0.00162937, 0.0015887 , 0.00141313, 0.00224254, 0.00193861,\n",
       "        0.00140156, 0.00120516, 0.00166139]),\n",
       " 'rank_test_neg_log_loss': array([184, 178, 169, 198, 209, 207, 229, 235, 227, 246, 245, 228, 170,\n",
       "        177, 186, 203, 206, 195, 232, 224, 230, 243, 221, 244, 162, 174,\n",
       "        172, 193, 192, 187, 216, 212, 210, 214, 239, 238, 166, 163, 164,\n",
       "        185, 179, 182, 200, 201, 199, 217, 190, 215, 165, 219, 188, 225,\n",
       "        211, 208, 255, 254, 251, 250, 257, 252, 176, 181, 180, 202, 196,\n",
       "        205, 240, 233, 234, 247, 248, 253, 171, 173, 175, 191, 194, 197,\n",
       "        222, 226, 220, 241, 242, 249, 158, 167, 168, 189, 183, 204, 213,\n",
       "        218, 223, 236, 237, 231, 135, 276, 113, 114, 265, 119,  34, 258,\n",
       "         42,  44, 266,  43, 120, 271, 108, 121, 263,  90,  54, 260, 115,\n",
       "         30, 259,  49,  63, 279,  85, 132, 262, 112, 110, 261,  45,  52,\n",
       "        267,  46,  41, 274, 133,  47, 268,  53,  40, 264, 123,  39, 256,\n",
       "        104, 152, 269, 149, 140, 272, 159, 148, 286, 150,  31, 273, 125,\n",
       "        154, 288, 146, 118, 281, 157, 116, 282, 122, 143, 280, 111, 128,\n",
       "        285, 155, 161, 287, 145, 137, 270, 129, 147, 277, 151, 160, 275,\n",
       "        156, 153, 284, 144, 141, 278,  51, 142, 283, 136,  78,  60, 102,\n",
       "         70, 100,  97,  86,  73,  94,  89,  96,  79, 105, 101,  61, 103,\n",
       "         68,  69,  80,  75,  77,  81,  91,  83,  99,  98,  93,  67,  84,\n",
       "        106,  66,  92,  74,  76,  88,  95,  82,  87,  71,  65,  72,  64,\n",
       "         56,  62,  58,  57,  55,  59,  16,  28,  20,  32,   8,  21, 131,\n",
       "         36,  50, 138,  29,  48,   9,  22,  33,  35,  27,  12, 139,  38,\n",
       "        107, 126, 130, 117,  18,  26,   2,   6,   5,  11,  37, 109,  14,\n",
       "        134, 124, 127,  15,   1,  23,  17,  10,   3,   4,  25,   7,  19,\n",
       "         13,  24])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRIAL_4_MLP.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FOUR NEG LOG LOSS RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'tanh',\n",
       " 'classifier__alpha': 0.1,\n",
       " 'classifier__hidden_layer_sizes': (5,),\n",
       " 'classifier__learning_rate': 'invscaling',\n",
       " 'classifier__solver': 'adam'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL FOUR NEG LOG LOSS RESULTS ---------------------------------------------\")\n",
    "TRIAL_4_MLP.cv_results_['rank_test_neg_log_loss']\n",
    "TRIAL_4_MLP.cv_results_['params'][ np.argmin(TRIAL_4_MLP.cv_results_['rank_test_neg_log_loss']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FOUR F1 RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'tanh',\n",
       " 'classifier__alpha': 0.1,\n",
       " 'classifier__hidden_layer_sizes': (5,),\n",
       " 'classifier__learning_rate': 'adaptive',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL FOUR F1 RESULTS ---------------------------------------------\")\n",
    "TRIAL_4_MLP.cv_results_['rank_test_f1_micro']\n",
    "TRIAL_4_MLP.cv_results_['params'][ np.argmin(TRIAL_4_MLP.cv_results_['rank_test_f1_micro']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FOUR ROC AUC RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.1,\n",
       " 'classifier__hidden_layer_sizes': (20,),\n",
       " 'classifier__learning_rate': 'constant',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL FOUR ROC AUC RESULTS ---------------------------------------------\")\n",
    "TRIAL_4_MLP.cv_results_['rank_test_roc_auc_ovo']\n",
    "TRIAL_4_MLP.cv_results_['params'][ np.argmin(TRIAL_4_MLP.cv_results_['rank_test_roc_auc_ovo']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 4 MPL using best NEG LOG LOSS hyperparameters :0.9319\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 4 MPL using best F1 hyperparameters :0.93105\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 4 MPL using best ROC_AUC hyperparameters :0.9228\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best NEG LOG LOSS results to train on test set, \n",
    "p = TRIAL_4_MLP.cv_results_['params'][ np.argmin(TRIAL_4_MLP.cv_results_['rank_test_neg_log_loss']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train__neg_log_loss = pipe.predict(X_train)\n",
    "y_pred_test_neg_log_loss = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 4 MPL using best NEG LOG LOSS hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_neg_log_loss)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best F1 results to train on test set, LR_Trial 1\n",
    "p = TRIAL_4_MLP.cv_results_['params'][ np.argmin(TRIAL_4_MLP.cv_results_['rank_test_f1_micro']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train_roc_auc = pipe.predict(X_train)\n",
    "y_pred_test_f1 = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 4 MPL using best F1 hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_f1)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best ROC_AUC results to train on test set, LR_Trial 1\n",
    "p  = TRIAL_4_MLP.cv_results_['params'][ np.argmin(TRIAL_4_MLP.cv_results_['rank_test_roc_auc_ovo']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train_roc_auc = pipe.predict(X_train)\n",
    "y_pred_test_roc_auc = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 4 MPL using best ROC_AUC hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_roc_auc)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTI LAYERED PERCEPTRON TRIAL FIVE ON POKER DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    9.5s\n",
      "[Parallel(n_jobs=-1)]: Done 189 tasks      | elapsed:   10.2s\n",
      "[Parallel(n_jobs=-1)]: Done 210 tasks      | elapsed:   10.8s\n",
      "[Parallel(n_jobs=-1)]: Done 233 tasks      | elapsed:   12.8s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:   13.8s\n",
      "[Parallel(n_jobs=-1)]: Done 281 tasks      | elapsed:   15.3s\n",
      "[Parallel(n_jobs=-1)]: Done 306 tasks      | elapsed:   16.9s\n",
      "[Parallel(n_jobs=-1)]: Done 333 tasks      | elapsed:   18.4s\n",
      "[Parallel(n_jobs=-1)]: Done 360 tasks      | elapsed:   20.0s\n",
      "[Parallel(n_jobs=-1)]: Done 389 tasks      | elapsed:   21.2s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:   23.3s\n",
      "[Parallel(n_jobs=-1)]: Done 449 tasks      | elapsed:   24.4s\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:   26.8s\n",
      "[Parallel(n_jobs=-1)]: Done 513 tasks      | elapsed:   29.0s\n",
      "[Parallel(n_jobs=-1)]: Done 546 tasks      | elapsed:   31.7s\n",
      "[Parallel(n_jobs=-1)]: Done 581 tasks      | elapsed:   34.2s\n",
      "[Parallel(n_jobs=-1)]: Done 616 tasks      | elapsed:   37.2s\n",
      "[Parallel(n_jobs=-1)]: Done 653 tasks      | elapsed:   40.0s\n",
      "[Parallel(n_jobs=-1)]: Done 690 tasks      | elapsed:   42.4s\n",
      "[Parallel(n_jobs=-1)]: Done 729 tasks      | elapsed:   45.7s\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:   48.4s\n",
      "[Parallel(n_jobs=-1)]: Done 809 tasks      | elapsed:   51.7s\n",
      "[Parallel(n_jobs=-1)]: Done 850 tasks      | elapsed:   55.7s\n",
      "[Parallel(n_jobs=-1)]: Done 893 tasks      | elapsed:   59.8s\n",
      "[Parallel(n_jobs=-1)]: Done 936 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 981 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1026 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1073 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1120 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1169 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1269 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1320 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1373 tasks      | elapsed:  1.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FIVE RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1440 out of 1440 | elapsed:  1.8min finished\n"
     ]
    }
   ],
   "source": [
    "ITERATIONS = 1\n",
    "for i in range(1):\n",
    "    #For reach \"trial\", randomly select 5000 data points\n",
    "    random5000DataPoints = pokerData.sample(n = 5000, replace = True)\n",
    "    #From that 5000, create x set and y set\n",
    "    xSet = random5000DataPoints.iloc[:,0:9].values\n",
    "    ySet = random5000DataPoints.iloc[:,10].values\n",
    "    # Run GridSearch to find best hyperparameters per 5000 data points \n",
    "    \n",
    "    pipe = Pipeline([('std', StandardScaler()),('classifier', MLPClassifier(max_iter = 250))])\n",
    "\n",
    "    search_space = [\n",
    "                {\n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                {\n",
    "                 \n",
    "                  \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['sgd'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                \n",
    "                {\n",
    "                \n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['adam'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                ]\n",
    "    \n",
    "    #'precision_micro','roc_auc_ovo','jaccard','neg_log_loss'] JACCARD DOES NOT WORK FOR NON BINARY?\n",
    "    runHyperParamSearch = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=5), \n",
    "                          scoring=['recall_micro','f1_micro','roc_auc_ovo','neg_log_loss'], \n",
    "                          refit=False, verbose=10, n_jobs = -1)\n",
    "    TRIAL_5_MLP = runHyperParamSearch.fit(xSet, ySet)\n",
    "    print(\"---------------------------------TRIAL FIVE RESULTS ---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.30916595, 0.36201282, 0.38473201, 0.57229352, 0.69560037,\n",
       "        0.66467185, 1.13427572, 1.0832314 , 1.23586349, 1.52260756,\n",
       "        1.48817978, 1.44514079, 0.37432308, 0.36050954, 0.35630465,\n",
       "        0.61633105, 0.55387692, 0.55707984, 1.03569026, 1.06451492,\n",
       "        1.19462719, 1.55693717, 1.50719595, 1.41041179, 0.35310273,\n",
       "        0.31677227, 0.26172552, 0.55217552, 0.52905612, 0.45279126,\n",
       "        0.97373838, 0.89216738, 1.00386467, 1.37918515, 1.35106111,\n",
       "        1.28100104, 0.28704529, 0.23748965, 0.25451899, 0.48852072,\n",
       "        0.51764822, 0.44278126, 0.92519712, 0.95912571, 0.95772467,\n",
       "        1.30492263, 1.39840245, 1.33174357, 0.46720276, 0.54877238,\n",
       "        0.45218706, 0.82751293, 0.61413097, 0.605722  , 1.03749299,\n",
       "        1.20873847, 1.31703291, 1.56114163, 1.58576341, 1.42722878,\n",
       "        0.41946168, 0.40895114, 0.42296281, 0.73603292, 0.59220953,\n",
       "        0.67728438, 0.99936132, 1.0558085 , 0.93210115, 1.4533493 ,\n",
       "        1.3053215 , 1.41281533, 0.43577356, 0.2721312 , 0.41946087,\n",
       "        0.47781277, 0.64595518, 0.66657395, 1.13227582, 1.07672586,\n",
       "        1.19182491, 1.49298339, 1.57695413, 1.36557441, 0.4371747 ,\n",
       "        0.3402925 , 0.31286941, 0.62894139, 0.61592913, 0.56658845,\n",
       "        0.97524037, 0.86594629, 1.26168704, 1.21364312, 1.40180435,\n",
       "        1.48637886, 0.77136259, 0.50763655, 1.15519309, 0.53716192,\n",
       "        2.01983724, 1.05130458, 0.47600951, 2.12913136, 1.05320573,\n",
       "        0.42556615, 2.35432463, 1.08042974, 0.71251273, 1.90864153,\n",
       "        1.14498458, 0.52825427, 1.87901635, 1.10144758, 0.50133123,\n",
       "        1.86950779, 1.17741308, 0.48621836, 2.75597029, 1.24505315,\n",
       "        0.88536205, 1.24757309, 1.43913774, 0.70200367, 1.34035277,\n",
       "        1.07602549, 0.50813708, 1.68534956, 1.063415  , 0.43347354,\n",
       "        2.05106406, 1.12426662, 0.679284  , 1.79524407, 1.17220807,\n",
       "        0.55477743, 1.28930893, 1.05430722, 0.49192381, 2.42398434,\n",
       "        1.06081247, 0.44468317, 2.40436783, 1.07922807, 0.87775507,\n",
       "        0.94121013, 1.47176623, 0.78087149, 1.11285734, 1.38789372,\n",
       "        0.8380209 , 1.47967257, 1.43723626, 0.82420893, 1.85179262,\n",
       "        1.43943768, 0.9088819 , 1.06481557, 1.35436497, 0.83061452,\n",
       "        0.94781513, 1.42492561, 0.83001423, 1.93246226, 1.5238111 ,\n",
       "        0.88976502, 2.3377099 , 1.51750474, 0.94901624, 1.83808103,\n",
       "        1.71267314, 1.03328867, 1.70696836, 1.43733587, 0.83491874,\n",
       "        1.96088667, 1.66163001, 0.94761515, 1.81215849, 1.57895813,\n",
       "        1.0649157 , 0.73483186, 1.54783082, 0.90758023, 1.5946713 ,\n",
       "        1.35756769, 0.85593629, 1.90653987, 1.53371911, 0.87675414,\n",
       "        1.99271336, 1.58546262, 0.85423431, 0.98484654, 0.90718012,\n",
       "        0.68148627, 0.63894992, 0.64245329, 0.62974153, 0.59911566,\n",
       "        0.65666451, 0.55077324, 0.54006438, 0.4735076 , 0.78587565,\n",
       "        0.78017125, 0.88616214, 0.67748256, 0.55898018, 0.55577826,\n",
       "        0.55848064, 0.59421115, 0.58250132, 0.54356766, 0.47180634,\n",
       "        0.53716178, 0.78777699, 0.89877276, 1.05770936, 0.59110808,\n",
       "        0.68528967, 0.67698216, 0.59060817, 0.68869228, 0.563485  ,\n",
       "        0.64645629, 0.49862928, 0.58280139, 0.85143209, 1.001862  ,\n",
       "        0.80929627, 0.58540354, 0.71391392, 0.61733079, 0.509729  ,\n",
       "        0.54166594, 0.59461141, 0.50213261, 0.4848176 , 0.54857178,\n",
       "        1.21844711, 1.1574955 , 1.3332468 , 1.69615831, 1.7143743 ,\n",
       "        1.28260279, 3.19204555, 3.19704924, 2.87477202, 3.25730138,\n",
       "        3.37430205, 3.3948195 , 1.45325003, 1.42212296, 1.46706133,\n",
       "        1.55253501, 1.68595042, 2.0273438 , 3.01849613, 3.0716414 ,\n",
       "        2.77698784, 3.13079224, 3.36609468, 3.32726111, 1.09574251,\n",
       "        1.25808172, 1.53141708, 1.55914111, 1.82617102, 1.44744506,\n",
       "        2.90719981, 2.484937  , 2.42858825, 3.69657907, 3.36529412,\n",
       "        3.45767307, 1.44704423, 1.24927402, 1.32784204, 1.2872077 ,\n",
       "        1.4916831 , 1.35896897, 1.85199289, 2.14884753, 1.74690156,\n",
       "        1.77532644, 1.91955085, 2.03364873]),\n",
       " 'std_fit_time': array([0.0725144 , 0.11166424, 0.1448841 , 0.09978835, 0.09370432,\n",
       "        0.09390467, 0.09425187, 0.13270136, 0.06921854, 0.12323555,\n",
       "        0.04902264, 0.07553755, 0.05489617, 0.12676039, 0.11294925,\n",
       "        0.12054103, 0.07866731, 0.093272  , 0.13589779, 0.09334956,\n",
       "        0.16441455, 0.08942378, 0.14848149, 0.06591992, 0.09710749,\n",
       "        0.10350594, 0.03290943, 0.11587958, 0.11635608, 0.05547924,\n",
       "        0.15265594, 0.12647371, 0.11871446, 0.13802728, 0.09465834,\n",
       "        0.13440929, 0.04203036, 0.03132184, 0.04719963, 0.11284614,\n",
       "        0.06525349, 0.04548201, 0.19028711, 0.17367634, 0.20126315,\n",
       "        0.1582675 , 0.09134361, 0.05962282, 0.06567355, 0.11113283,\n",
       "        0.11388976, 0.07557471, 0.05480775, 0.09931837, 0.14191883,\n",
       "        0.12922463, 0.074316  , 0.17374355, 0.15624539, 0.16143152,\n",
       "        0.10571267, 0.17587908, 0.13197206, 0.18304221, 0.1519128 ,\n",
       "        0.15504401, 0.16211495, 0.19813074, 0.09263316, 0.17244738,\n",
       "        0.07793021, 0.12319962, 0.20024976, 0.06890015, 0.12555192,\n",
       "        0.08007878, 0.17996775, 0.14986335, 0.20518593, 0.29262995,\n",
       "        0.16236092, 0.26058537, 0.32115475, 0.31179904, 0.06559293,\n",
       "        0.13653197, 0.07570088, 0.07246593, 0.15807121, 0.08566996,\n",
       "        0.20799862, 0.12560838, 0.18330444, 0.10344341, 0.24192494,\n",
       "        0.21757366, 0.04576285, 0.17483092, 0.01275245, 0.03648198,\n",
       "        0.4730587 , 0.01030627, 0.00586503, 0.65193122, 0.01923271,\n",
       "        0.00643698, 0.36589105, 0.05221567, 0.08112022, 0.74987591,\n",
       "        0.05306924, 0.0400351 , 0.71556114, 0.06205587, 0.01602163,\n",
       "        1.04204721, 0.03299908, 0.02723939, 0.33333287, 0.13800103,\n",
       "        0.11643205, 0.92191003, 0.04777752, 0.06882508, 0.75014747,\n",
       "        0.0216595 , 0.02055215, 0.56252094, 0.01763049, 0.0148236 ,\n",
       "        0.60321568, 0.03477713, 0.03592864, 0.48605093, 0.04691976,\n",
       "        0.02555715, 0.68635925, 0.02212668, 0.01151218, 0.47408927,\n",
       "        0.00743225, 0.01245072, 0.64618098, 0.00703209, 0.10080986,\n",
       "        0.79517896, 0.03823763, 0.08910444, 0.92816135, 0.02168579,\n",
       "        0.0888444 , 0.57292555, 0.1068746 , 0.04113384, 0.7877732 ,\n",
       "        0.06138166, 0.09954355, 0.71877197, 0.01696924, 0.07770768,\n",
       "        0.86325944, 0.09022266, 0.0640436 , 0.63585363, 0.09945511,\n",
       "        0.03924759, 0.87179677, 0.09893656, 0.16179183, 0.84411673,\n",
       "        0.05282034, 0.04743506, 1.0314958 , 0.05812772, 0.02301631,\n",
       "        0.86878719, 0.12133274, 0.05383356, 0.72420866, 0.02996821,\n",
       "        0.08788258, 0.36006008, 0.10241084, 0.09078414, 0.80999269,\n",
       "        0.04693603, 0.09720135, 0.77951623, 0.07972844, 0.10395447,\n",
       "        0.81518027, 0.15169157, 0.13108971, 0.10056649, 0.11117986,\n",
       "        0.05527331, 0.11724732, 0.05325264, 0.13353233, 0.11988992,\n",
       "        0.09727323, 0.11087744, 0.03285248, 0.10848624, 0.14511695,\n",
       "        0.18346547, 0.18191767, 0.04113784, 0.08592226, 0.06492491,\n",
       "        0.13346317, 0.09732129, 0.08549339, 0.0853929 , 0.0514312 ,\n",
       "        0.05388454, 0.03073301, 0.15250461, 0.09022828, 0.09324347,\n",
       "        0.07430058, 0.07449012, 0.11926135, 0.08860658, 0.10487737,\n",
       "        0.13291746, 0.10029212, 0.04560989, 0.18495093, 0.15567019,\n",
       "        0.10096259, 0.15044196, 0.14668298, 0.04053383, 0.11510786,\n",
       "        0.05173138, 0.06040429, 0.08643028, 0.04042968, 0.08740287,\n",
       "        0.29903818, 0.22956898, 0.14809605, 0.48739946, 0.52885502,\n",
       "        0.11606383, 0.21364926, 0.31387243, 0.63932733, 0.05698579,\n",
       "        0.11671388, 0.14718362, 0.27159596, 0.36833817, 0.25741808,\n",
       "        0.09386866, 0.22036574, 0.39333959, 0.03979802, 0.06750233,\n",
       "        0.37916041, 0.35540273, 0.09076962, 0.12446353, 0.28823842,\n",
       "        0.31363669, 0.30178446, 0.23676608, 0.69649997, 0.1067804 ,\n",
       "        0.71973119, 0.81273722, 0.62139706, 0.18418617, 0.27113084,\n",
       "        0.22944368, 0.26859144, 0.20052314, 0.30040831, 0.14264006,\n",
       "        0.1202412 , 0.18614067, 0.43706712, 0.52539997, 0.29348354,\n",
       "        0.36076645, 0.27710996, 0.26632915]),\n",
       " 'mean_score_time': array([0.00750637, 0.00770578, 0.00790629, 0.00840678, 0.00940619,\n",
       "        0.0089076 , 0.0124115 , 0.00950699, 0.01471233, 0.01161013,\n",
       "        0.01301079, 0.00850768, 0.01170936, 0.00980787, 0.00780625,\n",
       "        0.00930786, 0.00980921, 0.01021008, 0.00980978, 0.01020932,\n",
       "        0.01611376, 0.01291113, 0.00910707, 0.01030827, 0.00880752,\n",
       "        0.00760703, 0.00900779, 0.008007  , 0.00830669, 0.00810618,\n",
       "        0.010007  , 0.01130838, 0.00950851, 0.01341038, 0.0087069 ,\n",
       "        0.00840735, 0.01080995, 0.008007  , 0.007306  , 0.00810714,\n",
       "        0.00870543, 0.00760808, 0.01020918, 0.00950804, 0.01100974,\n",
       "        0.0099082 , 0.00910745, 0.00820627, 0.00850692, 0.00740581,\n",
       "        0.00760651, 0.00950727, 0.00800557, 0.00850816, 0.01090989,\n",
       "        0.01081042, 0.01271167, 0.01111054, 0.00980844, 0.00950747,\n",
       "        0.00910597, 0.00880742, 0.00780745, 0.00940871, 0.00840797,\n",
       "        0.00930743, 0.00940657, 0.0110105 , 0.01131077, 0.01231174,\n",
       "        0.00900741, 0.00830698, 0.00840788, 0.0074069 , 0.00810571,\n",
       "        0.00850635, 0.00850792, 0.0085084 , 0.01030717, 0.01020913,\n",
       "        0.01341214, 0.00990686, 0.00950851, 0.00961003, 0.00850739,\n",
       "        0.00820813, 0.00820842, 0.00840688, 0.00861115, 0.00860815,\n",
       "        0.00920696, 0.00920887, 0.01080666, 0.0101089 , 0.00910807,\n",
       "        0.00800681, 0.00690603, 0.00660601, 0.00670605, 0.00670567,\n",
       "        0.00670562, 0.00640554, 0.00700626, 0.00730643, 0.00700703,\n",
       "        0.00700612, 0.00670595, 0.00760632, 0.00650568, 0.00700641,\n",
       "        0.00670586, 0.00650578, 0.00760632, 0.00720682, 0.00700593,\n",
       "        0.00810714, 0.0091074 , 0.00780654, 0.00820727, 0.00770617,\n",
       "        0.00820665, 0.00790672, 0.00640545, 0.00690627, 0.00660543,\n",
       "        0.00640554, 0.00790696, 0.00770664, 0.00670576, 0.00730572,\n",
       "        0.00720639, 0.00840731, 0.00690603, 0.00640564, 0.00660529,\n",
       "        0.00660558, 0.00660572, 0.00670524, 0.00670567, 0.01050925,\n",
       "        0.00700588, 0.00690579, 0.00720587, 0.00700626, 0.00710626,\n",
       "        0.00670552, 0.00670524, 0.00700583, 0.00680609, 0.00680585,\n",
       "        0.0072063 , 0.00710626, 0.00730658, 0.0077065 , 0.00730634,\n",
       "        0.00740676, 0.00660558, 0.00690637, 0.00650573, 0.00720615,\n",
       "        0.00660567, 0.00730619, 0.00740628, 0.0082068 , 0.00710611,\n",
       "        0.00780754, 0.00840702, 0.00750675, 0.00980844, 0.00760603,\n",
       "        0.00713606, 0.00980802, 0.01090932, 0.0071063 , 0.0073061 ,\n",
       "        0.00800686, 0.00800648, 0.00770659, 0.00740647, 0.00810723,\n",
       "        0.00750675, 0.00660591, 0.00650582, 0.00700612, 0.0072062 ,\n",
       "        0.00680623, 0.00760674, 0.00820746, 0.01211014, 0.00880723,\n",
       "        0.00800676, 0.00750623, 0.00830722, 0.006706  , 0.00850706,\n",
       "        0.0068058 , 0.00730624, 0.01100931, 0.00800743, 0.00800681,\n",
       "        0.00700583, 0.0071063 , 0.00680609, 0.00690608, 0.00630584,\n",
       "        0.00640559, 0.0063055 , 0.00660601, 0.00700655, 0.00730624,\n",
       "        0.00740633, 0.00690589, 0.00690565, 0.00690603, 0.0072062 ,\n",
       "        0.00710602, 0.00670571, 0.00640593, 0.00640559, 0.0065062 ,\n",
       "        0.00700612, 0.00740638, 0.00670595, 0.0087081 , 0.010709  ,\n",
       "        0.00780764, 0.00730639, 0.00720549, 0.00630565, 0.00640497,\n",
       "        0.00640583, 0.00720615, 0.00730648, 0.00720596, 0.00790639,\n",
       "        0.00680566, 0.0072062 , 0.00720558, 0.00680575, 0.00720644,\n",
       "        0.00670528, 0.00660582, 0.00660563, 0.00660582, 0.00690627,\n",
       "        0.00680676, 0.00710583, 0.00870709, 0.00740671, 0.00840745,\n",
       "        0.00820708, 0.00810709, 0.00650563, 0.00630517, 0.00960889,\n",
       "        0.00690598, 0.0069057 , 0.00660582, 0.00740657, 0.00770669,\n",
       "        0.00890799, 0.00930824, 0.00820713, 0.01311145, 0.00640554,\n",
       "        0.00640521, 0.00730672, 0.00670605, 0.00700631, 0.00740623,\n",
       "        0.00730715, 0.00790696, 0.01000891, 0.00860729, 0.01201038,\n",
       "        0.00800719, 0.00700607, 0.00660582, 0.0067059 , 0.00700569,\n",
       "        0.00720625, 0.00660596, 0.00710583, 0.00780716, 0.00720611,\n",
       "        0.00610528, 0.00540514, 0.00460391]),\n",
       " 'std_score_time': array([7.08405626e-04, 1.16683884e-03, 1.06854179e-03, 1.39380160e-03,\n",
       "        1.62708061e-03, 5.84633750e-04, 3.91954335e-03, 7.08339944e-04,\n",
       "        6.17460041e-03, 2.59833245e-03, 3.85148330e-03, 4.47128258e-04,\n",
       "        5.43876827e-03, 2.61949851e-03, 8.71531573e-04, 2.15980220e-03,\n",
       "        3.14277605e-03, 2.44259281e-03, 9.80820263e-04, 1.25084893e-03,\n",
       "        7.25521634e-03, 4.06962101e-03, 1.02093652e-03, 2.13792487e-03,\n",
       "        6.80237205e-04, 4.90557739e-04, 3.75694684e-03, 6.32749405e-04,\n",
       "        5.10295822e-04, 3.73800778e-04, 5.48342779e-04, 2.22795374e-03,\n",
       "        6.33578354e-04, 5.09723232e-03, 5.10931818e-04, 3.74996274e-04,\n",
       "        4.13631015e-03, 4.47501585e-04, 2.45012908e-04, 8.01063200e-04,\n",
       "        6.78791109e-04, 8.01160199e-04, 1.72459203e-03, 8.96818039e-04,\n",
       "        2.41046093e-03, 4.88373652e-04, 1.32078471e-03, 2.46634016e-04,\n",
       "        9.49269824e-04, 3.74879002e-04, 5.83193054e-04, 1.58271312e-03,\n",
       "        3.15154387e-04, 3.16281217e-04, 1.59496599e-03, 9.28400495e-04,\n",
       "        2.87466354e-03, 3.84314385e-03, 1.60129812e-03, 1.67587776e-03,\n",
       "        6.64327390e-04, 1.88842376e-03, 9.28200125e-04, 1.49915872e-03,\n",
       "        6.63673968e-04, 1.47112979e-03, 8.59519552e-04, 2.07519627e-03,\n",
       "        1.40138392e-03, 4.89966949e-03, 6.33014329e-04, 2.46257656e-04,\n",
       "        8.61768080e-04, 4.90942989e-04, 5.83649994e-04, 1.14084661e-03,\n",
       "        6.34748377e-04, 5.46988536e-04, 8.74492979e-04, 2.18419888e-03,\n",
       "        4.68609876e-03, 8.61750352e-04, 5.48162291e-04, 6.63817073e-04,\n",
       "        1.04982535e-03, 9.27212982e-04, 3.99462815e-04, 9.17475076e-04,\n",
       "        4.89223570e-04, 3.74217822e-04, 8.11760083e-04, 5.12776996e-04,\n",
       "        1.20919581e-03, 8.00766589e-04, 1.24150014e-03, 5.47509836e-04,\n",
       "        3.74867618e-04, 4.91361953e-04, 7.48825283e-04, 2.44970402e-04,\n",
       "        3.99899522e-04, 2.00486261e-04, 1.90734863e-07, 1.12335916e-03,\n",
       "        3.16588152e-04, 3.16581896e-04, 2.45028582e-04, 5.83674556e-04,\n",
       "        5.48031197e-04, 7.75371647e-04, 5.10136308e-04, 4.47394542e-04,\n",
       "        1.11444017e-03, 1.40123383e-03, 5.48248839e-04, 1.53022793e-03,\n",
       "        1.90985456e-03, 6.00855463e-04, 1.12369028e-03, 1.16759026e-03,\n",
       "        2.08969351e-03, 1.02044548e-03, 3.74253523e-04, 3.74865191e-04,\n",
       "        2.00367711e-04, 2.00248013e-04, 1.02051558e-03, 6.78451208e-04,\n",
       "        2.45184316e-04, 6.00767561e-04, 2.45321267e-04, 1.24205338e-03,\n",
       "        3.74292081e-04, 2.00057392e-04, 5.83453913e-04, 3.74941924e-04,\n",
       "        4.90251878e-04, 2.45418659e-04, 2.45067608e-04, 6.03015045e-03,\n",
       "        3.16808125e-04, 2.00391801e-04, 3.99756556e-04, 3.16355872e-04,\n",
       "        9.70144521e-04, 2.44795259e-04, 4.00448215e-04, 5.48597049e-04,\n",
       "        2.45243400e-04, 2.45340081e-04, 4.00495657e-04, 2.00057165e-04,\n",
       "        9.28004326e-04, 6.78844990e-04, 4.00364842e-04, 1.99938417e-04,\n",
       "        2.00176296e-04, 5.83658121e-04, 3.16280339e-04, 5.10098841e-04,\n",
       "        2.00367144e-04, 7.49182115e-04, 5.83282318e-04, 1.28944533e-03,\n",
       "        1.99890592e-04, 8.71880991e-04, 1.02073534e-03, 7.07797354e-04,\n",
       "        4.20597790e-03, 1.24176893e-03, 1.02920831e-03, 2.84116128e-03,\n",
       "        6.03321419e-03, 3.74865040e-04, 2.45282086e-04, 8.37454297e-04,\n",
       "        8.37539874e-04, 6.78289408e-04, 2.00392028e-04, 1.11426879e-03,\n",
       "        1.26587503e-03, 4.90310284e-04, 7.62939453e-07, 5.48292330e-04,\n",
       "        5.09799962e-04, 3.99518650e-04, 8.00460730e-04, 1.36505534e-03,\n",
       "        9.21320880e-03, 1.66276878e-03, 5.48118172e-04, 4.47554480e-04,\n",
       "        4.10975311e-03, 6.79013769e-04, 3.77109053e-03, 4.00257309e-04,\n",
       "        1.43639194e-03, 6.13668072e-03, 1.76198950e-03, 1.81762665e-03,\n",
       "        5.48031450e-04, 2.00033671e-04, 2.45437836e-04, 3.74942045e-04,\n",
       "        2.45106572e-04, 2.00153157e-04, 2.45320618e-04, 2.00319319e-04,\n",
       "        1.26579953e-03, 5.10182995e-04, 5.83658140e-04, 3.74827056e-04,\n",
       "        2.00082065e-04, 2.00033500e-04, 5.09706069e-04, 4.90573429e-04,\n",
       "        4.00567065e-04, 5.84189793e-04, 2.00034126e-04, 3.16431236e-04,\n",
       "        7.75679528e-04, 1.32022407e-03, 2.45029788e-04, 1.86152107e-03,\n",
       "        6.49132051e-03, 8.73216235e-04, 4.00388251e-04, 2.45379725e-04,\n",
       "        6.00361888e-04, 3.74380872e-04, 2.00273764e-04, 1.16692366e-03,\n",
       "        1.12343118e-03, 1.20893962e-03, 2.06072752e-03, 2.44893277e-04,\n",
       "        6.78837761e-04, 7.48621561e-04, 4.00114301e-04, 2.45281716e-04,\n",
       "        6.79083801e-04, 2.00176921e-04, 2.00153611e-04, 2.00295697e-04,\n",
       "        2.00391064e-04, 4.00674622e-04, 2.00152930e-04, 2.22874498e-03,\n",
       "        3.74227896e-04, 1.59474124e-03, 9.27772833e-04, 7.35273955e-04,\n",
       "        3.16431042e-04, 2.44854437e-04, 6.70991575e-03, 5.83879018e-04,\n",
       "        3.73820279e-04, 2.00057790e-04, 3.74636259e-04, 1.16694825e-03,\n",
       "        2.55905144e-03, 2.40212880e-03, 1.16723442e-03, 7.07280284e-03,\n",
       "        2.00128584e-04, 1.99961911e-04, 1.16710359e-03, 2.45049042e-04,\n",
       "        4.90933902e-07, 6.63938289e-04, 2.45359552e-04, 1.20058069e-03,\n",
       "        4.55370303e-03, 6.63808921e-04, 7.34797226e-03, 5.48597174e-04,\n",
       "        1.04936938e-03, 7.35624295e-04, 6.78619854e-04, 4.47875072e-04,\n",
       "        5.10575720e-04, 2.00104905e-04, 2.00153043e-04, 9.80523352e-04,\n",
       "        2.45749335e-04, 1.15817956e-03, 9.17468613e-04, 7.35416620e-04]),\n",
       " 'param_classifier': masked_array(data=[MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__activation': masked_array(data=['logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__alpha': masked_array(data=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__hidden_layer_sizes': masked_array(data=[(5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__learning_rate': masked_array(data=['constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'}],\n",
       " 'split0_test_recall_micro': array([0.929, 0.933, 0.931, 0.923, 0.929, 0.929, 0.909, 0.918, 0.908,\n",
       "        0.902, 0.907, 0.902, 0.928, 0.931, 0.933, 0.929, 0.921, 0.926,\n",
       "        0.921, 0.913, 0.92 , 0.904, 0.911, 0.915, 0.932, 0.933, 0.934,\n",
       "        0.928, 0.931, 0.932, 0.918, 0.922, 0.919, 0.911, 0.902, 0.906,\n",
       "        0.933, 0.935, 0.933, 0.928, 0.937, 0.93 , 0.929, 0.918, 0.922,\n",
       "        0.922, 0.925, 0.922, 0.933, 0.936, 0.934, 0.924, 0.922, 0.925,\n",
       "        0.914, 0.912, 0.919, 0.899, 0.897, 0.89 , 0.933, 0.931, 0.93 ,\n",
       "        0.923, 0.923, 0.928, 0.925, 0.919, 0.921, 0.91 , 0.908, 0.902,\n",
       "        0.932, 0.932, 0.933, 0.932, 0.919, 0.923, 0.909, 0.92 , 0.917,\n",
       "        0.904, 0.897, 0.912, 0.932, 0.934, 0.935, 0.93 , 0.929, 0.932,\n",
       "        0.92 , 0.916, 0.92 , 0.899, 0.913, 0.913, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.26 , 0.933, 0.933, 0.909, 0.933, 0.933, 0.775, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.405, 0.933,\n",
       "        0.933, 0.927, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.232, 0.933, 0.933, 0.816, 0.933, 0.933, 0.619, 0.933,\n",
       "        0.933, 0.901, 0.933, 0.933, 0.576, 0.933, 0.933, 0.748, 0.933,\n",
       "        0.933, 0.573, 0.933, 0.933, 0.492, 0.933, 0.933, 0.379, 0.933,\n",
       "        0.933, 0.381, 0.933, 0.933, 0.557, 0.933, 0.933, 0.699, 0.933,\n",
       "        0.933, 0.572, 0.933, 0.933, 0.518, 0.933, 0.933, 0.47 , 0.933,\n",
       "        0.933, 0.618, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.932, 0.933, 0.934,\n",
       "        0.933, 0.934, 0.934, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.934, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933]),\n",
       " 'split1_test_recall_micro': array([0.934, 0.933, 0.932, 0.924, 0.928, 0.931, 0.922, 0.917, 0.921,\n",
       "        0.91 , 0.915, 0.905, 0.933, 0.934, 0.933, 0.929, 0.926, 0.928,\n",
       "        0.919, 0.914, 0.913, 0.906, 0.898, 0.902, 0.934, 0.933, 0.933,\n",
       "        0.929, 0.924, 0.931, 0.914, 0.92 , 0.922, 0.917, 0.909, 0.901,\n",
       "        0.933, 0.934, 0.934, 0.929, 0.933, 0.93 , 0.926, 0.925, 0.932,\n",
       "        0.914, 0.925, 0.912, 0.932, 0.933, 0.935, 0.919, 0.927, 0.925,\n",
       "        0.916, 0.915, 0.913, 0.897, 0.902, 0.901, 0.93 , 0.936, 0.935,\n",
       "        0.924, 0.928, 0.928, 0.91 , 0.92 , 0.911, 0.912, 0.899, 0.886,\n",
       "        0.93 , 0.933, 0.932, 0.924, 0.923, 0.929, 0.922, 0.916, 0.914,\n",
       "        0.915, 0.91 , 0.912, 0.934, 0.933, 0.933, 0.927, 0.926, 0.927,\n",
       "        0.916, 0.914, 0.924, 0.915, 0.911, 0.896, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.109, 0.933, 0.933, 0.933, 0.933, 0.933, 0.907, 0.933,\n",
       "        0.933, 0.068, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.067, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.926, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.602, 0.933, 0.933, 0.768, 0.933, 0.933, 0.516, 0.933,\n",
       "        0.933, 0.59 , 0.933, 0.933, 0.621, 0.933, 0.933, 0.62 , 0.933,\n",
       "        0.933, 0.717, 0.933, 0.933, 0.569, 0.933, 0.933, 0.847, 0.933,\n",
       "        0.933, 0.729, 0.933, 0.933, 0.629, 0.933, 0.933, 0.719, 0.933,\n",
       "        0.933, 0.561, 0.933, 0.933, 0.731, 0.933, 0.933, 0.547, 0.933,\n",
       "        0.933, 0.806, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.934, 0.933, 0.933, 0.933, 0.934, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.934, 0.932, 0.933,\n",
       "        0.934, 0.934, 0.932, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.932, 0.933, 0.932, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.934, 0.933, 0.933, 0.933, 0.933]),\n",
       " 'split2_test_recall_micro': array([0.93 , 0.93 , 0.934, 0.924, 0.924, 0.925, 0.917, 0.913, 0.913,\n",
       "        0.912, 0.911, 0.916, 0.933, 0.931, 0.933, 0.93 , 0.923, 0.927,\n",
       "        0.924, 0.921, 0.913, 0.895, 0.896, 0.914, 0.936, 0.93 , 0.933,\n",
       "        0.932, 0.926, 0.929, 0.922, 0.926, 0.911, 0.926, 0.906, 0.925,\n",
       "        0.933, 0.932, 0.933, 0.931, 0.93 , 0.932, 0.922, 0.92 , 0.925,\n",
       "        0.915, 0.91 , 0.92 , 0.933, 0.936, 0.932, 0.926, 0.92 , 0.926,\n",
       "        0.912, 0.916, 0.91 , 0.895, 0.907, 0.883, 0.931, 0.933, 0.933,\n",
       "        0.927, 0.931, 0.925, 0.909, 0.912, 0.918, 0.878, 0.897, 0.891,\n",
       "        0.932, 0.933, 0.933, 0.93 , 0.926, 0.929, 0.914, 0.923, 0.909,\n",
       "        0.913, 0.897, 0.894, 0.934, 0.933, 0.934, 0.926, 0.929, 0.924,\n",
       "        0.916, 0.924, 0.914, 0.903, 0.899, 0.912, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.524, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.069, 0.933, 0.933, 0.778, 0.933, 0.933, 0.874, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.544, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.509, 0.933,\n",
       "        0.933, 0.862, 0.933, 0.933, 0.344, 0.933, 0.933, 0.612, 0.933,\n",
       "        0.933, 0.634, 0.933, 0.933, 0.312, 0.933, 0.933, 0.25 , 0.933,\n",
       "        0.933, 0.556, 0.933, 0.933, 0.729, 0.933, 0.933, 0.609, 0.933,\n",
       "        0.933, 0.373, 0.933, 0.933, 0.438, 0.933, 0.933, 0.632, 0.933,\n",
       "        0.933, 0.913, 0.933, 0.933, 0.794, 0.933, 0.933, 0.875, 0.933,\n",
       "        0.933, 0.332, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.934, 0.933, 0.934, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.935, 0.933, 0.933,\n",
       "        0.934, 0.935, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.934, 0.933, 0.935, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933]),\n",
       " 'split3_test_recall_micro': array([0.928, 0.924, 0.933, 0.925, 0.924, 0.926, 0.923, 0.914, 0.912,\n",
       "        0.89 , 0.904, 0.914, 0.93 , 0.931, 0.933, 0.922, 0.929, 0.929,\n",
       "        0.915, 0.915, 0.915, 0.908, 0.899, 0.901, 0.933, 0.933, 0.933,\n",
       "        0.926, 0.931, 0.928, 0.918, 0.924, 0.921, 0.912, 0.909, 0.919,\n",
       "        0.93 , 0.932, 0.93 , 0.929, 0.932, 0.931, 0.919, 0.922, 0.927,\n",
       "        0.919, 0.92 , 0.921, 0.93 , 0.934, 0.931, 0.921, 0.927, 0.924,\n",
       "        0.911, 0.912, 0.927, 0.894, 0.907, 0.907, 0.933, 0.935, 0.933,\n",
       "        0.924, 0.923, 0.919, 0.91 , 0.905, 0.912, 0.9  , 0.893, 0.895,\n",
       "        0.93 , 0.932, 0.929, 0.92 , 0.931, 0.917, 0.919, 0.923, 0.906,\n",
       "        0.909, 0.905, 0.909, 0.933, 0.934, 0.934, 0.928, 0.927, 0.926,\n",
       "        0.922, 0.925, 0.919, 0.911, 0.899, 0.91 , 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.864, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.067, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.592, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.537, 0.933, 0.933, 0.579, 0.933, 0.933, 0.775, 0.933,\n",
       "        0.933, 0.712, 0.933, 0.933, 0.605, 0.933, 0.933, 0.761, 0.933,\n",
       "        0.933, 0.617, 0.933, 0.933, 0.675, 0.933, 0.933, 0.45 , 0.933,\n",
       "        0.933, 0.492, 0.933, 0.933, 0.651, 0.933, 0.933, 0.413, 0.933,\n",
       "        0.933, 0.692, 0.933, 0.933, 0.411, 0.933, 0.933, 0.516, 0.933,\n",
       "        0.933, 0.462, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.932, 0.933, 0.933, 0.931, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933]),\n",
       " 'split4_test_recall_micro': array([0.932, 0.927, 0.931, 0.919, 0.926, 0.923, 0.909, 0.915, 0.91 ,\n",
       "        0.901, 0.896, 0.899, 0.932, 0.931, 0.932, 0.926, 0.924, 0.93 ,\n",
       "        0.924, 0.913, 0.923, 0.918, 0.914, 0.904, 0.932, 0.932, 0.932,\n",
       "        0.923, 0.93 , 0.928, 0.926, 0.923, 0.914, 0.899, 0.92 , 0.899,\n",
       "        0.932, 0.932, 0.928, 0.928, 0.927, 0.931, 0.921, 0.923, 0.932,\n",
       "        0.897, 0.924, 0.921, 0.929, 0.931, 0.927, 0.918, 0.924, 0.921,\n",
       "        0.911, 0.916, 0.925, 0.895, 0.906, 0.89 , 0.934, 0.932, 0.931,\n",
       "        0.919, 0.92 , 0.927, 0.914, 0.907, 0.92 , 0.893, 0.9  , 0.892,\n",
       "        0.929, 0.929, 0.931, 0.92 , 0.924, 0.926, 0.91 , 0.92 , 0.92 ,\n",
       "        0.914, 0.909, 0.911, 0.931, 0.931, 0.93 , 0.926, 0.929, 0.929,\n",
       "        0.918, 0.924, 0.917, 0.915, 0.92 , 0.915, 0.932, 0.932, 0.932,\n",
       "        0.932, 0.163, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932,\n",
       "        0.932, 0.094, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932,\n",
       "        0.932, 0.932, 0.932, 0.932, 0.497, 0.932, 0.932, 0.93 , 0.932,\n",
       "        0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932,\n",
       "        0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932,\n",
       "        0.932, 0.859, 0.932, 0.932, 0.843, 0.932, 0.932, 0.659, 0.932,\n",
       "        0.932, 0.59 , 0.932, 0.932, 0.796, 0.932, 0.932, 0.884, 0.932,\n",
       "        0.932, 0.429, 0.932, 0.932, 0.625, 0.932, 0.932, 0.341, 0.932,\n",
       "        0.932, 0.782, 0.932, 0.932, 0.837, 0.932, 0.932, 0.573, 0.932,\n",
       "        0.932, 0.574, 0.932, 0.932, 0.391, 0.932, 0.932, 0.656, 0.932,\n",
       "        0.932, 0.641, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932,\n",
       "        0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932,\n",
       "        0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932,\n",
       "        0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932,\n",
       "        0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932,\n",
       "        0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.931, 0.932, 0.932,\n",
       "        0.932, 0.932, 0.932, 0.934, 0.932, 0.933, 0.93 , 0.932, 0.932,\n",
       "        0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932,\n",
       "        0.932, 0.932, 0.93 , 0.932, 0.932, 0.932, 0.932, 0.932, 0.932,\n",
       "        0.932, 0.932, 0.932, 0.932, 0.931, 0.932, 0.932, 0.932, 0.932,\n",
       "        0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.931]),\n",
       " 'mean_test_recall_micro': array([0.9306, 0.9294, 0.9322, 0.923 , 0.9262, 0.9268, 0.916 , 0.9154,\n",
       "        0.9128, 0.903 , 0.9066, 0.9072, 0.9312, 0.9316, 0.9328, 0.9272,\n",
       "        0.9246, 0.928 , 0.9206, 0.9152, 0.9168, 0.9062, 0.9036, 0.9072,\n",
       "        0.9334, 0.9322, 0.933 , 0.9276, 0.9284, 0.9296, 0.9196, 0.923 ,\n",
       "        0.9174, 0.913 , 0.9092, 0.91  , 0.9322, 0.933 , 0.9316, 0.929 ,\n",
       "        0.9318, 0.9308, 0.9234, 0.9216, 0.9276, 0.9134, 0.9208, 0.9192,\n",
       "        0.9314, 0.934 , 0.9318, 0.9216, 0.924 , 0.9242, 0.9128, 0.9142,\n",
       "        0.9188, 0.896 , 0.9038, 0.8942, 0.9322, 0.9334, 0.9324, 0.9234,\n",
       "        0.925 , 0.9254, 0.9136, 0.9126, 0.9164, 0.8986, 0.8994, 0.8932,\n",
       "        0.9306, 0.9318, 0.9316, 0.9252, 0.9246, 0.9248, 0.9148, 0.9204,\n",
       "        0.9132, 0.911 , 0.9036, 0.9076, 0.9328, 0.933 , 0.9332, 0.9274,\n",
       "        0.928 , 0.9276, 0.9184, 0.9206, 0.9188, 0.9086, 0.9084, 0.9092,\n",
       "        0.9328, 0.9328, 0.9328, 0.9328, 0.5324, 0.9328, 0.9328, 0.919 ,\n",
       "        0.9328, 0.9328, 0.9276, 0.9328, 0.9328, 0.2848, 0.9328, 0.9328,\n",
       "        0.897 , 0.9328, 0.9328, 0.8894, 0.9328, 0.9328, 0.9328, 0.9328,\n",
       "        0.9328, 0.6726, 0.9328, 0.9328, 0.9324, 0.9328, 0.9328, 0.9328,\n",
       "        0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.508 , 0.9328, 0.9328,\n",
       "        0.9316, 0.9328, 0.9328, 0.9314, 0.9328, 0.9328, 0.848 , 0.9328,\n",
       "        0.9328, 0.6184, 0.9328, 0.9328, 0.67  , 0.9328, 0.9328, 0.6362,\n",
       "        0.9328, 0.9328, 0.6854, 0.9328, 0.9328, 0.582 , 0.9328, 0.9328,\n",
       "        0.6526, 0.9328, 0.9328, 0.5784, 0.9328, 0.9328, 0.618 , 0.9328,\n",
       "        0.9328, 0.5252, 0.9328, 0.9328, 0.5514, 0.9328, 0.9328, 0.6224,\n",
       "        0.9328, 0.9328, 0.6072, 0.9328, 0.9328, 0.6624, 0.9328, 0.9328,\n",
       "        0.569 , 0.9328, 0.9328, 0.6128, 0.9328, 0.9328, 0.5718, 0.9328,\n",
       "        0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.9328,\n",
       "        0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.9328,\n",
       "        0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.9328,\n",
       "        0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.9328,\n",
       "        0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.9328,\n",
       "        0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.9328,\n",
       "        0.9326, 0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.9336, 0.9328,\n",
       "        0.9332, 0.9324, 0.933 , 0.9328, 0.9328, 0.9328, 0.9328, 0.9328,\n",
       "        0.9328, 0.9328, 0.9332, 0.9326, 0.933 , 0.9332, 0.9336, 0.9324,\n",
       "        0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.9326, 0.9328,\n",
       "        0.9328, 0.9324, 0.9328, 0.933 , 0.9328, 0.9328, 0.9328, 0.9328,\n",
       "        0.9328, 0.9328, 0.9328, 0.933 , 0.9328, 0.9328, 0.9328, 0.9326]),\n",
       " 'std_test_recall_micro': array([0.00215407, 0.00349857, 0.00116619, 0.00209762, 0.00203961,\n",
       "        0.00285657, 0.0060663 , 0.00185472, 0.00444522, 0.00779744,\n",
       "        0.00646838, 0.00667533, 0.00193907, 0.0012    , 0.0004    ,\n",
       "        0.00292575, 0.00272764, 0.00141421, 0.00338231, 0.00299333,\n",
       "        0.00401995, 0.00738647, 0.00739189, 0.00604649, 0.00149666,\n",
       "        0.00116619, 0.00063246, 0.00300666, 0.00287054, 0.00162481,\n",
       "        0.00407922, 0.002     , 0.00422374, 0.00878635, 0.00597997,\n",
       "        0.01023719, 0.00116619, 0.00126491, 0.00224499, 0.00109545,\n",
       "        0.00331059, 0.00074833, 0.00361109, 0.00241661, 0.00392938,\n",
       "        0.00868562, 0.00570614, 0.00365513, 0.00162481, 0.00189737,\n",
       "        0.00278568, 0.00300666, 0.00275681, 0.00172047, 0.00193907,\n",
       "        0.00183303, 0.00658483, 0.00178885, 0.00386782, 0.00861162,\n",
       "        0.00146969, 0.00185472, 0.00174356, 0.00257682, 0.00394968,\n",
       "        0.00338231, 0.00595315, 0.00608605, 0.00412795, 0.01238709,\n",
       "        0.00492341, 0.00526878, 0.0012    , 0.00146969, 0.00149666,\n",
       "        0.004996  , 0.00392938, 0.00448999, 0.00503587, 0.00257682,\n",
       "        0.00511468, 0.00404969, 0.00564269, 0.00688767, 0.00116619,\n",
       "        0.00109545, 0.00172047, 0.00149666, 0.00126491, 0.00272764,\n",
       "        0.00233238, 0.00463033, 0.00331059, 0.00649923, 0.0082365 ,\n",
       "        0.00679412, 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.35686277, 0.0004    , 0.0004    , 0.02750273, 0.0004    ,\n",
       "        0.0004    , 0.01030728, 0.0004    , 0.0004    , 0.33188998,\n",
       "        0.0004    , 0.0004    , 0.06020299, 0.0004    , 0.0004    ,\n",
       "        0.06154868, 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.34670195, 0.0004    , 0.0004    , 0.0012    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.28043466, 0.0004    ,\n",
       "        0.0004    , 0.00233238, 0.0004    , 0.0004    , 0.00272764,\n",
       "        0.0004    , 0.0004    , 0.16950044, 0.0004    , 0.0004    ,\n",
       "        0.23385004, 0.0004    , 0.0004    , 0.18729976, 0.0004    ,\n",
       "        0.0004    , 0.0838031 , 0.0004    , 0.0004    , 0.11664922,\n",
       "        0.0004    , 0.0004    , 0.15542329, 0.0004    , 0.0004    ,\n",
       "        0.2179666 , 0.0004    , 0.0004    , 0.09332867, 0.0004    ,\n",
       "        0.0004    , 0.0823359 , 0.0004    , 0.0004    , 0.18522246,\n",
       "        0.0004    , 0.0004    , 0.17269001, 0.0004    , 0.0004    ,\n",
       "        0.13053367, 0.0004    , 0.0004    , 0.10993162, 0.0004    ,\n",
       "        0.0004    , 0.13412323, 0.0004    , 0.0004    , 0.16499576,\n",
       "        0.0004    , 0.0004    , 0.1447113 , 0.0004    , 0.0004    ,\n",
       "        0.16206961, 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0008    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004899 , 0.0004    , 0.0004    , 0.0012    ,\n",
       "        0.00063246, 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.00116619, 0.0004899 ,\n",
       "        0.00063246, 0.00074833, 0.0010198 , 0.00135647, 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004899 , 0.0004    , 0.0004    , 0.0010198 , 0.0009798 ,\n",
       "        0.00109545, 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.00063246, 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0008    ]),\n",
       " 'rank_test_recall_micro': array([191, 194, 176, 217, 206, 205, 234, 235, 243, 260, 255, 254, 189,\n",
       "        183,  17, 204, 211, 197, 222, 236, 232, 256, 258, 253,   4, 176,\n",
       "         11, 200, 196, 193, 225, 217, 231, 242, 248, 247, 176,  11, 183,\n",
       "        195, 180, 190, 215, 219, 200, 240, 221, 226, 187,   1, 180, 219,\n",
       "        214, 213, 243, 238, 228, 264, 257, 265, 176,   5, 172, 215, 209,\n",
       "        207, 239, 245, 233, 262, 261, 266, 191, 182, 183, 208, 211, 210,\n",
       "        237, 224, 241, 246, 258, 252,  17,  11,   6, 203, 197, 200, 230,\n",
       "        222, 228, 250, 251, 248,  17,  17,  17,  17, 285,  17,  17, 227,\n",
       "         17,  17, 199,  17,  17, 288,  17,  17, 263,  17,  17, 267,  17,\n",
       "         17,  17,  17,  17, 270,  17,  17, 172,  17,  17,  17,  17,  17,\n",
       "         17,  17,  17, 287,  17,  17, 183,  17,  17, 187,  17,  17, 268,\n",
       "         17,  17, 276,  17,  17, 271,  17,  17, 274,  17,  17, 269,  17,\n",
       "         17, 280,  17,  17, 273,  17,  17, 281,  17,  17, 277,  17,  17,\n",
       "        286,  17,  17, 284,  17,  17, 275,  17,  17, 279,  17,  17, 272,\n",
       "         17,  17, 283,  17,  17, 278,  17,  17, 282,  17,  17,  17,  17,\n",
       "         17,  17,  17,  17,  17,  17,  17,  17,  17,  17,  17,  17,  17,\n",
       "         17,  17,  17,  17,  17,  17,  17,  17,  17,  17,  17,  17,  17,\n",
       "         17,  17,  17,  17,  17,  17,  17,  17,  17,  17,  17,  17,  17,\n",
       "         17,  17,  17,  17,  17,  17, 167,  17,  17,  17,  17,  17,   2,\n",
       "         17,   6, 172,  11,  17,  17,  17,  17,  17,  17,  17,   6, 167,\n",
       "         11,   6,   2, 172,  17,  17,  17,  17,  17,  17, 167,  17,  17,\n",
       "        171, 166,  10,  17,  17,  17,  17,  17,  17,  17,  11,  17,  17,\n",
       "         17, 167]),\n",
       " 'split0_test_f1_micro': array([0.929, 0.933, 0.931, 0.923, 0.929, 0.929, 0.909, 0.918, 0.908,\n",
       "        0.902, 0.907, 0.902, 0.928, 0.931, 0.933, 0.929, 0.921, 0.926,\n",
       "        0.921, 0.913, 0.92 , 0.904, 0.911, 0.915, 0.932, 0.933, 0.934,\n",
       "        0.928, 0.931, 0.932, 0.918, 0.922, 0.919, 0.911, 0.902, 0.906,\n",
       "        0.933, 0.935, 0.933, 0.928, 0.937, 0.93 , 0.929, 0.918, 0.922,\n",
       "        0.922, 0.925, 0.922, 0.933, 0.936, 0.934, 0.924, 0.922, 0.925,\n",
       "        0.914, 0.912, 0.919, 0.899, 0.897, 0.89 , 0.933, 0.931, 0.93 ,\n",
       "        0.923, 0.923, 0.928, 0.925, 0.919, 0.921, 0.91 , 0.908, 0.902,\n",
       "        0.932, 0.932, 0.933, 0.932, 0.919, 0.923, 0.909, 0.92 , 0.917,\n",
       "        0.904, 0.897, 0.912, 0.932, 0.934, 0.935, 0.93 , 0.929, 0.932,\n",
       "        0.92 , 0.916, 0.92 , 0.899, 0.913, 0.913, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.26 , 0.933, 0.933, 0.909, 0.933, 0.933, 0.775, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.405, 0.933,\n",
       "        0.933, 0.927, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.232, 0.933, 0.933, 0.816, 0.933, 0.933, 0.619, 0.933,\n",
       "        0.933, 0.901, 0.933, 0.933, 0.576, 0.933, 0.933, 0.748, 0.933,\n",
       "        0.933, 0.573, 0.933, 0.933, 0.492, 0.933, 0.933, 0.379, 0.933,\n",
       "        0.933, 0.381, 0.933, 0.933, 0.557, 0.933, 0.933, 0.699, 0.933,\n",
       "        0.933, 0.572, 0.933, 0.933, 0.518, 0.933, 0.933, 0.47 , 0.933,\n",
       "        0.933, 0.618, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.932, 0.933, 0.934,\n",
       "        0.933, 0.934, 0.934, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.934, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933]),\n",
       " 'split1_test_f1_micro': array([0.934, 0.933, 0.932, 0.924, 0.928, 0.931, 0.922, 0.917, 0.921,\n",
       "        0.91 , 0.915, 0.905, 0.933, 0.934, 0.933, 0.929, 0.926, 0.928,\n",
       "        0.919, 0.914, 0.913, 0.906, 0.898, 0.902, 0.934, 0.933, 0.933,\n",
       "        0.929, 0.924, 0.931, 0.914, 0.92 , 0.922, 0.917, 0.909, 0.901,\n",
       "        0.933, 0.934, 0.934, 0.929, 0.933, 0.93 , 0.926, 0.925, 0.932,\n",
       "        0.914, 0.925, 0.912, 0.932, 0.933, 0.935, 0.919, 0.927, 0.925,\n",
       "        0.916, 0.915, 0.913, 0.897, 0.902, 0.901, 0.93 , 0.936, 0.935,\n",
       "        0.924, 0.928, 0.928, 0.91 , 0.92 , 0.911, 0.912, 0.899, 0.886,\n",
       "        0.93 , 0.933, 0.932, 0.924, 0.923, 0.929, 0.922, 0.916, 0.914,\n",
       "        0.915, 0.91 , 0.912, 0.934, 0.933, 0.933, 0.927, 0.926, 0.927,\n",
       "        0.916, 0.914, 0.924, 0.915, 0.911, 0.896, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.109, 0.933, 0.933, 0.933, 0.933, 0.933, 0.907, 0.933,\n",
       "        0.933, 0.068, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.067, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.926, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.602, 0.933, 0.933, 0.768, 0.933, 0.933, 0.516, 0.933,\n",
       "        0.933, 0.59 , 0.933, 0.933, 0.621, 0.933, 0.933, 0.62 , 0.933,\n",
       "        0.933, 0.717, 0.933, 0.933, 0.569, 0.933, 0.933, 0.847, 0.933,\n",
       "        0.933, 0.729, 0.933, 0.933, 0.629, 0.933, 0.933, 0.719, 0.933,\n",
       "        0.933, 0.561, 0.933, 0.933, 0.731, 0.933, 0.933, 0.547, 0.933,\n",
       "        0.933, 0.806, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.934, 0.933, 0.933, 0.933, 0.934, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.934, 0.932, 0.933,\n",
       "        0.934, 0.934, 0.932, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.932, 0.933, 0.932, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.934, 0.933, 0.933, 0.933, 0.933]),\n",
       " 'split2_test_f1_micro': array([0.93 , 0.93 , 0.934, 0.924, 0.924, 0.925, 0.917, 0.913, 0.913,\n",
       "        0.912, 0.911, 0.916, 0.933, 0.931, 0.933, 0.93 , 0.923, 0.927,\n",
       "        0.924, 0.921, 0.913, 0.895, 0.896, 0.914, 0.936, 0.93 , 0.933,\n",
       "        0.932, 0.926, 0.929, 0.922, 0.926, 0.911, 0.926, 0.906, 0.925,\n",
       "        0.933, 0.932, 0.933, 0.931, 0.93 , 0.932, 0.922, 0.92 , 0.925,\n",
       "        0.915, 0.91 , 0.92 , 0.933, 0.936, 0.932, 0.926, 0.92 , 0.926,\n",
       "        0.912, 0.916, 0.91 , 0.895, 0.907, 0.883, 0.931, 0.933, 0.933,\n",
       "        0.927, 0.931, 0.925, 0.909, 0.912, 0.918, 0.878, 0.897, 0.891,\n",
       "        0.932, 0.933, 0.933, 0.93 , 0.926, 0.929, 0.914, 0.923, 0.909,\n",
       "        0.913, 0.897, 0.894, 0.934, 0.933, 0.934, 0.926, 0.929, 0.924,\n",
       "        0.916, 0.924, 0.914, 0.903, 0.899, 0.912, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.524, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.069, 0.933, 0.933, 0.778, 0.933, 0.933, 0.874, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.544, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.509, 0.933,\n",
       "        0.933, 0.862, 0.933, 0.933, 0.344, 0.933, 0.933, 0.612, 0.933,\n",
       "        0.933, 0.634, 0.933, 0.933, 0.312, 0.933, 0.933, 0.25 , 0.933,\n",
       "        0.933, 0.556, 0.933, 0.933, 0.729, 0.933, 0.933, 0.609, 0.933,\n",
       "        0.933, 0.373, 0.933, 0.933, 0.438, 0.933, 0.933, 0.632, 0.933,\n",
       "        0.933, 0.913, 0.933, 0.933, 0.794, 0.933, 0.933, 0.875, 0.933,\n",
       "        0.933, 0.332, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.934, 0.933, 0.934, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.935, 0.933, 0.933,\n",
       "        0.934, 0.935, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.934, 0.933, 0.935, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933]),\n",
       " 'split3_test_f1_micro': array([0.928, 0.924, 0.933, 0.925, 0.924, 0.926, 0.923, 0.914, 0.912,\n",
       "        0.89 , 0.904, 0.914, 0.93 , 0.931, 0.933, 0.922, 0.929, 0.929,\n",
       "        0.915, 0.915, 0.915, 0.908, 0.899, 0.901, 0.933, 0.933, 0.933,\n",
       "        0.926, 0.931, 0.928, 0.918, 0.924, 0.921, 0.912, 0.909, 0.919,\n",
       "        0.93 , 0.932, 0.93 , 0.929, 0.932, 0.931, 0.919, 0.922, 0.927,\n",
       "        0.919, 0.92 , 0.921, 0.93 , 0.934, 0.931, 0.921, 0.927, 0.924,\n",
       "        0.911, 0.912, 0.927, 0.894, 0.907, 0.907, 0.933, 0.935, 0.933,\n",
       "        0.924, 0.923, 0.919, 0.91 , 0.905, 0.912, 0.9  , 0.893, 0.895,\n",
       "        0.93 , 0.932, 0.929, 0.92 , 0.931, 0.917, 0.919, 0.923, 0.906,\n",
       "        0.909, 0.905, 0.909, 0.933, 0.934, 0.934, 0.928, 0.927, 0.926,\n",
       "        0.922, 0.925, 0.919, 0.911, 0.899, 0.91 , 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.864, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.067, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.592, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.537, 0.933, 0.933, 0.579, 0.933, 0.933, 0.775, 0.933,\n",
       "        0.933, 0.712, 0.933, 0.933, 0.605, 0.933, 0.933, 0.761, 0.933,\n",
       "        0.933, 0.617, 0.933, 0.933, 0.675, 0.933, 0.933, 0.45 , 0.933,\n",
       "        0.933, 0.492, 0.933, 0.933, 0.651, 0.933, 0.933, 0.413, 0.933,\n",
       "        0.933, 0.692, 0.933, 0.933, 0.411, 0.933, 0.933, 0.516, 0.933,\n",
       "        0.933, 0.462, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.932, 0.933, 0.933, 0.931, 0.933, 0.933, 0.933, 0.933, 0.933,\n",
       "        0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933, 0.933]),\n",
       " 'split4_test_f1_micro': array([0.932, 0.927, 0.931, 0.919, 0.926, 0.923, 0.909, 0.915, 0.91 ,\n",
       "        0.901, 0.896, 0.899, 0.932, 0.931, 0.932, 0.926, 0.924, 0.93 ,\n",
       "        0.924, 0.913, 0.923, 0.918, 0.914, 0.904, 0.932, 0.932, 0.932,\n",
       "        0.923, 0.93 , 0.928, 0.926, 0.923, 0.914, 0.899, 0.92 , 0.899,\n",
       "        0.932, 0.932, 0.928, 0.928, 0.927, 0.931, 0.921, 0.923, 0.932,\n",
       "        0.897, 0.924, 0.921, 0.929, 0.931, 0.927, 0.918, 0.924, 0.921,\n",
       "        0.911, 0.916, 0.925, 0.895, 0.906, 0.89 , 0.934, 0.932, 0.931,\n",
       "        0.919, 0.92 , 0.927, 0.914, 0.907, 0.92 , 0.893, 0.9  , 0.892,\n",
       "        0.929, 0.929, 0.931, 0.92 , 0.924, 0.926, 0.91 , 0.92 , 0.92 ,\n",
       "        0.914, 0.909, 0.911, 0.931, 0.931, 0.93 , 0.926, 0.929, 0.929,\n",
       "        0.918, 0.924, 0.917, 0.915, 0.92 , 0.915, 0.932, 0.932, 0.932,\n",
       "        0.932, 0.163, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932,\n",
       "        0.932, 0.094, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932,\n",
       "        0.932, 0.932, 0.932, 0.932, 0.497, 0.932, 0.932, 0.93 , 0.932,\n",
       "        0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932,\n",
       "        0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932,\n",
       "        0.932, 0.859, 0.932, 0.932, 0.843, 0.932, 0.932, 0.659, 0.932,\n",
       "        0.932, 0.59 , 0.932, 0.932, 0.796, 0.932, 0.932, 0.884, 0.932,\n",
       "        0.932, 0.429, 0.932, 0.932, 0.625, 0.932, 0.932, 0.341, 0.932,\n",
       "        0.932, 0.782, 0.932, 0.932, 0.837, 0.932, 0.932, 0.573, 0.932,\n",
       "        0.932, 0.574, 0.932, 0.932, 0.391, 0.932, 0.932, 0.656, 0.932,\n",
       "        0.932, 0.641, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932,\n",
       "        0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932,\n",
       "        0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932,\n",
       "        0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932,\n",
       "        0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932,\n",
       "        0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.931, 0.932, 0.932,\n",
       "        0.932, 0.932, 0.932, 0.934, 0.932, 0.933, 0.93 , 0.932, 0.932,\n",
       "        0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932,\n",
       "        0.932, 0.932, 0.93 , 0.932, 0.932, 0.932, 0.932, 0.932, 0.932,\n",
       "        0.932, 0.932, 0.932, 0.932, 0.931, 0.932, 0.932, 0.932, 0.932,\n",
       "        0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.932, 0.931]),\n",
       " 'mean_test_f1_micro': array([0.9306, 0.9294, 0.9322, 0.923 , 0.9262, 0.9268, 0.916 , 0.9154,\n",
       "        0.9128, 0.903 , 0.9066, 0.9072, 0.9312, 0.9316, 0.9328, 0.9272,\n",
       "        0.9246, 0.928 , 0.9206, 0.9152, 0.9168, 0.9062, 0.9036, 0.9072,\n",
       "        0.9334, 0.9322, 0.933 , 0.9276, 0.9284, 0.9296, 0.9196, 0.923 ,\n",
       "        0.9174, 0.913 , 0.9092, 0.91  , 0.9322, 0.933 , 0.9316, 0.929 ,\n",
       "        0.9318, 0.9308, 0.9234, 0.9216, 0.9276, 0.9134, 0.9208, 0.9192,\n",
       "        0.9314, 0.934 , 0.9318, 0.9216, 0.924 , 0.9242, 0.9128, 0.9142,\n",
       "        0.9188, 0.896 , 0.9038, 0.8942, 0.9322, 0.9334, 0.9324, 0.9234,\n",
       "        0.925 , 0.9254, 0.9136, 0.9126, 0.9164, 0.8986, 0.8994, 0.8932,\n",
       "        0.9306, 0.9318, 0.9316, 0.9252, 0.9246, 0.9248, 0.9148, 0.9204,\n",
       "        0.9132, 0.911 , 0.9036, 0.9076, 0.9328, 0.933 , 0.9332, 0.9274,\n",
       "        0.928 , 0.9276, 0.9184, 0.9206, 0.9188, 0.9086, 0.9084, 0.9092,\n",
       "        0.9328, 0.9328, 0.9328, 0.9328, 0.5324, 0.9328, 0.9328, 0.919 ,\n",
       "        0.9328, 0.9328, 0.9276, 0.9328, 0.9328, 0.2848, 0.9328, 0.9328,\n",
       "        0.897 , 0.9328, 0.9328, 0.8894, 0.9328, 0.9328, 0.9328, 0.9328,\n",
       "        0.9328, 0.6726, 0.9328, 0.9328, 0.9324, 0.9328, 0.9328, 0.9328,\n",
       "        0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.508 , 0.9328, 0.9328,\n",
       "        0.9316, 0.9328, 0.9328, 0.9314, 0.9328, 0.9328, 0.848 , 0.9328,\n",
       "        0.9328, 0.6184, 0.9328, 0.9328, 0.67  , 0.9328, 0.9328, 0.6362,\n",
       "        0.9328, 0.9328, 0.6854, 0.9328, 0.9328, 0.582 , 0.9328, 0.9328,\n",
       "        0.6526, 0.9328, 0.9328, 0.5784, 0.9328, 0.9328, 0.618 , 0.9328,\n",
       "        0.9328, 0.5252, 0.9328, 0.9328, 0.5514, 0.9328, 0.9328, 0.6224,\n",
       "        0.9328, 0.9328, 0.6072, 0.9328, 0.9328, 0.6624, 0.9328, 0.9328,\n",
       "        0.569 , 0.9328, 0.9328, 0.6128, 0.9328, 0.9328, 0.5718, 0.9328,\n",
       "        0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.9328,\n",
       "        0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.9328,\n",
       "        0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.9328,\n",
       "        0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.9328,\n",
       "        0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.9328,\n",
       "        0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.9328,\n",
       "        0.9326, 0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.9336, 0.9328,\n",
       "        0.9332, 0.9324, 0.933 , 0.9328, 0.9328, 0.9328, 0.9328, 0.9328,\n",
       "        0.9328, 0.9328, 0.9332, 0.9326, 0.933 , 0.9332, 0.9336, 0.9324,\n",
       "        0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.9328, 0.9326, 0.9328,\n",
       "        0.9328, 0.9324, 0.9328, 0.933 , 0.9328, 0.9328, 0.9328, 0.9328,\n",
       "        0.9328, 0.9328, 0.9328, 0.933 , 0.9328, 0.9328, 0.9328, 0.9326]),\n",
       " 'std_test_f1_micro': array([0.00215407, 0.00349857, 0.00116619, 0.00209762, 0.00203961,\n",
       "        0.00285657, 0.0060663 , 0.00185472, 0.00444522, 0.00779744,\n",
       "        0.00646838, 0.00667533, 0.00193907, 0.0012    , 0.0004    ,\n",
       "        0.00292575, 0.00272764, 0.00141421, 0.00338231, 0.00299333,\n",
       "        0.00401995, 0.00738647, 0.00739189, 0.00604649, 0.00149666,\n",
       "        0.00116619, 0.00063246, 0.00300666, 0.00287054, 0.00162481,\n",
       "        0.00407922, 0.002     , 0.00422374, 0.00878635, 0.00597997,\n",
       "        0.01023719, 0.00116619, 0.00126491, 0.00224499, 0.00109545,\n",
       "        0.00331059, 0.00074833, 0.00361109, 0.00241661, 0.00392938,\n",
       "        0.00868562, 0.00570614, 0.00365513, 0.00162481, 0.00189737,\n",
       "        0.00278568, 0.00300666, 0.00275681, 0.00172047, 0.00193907,\n",
       "        0.00183303, 0.00658483, 0.00178885, 0.00386782, 0.00861162,\n",
       "        0.00146969, 0.00185472, 0.00174356, 0.00257682, 0.00394968,\n",
       "        0.00338231, 0.00595315, 0.00608605, 0.00412795, 0.01238709,\n",
       "        0.00492341, 0.00526878, 0.0012    , 0.00146969, 0.00149666,\n",
       "        0.004996  , 0.00392938, 0.00448999, 0.00503587, 0.00257682,\n",
       "        0.00511468, 0.00404969, 0.00564269, 0.00688767, 0.00116619,\n",
       "        0.00109545, 0.00172047, 0.00149666, 0.00126491, 0.00272764,\n",
       "        0.00233238, 0.00463033, 0.00331059, 0.00649923, 0.0082365 ,\n",
       "        0.00679412, 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.35686277, 0.0004    , 0.0004    , 0.02750273, 0.0004    ,\n",
       "        0.0004    , 0.01030728, 0.0004    , 0.0004    , 0.33188998,\n",
       "        0.0004    , 0.0004    , 0.06020299, 0.0004    , 0.0004    ,\n",
       "        0.06154868, 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.34670195, 0.0004    , 0.0004    , 0.0012    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.28043466, 0.0004    ,\n",
       "        0.0004    , 0.00233238, 0.0004    , 0.0004    , 0.00272764,\n",
       "        0.0004    , 0.0004    , 0.16950044, 0.0004    , 0.0004    ,\n",
       "        0.23385004, 0.0004    , 0.0004    , 0.18729976, 0.0004    ,\n",
       "        0.0004    , 0.0838031 , 0.0004    , 0.0004    , 0.11664922,\n",
       "        0.0004    , 0.0004    , 0.15542329, 0.0004    , 0.0004    ,\n",
       "        0.2179666 , 0.0004    , 0.0004    , 0.09332867, 0.0004    ,\n",
       "        0.0004    , 0.0823359 , 0.0004    , 0.0004    , 0.18522246,\n",
       "        0.0004    , 0.0004    , 0.17269001, 0.0004    , 0.0004    ,\n",
       "        0.13053367, 0.0004    , 0.0004    , 0.10993162, 0.0004    ,\n",
       "        0.0004    , 0.13412323, 0.0004    , 0.0004    , 0.16499576,\n",
       "        0.0004    , 0.0004    , 0.1447113 , 0.0004    , 0.0004    ,\n",
       "        0.16206961, 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0008    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004899 , 0.0004    , 0.0004    , 0.0012    ,\n",
       "        0.00063246, 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.00116619, 0.0004899 ,\n",
       "        0.00063246, 0.00074833, 0.0010198 , 0.00135647, 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004899 , 0.0004    , 0.0004    , 0.0010198 , 0.0009798 ,\n",
       "        0.00109545, 0.0004    , 0.0004    , 0.0004    , 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0004    , 0.00063246, 0.0004    ,\n",
       "        0.0004    , 0.0004    , 0.0008    ]),\n",
       " 'rank_test_f1_micro': array([191, 194, 176, 217, 206, 205, 234, 235, 243, 260, 255, 254, 189,\n",
       "        183,  17, 204, 211, 197, 223, 236, 232, 256, 258, 253,   4, 176,\n",
       "         11, 200, 196, 193, 225, 217, 231, 242, 248, 247, 176,  11, 183,\n",
       "        195, 180, 190, 215, 219, 200, 240, 221, 226, 187,   1, 180, 220,\n",
       "        214, 213, 243, 238, 228, 264, 257, 265, 176,   5, 172, 215, 209,\n",
       "        207, 239, 245, 233, 262, 261, 266, 191, 182, 183, 208, 211, 210,\n",
       "        237, 224, 241, 246, 258, 252,  17,  11,   6, 203, 197, 200, 230,\n",
       "        222, 228, 250, 251, 248,  17,  17,  17,  17, 285,  17,  17, 227,\n",
       "         17,  17, 199,  17,  17, 288,  17,  17, 263,  17,  17, 267,  17,\n",
       "         17,  17,  17,  17, 270,  17,  17, 172,  17,  17,  17,  17,  17,\n",
       "         17,  17,  17, 287,  17,  17, 183,  17,  17, 187,  17,  17, 268,\n",
       "         17,  17, 276,  17,  17, 271,  17,  17, 274,  17,  17, 269,  17,\n",
       "         17, 280,  17,  17, 273,  17,  17, 281,  17,  17, 277,  17,  17,\n",
       "        286,  17,  17, 284,  17,  17, 275,  17,  17, 279,  17,  17, 272,\n",
       "         17,  17, 283,  17,  17, 278,  17,  17, 282,  17,  17,  17,  17,\n",
       "         17,  17,  17,  17,  17,  17,  17,  17,  17,  17,  17,  17,  17,\n",
       "         17,  17,  17,  17,  17,  17,  17,  17,  17,  17,  17,  17,  17,\n",
       "         17,  17,  17,  17,  17,  17,  17,  17,  17,  17,  17,  17,  17,\n",
       "         17,  17,  17,  17,  17,  17, 167,  17,  17,  17,  17,  17,   2,\n",
       "         17,   6, 172,  11,  17,  17,  17,  17,  17,  17,  17,   6, 167,\n",
       "         11,   6,   2, 172,  17,  17,  17,  17,  17,  17, 167,  17,  17,\n",
       "        171, 166,  10,  17,  17,  17,  17,  17,  17,  17,  11,  17,  17,\n",
       "         17, 167]),\n",
       " 'split0_test_roc_auc_ovo': array([0.52807506, 0.6295692 , 0.57684248, 0.57338708, 0.64837389,\n",
       "        0.64787797, 0.63272864, 0.60499752, 0.53666555, 0.562493  ,\n",
       "        0.62317032, 0.5698837 , 0.59538321, 0.54041689, 0.51566924,\n",
       "        0.59048807, 0.57127546, 0.57149942, 0.61841916, 0.5698997 ,\n",
       "        0.5534066 , 0.58103374, 0.62801747, 0.61518773, 0.57913007,\n",
       "        0.56983571, 0.55471837, 0.62181056, 0.60925277, 0.6232823 ,\n",
       "        0.57567468, 0.67901649, 0.58488906, 0.64270288, 0.62165059,\n",
       "        0.60237398, 0.59256771, 0.60083825, 0.61390795, 0.58488906,\n",
       "        0.63721585, 0.59672698, 0.60990866, 0.62652973, 0.61981091,\n",
       "        0.57871415, 0.60706116, 0.64591832, 0.59311161, 0.51678905,\n",
       "        0.58829646, 0.58490506, 0.54935931, 0.60273392, 0.59995841,\n",
       "        0.60540545, 0.60235798, 0.65022156, 0.62318632, 0.57609861,\n",
       "        0.61776327, 0.59109597, 0.58568892, 0.62064277, 0.56518053,\n",
       "        0.56612436, 0.56050935, 0.60381373, 0.58143367, 0.59821471,\n",
       "        0.63755179, 0.57252324, 0.56178913, 0.61520372, 0.5653245 ,\n",
       "        0.56738814, 0.54585593, 0.61443586, 0.6011422 , 0.56527651,\n",
       "        0.613636  , 0.55468638, 0.52075635, 0.57362704, 0.61346003,\n",
       "        0.62518597, 0.60149414, 0.63147286, 0.59141591, 0.59343156,\n",
       "        0.64118315, 0.60446961, 0.63008111, 0.61473981, 0.60286989,\n",
       "        0.58946425, 0.57396298, 0.55547024, 0.53256227, 0.44817712,\n",
       "        0.51461343, 0.44324999, 0.47182096, 0.5710195 , 0.4443378 ,\n",
       "        0.56550047, 0.53947305, 0.41544688, 0.50359137, 0.48681032,\n",
       "        0.50242357, 0.56881189, 0.58783254, 0.50443922, 0.55903761,\n",
       "        0.57287517, 0.47230088, 0.49972005, 0.56806002, 0.47220489,\n",
       "        0.56970773, 0.48381885, 0.47633217, 0.48871399, 0.45924717,\n",
       "        0.50608693, 0.534066  , 0.53761738, 0.5431684 , 0.55195086,\n",
       "        0.39061925, 0.52377981, 0.46650989, 0.51158996, 0.50981427,\n",
       "        0.48543456, 0.5357937 , 0.56426869, 0.52488362, 0.60922078,\n",
       "        0.55091104, 0.43923469, 0.46057494, 0.48996177, 0.43323575,\n",
       "        0.47314873, 0.44388988, 0.52200413, 0.46316648, 0.50663083,\n",
       "        0.49727248, 0.44265809, 0.50040793, 0.47617219, 0.54270448,\n",
       "        0.46746973, 0.48549855, 0.52942682, 0.48500264, 0.48095535,\n",
       "        0.47180496, 0.47850778, 0.54003295, 0.5482875 , 0.46527811,\n",
       "        0.4931772 , 0.50632689, 0.48753019, 0.48802611, 0.46825359,\n",
       "        0.48132329, 0.52022844, 0.54974325, 0.51512534, 0.5011598 ,\n",
       "        0.46502216, 0.48639439, 0.50043992, 0.46415831, 0.47098911,\n",
       "        0.46686183, 0.48650637, 0.4857705 , 0.49269729, 0.49152949,\n",
       "        0.50059989, 0.51746093, 0.50595895, 0.49674457, 0.55809378,\n",
       "        0.45486394, 0.50949433, 0.52205212, 0.51325367, 0.5198605 ,\n",
       "        0.51349363, 0.51848475, 0.51821279, 0.52067636, 0.51562125,\n",
       "        0.51712499, 0.51754091, 0.51893267, 0.52056438, 0.50997424,\n",
       "        0.51966854, 0.51357361, 0.51637312, 0.517029  , 0.51546128,\n",
       "        0.51898066, 0.51742893, 0.51834077, 0.51725296, 0.5193166 ,\n",
       "        0.51907664, 0.52146022, 0.51512534, 0.51683704, 0.52187615,\n",
       "        0.51906065, 0.51824479, 0.51771688, 0.51686903, 0.5147894 ,\n",
       "        0.51870871, 0.51718897, 0.51582921, 0.51811681, 0.51464542,\n",
       "        0.51835677, 0.51691702, 0.51770088, 0.51557326, 0.51882069,\n",
       "        0.51712499, 0.51517333, 0.51843676, 0.52024444, 0.51861272,\n",
       "        0.602294  , 0.56614036, 0.56572443, 0.62352226, 0.62529795,\n",
       "        0.59979844, 0.62067476, 0.61718738, 0.58682472, 0.60707715,\n",
       "        0.6233143 , 0.60307786, 0.60504551, 0.59896658, 0.60637328,\n",
       "        0.61104446, 0.61779527, 0.60670922, 0.59919054, 0.61673945,\n",
       "        0.59562317, 0.60725312, 0.61790725, 0.60678921, 0.59507927,\n",
       "        0.61515573, 0.5704436 , 0.59219977, 0.60306186, 0.62524996,\n",
       "        0.61835517, 0.61661148, 0.61957096, 0.6124842 , 0.61569964,\n",
       "        0.60103022, 0.60771704, 0.59634304, 0.60918878, 0.61187631,\n",
       "        0.60430964, 0.61598759, 0.62065876, 0.59264769, 0.61641951,\n",
       "        0.61653149, 0.61074051, 0.60472557]),\n",
       " 'split1_test_roc_auc_ovo': array([0.5960311 , 0.59044008, 0.55657404, 0.69179824, 0.56601238,\n",
       "        0.62452208, 0.62248244, 0.66970613, 0.65121339, 0.6448785 ,\n",
       "        0.61665947, 0.62862536, 0.62037081, 0.65231719, 0.59616707,\n",
       "        0.63210475, 0.60261394, 0.64564637, 0.63953544, 0.61070852,\n",
       "        0.65580458, 0.62523396, 0.60429364, 0.54697573, 0.64668618,\n",
       "        0.61218026, 0.63017709, 0.59368751, 0.68044024, 0.62254643,\n",
       "        0.6204828 , 0.59370351, 0.63267265, 0.60642127, 0.59378349,\n",
       "        0.6187551 , 0.69558958, 0.63281662, 0.60294988, 0.66623474,\n",
       "        0.67908048, 0.62349027, 0.57141943, 0.60130217, 0.58952824,\n",
       "        0.68258387, 0.64516645, 0.62547392, 0.62062677, 0.61997089,\n",
       "        0.65951593, 0.64187103, 0.66814641, 0.6283854 , 0.66132361,\n",
       "        0.63881557, 0.57665051, 0.62259442, 0.60661324, 0.66836237,\n",
       "        0.61459583, 0.6454384 , 0.6056854 , 0.6170274 , 0.63988738,\n",
       "        0.625002  , 0.66919422, 0.5875126 , 0.63870359, 0.63564813,\n",
       "        0.63967942, 0.6068212 , 0.66927421, 0.60774104, 0.60192606,\n",
       "        0.56908384, 0.66937019, 0.65175729, 0.63518421, 0.64662219,\n",
       "        0.64719809, 0.60528547, 0.55305466, 0.64340676, 0.63596807,\n",
       "        0.64505447, 0.64211099, 0.55742189, 0.5800899 , 0.65618851,\n",
       "        0.59501528, 0.61696341, 0.63721585, 0.5954472 , 0.63608005,\n",
       "        0.63803171, 0.54793556, 0.53542577, 0.43641919, 0.45139255,\n",
       "        0.41399114, 0.44043448, 0.55719793, 0.44651341, 0.52259602,\n",
       "        0.46003103, 0.52573147, 0.53400202, 0.46039897, 0.47063717,\n",
       "        0.44219417, 0.49421702, 0.48074739, 0.53342612, 0.44960087,\n",
       "        0.5141975 , 0.52547552, 0.44283406, 0.57253923, 0.46073491,\n",
       "        0.43793892, 0.49061765, 0.51047016, 0.47860377, 0.50021596,\n",
       "        0.50883844, 0.49223337, 0.58972021, 0.53833725, 0.49468094,\n",
       "        0.45334421, 0.56438067, 0.4755803 , 0.52163619, 0.48017149,\n",
       "        0.52713922, 0.46086289, 0.54057686, 0.41751052, 0.53051463,\n",
       "        0.58269745, 0.48853802, 0.57663451, 0.4556958 , 0.57921006,\n",
       "        0.43320376, 0.53358609, 0.61122043, 0.52059638, 0.568204  ,\n",
       "        0.60261394, 0.53563373, 0.67048999, 0.53209835, 0.48746621,\n",
       "        0.6261298 , 0.58909632, 0.48652237, 0.60563741, 0.62200253,\n",
       "        0.5232839 , 0.59384748, 0.56106925, 0.5443362 , 0.54342436,\n",
       "        0.58221753, 0.45705556, 0.57493881, 0.48370687, 0.52909088,\n",
       "        0.57836221, 0.61411592, 0.53766537, 0.58852042, 0.66644271,\n",
       "        0.49642463, 0.61565165, 0.54678377, 0.54572795, 0.61349203,\n",
       "        0.54775959, 0.50303147, 0.5289629 , 0.60278991, 0.56868391,\n",
       "        0.59074403, 0.58120971, 0.4420342 , 0.57826623, 0.59767081,\n",
       "        0.5022636 , 0.58823247, 0.58680872, 0.58301739, 0.5926317 ,\n",
       "        0.59155989, 0.59155989, 0.58776855, 0.58983219, 0.59671098,\n",
       "        0.59104798, 0.58744861, 0.59328758, 0.59277567, 0.59008814,\n",
       "        0.58776855, 0.58877638, 0.58344931, 0.58909632, 0.59183184,\n",
       "        0.5925997 , 0.59330358, 0.59055206, 0.59042409, 0.58988018,\n",
       "        0.59095199, 0.59111196, 0.59882261, 0.58824847, 0.58999216,\n",
       "        0.59183184, 0.58773656, 0.59020012, 0.59154389, 0.59333557,\n",
       "        0.59152789, 0.59063205, 0.5920238 , 0.5954632 , 0.58840844,\n",
       "        0.58776855, 0.58935227, 0.59562317, 0.58935227, 0.59063205,\n",
       "        0.59186383, 0.58988018, 0.59103198, 0.59111196, 0.59060005,\n",
       "        0.68322375, 0.66386716, 0.68655117, 0.66289133, 0.64126314,\n",
       "        0.66613876, 0.65084545, 0.67568908, 0.65190127, 0.64339076,\n",
       "        0.66463502, 0.6971413 , 0.67914447, 0.69151029, 0.65175729,\n",
       "        0.65814017, 0.66682664, 0.66341924, 0.64086321, 0.64703812,\n",
       "        0.64455856, 0.66295532, 0.66329126, 0.64137512, 0.68335173,\n",
       "        0.6693222 , 0.67060197, 0.6755611 , 0.67025004, 0.66757851,\n",
       "        0.67364144, 0.66537089, 0.67263362, 0.6766969 , 0.65346899,\n",
       "        0.64697413, 0.63838364, 0.68247188, 0.56236502, 0.67791269,\n",
       "        0.66967414, 0.67975236, 0.6801043 , 0.65887604, 0.66065173,\n",
       "        0.67551311, 0.67674489, 0.681832  ]),\n",
       " 'split2_test_roc_auc_ovo': array([0.53900913, 0.53101854, 0.58901633, 0.57511478, 0.55271072,\n",
       "        0.52588344, 0.54268049, 0.60205404, 0.60712515, 0.5926317 ,\n",
       "        0.56207707, 0.58095375, 0.57817824, 0.57713043, 0.52667531,\n",
       "        0.60726912, 0.61245221, 0.58660076, 0.61299611, 0.67255363,\n",
       "        0.61325207, 0.61787525, 0.64700613, 0.61608357, 0.56303691,\n",
       "        0.56927581, 0.56498856, 0.58103374, 0.63072099, 0.59008814,\n",
       "        0.61805122, 0.61488378, 0.61314009, 0.59472733, 0.5698677 ,\n",
       "        0.63494425, 0.57546672, 0.60278991, 0.58717666, 0.5977348 ,\n",
       "        0.59224776, 0.60491753, 0.58167363, 0.5562221 , 0.6073971 ,\n",
       "        0.63766377, 0.59459935, 0.61437187, 0.53678553, 0.5641887 ,\n",
       "        0.6011342 , 0.59471133, 0.60390171, 0.59095199, 0.55702996,\n",
       "        0.55063109, 0.58032986, 0.58455312, 0.5829374 , 0.59640703,\n",
       "        0.58556094, 0.61455584, 0.56457264, 0.60162211, 0.56897986,\n",
       "        0.58810449, 0.5721713 , 0.57948201, 0.56052535, 0.55046312,\n",
       "        0.56740414, 0.56909984, 0.52748316, 0.58644079, 0.57821823,\n",
       "        0.60973269, 0.54723169, 0.55393451, 0.54408024, 0.51698101,\n",
       "        0.53540977, 0.56482859, 0.57649054, 0.59403945, 0.59327158,\n",
       "        0.56090928, 0.5778183 , 0.56969173, 0.57169138, 0.58320936,\n",
       "        0.58437715, 0.58597687, 0.56682824, 0.55898962, 0.58903233,\n",
       "        0.59816672, 0.45703956, 0.53109053, 0.55828574, 0.49962407,\n",
       "        0.48588248, 0.47906768, 0.44380989, 0.49704852, 0.46179072,\n",
       "        0.48725824, 0.54382429, 0.4795476 , 0.46001504, 0.51448545,\n",
       "        0.52361984, 0.55303867, 0.54360033, 0.51776487, 0.45788741,\n",
       "        0.54913535, 0.4454416 , 0.56116523, 0.56161316, 0.48028347,\n",
       "        0.54582393, 0.49048967, 0.50608693, 0.42922046, 0.40751228,\n",
       "        0.46027099, 0.47023724, 0.49253731, 0.46985331, 0.44862504,\n",
       "        0.4550879 , 0.42422934, 0.45297628, 0.50599095, 0.48836205,\n",
       "        0.51859673, 0.49575275, 0.49698453, 0.45542385, 0.46679784,\n",
       "        0.58231351, 0.44860904, 0.50261554, 0.48172322, 0.46379037,\n",
       "        0.44606549, 0.5533906 , 0.56700421, 0.51626114, 0.517061  ,\n",
       "        0.54844747, 0.5096383 , 0.49336917, 0.48050743, 0.47030123,\n",
       "        0.55108701, 0.48277903, 0.42478924, 0.54636784, 0.50351138,\n",
       "        0.54745565, 0.51206988, 0.5176369 , 0.54521604, 0.49480891,\n",
       "        0.47927565, 0.58916031, 0.51997248, 0.43176401, 0.53611364,\n",
       "        0.5596775 , 0.50975028, 0.43688311, 0.47034922, 0.51405353,\n",
       "        0.57313113, 0.49655261, 0.51382957, 0.49047368, 0.53513782,\n",
       "        0.51384556, 0.40866408, 0.51298172, 0.5312345 , 0.50698277,\n",
       "        0.55710195, 0.5613732 , 0.57649054, 0.52808306, 0.53705748,\n",
       "        0.58479308, 0.55799779, 0.55404649, 0.55353458, 0.5551023 ,\n",
       "        0.55276671, 0.54745565, 0.54836749, 0.55083105, 0.54777559,\n",
       "        0.5516789 , 0.54868743, 0.54785558, 0.5471517 , 0.56182112,\n",
       "        0.55647806, 0.55409448, 0.55118299, 0.55043112, 0.55036714,\n",
       "        0.5494073 , 0.54702372, 0.54683176, 0.54846347, 0.54823951,\n",
       "        0.54593592, 0.55457439, 0.55750188, 0.55801379, 0.54694374,\n",
       "        0.5522708 , 0.54623986, 0.54804754, 0.55015917, 0.54903937,\n",
       "        0.5460479 , 0.54425621, 0.54897538, 0.56106925, 0.55937355,\n",
       "        0.55031914, 0.54955128, 0.55315065, 0.55142295, 0.54510406,\n",
       "        0.54867143, 0.54607989, 0.54783958, 0.54636784, 0.54793556,\n",
       "        0.55838172, 0.59352754, 0.58583289, 0.60501352, 0.60363776,\n",
       "        0.55675001, 0.58884036, 0.57997792, 0.58476108, 0.59347955,\n",
       "        0.58356129, 0.57295516, 0.60064629, 0.5789861 , 0.59435939,\n",
       "        0.59253571, 0.58378525, 0.59965446, 0.5801059 , 0.58204156,\n",
       "        0.58632881, 0.59045608, 0.60242197, 0.59068004, 0.58823247,\n",
       "        0.60230999, 0.58792852, 0.58160964, 0.57204332, 0.58452112,\n",
       "        0.59269569, 0.56756411, 0.58658476, 0.59690295, 0.58431316,\n",
       "        0.58786454, 0.58973621, 0.58956024, 0.58455312, 0.607941  ,\n",
       "        0.60714114, 0.59072803, 0.58421718, 0.57495481, 0.57473085,\n",
       "        0.58876038, 0.59037609, 0.58426517]),\n",
       " 'split3_test_roc_auc_ovo': array([0.5528467 , 0.56258898, 0.58723265, 0.62549791, 0.6011262 ,\n",
       "        0.53185039, 0.58242549, 0.60730112, 0.65125338, 0.5784262 ,\n",
       "        0.58786454, 0.61651549, 0.53742541, 0.55381453, 0.58343332,\n",
       "        0.63131289, 0.6039577 , 0.59131993, 0.61013262, 0.61736334,\n",
       "        0.5659324 , 0.63489626, 0.56466862, 0.56898786, 0.5545424 ,\n",
       "        0.58157764, 0.57799427, 0.55255875, 0.62128265, 0.61993889,\n",
       "        0.61880309, 0.59531922, 0.56636432, 0.58949625, 0.62539393,\n",
       "        0.57764233, 0.61595559, 0.66159556, 0.58450513, 0.59834269,\n",
       "        0.60574939, 0.61606757, 0.55588616, 0.56671626, 0.64223897,\n",
       "        0.59293564, 0.61493177, 0.59579914, 0.60524548, 0.64844587,\n",
       "        0.57356305, 0.62739358, 0.56751612, 0.56106925, 0.55906161,\n",
       "        0.59391147, 0.59595911, 0.56556446, 0.63826367, 0.61275615,\n",
       "        0.57090752, 0.56710819, 0.58682472, 0.62289837, 0.64852586,\n",
       "        0.57964198, 0.60430964, 0.59967046, 0.63433636, 0.51670906,\n",
       "        0.59869463, 0.61296412, 0.60651725, 0.61272416, 0.64719809,\n",
       "        0.58112972, 0.56620435, 0.63459231, 0.53507383, 0.59359153,\n",
       "        0.62985715, 0.60270992, 0.55372654, 0.58554494, 0.49640863,\n",
       "        0.6318408 , 0.52649934, 0.54240054, 0.64231895, 0.59176785,\n",
       "        0.56530851, 0.59485531, 0.61635552, 0.58077778, 0.61920302,\n",
       "        0.6073971 , 0.51091808, 0.50053591, 0.50301547, 0.50160772,\n",
       "        0.55892563, 0.46348643, 0.43168402, 0.47913167, 0.57799427,\n",
       "        0.55220681, 0.47180496, 0.52563549, 0.50050391, 0.53005071,\n",
       "        0.47762794, 0.46969333, 0.47034922, 0.53595367, 0.4795476 ,\n",
       "        0.49893619, 0.45416007, 0.5551023 , 0.50778263, 0.52419574,\n",
       "        0.49140151, 0.5545584 , 0.54969525, 0.57093951, 0.49532082,\n",
       "        0.448849  , 0.48077938, 0.50253555, 0.50186367, 0.55526227,\n",
       "        0.55702196, 0.52579546, 0.56433268, 0.50472717, 0.58244149,\n",
       "        0.53659356, 0.50143175, 0.45406408, 0.46527811, 0.52435571,\n",
       "        0.55118299, 0.52165219, 0.52435571, 0.50352738, 0.55596615,\n",
       "        0.50771864, 0.51662907, 0.51570124, 0.50579898, 0.47732399,\n",
       "        0.47837981, 0.41459903, 0.4977204 , 0.50643887, 0.51323767,\n",
       "        0.49424901, 0.52950681, 0.48265105, 0.48186719, 0.55750188,\n",
       "        0.51184592, 0.48901793, 0.51658108, 0.53104254, 0.49965606,\n",
       "        0.49045768, 0.55004719, 0.51342964, 0.48653837, 0.46004703,\n",
       "        0.51642111, 0.49693654, 0.47073315, 0.488618  , 0.53657756,\n",
       "        0.50386332, 0.53499384, 0.52761914, 0.45609573, 0.55617411,\n",
       "        0.46316648, 0.56756411, 0.53944106, 0.54979124, 0.54590392,\n",
       "        0.49378509, 0.52229208, 0.48324295, 0.52745917, 0.50547904,\n",
       "        0.51530131, 0.52432372, 0.52250004, 0.5215882 , 0.52019645,\n",
       "        0.51549327, 0.51677305, 0.51715698, 0.51742893, 0.51664507,\n",
       "        0.51859673, 0.51838876, 0.51605317, 0.51717298, 0.51976452,\n",
       "        0.51800483, 0.52029243, 0.51747692, 0.52016445, 0.51829278,\n",
       "        0.51693302, 0.51925261, 0.51627714, 0.51653309, 0.51602118,\n",
       "        0.51744493, 0.52131625, 0.51832477, 0.51718897, 0.51416551,\n",
       "        0.51774888, 0.51664507, 0.51739694, 0.5181648 , 0.51738094,\n",
       "        0.51742893, 0.51798883, 0.5176209 , 0.51771688, 0.51941258,\n",
       "        0.51982851, 0.51992449, 0.52086833, 0.51734895, 0.51747692,\n",
       "        0.51690103, 0.51792485, 0.51886868, 0.51710899, 0.51803683,\n",
       "        0.6352162 , 0.62881733, 0.55817376, 0.64307082, 0.63444834,\n",
       "        0.62816144, 0.62032282, 0.63395242, 0.63963142, 0.62676969,\n",
       "        0.64644623, 0.63281662, 0.63971141, 0.62041881, 0.63320056,\n",
       "        0.64326279, 0.63017709, 0.62185855, 0.63422438, 0.58909632,\n",
       "        0.62801747, 0.6113644 , 0.63388844, 0.64617427, 0.51754091,\n",
       "        0.61141239, 0.62123466, 0.65761226, 0.64882981, 0.63702388,\n",
       "        0.61485179, 0.63574411, 0.63163283, 0.62513798, 0.61528371,\n",
       "        0.62654573, 0.60522148, 0.61501176, 0.6090768 , 0.62774552,\n",
       "        0.6289933 , 0.64342276, 0.63996737, 0.64745405, 0.63604806,\n",
       "        0.63177681, 0.64852586, 0.61552367]),\n",
       " 'split4_test_roc_auc_ovo': array([0.60124495, 0.55928901, 0.57955693, 0.57677196, 0.61112251,\n",
       "        0.62711437, 0.61419938, 0.60109505, 0.65587131, 0.62286986,\n",
       "        0.63299987, 0.56101679, 0.64106286, 0.57190419, 0.60173409,\n",
       "        0.53722229, 0.58356476, 0.60200234, 0.56403055, 0.59192754,\n",
       "        0.59167508, 0.6127714 , 0.61064125, 0.60041656, 0.55096567,\n",
       "        0.59811285, 0.58605781, 0.58607359, 0.61434928, 0.59214845,\n",
       "        0.61968253, 0.63038059, 0.63584007, 0.58162396, 0.64787932,\n",
       "        0.63915362, 0.64376105, 0.58485862, 0.54118278, 0.59896491,\n",
       "        0.55552575, 0.6306646 , 0.59707145, 0.66889674, 0.60994698,\n",
       "        0.62405327, 0.65658925, 0.63544559, 0.58343853, 0.64104708,\n",
       "        0.51555005, 0.61884625, 0.55266189, 0.58400656, 0.57798694,\n",
       "        0.626641  , 0.62197835, 0.65295222, 0.60440072, 0.59565135,\n",
       "        0.56683918, 0.60941839, 0.60963141, 0.61744982, 0.5714466 ,\n",
       "        0.61246371, 0.59414447, 0.63708659, 0.62329588, 0.59763948,\n",
       "        0.6280611 , 0.58302828, 0.58306772, 0.60057435, 0.58580535,\n",
       "        0.56923757, 0.54514327, 0.64320879, 0.5891978 , 0.64611209,\n",
       "        0.62523668, 0.61714214, 0.58380144, 0.57551755, 0.61392325,\n",
       "        0.58347008, 0.56101679, 0.56961626, 0.59266915, 0.582192  ,\n",
       "        0.58607359, 0.59863355, 0.58476395, 0.59905958, 0.63030169,\n",
       "        0.63235294, 0.45200076, 0.4981381 , 0.45245834, 0.53801123,\n",
       "        0.4311569 , 0.53472924, 0.54539573, 0.46617016, 0.4595746 ,\n",
       "        0.50359758, 0.47319174, 0.48045001, 0.50547526, 0.51614176,\n",
       "        0.555936  , 0.53231507, 0.54209796, 0.51737251, 0.53887907,\n",
       "        0.52761298, 0.50072583, 0.51374337, 0.48376357, 0.53010603,\n",
       "        0.55189662, 0.5144692 , 0.47559013, 0.52092275, 0.45847008,\n",
       "        0.40690482, 0.52411007, 0.51042982, 0.49502966, 0.53337225,\n",
       "        0.5581766 , 0.557577  , 0.53853194, 0.45757069, 0.51240217,\n",
       "        0.46582302, 0.50036291, 0.51109253, 0.45752335, 0.565782  ,\n",
       "        0.46803206, 0.49919528, 0.52123832, 0.48759783, 0.54239775,\n",
       "        0.50612219, 0.511487  , 0.50864681, 0.53228351, 0.45143272,\n",
       "        0.52445721, 0.54522217, 0.48024489, 0.52791277, 0.52904885,\n",
       "        0.50149899, 0.42778023, 0.53288311, 0.48305352, 0.58127682,\n",
       "        0.54626357, 0.51505302, 0.54283956, 0.50713204, 0.52901729,\n",
       "        0.55421611, 0.50670601, 0.53939977, 0.52369982, 0.4672589 ,\n",
       "        0.49275751, 0.42808003, 0.60773794, 0.54659493, 0.47172431,\n",
       "        0.56590823, 0.54531684, 0.52044938, 0.49816965, 0.5250568 ,\n",
       "        0.5499716 , 0.46844231, 0.55461058, 0.50001578, 0.47159808,\n",
       "        0.52463078, 0.53122633, 0.51598397, 0.4800082 , 0.52202727,\n",
       "        0.45758647, 0.4869509 , 0.53728541, 0.53547084, 0.54137213,\n",
       "        0.53537617, 0.53685938, 0.53515526, 0.53583375, 0.53528149,\n",
       "        0.54006248, 0.53348271, 0.53496592, 0.53335648, 0.53384562,\n",
       "        0.53294623, 0.5358022 , 0.53889485, 0.53531305, 0.53305668,\n",
       "        0.53427165, 0.53501325, 0.53611777, 0.53496592, 0.5333407 ,\n",
       "        0.53299356, 0.53927354, 0.53783767, 0.53865817, 0.53868972,\n",
       "        0.53501325, 0.53510793, 0.53395607, 0.53319869, 0.53663848,\n",
       "        0.53097387, 0.53089498, 0.5358022 , 0.54217685, 0.53767988,\n",
       "        0.53842148, 0.53644913, 0.53564441, 0.53493436, 0.53482391,\n",
       "        0.53491858, 0.53337225, 0.53348271, 0.53315135, 0.53499748,\n",
       "        0.61155643, 0.56685496, 0.61887781, 0.59828642, 0.62651477,\n",
       "        0.62541025, 0.62747728, 0.647974  , 0.62616763, 0.6476742 ,\n",
       "        0.64365059, 0.62075549, 0.61998233, 0.62629387, 0.61786796,\n",
       "        0.61420727, 0.6392483 , 0.62646743, 0.64241984, 0.62875536,\n",
       "        0.63634499, 0.63116953, 0.65122444, 0.62454241, 0.6162743 ,\n",
       "        0.61116195, 0.6310433 , 0.62042414, 0.62842401, 0.62659366,\n",
       "        0.61991921, 0.6272406 , 0.62416372, 0.64024236, 0.6533388 ,\n",
       "        0.65283388, 0.62621497, 0.61417571, 0.62402171, 0.61474375,\n",
       "        0.65307056, 0.62077127, 0.62583628, 0.63098018, 0.6223176 ,\n",
       "        0.62282252, 0.62198624, 0.61826243]),\n",
       " 'mean_test_roc_auc_ovo': array([0.56344139, 0.57458116, 0.57784449, 0.608514  , 0.59586914,\n",
       "        0.59144965, 0.59890329, 0.61703077, 0.62042575, 0.60025985,\n",
       "        0.60455425, 0.59139902, 0.59448411, 0.57911664, 0.56473581,\n",
       "        0.59967943, 0.59477281, 0.59941376, 0.60902278, 0.61249055,\n",
       "        0.59601414, 0.61436212, 0.61092542, 0.58953029, 0.57887225,\n",
       "        0.58619645, 0.58278722, 0.58703283, 0.63120919, 0.60960084,\n",
       "        0.61053886, 0.62266072, 0.60658124, 0.60299434, 0.61171501,\n",
       "        0.61457386, 0.62466813, 0.61657979, 0.58594448, 0.60923324,\n",
       "        0.61396385, 0.61437339, 0.58319186, 0.6039334 , 0.61378444,\n",
       "        0.62319014, 0.6236696 , 0.62340177, 0.58784158, 0.59808832,\n",
       "        0.58761194, 0.61354545, 0.58831709, 0.59342943, 0.5910721 ,\n",
       "        0.60308092, 0.59545516, 0.61517716, 0.61108027, 0.6098551 ,\n",
       "        0.59113335, 0.60552336, 0.59048062, 0.6159281 , 0.59880405,\n",
       "        0.59426731, 0.6000658 , 0.60151308, 0.60765897, 0.5797349 ,\n",
       "        0.61427821, 0.58888733, 0.58962629, 0.60453681, 0.59569445,\n",
       "        0.57931439, 0.57476109, 0.61958575, 0.58093566, 0.59371667,\n",
       "        0.61026754, 0.5889305 , 0.55756591, 0.59442715, 0.59060631,\n",
       "        0.60929212, 0.58178791, 0.57412066, 0.59563706, 0.60135785,\n",
       "        0.59439154, 0.60017975, 0.60704893, 0.5898028 , 0.6154974 ,\n",
       "        0.61308254, 0.50837139, 0.52413211, 0.4965482 , 0.48776254,\n",
       "        0.48091392, 0.47219356, 0.48998171, 0.49197665, 0.49325868,\n",
       "        0.51371883, 0.5108051 , 0.4870164 , 0.48599691, 0.50362508,\n",
       "        0.50036031, 0.5236152 , 0.52492549, 0.52179128, 0.49699051,\n",
       "        0.53255144, 0.47962078, 0.514513  , 0.53875172, 0.49350501,\n",
       "        0.51935374, 0.50679075, 0.50363493, 0.49768009, 0.46415326,\n",
       "        0.46619004, 0.50028521, 0.52656805, 0.50965046, 0.51677827,\n",
       "        0.48284999, 0.51915246, 0.49958622, 0.50030299, 0.51463829,\n",
       "        0.50671742, 0.4988408 , 0.51339734, 0.46412389, 0.53933419,\n",
       "        0.54702741, 0.47944584, 0.51708381, 0.4837012 , 0.51492002,\n",
       "        0.47325176, 0.51179653, 0.54491536, 0.5076213 , 0.50413051,\n",
       "        0.53023418, 0.48955026, 0.52844648, 0.50462592, 0.50855169,\n",
       "        0.52808691, 0.50293219, 0.49125452, 0.52038572, 0.54904959,\n",
       "        0.5201308 , 0.51769922, 0.53563195, 0.53520286, 0.50643695,\n",
       "        0.51986883, 0.52185919, 0.52705418, 0.48274704, 0.49215281,\n",
       "        0.52570832, 0.51382224, 0.52055256, 0.52184158, 0.53799158,\n",
       "        0.52086989, 0.53578187, 0.52182436, 0.49092506, 0.54016997,\n",
       "        0.50832101, 0.48684167, 0.52435335, 0.53530574, 0.51693964,\n",
       "        0.53337235, 0.54271245, 0.50474212, 0.52211225, 0.54406567,\n",
       "        0.50296168, 0.53339984, 0.54453856, 0.54137294, 0.54583262,\n",
       "        0.54173793, 0.54222654, 0.54133222, 0.54292046, 0.54240688,\n",
       "        0.54370222, 0.54110968, 0.54221898, 0.54220424, 0.54309873,\n",
       "        0.54297324, 0.54250782, 0.54147544, 0.54240679, 0.54180194,\n",
       "        0.54243847, 0.54240442, 0.5416239 , 0.54152791, 0.54135963,\n",
       "        0.54128061, 0.54554727, 0.54552245, 0.54378929, 0.54233346,\n",
       "        0.54318508, 0.54079484, 0.54146351, 0.54198712, 0.54223675,\n",
       "        0.54093746, 0.54019221, 0.5420503 , 0.5469086 , 0.54390397,\n",
       "        0.54293889, 0.54243884, 0.54459749, 0.54172636, 0.54137153,\n",
       "        0.54189597, 0.5404861 , 0.54193194, 0.54159692, 0.54203653,\n",
       "        0.61813442, 0.60384147, 0.60303201, 0.62655687, 0.62623239,\n",
       "        0.61525178, 0.62163214, 0.63095616, 0.61785723, 0.62367827,\n",
       "        0.63232149, 0.62534929, 0.628906  , 0.62323513, 0.6207117 ,\n",
       "        0.62383808, 0.62756651, 0.62362178, 0.61936077, 0.61273416,\n",
       "        0.6181746 , 0.62063969, 0.63374667, 0.62191221, 0.60009574,\n",
       "        0.62187245, 0.61625041, 0.62548138, 0.62452181, 0.62819343,\n",
       "        0.62389266, 0.62250624, 0.62691718, 0.63029288, 0.62442086,\n",
       "        0.6230497 , 0.61345467, 0.61951253, 0.59784109, 0.62804385,\n",
       "        0.63263776, 0.6301324 , 0.63015678, 0.62098255, 0.62203355,\n",
       "        0.62708086, 0.62967472, 0.62092177]),\n",
       " 'std_test_roc_auc_ovo': array([0.02983675, 0.03331832, 0.01156993, 0.04600366, 0.03397389,\n",
       "        0.05177048, 0.03276163, 0.0264288 , 0.04549129, 0.02987731,\n",
       "        0.02601963, 0.02650181, 0.03567096, 0.03886426, 0.03622851,\n",
       "        0.03491434, 0.01507417, 0.02511194, 0.0247412 , 0.03426751,\n",
       "        0.03636257, 0.01825103, 0.02748712, 0.0272572 , 0.03527116,\n",
       "        0.01668816, 0.02601585, 0.02226723, 0.02564688, 0.01514581,\n",
       "        0.01745126, 0.03124781, 0.02708435, 0.02142777, 0.02707923,\n",
       "        0.02258041, 0.04224101, 0.02732853, 0.02481265, 0.02897608,\n",
       "        0.04179691, 0.0122654 , 0.01893635, 0.04098594, 0.0172619 ,\n",
       "        0.03641784, 0.02342258, 0.01731928, 0.02839593, 0.05023732,\n",
       "        0.0463547 , 0.02096284, 0.04435602, 0.02213867, 0.03839011,\n",
       "        0.03056962, 0.01633262, 0.03495521, 0.01866593, 0.03147575,\n",
       "        0.02140086, 0.02597631, 0.01615851, 0.00746991, 0.03722513,\n",
       "        0.02153897, 0.03788041, 0.01977647, 0.03107832, 0.04151496,\n",
       "        0.02762924, 0.01786042, 0.04755794, 0.01033683, 0.02834433,\n",
       "        0.0159813 , 0.04794724, 0.03508521, 0.03709744, 0.04948029,\n",
       "        0.03895312, 0.02452271, 0.02207478, 0.02556663, 0.04899789,\n",
       "        0.0317732 , 0.03881039, 0.03037826, 0.02458022, 0.02777731,\n",
       "        0.02531854, 0.01032043, 0.02698689, 0.01881651, 0.01741492,\n",
       "        0.01900757, 0.04834599, 0.02186556, 0.04628126, 0.03390309,\n",
       "        0.05329276, 0.03429329, 0.05186329, 0.04282843, 0.05011767,\n",
       "        0.03961728, 0.03184553, 0.04223785, 0.02111746, 0.02165118,\n",
       "        0.03881747, 0.03675864, 0.04366054, 0.01159978, 0.04400813,\n",
       "        0.02604715, 0.02972583, 0.04285141, 0.03607215, 0.02822743,\n",
       "        0.04838375, 0.02606035, 0.02722586, 0.04698549, 0.03327613,\n",
       "        0.03810809, 0.02472633, 0.03494675, 0.02758478, 0.04031537,\n",
       "        0.06530454, 0.05019557, 0.04370741, 0.02218322, 0.03606646,\n",
       "        0.02674346, 0.02377564, 0.03774056, 0.0345981 , 0.04721712,\n",
       "        0.04192869, 0.03105456, 0.03746735, 0.0157205 , 0.05631977,\n",
       "        0.03036899, 0.03698544, 0.0389309 , 0.02379402, 0.03943224,\n",
       "        0.04331115, 0.05185198, 0.07135884, 0.02320179, 0.02651868,\n",
       "        0.05597817, 0.05383004, 0.03925331, 0.04913706, 0.05126933,\n",
       "        0.02772557, 0.04048663, 0.01676384, 0.01523265, 0.02739774,\n",
       "        0.04072434, 0.0447088 , 0.02913169, 0.02939239, 0.03322239,\n",
       "        0.03759887, 0.05958741, 0.06038011, 0.04209317, 0.06756131,\n",
       "        0.04186992, 0.04571384, 0.01536003, 0.03157716, 0.04619952,\n",
       "        0.03762576, 0.05143658, 0.02358956, 0.03960146, 0.03554454,\n",
       "        0.03625522, 0.02456349, 0.0439777 , 0.03356718, 0.03191875,\n",
       "        0.0473739 , 0.0358414 , 0.02417069, 0.0248963 , 0.026936  ,\n",
       "        0.02874319, 0.02720839, 0.02593427, 0.02629877, 0.02969737,\n",
       "        0.0270375 , 0.02582486, 0.02799744, 0.02740466, 0.02927753,\n",
       "        0.02629096, 0.02702113, 0.02477401, 0.02620449, 0.02792613,\n",
       "        0.02768468, 0.02766396, 0.02696652, 0.02718828, 0.02680974,\n",
       "        0.0269026 , 0.02593962, 0.03068279, 0.02697882, 0.02652477,\n",
       "        0.02736722, 0.02591228, 0.02690008, 0.02756198, 0.02849517,\n",
       "        0.02731867, 0.02708974, 0.02781614, 0.02919447, 0.02725651,\n",
       "        0.02538875, 0.02625107, 0.02843742, 0.02715373, 0.02669417,\n",
       "        0.02767572, 0.02709767, 0.0268367 , 0.02683809, 0.02671113,\n",
       "        0.04097068, 0.03774289, 0.04674712, 0.02393929, 0.01268775,\n",
       "        0.03611261, 0.01982682, 0.03189978, 0.02742461, 0.02079184,\n",
       "        0.02767926, 0.04116338, 0.0285945 , 0.03801506, 0.02013912,\n",
       "        0.02362295, 0.02719159, 0.02216083, 0.02515069, 0.02429101,\n",
       "        0.02300383, 0.02481037, 0.02195677, 0.02087354, 0.05324379,\n",
       "        0.02409735, 0.03492322, 0.03632724, 0.03438516, 0.02663702,\n",
       "        0.02674465, 0.03190116, 0.02756985, 0.02724311, 0.02626106,\n",
       "        0.02527667, 0.017015  , 0.0330048 , 0.02179705, 0.02580464,\n",
       "        0.02551484, 0.02993054, 0.03101718, 0.03211861, 0.02813153,\n",
       "        0.02816245, 0.03010969, 0.03271786]),\n",
       " 'rank_test_roc_auc_ovo': array([143, 140, 138,  77, 102, 113,  97,  49,  42,  91,  82, 114, 107,\n",
       "        136, 142,  95, 106,  96,  76,  66, 101,  58,  69, 121, 137, 128,\n",
       "        131, 127,   4,  73,  70,  32,  80,  88,  67,  56,  20,  50, 129,\n",
       "         75,  60,  57, 130,  84,  61,  30,  26,  28, 125,  99, 126,  62,\n",
       "        124, 112, 116,  86, 105,  55,  68,  72, 115,  81, 118,  52,  98,\n",
       "        110,  94,  89,  78, 134,  59, 123, 120,  83, 103, 135, 139,  43,\n",
       "        133, 111,  71, 122, 144, 108, 117,  74, 132, 141, 104,  90, 109,\n",
       "         92,  79, 119,  53,  64, 245, 217, 265, 274, 281, 285, 272, 269,\n",
       "        267, 239, 242, 275, 277, 255, 258, 218, 215, 223, 264, 208, 282,\n",
       "        237, 200, 266, 229, 248, 254, 263, 287, 286, 260, 213, 243, 234,\n",
       "        279, 230, 261, 259, 236, 249, 262, 240, 288, 199, 146, 283, 232,\n",
       "        278, 235, 284, 241, 151, 247, 253, 209, 273, 210, 252, 244, 211,\n",
       "        257, 270, 226, 145, 227, 231, 203, 205, 250, 228, 220, 212, 280,\n",
       "        268, 214, 238, 225, 221, 201, 224, 202, 222, 271, 198, 246, 276,\n",
       "        216, 204, 233, 207, 163, 251, 219, 154, 256, 206, 153, 188, 148,\n",
       "        181, 172, 191, 162, 167, 157, 193, 173, 174, 159, 160, 164, 186,\n",
       "        168, 180, 166, 169, 183, 185, 190, 192, 149, 150, 156, 170, 158,\n",
       "        195, 187, 177, 171, 194, 197, 175, 147, 155, 161, 165, 152, 182,\n",
       "        189, 179, 196, 178, 184, 176,  47,  85,  87,  16,  17,  54,  37,\n",
       "          5,  48,  25,   3,  19,  10,  29,  40,  24,  13,  27,  45,  65,\n",
       "         46,  41,   1,  35,  93,  36,  51,  18,  21,  11,  23,  33,  15,\n",
       "          6,  22,  31,  63,  44, 100,  12,   2,   8,   7,  38,  34,  14,\n",
       "          9,  39]),\n",
       " 'split0_test_neg_log_loss': array([-0.27250876, -0.24652176, -0.2613072 , -0.28304411, -0.2665579 ,\n",
       "        -0.25844697, -0.31106686, -0.31275898, -0.590559  , -0.42567561,\n",
       "        -0.38191639, -0.42681672, -0.28961062, -0.26494716, -0.26771604,\n",
       "        -0.27853622, -0.2873745 , -0.28971158, -0.3001161 , -0.34097302,\n",
       "        -0.33639508, -0.40885887, -0.32916106, -0.35226528, -0.25481778,\n",
       "        -0.25636527, -0.26047554, -0.26470101, -0.2691991 , -0.26086522,\n",
       "        -0.31069056, -0.27430217, -0.30611397, -0.32699294, -0.34957748,\n",
       "        -0.34441576, -0.25020169, -0.24723628, -0.24603496, -0.27594647,\n",
       "        -0.25278868, -0.2651767 , -0.2801021 , -0.28338394, -0.26929556,\n",
       "        -0.31591371, -0.30429882, -0.30186683, -0.24994375, -0.26554861,\n",
       "        -0.25203496, -0.29565146, -0.436859  , -0.28011733, -0.3256446 ,\n",
       "        -0.33066259, -0.32784394, -0.40536579, -0.39887121, -0.55102511,\n",
       "        -0.25477839, -0.25948755, -0.26273458, -0.27303486, -0.30947414,\n",
       "        -0.29041737, -0.33955142, -0.32313544, -0.33464745, -0.37927531,\n",
       "        -0.36546615, -0.3860162 , -0.25792649, -0.24805621, -0.25775078,\n",
       "        -0.29251377, -0.30161459, -0.26860442, -0.31920875, -0.32535015,\n",
       "        -0.31677997, -0.42561032, -0.40721965, -0.39166343, -0.24857982,\n",
       "        -0.24386325, -0.24242841, -0.26306974, -0.2716888 , -0.26752912,\n",
       "        -0.28979763, -0.30269779, -0.30868784, -0.35147149, -0.33916995,\n",
       "        -0.3388974 , -0.24497835, -0.55149735, -0.24616373, -0.25098328,\n",
       "        -0.54144918, -0.25065557, -0.24689683, -0.57567539, -0.24860444,\n",
       "        -0.24506522, -0.44808999, -0.25075077, -0.24866136, -0.74369892,\n",
       "        -0.24692409, -0.24507156, -0.65468064, -0.24720457, -0.24480014,\n",
       "        -0.67331977, -0.24700621, -0.24672323, -0.56103949, -0.24650773,\n",
       "        -0.24525126, -0.45935813, -0.24833912, -0.24755146, -0.46331974,\n",
       "        -0.24712042, -0.24579221, -0.43036648, -0.24519166, -0.24526659,\n",
       "        -0.48927006, -0.24582062, -0.24916317, -0.70719543, -0.24658703,\n",
       "        -0.24694321, -0.61034613, -0.2447881 , -0.24584975, -0.50835377,\n",
       "        -0.24502708, -0.2479625 , -0.40090813, -0.24690115, -0.25363134,\n",
       "        -1.13224937, -0.25432822, -0.24883644, -0.45573167, -0.24982892,\n",
       "        -0.24967883, -0.6323807 , -0.24803959, -0.24947846, -0.4688571 ,\n",
       "        -0.25232949, -0.25115495, -0.73567334, -0.25167477, -0.25388684,\n",
       "        -0.52861095, -0.25309485, -0.24559675, -0.68678442, -0.25391843,\n",
       "        -0.24919723, -0.75092696, -0.25149003, -0.25135929, -0.89161846,\n",
       "        -0.25213826, -0.24880525, -0.91999094, -0.24726114, -0.24976489,\n",
       "        -0.72373983, -0.24960359, -0.2500185 , -0.58657524, -0.25182662,\n",
       "        -0.25172141, -0.7229426 , -0.25154818, -0.25037928, -0.74467345,\n",
       "        -0.24991482, -0.24829845, -0.75975993, -0.24771595, -0.24422465,\n",
       "        -0.63421015, -0.24789248, -0.24627579, -0.24693339, -0.24643763,\n",
       "        -0.24685068, -0.24658265, -0.24674604, -0.24656109, -0.24683877,\n",
       "        -0.24678748, -0.24668614, -0.24673122, -0.24651504, -0.24698107,\n",
       "        -0.24614719, -0.24688918, -0.24671611, -0.24685397, -0.24680912,\n",
       "        -0.24677926, -0.2468215 , -0.24688343, -0.24682998, -0.24683759,\n",
       "        -0.24674461, -0.24639581, -0.24662142, -0.24635223, -0.24653416,\n",
       "        -0.24669663, -0.24676625, -0.24656069, -0.24667824, -0.2469369 ,\n",
       "        -0.24671886, -0.24676604, -0.24699299, -0.24657438, -0.24638093,\n",
       "        -0.24618736, -0.24651123, -0.24642977, -0.24662512, -0.24639574,\n",
       "        -0.24645048, -0.24668696, -0.24658264, -0.24643396, -0.24654955,\n",
       "        -0.24078924, -0.24355616, -0.24291908, -0.23996959, -0.23809345,\n",
       "        -0.23992581, -0.24107057, -0.24362392, -0.24807671, -0.2442209 ,\n",
       "        -0.24257938, -0.24493824, -0.24039258, -0.24002981, -0.24048778,\n",
       "        -0.24090906, -0.23917117, -0.24082965, -0.24384612, -0.24148517,\n",
       "        -0.24599164, -0.24264927, -0.24366317, -0.24143455, -0.2409937 ,\n",
       "        -0.23845464, -0.2434634 , -0.24173281, -0.23996457, -0.2385126 ,\n",
       "        -0.2396172 , -0.24146436, -0.23894384, -0.24536293, -0.24335958,\n",
       "        -0.24166047, -0.2406258 , -0.24057206, -0.24012293, -0.24057143,\n",
       "        -0.24046758, -0.23933274, -0.2389353 , -0.24059222, -0.23910607,\n",
       "        -0.24022389, -0.24037844, -0.24027924]),\n",
       " 'split1_test_neg_log_loss': array([-0.24408419, -0.25173772, -0.26457957, -0.24705322, -0.29686758,\n",
       "        -0.35458905, -0.31914217, -0.27812509, -0.36932731, -0.35153203,\n",
       "        -0.3376824 , -0.37314312, -0.24004539, -0.23482897, -0.25406988,\n",
       "        -0.2647749 , -0.28117026, -0.26934414, -0.28608225, -0.31707399,\n",
       "        -0.29852823, -0.37025319, -0.3669007 , -0.42731406, -0.23990533,\n",
       "        -0.2432303 , -0.24514605, -0.27044087, -0.25215484, -0.26280944,\n",
       "        -0.30489282, -0.29513016, -0.2980345 , -0.32591952, -0.33222292,\n",
       "        -0.34091373, -0.23064058, -0.23978232, -0.24646798, -0.24783136,\n",
       "        -0.23856432, -0.25661314, -0.29810563, -0.27804399, -0.2853502 ,\n",
       "        -0.28373845, -0.29460324, -0.31083696, -0.2520271 , -0.24598592,\n",
       "        -0.24064025, -0.27335852, -0.25639724, -0.2763    , -0.30521884,\n",
       "        -0.32087865, -0.35935964, -0.40523975, -0.37554072, -0.3879485 ,\n",
       "        -0.2470145 , -0.23659405, -0.24399029, -0.27796591, -0.26390537,\n",
       "        -0.33096355, -0.31041393, -0.3424292 , -0.32376784, -0.37410327,\n",
       "        -0.37725628, -0.40613745, -0.23900278, -0.25502359, -0.24902111,\n",
       "        -0.2938769 , -0.25738737, -0.25198611, -0.2943345 , -0.30294367,\n",
       "        -0.29481032, -0.35169399, -0.42009546, -0.34895525, -0.23827782,\n",
       "        -0.24335594, -0.24754791, -0.28642197, -0.27731668, -0.26095096,\n",
       "        -0.29620797, -0.29924734, -0.27484621, -0.3168607 , -0.32042808,\n",
       "        -0.34726988, -0.24559501, -0.4110414 , -0.24936249, -0.24821469,\n",
       "        -0.74625219, -0.2485279 , -0.24491349, -0.50040056, -0.24612205,\n",
       "        -0.24746469, -0.64870011, -0.24561595, -0.25095281, -0.78732524,\n",
       "        -0.24907104, -0.24626295, -0.55019746, -0.2455696 , -0.24789902,\n",
       "        -0.47196604, -0.24608365, -0.24910188, -0.49569676, -0.24796546,\n",
       "        -0.24884933, -0.4592502 , -0.24684014, -0.24720182, -0.57316951,\n",
       "        -0.24719394, -0.24705705, -0.46525402, -0.24549655, -0.24625987,\n",
       "        -0.47010486, -0.24455202, -0.24798591, -0.83233012, -0.24773466,\n",
       "        -0.24591281, -0.49190795, -0.24541907, -0.24883972, -0.60973148,\n",
       "        -0.24394268, -0.24679494, -0.47383972, -0.24780517, -0.24476992,\n",
       "        -0.65313873, -0.24663526, -0.24306389, -0.50705766, -0.24443764,\n",
       "        -0.24122687, -0.74258871, -0.23753703, -0.2473486 , -0.69305442,\n",
       "        -0.24136879, -0.24173361, -0.6528478 , -0.2436231 , -0.24027137,\n",
       "        -0.65616869, -0.24393808, -0.24492153, -0.55747463, -0.24553596,\n",
       "        -0.24428422, -0.6905702 , -0.24529051, -0.24928734, -0.47032368,\n",
       "        -0.24309716, -0.23933268, -0.55392105, -0.24329338, -0.2394165 ,\n",
       "        -0.65074262, -0.24073874, -0.24579796, -0.59727925, -0.24169217,\n",
       "        -0.24669187, -0.6913463 , -0.24874368, -0.24184021, -0.54901181,\n",
       "        -0.24270897, -0.2437735 , -0.71243115, -0.24423923, -0.2412327 ,\n",
       "        -0.49828355, -0.24314182, -0.24288498, -0.24265191, -0.24278043,\n",
       "        -0.24263472, -0.24274763, -0.24271761, -0.24281146, -0.24259663,\n",
       "        -0.24280025, -0.24301232, -0.24257054, -0.24268002, -0.24234507,\n",
       "        -0.24286706, -0.24295054, -0.24303629, -0.24282619, -0.24261757,\n",
       "        -0.24270966, -0.24266314, -0.24277587, -0.24279332, -0.24274619,\n",
       "        -0.24284595, -0.24265833, -0.24222942, -0.24243917, -0.24267392,\n",
       "        -0.24262608, -0.24288665, -0.2427679 , -0.24262697, -0.24261201,\n",
       "        -0.2427133 , -0.2427972 , -0.24280298, -0.24259573, -0.2428691 ,\n",
       "        -0.24285602, -0.24283993, -0.2428041 , -0.24285116, -0.24292141,\n",
       "        -0.24281989, -0.24285013, -0.24291686, -0.24293824, -0.24294141,\n",
       "        -0.23267226, -0.23140453, -0.2298364 , -0.23255329, -0.23560312,\n",
       "        -0.23355616, -0.23522853, -0.2322421 , -0.2350567 , -0.23546992,\n",
       "        -0.23342283, -0.22932644, -0.23092313, -0.23229841, -0.23534236,\n",
       "        -0.23192852, -0.23158545, -0.23177175, -0.2405422 , -0.23771059,\n",
       "        -0.23429692, -0.2357227 , -0.23540402, -0.23888345, -0.23160495,\n",
       "        -0.23193283, -0.23239777, -0.2285365 , -0.23160309, -0.232001  ,\n",
       "        -0.23212299, -0.23256718, -0.23216868, -0.23173364, -0.23461675,\n",
       "        -0.2377613 , -0.23820304, -0.23100894, -0.24573032, -0.23143227,\n",
       "        -0.23256164, -0.23229406, -0.23056909, -0.23279427, -0.23256022,\n",
       "        -0.23102176, -0.23105881, -0.23078653]),\n",
       " 'split2_test_neg_log_loss': array([-0.47251289, -0.2817966 , -0.2503106 , -0.30106091, -0.3109714 ,\n",
       "        -0.31682829, -0.3660555 , -0.33069923, -0.33448992, -0.37855875,\n",
       "        -0.3875304 , -0.40231726, -0.25979313, -0.2674655 , -0.26692863,\n",
       "        -0.26679512, -0.28708954, -0.28601145, -0.30070369, -0.30962523,\n",
       "        -0.31404042, -0.37710967, -0.35648221, -0.3617871 , -0.25769059,\n",
       "        -0.25805839, -0.2571539 , -0.27439403, -0.25888091, -0.26449475,\n",
       "        -0.29421651, -0.29356668, -0.30733525, -0.31802887, -0.39339799,\n",
       "        -0.30911604, -0.25075823, -0.24461904, -0.25764373, -0.26749111,\n",
       "        -0.26645612, -0.26882743, -0.29820382, -0.32175452, -0.28396067,\n",
       "        -0.30762824, -0.32691231, -0.30746134, -0.26467709, -0.26656945,\n",
       "        -0.2576073 , -0.2905379 , -0.28338684, -0.41610947, -0.36417734,\n",
       "        -0.34998658, -0.34692762, -0.4461856 , -0.42250255, -0.48131227,\n",
       "        -0.26603175, -0.25432741, -0.26424468, -0.29463622, -0.3004071 ,\n",
       "        -0.29042968, -0.35413376, -0.33118581, -0.33793275, -0.46503944,\n",
       "        -0.42360285, -0.43481696, -0.26656472, -0.26146562, -0.25297326,\n",
       "        -0.26812674, -0.29426844, -0.29270958, -0.34551418, -0.40706   ,\n",
       "        -0.37135683, -0.38837403, -0.41020301, -0.42866659, -0.25262071,\n",
       "        -0.25556003, -0.25686547, -0.28313387, -0.28102093, -0.28920492,\n",
       "        -0.31871889, -0.29634851, -0.32074013, -0.38900408, -0.38028133,\n",
       "        -0.34391127, -0.2498397 , -0.48255866, -0.24455166, -0.24652296,\n",
       "        -0.68994648, -0.24714463, -0.24822652, -0.41323631, -0.24834833,\n",
       "        -0.24645764, -0.45421531, -0.24680549, -0.2481458 , -0.81514072,\n",
       "        -0.24595353, -0.24545883, -0.63638543, -0.24612094, -0.24739635,\n",
       "        -0.62399191, -0.2471235 , -0.24447213, -0.4251803 , -0.24707381,\n",
       "        -0.24540695, -0.3807279 , -0.2475152 , -0.24971369, -0.4605954 ,\n",
       "        -0.24795922, -0.24782729, -0.47743506, -0.2477217 , -0.24826318,\n",
       "        -0.43289573, -0.2477901 , -0.24911564, -0.69249567, -0.24816952,\n",
       "        -0.24620088, -0.4240766 , -0.24731985, -0.24777216, -0.59133967,\n",
       "        -0.24443769, -0.2473267 , -0.69308547, -0.24676649, -0.24972625,\n",
       "        -0.40796207, -0.24311636, -0.24513363, -0.97012254, -0.24627009,\n",
       "        -0.24496029, -0.66186256, -0.24984178, -0.24877723, -0.63330415,\n",
       "        -0.2451163 , -0.25067485, -0.98937645, -0.24612875, -0.24954536,\n",
       "        -0.87805016, -0.24864226, -0.24591188, -0.6853887 , -0.24975913,\n",
       "        -0.24943518, -0.54475585, -0.2453129 , -0.25643833, -0.66083013,\n",
       "        -0.2446452 , -0.25177144, -0.98042032, -0.25243066, -0.24804616,\n",
       "        -0.83302424, -0.24894727, -0.24710463, -0.63152495, -0.24571879,\n",
       "        -0.24886718, -0.42329296, -0.24956649, -0.24535626, -0.51153191,\n",
       "        -0.24522766, -0.24439278, -0.50240264, -0.24610843, -0.24559632,\n",
       "        -0.92323324, -0.24469716, -0.24447084, -0.24436264, -0.24423402,\n",
       "        -0.24436484, -0.24482909, -0.24469421, -0.24451624, -0.24477352,\n",
       "        -0.2445734 , -0.24472464, -0.24469378, -0.24485271, -0.24390694,\n",
       "        -0.24416409, -0.24447802, -0.24455614, -0.24458932, -0.24453235,\n",
       "        -0.24468936, -0.24481897, -0.24478002, -0.24473656, -0.24479442,\n",
       "        -0.24492999, -0.24448922, -0.2441095 , -0.24393289, -0.24474017,\n",
       "        -0.24456119, -0.2448681 , -0.24470127, -0.24469066, -0.24469777,\n",
       "        -0.24482467, -0.24503367, -0.2446947 , -0.24393792, -0.24415277,\n",
       "        -0.24463711, -0.24464012, -0.24444192, -0.24458445, -0.2449758 ,\n",
       "        -0.2447615 , -0.24479632, -0.24473421, -0.24485475, -0.24474478,\n",
       "        -0.24551199, -0.2418827 , -0.24144379, -0.24093027, -0.23985622,\n",
       "        -0.24545471, -0.24422076, -0.24546047, -0.24527008, -0.2431598 ,\n",
       "        -0.2466168 , -0.2488326 , -0.24005403, -0.246421  , -0.24193484,\n",
       "        -0.24198076, -0.24285085, -0.24078536, -0.24493655, -0.24682622,\n",
       "        -0.24580604, -0.24618602, -0.24485095, -0.24490864, -0.24129653,\n",
       "        -0.2420301 , -0.24250318, -0.2428774 , -0.2466013 , -0.24219833,\n",
       "        -0.24257727, -0.2446488 , -0.24322489, -0.24332041, -0.24364875,\n",
       "        -0.24384652, -0.24197584, -0.24041939, -0.24210605, -0.24011679,\n",
       "        -0.24011284, -0.24140435, -0.24221607, -0.24502186, -0.24461517,\n",
       "        -0.24186337, -0.24269722, -0.2429698 ]),\n",
       " 'split3_test_neg_log_loss': array([-0.26376546, -0.26466371, -0.25444523, -0.27465307, -0.2832734 ,\n",
       "        -0.29640034, -0.31534637, -0.31142207, -0.31597592, -0.4276109 ,\n",
       "        -0.40035988, -0.36871712, -0.27616193, -0.26387013, -0.26086276,\n",
       "        -0.26520328, -0.27816624, -0.27688233, -0.30201769, -0.31904795,\n",
       "        -0.34625053, -0.36249647, -0.46784166, -0.39992986, -0.25903568,\n",
       "        -0.26086051, -0.25924978, -0.30351122, -0.26505123, -0.26863679,\n",
       "        -0.29097475, -0.2850971 , -0.30416552, -0.36555354, -0.34963777,\n",
       "        -0.33778555, -0.24538794, -0.2366357 , -0.25660737, -0.26846316,\n",
       "        -0.25590473, -0.26417545, -0.31054831, -0.29193002, -0.26909291,\n",
       "        -0.32157622, -0.30544729, -0.32720531, -0.25350435, -0.24129219,\n",
       "        -0.26409988, -0.26768995, -0.29871976, -0.30807908, -0.40879241,\n",
       "        -0.32652572, -0.31748396, -0.41540171, -0.55428722, -0.38873429,\n",
       "        -0.26028629, -0.26106245, -0.26314846, -0.27899746, -0.26342113,\n",
       "        -0.30626375, -0.34046994, -0.33810599, -0.31173245, -0.45895617,\n",
       "        -0.50473268, -0.38185322, -0.24902489, -0.25214734, -0.24178126,\n",
       "        -0.28769219, -0.28856662, -0.26910426, -0.33868124, -0.30806848,\n",
       "        -0.32056847, -0.37766942, -0.38408577, -0.37025594, -0.26966182,\n",
       "        -0.24292848, -0.2627344 , -0.27865762, -0.26101062, -0.26976209,\n",
       "        -0.31299591, -0.29946862, -0.29553665, -0.36437301, -0.33890657,\n",
       "        -0.33515587, -0.24782241, -0.4570377 , -0.24665123, -0.24708408,\n",
       "        -0.52019479, -0.24786566, -0.24880904, -0.63559099, -0.2443887 ,\n",
       "        -0.24530549, -0.6068732 , -0.24558391, -0.24830878, -0.47919009,\n",
       "        -0.2475428 , -0.2470898 , -0.56827503, -0.24573272, -0.2475523 ,\n",
       "        -0.43382346, -0.24764096, -0.24491366, -0.60034088, -0.24610372,\n",
       "        -0.24788203, -1.13163612, -0.24584419, -0.24456862, -0.44214004,\n",
       "        -0.24742067, -0.24659615, -0.53659249, -0.24651793, -0.24511172,\n",
       "        -0.41229254, -0.24566625, -0.2458184 , -0.67957341, -0.24466509,\n",
       "        -0.24590338, -0.5448147 , -0.2476957 , -0.2478391 , -0.45980966,\n",
       "        -0.24528205, -0.24569999, -0.54173214, -0.24627227, -0.24623712,\n",
       "        -0.71521172, -0.24806388, -0.24944737, -0.69143381, -0.25168308,\n",
       "        -0.24945702, -0.50706994, -0.24851454, -0.24770982, -0.57829857,\n",
       "        -0.24940121, -0.24802822, -0.66803117, -0.25109319, -0.24467893,\n",
       "        -0.52302566, -0.24889547, -0.24891187, -0.63983821, -0.24835168,\n",
       "        -0.2514453 , -0.60938098, -0.24804417, -0.25152999, -0.95656702,\n",
       "        -0.2443851 , -0.24951694, -0.7439713 , -0.25078259, -0.24634127,\n",
       "        -0.62212783, -0.24548659, -0.24745078, -0.83654261, -0.24372913,\n",
       "        -0.25553031, -0.53925393, -0.24579477, -0.24534536, -0.82664443,\n",
       "        -0.24939565, -0.24725768, -0.7576723 , -0.24690702, -0.24706492,\n",
       "        -0.79299732, -0.24456561, -0.24598139, -0.24637484, -0.24647155,\n",
       "        -0.24680117, -0.24659545, -0.24652203, -0.24653856, -0.24655755,\n",
       "        -0.24657946, -0.24655056, -0.24674073, -0.24655267, -0.2466071 ,\n",
       "        -0.24639071, -0.24650972, -0.24643593, -0.24623653, -0.24655365,\n",
       "        -0.24654667, -0.24636221, -0.24668132, -0.24672309, -0.24681394,\n",
       "        -0.2465942 , -0.24618578, -0.24633782, -0.24635796, -0.24674048,\n",
       "        -0.2465743 , -0.24671991, -0.24659749, -0.24658915, -0.24670014,\n",
       "        -0.24659061, -0.24661257, -0.24668475, -0.24613959, -0.24631748,\n",
       "        -0.24626441, -0.24627495, -0.24613961, -0.2463796 , -0.24641042,\n",
       "        -0.24654345, -0.24623115, -0.24632867, -0.24658791, -0.24651801,\n",
       "        -0.23687905, -0.23642299, -0.24256054, -0.23617653, -0.23696067,\n",
       "        -0.23862267, -0.24393746, -0.24048577, -0.23707979, -0.24016728,\n",
       "        -0.23817911, -0.23929242, -0.23667402, -0.23835342, -0.23751991,\n",
       "        -0.23668278, -0.236964  , -0.23989502, -0.23775635, -0.24498437,\n",
       "        -0.23997619, -0.24707574, -0.23955355, -0.24012371, -0.24652904,\n",
       "        -0.23960962, -0.2380747 , -0.23480851, -0.23546659, -0.23740571,\n",
       "        -0.24292296, -0.23774559, -0.23978602, -0.24213005, -0.24461616,\n",
       "        -0.24339388, -0.23836505, -0.23867312, -0.2392421 , -0.23683582,\n",
       "        -0.2373159 , -0.23582675, -0.2366654 , -0.23635184, -0.23740026,\n",
       "        -0.23752124, -0.23560876, -0.23901305]),\n",
       " 'split4_test_neg_log_loss': array([-0.25879043, -0.26574839, -0.26624378, -0.29087998, -0.28629281,\n",
       "        -0.27587642, -0.31464994, -0.32163447, -0.3015378 , -0.37881999,\n",
       "        -0.41640996, -0.46864057, -0.25342829, -0.26400676, -0.26118938,\n",
       "        -0.313828  , -0.29092959, -0.28097236, -0.32695635, -0.32930961,\n",
       "        -0.32264789, -0.34315352, -0.34953467, -0.38445345, -0.26311372,\n",
       "        -0.25249739, -0.26098621, -0.28029089, -0.27116973, -0.27449087,\n",
       "        -0.28450776, -0.28346026, -0.29265792, -0.37761446, -0.30845683,\n",
       "        -0.35215859, -0.24804358, -0.25583199, -0.2587356 , -0.26642884,\n",
       "        -0.28065825, -0.26156429, -0.28460352, -0.26953173, -0.27651025,\n",
       "        -0.32724657, -0.27664802, -0.30098695, -0.25600327, -0.24822011,\n",
       "        -0.50433988, -0.28348609, -0.29912956, -0.28915214, -0.34170621,\n",
       "        -0.4157429 , -0.33844681, -0.3566747 , -0.40117109, -0.42270832,\n",
       "        -0.26508986, -0.25841671, -0.25800432, -0.28207501, -0.28834192,\n",
       "        -0.36253399, -0.34464009, -0.33337865, -0.31646897, -0.42314824,\n",
       "        -0.4032838 , -0.43750594, -0.35587225, -0.27104598, -0.26551453,\n",
       "        -0.29266603, -0.40054554, -0.3428021 , -0.33078434, -0.29599358,\n",
       "        -0.31100026, -0.38234611, -0.38816565, -0.3793793 , -0.24968443,\n",
       "        -0.25779644, -0.25845006, -0.29006749, -0.27134403, -0.27845646,\n",
       "        -0.31531727, -0.3026283 , -0.2995633 , -0.32792342, -0.3250537 ,\n",
       "        -0.33479199, -0.25092854, -0.52690448, -0.25006177, -0.24867204,\n",
       "        -0.76681551, -0.24796903, -0.24771133, -0.50660668, -0.25111103,\n",
       "        -0.24904992, -0.44652974, -0.24930651, -0.24984356, -0.78908394,\n",
       "        -0.24736682, -0.24812609, -0.42202791, -0.24892794, -0.24828129,\n",
       "        -0.36999787, -0.24862637, -0.24847459, -0.46462602, -0.24810413,\n",
       "        -0.24852217, -0.70140459, -0.25157324, -0.24857109, -0.62068155,\n",
       "        -0.25582217, -0.24842148, -0.45489464, -0.24911412, -0.2481051 ,\n",
       "        -0.41669217, -0.24745662, -0.24863315, -0.56754406, -0.25067962,\n",
       "        -0.2513771 , -0.45417937, -0.24887013, -0.25006804, -0.5813936 ,\n",
       "        -0.24993057, -0.24920427, -0.58978971, -0.24899813, -0.24947201,\n",
       "        -0.40375441, -0.25091249, -0.24974059, -0.43905719, -0.25441057,\n",
       "        -0.24975274, -0.618056  , -0.25214704, -0.24981406, -0.67901855,\n",
       "        -0.250587  , -0.25780808, -0.51882951, -0.25251226, -0.24629596,\n",
       "        -0.42189245, -0.25031323, -0.24864049, -0.90025224, -0.24976461,\n",
       "        -0.24686777, -0.65858938, -0.24771589, -0.25125741, -0.94044482,\n",
       "        -0.25208585, -0.25658464, -0.46131008, -0.2472999 , -0.2526016 ,\n",
       "        -0.4888273 , -0.24935879, -0.24898701, -0.67288457, -0.25116616,\n",
       "        -0.25009687, -0.73945842, -0.24779282, -0.25178385, -0.92579604,\n",
       "        -0.25026994, -0.24800414, -0.61440175, -0.2512342 , -0.24931268,\n",
       "        -0.61800152, -0.25242497, -0.24835621, -0.24809837, -0.24796245,\n",
       "        -0.24850013, -0.2482926 , -0.24865919, -0.24843754, -0.24844603,\n",
       "        -0.24828026, -0.24871468, -0.24872178, -0.24865903, -0.24846512,\n",
       "        -0.24819183, -0.24820103, -0.24807964, -0.24841144, -0.24853877,\n",
       "        -0.24836844, -0.24845145, -0.24832717, -0.24872604, -0.24870731,\n",
       "        -0.24858772, -0.24807646, -0.24776874, -0.24756535, -0.24820975,\n",
       "        -0.24840432, -0.248418  , -0.2484907 , -0.24868448, -0.24858822,\n",
       "        -0.24870123, -0.24883171, -0.24846704, -0.24775286, -0.24791401,\n",
       "        -0.24796515, -0.24838764, -0.2483727 , -0.24849039, -0.24848262,\n",
       "        -0.24836201, -0.24851481, -0.24858108, -0.24852687, -0.2484121 ,\n",
       "        -0.24191837, -0.24600129, -0.2413065 , -0.24454154, -0.24241166,\n",
       "        -0.24172401, -0.24490253, -0.24160431, -0.24340113, -0.24177984,\n",
       "        -0.24232936, -0.24518939, -0.24061417, -0.24055385, -0.24209153,\n",
       "        -0.24356359, -0.24009488, -0.24146496, -0.24020608, -0.24268458,\n",
       "        -0.24046848, -0.24166665, -0.24101704, -0.24427595, -0.24141552,\n",
       "        -0.24158999, -0.24175734, -0.24103369, -0.24164761, -0.24110342,\n",
       "        -0.245552  , -0.24377878, -0.24167254, -0.24293641, -0.24093676,\n",
       "        -0.23988691, -0.24062685, -0.24115935, -0.24139192, -0.24139636,\n",
       "        -0.23873303, -0.24089388, -0.24059729, -0.24037526, -0.24122043,\n",
       "        -0.24154503, -0.24118389, -0.24357682]),\n",
       " 'mean_test_neg_log_loss': array([-0.30233235, -0.26209363, -0.25937727, -0.27933825, -0.28879262,\n",
       "        -0.30042821, -0.32525217, -0.31092797, -0.38237799, -0.39243946,\n",
       "        -0.38477981, -0.40792696, -0.26380787, -0.2590237 , -0.26215334,\n",
       "        -0.2778275 , -0.28494603, -0.28058437, -0.30317521, -0.32320596,\n",
       "        -0.32357243, -0.37237434, -0.37398406, -0.38514995, -0.25491262,\n",
       "        -0.25420237, -0.2566023 , -0.27866761, -0.26329116, -0.26625941,\n",
       "        -0.29705648, -0.28631127, -0.30166143, -0.34282187, -0.3466586 ,\n",
       "        -0.33687793, -0.24500641, -0.24482107, -0.25309793, -0.26523219,\n",
       "        -0.25887442, -0.2632714 , -0.29431268, -0.28892884, -0.27684192,\n",
       "        -0.31122064, -0.30158193, -0.30967147, -0.25523111, -0.25352326,\n",
       "        -0.30374446, -0.28214478, -0.31489848, -0.3139516 , -0.34910788,\n",
       "        -0.34875929, -0.33801239, -0.40577351, -0.43047456, -0.4463457 ,\n",
       "        -0.25864016, -0.25397763, -0.25842447, -0.28134189, -0.28510993,\n",
       "        -0.31612167, -0.33784183, -0.33364702, -0.32490989, -0.42010448,\n",
       "        -0.41486835, -0.40926596, -0.27367822, -0.25754775, -0.25340819,\n",
       "        -0.28697512, -0.30847651, -0.28504129, -0.3257046 , -0.32788318,\n",
       "        -0.32290317, -0.38513878, -0.40195391, -0.3837841 , -0.25176492,\n",
       "        -0.24870083, -0.25360525, -0.28027014, -0.27247621, -0.27318071,\n",
       "        -0.30660753, -0.30007811, -0.29987483, -0.34992654, -0.34076793,\n",
       "        -0.34000528, -0.2478328 , -0.48580792, -0.24735818, -0.24829541,\n",
       "        -0.65293163, -0.24843256, -0.24731144, -0.52630199, -0.24771491,\n",
       "        -0.24666859, -0.52088167, -0.24761253, -0.24918246, -0.72288778,\n",
       "        -0.24737165, -0.24640185, -0.56631329, -0.24671116, -0.24718582,\n",
       "        -0.51461981, -0.24729614, -0.2467371 , -0.50937669, -0.24715097,\n",
       "        -0.24718235, -0.62647539, -0.24802238, -0.24752133, -0.51198125,\n",
       "        -0.24910328, -0.24713883, -0.47290854, -0.24680839, -0.24660129,\n",
       "        -0.44425107, -0.24625712, -0.24814325, -0.69582774, -0.24756718,\n",
       "        -0.24726748, -0.50506495, -0.24681857, -0.24807375, -0.55012564,\n",
       "        -0.24572402, -0.24739768, -0.53987103, -0.24734864, -0.24876733,\n",
       "        -0.66246326, -0.24861124, -0.24724438, -0.61268057, -0.24932606,\n",
       "        -0.24701515, -0.63239158, -0.247216  , -0.24862563, -0.61050656,\n",
       "        -0.24776056, -0.24987994, -0.71295165, -0.24900642, -0.24693569,\n",
       "        -0.60154958, -0.24897678, -0.24679651, -0.69394764, -0.24946596,\n",
       "        -0.24824594, -0.65084468, -0.2475707 , -0.25197447, -0.78395682,\n",
       "        -0.24727031, -0.24920219, -0.73192274, -0.24821353, -0.24723409,\n",
       "        -0.66369236, -0.24682699, -0.24787177, -0.66496132, -0.24682657,\n",
       "        -0.25058153, -0.62325884, -0.24868919, -0.24694099, -0.71153153,\n",
       "        -0.24750341, -0.24634531, -0.66933356, -0.24724097, -0.24548625,\n",
       "        -0.69334516, -0.24654441, -0.24559384, -0.24568423, -0.24557722,\n",
       "        -0.24583031, -0.24580948, -0.24586782, -0.24577298, -0.2458425 ,\n",
       "        -0.24580417, -0.24593767, -0.24589161, -0.24585189, -0.24566106,\n",
       "        -0.24555218, -0.2458057 , -0.24576482, -0.24578349, -0.24581029,\n",
       "        -0.24581868, -0.24582345, -0.24588956, -0.2459618 , -0.24597989,\n",
       "        -0.24594049, -0.24556112, -0.24541338, -0.24532952, -0.2457797 ,\n",
       "        -0.2457725 , -0.24593178, -0.24582361, -0.2458539 , -0.24590701,\n",
       "        -0.24590973, -0.24600824, -0.2459285 , -0.2454001 , -0.24552686,\n",
       "        -0.24558201, -0.24573077, -0.24563762, -0.24578615, -0.2458372 ,\n",
       "        -0.24578747, -0.24581587, -0.24582869, -0.24586835, -0.24583317,\n",
       "        -0.23955418, -0.23985353, -0.23961326, -0.23883424, -0.23858502,\n",
       "        -0.23985667, -0.24187197, -0.24068331, -0.24177688, -0.24095955,\n",
       "        -0.24062549, -0.24151582, -0.23773159, -0.2395313 , -0.23947528,\n",
       "        -0.23901294, -0.23813327, -0.23894935, -0.24145746, -0.24273819,\n",
       "        -0.24130786, -0.24266008, -0.24089775, -0.24192526, -0.24036795,\n",
       "        -0.23872344, -0.23963928, -0.23779778, -0.23905663, -0.23824421,\n",
       "        -0.24055848, -0.24004094, -0.23915919, -0.24109669, -0.2414356 ,\n",
       "        -0.24130982, -0.23995931, -0.23836657, -0.24171867, -0.23807053,\n",
       "        -0.2378382 , -0.23795036, -0.23779663, -0.23902709, -0.23898043,\n",
       "        -0.23843506, -0.23818543, -0.23932509]),\n",
       " 'std_test_neg_log_loss': array([0.08558857, 0.01231176, 0.00607399, 0.01834717, 0.0147526 ,\n",
       "        0.03341795, 0.02056209, 0.01780538, 0.10653383, 0.02964204,\n",
       "        0.02637095, 0.03693714, 0.01736038, 0.01216598, 0.00493324,\n",
       "        0.01869755, 0.00461556, 0.00711277, 0.01322511, 0.01088347,\n",
       "        0.01671911, 0.02149173, 0.04852342, 0.02692199, 0.00796496,\n",
       "        0.00611835, 0.00587843, 0.01342273, 0.00697913, 0.00484814,\n",
       "        0.00948493, 0.00753969, 0.00552278, 0.02399246, 0.02781988,\n",
       "        0.01468435, 0.00742727, 0.00662576, 0.00563215, 0.00932264,\n",
       "        0.0140739 , 0.00406358, 0.01085662, 0.01795554, 0.00693069,\n",
       "        0.01519388, 0.01633617, 0.00948696, 0.00511911, 0.01048185,\n",
       "        0.10059311, 0.01040014, 0.06292878, 0.05224583, 0.03555528,\n",
       "        0.03489259, 0.01456152, 0.0287645 , 0.06366839, 0.06239948,\n",
       "        0.0070571 , 0.00897297, 0.00752715, 0.00725502, 0.01875154,\n",
       "        0.02754034, 0.01465444, 0.00653654, 0.01010557, 0.03826487,\n",
       "        0.04927745, 0.02346123, 0.04210641, 0.00803961, 0.00800053,\n",
       "        0.00965892, 0.04843999, 0.03166044, 0.01796049, 0.04076011,\n",
       "        0.02577436, 0.02379309, 0.01366951, 0.0264272 , 0.01017258,\n",
       "        0.00655849, 0.00747069, 0.00938603, 0.00677944, 0.00977551,\n",
       "        0.01143748, 0.00238066, 0.0152209 , 0.02575123, 0.02110911,\n",
       "        0.00489804, 0.00231374, 0.0498645 , 0.00205571, 0.00154803,\n",
       "        0.10304928, 0.00119548, 0.00135367, 0.07513249, 0.00229465,\n",
       "        0.00146819, 0.08832139, 0.00207225, 0.0010659 , 0.12398903,\n",
       "        0.00101278, 0.00110675, 0.08222011, 0.00124631, 0.00123116,\n",
       "        0.11524828, 0.00083336, 0.0018475 , 0.06359611, 0.00078592,\n",
       "        0.00154562, 0.27462193, 0.00195491, 0.00171639, 0.07133863,\n",
       "        0.00337227, 0.00092019, 0.03540919, 0.00145394, 0.00135204,\n",
       "        0.03034769, 0.00120298, 0.00123737, 0.08425454, 0.00197286,\n",
       "        0.00208939, 0.06631098, 0.00150406, 0.00138968, 0.05679965,\n",
       "        0.00215425, 0.00116904, 0.09964018, 0.00096198, 0.00308078,\n",
       "        0.26655936, 0.00380467, 0.00266648, 0.19995053, 0.00360267,\n",
       "        0.00341406, 0.07605652, 0.00504493, 0.00096251, 0.08139987,\n",
       "        0.00398503, 0.00519431, 0.15507461, 0.00349361, 0.00458354,\n",
       "        0.15697041, 0.00297495, 0.00165   , 0.11334802, 0.00270873,\n",
       "        0.0024556 , 0.07018353, 0.00227611, 0.00237642, 0.18942002,\n",
       "        0.00398791, 0.00563499, 0.20108788, 0.00317282, 0.00442236,\n",
       "        0.1137999 , 0.00339053, 0.00147774, 0.0909353 , 0.00402532,\n",
       "        0.00296961, 0.12257102, 0.00190408, 0.00364282, 0.1591673 ,\n",
       "        0.00300715, 0.00188815, 0.09871322, 0.00230548, 0.00271404,\n",
       "        0.14832514, 0.00332577, 0.00183548, 0.00193943, 0.00183607,\n",
       "        0.00207225, 0.00188242, 0.00201456, 0.00193169, 0.00199803,\n",
       "        0.00190983, 0.00193271, 0.00209285, 0.00199349, 0.00221689,\n",
       "        0.00185265, 0.00186128, 0.00176746, 0.00191984, 0.00204066,\n",
       "        0.00194398, 0.00195905, 0.00192264, 0.00202557, 0.00203628,\n",
       "        0.00193282, 0.00184331, 0.00198491, 0.00186586, 0.00190389,\n",
       "        0.0019895 , 0.00189225, 0.00194178, 0.00204932, 0.00205902,\n",
       "        0.00201521, 0.00200858, 0.00197182, 0.00186889, 0.00178912,\n",
       "        0.00172252, 0.0018714 , 0.00188779, 0.00191975, 0.00183754,\n",
       "        0.00187075, 0.00190003, 0.00190109, 0.00187181, 0.00185364,\n",
       "        0.00440744, 0.00526832, 0.00492788, 0.00411751, 0.00236774,\n",
       "        0.00389997, 0.00357057, 0.00455214, 0.00493461, 0.00306314,\n",
       "        0.00448285, 0.00681568, 0.00369479, 0.0045269 , 0.00263909,\n",
       "        0.00421261, 0.00377947, 0.00362345, 0.00260429, 0.00311792,\n",
       "        0.00433134, 0.00402603, 0.00332498, 0.00233089, 0.00483943,\n",
       "        0.00363651, 0.00405461, 0.0054123 , 0.00515564, 0.00356547,\n",
       "        0.00461833, 0.00443522, 0.00379877, 0.00480135, 0.00361832,\n",
       "        0.00225911, 0.00145481, 0.00377112, 0.00223741, 0.00366316,\n",
       "        0.00286368, 0.00343537, 0.00405435, 0.00415234, 0.00401248,\n",
       "        0.00401042, 0.00427982, 0.00458859]),\n",
       " 'rank_test_neg_log_loss': array([212, 180, 179, 193, 203, 209, 227, 218, 243, 248, 245, 251, 184,\n",
       "        178, 181, 191, 198, 195, 213, 224, 225, 241, 242, 247, 171, 170,\n",
       "        173, 192, 183, 186, 206, 201, 211, 236, 237, 231,  50,  49, 165,\n",
       "        185, 177, 182, 205, 204, 190, 219, 210, 217, 172, 167, 214, 197,\n",
       "        221, 220, 239, 238, 233, 250, 255, 257, 176, 169, 175, 196, 200,\n",
       "        222, 232, 230, 226, 254, 253, 252, 189, 174, 166, 202, 216, 199,\n",
       "        228, 229, 223, 246, 249, 244, 163, 152, 168, 194, 187, 188, 215,\n",
       "        208, 207, 240, 235, 234, 140, 259, 130, 147, 276, 148, 128, 265,\n",
       "        138, 106, 264, 137, 157, 286, 131, 103, 268, 107, 120, 263, 127,\n",
       "        108, 261, 118, 119, 273, 142, 134, 262, 156, 117, 258, 110, 105,\n",
       "        256, 101, 144, 283, 135, 125, 260, 111, 143, 267,  64, 132, 266,\n",
       "        129, 153, 277, 149, 124, 271, 159, 116, 274, 121, 150, 270, 139,\n",
       "        161, 285, 155, 114, 269, 154, 109, 282, 160, 146, 275, 136, 164,\n",
       "        288, 126, 158, 287, 145, 122, 278, 113, 141, 279, 112, 162, 272,\n",
       "        151, 115, 284, 133, 102, 280, 123,  54, 281, 104,  60,  63,  58,\n",
       "         82,  75,  88,  68,  85,  73,  96,  91,  86,  62,  56,  74,  66,\n",
       "         70,  76,  78,  79,  90,  98,  99,  97,  57,  53,  51,  69,  67,\n",
       "         95,  80,  87,  92,  93, 100,  94,  52,  55,  59,  65,  61,  71,\n",
       "         84,  72,  77,  81,  89,  83,  24,  27,  25,  14,  12,  28,  45,\n",
       "         34,  44,  36,  33,  42,   1,  23,  22,  17,   7,  15,  41,  48,\n",
       "         38,  47,  35,  46,  31,  13,  26,   3,  19,   9,  32,  30,  20,\n",
       "         37,  40,  39,  29,  10,  43,   6,   4,   5,   2,  18,  16,  11,\n",
       "          8,  21])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRIAL_5_MLP.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FIVE NEG LOG LOSS RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'tanh',\n",
       " 'classifier__alpha': 0.001,\n",
       " 'classifier__hidden_layer_sizes': (5,),\n",
       " 'classifier__learning_rate': 'constant',\n",
       " 'classifier__solver': 'adam'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL FIVE NEG LOG LOSS RESULTS ---------------------------------------------\")\n",
    "TRIAL_5_MLP.cv_results_['rank_test_neg_log_loss']\n",
    "TRIAL_5_MLP.cv_results_['params'][ np.argmin(TRIAL_5_MLP.cv_results_['rank_test_neg_log_loss']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FIVE F1 RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'tanh',\n",
       " 'classifier__alpha': 0.0001,\n",
       " 'classifier__hidden_layer_sizes': (5,),\n",
       " 'classifier__learning_rate': 'invscaling',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL FIVE F1 RESULTS ---------------------------------------------\")\n",
    "TRIAL_5_MLP.cv_results_['rank_test_f1_micro']\n",
    "TRIAL_5_MLP.cv_results_['params'][ np.argmin(TRIAL_5_MLP.cv_results_['rank_test_f1_micro']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FIVE ROC AUC RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'tanh',\n",
       " 'classifier__alpha': 0.001,\n",
       " 'classifier__hidden_layer_sizes': (20,),\n",
       " 'classifier__learning_rate': 'invscaling',\n",
       " 'classifier__solver': 'adam'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL FIVE ROC AUC RESULTS ---------------------------------------------\")\n",
    "TRIAL_5_MLP.cv_results_['rank_test_roc_auc_ovo']\n",
    "TRIAL_5_MLP.cv_results_['params'][ np.argmin(TRIAL_5_MLP.cv_results_['rank_test_roc_auc_ovo']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 5 MLP using best NEG LOG LOSS hyperparameters :0.9319\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 5 MLP using best F1 hyperparameters :0.93075\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 5 MLP using best ROC_AUC hyperparameters :0.9319\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best NEG LOG LOSS results to train on test set, \n",
    "p = TRIAL_5_MLP.cv_results_['params'][ np.argmin(TRIAL_5_MLP.cv_results_['rank_test_neg_log_loss']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train__neg_log_loss = pipe.predict(X_train)\n",
    "y_pred_test_neg_log_loss = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 5 MLP using best NEG LOG LOSS hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_neg_log_loss)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best F1 results to train on test set, LR_Trial 1\n",
    "p = TRIAL_5_MLP.cv_results_['params'][ np.argmin(TRIAL_5_MLP.cv_results_['rank_test_f1_micro']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train_roc_auc = pipe.predict(X_train)\n",
    "y_pred_test_f1 = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 5 MLP using best F1 hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_f1)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best ROC_AUC results to train on test set, LR_Trial 1\n",
    "p  = TRIAL_5_MLP.cv_results_['params'][ np.argmin(TRIAL_5_MLP.cv_results_['rank_test_roc_auc_ovo']) ]\n",
    "pipe.set_params(**p)\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred_train_roc_auc = pipe.predict(X_train)\n",
    "y_pred_test_roc_auc = pipe.predict(X_test)\n",
    "print('Accuracy of Trial 5 MLP using best ROC_AUC hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred_test_roc_auc)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
