{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries;\n",
    "import numpy as np\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# use seaborn plotting defaults\n",
    "import seaborn as sns; sns.set_style('white')\n",
    "\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from numpy import genfromtxt\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import load_digits, make_blobs\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, plot_confusion_matrix\n",
    "from sklearn.model_selection import KFold, train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, StandardScaler \n",
    "from sklearn.preprocessing import RobustScaler, Normalizer, QuantileTransformer, PowerTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import seaborn as sns\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blue_win</th>\n",
       "      <th>blueGold</th>\n",
       "      <th>blueMinionsKilled</th>\n",
       "      <th>blueJungleMinionsKilled</th>\n",
       "      <th>blueAvgLevel</th>\n",
       "      <th>redGold</th>\n",
       "      <th>redMinionsKilled</th>\n",
       "      <th>redJungleMinionsKilled</th>\n",
       "      <th>redAvgLevel</th>\n",
       "      <th>blueChampKills</th>\n",
       "      <th>blueHeraldKills</th>\n",
       "      <th>blueDragonKills</th>\n",
       "      <th>blueTowersDestroyed</th>\n",
       "      <th>redChampKills</th>\n",
       "      <th>redHeraldKills</th>\n",
       "      <th>redDragonKills</th>\n",
       "      <th>redTowersDestroyed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>24575.0</td>\n",
       "      <td>349.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>25856.0</td>\n",
       "      <td>346.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>27210.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>28765.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>32048.0</td>\n",
       "      <td>346.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>25305.0</td>\n",
       "      <td>293.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>20261.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>8.2</td>\n",
       "      <td>30429.0</td>\n",
       "      <td>356.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>30217.0</td>\n",
       "      <td>376.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>9.8</td>\n",
       "      <td>23889.0</td>\n",
       "      <td>334.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>8.8</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   blue_win  blueGold  blueMinionsKilled  blueJungleMinionsKilled  \\\n",
       "0         0   24575.0              349.0                     89.0   \n",
       "1         0   27210.0              290.0                     36.0   \n",
       "2         1   32048.0              346.0                     92.0   \n",
       "3         0   20261.0              223.0                     60.0   \n",
       "4         1   30217.0              376.0                    110.0   \n",
       "\n",
       "   blueAvgLevel  redGold  redMinionsKilled  redJungleMinionsKilled  \\\n",
       "0           8.6  25856.0             346.0                    80.0   \n",
       "1           9.0  28765.0             294.0                    92.0   \n",
       "2           9.4  25305.0             293.0                    84.0   \n",
       "3           8.2  30429.0             356.0                   107.0   \n",
       "4           9.8  23889.0             334.0                    60.0   \n",
       "\n",
       "   redAvgLevel  blueChampKills  blueHeraldKills  blueDragonKills  \\\n",
       "0          9.2             6.0              1.0              0.0   \n",
       "1          9.4            20.0              0.0              0.0   \n",
       "2          9.4            17.0              3.0              0.0   \n",
       "3          9.4             7.0              0.0              0.0   \n",
       "4          8.8            16.0              3.0              0.0   \n",
       "\n",
       "   blueTowersDestroyed  redChampKills  redHeraldKills  redDragonKills  \\\n",
       "0                  1.0           12.0             2.0             0.0   \n",
       "1                  0.0           19.0             2.0             0.0   \n",
       "2                  0.0           11.0             0.0             0.0   \n",
       "3                  3.0           16.0             3.0             0.0   \n",
       "4                  0.0            8.0             0.0             0.0   \n",
       "\n",
       "   redTowersDestroyed  \n",
       "0                 1.0  \n",
       "1                 0.0  \n",
       "2                 4.0  \n",
       "3                 0.0  \n",
       "4                 2.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leagueData = pd.read_csv('LeagueMatches.csv')\n",
    "del leagueData['Unnamed: 0']\n",
    "del leagueData['matchId']\n",
    "leagueData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = leagueData.iloc[:,1:16].values\n",
    "y = leagueData.iloc[:,0].values\n",
    "#To be used for validation purposes with best model for each trial 5000 train and 45000 test\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.90, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTI LAYERED PERCEPTRON TRIAL ONE ON LEAGUE OF LEGENDS DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    8.0s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:    9.4s\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:   10.9s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:   11.7s\n",
      "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed:   12.9s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   15.2s\n",
      "[Parallel(n_jobs=-1)]: Done 189 tasks      | elapsed:   16.1s\n",
      "[Parallel(n_jobs=-1)]: Done 210 tasks      | elapsed:   17.3s\n",
      "[Parallel(n_jobs=-1)]: Done 233 tasks      | elapsed:   20.1s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:   21.4s\n",
      "[Parallel(n_jobs=-1)]: Done 281 tasks      | elapsed:   23.5s\n",
      "[Parallel(n_jobs=-1)]: Done 306 tasks      | elapsed:   25.6s\n",
      "[Parallel(n_jobs=-1)]: Done 333 tasks      | elapsed:   28.1s\n",
      "[Parallel(n_jobs=-1)]: Done 360 tasks      | elapsed:   30.7s\n",
      "[Parallel(n_jobs=-1)]: Done 389 tasks      | elapsed:   32.4s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:   35.9s\n",
      "[Parallel(n_jobs=-1)]: Done 449 tasks      | elapsed:   37.7s\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:   40.9s\n",
      "[Parallel(n_jobs=-1)]: Done 513 tasks      | elapsed:   44.4s\n",
      "[Parallel(n_jobs=-1)]: Done 546 tasks      | elapsed:   47.2s\n",
      "[Parallel(n_jobs=-1)]: Done 581 tasks      | elapsed:   50.2s\n",
      "[Parallel(n_jobs=-1)]: Done 616 tasks      | elapsed:   53.7s\n",
      "[Parallel(n_jobs=-1)]: Done 653 tasks      | elapsed:   56.7s\n",
      "[Parallel(n_jobs=-1)]: Done 690 tasks      | elapsed:   59.9s\n",
      "[Parallel(n_jobs=-1)]: Done 729 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 809 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 850 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 893 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 936 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 981 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1026 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1073 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1120 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1169 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1269 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1320 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1373 tasks      | elapsed:  2.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL ONE RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1440 out of 1440 | elapsed:  2.4min finished\n"
     ]
    }
   ],
   "source": [
    "ITERATIONS = 1\n",
    "for i in range(1):\n",
    "    #For reach \"trial\", randomly select 5000 data points\n",
    "    random5000DataPoints = leagueData.sample(n = 5000, replace = True)\n",
    "    #From that 5000, create x set and y set\n",
    "    xSet = random5000DataPoints.iloc[:,1:17].values\n",
    "    ySet = random5000DataPoints.iloc[:,0].values\n",
    "    # Run GridSearch to find best hyperparameters per 5000 data points \n",
    "    \n",
    "    pipe = Pipeline([('std', StandardScaler()),('classifier', MLPClassifier(max_iter = 250))])\n",
    "\n",
    "    search_space = [\n",
    "                {\n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                {\n",
    "                 \n",
    "                  \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['sgd'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                \n",
    "                {\n",
    "                \n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['adam'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                ]\n",
    "    \n",
    "    runHyperParamSearch = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=5), \n",
    "                          scoring=['recall_micro', 'f1_micro','precision_micro','roc_auc_ovo','jaccard','neg_log_loss'], \n",
    "                          refit=False, verbose=10, n_jobs = -1)\n",
    "    TRIAL_1_MLP = runHyperParamSearch.fit(xSet, ySet)\n",
    "    print(\"---------------------------------TRIAL ONE RESULTS ---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.59221268, 0.59541178, 0.60402465, 0.84482508, 0.89356775,\n",
       "        0.88736343, 1.42082396, 1.4239234 , 1.41171546, 1.7648181 ,\n",
       "        1.7181766 , 1.69896083, 0.80399184, 0.64155059, 0.60311742,\n",
       "        0.81380124, 0.87535386, 0.88636317, 1.27329545, 1.3654748 ,\n",
       "        1.37548232, 1.69195466, 1.65602283, 1.55493703, 0.7342289 ,\n",
       "        0.64755607, 0.61943226, 0.79958682, 0.83741989, 0.90127654,\n",
       "        1.30512586, 1.32314115, 1.40330796, 1.73959575, 1.76732068,\n",
       "        1.70066109, 0.72292061, 0.60422006, 0.63804936, 0.89396996,\n",
       "        0.90728049, 0.88496189, 1.35446577, 1.41762099, 1.50519466,\n",
       "        1.75570965, 1.7364933 , 1.70336246, 0.80849371, 0.69279556,\n",
       "        0.66567082, 0.94981718, 0.98464584, 1.05370488, 1.491084  ,\n",
       "        1.59006677, 1.60448012, 2.0435576 , 1.94857516, 1.95468192,\n",
       "        0.8079937 , 0.73263001, 0.73823442, 1.02087817, 1.03548999,\n",
       "        1.06141324, 1.5236105 , 1.58296046, 1.60888357, 2.1461473 ,\n",
       "        2.15014782, 2.10351024, 0.81750388, 0.71001201, 0.6906951 ,\n",
       "        0.96302814, 1.0151742 , 1.02618318, 1.68795218, 1.70006161,\n",
       "        1.59477191, 1.95958505, 2.09740219, 2.14464436, 0.82030525,\n",
       "        0.71581607, 0.68428779, 0.97974243, 0.96953454, 1.02378139,\n",
       "        1.49498668, 1.58095808, 1.62449555, 1.93986669, 2.01603103,\n",
       "        2.02534051, 2.18307719, 0.16494126, 2.33690968, 1.71227226,\n",
       "        0.17294869, 2.35852866, 1.70146322, 0.19176464, 2.26875124,\n",
       "        1.76691937, 0.24631186, 2.34571681, 1.83677988, 0.18025508,\n",
       "        2.31068649, 1.72498369, 0.20457578, 2.28686695, 1.69786086,\n",
       "        0.17705202, 2.40857177, 1.75540938, 0.2225913 , 2.31759248,\n",
       "        1.85999932, 0.21508517, 2.30548296, 1.72168059, 0.20988083,\n",
       "        2.19118433, 1.78093195, 0.22048993, 2.3056828 , 1.71457396,\n",
       "        0.18706107, 2.32810221, 1.89913335, 0.15813603, 2.27605791,\n",
       "        1.63670101, 0.16013794, 2.21670628, 1.76511798, 0.22549419,\n",
       "        2.29167075, 1.70816965, 0.21798754, 2.41057358, 1.01427197,\n",
       "        0.33779087, 1.58736477, 1.03939376, 0.85443497, 1.51460223,\n",
       "        0.80579338, 1.28270326, 1.68695087, 0.88285971, 1.6933557 ,\n",
       "        1.49708724, 1.0513042 , 0.49602642, 1.68444848, 0.86974826,\n",
       "        1.22064953, 1.52761369, 0.84602747, 1.9256566 , 1.50929804,\n",
       "        0.89366899, 1.91104312, 1.47977166, 1.10454941, 0.65085974,\n",
       "        1.59367023, 0.88416023, 1.37468262, 1.44764562, 0.8723505 ,\n",
       "        2.22030954, 1.47286749, 0.76505804, 1.22925758, 1.52030721,\n",
       "        1.1396801 , 1.34645786, 1.55733967, 0.93710575, 0.73092847,\n",
       "        1.54422836, 0.92339454, 1.51520333, 1.44474277, 0.85523558,\n",
       "        1.23746457, 1.50249162, 2.10651083, 1.88622169, 1.7688211 ,\n",
       "        1.52771392, 1.41731877, 1.65782595, 1.38689303, 1.35836864,\n",
       "        1.30762482, 1.06901932, 1.05320573, 1.12586789, 2.15625415,\n",
       "        1.84428644, 1.77923012, 1.56344461, 1.67894344, 1.69595881,\n",
       "        1.57235246, 1.30582333, 1.44544291, 1.18081565, 1.39399853,\n",
       "        1.23015752, 1.74299889, 1.71177197, 1.77202425, 1.48517728,\n",
       "        1.41751895, 1.38649244, 1.30051837, 1.15919685, 1.39109612,\n",
       "        1.45164866, 1.08052926, 1.11826138, 1.6966588 , 1.61518936,\n",
       "        1.47516861, 1.32824264, 1.24937429, 1.31833415, 1.26518817,\n",
       "        1.06801877, 0.97724056, 1.23366127, 1.22165132, 1.22745547,\n",
       "        1.95177855, 1.95057669, 1.78733692, 2.57691655, 2.87146988,\n",
       "        2.74235864, 3.10997491, 3.17813306, 2.92191267, 3.2669095 ,\n",
       "        3.29393253, 3.28852844, 2.00112133, 1.76131487, 1.80285006,\n",
       "        2.83143463, 2.42818823, 2.6977201 , 3.13599739, 3.1512095 ,\n",
       "        3.11087527, 3.30744424, 3.35838861, 3.27191372, 1.57115154,\n",
       "        1.62639847, 1.78063159, 2.52016749, 2.61855197, 2.85295353,\n",
       "        3.08715496, 3.1368978 , 3.21776762, 3.27611651, 3.25009475,\n",
       "        3.34317484, 1.75490932, 1.69375634, 1.61538935, 2.22961774,\n",
       "        2.34031281, 2.27265496, 3.00668578, 2.70042205, 2.5000505 ,\n",
       "        3.22167039, 3.05072298, 2.72143974]),\n",
       " 'std_fit_time': array([0.00837246, 0.01393429, 0.0194507 , 0.0265704 , 0.0399183 ,\n",
       "        0.03147409, 0.13130819, 0.05161123, 0.06572327, 0.09253556,\n",
       "        0.04859358, 0.05271466, 0.06060746, 0.02093962, 0.01579781,\n",
       "        0.0241033 , 0.03881123, 0.02579758, 0.04747107, 0.05013212,\n",
       "        0.06010892, 0.07020804, 0.0442225 , 0.03006434, 0.04460931,\n",
       "        0.0216448 , 0.02291532, 0.0204368 , 0.02652163, 0.04995219,\n",
       "        0.04282646, 0.03137874, 0.06394553, 0.06257808, 0.04721138,\n",
       "        0.07877624, 0.11755225, 0.04751834, 0.02499179, 0.02701946,\n",
       "        0.06877392, 0.09065356, 0.07217009, 0.05674582, 0.06760446,\n",
       "        0.01725397, 0.08779818, 0.05112679, 0.10963585, 0.01370388,\n",
       "        0.02019788, 0.0288816 , 0.02275985, 0.02190837, 0.03143722,\n",
       "        0.04880793, 0.0347434 , 0.1444968 , 0.05331123, 0.06721255,\n",
       "        0.06313917, 0.01627344, 0.03871898, 0.05492822, 0.01926243,\n",
       "        0.06999006, 0.06021634, 0.08260003, 0.01797226, 0.18925015,\n",
       "        0.08772371, 0.21229924, 0.06934971, 0.01291874, 0.01369815,\n",
       "        0.01426652, 0.06296206, 0.04731991, 0.1782679 , 0.20241958,\n",
       "        0.14931144, 0.08150878, 0.15708547, 0.18742044, 0.05823461,\n",
       "        0.03266608, 0.00981862, 0.03409366, 0.01170855, 0.05160713,\n",
       "        0.03675316, 0.00857874, 0.04090665, 0.04933899, 0.16323701,\n",
       "        0.13872258, 0.32884152, 0.01225548, 0.03482802, 0.1329692 ,\n",
       "        0.0321842 , 0.13849281, 0.08636515, 0.02629857, 0.05760193,\n",
       "        0.05939163, 0.05487125, 0.08786235, 0.23253679, 0.03227781,\n",
       "        0.03581609, 0.05713127, 0.05488814, 0.07909387, 0.13030192,\n",
       "        0.00505842, 0.09845162, 0.06866266, 0.05567501, 0.10069497,\n",
       "        0.21286107, 0.10802134, 0.04793584, 0.13693985, 0.05959645,\n",
       "        0.08838774, 0.13976665, 0.09701463, 0.06301715, 0.10162474,\n",
       "        0.01101123, 0.04669809, 0.22236836, 0.0190349 , 0.06384794,\n",
       "        0.10372149, 0.01482829, 0.09822296, 0.08603643, 0.09327115,\n",
       "        0.06385264, 0.10303717, 0.01528312, 0.09114003, 0.13493684,\n",
       "        0.1158877 , 0.36864365, 0.15049692, 0.59449182, 0.20466903,\n",
       "        0.15122458, 0.36564207, 0.17297999, 0.13400094, 1.14134392,\n",
       "        0.15551122, 0.18776734, 0.18762924, 0.13886664, 0.18674318,\n",
       "        0.96975762, 0.2174634 , 0.10051066, 0.75751062, 0.13034497,\n",
       "        0.14191813, 0.76141078, 0.10861001, 0.14369623, 0.29524123,\n",
       "        0.25769757, 0.13327886, 0.71853356, 0.10627636, 0.11186983,\n",
       "        0.59890634, 0.150791  , 0.07500885, 0.94217969, 0.12428224,\n",
       "        0.30216545, 0.93847101, 0.10113882, 0.10700572, 0.40209009,\n",
       "        0.10631134, 0.09407187, 0.90717209, 0.10522324, 0.04703872,\n",
       "        0.69878229, 0.13709296, 0.25053214, 0.31334581, 0.18163277,\n",
       "        0.16284309, 0.35790619, 0.10197664, 0.16812071, 0.2240051 ,\n",
       "        0.17704595, 0.18475452, 0.14364903, 0.2496071 , 0.2824084 ,\n",
       "        0.14327827, 0.27285979, 0.303162  , 0.14448801, 0.25179408,\n",
       "        0.14731649, 0.21743718, 0.3267032 , 0.20723645, 0.14031762,\n",
       "        0.16271601, 0.17620535, 0.14182796, 0.13191829, 0.22503106,\n",
       "        0.17413729, 0.2836647 , 0.23332898, 0.17508168, 0.31414979,\n",
       "        0.22056899, 0.19134823, 0.2002907 , 0.1706569 , 0.24652432,\n",
       "        0.08786348, 0.30778087, 0.0810063 , 0.1348155 , 0.14915258,\n",
       "        0.12383149, 0.14977141, 0.22589157, 0.16974691, 0.14451858,\n",
       "        0.47147799, 0.44364519, 0.17995219, 0.34861135, 0.0446052 ,\n",
       "        0.17295453, 0.05306371, 0.08001052, 0.53288717, 0.05432988,\n",
       "        0.08071375, 0.06108089, 0.15048662, 0.34323776, 0.44027039,\n",
       "        0.07786235, 0.62959601, 0.34200403, 0.02431608, 0.05843056,\n",
       "        0.06677062, 0.08046155, 0.0521839 , 0.06160217, 0.29254351,\n",
       "        0.49486821, 0.27240297, 0.30184766, 0.3073088 , 0.07840642,\n",
       "        0.06400739, 0.04548521, 0.02984319, 0.06291734, 0.06318268,\n",
       "        0.04903309, 0.30338741, 0.2768765 , 0.16454567, 0.43482366,\n",
       "        0.60919791, 0.53243197, 0.17379356, 0.46887828, 0.72763327,\n",
       "        0.04887048, 0.10115084, 0.11154287]),\n",
       " 'mean_score_time': array([0.00970569, 0.0126121 , 0.01370664, 0.01811495, 0.01341257,\n",
       "        0.01251178, 0.01491237, 0.01621532, 0.01651449, 0.01621475,\n",
       "        0.01411328, 0.01541309, 0.01331124, 0.01211004, 0.01321239,\n",
       "        0.01230989, 0.01211209, 0.01341166, 0.02021704, 0.01651254,\n",
       "        0.01671538, 0.01941614, 0.01291094, 0.01301112, 0.01170983,\n",
       "        0.01120996, 0.01121159, 0.01211114, 0.0126112 , 0.02161794,\n",
       "        0.0248199 , 0.01461058, 0.01891718, 0.01591439, 0.01411233,\n",
       "        0.01311235, 0.0129117 , 0.01160975, 0.01361117, 0.01151109,\n",
       "        0.01381269, 0.01361184, 0.01791687, 0.0194159 , 0.01581388,\n",
       "        0.01691446, 0.01381202, 0.01271162, 0.01181202, 0.01091003,\n",
       "        0.01280975, 0.01251178, 0.01511464, 0.02492161, 0.01691408,\n",
       "        0.01541324, 0.01621432, 0.01691494, 0.01451221, 0.0167129 ,\n",
       "        0.01451349, 0.01381211, 0.01141167, 0.01251125, 0.01411219,\n",
       "        0.01421256, 0.01891675, 0.01551313, 0.01641483, 0.01841574,\n",
       "        0.01491456, 0.01350856, 0.01350985, 0.01140995, 0.01121025,\n",
       "        0.01261096, 0.01891589, 0.01451378, 0.01751485, 0.01351185,\n",
       "        0.01441417, 0.01771512, 0.01351242, 0.01311083, 0.01181083,\n",
       "        0.01070981, 0.01060972, 0.01351175, 0.01351204, 0.01451173,\n",
       "        0.01671443, 0.01421261, 0.01731606, 0.01511331, 0.012011  ,\n",
       "        0.01130958, 0.00930805, 0.00970855, 0.0098084 , 0.00930853,\n",
       "        0.00950794, 0.0096086 , 0.00990887, 0.00980906, 0.01080942,\n",
       "        0.01140966, 0.00980797, 0.00980864, 0.00910716, 0.00960855,\n",
       "        0.01000881, 0.01030841, 0.00930791, 0.00970845, 0.01040893,\n",
       "        0.00970826, 0.01000881, 0.01040883, 0.00970826, 0.00960841,\n",
       "        0.0099082 , 0.00930834, 0.01030903, 0.00940809, 0.00930781,\n",
       "        0.00930824, 0.00970883, 0.00990853, 0.01020842, 0.01150975,\n",
       "        0.01010866, 0.00960841, 0.00980844, 0.0097084 , 0.00900784,\n",
       "        0.00960755, 0.00950837, 0.00970831, 0.00990872, 0.00950789,\n",
       "        0.00970817, 0.01030865, 0.01050935, 0.01010876, 0.00930791,\n",
       "        0.00910778, 0.01161003, 0.00990834, 0.00980825, 0.00990868,\n",
       "        0.01020904, 0.01000857, 0.01000881, 0.01010885, 0.01060963,\n",
       "        0.01010895, 0.0093081 , 0.00960808, 0.00930786, 0.00960841,\n",
       "        0.01261082, 0.00970836, 0.01000867, 0.01020908, 0.01010919,\n",
       "        0.01070971, 0.01020894, 0.01060977, 0.00920801, 0.00940828,\n",
       "        0.00950804, 0.00950823, 0.01170979, 0.00980825, 0.01000857,\n",
       "        0.00990915, 0.00990844, 0.01080914, 0.01030846, 0.01201024,\n",
       "        0.00930848, 0.00940785, 0.00930839, 0.0094079 , 0.00970826,\n",
       "        0.01000814, 0.01030903, 0.00980821, 0.01000891, 0.01030889,\n",
       "        0.01000819, 0.01080894, 0.00950823, 0.00950837, 0.00920792,\n",
       "        0.00940833, 0.00950818, 0.0093082 , 0.00950799, 0.00980806,\n",
       "        0.01010842, 0.0099082 , 0.00990853, 0.01110973, 0.00970831,\n",
       "        0.00910802, 0.00900774, 0.0102088 , 0.00970883, 0.00930829,\n",
       "        0.01020885, 0.01010852, 0.00960817, 0.01000886, 0.00990882,\n",
       "        0.01000857, 0.00910802, 0.00960865, 0.00950789, 0.01010876,\n",
       "        0.00920811, 0.01000876, 0.00980892, 0.00960832, 0.00970783,\n",
       "        0.00990849, 0.00970902, 0.00990834, 0.00930791, 0.00910788,\n",
       "        0.00980835, 0.0097086 , 0.00940852, 0.00960836, 0.00990891,\n",
       "        0.01000834, 0.00960817, 0.00970793, 0.00980806, 0.01120982,\n",
       "        0.00930805, 0.00970836, 0.00900798, 0.01020889, 0.00990872,\n",
       "        0.0099082 , 0.01080952, 0.01100945, 0.01040921, 0.01090951,\n",
       "        0.01070962, 0.01080875, 0.00890775, 0.00900769, 0.00930824,\n",
       "        0.01030879, 0.00990858, 0.0099082 , 0.01050935, 0.01000876,\n",
       "        0.01090922, 0.01161003, 0.01020889, 0.0101088 , 0.00920792,\n",
       "        0.00950866, 0.00940814, 0.00990877, 0.0113102 , 0.01000896,\n",
       "        0.01181026, 0.01020908, 0.0102088 , 0.01111012, 0.01341195,\n",
       "        0.01040912, 0.00940828, 0.00920806, 0.0093081 , 0.00950799,\n",
       "        0.01040893, 0.00960813, 0.01050925, 0.00990896, 0.01020889,\n",
       "        0.00910745, 0.00750666, 0.00610528]),\n",
       " 'std_score_time': array([2.41503809e-04, 2.76687725e-03, 3.61975361e-03, 6.65736791e-03,\n",
       "        1.28034274e-03, 1.37891084e-03, 1.24213406e-03, 4.32124545e-03,\n",
       "        1.61423340e-03, 1.80669174e-03, 1.85627328e-03, 5.09769094e-03,\n",
       "        1.91458680e-03, 1.82953723e-03, 1.75011299e-03, 6.00688418e-04,\n",
       "        8.00645633e-04, 1.53059345e-03, 1.11916630e-02, 8.96134733e-04,\n",
       "        9.27546940e-04, 1.04262781e-02, 2.00926391e-04, 1.09756513e-03,\n",
       "        1.43660150e-03, 9.27003396e-04, 4.00223786e-04, 1.06809128e-03,\n",
       "        1.99890535e-04, 1.46739020e-02, 1.11170781e-02, 1.98531848e-03,\n",
       "        5.18556673e-03, 1.80004544e-03, 1.35717708e-03, 1.15927519e-03,\n",
       "        3.02501218e-03, 1.32018105e-03, 2.29225107e-03, 8.38395332e-04,\n",
       "        2.01734124e-03, 1.20049439e-03, 3.37029385e-03, 8.59437326e-03,\n",
       "        1.60197041e-03, 2.03453840e-03, 1.43694732e-03, 7.49513641e-04,\n",
       "        8.73661315e-04, 5.83478647e-04, 1.50513372e-03, 2.34863716e-03,\n",
       "        3.12311603e-03, 6.62594966e-03, 2.35492984e-03, 1.02146523e-03,\n",
       "        1.83360208e-03, 4.66824639e-03, 2.53187252e-03, 3.65793217e-03,\n",
       "        4.91419084e-03, 2.01733048e-03, 7.35456589e-04, 6.33242134e-04,\n",
       "        2.01219709e-03, 1.28946409e-03, 7.56586696e-03, 1.05041609e-03,\n",
       "        2.48246925e-03, 3.02522177e-03, 1.39447759e-03, 1.84582529e-03,\n",
       "        4.17229760e-03, 9.18379620e-04, 9.27873583e-04, 1.98683201e-03,\n",
       "        6.45072476e-03, 3.08547998e-03, 5.26600468e-03, 7.74664420e-04,\n",
       "        1.19977004e-03, 7.43274246e-03, 1.79109714e-03, 1.82928151e-03,\n",
       "        6.78536066e-04, 5.10483796e-04, 3.75414522e-04, 2.07549450e-03,\n",
       "        1.81826988e-03, 8.38369792e-04, 5.01067630e-03, 2.65736355e-03,\n",
       "        2.03871565e-03, 2.03585962e-03, 1.64500834e-03, 8.13260674e-04,\n",
       "        4.00758141e-04, 7.48704913e-04, 2.45165981e-04, 2.45632766e-04,\n",
       "        3.16958842e-04, 4.90349681e-04, 5.83102759e-04, 6.00552623e-04,\n",
       "        1.88892899e-03, 1.96144517e-03, 2.45106758e-04, 2.44970355e-04,\n",
       "        2.00009595e-04, 8.00454643e-04, 7.75156273e-04, 1.16726713e-03,\n",
       "        2.45418288e-04, 2.44950946e-04, 1.11496244e-03, 4.00615061e-04,\n",
       "        6.33842418e-04, 6.63463867e-04, 2.44620186e-04, 2.00081610e-04,\n",
       "        8.60901897e-04, 2.45476538e-04, 1.28940835e-03, 1.99962025e-04,\n",
       "        2.44951132e-04, 2.45204930e-04, 2.44639648e-04, 4.90291275e-04,\n",
       "        9.28456724e-04, 2.32577620e-03, 5.83936357e-04, 1.99606540e-04,\n",
       "        2.45204188e-04, 1.16720169e-03, 4.15696997e-07, 2.00272004e-04,\n",
       "        4.47927867e-04, 2.45164868e-04, 3.74431939e-04, 6.21719590e-07,\n",
       "        5.10407572e-04, 6.00465332e-04, 2.00111876e-03, 1.99652316e-04,\n",
       "        6.00552661e-04, 2.00176580e-04, 3.42917054e-03, 5.83829972e-04,\n",
       "        3.99864064e-04, 3.74929056e-04, 4.00210009e-04, 5.48292321e-04,\n",
       "        7.38712949e-07, 3.74699768e-04, 8.00848099e-04, 4.89930677e-04,\n",
       "        2.45380142e-04, 2.00368733e-04, 4.00245363e-04, 3.74126070e-04,\n",
       "        5.71795680e-03, 2.44931824e-04, 5.48118176e-04, 2.44893138e-04,\n",
       "        2.00510507e-04, 6.78289911e-04, 2.45204698e-04, 5.83821813e-04,\n",
       "        2.45242797e-04, 5.83609205e-04, 5.48031284e-04, 3.16506446e-04,\n",
       "        4.41523311e-03, 6.00362475e-04, 5.48248880e-04, 1.99675752e-04,\n",
       "        2.00510394e-04, 7.49080449e-04, 4.00221372e-04, 2.51229640e-03,\n",
       "        4.00316725e-04, 5.83371992e-04, 4.00269773e-04, 2.00581629e-04,\n",
       "        5.10089507e-04, 1.26551706e-03, 6.01029517e-04, 2.45885768e-04,\n",
       "        3.15828246e-04, 2.45203864e-04, 5.56082906e-07, 6.78697137e-04,\n",
       "        5.48422905e-04, 6.33239046e-04, 4.00328988e-04, 2.00200273e-04,\n",
       "        3.16506475e-04, 2.44873992e-04, 1.00701867e-06, 4.00364473e-04,\n",
       "        5.83609088e-04, 2.00152702e-04, 5.83903671e-04, 2.71213418e-03,\n",
       "        9.27999139e-04, 2.00057392e-04, 7.13664510e-07, 9.80600926e-04,\n",
       "        2.44931731e-04, 2.45633043e-04, 6.00695806e-04, 5.83347355e-04,\n",
       "        2.00200159e-04, 5.47987631e-04, 3.74584728e-04, 5.48335800e-04,\n",
       "        1.99938986e-04, 1.20084292e-03, 4.47394684e-04, 1.71612045e-03,\n",
       "        2.45262461e-04, 7.07696217e-04, 2.44620232e-04, 4.90115569e-04,\n",
       "        2.44581144e-04, 1.99938076e-04, 2.44970773e-04, 5.83584605e-04,\n",
       "        4.00507602e-04, 2.00844037e-04, 1.12322300e-03, 3.99613989e-04,\n",
       "        1.99938531e-04, 4.90397787e-04, 5.83397022e-04, 6.33050689e-04,\n",
       "        2.00204248e-04, 2.44698920e-04, 2.45379077e-04, 3.40266232e-03,\n",
       "        2.45242983e-04, 5.10052058e-04, 3.16581860e-04, 1.16682556e-03,\n",
       "        3.75451974e-04, 8.61123602e-04, 2.44678876e-04, 2.02641803e-03,\n",
       "        3.74062737e-04, 6.64110908e-04, 4.00018763e-04, 6.00592375e-04,\n",
       "        1.99890307e-04, 6.21719590e-07, 4.00376331e-04, 6.78992361e-04,\n",
       "        5.83788942e-04, 4.90368817e-04, 8.95402457e-04, 3.16129637e-04,\n",
       "        1.59525291e-03, 1.32023139e-03, 2.44951039e-04, 2.00343494e-04,\n",
       "        5.10473039e-04, 3.16581860e-04, 5.83380118e-04, 5.83535550e-04,\n",
       "        3.35830772e-03, 8.95588673e-04, 2.62178496e-03, 6.78768022e-04,\n",
       "        4.00447987e-04, 5.83878940e-04, 3.40016051e-03, 8.00490512e-04,\n",
       "        5.83568140e-04, 4.00614806e-04, 4.00781787e-04, 4.47661256e-04,\n",
       "        1.24233014e-03, 2.00225427e-04, 7.75217854e-04, 3.74304486e-04,\n",
       "        5.10014699e-04, 8.00955435e-04, 1.45011859e-03, 2.00104735e-04]),\n",
       " 'param_classifier': masked_array(data=[MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__activation': masked_array(data=['logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__alpha': masked_array(data=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__hidden_layer_sizes': masked_array(data=[(5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__learning_rate': masked_array(data=['constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'}],\n",
       " 'split0_test_recall_micro': array([0.786, 0.797, 0.793, 0.776, 0.791, 0.76 , 0.759, 0.763, 0.775,\n",
       "        0.762, 0.763, 0.762, 0.792, 0.794, 0.788, 0.768, 0.774, 0.755,\n",
       "        0.772, 0.749, 0.781, 0.754, 0.756, 0.733, 0.783, 0.796, 0.799,\n",
       "        0.791, 0.781, 0.775, 0.775, 0.753, 0.77 , 0.764, 0.773, 0.749,\n",
       "        0.798, 0.795, 0.795, 0.797, 0.772, 0.774, 0.777, 0.77 , 0.779,\n",
       "        0.784, 0.748, 0.754, 0.803, 0.794, 0.796, 0.772, 0.78 , 0.783,\n",
       "        0.757, 0.752, 0.764, 0.747, 0.753, 0.741, 0.789, 0.794, 0.788,\n",
       "        0.762, 0.787, 0.792, 0.748, 0.755, 0.751, 0.749, 0.768, 0.755,\n",
       "        0.802, 0.798, 0.784, 0.785, 0.764, 0.783, 0.774, 0.778, 0.772,\n",
       "        0.76 , 0.762, 0.739, 0.795, 0.797, 0.796, 0.78 , 0.777, 0.78 ,\n",
       "        0.756, 0.776, 0.77 , 0.756, 0.76 , 0.756, 0.798, 0.635, 0.801,\n",
       "        0.802, 0.507, 0.797, 0.801, 0.493, 0.801, 0.801, 0.509, 0.805,\n",
       "        0.801, 0.701, 0.807, 0.8  , 0.51 , 0.798, 0.795, 0.525, 0.803,\n",
       "        0.803, 0.495, 0.803, 0.798, 0.493, 0.8  , 0.804, 0.514, 0.801,\n",
       "        0.806, 0.507, 0.802, 0.8  , 0.506, 0.802, 0.805, 0.493, 0.795,\n",
       "        0.797, 0.493, 0.801, 0.798, 0.518, 0.803, 0.801, 0.507, 0.793,\n",
       "        0.792, 0.528, 0.793, 0.804, 0.564, 0.799, 0.791, 0.649, 0.806,\n",
       "        0.801, 0.54 , 0.799, 0.799, 0.63 , 0.797, 0.798, 0.46 , 0.79 ,\n",
       "        0.797, 0.566, 0.792, 0.801, 0.691, 0.793, 0.796, 0.603, 0.791,\n",
       "        0.798, 0.449, 0.794, 0.807, 0.561, 0.791, 0.796, 0.662, 0.797,\n",
       "        0.798, 0.518, 0.796, 0.798, 0.673, 0.793, 0.798, 0.747, 0.797,\n",
       "        0.802, 0.565, 0.792, 0.796, 0.802, 0.799, 0.801, 0.802, 0.799,\n",
       "        0.798, 0.802, 0.801, 0.802, 0.797, 0.8  , 0.798, 0.797, 0.802,\n",
       "        0.797, 0.799, 0.801, 0.799, 0.799, 0.801, 0.804, 0.803, 0.802,\n",
       "        0.8  , 0.802, 0.8  , 0.798, 0.797, 0.797, 0.799, 0.8  , 0.796,\n",
       "        0.802, 0.803, 0.8  , 0.799, 0.802, 0.799, 0.8  , 0.8  , 0.8  ,\n",
       "        0.8  , 0.799, 0.802, 0.8  , 0.801, 0.803, 0.799, 0.804, 0.8  ,\n",
       "        0.784, 0.803, 0.804, 0.801, 0.795, 0.787, 0.8  , 0.799, 0.796,\n",
       "        0.796, 0.796, 0.797, 0.807, 0.797, 0.801, 0.802, 0.786, 0.8  ,\n",
       "        0.799, 0.788, 0.792, 0.794, 0.795, 0.804, 0.799, 0.792, 0.801,\n",
       "        0.802, 0.802, 0.804, 0.801, 0.799, 0.789, 0.8  , 0.797, 0.792,\n",
       "        0.797, 0.796, 0.806, 0.787, 0.81 , 0.792, 0.8  , 0.804, 0.803]),\n",
       " 'split1_test_recall_micro': array([0.774, 0.764, 0.778, 0.744, 0.768, 0.771, 0.737, 0.757, 0.752,\n",
       "        0.759, 0.749, 0.763, 0.773, 0.767, 0.777, 0.759, 0.755, 0.773,\n",
       "        0.764, 0.769, 0.792, 0.741, 0.759, 0.746, 0.776, 0.777, 0.777,\n",
       "        0.781, 0.768, 0.774, 0.758, 0.767, 0.754, 0.767, 0.747, 0.755,\n",
       "        0.77 , 0.769, 0.773, 0.754, 0.772, 0.763, 0.777, 0.748, 0.776,\n",
       "        0.745, 0.744, 0.747, 0.781, 0.788, 0.776, 0.772, 0.778, 0.775,\n",
       "        0.76 , 0.756, 0.74 , 0.752, 0.736, 0.756, 0.773, 0.766, 0.778,\n",
       "        0.766, 0.771, 0.776, 0.745, 0.764, 0.78 , 0.752, 0.737, 0.75 ,\n",
       "        0.778, 0.775, 0.778, 0.767, 0.78 , 0.776, 0.762, 0.753, 0.754,\n",
       "        0.73 , 0.748, 0.753, 0.765, 0.776, 0.778, 0.763, 0.772, 0.771,\n",
       "        0.758, 0.767, 0.756, 0.75 , 0.738, 0.716, 0.793, 0.309, 0.797,\n",
       "        0.788, 0.525, 0.791, 0.79 , 0.493, 0.791, 0.789, 0.507, 0.795,\n",
       "        0.789, 0.507, 0.792, 0.788, 0.493, 0.791, 0.792, 0.509, 0.787,\n",
       "        0.792, 0.223, 0.791, 0.79 , 0.487, 0.788, 0.795, 0.507, 0.792,\n",
       "        0.794, 0.507, 0.789, 0.79 , 0.507, 0.795, 0.79 , 0.435, 0.793,\n",
       "        0.788, 0.467, 0.795, 0.789, 0.507, 0.792, 0.792, 0.493, 0.794,\n",
       "        0.794, 0.686, 0.787, 0.796, 0.579, 0.786, 0.784, 0.585, 0.8  ,\n",
       "        0.793, 0.449, 0.788, 0.779, 0.575, 0.776, 0.79 , 0.524, 0.78 ,\n",
       "        0.784, 0.509, 0.792, 0.783, 0.618, 0.782, 0.794, 0.606, 0.783,\n",
       "        0.791, 0.51 , 0.791, 0.784, 0.42 , 0.784, 0.787, 0.656, 0.791,\n",
       "        0.785, 0.441, 0.791, 0.783, 0.584, 0.785, 0.789, 0.574, 0.784,\n",
       "        0.792, 0.668, 0.789, 0.785, 0.785, 0.782, 0.784, 0.782, 0.782,\n",
       "        0.787, 0.782, 0.785, 0.785, 0.784, 0.783, 0.788, 0.783, 0.785,\n",
       "        0.785, 0.783, 0.784, 0.785, 0.786, 0.786, 0.784, 0.785, 0.785,\n",
       "        0.783, 0.783, 0.786, 0.783, 0.784, 0.786, 0.785, 0.785, 0.783,\n",
       "        0.786, 0.785, 0.785, 0.786, 0.785, 0.784, 0.783, 0.786, 0.785,\n",
       "        0.784, 0.786, 0.786, 0.787, 0.786, 0.784, 0.787, 0.78 , 0.784,\n",
       "        0.788, 0.789, 0.783, 0.789, 0.779, 0.778, 0.782, 0.786, 0.786,\n",
       "        0.786, 0.783, 0.781, 0.79 , 0.785, 0.781, 0.797, 0.781, 0.773,\n",
       "        0.789, 0.784, 0.779, 0.783, 0.788, 0.791, 0.791, 0.782, 0.786,\n",
       "        0.78 , 0.776, 0.789, 0.784, 0.778, 0.769, 0.789, 0.783, 0.784,\n",
       "        0.79 , 0.79 , 0.786, 0.788, 0.787, 0.789, 0.791, 0.789, 0.782]),\n",
       " 'split2_test_recall_micro': array([0.793, 0.802, 0.804, 0.785, 0.792, 0.784, 0.776, 0.775, 0.789,\n",
       "        0.771, 0.77 , 0.757, 0.801, 0.801, 0.794, 0.777, 0.789, 0.798,\n",
       "        0.78 , 0.76 , 0.791, 0.781, 0.754, 0.755, 0.802, 0.801, 0.793,\n",
       "        0.779, 0.784, 0.787, 0.775, 0.776, 0.782, 0.761, 0.759, 0.765,\n",
       "        0.794, 0.8  , 0.801, 0.783, 0.786, 0.774, 0.781, 0.784, 0.773,\n",
       "        0.779, 0.774, 0.762, 0.802, 0.803, 0.787, 0.791, 0.79 , 0.786,\n",
       "        0.769, 0.774, 0.773, 0.755, 0.757, 0.745, 0.801, 0.799, 0.805,\n",
       "        0.782, 0.794, 0.779, 0.763, 0.781, 0.782, 0.736, 0.775, 0.767,\n",
       "        0.803, 0.8  , 0.793, 0.79 , 0.77 , 0.79 , 0.762, 0.78 , 0.772,\n",
       "        0.776, 0.759, 0.768, 0.802, 0.789, 0.782, 0.765, 0.781, 0.782,\n",
       "        0.786, 0.761, 0.779, 0.754, 0.783, 0.759, 0.822, 0.507, 0.817,\n",
       "        0.818, 0.626, 0.813, 0.817, 0.511, 0.811, 0.813, 0.507, 0.816,\n",
       "        0.811, 0.518, 0.808, 0.815, 0.453, 0.814, 0.814, 0.351, 0.813,\n",
       "        0.812, 0.5  , 0.818, 0.817, 0.507, 0.807, 0.815, 0.493, 0.81 ,\n",
       "        0.813, 0.493, 0.811, 0.818, 0.635, 0.817, 0.812, 0.519, 0.811,\n",
       "        0.808, 0.408, 0.817, 0.814, 0.767, 0.817, 0.816, 0.346, 0.81 ,\n",
       "        0.801, 0.69 , 0.807, 0.808, 0.746, 0.806, 0.81 , 0.594, 0.802,\n",
       "        0.803, 0.679, 0.804, 0.808, 0.661, 0.808, 0.81 , 0.618, 0.813,\n",
       "        0.805, 0.512, 0.811, 0.812, 0.659, 0.81 , 0.807, 0.604, 0.798,\n",
       "        0.805, 0.718, 0.803, 0.811, 0.547, 0.815, 0.809, 0.716, 0.804,\n",
       "        0.8  , 0.735, 0.808, 0.804, 0.685, 0.815, 0.808, 0.594, 0.81 ,\n",
       "        0.806, 0.718, 0.804, 0.807, 0.806, 0.806, 0.812, 0.811, 0.809,\n",
       "        0.809, 0.807, 0.808, 0.808, 0.809, 0.807, 0.808, 0.808, 0.805,\n",
       "        0.806, 0.811, 0.806, 0.808, 0.807, 0.81 , 0.808, 0.808, 0.809,\n",
       "        0.805, 0.81 , 0.81 , 0.807, 0.808, 0.807, 0.808, 0.815, 0.808,\n",
       "        0.807, 0.811, 0.809, 0.811, 0.809, 0.811, 0.81 , 0.812, 0.81 ,\n",
       "        0.81 , 0.812, 0.81 , 0.81 , 0.811, 0.808, 0.806, 0.799, 0.807,\n",
       "        0.801, 0.797, 0.804, 0.804, 0.806, 0.8  , 0.795, 0.801, 0.815,\n",
       "        0.797, 0.802, 0.794, 0.793, 0.808, 0.799, 0.795, 0.802, 0.806,\n",
       "        0.811, 0.813, 0.801, 0.809, 0.806, 0.8  , 0.799, 0.807, 0.8  ,\n",
       "        0.814, 0.799, 0.798, 0.805, 0.797, 0.798, 0.802, 0.808, 0.81 ,\n",
       "        0.81 , 0.803, 0.806, 0.806, 0.798, 0.8  , 0.797, 0.798, 0.799]),\n",
       " 'split3_test_recall_micro': array([0.777, 0.784, 0.779, 0.768, 0.757, 0.768, 0.757, 0.771, 0.755,\n",
       "        0.761, 0.742, 0.759, 0.782, 0.782, 0.786, 0.765, 0.76 , 0.779,\n",
       "        0.753, 0.76 , 0.755, 0.754, 0.753, 0.755, 0.777, 0.778, 0.776,\n",
       "        0.752, 0.78 , 0.775, 0.759, 0.772, 0.76 , 0.747, 0.775, 0.758,\n",
       "        0.769, 0.79 , 0.779, 0.785, 0.78 , 0.768, 0.763, 0.768, 0.753,\n",
       "        0.77 , 0.769, 0.765, 0.792, 0.784, 0.771, 0.763, 0.763, 0.767,\n",
       "        0.754, 0.765, 0.766, 0.74 , 0.741, 0.727, 0.776, 0.783, 0.765,\n",
       "        0.757, 0.772, 0.776, 0.762, 0.75 , 0.759, 0.738, 0.732, 0.76 ,\n",
       "        0.776, 0.784, 0.788, 0.753, 0.757, 0.779, 0.755, 0.736, 0.747,\n",
       "        0.766, 0.759, 0.755, 0.796, 0.784, 0.791, 0.776, 0.77 , 0.775,\n",
       "        0.77 , 0.755, 0.75 , 0.754, 0.759, 0.76 , 0.787, 0.591, 0.785,\n",
       "        0.787, 0.476, 0.788, 0.785, 0.493, 0.785, 0.782, 0.496, 0.782,\n",
       "        0.783, 0.493, 0.783, 0.785, 0.493, 0.791, 0.784, 0.572, 0.782,\n",
       "        0.788, 0.507, 0.782, 0.785, 0.478, 0.787, 0.78 , 0.493, 0.786,\n",
       "        0.787, 0.576, 0.785, 0.782, 0.437, 0.788, 0.792, 0.507, 0.783,\n",
       "        0.784, 0.594, 0.786, 0.789, 0.507, 0.784, 0.785, 0.507, 0.784,\n",
       "        0.791, 0.68 , 0.78 , 0.789, 0.649, 0.788, 0.785, 0.723, 0.793,\n",
       "        0.792, 0.723, 0.784, 0.787, 0.606, 0.787, 0.79 , 0.732, 0.787,\n",
       "        0.786, 0.621, 0.79 , 0.781, 0.552, 0.782, 0.779, 0.565, 0.782,\n",
       "        0.787, 0.599, 0.786, 0.79 , 0.653, 0.789, 0.785, 0.719, 0.785,\n",
       "        0.779, 0.479, 0.785, 0.784, 0.583, 0.791, 0.789, 0.723, 0.788,\n",
       "        0.789, 0.676, 0.788, 0.783, 0.785, 0.782, 0.788, 0.787, 0.783,\n",
       "        0.785, 0.784, 0.785, 0.788, 0.785, 0.785, 0.784, 0.788, 0.782,\n",
       "        0.786, 0.784, 0.787, 0.782, 0.785, 0.785, 0.787, 0.783, 0.785,\n",
       "        0.787, 0.783, 0.782, 0.784, 0.783, 0.786, 0.787, 0.787, 0.784,\n",
       "        0.784, 0.785, 0.785, 0.785, 0.786, 0.785, 0.786, 0.785, 0.784,\n",
       "        0.784, 0.785, 0.786, 0.786, 0.785, 0.784, 0.782, 0.781, 0.786,\n",
       "        0.782, 0.789, 0.786, 0.785, 0.783, 0.78 , 0.79 , 0.783, 0.792,\n",
       "        0.785, 0.785, 0.786, 0.786, 0.778, 0.788, 0.789, 0.778, 0.787,\n",
       "        0.79 , 0.786, 0.781, 0.775, 0.778, 0.787, 0.779, 0.781, 0.783,\n",
       "        0.784, 0.783, 0.783, 0.781, 0.782, 0.786, 0.784, 0.785, 0.793,\n",
       "        0.782, 0.784, 0.784, 0.782, 0.785, 0.782, 0.785, 0.781, 0.786]),\n",
       " 'split4_test_recall_micro': array([0.772, 0.759, 0.756, 0.773, 0.748, 0.766, 0.727, 0.758, 0.744,\n",
       "        0.748, 0.754, 0.753, 0.766, 0.769, 0.766, 0.744, 0.753, 0.762,\n",
       "        0.739, 0.761, 0.753, 0.766, 0.771, 0.743, 0.761, 0.77 , 0.762,\n",
       "        0.755, 0.751, 0.762, 0.74 , 0.749, 0.733, 0.748, 0.742, 0.753,\n",
       "        0.759, 0.764, 0.771, 0.764, 0.766, 0.757, 0.751, 0.764, 0.748,\n",
       "        0.758, 0.755, 0.746, 0.762, 0.779, 0.765, 0.768, 0.762, 0.772,\n",
       "        0.741, 0.772, 0.756, 0.748, 0.726, 0.743, 0.763, 0.785, 0.769,\n",
       "        0.749, 0.772, 0.757, 0.752, 0.738, 0.744, 0.749, 0.741, 0.748,\n",
       "        0.772, 0.766, 0.774, 0.764, 0.762, 0.777, 0.741, 0.754, 0.754,\n",
       "        0.744, 0.744, 0.734, 0.758, 0.762, 0.761, 0.763, 0.778, 0.763,\n",
       "        0.763, 0.758, 0.749, 0.725, 0.754, 0.759, 0.769, 0.505, 0.775,\n",
       "        0.769, 0.382, 0.765, 0.765, 0.516, 0.768, 0.769, 0.493, 0.772,\n",
       "        0.768, 0.507, 0.771, 0.766, 0.493, 0.768, 0.765, 0.507, 0.772,\n",
       "        0.772, 0.493, 0.772, 0.773, 0.493, 0.771, 0.769, 0.493, 0.765,\n",
       "        0.767, 0.528, 0.769, 0.768, 0.44 , 0.768, 0.766, 0.493, 0.769,\n",
       "        0.768, 0.699, 0.769, 0.767, 0.507, 0.768, 0.769, 0.493, 0.768,\n",
       "        0.774, 0.519, 0.773, 0.762, 0.577, 0.768, 0.769, 0.645, 0.763,\n",
       "        0.763, 0.638, 0.765, 0.77 , 0.619, 0.765, 0.767, 0.744, 0.788,\n",
       "        0.767, 0.463, 0.785, 0.768, 0.571, 0.769, 0.759, 0.567, 0.759,\n",
       "        0.772, 0.648, 0.771, 0.768, 0.604, 0.765, 0.758, 0.515, 0.764,\n",
       "        0.766, 0.648, 0.769, 0.769, 0.62 , 0.772, 0.768, 0.516, 0.766,\n",
       "        0.762, 0.589, 0.766, 0.768, 0.767, 0.764, 0.771, 0.766, 0.768,\n",
       "        0.767, 0.768, 0.764, 0.766, 0.764, 0.767, 0.772, 0.765, 0.767,\n",
       "        0.765, 0.766, 0.766, 0.767, 0.767, 0.769, 0.769, 0.769, 0.767,\n",
       "        0.766, 0.765, 0.764, 0.769, 0.766, 0.769, 0.767, 0.767, 0.766,\n",
       "        0.77 , 0.768, 0.767, 0.764, 0.766, 0.766, 0.766, 0.766, 0.767,\n",
       "        0.767, 0.766, 0.766, 0.768, 0.767, 0.77 , 0.768, 0.768, 0.772,\n",
       "        0.764, 0.765, 0.76 , 0.781, 0.754, 0.767, 0.763, 0.782, 0.764,\n",
       "        0.762, 0.767, 0.776, 0.78 , 0.766, 0.761, 0.763, 0.763, 0.766,\n",
       "        0.771, 0.764, 0.776, 0.768, 0.768, 0.77 , 0.772, 0.762, 0.762,\n",
       "        0.768, 0.77 , 0.766, 0.767, 0.764, 0.773, 0.767, 0.772, 0.769,\n",
       "        0.768, 0.767, 0.767, 0.769, 0.773, 0.763, 0.773, 0.775, 0.769]),\n",
       " 'mean_test_recall_micro': array([0.7804, 0.7812, 0.782 , 0.7692, 0.7712, 0.7698, 0.7512, 0.7648,\n",
       "        0.763 , 0.7602, 0.7556, 0.7588, 0.7828, 0.7826, 0.7822, 0.7626,\n",
       "        0.7662, 0.7734, 0.7616, 0.7598, 0.7744, 0.7592, 0.7586, 0.7464,\n",
       "        0.7798, 0.7844, 0.7814, 0.7716, 0.7728, 0.7746, 0.7614, 0.7634,\n",
       "        0.7598, 0.7574, 0.7592, 0.756 , 0.778 , 0.7836, 0.7838, 0.7766,\n",
       "        0.7752, 0.7672, 0.7698, 0.7668, 0.7658, 0.7672, 0.758 , 0.7548,\n",
       "        0.788 , 0.7896, 0.779 , 0.7732, 0.7746, 0.7766, 0.7562, 0.7638,\n",
       "        0.7598, 0.7484, 0.7426, 0.7424, 0.7804, 0.7854, 0.781 , 0.7632,\n",
       "        0.7792, 0.776 , 0.754 , 0.7576, 0.7632, 0.7448, 0.7506, 0.756 ,\n",
       "        0.7862, 0.7846, 0.7834, 0.7718, 0.7666, 0.781 , 0.7588, 0.7602,\n",
       "        0.7598, 0.7552, 0.7544, 0.7498, 0.7832, 0.7816, 0.7816, 0.7694,\n",
       "        0.7756, 0.7742, 0.7666, 0.7634, 0.7608, 0.7478, 0.7588, 0.75  ,\n",
       "        0.7938, 0.5094, 0.795 , 0.7928, 0.5032, 0.7908, 0.7916, 0.5012,\n",
       "        0.7912, 0.7908, 0.5024, 0.794 , 0.7904, 0.5452, 0.7922, 0.7908,\n",
       "        0.4884, 0.7924, 0.79  , 0.4928, 0.7914, 0.7934, 0.4436, 0.7932,\n",
       "        0.7926, 0.4916, 0.7906, 0.7926, 0.5   , 0.7908, 0.7934, 0.5222,\n",
       "        0.7912, 0.7916, 0.505 , 0.794 , 0.793 , 0.4894, 0.7902, 0.789 ,\n",
       "        0.5322, 0.7936, 0.7914, 0.5612, 0.7928, 0.7926, 0.4692, 0.7898,\n",
       "        0.7904, 0.6206, 0.788 , 0.7918, 0.623 , 0.7894, 0.7878, 0.6392,\n",
       "        0.7928, 0.7904, 0.6058, 0.788 , 0.7886, 0.6182, 0.7866, 0.791 ,\n",
       "        0.6156, 0.7916, 0.7878, 0.5342, 0.794 , 0.789 , 0.6182, 0.7872,\n",
       "        0.787 , 0.589 , 0.7826, 0.7906, 0.5848, 0.789 , 0.792 , 0.557 ,\n",
       "        0.7888, 0.787 , 0.6536, 0.7882, 0.7856, 0.5642, 0.7898, 0.7876,\n",
       "        0.629 , 0.7912, 0.7904, 0.6308, 0.789 , 0.7902, 0.6432, 0.7878,\n",
       "        0.7878, 0.789 , 0.7866, 0.7912, 0.7896, 0.7882, 0.7892, 0.7886,\n",
       "        0.7886, 0.7898, 0.7878, 0.7884, 0.79  , 0.7882, 0.7882, 0.7878,\n",
       "        0.7886, 0.7888, 0.7882, 0.7888, 0.7902, 0.7904, 0.7896, 0.7896,\n",
       "        0.7882, 0.7886, 0.7884, 0.7882, 0.7876, 0.789 , 0.7892, 0.7908,\n",
       "        0.7874, 0.7898, 0.7904, 0.7892, 0.789 , 0.7896, 0.789 , 0.789 ,\n",
       "        0.7898, 0.7892, 0.789 , 0.7896, 0.79  , 0.7902, 0.79  , 0.7898,\n",
       "        0.7884, 0.7864, 0.7898, 0.7838, 0.7886, 0.7874, 0.792 , 0.7834,\n",
       "        0.7824, 0.786 , 0.7902, 0.7906, 0.7852, 0.7866, 0.7868, 0.7912,\n",
       "        0.7868, 0.786 , 0.7892, 0.782 , 0.7864, 0.792 , 0.787 , 0.7858,\n",
       "        0.7858, 0.787 , 0.7904, 0.788 , 0.7848, 0.7864, 0.7896, 0.786 ,\n",
       "        0.788 , 0.7876, 0.784 , 0.783 , 0.7884, 0.789 , 0.7896, 0.7894,\n",
       "        0.788 , 0.7898, 0.7864, 0.7906, 0.7852, 0.7892, 0.7894, 0.7878]),\n",
       " 'std_test_recall_micro': array([0.00791454, 0.01719767, 0.01616168, 0.01376081, 0.01774711,\n",
       "        0.0079599 , 0.01730202, 0.00711056, 0.01652876, 0.00735935,\n",
       "        0.00993177, 0.0036    , 0.01260793, 0.01339552, 0.00976524,\n",
       "        0.0109654 , 0.01355581, 0.01486741, 0.01440278, 0.00636867,\n",
       "        0.01710672, 0.013467  , 0.00652993, 0.0082365 , 0.01325745,\n",
       "        0.0119432 , 0.01318484, 0.01535708, 0.01218852, 0.00791454,\n",
       "        0.01300154, 0.01059434, 0.01642437, 0.00830903, 0.01330263,\n",
       "        0.00536656, 0.01524467, 0.01440278, 0.01203993, 0.01547385,\n",
       "        0.00699714, 0.00655439, 0.01121428, 0.01156547, 0.01273421,\n",
       "        0.01419014, 0.01167904, 0.00767854, 0.01524467, 0.00830903,\n",
       "        0.01115347, 0.00949526, 0.01068831, 0.00700286, 0.00910824,\n",
       "        0.00863481, 0.01128539, 0.00508331, 0.01128893, 0.00928655,\n",
       "        0.01323027, 0.01132431, 0.01438054, 0.01097998, 0.00949526,\n",
       "        0.01118928, 0.00729383, 0.01440278, 0.01530229, 0.00649307,\n",
       "        0.01744248, 0.00689928, 0.01345214, 0.01307823, 0.0068    ,\n",
       "        0.01373172, 0.00788923, 0.00509902, 0.0107963 , 0.01664212,\n",
       "        0.01028397, 0.01632667, 0.00705975, 0.01212271, 0.01801555,\n",
       "        0.0119432 , 0.0121095 , 0.00717217, 0.00402989, 0.00679412,\n",
       "        0.01083697, 0.0074458 , 0.01178813, 0.01156547, 0.01444161,\n",
       "        0.01705286, 0.0171744 , 0.11190996, 0.01431084, 0.0163878 ,\n",
       "        0.07873855, 0.01552289, 0.01724645, 0.01016661, 0.01459315,\n",
       "        0.0151842 , 0.00656049, 0.01570987, 0.0148    , 0.078303  ,\n",
       "        0.01416192, 0.01628987, 0.01888491, 0.01481351, 0.01591226,\n",
       "        0.07467369, 0.01473228, 0.01361764, 0.1104058 , 0.01606736,\n",
       "        0.0146506 , 0.00945727, 0.01233856, 0.01645114, 0.00885438,\n",
       "        0.01524992, 0.0160075 , 0.02913005, 0.01445545, 0.01684755,\n",
       "        0.07176907, 0.01616168, 0.0157734 , 0.02888321, 0.013891  ,\n",
       "        0.01335665, 0.10281517, 0.0159198 , 0.01523942, 0.10298815,\n",
       "        0.01663009, 0.01570478, 0.06191736, 0.01374627, 0.00891291,\n",
       "        0.07939673, 0.01162755, 0.01627759, 0.06831984, 0.01295531,\n",
       "        0.01325745, 0.04926013, 0.01548419, 0.01436106, 0.09904221,\n",
       "        0.0135794 , 0.01360294, 0.02823756, 0.01513407, 0.01405703,\n",
       "        0.11193141, 0.01121784, 0.01289031, 0.05430064, 0.00887694,\n",
       "        0.01558204, 0.05210528, 0.01370255, 0.0166012 , 0.01881489,\n",
       "        0.01315447, 0.0111463 , 0.09589035, 0.01056409, 0.01568439,\n",
       "        0.07785885, 0.016005  , 0.01679286, 0.07410425, 0.01364405,\n",
       "        0.01256344, 0.11024228, 0.01285924, 0.0123061 , 0.0431138 ,\n",
       "        0.01397712, 0.01321514, 0.08917713, 0.01456022, 0.01541947,\n",
       "        0.05716432, 0.01230285, 0.01310572, 0.01395708, 0.0147187 ,\n",
       "        0.01413365, 0.01570478, 0.01430245, 0.01403424, 0.01419296,\n",
       "        0.01523942, 0.01464787, 0.01498533, 0.01399428, 0.01226377,\n",
       "        0.0143861 , 0.01393413, 0.01376081, 0.01531796, 0.01407693,\n",
       "        0.01419014, 0.0136587 , 0.01416192, 0.01417886, 0.01419296,\n",
       "        0.0147187 , 0.01373172, 0.01585686, 0.01576832, 0.01313621,\n",
       "        0.01417886, 0.01269646, 0.013891  , 0.01602997, 0.01405133,\n",
       "        0.01330263, 0.01512085, 0.0144    , 0.01570987, 0.01497465,\n",
       "        0.0151921 , 0.01507315, 0.0154971 , 0.01474313, 0.0148054 ,\n",
       "        0.0153701 , 0.01517893, 0.01420422, 0.01504659, 0.013891  ,\n",
       "        0.01327554, 0.01324538, 0.0123677 , 0.0119063 , 0.0129244 ,\n",
       "        0.01626776, 0.00898888, 0.01748828, 0.0108922 , 0.01294604,\n",
       "        0.00813388, 0.01646329, 0.01260793, 0.01204326, 0.00783326,\n",
       "        0.00901998, 0.01460685, 0.01447757, 0.01374627, 0.01260159,\n",
       "        0.01526565, 0.01314534, 0.01559487, 0.00932523, 0.01446928,\n",
       "        0.01317574, 0.01187603, 0.01084435, 0.01474313, 0.01417886,\n",
       "        0.01636582, 0.01256981, 0.01316055, 0.01387948, 0.01291511,\n",
       "        0.01063955, 0.01262696, 0.0123774 , 0.01333567, 0.01410815,\n",
       "        0.01224745, 0.01478377, 0.01190966, 0.01253156, 0.01251239,\n",
       "        0.0096    , 0.01063203, 0.01222129]),\n",
       " 'rank_test_recall_micro': array([181, 178, 173, 205, 201, 202, 247, 213, 219, 224, 242, 232, 168,\n",
       "        169, 172, 220, 211, 196, 221, 226, 194, 230, 235, 253, 183, 159,\n",
       "        177, 200, 198, 192, 222, 215, 226, 238, 231, 240, 186, 163, 161,\n",
       "        187, 191, 206, 203, 208, 212, 206, 236, 244, 115,  67, 185, 197,\n",
       "        192, 187, 239, 214, 226, 251, 255, 256, 181, 154, 179, 217, 184,\n",
       "        189, 246, 237, 218, 254, 248, 240, 147, 158, 164, 199, 209, 179,\n",
       "        232, 224, 226, 243, 245, 250, 166, 175, 175, 204, 190, 195, 210,\n",
       "        215, 223, 252, 232, 249,   5, 277,   1,  12, 279,  34,  23, 281,\n",
       "         28,  34, 280,   3,  45, 273,  18,  34, 286,  17,  55, 283,  27,\n",
       "          7, 288,   9,  14, 284,  42,  15, 282,  34,   8, 276,  28,  23,\n",
       "        278,   3,  10, 285,  53,  92, 275,   6,  26, 271,  12,  15, 287,\n",
       "         65,  45, 263, 115,  22, 262,  75, 121, 259,  11,  45, 267, 115,\n",
       "        101, 264, 140,  33, 266,  23, 121, 274,   2,  84, 264, 133, 136,\n",
       "        268, 170,  39, 269,  92,  19, 272,  95, 134, 257, 108, 153, 270,\n",
       "         59, 130, 261,  32,  43, 260,  84,  50, 258, 121, 121,  84, 142,\n",
       "         28,  72, 109,  79, 101,  98,  59, 121, 104,  56, 109, 109, 121,\n",
       "         98,  96, 109,  96,  50,  45,  72,  67, 109, 101, 104, 109, 128,\n",
       "         84,  79,  38, 131,  61,  45,  79,  84,  67,  84,  84,  61,  79,\n",
       "         92,  67,  56,  50,  56,  61, 104, 143,  65, 161,  98, 131,  19,\n",
       "        164, 171, 149,  53,  39, 155, 140, 138,  28, 138, 148,  79, 173,\n",
       "        146,  19, 137, 151, 151, 134,  44, 115, 157, 143,  74, 149, 115,\n",
       "        128, 160, 167, 104,  84,  67,  75, 115,  61, 143,  39, 155,  78,\n",
       "         75, 121]),\n",
       " 'split0_test_f1_micro': array([0.786, 0.797, 0.793, 0.776, 0.791, 0.76 , 0.759, 0.763, 0.775,\n",
       "        0.762, 0.763, 0.762, 0.792, 0.794, 0.788, 0.768, 0.774, 0.755,\n",
       "        0.772, 0.749, 0.781, 0.754, 0.756, 0.733, 0.783, 0.796, 0.799,\n",
       "        0.791, 0.781, 0.775, 0.775, 0.753, 0.77 , 0.764, 0.773, 0.749,\n",
       "        0.798, 0.795, 0.795, 0.797, 0.772, 0.774, 0.777, 0.77 , 0.779,\n",
       "        0.784, 0.748, 0.754, 0.803, 0.794, 0.796, 0.772, 0.78 , 0.783,\n",
       "        0.757, 0.752, 0.764, 0.747, 0.753, 0.741, 0.789, 0.794, 0.788,\n",
       "        0.762, 0.787, 0.792, 0.748, 0.755, 0.751, 0.749, 0.768, 0.755,\n",
       "        0.802, 0.798, 0.784, 0.785, 0.764, 0.783, 0.774, 0.778, 0.772,\n",
       "        0.76 , 0.762, 0.739, 0.795, 0.797, 0.796, 0.78 , 0.777, 0.78 ,\n",
       "        0.756, 0.776, 0.77 , 0.756, 0.76 , 0.756, 0.798, 0.635, 0.801,\n",
       "        0.802, 0.507, 0.797, 0.801, 0.493, 0.801, 0.801, 0.509, 0.805,\n",
       "        0.801, 0.701, 0.807, 0.8  , 0.51 , 0.798, 0.795, 0.525, 0.803,\n",
       "        0.803, 0.495, 0.803, 0.798, 0.493, 0.8  , 0.804, 0.514, 0.801,\n",
       "        0.806, 0.507, 0.802, 0.8  , 0.506, 0.802, 0.805, 0.493, 0.795,\n",
       "        0.797, 0.493, 0.801, 0.798, 0.518, 0.803, 0.801, 0.507, 0.793,\n",
       "        0.792, 0.528, 0.793, 0.804, 0.564, 0.799, 0.791, 0.649, 0.806,\n",
       "        0.801, 0.54 , 0.799, 0.799, 0.63 , 0.797, 0.798, 0.46 , 0.79 ,\n",
       "        0.797, 0.566, 0.792, 0.801, 0.691, 0.793, 0.796, 0.603, 0.791,\n",
       "        0.798, 0.449, 0.794, 0.807, 0.561, 0.791, 0.796, 0.662, 0.797,\n",
       "        0.798, 0.518, 0.796, 0.798, 0.673, 0.793, 0.798, 0.747, 0.797,\n",
       "        0.802, 0.565, 0.792, 0.796, 0.802, 0.799, 0.801, 0.802, 0.799,\n",
       "        0.798, 0.802, 0.801, 0.802, 0.797, 0.8  , 0.798, 0.797, 0.802,\n",
       "        0.797, 0.799, 0.801, 0.799, 0.799, 0.801, 0.804, 0.803, 0.802,\n",
       "        0.8  , 0.802, 0.8  , 0.798, 0.797, 0.797, 0.799, 0.8  , 0.796,\n",
       "        0.802, 0.803, 0.8  , 0.799, 0.802, 0.799, 0.8  , 0.8  , 0.8  ,\n",
       "        0.8  , 0.799, 0.802, 0.8  , 0.801, 0.803, 0.799, 0.804, 0.8  ,\n",
       "        0.784, 0.803, 0.804, 0.801, 0.795, 0.787, 0.8  , 0.799, 0.796,\n",
       "        0.796, 0.796, 0.797, 0.807, 0.797, 0.801, 0.802, 0.786, 0.8  ,\n",
       "        0.799, 0.788, 0.792, 0.794, 0.795, 0.804, 0.799, 0.792, 0.801,\n",
       "        0.802, 0.802, 0.804, 0.801, 0.799, 0.789, 0.8  , 0.797, 0.792,\n",
       "        0.797, 0.796, 0.806, 0.787, 0.81 , 0.792, 0.8  , 0.804, 0.803]),\n",
       " 'split1_test_f1_micro': array([0.774, 0.764, 0.778, 0.744, 0.768, 0.771, 0.737, 0.757, 0.752,\n",
       "        0.759, 0.749, 0.763, 0.773, 0.767, 0.777, 0.759, 0.755, 0.773,\n",
       "        0.764, 0.769, 0.792, 0.741, 0.759, 0.746, 0.776, 0.777, 0.777,\n",
       "        0.781, 0.768, 0.774, 0.758, 0.767, 0.754, 0.767, 0.747, 0.755,\n",
       "        0.77 , 0.769, 0.773, 0.754, 0.772, 0.763, 0.777, 0.748, 0.776,\n",
       "        0.745, 0.744, 0.747, 0.781, 0.788, 0.776, 0.772, 0.778, 0.775,\n",
       "        0.76 , 0.756, 0.74 , 0.752, 0.736, 0.756, 0.773, 0.766, 0.778,\n",
       "        0.766, 0.771, 0.776, 0.745, 0.764, 0.78 , 0.752, 0.737, 0.75 ,\n",
       "        0.778, 0.775, 0.778, 0.767, 0.78 , 0.776, 0.762, 0.753, 0.754,\n",
       "        0.73 , 0.748, 0.753, 0.765, 0.776, 0.778, 0.763, 0.772, 0.771,\n",
       "        0.758, 0.767, 0.756, 0.75 , 0.738, 0.716, 0.793, 0.309, 0.797,\n",
       "        0.788, 0.525, 0.791, 0.79 , 0.493, 0.791, 0.789, 0.507, 0.795,\n",
       "        0.789, 0.507, 0.792, 0.788, 0.493, 0.791, 0.792, 0.509, 0.787,\n",
       "        0.792, 0.223, 0.791, 0.79 , 0.487, 0.788, 0.795, 0.507, 0.792,\n",
       "        0.794, 0.507, 0.789, 0.79 , 0.507, 0.795, 0.79 , 0.435, 0.793,\n",
       "        0.788, 0.467, 0.795, 0.789, 0.507, 0.792, 0.792, 0.493, 0.794,\n",
       "        0.794, 0.686, 0.787, 0.796, 0.579, 0.786, 0.784, 0.585, 0.8  ,\n",
       "        0.793, 0.449, 0.788, 0.779, 0.575, 0.776, 0.79 , 0.524, 0.78 ,\n",
       "        0.784, 0.509, 0.792, 0.783, 0.618, 0.782, 0.794, 0.606, 0.783,\n",
       "        0.791, 0.51 , 0.791, 0.784, 0.42 , 0.784, 0.787, 0.656, 0.791,\n",
       "        0.785, 0.441, 0.791, 0.783, 0.584, 0.785, 0.789, 0.574, 0.784,\n",
       "        0.792, 0.668, 0.789, 0.785, 0.785, 0.782, 0.784, 0.782, 0.782,\n",
       "        0.787, 0.782, 0.785, 0.785, 0.784, 0.783, 0.788, 0.783, 0.785,\n",
       "        0.785, 0.783, 0.784, 0.785, 0.786, 0.786, 0.784, 0.785, 0.785,\n",
       "        0.783, 0.783, 0.786, 0.783, 0.784, 0.786, 0.785, 0.785, 0.783,\n",
       "        0.786, 0.785, 0.785, 0.786, 0.785, 0.784, 0.783, 0.786, 0.785,\n",
       "        0.784, 0.786, 0.786, 0.787, 0.786, 0.784, 0.787, 0.78 , 0.784,\n",
       "        0.788, 0.789, 0.783, 0.789, 0.779, 0.778, 0.782, 0.786, 0.786,\n",
       "        0.786, 0.783, 0.781, 0.79 , 0.785, 0.781, 0.797, 0.781, 0.773,\n",
       "        0.789, 0.784, 0.779, 0.783, 0.788, 0.791, 0.791, 0.782, 0.786,\n",
       "        0.78 , 0.776, 0.789, 0.784, 0.778, 0.769, 0.789, 0.783, 0.784,\n",
       "        0.79 , 0.79 , 0.786, 0.788, 0.787, 0.789, 0.791, 0.789, 0.782]),\n",
       " 'split2_test_f1_micro': array([0.793, 0.802, 0.804, 0.785, 0.792, 0.784, 0.776, 0.775, 0.789,\n",
       "        0.771, 0.77 , 0.757, 0.801, 0.801, 0.794, 0.777, 0.789, 0.798,\n",
       "        0.78 , 0.76 , 0.791, 0.781, 0.754, 0.755, 0.802, 0.801, 0.793,\n",
       "        0.779, 0.784, 0.787, 0.775, 0.776, 0.782, 0.761, 0.759, 0.765,\n",
       "        0.794, 0.8  , 0.801, 0.783, 0.786, 0.774, 0.781, 0.784, 0.773,\n",
       "        0.779, 0.774, 0.762, 0.802, 0.803, 0.787, 0.791, 0.79 , 0.786,\n",
       "        0.769, 0.774, 0.773, 0.755, 0.757, 0.745, 0.801, 0.799, 0.805,\n",
       "        0.782, 0.794, 0.779, 0.763, 0.781, 0.782, 0.736, 0.775, 0.767,\n",
       "        0.803, 0.8  , 0.793, 0.79 , 0.77 , 0.79 , 0.762, 0.78 , 0.772,\n",
       "        0.776, 0.759, 0.768, 0.802, 0.789, 0.782, 0.765, 0.781, 0.782,\n",
       "        0.786, 0.761, 0.779, 0.754, 0.783, 0.759, 0.822, 0.507, 0.817,\n",
       "        0.818, 0.626, 0.813, 0.817, 0.511, 0.811, 0.813, 0.507, 0.816,\n",
       "        0.811, 0.518, 0.808, 0.815, 0.453, 0.814, 0.814, 0.351, 0.813,\n",
       "        0.812, 0.5  , 0.818, 0.817, 0.507, 0.807, 0.815, 0.493, 0.81 ,\n",
       "        0.813, 0.493, 0.811, 0.818, 0.635, 0.817, 0.812, 0.519, 0.811,\n",
       "        0.808, 0.408, 0.817, 0.814, 0.767, 0.817, 0.816, 0.346, 0.81 ,\n",
       "        0.801, 0.69 , 0.807, 0.808, 0.746, 0.806, 0.81 , 0.594, 0.802,\n",
       "        0.803, 0.679, 0.804, 0.808, 0.661, 0.808, 0.81 , 0.618, 0.813,\n",
       "        0.805, 0.512, 0.811, 0.812, 0.659, 0.81 , 0.807, 0.604, 0.798,\n",
       "        0.805, 0.718, 0.803, 0.811, 0.547, 0.815, 0.809, 0.716, 0.804,\n",
       "        0.8  , 0.735, 0.808, 0.804, 0.685, 0.815, 0.808, 0.594, 0.81 ,\n",
       "        0.806, 0.718, 0.804, 0.807, 0.806, 0.806, 0.812, 0.811, 0.809,\n",
       "        0.809, 0.807, 0.808, 0.808, 0.809, 0.807, 0.808, 0.808, 0.805,\n",
       "        0.806, 0.811, 0.806, 0.808, 0.807, 0.81 , 0.808, 0.808, 0.809,\n",
       "        0.805, 0.81 , 0.81 , 0.807, 0.808, 0.807, 0.808, 0.815, 0.808,\n",
       "        0.807, 0.811, 0.809, 0.811, 0.809, 0.811, 0.81 , 0.812, 0.81 ,\n",
       "        0.81 , 0.812, 0.81 , 0.81 , 0.811, 0.808, 0.806, 0.799, 0.807,\n",
       "        0.801, 0.797, 0.804, 0.804, 0.806, 0.8  , 0.795, 0.801, 0.815,\n",
       "        0.797, 0.802, 0.794, 0.793, 0.808, 0.799, 0.795, 0.802, 0.806,\n",
       "        0.811, 0.813, 0.801, 0.809, 0.806, 0.8  , 0.799, 0.807, 0.8  ,\n",
       "        0.814, 0.799, 0.798, 0.805, 0.797, 0.798, 0.802, 0.808, 0.81 ,\n",
       "        0.81 , 0.803, 0.806, 0.806, 0.798, 0.8  , 0.797, 0.798, 0.799]),\n",
       " 'split3_test_f1_micro': array([0.777, 0.784, 0.779, 0.768, 0.757, 0.768, 0.757, 0.771, 0.755,\n",
       "        0.761, 0.742, 0.759, 0.782, 0.782, 0.786, 0.765, 0.76 , 0.779,\n",
       "        0.753, 0.76 , 0.755, 0.754, 0.753, 0.755, 0.777, 0.778, 0.776,\n",
       "        0.752, 0.78 , 0.775, 0.759, 0.772, 0.76 , 0.747, 0.775, 0.758,\n",
       "        0.769, 0.79 , 0.779, 0.785, 0.78 , 0.768, 0.763, 0.768, 0.753,\n",
       "        0.77 , 0.769, 0.765, 0.792, 0.784, 0.771, 0.763, 0.763, 0.767,\n",
       "        0.754, 0.765, 0.766, 0.74 , 0.741, 0.727, 0.776, 0.783, 0.765,\n",
       "        0.757, 0.772, 0.776, 0.762, 0.75 , 0.759, 0.738, 0.732, 0.76 ,\n",
       "        0.776, 0.784, 0.788, 0.753, 0.757, 0.779, 0.755, 0.736, 0.747,\n",
       "        0.766, 0.759, 0.755, 0.796, 0.784, 0.791, 0.776, 0.77 , 0.775,\n",
       "        0.77 , 0.755, 0.75 , 0.754, 0.759, 0.76 , 0.787, 0.591, 0.785,\n",
       "        0.787, 0.476, 0.788, 0.785, 0.493, 0.785, 0.782, 0.496, 0.782,\n",
       "        0.783, 0.493, 0.783, 0.785, 0.493, 0.791, 0.784, 0.572, 0.782,\n",
       "        0.788, 0.507, 0.782, 0.785, 0.478, 0.787, 0.78 , 0.493, 0.786,\n",
       "        0.787, 0.576, 0.785, 0.782, 0.437, 0.788, 0.792, 0.507, 0.783,\n",
       "        0.784, 0.594, 0.786, 0.789, 0.507, 0.784, 0.785, 0.507, 0.784,\n",
       "        0.791, 0.68 , 0.78 , 0.789, 0.649, 0.788, 0.785, 0.723, 0.793,\n",
       "        0.792, 0.723, 0.784, 0.787, 0.606, 0.787, 0.79 , 0.732, 0.787,\n",
       "        0.786, 0.621, 0.79 , 0.781, 0.552, 0.782, 0.779, 0.565, 0.782,\n",
       "        0.787, 0.599, 0.786, 0.79 , 0.653, 0.789, 0.785, 0.719, 0.785,\n",
       "        0.779, 0.479, 0.785, 0.784, 0.583, 0.791, 0.789, 0.723, 0.788,\n",
       "        0.789, 0.676, 0.788, 0.783, 0.785, 0.782, 0.788, 0.787, 0.783,\n",
       "        0.785, 0.784, 0.785, 0.788, 0.785, 0.785, 0.784, 0.788, 0.782,\n",
       "        0.786, 0.784, 0.787, 0.782, 0.785, 0.785, 0.787, 0.783, 0.785,\n",
       "        0.787, 0.783, 0.782, 0.784, 0.783, 0.786, 0.787, 0.787, 0.784,\n",
       "        0.784, 0.785, 0.785, 0.785, 0.786, 0.785, 0.786, 0.785, 0.784,\n",
       "        0.784, 0.785, 0.786, 0.786, 0.785, 0.784, 0.782, 0.781, 0.786,\n",
       "        0.782, 0.789, 0.786, 0.785, 0.783, 0.78 , 0.79 , 0.783, 0.792,\n",
       "        0.785, 0.785, 0.786, 0.786, 0.778, 0.788, 0.789, 0.778, 0.787,\n",
       "        0.79 , 0.786, 0.781, 0.775, 0.778, 0.787, 0.779, 0.781, 0.783,\n",
       "        0.784, 0.783, 0.783, 0.781, 0.782, 0.786, 0.784, 0.785, 0.793,\n",
       "        0.782, 0.784, 0.784, 0.782, 0.785, 0.782, 0.785, 0.781, 0.786]),\n",
       " 'split4_test_f1_micro': array([0.772, 0.759, 0.756, 0.773, 0.748, 0.766, 0.727, 0.758, 0.744,\n",
       "        0.748, 0.754, 0.753, 0.766, 0.769, 0.766, 0.744, 0.753, 0.762,\n",
       "        0.739, 0.761, 0.753, 0.766, 0.771, 0.743, 0.761, 0.77 , 0.762,\n",
       "        0.755, 0.751, 0.762, 0.74 , 0.749, 0.733, 0.748, 0.742, 0.753,\n",
       "        0.759, 0.764, 0.771, 0.764, 0.766, 0.757, 0.751, 0.764, 0.748,\n",
       "        0.758, 0.755, 0.746, 0.762, 0.779, 0.765, 0.768, 0.762, 0.772,\n",
       "        0.741, 0.772, 0.756, 0.748, 0.726, 0.743, 0.763, 0.785, 0.769,\n",
       "        0.749, 0.772, 0.757, 0.752, 0.738, 0.744, 0.749, 0.741, 0.748,\n",
       "        0.772, 0.766, 0.774, 0.764, 0.762, 0.777, 0.741, 0.754, 0.754,\n",
       "        0.744, 0.744, 0.734, 0.758, 0.762, 0.761, 0.763, 0.778, 0.763,\n",
       "        0.763, 0.758, 0.749, 0.725, 0.754, 0.759, 0.769, 0.505, 0.775,\n",
       "        0.769, 0.382, 0.765, 0.765, 0.516, 0.768, 0.769, 0.493, 0.772,\n",
       "        0.768, 0.507, 0.771, 0.766, 0.493, 0.768, 0.765, 0.507, 0.772,\n",
       "        0.772, 0.493, 0.772, 0.773, 0.493, 0.771, 0.769, 0.493, 0.765,\n",
       "        0.767, 0.528, 0.769, 0.768, 0.44 , 0.768, 0.766, 0.493, 0.769,\n",
       "        0.768, 0.699, 0.769, 0.767, 0.507, 0.768, 0.769, 0.493, 0.768,\n",
       "        0.774, 0.519, 0.773, 0.762, 0.577, 0.768, 0.769, 0.645, 0.763,\n",
       "        0.763, 0.638, 0.765, 0.77 , 0.619, 0.765, 0.767, 0.744, 0.788,\n",
       "        0.767, 0.463, 0.785, 0.768, 0.571, 0.769, 0.759, 0.567, 0.759,\n",
       "        0.772, 0.648, 0.771, 0.768, 0.604, 0.765, 0.758, 0.515, 0.764,\n",
       "        0.766, 0.648, 0.769, 0.769, 0.62 , 0.772, 0.768, 0.516, 0.766,\n",
       "        0.762, 0.589, 0.766, 0.768, 0.767, 0.764, 0.771, 0.766, 0.768,\n",
       "        0.767, 0.768, 0.764, 0.766, 0.764, 0.767, 0.772, 0.765, 0.767,\n",
       "        0.765, 0.766, 0.766, 0.767, 0.767, 0.769, 0.769, 0.769, 0.767,\n",
       "        0.766, 0.765, 0.764, 0.769, 0.766, 0.769, 0.767, 0.767, 0.766,\n",
       "        0.77 , 0.768, 0.767, 0.764, 0.766, 0.766, 0.766, 0.766, 0.767,\n",
       "        0.767, 0.766, 0.766, 0.768, 0.767, 0.77 , 0.768, 0.768, 0.772,\n",
       "        0.764, 0.765, 0.76 , 0.781, 0.754, 0.767, 0.763, 0.782, 0.764,\n",
       "        0.762, 0.767, 0.776, 0.78 , 0.766, 0.761, 0.763, 0.763, 0.766,\n",
       "        0.771, 0.764, 0.776, 0.768, 0.768, 0.77 , 0.772, 0.762, 0.762,\n",
       "        0.768, 0.77 , 0.766, 0.767, 0.764, 0.773, 0.767, 0.772, 0.769,\n",
       "        0.768, 0.767, 0.767, 0.769, 0.773, 0.763, 0.773, 0.775, 0.769]),\n",
       " 'mean_test_f1_micro': array([0.7804, 0.7812, 0.782 , 0.7692, 0.7712, 0.7698, 0.7512, 0.7648,\n",
       "        0.763 , 0.7602, 0.7556, 0.7588, 0.7828, 0.7826, 0.7822, 0.7626,\n",
       "        0.7662, 0.7734, 0.7616, 0.7598, 0.7744, 0.7592, 0.7586, 0.7464,\n",
       "        0.7798, 0.7844, 0.7814, 0.7716, 0.7728, 0.7746, 0.7614, 0.7634,\n",
       "        0.7598, 0.7574, 0.7592, 0.756 , 0.778 , 0.7836, 0.7838, 0.7766,\n",
       "        0.7752, 0.7672, 0.7698, 0.7668, 0.7658, 0.7672, 0.758 , 0.7548,\n",
       "        0.788 , 0.7896, 0.779 , 0.7732, 0.7746, 0.7766, 0.7562, 0.7638,\n",
       "        0.7598, 0.7484, 0.7426, 0.7424, 0.7804, 0.7854, 0.781 , 0.7632,\n",
       "        0.7792, 0.776 , 0.754 , 0.7576, 0.7632, 0.7448, 0.7506, 0.756 ,\n",
       "        0.7862, 0.7846, 0.7834, 0.7718, 0.7666, 0.781 , 0.7588, 0.7602,\n",
       "        0.7598, 0.7552, 0.7544, 0.7498, 0.7832, 0.7816, 0.7816, 0.7694,\n",
       "        0.7756, 0.7742, 0.7666, 0.7634, 0.7608, 0.7478, 0.7588, 0.75  ,\n",
       "        0.7938, 0.5094, 0.795 , 0.7928, 0.5032, 0.7908, 0.7916, 0.5012,\n",
       "        0.7912, 0.7908, 0.5024, 0.794 , 0.7904, 0.5452, 0.7922, 0.7908,\n",
       "        0.4884, 0.7924, 0.79  , 0.4928, 0.7914, 0.7934, 0.4436, 0.7932,\n",
       "        0.7926, 0.4916, 0.7906, 0.7926, 0.5   , 0.7908, 0.7934, 0.5222,\n",
       "        0.7912, 0.7916, 0.505 , 0.794 , 0.793 , 0.4894, 0.7902, 0.789 ,\n",
       "        0.5322, 0.7936, 0.7914, 0.5612, 0.7928, 0.7926, 0.4692, 0.7898,\n",
       "        0.7904, 0.6206, 0.788 , 0.7918, 0.623 , 0.7894, 0.7878, 0.6392,\n",
       "        0.7928, 0.7904, 0.6058, 0.788 , 0.7886, 0.6182, 0.7866, 0.791 ,\n",
       "        0.6156, 0.7916, 0.7878, 0.5342, 0.794 , 0.789 , 0.6182, 0.7872,\n",
       "        0.787 , 0.589 , 0.7826, 0.7906, 0.5848, 0.789 , 0.792 , 0.557 ,\n",
       "        0.7888, 0.787 , 0.6536, 0.7882, 0.7856, 0.5642, 0.7898, 0.7876,\n",
       "        0.629 , 0.7912, 0.7904, 0.6308, 0.789 , 0.7902, 0.6432, 0.7878,\n",
       "        0.7878, 0.789 , 0.7866, 0.7912, 0.7896, 0.7882, 0.7892, 0.7886,\n",
       "        0.7886, 0.7898, 0.7878, 0.7884, 0.79  , 0.7882, 0.7882, 0.7878,\n",
       "        0.7886, 0.7888, 0.7882, 0.7888, 0.7902, 0.7904, 0.7896, 0.7896,\n",
       "        0.7882, 0.7886, 0.7884, 0.7882, 0.7876, 0.789 , 0.7892, 0.7908,\n",
       "        0.7874, 0.7898, 0.7904, 0.7892, 0.789 , 0.7896, 0.789 , 0.789 ,\n",
       "        0.7898, 0.7892, 0.789 , 0.7896, 0.79  , 0.7902, 0.79  , 0.7898,\n",
       "        0.7884, 0.7864, 0.7898, 0.7838, 0.7886, 0.7874, 0.792 , 0.7834,\n",
       "        0.7824, 0.786 , 0.7902, 0.7906, 0.7852, 0.7866, 0.7868, 0.7912,\n",
       "        0.7868, 0.786 , 0.7892, 0.782 , 0.7864, 0.792 , 0.787 , 0.7858,\n",
       "        0.7858, 0.787 , 0.7904, 0.788 , 0.7848, 0.7864, 0.7896, 0.786 ,\n",
       "        0.788 , 0.7876, 0.784 , 0.783 , 0.7884, 0.789 , 0.7896, 0.7894,\n",
       "        0.788 , 0.7898, 0.7864, 0.7906, 0.7852, 0.7892, 0.7894, 0.7878]),\n",
       " 'std_test_f1_micro': array([0.00791454, 0.01719767, 0.01616168, 0.01376081, 0.01774711,\n",
       "        0.0079599 , 0.01730202, 0.00711056, 0.01652876, 0.00735935,\n",
       "        0.00993177, 0.0036    , 0.01260793, 0.01339552, 0.00976524,\n",
       "        0.0109654 , 0.01355581, 0.01486741, 0.01440278, 0.00636867,\n",
       "        0.01710672, 0.013467  , 0.00652993, 0.0082365 , 0.01325745,\n",
       "        0.0119432 , 0.01318484, 0.01535708, 0.01218852, 0.00791454,\n",
       "        0.01300154, 0.01059434, 0.01642437, 0.00830903, 0.01330263,\n",
       "        0.00536656, 0.01524467, 0.01440278, 0.01203993, 0.01547385,\n",
       "        0.00699714, 0.00655439, 0.01121428, 0.01156547, 0.01273421,\n",
       "        0.01419014, 0.01167904, 0.00767854, 0.01524467, 0.00830903,\n",
       "        0.01115347, 0.00949526, 0.01068831, 0.00700286, 0.00910824,\n",
       "        0.00863481, 0.01128539, 0.00508331, 0.01128893, 0.00928655,\n",
       "        0.01323027, 0.01132431, 0.01438054, 0.01097998, 0.00949526,\n",
       "        0.01118928, 0.00729383, 0.01440278, 0.01530229, 0.00649307,\n",
       "        0.01744248, 0.00689928, 0.01345214, 0.01307823, 0.0068    ,\n",
       "        0.01373172, 0.00788923, 0.00509902, 0.0107963 , 0.01664212,\n",
       "        0.01028397, 0.01632667, 0.00705975, 0.01212271, 0.01801555,\n",
       "        0.0119432 , 0.0121095 , 0.00717217, 0.00402989, 0.00679412,\n",
       "        0.01083697, 0.0074458 , 0.01178813, 0.01156547, 0.01444161,\n",
       "        0.01705286, 0.0171744 , 0.11190996, 0.01431084, 0.0163878 ,\n",
       "        0.07873855, 0.01552289, 0.01724645, 0.01016661, 0.01459315,\n",
       "        0.0151842 , 0.00656049, 0.01570987, 0.0148    , 0.078303  ,\n",
       "        0.01416192, 0.01628987, 0.01888491, 0.01481351, 0.01591226,\n",
       "        0.07467369, 0.01473228, 0.01361764, 0.1104058 , 0.01606736,\n",
       "        0.0146506 , 0.00945727, 0.01233856, 0.01645114, 0.00885438,\n",
       "        0.01524992, 0.0160075 , 0.02913005, 0.01445545, 0.01684755,\n",
       "        0.07176907, 0.01616168, 0.0157734 , 0.02888321, 0.013891  ,\n",
       "        0.01335665, 0.10281517, 0.0159198 , 0.01523942, 0.10298815,\n",
       "        0.01663009, 0.01570478, 0.06191736, 0.01374627, 0.00891291,\n",
       "        0.07939673, 0.01162755, 0.01627759, 0.06831984, 0.01295531,\n",
       "        0.01325745, 0.04926013, 0.01548419, 0.01436106, 0.09904221,\n",
       "        0.0135794 , 0.01360294, 0.02823756, 0.01513407, 0.01405703,\n",
       "        0.11193141, 0.01121784, 0.01289031, 0.05430064, 0.00887694,\n",
       "        0.01558204, 0.05210528, 0.01370255, 0.0166012 , 0.01881489,\n",
       "        0.01315447, 0.0111463 , 0.09589035, 0.01056409, 0.01568439,\n",
       "        0.07785885, 0.016005  , 0.01679286, 0.07410425, 0.01364405,\n",
       "        0.01256344, 0.11024228, 0.01285924, 0.0123061 , 0.0431138 ,\n",
       "        0.01397712, 0.01321514, 0.08917713, 0.01456022, 0.01541947,\n",
       "        0.05716432, 0.01230285, 0.01310572, 0.01395708, 0.0147187 ,\n",
       "        0.01413365, 0.01570478, 0.01430245, 0.01403424, 0.01419296,\n",
       "        0.01523942, 0.01464787, 0.01498533, 0.01399428, 0.01226377,\n",
       "        0.0143861 , 0.01393413, 0.01376081, 0.01531796, 0.01407693,\n",
       "        0.01419014, 0.0136587 , 0.01416192, 0.01417886, 0.01419296,\n",
       "        0.0147187 , 0.01373172, 0.01585686, 0.01576832, 0.01313621,\n",
       "        0.01417886, 0.01269646, 0.013891  , 0.01602997, 0.01405133,\n",
       "        0.01330263, 0.01512085, 0.0144    , 0.01570987, 0.01497465,\n",
       "        0.0151921 , 0.01507315, 0.0154971 , 0.01474313, 0.0148054 ,\n",
       "        0.0153701 , 0.01517893, 0.01420422, 0.01504659, 0.013891  ,\n",
       "        0.01327554, 0.01324538, 0.0123677 , 0.0119063 , 0.0129244 ,\n",
       "        0.01626776, 0.00898888, 0.01748828, 0.0108922 , 0.01294604,\n",
       "        0.00813388, 0.01646329, 0.01260793, 0.01204326, 0.00783326,\n",
       "        0.00901998, 0.01460685, 0.01447757, 0.01374627, 0.01260159,\n",
       "        0.01526565, 0.01314534, 0.01559487, 0.00932523, 0.01446928,\n",
       "        0.01317574, 0.01187603, 0.01084435, 0.01474313, 0.01417886,\n",
       "        0.01636582, 0.01256981, 0.01316055, 0.01387948, 0.01291511,\n",
       "        0.01063955, 0.01262696, 0.0123774 , 0.01333567, 0.01410815,\n",
       "        0.01224745, 0.01478377, 0.01190966, 0.01253156, 0.01251239,\n",
       "        0.0096    , 0.01063203, 0.01222129]),\n",
       " 'rank_test_f1_micro': array([181, 178, 173, 205, 201, 202, 247, 213, 219, 224, 242, 232, 168,\n",
       "        169, 172, 220, 211, 196, 221, 226, 194, 230, 235, 253, 183, 159,\n",
       "        177, 200, 198, 192, 222, 215, 226, 238, 231, 240, 186, 163, 161,\n",
       "        187, 191, 207, 203, 208, 212, 206, 236, 244, 115,  67, 185, 197,\n",
       "        192, 187, 239, 214, 226, 251, 255, 256, 181, 154, 179, 217, 184,\n",
       "        189, 246, 237, 218, 254, 248, 240, 147, 158, 164, 199, 209, 179,\n",
       "        233, 224, 226, 243, 245, 250, 166, 175, 176, 204, 190, 195, 210,\n",
       "        215, 223, 252, 233, 249,   5, 277,   1,  13, 279,  35,  23, 281,\n",
       "         28,  34, 280,   3,  46, 273,  18,  35, 286,  17,  55, 283,  27,\n",
       "          7, 288,   9,  14, 284,  42,  14, 282,  35,   8, 276,  28,  23,\n",
       "        278,   3,  10, 285,  53,  84, 275,   6,  26, 271,  11,  14, 287,\n",
       "         59,  44, 263, 120,  22, 262,  75, 121, 259,  11,  46, 267, 115,\n",
       "        101, 264, 140,  33, 266,  23, 121, 274,   2,  84, 264, 133, 134,\n",
       "        268, 169,  39, 269,  84,  19, 272,  95, 134, 257, 108, 153, 270,\n",
       "         59, 128, 261,  32,  43, 260,  84,  50, 258, 121, 121,  84, 142,\n",
       "         28,  72, 109,  80,  98,  98,  59, 121, 105,  56, 109, 109, 121,\n",
       "        101,  96, 109,  96,  50,  46,  72,  67, 109, 101, 105, 109, 128,\n",
       "         84,  80,  35, 131,  64,  46,  80,  84,  67,  84,  84,  64,  78,\n",
       "         84,  72,  56,  50,  56,  59, 105, 143,  59, 161,  98, 131,  19,\n",
       "        165, 171, 149,  53,  39, 155, 140, 138,  28, 138, 148,  80, 173,\n",
       "        146,  19, 137, 151, 151, 134,  44, 115, 157, 143,  67, 149, 115,\n",
       "        128, 160, 167, 104,  84,  67,  75, 115,  64, 143,  39, 155,  78,\n",
       "         75, 121]),\n",
       " 'split0_test_precision_micro': array([0.786, 0.797, 0.793, 0.776, 0.791, 0.76 , 0.759, 0.763, 0.775,\n",
       "        0.762, 0.763, 0.762, 0.792, 0.794, 0.788, 0.768, 0.774, 0.755,\n",
       "        0.772, 0.749, 0.781, 0.754, 0.756, 0.733, 0.783, 0.796, 0.799,\n",
       "        0.791, 0.781, 0.775, 0.775, 0.753, 0.77 , 0.764, 0.773, 0.749,\n",
       "        0.798, 0.795, 0.795, 0.797, 0.772, 0.774, 0.777, 0.77 , 0.779,\n",
       "        0.784, 0.748, 0.754, 0.803, 0.794, 0.796, 0.772, 0.78 , 0.783,\n",
       "        0.757, 0.752, 0.764, 0.747, 0.753, 0.741, 0.789, 0.794, 0.788,\n",
       "        0.762, 0.787, 0.792, 0.748, 0.755, 0.751, 0.749, 0.768, 0.755,\n",
       "        0.802, 0.798, 0.784, 0.785, 0.764, 0.783, 0.774, 0.778, 0.772,\n",
       "        0.76 , 0.762, 0.739, 0.795, 0.797, 0.796, 0.78 , 0.777, 0.78 ,\n",
       "        0.756, 0.776, 0.77 , 0.756, 0.76 , 0.756, 0.798, 0.635, 0.801,\n",
       "        0.802, 0.507, 0.797, 0.801, 0.493, 0.801, 0.801, 0.509, 0.805,\n",
       "        0.801, 0.701, 0.807, 0.8  , 0.51 , 0.798, 0.795, 0.525, 0.803,\n",
       "        0.803, 0.495, 0.803, 0.798, 0.493, 0.8  , 0.804, 0.514, 0.801,\n",
       "        0.806, 0.507, 0.802, 0.8  , 0.506, 0.802, 0.805, 0.493, 0.795,\n",
       "        0.797, 0.493, 0.801, 0.798, 0.518, 0.803, 0.801, 0.507, 0.793,\n",
       "        0.792, 0.528, 0.793, 0.804, 0.564, 0.799, 0.791, 0.649, 0.806,\n",
       "        0.801, 0.54 , 0.799, 0.799, 0.63 , 0.797, 0.798, 0.46 , 0.79 ,\n",
       "        0.797, 0.566, 0.792, 0.801, 0.691, 0.793, 0.796, 0.603, 0.791,\n",
       "        0.798, 0.449, 0.794, 0.807, 0.561, 0.791, 0.796, 0.662, 0.797,\n",
       "        0.798, 0.518, 0.796, 0.798, 0.673, 0.793, 0.798, 0.747, 0.797,\n",
       "        0.802, 0.565, 0.792, 0.796, 0.802, 0.799, 0.801, 0.802, 0.799,\n",
       "        0.798, 0.802, 0.801, 0.802, 0.797, 0.8  , 0.798, 0.797, 0.802,\n",
       "        0.797, 0.799, 0.801, 0.799, 0.799, 0.801, 0.804, 0.803, 0.802,\n",
       "        0.8  , 0.802, 0.8  , 0.798, 0.797, 0.797, 0.799, 0.8  , 0.796,\n",
       "        0.802, 0.803, 0.8  , 0.799, 0.802, 0.799, 0.8  , 0.8  , 0.8  ,\n",
       "        0.8  , 0.799, 0.802, 0.8  , 0.801, 0.803, 0.799, 0.804, 0.8  ,\n",
       "        0.784, 0.803, 0.804, 0.801, 0.795, 0.787, 0.8  , 0.799, 0.796,\n",
       "        0.796, 0.796, 0.797, 0.807, 0.797, 0.801, 0.802, 0.786, 0.8  ,\n",
       "        0.799, 0.788, 0.792, 0.794, 0.795, 0.804, 0.799, 0.792, 0.801,\n",
       "        0.802, 0.802, 0.804, 0.801, 0.799, 0.789, 0.8  , 0.797, 0.792,\n",
       "        0.797, 0.796, 0.806, 0.787, 0.81 , 0.792, 0.8  , 0.804, 0.803]),\n",
       " 'split1_test_precision_micro': array([0.774, 0.764, 0.778, 0.744, 0.768, 0.771, 0.737, 0.757, 0.752,\n",
       "        0.759, 0.749, 0.763, 0.773, 0.767, 0.777, 0.759, 0.755, 0.773,\n",
       "        0.764, 0.769, 0.792, 0.741, 0.759, 0.746, 0.776, 0.777, 0.777,\n",
       "        0.781, 0.768, 0.774, 0.758, 0.767, 0.754, 0.767, 0.747, 0.755,\n",
       "        0.77 , 0.769, 0.773, 0.754, 0.772, 0.763, 0.777, 0.748, 0.776,\n",
       "        0.745, 0.744, 0.747, 0.781, 0.788, 0.776, 0.772, 0.778, 0.775,\n",
       "        0.76 , 0.756, 0.74 , 0.752, 0.736, 0.756, 0.773, 0.766, 0.778,\n",
       "        0.766, 0.771, 0.776, 0.745, 0.764, 0.78 , 0.752, 0.737, 0.75 ,\n",
       "        0.778, 0.775, 0.778, 0.767, 0.78 , 0.776, 0.762, 0.753, 0.754,\n",
       "        0.73 , 0.748, 0.753, 0.765, 0.776, 0.778, 0.763, 0.772, 0.771,\n",
       "        0.758, 0.767, 0.756, 0.75 , 0.738, 0.716, 0.793, 0.309, 0.797,\n",
       "        0.788, 0.525, 0.791, 0.79 , 0.493, 0.791, 0.789, 0.507, 0.795,\n",
       "        0.789, 0.507, 0.792, 0.788, 0.493, 0.791, 0.792, 0.509, 0.787,\n",
       "        0.792, 0.223, 0.791, 0.79 , 0.487, 0.788, 0.795, 0.507, 0.792,\n",
       "        0.794, 0.507, 0.789, 0.79 , 0.507, 0.795, 0.79 , 0.435, 0.793,\n",
       "        0.788, 0.467, 0.795, 0.789, 0.507, 0.792, 0.792, 0.493, 0.794,\n",
       "        0.794, 0.686, 0.787, 0.796, 0.579, 0.786, 0.784, 0.585, 0.8  ,\n",
       "        0.793, 0.449, 0.788, 0.779, 0.575, 0.776, 0.79 , 0.524, 0.78 ,\n",
       "        0.784, 0.509, 0.792, 0.783, 0.618, 0.782, 0.794, 0.606, 0.783,\n",
       "        0.791, 0.51 , 0.791, 0.784, 0.42 , 0.784, 0.787, 0.656, 0.791,\n",
       "        0.785, 0.441, 0.791, 0.783, 0.584, 0.785, 0.789, 0.574, 0.784,\n",
       "        0.792, 0.668, 0.789, 0.785, 0.785, 0.782, 0.784, 0.782, 0.782,\n",
       "        0.787, 0.782, 0.785, 0.785, 0.784, 0.783, 0.788, 0.783, 0.785,\n",
       "        0.785, 0.783, 0.784, 0.785, 0.786, 0.786, 0.784, 0.785, 0.785,\n",
       "        0.783, 0.783, 0.786, 0.783, 0.784, 0.786, 0.785, 0.785, 0.783,\n",
       "        0.786, 0.785, 0.785, 0.786, 0.785, 0.784, 0.783, 0.786, 0.785,\n",
       "        0.784, 0.786, 0.786, 0.787, 0.786, 0.784, 0.787, 0.78 , 0.784,\n",
       "        0.788, 0.789, 0.783, 0.789, 0.779, 0.778, 0.782, 0.786, 0.786,\n",
       "        0.786, 0.783, 0.781, 0.79 , 0.785, 0.781, 0.797, 0.781, 0.773,\n",
       "        0.789, 0.784, 0.779, 0.783, 0.788, 0.791, 0.791, 0.782, 0.786,\n",
       "        0.78 , 0.776, 0.789, 0.784, 0.778, 0.769, 0.789, 0.783, 0.784,\n",
       "        0.79 , 0.79 , 0.786, 0.788, 0.787, 0.789, 0.791, 0.789, 0.782]),\n",
       " 'split2_test_precision_micro': array([0.793, 0.802, 0.804, 0.785, 0.792, 0.784, 0.776, 0.775, 0.789,\n",
       "        0.771, 0.77 , 0.757, 0.801, 0.801, 0.794, 0.777, 0.789, 0.798,\n",
       "        0.78 , 0.76 , 0.791, 0.781, 0.754, 0.755, 0.802, 0.801, 0.793,\n",
       "        0.779, 0.784, 0.787, 0.775, 0.776, 0.782, 0.761, 0.759, 0.765,\n",
       "        0.794, 0.8  , 0.801, 0.783, 0.786, 0.774, 0.781, 0.784, 0.773,\n",
       "        0.779, 0.774, 0.762, 0.802, 0.803, 0.787, 0.791, 0.79 , 0.786,\n",
       "        0.769, 0.774, 0.773, 0.755, 0.757, 0.745, 0.801, 0.799, 0.805,\n",
       "        0.782, 0.794, 0.779, 0.763, 0.781, 0.782, 0.736, 0.775, 0.767,\n",
       "        0.803, 0.8  , 0.793, 0.79 , 0.77 , 0.79 , 0.762, 0.78 , 0.772,\n",
       "        0.776, 0.759, 0.768, 0.802, 0.789, 0.782, 0.765, 0.781, 0.782,\n",
       "        0.786, 0.761, 0.779, 0.754, 0.783, 0.759, 0.822, 0.507, 0.817,\n",
       "        0.818, 0.626, 0.813, 0.817, 0.511, 0.811, 0.813, 0.507, 0.816,\n",
       "        0.811, 0.518, 0.808, 0.815, 0.453, 0.814, 0.814, 0.351, 0.813,\n",
       "        0.812, 0.5  , 0.818, 0.817, 0.507, 0.807, 0.815, 0.493, 0.81 ,\n",
       "        0.813, 0.493, 0.811, 0.818, 0.635, 0.817, 0.812, 0.519, 0.811,\n",
       "        0.808, 0.408, 0.817, 0.814, 0.767, 0.817, 0.816, 0.346, 0.81 ,\n",
       "        0.801, 0.69 , 0.807, 0.808, 0.746, 0.806, 0.81 , 0.594, 0.802,\n",
       "        0.803, 0.679, 0.804, 0.808, 0.661, 0.808, 0.81 , 0.618, 0.813,\n",
       "        0.805, 0.512, 0.811, 0.812, 0.659, 0.81 , 0.807, 0.604, 0.798,\n",
       "        0.805, 0.718, 0.803, 0.811, 0.547, 0.815, 0.809, 0.716, 0.804,\n",
       "        0.8  , 0.735, 0.808, 0.804, 0.685, 0.815, 0.808, 0.594, 0.81 ,\n",
       "        0.806, 0.718, 0.804, 0.807, 0.806, 0.806, 0.812, 0.811, 0.809,\n",
       "        0.809, 0.807, 0.808, 0.808, 0.809, 0.807, 0.808, 0.808, 0.805,\n",
       "        0.806, 0.811, 0.806, 0.808, 0.807, 0.81 , 0.808, 0.808, 0.809,\n",
       "        0.805, 0.81 , 0.81 , 0.807, 0.808, 0.807, 0.808, 0.815, 0.808,\n",
       "        0.807, 0.811, 0.809, 0.811, 0.809, 0.811, 0.81 , 0.812, 0.81 ,\n",
       "        0.81 , 0.812, 0.81 , 0.81 , 0.811, 0.808, 0.806, 0.799, 0.807,\n",
       "        0.801, 0.797, 0.804, 0.804, 0.806, 0.8  , 0.795, 0.801, 0.815,\n",
       "        0.797, 0.802, 0.794, 0.793, 0.808, 0.799, 0.795, 0.802, 0.806,\n",
       "        0.811, 0.813, 0.801, 0.809, 0.806, 0.8  , 0.799, 0.807, 0.8  ,\n",
       "        0.814, 0.799, 0.798, 0.805, 0.797, 0.798, 0.802, 0.808, 0.81 ,\n",
       "        0.81 , 0.803, 0.806, 0.806, 0.798, 0.8  , 0.797, 0.798, 0.799]),\n",
       " 'split3_test_precision_micro': array([0.777, 0.784, 0.779, 0.768, 0.757, 0.768, 0.757, 0.771, 0.755,\n",
       "        0.761, 0.742, 0.759, 0.782, 0.782, 0.786, 0.765, 0.76 , 0.779,\n",
       "        0.753, 0.76 , 0.755, 0.754, 0.753, 0.755, 0.777, 0.778, 0.776,\n",
       "        0.752, 0.78 , 0.775, 0.759, 0.772, 0.76 , 0.747, 0.775, 0.758,\n",
       "        0.769, 0.79 , 0.779, 0.785, 0.78 , 0.768, 0.763, 0.768, 0.753,\n",
       "        0.77 , 0.769, 0.765, 0.792, 0.784, 0.771, 0.763, 0.763, 0.767,\n",
       "        0.754, 0.765, 0.766, 0.74 , 0.741, 0.727, 0.776, 0.783, 0.765,\n",
       "        0.757, 0.772, 0.776, 0.762, 0.75 , 0.759, 0.738, 0.732, 0.76 ,\n",
       "        0.776, 0.784, 0.788, 0.753, 0.757, 0.779, 0.755, 0.736, 0.747,\n",
       "        0.766, 0.759, 0.755, 0.796, 0.784, 0.791, 0.776, 0.77 , 0.775,\n",
       "        0.77 , 0.755, 0.75 , 0.754, 0.759, 0.76 , 0.787, 0.591, 0.785,\n",
       "        0.787, 0.476, 0.788, 0.785, 0.493, 0.785, 0.782, 0.496, 0.782,\n",
       "        0.783, 0.493, 0.783, 0.785, 0.493, 0.791, 0.784, 0.572, 0.782,\n",
       "        0.788, 0.507, 0.782, 0.785, 0.478, 0.787, 0.78 , 0.493, 0.786,\n",
       "        0.787, 0.576, 0.785, 0.782, 0.437, 0.788, 0.792, 0.507, 0.783,\n",
       "        0.784, 0.594, 0.786, 0.789, 0.507, 0.784, 0.785, 0.507, 0.784,\n",
       "        0.791, 0.68 , 0.78 , 0.789, 0.649, 0.788, 0.785, 0.723, 0.793,\n",
       "        0.792, 0.723, 0.784, 0.787, 0.606, 0.787, 0.79 , 0.732, 0.787,\n",
       "        0.786, 0.621, 0.79 , 0.781, 0.552, 0.782, 0.779, 0.565, 0.782,\n",
       "        0.787, 0.599, 0.786, 0.79 , 0.653, 0.789, 0.785, 0.719, 0.785,\n",
       "        0.779, 0.479, 0.785, 0.784, 0.583, 0.791, 0.789, 0.723, 0.788,\n",
       "        0.789, 0.676, 0.788, 0.783, 0.785, 0.782, 0.788, 0.787, 0.783,\n",
       "        0.785, 0.784, 0.785, 0.788, 0.785, 0.785, 0.784, 0.788, 0.782,\n",
       "        0.786, 0.784, 0.787, 0.782, 0.785, 0.785, 0.787, 0.783, 0.785,\n",
       "        0.787, 0.783, 0.782, 0.784, 0.783, 0.786, 0.787, 0.787, 0.784,\n",
       "        0.784, 0.785, 0.785, 0.785, 0.786, 0.785, 0.786, 0.785, 0.784,\n",
       "        0.784, 0.785, 0.786, 0.786, 0.785, 0.784, 0.782, 0.781, 0.786,\n",
       "        0.782, 0.789, 0.786, 0.785, 0.783, 0.78 , 0.79 , 0.783, 0.792,\n",
       "        0.785, 0.785, 0.786, 0.786, 0.778, 0.788, 0.789, 0.778, 0.787,\n",
       "        0.79 , 0.786, 0.781, 0.775, 0.778, 0.787, 0.779, 0.781, 0.783,\n",
       "        0.784, 0.783, 0.783, 0.781, 0.782, 0.786, 0.784, 0.785, 0.793,\n",
       "        0.782, 0.784, 0.784, 0.782, 0.785, 0.782, 0.785, 0.781, 0.786]),\n",
       " 'split4_test_precision_micro': array([0.772, 0.759, 0.756, 0.773, 0.748, 0.766, 0.727, 0.758, 0.744,\n",
       "        0.748, 0.754, 0.753, 0.766, 0.769, 0.766, 0.744, 0.753, 0.762,\n",
       "        0.739, 0.761, 0.753, 0.766, 0.771, 0.743, 0.761, 0.77 , 0.762,\n",
       "        0.755, 0.751, 0.762, 0.74 , 0.749, 0.733, 0.748, 0.742, 0.753,\n",
       "        0.759, 0.764, 0.771, 0.764, 0.766, 0.757, 0.751, 0.764, 0.748,\n",
       "        0.758, 0.755, 0.746, 0.762, 0.779, 0.765, 0.768, 0.762, 0.772,\n",
       "        0.741, 0.772, 0.756, 0.748, 0.726, 0.743, 0.763, 0.785, 0.769,\n",
       "        0.749, 0.772, 0.757, 0.752, 0.738, 0.744, 0.749, 0.741, 0.748,\n",
       "        0.772, 0.766, 0.774, 0.764, 0.762, 0.777, 0.741, 0.754, 0.754,\n",
       "        0.744, 0.744, 0.734, 0.758, 0.762, 0.761, 0.763, 0.778, 0.763,\n",
       "        0.763, 0.758, 0.749, 0.725, 0.754, 0.759, 0.769, 0.505, 0.775,\n",
       "        0.769, 0.382, 0.765, 0.765, 0.516, 0.768, 0.769, 0.493, 0.772,\n",
       "        0.768, 0.507, 0.771, 0.766, 0.493, 0.768, 0.765, 0.507, 0.772,\n",
       "        0.772, 0.493, 0.772, 0.773, 0.493, 0.771, 0.769, 0.493, 0.765,\n",
       "        0.767, 0.528, 0.769, 0.768, 0.44 , 0.768, 0.766, 0.493, 0.769,\n",
       "        0.768, 0.699, 0.769, 0.767, 0.507, 0.768, 0.769, 0.493, 0.768,\n",
       "        0.774, 0.519, 0.773, 0.762, 0.577, 0.768, 0.769, 0.645, 0.763,\n",
       "        0.763, 0.638, 0.765, 0.77 , 0.619, 0.765, 0.767, 0.744, 0.788,\n",
       "        0.767, 0.463, 0.785, 0.768, 0.571, 0.769, 0.759, 0.567, 0.759,\n",
       "        0.772, 0.648, 0.771, 0.768, 0.604, 0.765, 0.758, 0.515, 0.764,\n",
       "        0.766, 0.648, 0.769, 0.769, 0.62 , 0.772, 0.768, 0.516, 0.766,\n",
       "        0.762, 0.589, 0.766, 0.768, 0.767, 0.764, 0.771, 0.766, 0.768,\n",
       "        0.767, 0.768, 0.764, 0.766, 0.764, 0.767, 0.772, 0.765, 0.767,\n",
       "        0.765, 0.766, 0.766, 0.767, 0.767, 0.769, 0.769, 0.769, 0.767,\n",
       "        0.766, 0.765, 0.764, 0.769, 0.766, 0.769, 0.767, 0.767, 0.766,\n",
       "        0.77 , 0.768, 0.767, 0.764, 0.766, 0.766, 0.766, 0.766, 0.767,\n",
       "        0.767, 0.766, 0.766, 0.768, 0.767, 0.77 , 0.768, 0.768, 0.772,\n",
       "        0.764, 0.765, 0.76 , 0.781, 0.754, 0.767, 0.763, 0.782, 0.764,\n",
       "        0.762, 0.767, 0.776, 0.78 , 0.766, 0.761, 0.763, 0.763, 0.766,\n",
       "        0.771, 0.764, 0.776, 0.768, 0.768, 0.77 , 0.772, 0.762, 0.762,\n",
       "        0.768, 0.77 , 0.766, 0.767, 0.764, 0.773, 0.767, 0.772, 0.769,\n",
       "        0.768, 0.767, 0.767, 0.769, 0.773, 0.763, 0.773, 0.775, 0.769]),\n",
       " 'mean_test_precision_micro': array([0.7804, 0.7812, 0.782 , 0.7692, 0.7712, 0.7698, 0.7512, 0.7648,\n",
       "        0.763 , 0.7602, 0.7556, 0.7588, 0.7828, 0.7826, 0.7822, 0.7626,\n",
       "        0.7662, 0.7734, 0.7616, 0.7598, 0.7744, 0.7592, 0.7586, 0.7464,\n",
       "        0.7798, 0.7844, 0.7814, 0.7716, 0.7728, 0.7746, 0.7614, 0.7634,\n",
       "        0.7598, 0.7574, 0.7592, 0.756 , 0.778 , 0.7836, 0.7838, 0.7766,\n",
       "        0.7752, 0.7672, 0.7698, 0.7668, 0.7658, 0.7672, 0.758 , 0.7548,\n",
       "        0.788 , 0.7896, 0.779 , 0.7732, 0.7746, 0.7766, 0.7562, 0.7638,\n",
       "        0.7598, 0.7484, 0.7426, 0.7424, 0.7804, 0.7854, 0.781 , 0.7632,\n",
       "        0.7792, 0.776 , 0.754 , 0.7576, 0.7632, 0.7448, 0.7506, 0.756 ,\n",
       "        0.7862, 0.7846, 0.7834, 0.7718, 0.7666, 0.781 , 0.7588, 0.7602,\n",
       "        0.7598, 0.7552, 0.7544, 0.7498, 0.7832, 0.7816, 0.7816, 0.7694,\n",
       "        0.7756, 0.7742, 0.7666, 0.7634, 0.7608, 0.7478, 0.7588, 0.75  ,\n",
       "        0.7938, 0.5094, 0.795 , 0.7928, 0.5032, 0.7908, 0.7916, 0.5012,\n",
       "        0.7912, 0.7908, 0.5024, 0.794 , 0.7904, 0.5452, 0.7922, 0.7908,\n",
       "        0.4884, 0.7924, 0.79  , 0.4928, 0.7914, 0.7934, 0.4436, 0.7932,\n",
       "        0.7926, 0.4916, 0.7906, 0.7926, 0.5   , 0.7908, 0.7934, 0.5222,\n",
       "        0.7912, 0.7916, 0.505 , 0.794 , 0.793 , 0.4894, 0.7902, 0.789 ,\n",
       "        0.5322, 0.7936, 0.7914, 0.5612, 0.7928, 0.7926, 0.4692, 0.7898,\n",
       "        0.7904, 0.6206, 0.788 , 0.7918, 0.623 , 0.7894, 0.7878, 0.6392,\n",
       "        0.7928, 0.7904, 0.6058, 0.788 , 0.7886, 0.6182, 0.7866, 0.791 ,\n",
       "        0.6156, 0.7916, 0.7878, 0.5342, 0.794 , 0.789 , 0.6182, 0.7872,\n",
       "        0.787 , 0.589 , 0.7826, 0.7906, 0.5848, 0.789 , 0.792 , 0.557 ,\n",
       "        0.7888, 0.787 , 0.6536, 0.7882, 0.7856, 0.5642, 0.7898, 0.7876,\n",
       "        0.629 , 0.7912, 0.7904, 0.6308, 0.789 , 0.7902, 0.6432, 0.7878,\n",
       "        0.7878, 0.789 , 0.7866, 0.7912, 0.7896, 0.7882, 0.7892, 0.7886,\n",
       "        0.7886, 0.7898, 0.7878, 0.7884, 0.79  , 0.7882, 0.7882, 0.7878,\n",
       "        0.7886, 0.7888, 0.7882, 0.7888, 0.7902, 0.7904, 0.7896, 0.7896,\n",
       "        0.7882, 0.7886, 0.7884, 0.7882, 0.7876, 0.789 , 0.7892, 0.7908,\n",
       "        0.7874, 0.7898, 0.7904, 0.7892, 0.789 , 0.7896, 0.789 , 0.789 ,\n",
       "        0.7898, 0.7892, 0.789 , 0.7896, 0.79  , 0.7902, 0.79  , 0.7898,\n",
       "        0.7884, 0.7864, 0.7898, 0.7838, 0.7886, 0.7874, 0.792 , 0.7834,\n",
       "        0.7824, 0.786 , 0.7902, 0.7906, 0.7852, 0.7866, 0.7868, 0.7912,\n",
       "        0.7868, 0.786 , 0.7892, 0.782 , 0.7864, 0.792 , 0.787 , 0.7858,\n",
       "        0.7858, 0.787 , 0.7904, 0.788 , 0.7848, 0.7864, 0.7896, 0.786 ,\n",
       "        0.788 , 0.7876, 0.784 , 0.783 , 0.7884, 0.789 , 0.7896, 0.7894,\n",
       "        0.788 , 0.7898, 0.7864, 0.7906, 0.7852, 0.7892, 0.7894, 0.7878]),\n",
       " 'std_test_precision_micro': array([0.00791454, 0.01719767, 0.01616168, 0.01376081, 0.01774711,\n",
       "        0.0079599 , 0.01730202, 0.00711056, 0.01652876, 0.00735935,\n",
       "        0.00993177, 0.0036    , 0.01260793, 0.01339552, 0.00976524,\n",
       "        0.0109654 , 0.01355581, 0.01486741, 0.01440278, 0.00636867,\n",
       "        0.01710672, 0.013467  , 0.00652993, 0.0082365 , 0.01325745,\n",
       "        0.0119432 , 0.01318484, 0.01535708, 0.01218852, 0.00791454,\n",
       "        0.01300154, 0.01059434, 0.01642437, 0.00830903, 0.01330263,\n",
       "        0.00536656, 0.01524467, 0.01440278, 0.01203993, 0.01547385,\n",
       "        0.00699714, 0.00655439, 0.01121428, 0.01156547, 0.01273421,\n",
       "        0.01419014, 0.01167904, 0.00767854, 0.01524467, 0.00830903,\n",
       "        0.01115347, 0.00949526, 0.01068831, 0.00700286, 0.00910824,\n",
       "        0.00863481, 0.01128539, 0.00508331, 0.01128893, 0.00928655,\n",
       "        0.01323027, 0.01132431, 0.01438054, 0.01097998, 0.00949526,\n",
       "        0.01118928, 0.00729383, 0.01440278, 0.01530229, 0.00649307,\n",
       "        0.01744248, 0.00689928, 0.01345214, 0.01307823, 0.0068    ,\n",
       "        0.01373172, 0.00788923, 0.00509902, 0.0107963 , 0.01664212,\n",
       "        0.01028397, 0.01632667, 0.00705975, 0.01212271, 0.01801555,\n",
       "        0.0119432 , 0.0121095 , 0.00717217, 0.00402989, 0.00679412,\n",
       "        0.01083697, 0.0074458 , 0.01178813, 0.01156547, 0.01444161,\n",
       "        0.01705286, 0.0171744 , 0.11190996, 0.01431084, 0.0163878 ,\n",
       "        0.07873855, 0.01552289, 0.01724645, 0.01016661, 0.01459315,\n",
       "        0.0151842 , 0.00656049, 0.01570987, 0.0148    , 0.078303  ,\n",
       "        0.01416192, 0.01628987, 0.01888491, 0.01481351, 0.01591226,\n",
       "        0.07467369, 0.01473228, 0.01361764, 0.1104058 , 0.01606736,\n",
       "        0.0146506 , 0.00945727, 0.01233856, 0.01645114, 0.00885438,\n",
       "        0.01524992, 0.0160075 , 0.02913005, 0.01445545, 0.01684755,\n",
       "        0.07176907, 0.01616168, 0.0157734 , 0.02888321, 0.013891  ,\n",
       "        0.01335665, 0.10281517, 0.0159198 , 0.01523942, 0.10298815,\n",
       "        0.01663009, 0.01570478, 0.06191736, 0.01374627, 0.00891291,\n",
       "        0.07939673, 0.01162755, 0.01627759, 0.06831984, 0.01295531,\n",
       "        0.01325745, 0.04926013, 0.01548419, 0.01436106, 0.09904221,\n",
       "        0.0135794 , 0.01360294, 0.02823756, 0.01513407, 0.01405703,\n",
       "        0.11193141, 0.01121784, 0.01289031, 0.05430064, 0.00887694,\n",
       "        0.01558204, 0.05210528, 0.01370255, 0.0166012 , 0.01881489,\n",
       "        0.01315447, 0.0111463 , 0.09589035, 0.01056409, 0.01568439,\n",
       "        0.07785885, 0.016005  , 0.01679286, 0.07410425, 0.01364405,\n",
       "        0.01256344, 0.11024228, 0.01285924, 0.0123061 , 0.0431138 ,\n",
       "        0.01397712, 0.01321514, 0.08917713, 0.01456022, 0.01541947,\n",
       "        0.05716432, 0.01230285, 0.01310572, 0.01395708, 0.0147187 ,\n",
       "        0.01413365, 0.01570478, 0.01430245, 0.01403424, 0.01419296,\n",
       "        0.01523942, 0.01464787, 0.01498533, 0.01399428, 0.01226377,\n",
       "        0.0143861 , 0.01393413, 0.01376081, 0.01531796, 0.01407693,\n",
       "        0.01419014, 0.0136587 , 0.01416192, 0.01417886, 0.01419296,\n",
       "        0.0147187 , 0.01373172, 0.01585686, 0.01576832, 0.01313621,\n",
       "        0.01417886, 0.01269646, 0.013891  , 0.01602997, 0.01405133,\n",
       "        0.01330263, 0.01512085, 0.0144    , 0.01570987, 0.01497465,\n",
       "        0.0151921 , 0.01507315, 0.0154971 , 0.01474313, 0.0148054 ,\n",
       "        0.0153701 , 0.01517893, 0.01420422, 0.01504659, 0.013891  ,\n",
       "        0.01327554, 0.01324538, 0.0123677 , 0.0119063 , 0.0129244 ,\n",
       "        0.01626776, 0.00898888, 0.01748828, 0.0108922 , 0.01294604,\n",
       "        0.00813388, 0.01646329, 0.01260793, 0.01204326, 0.00783326,\n",
       "        0.00901998, 0.01460685, 0.01447757, 0.01374627, 0.01260159,\n",
       "        0.01526565, 0.01314534, 0.01559487, 0.00932523, 0.01446928,\n",
       "        0.01317574, 0.01187603, 0.01084435, 0.01474313, 0.01417886,\n",
       "        0.01636582, 0.01256981, 0.01316055, 0.01387948, 0.01291511,\n",
       "        0.01063955, 0.01262696, 0.0123774 , 0.01333567, 0.01410815,\n",
       "        0.01224745, 0.01478377, 0.01190966, 0.01253156, 0.01251239,\n",
       "        0.0096    , 0.01063203, 0.01222129]),\n",
       " 'rank_test_precision_micro': array([181, 178, 173, 205, 201, 202, 247, 213, 219, 224, 242, 232, 168,\n",
       "        169, 172, 220, 211, 196, 221, 226, 194, 230, 235, 253, 183, 159,\n",
       "        177, 200, 198, 192, 222, 215, 226, 238, 231, 240, 186, 163, 161,\n",
       "        187, 191, 206, 203, 208, 212, 206, 236, 244, 115,  67, 185, 197,\n",
       "        192, 187, 239, 214, 226, 251, 255, 256, 181, 154, 179, 217, 184,\n",
       "        189, 246, 237, 218, 254, 248, 240, 147, 158, 164, 199, 209, 179,\n",
       "        232, 224, 226, 243, 245, 250, 166, 175, 175, 204, 190, 195, 210,\n",
       "        215, 223, 252, 232, 249,   5, 277,   1,  12, 279,  34,  23, 281,\n",
       "         28,  34, 280,   3,  45, 273,  18,  34, 286,  17,  55, 283,  27,\n",
       "          7, 288,   9,  14, 284,  42,  15, 282,  34,   8, 276,  28,  23,\n",
       "        278,   3,  10, 285,  53,  92, 275,   6,  26, 271,  12,  15, 287,\n",
       "         65,  45, 263, 115,  22, 262,  75, 121, 259,  11,  45, 267, 115,\n",
       "        101, 264, 140,  33, 266,  23, 121, 274,   2,  84, 264, 133, 136,\n",
       "        268, 170,  39, 269,  92,  19, 272,  95, 134, 257, 108, 153, 270,\n",
       "         59, 130, 261,  32,  43, 260,  84,  50, 258, 121, 121,  84, 142,\n",
       "         28,  72, 109,  79, 101,  98,  59, 121, 104,  56, 109, 109, 121,\n",
       "         98,  96, 109,  96,  50,  45,  72,  67, 109, 101, 104, 109, 128,\n",
       "         84,  79,  38, 131,  61,  45,  79,  84,  67,  84,  84,  61,  79,\n",
       "         92,  67,  56,  50,  56,  61, 104, 143,  65, 161,  98, 131,  19,\n",
       "        164, 171, 149,  53,  39, 155, 140, 138,  28, 138, 148,  79, 173,\n",
       "        146,  19, 137, 151, 151, 134,  44, 115, 157, 143,  74, 149, 115,\n",
       "        128, 160, 167, 104,  84,  67,  75, 115,  61, 143,  39, 155,  78,\n",
       "         75, 121]),\n",
       " 'split0_test_roc_auc_ovo': array([0.86769407, 0.87489948, 0.86942641, 0.85708399, 0.85788014,\n",
       "        0.85120684, 0.83225912, 0.846962  , 0.84360135, 0.83255518,\n",
       "        0.83176303, 0.83590784, 0.87233498, 0.86947842, 0.87009054,\n",
       "        0.85475153, 0.86881429, 0.84242512, 0.85012662, 0.83437154,\n",
       "        0.85954447, 0.83635993, 0.83352737, 0.81772027, 0.86737401,\n",
       "        0.8709427 , 0.8739553 , 0.86321719, 0.85126685, 0.86161688,\n",
       "        0.84500962, 0.83661998, 0.84034471, 0.85376334, 0.8444015 ,\n",
       "        0.82588587, 0.87509152, 0.86801813, 0.87437938, 0.8683862 ,\n",
       "        0.85548367, 0.85604378, 0.84637389, 0.84358534, 0.85211101,\n",
       "        0.85520362, 0.83768419, 0.83524771, 0.86855824, 0.87343519,\n",
       "        0.8749915 , 0.8545835 , 0.85728003, 0.85777612, 0.83458358,\n",
       "        0.82502971, 0.84464155, 0.82722614, 0.82529376, 0.82118895,\n",
       "        0.86438942, 0.86977048, 0.86842621, 0.84146893, 0.86031662,\n",
       "        0.8668619 , 0.83047077, 0.83362739, 0.83678001, 0.82876644,\n",
       "        0.83900044, 0.83480762, 0.87341519, 0.87989646, 0.86963045,\n",
       "        0.85988054, 0.85634384, 0.85364331, 0.84580978, 0.84878636,\n",
       "        0.84613784, 0.82864241, 0.83413949, 0.81956864, 0.87178287,\n",
       "        0.87006253, 0.86636981, 0.85932443, 0.85776412, 0.8556277 ,\n",
       "        0.85105881, 0.8515549 , 0.8484943 , 0.83543575, 0.84483759,\n",
       "        0.83672   , 0.87780005, 0.76468988, 0.87808811, 0.88001248,\n",
       "        0.60681894, 0.87818812, 0.87830815, 0.81355946, 0.87794408,\n",
       "        0.87844418, 0.72925493, 0.87938836, 0.8785522 , 0.78072902,\n",
       "        0.87833615, 0.87735196, 0.66260187, 0.87617973, 0.87869622,\n",
       "        0.6622538 , 0.87818812, 0.87830415, 0.52106613, 0.87825614,\n",
       "        0.87831615, 0.21095735, 0.87815212, 0.87947638, 0.67990526,\n",
       "        0.87719593, 0.87822813, 0.68102548, 0.87812811, 0.87801609,\n",
       "        0.36455945, 0.87885226, 0.87773604, 0.79985277, 0.87772003,\n",
       "        0.87659981, 0.39721385, 0.877564  , 0.87776804, 0.63352417,\n",
       "        0.87910831, 0.87795208, 0.70413801, 0.87772403, 0.87627175,\n",
       "        0.57558081, 0.87251501, 0.87725994, 0.66823497, 0.87758401,\n",
       "        0.87627175, 0.69709263, 0.87894427, 0.87762401, 0.57161204,\n",
       "        0.87879624, 0.87889626, 0.66490232, 0.8739593 , 0.87670383,\n",
       "        0.40747987, 0.87065465, 0.87431937, 0.63814108, 0.87633176,\n",
       "        0.87955639, 0.7785886 , 0.87380727, 0.87535157, 0.70045729,\n",
       "        0.87579566, 0.87630376, 0.44555933, 0.87530756, 0.8765558 ,\n",
       "        0.62768303, 0.87816812, 0.87843217, 0.7474545 , 0.87834816,\n",
       "        0.87694788, 0.52184228, 0.87468344, 0.87844818, 0.85022664,\n",
       "        0.8760197 , 0.87712392, 0.8066541 , 0.8780801 , 0.87711992,\n",
       "        0.75040708, 0.87822013, 0.88039656, 0.88056459, 0.87999248,\n",
       "        0.88075263, 0.8811407 , 0.88097267, 0.88120072, 0.88032054,\n",
       "        0.88030454, 0.88065261, 0.88056859, 0.88101268, 0.88024853,\n",
       "        0.88036055, 0.88104869, 0.88095267, 0.88078063, 0.88039656,\n",
       "        0.88076063, 0.88075263, 0.88085665, 0.88064861, 0.88056459,\n",
       "        0.88076063, 0.88004849, 0.88076863, 0.88076863, 0.88027253,\n",
       "        0.88064461, 0.88051258, 0.88056459, 0.88040056, 0.88109269,\n",
       "        0.88073262, 0.88084465, 0.88036855, 0.88032454, 0.88035255,\n",
       "        0.8806246 , 0.88044857, 0.88066061, 0.88045257, 0.88040056,\n",
       "        0.88023253, 0.88018852, 0.88057259, 0.8806046 , 0.88052458,\n",
       "        0.87659581, 0.88236494, 0.8801005 , 0.87521554, 0.87569964,\n",
       "        0.87942437, 0.87415133, 0.87478746, 0.87171486, 0.87779605,\n",
       "        0.87685986, 0.8729431 , 0.87955639, 0.87841217, 0.88104869,\n",
       "        0.8755116 , 0.87693988, 0.87831215, 0.87794008, 0.87659181,\n",
       "        0.87537557, 0.87181087, 0.87426336, 0.87439938, 0.87738797,\n",
       "        0.87731195, 0.87875224, 0.87474745, 0.87445939, 0.87621574,\n",
       "        0.87453141, 0.87566363, 0.87673584, 0.87511552, 0.87474745,\n",
       "        0.87559962, 0.88002048, 0.87920432, 0.88007249, 0.87689987,\n",
       "        0.87852819, 0.88180883, 0.87732396, 0.88107669, 0.87731195,\n",
       "        0.87636777, 0.87685986, 0.87590768]),\n",
       " 'split1_test_roc_auc_ovo': array([0.85980852, 0.86008858, 0.86129681, 0.8453977 , 0.85904437,\n",
       "        0.85505159, 0.83339134, 0.84729007, 0.83892443, 0.83543174,\n",
       "        0.82899448, 0.83902845, 0.86331321, 0.87269105, 0.87033859,\n",
       "        0.86002456, 0.84775416, 0.86024061, 0.84291721, 0.84349733,\n",
       "        0.86504555, 0.83427152, 0.83752816, 0.8326592 , 0.8693904 ,\n",
       "        0.86218499, 0.86289713, 0.87257903, 0.8540994 , 0.85798016,\n",
       "        0.8433453 , 0.84680997, 0.84576577, 0.83750015, 0.84660193,\n",
       "        0.83805226, 0.86981448, 0.86537761, 0.86822617, 0.84988658,\n",
       "        0.8540994 , 0.85150289, 0.86283712, 0.84472157, 0.86027661,\n",
       "        0.83428752, 0.83272721, 0.83200307, 0.86054867, 0.86407336,\n",
       "        0.85938044, 0.86220099, 0.86306516, 0.86057667, 0.83169501,\n",
       "        0.84666195, 0.811233  , 0.83535973, 0.82256522, 0.83699205,\n",
       "        0.85878832, 0.8653256 , 0.86367728, 0.84494961, 0.8556117 ,\n",
       "        0.84497361, 0.82492168, 0.8535753 , 0.86066469, 0.82820633,\n",
       "        0.81654004, 0.83221911, 0.8561198 , 0.86435741, 0.86573368,\n",
       "        0.85617181, 0.86533361, 0.85898436, 0.83970858, 0.8407928 ,\n",
       "        0.8351837 , 0.81270329, 0.83229913, 0.84551772, 0.85533165,\n",
       "        0.86745802, 0.86941441, 0.85224304, 0.85728003, 0.85657189,\n",
       "        0.84492961, 0.85086277, 0.84006065, 0.8459298 , 0.83131494,\n",
       "        0.81242323, 0.87459142, 0.23332573, 0.87214694, 0.87309913,\n",
       "        0.76695032, 0.87161884, 0.87310313, 0.76820657, 0.87147481,\n",
       "        0.87343119, 0.65341607, 0.87189489, 0.87437538, 0.43483723,\n",
       "        0.87530356, 0.87276306, 0.78098507, 0.87159483, 0.87215894,\n",
       "        0.65533245, 0.87020656, 0.87196691, 0.14486439, 0.87327916,\n",
       "        0.87121076, 0.27198931, 0.87252301, 0.87385127, 0.78270541,\n",
       "        0.87340719, 0.87334318, 0.64566255, 0.8744954 , 0.87113074,\n",
       "        0.36387132, 0.87117075, 0.87317514, 0.19457414, 0.8714348 ,\n",
       "        0.87325516, 0.20842885, 0.8734872 , 0.87167485, 0.47437298,\n",
       "        0.87163884, 0.87199491, 0.54777536, 0.87046261, 0.87378326,\n",
       "        0.76088913, 0.87266304, 0.8739593 , 0.632644  , 0.87400731,\n",
       "        0.87672784, 0.70108541, 0.87577965, 0.87522354, 0.41566147,\n",
       "        0.87634776, 0.87167885, 0.61548063, 0.87527555, 0.87331117,\n",
       "        0.55293237, 0.87221095, 0.87415934, 0.54613504, 0.87126277,\n",
       "        0.87745198, 0.67648459, 0.87437138, 0.87130278, 0.65504439,\n",
       "        0.87537557, 0.87121476, 0.46899592, 0.87286708, 0.8739793 ,\n",
       "        0.39929826, 0.8729751 , 0.87430736, 0.71449204, 0.87541558,\n",
       "        0.87506751, 0.37116475, 0.87212294, 0.87387528, 0.59878136,\n",
       "        0.87166685, 0.87535557, 0.70278575, 0.87171086, 0.87939236,\n",
       "        0.72608631, 0.87510752, 0.8785602 , 0.87743198, 0.87746798,\n",
       "        0.87823613, 0.87749999, 0.87780405, 0.87778004, 0.87766402,\n",
       "        0.87786806, 0.87750799, 0.87730795, 0.87767202, 0.87726394,\n",
       "        0.87772003, 0.87746798, 0.87762001, 0.87768803, 0.87772003,\n",
       "        0.87770403, 0.877548  , 0.87780805, 0.8780561 , 0.87765602,\n",
       "        0.87720393, 0.87847618, 0.87816812, 0.87713992, 0.87760801,\n",
       "        0.87768403, 0.87732396, 0.87756   , 0.87747999, 0.87743598,\n",
       "        0.87750399, 0.87708391, 0.87734396, 0.87761601, 0.87741997,\n",
       "        0.87731595, 0.87699589, 0.87733196, 0.877568  , 0.87768403,\n",
       "        0.87713592, 0.8770479 , 0.87708791, 0.87768003, 0.87739997,\n",
       "        0.87617973, 0.87634376, 0.8754916 , 0.87533957, 0.87634376,\n",
       "        0.87534757, 0.87071866, 0.87378326, 0.87423535, 0.86656985,\n",
       "        0.87176287, 0.87104272, 0.8755076 , 0.87520754, 0.87696388,\n",
       "        0.87799609, 0.87415133, 0.87152282, 0.87247901, 0.87086269,\n",
       "        0.86908634, 0.87487948, 0.87201091, 0.87062664, 0.87679985,\n",
       "        0.87606371, 0.8800925 , 0.8765158 , 0.87205092, 0.87715192,\n",
       "        0.87058263, 0.86979448, 0.87540358, 0.87370325, 0.87262703,\n",
       "        0.86759805, 0.87554361, 0.87729195, 0.87645979, 0.87745998,\n",
       "        0.87486747, 0.8755236 , 0.8754956 , 0.87380727, 0.87560362,\n",
       "        0.87639577, 0.87522754, 0.87416334]),\n",
       " 'split2_test_roc_auc_ovo': array([0.88427732, 0.88610968, 0.88680581, 0.86357726, 0.87715592,\n",
       "        0.87374325, 0.86307716, 0.86496953, 0.86220499, 0.84826226,\n",
       "        0.86221699, 0.84926646, 0.87955639, 0.88394525, 0.8852255 ,\n",
       "        0.86464147, 0.86622578, 0.87459542, 0.86512156, 0.84347332,\n",
       "        0.86974647, 0.84695   , 0.83674   , 0.82887446, 0.88448536,\n",
       "        0.88217291, 0.88485743, 0.86982649, 0.86909434, 0.87881225,\n",
       "        0.87123876, 0.86164488, 0.86767006, 0.83756416, 0.83611588,\n",
       "        0.83607587, 0.88082464, 0.8867098 , 0.88615769, 0.87666783,\n",
       "        0.86992651, 0.87099071, 0.86965845, 0.85733604, 0.86694992,\n",
       "        0.85949646, 0.86075671, 0.84986657, 0.88506147, 0.87830415,\n",
       "        0.87533957, 0.86504555, 0.87254702, 0.87257502, 0.85615181,\n",
       "        0.85077475, 0.85726402, 0.84516165, 0.83575981, 0.82570584,\n",
       "        0.88381323, 0.88605767, 0.87938036, 0.87112674, 0.87596369,\n",
       "        0.87069866, 0.84646991, 0.85014263, 0.86350125, 0.81598793,\n",
       "        0.85241107, 0.8443775 , 0.88396926, 0.88298106, 0.88044857,\n",
       "        0.87523955, 0.86779409, 0.8795964 , 0.83908846, 0.86291713,\n",
       "        0.85660389, 0.852063  , 0.83900845, 0.83884041, 0.88975039,\n",
       "        0.88544155, 0.87640778, 0.86260107, 0.87050262, 0.87359923,\n",
       "        0.85740405, 0.84312925, 0.86035663, 0.83561578, 0.85993655,\n",
       "        0.84071278, 0.89292701, 0.30275934, 0.89275898, 0.89242692,\n",
       "        0.71982909, 0.89163476, 0.89279499, 0.66539842, 0.89290301,\n",
       "        0.89097463, 0.41373709, 0.89137071, 0.89174278, 0.8734632 ,\n",
       "        0.89319907, 0.89191482, 0.32534377, 0.8918388 , 0.89278699,\n",
       "        0.23394985, 0.89118267, 0.89245092, 0.62234998, 0.89150674,\n",
       "        0.89109465, 0.39943029, 0.89115867, 0.89306704, 0.1474329 ,\n",
       "        0.892875  , 0.89159875, 0.85720001, 0.89238291, 0.89252293,\n",
       "        0.83508368, 0.89272697, 0.89238691, 0.46374689, 0.89323107,\n",
       "        0.89153874, 0.1586711 , 0.89320707, 0.89201083, 0.86458146,\n",
       "        0.89211485, 0.89167877, 0.26894871, 0.89122268, 0.88998244,\n",
       "        0.86447744, 0.88972639, 0.89276698, 0.85691596, 0.89157475,\n",
       "        0.89119067, 0.64717085, 0.88953835, 0.89015047, 0.75566411,\n",
       "        0.88860617, 0.89084661, 0.73483603, 0.89045853, 0.89151074,\n",
       "        0.64205384, 0.89187081, 0.89129869, 0.55530884, 0.89268697,\n",
       "        0.89279499, 0.71464807, 0.89098263, 0.88951034, 0.71228361,\n",
       "        0.89161876, 0.88950234, 0.81826038, 0.89094663, 0.88968638,\n",
       "        0.59905341, 0.89278299, 0.89340711, 0.81820837, 0.89129469,\n",
       "        0.8897784 , 0.80804638, 0.89036651, 0.8898144 , 0.80532984,\n",
       "        0.89263496, 0.89258695, 0.65440026, 0.89105065, 0.89358314,\n",
       "        0.78737433, 0.89052654, 0.89189481, 0.89201884, 0.89239491,\n",
       "        0.89197883, 0.89241491, 0.89211886, 0.89250693, 0.89223488,\n",
       "        0.89212286, 0.89249493, 0.89231889, 0.89205884, 0.89328708,\n",
       "        0.89220287, 0.89241891, 0.89216286, 0.89237891, 0.89222688,\n",
       "        0.89211485, 0.89208685, 0.89229089, 0.89204284, 0.89222688,\n",
       "        0.89212686, 0.89213486, 0.89237491, 0.89300303, 0.89255094,\n",
       "        0.8923469 , 0.89207085, 0.89246692, 0.8923309 , 0.89203884,\n",
       "        0.89230289, 0.8923629 , 0.89202684, 0.89241491, 0.89207485,\n",
       "        0.89217087, 0.89227489, 0.89254694, 0.89221487, 0.89225888,\n",
       "        0.89238291, 0.89212686, 0.89203884, 0.8923469 , 0.89196683,\n",
       "        0.89174278, 0.88968638, 0.88924229, 0.8902825 , 0.88933431,\n",
       "        0.89075859, 0.88947834, 0.88655376, 0.88695384, 0.88895023,\n",
       "        0.89027449, 0.89050254, 0.88898624, 0.89061456, 0.8898144 ,\n",
       "        0.88707387, 0.89120668, 0.88938232, 0.887746  , 0.88790203,\n",
       "        0.88912627, 0.8862137 , 0.88837812, 0.88766198, 0.89396322,\n",
       "        0.88976239, 0.89066257, 0.88846214, 0.88857816, 0.89002645,\n",
       "        0.88991442, 0.89008246, 0.88923029, 0.88772599, 0.88614969,\n",
       "        0.88362519, 0.89101464, 0.89177079, 0.89114266, 0.89198283,\n",
       "        0.89127869, 0.88993043, 0.89262695, 0.89154274, 0.89246692,\n",
       "        0.89019848, 0.89190281, 0.88999044]),\n",
       " 'split3_test_roc_auc_ovo': array([0.87289509, 0.87562762, 0.86674988, 0.86313717, 0.84702602,\n",
       "        0.86032862, 0.84297722, 0.84664994, 0.83287924, 0.83845634,\n",
       "        0.83351137, 0.82967062, 0.87367924, 0.86636181, 0.87031058,\n",
       "        0.85864029, 0.85071474, 0.85531964, 0.84086081, 0.83684002,\n",
       "        0.84299723, 0.8351877 , 0.83695204, 0.8382483 , 0.87359522,\n",
       "        0.87053062, 0.87178687, 0.8439014 , 0.85809219, 0.86168089,\n",
       "        0.84176499, 0.85149089, 0.84507363, 0.83390745, 0.8479582 ,\n",
       "        0.83846834, 0.86127681, 0.87084269, 0.87118275, 0.85639185,\n",
       "        0.86355326, 0.85921641, 0.85019064, 0.85064273, 0.85323923,\n",
       "        0.84598181, 0.84362935, 0.84348532, 0.87418334, 0.87277506,\n",
       "        0.8688943 , 0.84575977, 0.85802017, 0.86456946, 0.83394745,\n",
       "        0.8398206 , 0.85769611, 0.83091486, 0.82620594, 0.81983669,\n",
       "        0.86710595, 0.8714268 , 0.86649383, 0.84828226, 0.85166293,\n",
       "        0.85542366, 0.8331593 , 0.84122888, 0.84298122, 0.81507575,\n",
       "        0.81373949, 0.8453897 , 0.86546163, 0.86463747, 0.87237098,\n",
       "        0.85156691, 0.85423943, 0.86710995, 0.83589183, 0.82411753,\n",
       "        0.82883845, 0.83173902, 0.84075279, 0.83489164, 0.87615973,\n",
       "        0.87207493, 0.86776608, 0.86062868, 0.85790015, 0.86266908,\n",
       "        0.85211101, 0.84673796, 0.84501762, 0.83531172, 0.82764222,\n",
       "        0.84249713, 0.87237899, 0.74908282, 0.87382727, 0.87432737,\n",
       "        0.36757204, 0.87235098, 0.87331517, 0.34658393, 0.87363523,\n",
       "        0.87433537, 0.72980304, 0.8744874 , 0.8734512 , 0.22919692,\n",
       "        0.87298711, 0.87332317, 0.76745042, 0.8744834 , 0.87559562,\n",
       "        0.85055071, 0.87494349, 0.87343119, 0.69275178, 0.87434737,\n",
       "        0.87415133, 0.41390913, 0.87469944, 0.8739593 , 0.79725226,\n",
       "        0.87437538, 0.87409532, 0.83231513, 0.87526755, 0.87405131,\n",
       "        0.42214674, 0.87439138, 0.87238299, 0.26920076, 0.87407532,\n",
       "        0.87415934, 0.69469216, 0.87225896, 0.87431137, 0.79192322,\n",
       "        0.87387928, 0.87387928, 0.77524795, 0.8744714 , 0.87127877,\n",
       "        0.79104705, 0.87025857, 0.872463  , 0.71393593, 0.87174286,\n",
       "        0.87186289, 0.81075891, 0.87359923, 0.87184288, 0.8428532 ,\n",
       "        0.87278707, 0.86895432, 0.69318386, 0.87227497, 0.87171086,\n",
       "        0.82102892, 0.87108273, 0.87438738, 0.65655669, 0.8734712 ,\n",
       "        0.87428736, 0.59001564, 0.87071866, 0.87319915, 0.58938752,\n",
       "        0.87279907, 0.87081468, 0.64168177, 0.87095871, 0.87699989,\n",
       "        0.69347192, 0.87314714, 0.87028258, 0.82052882, 0.87410733,\n",
       "        0.8694024 , 0.36460346, 0.87098671, 0.87451541, 0.67546439,\n",
       "        0.87286708, 0.87217895, 0.8076543 , 0.87256302, 0.87535557,\n",
       "        0.77471184, 0.87177887, 0.87512752, 0.8759997 , 0.87477546,\n",
       "        0.87519954, 0.87519554, 0.87508752, 0.87507151, 0.87544359,\n",
       "        0.87515953, 0.87479946, 0.87527155, 0.8749955 , 0.87528756,\n",
       "        0.87491148, 0.87468744, 0.8749915 , 0.87489948, 0.87539558,\n",
       "        0.87556361, 0.87531556, 0.87508352, 0.87520354, 0.87524355,\n",
       "        0.87523155, 0.87497149, 0.87470344, 0.87470344, 0.87513553,\n",
       "        0.87533957, 0.87542358, 0.87519154, 0.87512352, 0.87497149,\n",
       "        0.87530356, 0.87537957, 0.87535557, 0.87527555, 0.87487948,\n",
       "        0.87519954, 0.87505951, 0.87537957, 0.87529156, 0.87521954,\n",
       "        0.87528756, 0.87521154, 0.87526355, 0.87539558, 0.87541958,\n",
       "        0.8739753 , 0.87545559, 0.87420734, 0.87019456, 0.87264304,\n",
       "        0.87439538, 0.87483147, 0.87416734, 0.87425135, 0.86943441,\n",
       "        0.8698985 , 0.87071066, 0.87359923, 0.87647179, 0.87442739,\n",
       "        0.87467544, 0.86975047, 0.87249501, 0.87329917, 0.86883829,\n",
       "        0.87515553, 0.87217895, 0.87560762, 0.87053863, 0.87199891,\n",
       "        0.87208293, 0.8750155 , 0.87213094, 0.87253502, 0.87437538,\n",
       "        0.87116275, 0.86815416, 0.86917436, 0.87433937, 0.87350321,\n",
       "        0.86963045, 0.87393129, 0.8739833 , 0.87580366, 0.87259103,\n",
       "        0.87289109, 0.87511952, 0.87309113, 0.87527155, 0.87393129,\n",
       "        0.87422735, 0.87445139, 0.87539558]),\n",
       " 'split4_test_roc_auc_ovo': array([0.86415737, 0.85890835, 0.85693196, 0.85593176, 0.81920856,\n",
       "        0.84971054, 0.82391749, 0.83091886, 0.83991662, 0.82835836,\n",
       "        0.82273326, 0.83426752, 0.86245704, 0.86504555, 0.86535361,\n",
       "        0.84935447, 0.83820429, 0.84306924, 0.82837036, 0.83035875,\n",
       "        0.83944453, 0.8428572 , 0.85361131, 0.81150305, 0.85683994,\n",
       "        0.86303716, 0.85923641, 0.84507363, 0.84221307, 0.84175298,\n",
       "        0.83913247, 0.83431553, 0.84170097, 0.83377542, 0.81308336,\n",
       "        0.82473365, 0.86366528, 0.86629379, 0.86458546, 0.84904241,\n",
       "        0.85597177, 0.8535713 , 0.84344131, 0.84164496, 0.83604787,\n",
       "        0.84789819, 0.83983261, 0.82571784, 0.85940444, 0.86820617,\n",
       "        0.85837224, 0.84496561, 0.852023  , 0.84996259, 0.83177903,\n",
       "        0.83509568, 0.83438754, 0.81828838, 0.81600794, 0.81239923,\n",
       "        0.85672792, 0.86719797, 0.85982052, 0.84629787, 0.8510188 ,\n",
       "        0.83456758, 0.83934851, 0.81808835, 0.82608991, 0.82899448,\n",
       "        0.82351341, 0.82277326, 0.86711395, 0.85998456, 0.86335722,\n",
       "        0.85441547, 0.84910242, 0.85038668, 0.82458562, 0.84018068,\n",
       "        0.83412749, 0.8270661 , 0.83232714, 0.81074691, 0.85946045,\n",
       "        0.85830423, 0.85769211, 0.84776616, 0.8515229 , 0.852055  ,\n",
       "        0.83689603, 0.83171902, 0.8413409 , 0.82185308, 0.82605391,\n",
       "        0.82853039, 0.86741401, 0.44937208, 0.86928638, 0.86931038,\n",
       "        0.23765858, 0.86707795, 0.86638181, 0.57712112, 0.86706594,\n",
       "        0.86673788, 0.36492753, 0.86875828, 0.86899832, 0.83900845,\n",
       "        0.8663458 , 0.86804614, 0.82936256, 0.86562566, 0.86647783,\n",
       "        0.57842137, 0.86769407, 0.86761805, 0.36371529, 0.86809015,\n",
       "        0.86995851, 0.74832667, 0.86749003, 0.86716196, 0.42640758,\n",
       "        0.86723798, 0.86721797, 0.52484287, 0.8668179 , 0.86730599,\n",
       "        0.24682438, 0.86804614, 0.86506555, 0.76760245, 0.86661786,\n",
       "        0.86606575, 0.78221331, 0.86788611, 0.86879028, 0.56483471,\n",
       "        0.86596973, 0.8678661 , 0.84090482, 0.86730199, 0.86424139,\n",
       "        0.63618469, 0.86436141, 0.86350925, 0.59636489, 0.86710595,\n",
       "        0.86330121, 0.70851487, 0.86517357, 0.86566567, 0.72488608,\n",
       "        0.86914235, 0.86229301, 0.66046145, 0.86149685, 0.86470948,\n",
       "        0.83277922, 0.86916636, 0.86339723, 0.46596733, 0.86677389,\n",
       "        0.86470148, 0.59281619, 0.86799413, 0.86103676, 0.59981356,\n",
       "        0.86252906, 0.86333321, 0.70806678, 0.86780209, 0.86339723,\n",
       "        0.69047933, 0.86389732, 0.86466948, 0.52661922, 0.86748203,\n",
       "        0.86155286, 0.72868282, 0.86132082, 0.86543763, 0.72078927,\n",
       "        0.86691392, 0.86037663, 0.56603494, 0.86778609, 0.86311317,\n",
       "        0.64367416, 0.86475749, 0.86819017, 0.8683422 , 0.86829018,\n",
       "        0.86864225, 0.86852223, 0.86868226, 0.86839421, 0.86833019,\n",
       "        0.86842221, 0.86808614, 0.8678381 , 0.86831819, 0.86893031,\n",
       "        0.8678821 , 0.86768607, 0.86803413, 0.86818216, 0.86830219,\n",
       "        0.86839421, 0.86791011, 0.86877428, 0.86842221, 0.86877028,\n",
       "        0.86883029, 0.86856624, 0.86873027, 0.86813816, 0.86811415,\n",
       "        0.8683462 , 0.8683542 , 0.86826218, 0.8688943 , 0.86799813,\n",
       "        0.86895432, 0.86833819, 0.86799013, 0.86818216, 0.86892231,\n",
       "        0.86822217, 0.86839421, 0.86848622, 0.86858624, 0.8688783 ,\n",
       "        0.86857024, 0.86840621, 0.86853023, 0.86817816, 0.86909434,\n",
       "        0.8648135 , 0.86822217, 0.86805814, 0.86849022, 0.86382131,\n",
       "        0.86536961, 0.86618577, 0.86136483, 0.86699393, 0.86499354,\n",
       "        0.86768207, 0.86518958, 0.86774208, 0.86424139, 0.86620178,\n",
       "        0.87047461, 0.86142884, 0.86658985, 0.86335322, 0.86808614,\n",
       "        0.86146085, 0.8642654 , 0.86484151, 0.87076667, 0.86750603,\n",
       "        0.8668499 , 0.86976247, 0.86666187, 0.86794612, 0.86415737,\n",
       "        0.86655785, 0.86297714, 0.86281711, 0.86487352, 0.86292113,\n",
       "        0.8637893 , 0.86878228, 0.86686591, 0.86515757, 0.86857824,\n",
       "        0.87032258, 0.86586571, 0.86764606, 0.86754204, 0.86689391,\n",
       "        0.86896232, 0.86767006, 0.8663098 ]),\n",
       " 'mean_test_roc_auc_ovo': array([0.86976647, 0.87112674, 0.86824218, 0.85702558, 0.852063  ,\n",
       "        0.85800817, 0.83912447, 0.84735808, 0.84350533, 0.83661278,\n",
       "        0.83584383, 0.83762818, 0.87026817, 0.87150441, 0.87226376,\n",
       "        0.85748247, 0.85434265, 0.85513001, 0.84547931, 0.83770819,\n",
       "        0.85535565, 0.83912527, 0.83967178, 0.82580106, 0.87033699,\n",
       "        0.86977368, 0.87054663, 0.85891955, 0.85495317, 0.86036863,\n",
       "        0.84809823, 0.84617625, 0.84811103, 0.8393021 , 0.83763218,\n",
       "        0.8326432 , 0.87013455, 0.8714484 , 0.87290629, 0.86007497,\n",
       "        0.85980692, 0.85826502, 0.85450028, 0.84758613, 0.85372493,\n",
       "        0.84857352, 0.84292601, 0.8372641 , 0.86955123, 0.87135879,\n",
       "        0.86739561, 0.85451108, 0.86058708, 0.86109197, 0.83763138,\n",
       "        0.83947654, 0.84104444, 0.83139015, 0.82516653, 0.82322455,\n",
       "        0.86616497, 0.8719557 , 0.86755964, 0.85042508, 0.85891475,\n",
       "        0.85450508, 0.83487404, 0.83933251, 0.84600342, 0.82340619,\n",
       "        0.82904089, 0.83591344, 0.86921597, 0.87037139, 0.87030818,\n",
       "        0.85945485, 0.85856268, 0.86194414, 0.83701686, 0.8433589 ,\n",
       "        0.84017827, 0.83044277, 0.8357054 , 0.82991306, 0.87049702,\n",
       "        0.87066825, 0.86753004, 0.85651268, 0.85899396, 0.86010458,\n",
       "        0.8484799 , 0.84480078, 0.84705402, 0.83482923, 0.83795704,\n",
       "        0.83217671, 0.8770223 , 0.49984597, 0.87722154, 0.87783526,\n",
       "        0.53976579, 0.87617413, 0.87678065, 0.6341739 , 0.87660461,\n",
       "        0.87678465, 0.57822773, 0.87717993, 0.87742398, 0.63144696,\n",
       "        0.87723434, 0.87667983, 0.67314874, 0.87594449, 0.87714312,\n",
       "        0.59610164, 0.87644298, 0.87675424, 0.46894951, 0.87709591,\n",
       "        0.87694628, 0.40892255, 0.87680465, 0.87750319, 0.56674068,\n",
       "        0.8770183 , 0.87689667, 0.70820921, 0.87741837, 0.87660541,\n",
       "        0.44649711, 0.8770375 , 0.87614933, 0.4989954 , 0.87661582,\n",
       "        0.87632376, 0.44824386, 0.87688067, 0.87691107, 0.66584731,\n",
       "        0.8765422 , 0.87667423, 0.62740297, 0.87623654, 0.87511152,\n",
       "        0.72563582, 0.87390489, 0.87599169, 0.69361915, 0.87640297,\n",
       "        0.87587087, 0.71292453, 0.87660701, 0.87610132, 0.66213538,\n",
       "        0.87713592, 0.87453381, 0.67377286, 0.87469304, 0.87558922,\n",
       "        0.65125485, 0.8749971 , 0.8755124 , 0.57242179, 0.87610532,\n",
       "        0.87775844, 0.67051062, 0.87557481, 0.87408012, 0.65139727,\n",
       "        0.87562362, 0.87423375, 0.61651284, 0.87557641, 0.87612372,\n",
       "        0.60199719, 0.87619413, 0.87621974, 0.72546059, 0.87732956,\n",
       "        0.87454981, 0.55886794, 0.87389608, 0.87641818, 0.7301183 ,\n",
       "        0.8760205 , 0.8755244 , 0.70750587, 0.87623814, 0.87771283,\n",
       "        0.73645074, 0.87607811, 0.87883385, 0.87887146, 0.8785842 ,\n",
       "        0.87896188, 0.87895468, 0.87893307, 0.87899068, 0.87879864,\n",
       "        0.87877544, 0.87870823, 0.87866102, 0.87881145, 0.87900348,\n",
       "        0.87861541, 0.87866182, 0.87875224, 0.87878584, 0.87880825,\n",
       "        0.87890747, 0.87872263, 0.87896268, 0.87887466, 0.87889226,\n",
       "        0.87883065, 0.87883945, 0.87894907, 0.87875064, 0.87873623,\n",
       "        0.87887226, 0.87873703, 0.87880905, 0.87884585, 0.87870743,\n",
       "        0.87895948, 0.87880185, 0.87861701, 0.87876264, 0.87872983,\n",
       "        0.87870663, 0.87863461, 0.87888106, 0.87882265, 0.87888826,\n",
       "        0.87872183, 0.8785962 , 0.87869862, 0.87884105, 0.87888106,\n",
       "        0.87666143, 0.87841457, 0.87741997, 0.87590448, 0.87556841,\n",
       "        0.8770591 , 0.87507311, 0.87413133, 0.87482987, 0.87354882,\n",
       "        0.87529556, 0.87407772, 0.87707831, 0.87698949, 0.87769123,\n",
       "        0.87714632, 0.87469544, 0.87566043, 0.87496349, 0.87445619,\n",
       "        0.87404091, 0.87386968, 0.8750203 , 0.87479866, 0.8775312 ,\n",
       "        0.87641418, 0.87885706, 0.87570364, 0.87511392, 0.87638537,\n",
       "        0.87454981, 0.87333437, 0.87467224, 0.87515153, 0.8739897 ,\n",
       "        0.87204852, 0.87785846, 0.87782325, 0.87772723, 0.87750239,\n",
       "        0.87757761, 0.87764962, 0.87723674, 0.87784806, 0.87724154,\n",
       "        0.87723034, 0.87722234, 0.87635337]),\n",
       " 'std_test_roc_auc_ovo': array([0.00842936, 0.0102967 , 0.01024107, 0.00658306, 0.01906315,\n",
       "        0.00868133, 0.01341514, 0.0107803 , 0.00996457, 0.00670975,\n",
       "        0.0136853 , 0.00655724, 0.00650482, 0.00675945, 0.00675279,\n",
       "        0.0051493 , 0.01155555, 0.01193258, 0.0120667 , 0.00515077,\n",
       "        0.01203686, 0.00493689, 0.00710847, 0.00980964, 0.00897372,\n",
       "        0.00719235, 0.00899324, 0.01217593, 0.00879152, 0.01179571,\n",
       "        0.01173123, 0.00999925, 0.00998686, 0.00741676, 0.01294489,\n",
       "        0.00605303, 0.00720088, 0.00785453, 0.00737401, 0.01080264,\n",
       "        0.00604013, 0.00686352, 0.01006166, 0.00572561, 0.01032612,\n",
       "        0.00865484, 0.00958634, 0.00851571, 0.0094553 , 0.00484916,\n",
       "        0.00733095, 0.00821969, 0.00692893, 0.00747097, 0.00933121,\n",
       "        0.00902553, 0.01724139, 0.00888406, 0.00638687, 0.00810567,\n",
       "        0.00958094, 0.00735547, 0.00658153, 0.01058657, 0.00914852,\n",
       "        0.01344708, 0.00742783, 0.01270571, 0.01422589, 0.00644095,\n",
       "        0.01460353, 0.00835434, 0.00922297, 0.00923756, 0.00594616,\n",
       "        0.00833944, 0.0069882 , 0.01048358, 0.00699505, 0.0126404 ,\n",
       "        0.00995457, 0.01263683, 0.0035172 , 0.01282449, 0.01229802,\n",
       "        0.00876094, 0.00600919, 0.00559727, 0.00622756, 0.00756307,\n",
       "        0.00701807, 0.00721173, 0.00727637, 0.00765274, 0.01282683,\n",
       "        0.01098647, 0.00864122, 0.22121713, 0.00827529, 0.00806308,\n",
       "        0.20467104, 0.00849935, 0.00885959, 0.16557893, 0.00887187,\n",
       "        0.0080286 , 0.1574749 , 0.00790394, 0.0077765 , 0.25471523,\n",
       "        0.00890408, 0.00816952, 0.18221074, 0.00872029, 0.00881054,\n",
       "        0.20203724, 0.00822179, 0.00855826, 0.19618395, 0.00790245,\n",
       "        0.00763662, 0.18614567, 0.00796718, 0.0087059 , 0.24823174,\n",
       "        0.00856909, 0.00814955, 0.12318668, 0.00836637, 0.00869865,\n",
       "        0.20248578, 0.00862228, 0.00907808, 0.24875098, 0.00906137,\n",
       "        0.00837735, 0.25149157, 0.00872693, 0.00811001, 0.14373667,\n",
       "        0.00885442, 0.00817386, 0.20403894, 0.00828344, 0.00845109,\n",
       "        0.10518215, 0.00846365, 0.00954372, 0.09044262, 0.00831226,\n",
       "        0.00905154, 0.05348901, 0.00791375, 0.00809327, 0.15118735,\n",
       "        0.0065993 , 0.00973917, 0.03939381, 0.00926731, 0.00886946,\n",
       "        0.16183482, 0.00849307, 0.00894991, 0.06886814, 0.00885965,\n",
       "        0.00907697, 0.07236093, 0.00803769, 0.00914688, 0.05026613,\n",
       "        0.00933147, 0.00868379, 0.14191829, 0.00806746, 0.00838406,\n",
       "        0.10765872, 0.00948625, 0.00972231, 0.10750075, 0.00783687,\n",
       "        0.00930962, 0.18180707, 0.00939831, 0.00792766, 0.08989579,\n",
       "        0.00880759, 0.01033793, 0.09243007, 0.00810238, 0.00973392,\n",
       "        0.05092508, 0.0084962 , 0.00774765, 0.0077059 , 0.00793017,\n",
       "        0.00766582, 0.0078872 , 0.0077352 , 0.00795844, 0.00781   ,\n",
       "        0.00776633, 0.00804241, 0.00800789, 0.00782697, 0.00804828,\n",
       "        0.0079673 , 0.00815651, 0.00793712, 0.00796866, 0.00782036,\n",
       "        0.00775922, 0.00790723, 0.00776419, 0.0077453 , 0.00771887,\n",
       "        0.00769572, 0.00773126, 0.007831  , 0.00823167, 0.00800341,\n",
       "        0.00786635, 0.00776817, 0.00794358, 0.00773328, 0.00792059,\n",
       "        0.00770426, 0.00790223, 0.00784958, 0.00792774, 0.00766095,\n",
       "        0.00786285, 0.00786995, 0.00790741, 0.00775674, 0.00769636,\n",
       "        0.00782651, 0.00778849, 0.00773645, 0.00790487, 0.00753517,\n",
       "        0.0086634 , 0.00720539, 0.00705175, 0.00768239, 0.00820574,\n",
       "        0.00824548, 0.0078277 , 0.00797312, 0.00661489, 0.00887772,\n",
       "        0.00807947, 0.00860941, 0.00706649, 0.0084098 , 0.00776445,\n",
       "        0.0055239 , 0.00978172, 0.00780688, 0.00794976, 0.00735395,\n",
       "        0.00909283, 0.00710963, 0.00764217, 0.00659447, 0.008963  ,\n",
       "        0.00760709, 0.00690473, 0.00719543, 0.00705867, 0.00824935,\n",
       "        0.00808893, 0.0093012 , 0.00880342, 0.00730087, 0.00738981,\n",
       "        0.00693539, 0.00749557, 0.00814287, 0.00835197, 0.00792225,\n",
       "        0.00735618, 0.0079737 , 0.00835542, 0.00808887, 0.0083968 ,\n",
       "        0.00702839, 0.00798187, 0.00765187]),\n",
       " 'rank_test_roc_auc_ovo': array([177, 167, 180, 200, 210, 198, 233, 217, 223, 241, 243, 238, 174,\n",
       "        164, 161, 199, 208, 203, 221, 235, 202, 232, 228, 253, 172, 176,\n",
       "        169, 194, 204, 188, 215, 219, 214, 231, 236, 247, 175, 165, 160,\n",
       "        190, 191, 197, 207, 216, 209, 212, 225, 239, 178, 166, 183, 205,\n",
       "        187, 186, 237, 229, 226, 249, 254, 256, 184, 163, 181, 211, 195,\n",
       "        206, 245, 230, 220, 255, 252, 242, 179, 171, 173, 192, 196, 185,\n",
       "        240, 224, 227, 250, 244, 251, 170, 168, 182, 201, 193, 189, 213,\n",
       "        222, 218, 246, 234, 248,  82, 283,  73,  53, 282, 112,  91, 272,\n",
       "         99,  90, 278,  74,  64, 273,  70,  93, 266, 120,  76, 277, 101,\n",
       "         92, 285,  78,  85, 288,  89,  62, 280,  83,  87, 262,  66,  98,\n",
       "        287,  81, 113, 284,  96, 107, 286,  88,  86, 268, 100,  94, 274,\n",
       "        109, 135, 259, 155, 119, 264, 104, 122, 261,  97, 116, 269,  77,\n",
       "        147, 265, 143, 126, 271, 138, 131, 279, 115,  55, 267, 128, 151,\n",
       "        270, 125, 149, 275, 127, 114, 276, 111, 110, 260,  67, 145, 281,\n",
       "        156, 102, 258, 118, 130, 263, 108,  57, 257, 117,  21,  16,  49,\n",
       "          4,   6,   8,   2,  28,  30,  39,  44,  24,   1,  47,  43,  32,\n",
       "         29,  26,   9,  37,   3,  14,  10,  22,  20,   7,  33,  35,  15,\n",
       "         34,  25,  18,  40,   5,  27,  46,  31,  36,  41,  45,  12,  23,\n",
       "         11,  38,  48,  42,  19,  12,  95,  50,  65, 121, 129,  80, 136,\n",
       "        150, 140, 158, 132, 152,  79,  84,  58,  75, 142, 124, 139, 148,\n",
       "        153, 157, 137, 141,  61, 103,  17, 123, 134, 105, 146, 159, 144,\n",
       "        133, 154, 162,  51,  54,  56,  63,  60,  59,  69,  52,  68,  71,\n",
       "         72, 106]),\n",
       " 'split0_test_jaccard': array([0.65814696, 0.66666667, 0.66666667, 0.6416    , 0.65849673,\n",
       "        0.63302752, 0.6228482 , 0.62261146, 0.63355049, 0.62341772,\n",
       "        0.62618297, 0.62162162, 0.65957447, 0.66339869, 0.65528455,\n",
       "        0.62998405, 0.63782051, 0.61477987, 0.63866878, 0.60842434,\n",
       "        0.6507177 , 0.61741835, 0.6108453 , 0.58604651, 0.6528    ,\n",
       "        0.66775244, 0.6710311 , 0.65681445, 0.65015974, 0.64057508,\n",
       "        0.63942308, 0.61466459, 0.62962963, 0.62776025, 0.63738019,\n",
       "        0.6078125 , 0.66939444, 0.672     , 0.66503268, 0.66611842,\n",
       "        0.63285024, 0.63430421, 0.63915858, 0.63434022, 0.63947798,\n",
       "        0.65876777, 0.60808709, 0.61198738, 0.67862969, 0.66449511,\n",
       "        0.66721044, 0.63106796, 0.6485623 , 0.64772727, 0.61550633,\n",
       "        0.61490683, 0.62420382, 0.59968354, 0.61102362, 0.59404389,\n",
       "        0.65857605, 0.67092652, 0.65971108, 0.61488673, 0.65755627,\n",
       "        0.66068515, 0.6056338 , 0.61049285, 0.60787402, 0.59968102,\n",
       "        0.62939297, 0.61838006, 0.6738056 , 0.66885246, 0.65217391,\n",
       "        0.6504065 , 0.62480127, 0.6528    , 0.63192182, 0.64365971,\n",
       "        0.63285024, 0.61476726, 0.62282092, 0.60273973, 0.66503268,\n",
       "        0.6699187 , 0.66829268, 0.64686998, 0.64205457, 0.64912281,\n",
       "        0.61269841, 0.63870968, 0.63492063, 0.61815336, 0.62204724,\n",
       "        0.6133122 , 0.6683087 , 0.32532348, 0.6748366 , 0.67487685,\n",
       "        0.507     , 0.66721311, 0.67430442, 0.        , 0.6748366 ,\n",
       "        0.67377049, 0.50801603, 0.68032787, 0.67536705, 0.55703704,\n",
       "        0.68256579, 0.67213115, 0.04669261, 0.66939444, 0.66557912,\n",
       "        0.08829175, 0.67810458, 0.67757774, 0.01941748, 0.67545305,\n",
       "        0.66885246, 0.        , 0.67105263, 0.67656766, 0.05078125,\n",
       "        0.67323481, 0.68092105, 0.507     , 0.67487685, 0.6742671 ,\n",
       "        0.506     , 0.67540984, 0.67980296, 0.        , 0.66448445,\n",
       "        0.66721311, 0.        , 0.67430442, 0.66993464, 0.51263903,\n",
       "        0.67810458, 0.67161716, 0.507     , 0.66121113, 0.65676568,\n",
       "        0.50782065, 0.66985646, 0.67816092, 0.50678733, 0.67317073,\n",
       "        0.65905383, 0.41597338, 0.68144499, 0.67589577, 0.27899687,\n",
       "        0.67156863, 0.66940789, 0.51251647, 0.67045455, 0.66993464,\n",
       "        0.4       , 0.65798046, 0.66556837, 0.48578199, 0.66559486,\n",
       "        0.67377049, 0.49674267, 0.66176471, 0.67043619, 0.50125628,\n",
       "        0.66290323, 0.67100977, 0.39912759, 0.66062603, 0.67940199,\n",
       "        0.52591793, 0.6618123 , 0.66721044, 0.57160963, 0.66830065,\n",
       "        0.67366721, 0.22258065, 0.66775244, 0.66939444, 0.59729064,\n",
       "        0.66176471, 0.66993464, 0.6015748 , 0.67098865, 0.67647059,\n",
       "        0.15860735, 0.66178862, 0.67149758, 0.67540984, 0.67317073,\n",
       "        0.67536705, 0.67594108, 0.67263844, 0.67154472, 0.67540984,\n",
       "        0.6748366 , 0.67540984, 0.67098865, 0.67373573, 0.67524116,\n",
       "        0.67045455, 0.67594108, 0.67152104, 0.67263844, 0.6748366 ,\n",
       "        0.6721044 , 0.67263844, 0.67377049, 0.67816092, 0.67704918,\n",
       "        0.67540984, 0.67266776, 0.67647059, 0.67479675, 0.67154472,\n",
       "        0.67045455, 0.67045455, 0.67263844, 0.67373573, 0.66883117,\n",
       "        0.67594108, 0.67651888, 0.67373573, 0.67263844, 0.67487685,\n",
       "        0.6737013 , 0.67213115, 0.67373573, 0.67373573, 0.67373573,\n",
       "        0.6721044 , 0.67540984, 0.67373573, 0.6748366 , 0.67704918,\n",
       "        0.67423015, 0.68026101, 0.67159278, 0.64991896, 0.67757774,\n",
       "        0.68026101, 0.67215815, 0.66882068, 0.65533981, 0.67266776,\n",
       "        0.67423015, 0.66775244, 0.66721044, 0.66990291, 0.67258065,\n",
       "        0.68464052, 0.67152104, 0.6748366 , 0.67647059, 0.65814696,\n",
       "        0.67159278, 0.66995074, 0.65695793, 0.66288493, 0.66720517,\n",
       "        0.66774716, 0.6792144 , 0.6737013 , 0.66233766, 0.67536705,\n",
       "        0.67540984, 0.67804878, 0.6792144 , 0.67323481, 0.67156863,\n",
       "        0.6607717 , 0.6763754 , 0.6699187 , 0.66288493, 0.67045455,\n",
       "        0.66775244, 0.68300654, 0.65533981, 0.6875    , 0.66178862,\n",
       "        0.67585089, 0.6792144 , 0.67915309]),\n",
       " 'split1_test_jaccard': array([0.63252033, 0.61437908, 0.63426689, 0.5904    , 0.62276423,\n",
       "        0.6318328 , 0.5890625 , 0.60487805, 0.60128617, 0.61129032,\n",
       "        0.60031847, 0.61400651, 0.63446055, 0.62479871, 0.63261944,\n",
       "        0.60620915, 0.60987261, 0.62908497, 0.6224    , 0.62193126,\n",
       "        0.65275459, 0.5927673 , 0.61066236, 0.59618442, 0.62852405,\n",
       "        0.63562092, 0.63382594, 0.64098361, 0.62398703, 0.63011457,\n",
       "        0.61403509, 0.62113821, 0.60576923, 0.61803279, 0.59061489,\n",
       "        0.60355987, 0.62783172, 0.62316476, 0.62908497, 0.6076555 ,\n",
       "        0.62684124, 0.62559242, 0.63798701, 0.60314961, 0.63338789,\n",
       "        0.592     , 0.58776167, 0.6046875 , 0.64157119, 0.64725458,\n",
       "        0.63218391, 0.62561576, 0.63122924, 0.63592233, 0.61227787,\n",
       "        0.60897436, 0.58990536, 0.60192616, 0.57827476, 0.60708535,\n",
       "        0.6272578 , 0.6195122 , 0.63902439, 0.62318841, 0.62642741,\n",
       "        0.63036304, 0.59587956, 0.61374795, 0.64052288, 0.6025641 ,\n",
       "        0.58647799, 0.6       , 0.63426689, 0.63235294, 0.63606557,\n",
       "        0.62236629, 0.63636364, 0.63157895, 0.61736334, 0.6066879 ,\n",
       "        0.6064    , 0.57006369, 0.6       , 0.60543131, 0.6147541 ,\n",
       "        0.63157895, 0.63486842, 0.61463415, 0.62438221, 0.63242376,\n",
       "        0.60967742, 0.62419355, 0.60325203, 0.59546926, 0.58869702,\n",
       "        0.56037152, 0.655     , 0.09078947, 0.6559322 , 0.64489112,\n",
       "        0.5158002 , 0.65050167, 0.64765101, 0.        , 0.64932886,\n",
       "        0.64656616, 0.507     , 0.65488215, 0.64656616, 0.507     ,\n",
       "        0.64924115, 0.64548495, 0.        , 0.6463621 , 0.64983165,\n",
       "        0.50801603, 0.64559068, 0.65100671, 0.14615385, 0.6487395 ,\n",
       "        0.64705882, 0.        , 0.64369748, 0.65661642, 0.507     ,\n",
       "        0.65159129, 0.65319865, 0.507     , 0.64537815, 0.64705882,\n",
       "        0.507     , 0.65371622, 0.64705882, 0.00702988, 0.65326633,\n",
       "        0.64369748, 0.00187266, 0.65313029, 0.64537815, 0.507     ,\n",
       "        0.65100671, 0.65100671, 0.        , 0.65378151, 0.66229508,\n",
       "        0.49598716, 0.65024631, 0.65306122, 0.52696629, 0.64154104,\n",
       "        0.63819095, 0.23853211, 0.66386555, 0.65557404, 0.34404762,\n",
       "        0.64784053, 0.63227953, 0.37683284, 0.63577236, 0.64941569,\n",
       "        0.45847554, 0.63210702, 0.63879599, 0.42840512, 0.64924115,\n",
       "        0.64013267, 0.39937107, 0.63787375, 0.65609349, 0.49095607,\n",
       "        0.63833333, 0.65166667, 0.43548387, 0.64932886, 0.64      ,\n",
       "        0.20656635, 0.640599  , 0.64321608, 0.47076923, 0.65050167,\n",
       "        0.64106845, 0.12106918, 0.64814815, 0.63651591, 0.48194271,\n",
       "        0.63865546, 0.65066225, 0.20373832, 0.64356436, 0.65217391,\n",
       "        0.43633277, 0.64833333, 0.64106845, 0.64285714, 0.6384743 ,\n",
       "        0.640599  , 0.6384743 , 0.6384743 , 0.64559068, 0.6384743 ,\n",
       "        0.64403974, 0.64344942, 0.64179104, 0.64013267, 0.64607679,\n",
       "        0.63953488, 0.64285714, 0.64403974, 0.64013267, 0.64179104,\n",
       "        0.6422629 , 0.64392679, 0.64451827, 0.640599  , 0.64285714,\n",
       "        0.64344942, 0.63953488, 0.64013267, 0.64569536, 0.63833333,\n",
       "        0.64179104, 0.64510779, 0.64285714, 0.64344942, 0.64013267,\n",
       "        0.64333333, 0.64344942, 0.64285714, 0.64510779, 0.64046823,\n",
       "        0.64119601, 0.64013267, 0.64510779, 0.64344942, 0.64238411,\n",
       "        0.6427379 , 0.6427379 , 0.64321608, 0.64392679, 0.64179104,\n",
       "        0.64676617, 0.6369637 , 0.64238411, 0.64958678, 0.64891847,\n",
       "        0.64309211, 0.64774624, 0.63531353, 0.63546798, 0.64437194,\n",
       "        0.6497545 , 0.64860427, 0.64510779, 0.64072848, 0.64039409,\n",
       "        0.65231788, 0.6446281 , 0.64039409, 0.65824916, 0.63741722,\n",
       "        0.62969005, 0.64891847, 0.640599  , 0.63591433, 0.64309211,\n",
       "        0.64784053, 0.65108514, 0.64991625, 0.64085667, 0.64569536,\n",
       "        0.6369637 , 0.63278689, 0.65123967, 0.640599  , 0.63606557,\n",
       "        0.62254902, 0.64891847, 0.64072848, 0.64415157, 0.64941569,\n",
       "        0.65058236, 0.64860427, 0.64958678, 0.645     , 0.64715719,\n",
       "        0.65625   , 0.65066225, 0.63966942]),\n",
       " 'split2_test_jaccard': array([0.65671642, 0.67218543, 0.67169179, 0.64403974, 0.65217391,\n",
       "        0.64705882, 0.63517915, 0.63942308, 0.65296053, 0.63242376,\n",
       "        0.62843296, 0.61244019, 0.66498316, 0.66441821, 0.66118421,\n",
       "        0.63857374, 0.65857605, 0.66721582, 0.64343598, 0.61661342,\n",
       "        0.65454545, 0.64448052, 0.6064    , 0.60483871, 0.67054908,\n",
       "        0.66943522, 0.65953947, 0.63829787, 0.64705882, 0.65252855,\n",
       "        0.6365105 , 0.63929147, 0.64203612, 0.61138211, 0.6228482 ,\n",
       "        0.62279294, 0.66174056, 0.66942149, 0.67107438, 0.64772727,\n",
       "        0.64686469, 0.63489499, 0.63980263, 0.64415157, 0.63387097,\n",
       "        0.64696486, 0.63782051, 0.61736334, 0.67272727, 0.66946309,\n",
       "        0.64676617, 0.65737705, 0.65686275, 0.64744646, 0.62560778,\n",
       "        0.63782051, 0.62908497, 0.61295419, 0.61182109, 0.59330144,\n",
       "        0.6748366 , 0.66721854, 0.67445743, 0.6360601 , 0.66006601,\n",
       "        0.63947798, 0.61897106, 0.64847512, 0.64552846, 0.5862069 ,\n",
       "        0.63355049, 0.62898089, 0.67438017, 0.6672213 , 0.65728477,\n",
       "        0.65517241, 0.62722853, 0.66019417, 0.61980831, 0.64573269,\n",
       "        0.63461538, 0.63278689, 0.60940032, 0.62459547, 0.67326733,\n",
       "        0.6523888 , 0.64437194, 0.6215781 , 0.64215686, 0.64437194,\n",
       "        0.65203252, 0.62123613, 0.63947798, 0.6076555 , 0.64600326,\n",
       "        0.61066236, 0.69932432, 0.507     , 0.69087838, 0.6910017 ,\n",
       "        0.5399754 , 0.68412162, 0.69035533, 0.50854271, 0.68235294,\n",
       "        0.68518519, 0.507     , 0.68971332, 0.68020305, 0.51263903,\n",
       "        0.67785235, 0.68644068, 0.42842215, 0.68474576, 0.68581081,\n",
       "        0.08333333, 0.6846543 , 0.68243243, 0.01574803, 0.69047619,\n",
       "        0.69243697, 0.507     , 0.67398649, 0.68697124, 0.        ,\n",
       "        0.68067227, 0.6846543 , 0.        , 0.68235294, 0.6910017 ,\n",
       "        0.30076628, 0.69035533, 0.68403361, 0.17777778, 0.68128162,\n",
       "        0.67731092, 0.40262361, 0.6893039 , 0.68474576, 0.65583456,\n",
       "        0.68983051, 0.68813559, 0.24913892, 0.67959528, 0.6705298 ,\n",
       "        0.61055276, 0.67563025, 0.67402377, 0.63920455, 0.67774086,\n",
       "        0.6779661 , 0.47680412, 0.67      , 0.66946309, 0.46231156,\n",
       "        0.6722408 , 0.6751269 , 0.51223022, 0.67567568, 0.68013468,\n",
       "        0.4068323 , 0.68412162, 0.67607973, 0.48029819, 0.68288591,\n",
       "        0.68350168, 0.52240896, 0.6779661 , 0.67671692, 0.32653061,\n",
       "        0.66721582, 0.675     , 0.61422709, 0.67330017, 0.68020305,\n",
       "        0.22298456, 0.68802698, 0.67681895, 0.60773481, 0.6744186 ,\n",
       "        0.66555184, 0.56486043, 0.68106312, 0.67333333, 0.58984375,\n",
       "        0.6875    , 0.67785235, 0.38484848, 0.678511  , 0.67558528,\n",
       "        0.58951965, 0.67114094, 0.67833333, 0.67827529, 0.67666667,\n",
       "        0.68456376, 0.68447412, 0.68272425, 0.6789916 , 0.67671692,\n",
       "        0.67892977, 0.67946578, 0.68060201, 0.67779633, 0.67731092,\n",
       "        0.67946578, 0.67715232, 0.67558528, 0.68128162, 0.67774086,\n",
       "        0.67839196, 0.67725753, 0.68333333, 0.67892977, 0.67839196,\n",
       "        0.68060201, 0.67391304, 0.68280467, 0.68386023, 0.67779633,\n",
       "        0.67839196, 0.67940199, 0.67892977, 0.68959732, 0.68      ,\n",
       "        0.67671692, 0.68181818, 0.68219634, 0.68181818, 0.67790894,\n",
       "        0.68074324, 0.68067227, 0.68561873, 0.68067227, 0.68174204,\n",
       "        0.68509213, 0.68013468, 0.68174204, 0.68341709, 0.67839196,\n",
       "        0.67666667, 0.6677686 , 0.67508418, 0.66943522, 0.6627907 ,\n",
       "        0.6744186 , 0.67656766, 0.67774086, 0.66499162, 0.66393443,\n",
       "        0.67107438, 0.68697124, 0.66110184, 0.66944908, 0.65837479,\n",
       "        0.65671642, 0.67839196, 0.665     , 0.66338259, 0.67326733,\n",
       "        0.67720466, 0.68394649, 0.6846543 , 0.6705298 , 0.68060201,\n",
       "        0.67880795, 0.66499162, 0.6677686 , 0.67833333, 0.66942149,\n",
       "        0.68527919, 0.66666667, 0.6661157 , 0.67554077, 0.66721311,\n",
       "        0.6683087 , 0.67      , 0.67892977, 0.68333333, 0.68120805,\n",
       "        0.67275748, 0.67558528, 0.67720466, 0.66556291, 0.6672213 ,\n",
       "        0.66611842, 0.6661157 , 0.66886326]),\n",
       " 'split3_test_jaccard': array([0.6432    , 0.64935065, 0.64123377, 0.62276423, 0.60806452,\n",
       "        0.62580645, 0.60932476, 0.63242376, 0.60355987, 0.61821086,\n",
       "        0.5975039 , 0.61501597, 0.65008026, 0.64262295, 0.65089723,\n",
       "        0.62698413, 0.61661342, 0.64526485, 0.60855784, 0.61783439,\n",
       "        0.60987261, 0.60890302, 0.60543131, 0.61049285, 0.63915858,\n",
       "        0.64251208, 0.64274322, 0.60192616, 0.648     , 0.63295269,\n",
       "        0.61562998, 0.63402889, 0.616     , 0.59904913, 0.63768116,\n",
       "        0.61155698, 0.625     , 0.6557377 , 0.64239482, 0.64638158,\n",
       "        0.64052288, 0.62520194, 0.61897106, 0.62700965, 0.60855784,\n",
       "        0.63081862, 0.6304    , 0.6191248 , 0.66233766, 0.64820847,\n",
       "        0.63004847, 0.63312693, 0.62618297, 0.6272    , 0.61860465,\n",
       "        0.62279294, 0.62197092, 0.59183673, 0.58823529, 0.57805255,\n",
       "        0.63577236, 0.6465798 , 0.62579618, 0.60932476, 0.63106796,\n",
       "        0.63636364, 0.61858974, 0.59349593, 0.61316212, 0.58869702,\n",
       "        0.58641975, 0.62085308, 0.63754045, 0.64763458, 0.65640194,\n",
       "        0.60543131, 0.60806452, 0.64920635, 0.61172742, 0.5862069 ,\n",
       "        0.59968354, 0.63149606, 0.62047244, 0.60420032, 0.66721044,\n",
       "        0.64935065, 0.65793781, 0.63754045, 0.63317384, 0.63295269,\n",
       "        0.62962963, 0.61049285, 0.60191083, 0.6       , 0.6144    ,\n",
       "        0.61476726, 0.65252855, 0.54454343, 0.64983713, 0.65309446,\n",
       "        0.02238806, 0.65472313, 0.6504065 , 0.        , 0.64811784,\n",
       "        0.64552846, 0.00591716, 0.64495114, 0.64600326, 0.        ,\n",
       "        0.64715447, 0.64983713, 0.        , 0.65960912, 0.64648118,\n",
       "        0.54126474, 0.64379085, 0.65302782, 0.507     , 0.64320786,\n",
       "        0.64926591, 0.46240989, 0.65365854, 0.64401294, 0.        ,\n",
       "        0.65203252, 0.65139116, 0.54408602, 0.64983713, 0.64437194,\n",
       "        0.19340974, 0.65188834, 0.65957447, 0.507     , 0.6442623 ,\n",
       "        0.65048544, 0.52680653, 0.64918033, 0.65746753, 0.507     ,\n",
       "        0.64820847, 0.64983713, 0.507     , 0.64820847, 0.65625   ,\n",
       "        0.5964691 , 0.64573269, 0.65296053, 0.4524181 , 0.64958678,\n",
       "        0.64926591, 0.5589172 , 0.66065574, 0.65733114, 0.6215847 ,\n",
       "        0.64590164, 0.64793388, 0.33558179, 0.65309446, 0.65853659,\n",
       "        0.54421769, 0.65252855, 0.6503268 , 0.44182622, 0.65346535,\n",
       "        0.63920923, 0.34502924, 0.64437194, 0.63888889, 0.28805237,\n",
       "        0.6489533 , 0.65533981, 0.50248139, 0.6503268 , 0.65403624,\n",
       "        0.42644628, 0.65522876, 0.64754098, 0.5504    , 0.64869281,\n",
       "        0.64354839, 0.46728016, 0.64638158, 0.64590164, 0.24181818,\n",
       "        0.65905383, 0.65409836, 0.6070922 , 0.65359477, 0.65353038,\n",
       "        0.56914894, 0.65415987, 0.64484452, 0.65210356, 0.64552846,\n",
       "        0.65528455, 0.65081967, 0.64715447, 0.64926591, 0.64820847,\n",
       "        0.64983713, 0.65302782, 0.64983713, 0.64869281, 0.64935065,\n",
       "        0.65640194, 0.64552846, 0.65089723, 0.64763458, 0.65365854,\n",
       "        0.64552846, 0.64983713, 0.64926591, 0.65139116, 0.6465798 ,\n",
       "        0.64926591, 0.65309446, 0.64715447, 0.64379085, 0.64820847,\n",
       "        0.64600326, 0.65089723, 0.65252855, 0.65252855, 0.64878049,\n",
       "        0.64763458, 0.64983713, 0.64926591, 0.64869281, 0.65316045,\n",
       "        0.6504065 , 0.6503268 , 0.64983713, 0.64820847, 0.64820847,\n",
       "        0.64983713, 0.6514658 , 0.6497545 , 0.64983713, 0.64820847,\n",
       "        0.64781906, 0.64505673, 0.6514658 , 0.64781906, 0.65746753,\n",
       "        0.65203252, 0.64983713, 0.64943457, 0.64227642, 0.65630115,\n",
       "        0.64772727, 0.66343042, 0.6504065 , 0.64869281, 0.65089723,\n",
       "        0.6525974 , 0.6407767 , 0.65640194, 0.65691057, 0.63843648,\n",
       "        0.65365854, 0.65686275, 0.65372168, 0.64677419, 0.63768116,\n",
       "        0.6407767 , 0.65533981, 0.64181524, 0.64448052, 0.64715447,\n",
       "        0.64878049, 0.64715447, 0.64772727, 0.64563107, 0.6461039 ,\n",
       "        0.65316045, 0.64935065, 0.65378422, 0.66176471, 0.64667747,\n",
       "        0.64878049, 0.64648118, 0.6461039 , 0.65153971, 0.64495114,\n",
       "        0.65153971, 0.64620355, 0.65089723]),\n",
       " 'split4_test_jaccard': array([0.63809524, 0.62225705, 0.61993769, 0.63853503, 0.6043956 ,\n",
       "        0.63207547, 0.58320611, 0.62996942, 0.59105431, 0.61111111,\n",
       "        0.61860465, 0.6122449 , 0.6349454 , 0.63849765, 0.63380282,\n",
       "        0.60186625, 0.61285266, 0.62870515, 0.59969325, 0.62421384,\n",
       "        0.61645963, 0.62679426, 0.63765823, 0.60522273, 0.62597809,\n",
       "        0.63779528, 0.628125  , 0.62015504, 0.61984733, 0.62695925,\n",
       "        0.59814529, 0.61145511, 0.59667674, 0.60990712, 0.60368664,\n",
       "        0.61285266, 0.62635659, 0.63239875, 0.64162754, 0.62893082,\n",
       "        0.634375  , 0.6244204 , 0.61455108, 0.62951334, 0.60747664,\n",
       "        0.61949686, 0.61778471, 0.60983103, 0.63157895, 0.64864865,\n",
       "        0.6310832 , 0.63806552, 0.62578616, 0.63809524, 0.60276074,\n",
       "        0.63866878, 0.62461538, 0.6043956 , 0.58039816, 0.60582822,\n",
       "        0.63026521, 0.65544872, 0.63622047, 0.6096423 , 0.64094488,\n",
       "        0.61489699, 0.61490683, 0.60121766, 0.60124611, 0.60658307,\n",
       "        0.60153846, 0.6056338 , 0.64150943, 0.63091483, 0.64409449,\n",
       "        0.62951334, 0.63214838, 0.64603175, 0.60578387, 0.61741835,\n",
       "        0.62211982, 0.60615385, 0.59937402, 0.59512938, 0.625387  ,\n",
       "        0.63100775, 0.62888199, 0.62794349, 0.64423077, 0.6296875 ,\n",
       "        0.62910798, 0.61464968, 0.61265432, 0.58396369, 0.62037037,\n",
       "        0.62106918, 0.63449367, 0.50351053, 0.64114833, 0.63507109,\n",
       "        0.03888025, 0.62816456, 0.62992126, 0.05098039, 0.63291139,\n",
       "        0.63391442, 0.        , 0.63924051, 0.6340694 , 0.507     ,\n",
       "        0.63708399, 0.63033175, 0.        , 0.63291139, 0.63050314,\n",
       "        0.507     , 0.63809524, 0.63578275, 0.        , 0.63866878,\n",
       "        0.63853503, 0.        , 0.63765823, 0.63333333, 0.        ,\n",
       "        0.63050314, 0.63191153, 0.41943419, 0.6327504 , 0.63174603,\n",
       "        0.43491423, 0.6340694 , 0.63265306, 0.        , 0.63622047,\n",
       "        0.63232964, 0.48103448, 0.63622047, 0.63249211, 0.507     ,\n",
       "        0.63349131, 0.63507109, 0.        , 0.63349131, 0.64240506,\n",
       "        0.50868233, 0.63968254, 0.628125  , 0.46792453, 0.63464567,\n",
       "        0.63449367, 0.54545455, 0.62910798, 0.62852665, 0.52617801,\n",
       "        0.62757528, 0.63317384, 0.42964072, 0.6310832 , 0.63536776,\n",
       "        0.59493671, 0.6608    , 0.6336478 , 0.29895561, 0.65489567,\n",
       "        0.6340694 , 0.35      , 0.63507109, 0.62635659, 0.34690799,\n",
       "        0.62865948, 0.63751987, 0.38568935, 0.63823065, 0.63522013,\n",
       "        0.52460984, 0.6328125 , 0.6242236 , 0.11978221, 0.63009404,\n",
       "        0.634375  , 0.52496626, 0.63679245, 0.63449367, 0.357022  ,\n",
       "        0.63809524, 0.63693271, 0.49635796, 0.63149606, 0.62695925,\n",
       "        0.5141844 , 0.63207547, 0.63919129, 0.63479624, 0.63410853,\n",
       "        0.64162754, 0.6377709 , 0.63693271, 0.63763608, 0.63862928,\n",
       "        0.63580247, 0.63608087, 0.63297045, 0.63707165, 0.64319249,\n",
       "        0.63734568, 0.63987635, 0.63565891, 0.6377709 , 0.63551402,\n",
       "        0.63763608, 0.63707165, 0.63849765, 0.63962559, 0.63793103,\n",
       "        0.6359375 , 0.63551402, 0.63509317, 0.63410853, 0.63736264,\n",
       "        0.63664596, 0.63793103, 0.63763608, 0.63875969, 0.6372093 ,\n",
       "        0.63949843, 0.63806552, 0.63707165, 0.63692308, 0.63551402,\n",
       "        0.6372093 , 0.6349454 , 0.6372093 , 0.63650546, 0.63536776,\n",
       "        0.63551402, 0.63608087, 0.6375    , 0.63650546, 0.63722397,\n",
       "        0.63975155, 0.63522013, 0.64150943, 0.63009404, 0.63338534,\n",
       "        0.62905719, 0.65511811, 0.615625  , 0.63479624, 0.63026521,\n",
       "        0.65396825, 0.62834646, 0.62928349, 0.6359375 , 0.64724409,\n",
       "        0.65189873, 0.63380282, 0.62480377, 0.62618297, 0.62794349,\n",
       "        0.63207547, 0.63765823, 0.63125   , 0.64835165, 0.63693271,\n",
       "        0.6375    , 0.63949843, 0.64094488, 0.62928349, 0.62870515,\n",
       "        0.63636364, 0.63836478, 0.63265306, 0.63307087, 0.63410853,\n",
       "        0.64139021, 0.63536776, 0.64319249, 0.6390625 , 0.63806552,\n",
       "        0.63307087, 0.63536776, 0.63736264, 0.64420063, 0.63084112,\n",
       "        0.64195584, 0.64454976, 0.64018692]),\n",
       " 'mean_test_jaccard': array([0.64573579, 0.64496778, 0.64675936, 0.6274678 , 0.629179  ,\n",
       "        0.63396021, 0.60792414, 0.62586115, 0.61648227, 0.61929075,\n",
       "        0.61420859, 0.61506584, 0.64880877, 0.64674724, 0.64675765,\n",
       "        0.62072346, 0.62714705, 0.63701013, 0.62255117, 0.61780345,\n",
       "        0.63687   , 0.61807269, 0.61419944, 0.60055704, 0.64340196,\n",
       "        0.65062319, 0.64705295, 0.63163543, 0.63781059, 0.63662603,\n",
       "        0.62074879, 0.62411565, 0.61802234, 0.61322628, 0.61844221,\n",
       "        0.61171499, 0.64206466, 0.65054454, 0.64984288, 0.63936272,\n",
       "        0.63629081, 0.62888279, 0.63009407, 0.62763288, 0.62455426,\n",
       "        0.62960962, 0.6163708 , 0.61259881, 0.65736895, 0.65561398,\n",
       "        0.64145844, 0.63705065, 0.63772468, 0.63927826, 0.61495147,\n",
       "        0.62463268, 0.61795609, 0.60215925, 0.59395059, 0.59566229,\n",
       "        0.6453416 , 0.65193716, 0.64704191, 0.61862046, 0.64321251,\n",
       "        0.63635736, 0.6107962 , 0.6134859 , 0.62166671, 0.59674642,\n",
       "        0.60747593, 0.61476957, 0.65230051, 0.64939522, 0.64920414,\n",
       "        0.63257797, 0.62572127, 0.64796224, 0.61732095, 0.61994111,\n",
       "        0.6191338 , 0.61105355, 0.61041354, 0.60641924, 0.64913031,\n",
       "        0.64684897, 0.64687057, 0.62971323, 0.63719965, 0.63771174,\n",
       "        0.62662919, 0.62185638, 0.61844316, 0.60104836, 0.61830358,\n",
       "        0.6040365 , 0.66193105, 0.39423338, 0.66252653, 0.65978704,\n",
       "        0.32480878, 0.65694482, 0.6585277 , 0.11190462, 0.65750953,\n",
       "        0.65699294, 0.30558664, 0.661823  , 0.65644178, 0.41673521,\n",
       "        0.65877955, 0.65684513, 0.09502295, 0.65860456, 0.65564118,\n",
       "        0.34558117, 0.65804713, 0.65996549, 0.13766387, 0.65930907,\n",
       "        0.65922984, 0.19388198, 0.65601067, 0.65950032, 0.11155625,\n",
       "        0.65760681, 0.66041534, 0.39550404, 0.65703909, 0.65768912,\n",
       "        0.38841805, 0.66108782, 0.66062458, 0.13836153, 0.65590303,\n",
       "        0.65420732, 0.28246746, 0.66042788, 0.65800364, 0.53789472,\n",
       "        0.66012832, 0.65913354, 0.25262778, 0.65525754, 0.65764912,\n",
       "        0.5439024 , 0.65622965, 0.65726629, 0.51866016, 0.65533702,\n",
       "        0.65179409, 0.44713627, 0.66101485, 0.65735814, 0.44662375,\n",
       "        0.65302538, 0.65158441, 0.43336041, 0.65321605, 0.65867787,\n",
       "        0.48089245, 0.65750753, 0.65288374, 0.42705343, 0.66121658,\n",
       "        0.65413669, 0.42271039, 0.65140952, 0.65369841, 0.39074067,\n",
       "        0.64921303, 0.65810722, 0.46740186, 0.6543625 , 0.65777228,\n",
       "        0.38130499, 0.65569591, 0.65180201, 0.46405918, 0.65440156,\n",
       "        0.65164218, 0.38015134, 0.65602755, 0.6519278 , 0.45358346,\n",
       "        0.65701385, 0.65789606, 0.45872235, 0.65563097, 0.65694388,\n",
       "        0.45355862, 0.65349965, 0.65498703, 0.65668841, 0.65358974,\n",
       "        0.65948838, 0.65749601, 0.65558483, 0.6566058 , 0.65548776,\n",
       "        0.65668914, 0.65748675, 0.65523786, 0.65548584, 0.6582344 ,\n",
       "        0.65664057, 0.65627107, 0.65554044, 0.65589164, 0.65670821,\n",
       "        0.65518476, 0.65614631, 0.65787713, 0.65774129, 0.65656182,\n",
       "        0.65693293, 0.65494483, 0.65633111, 0.65645034, 0.6546491 ,\n",
       "        0.65465736, 0.65675852, 0.65691799, 0.65961414, 0.65499073,\n",
       "        0.65662487, 0.65793783, 0.65702535, 0.65703606, 0.6563857 ,\n",
       "        0.65665127, 0.65564166, 0.65830174, 0.65651427, 0.65628762,\n",
       "        0.65705712, 0.65716582, 0.65718967, 0.65770461, 0.65653293,\n",
       "        0.65704672, 0.65305403, 0.65640726, 0.64937081, 0.65602796,\n",
       "        0.65577229, 0.66028546, 0.64938693, 0.64657441, 0.6535081 ,\n",
       "        0.65935091, 0.65902096, 0.65062201, 0.65294216, 0.65389817,\n",
       "        0.65963419, 0.65382412, 0.65228728, 0.65623918, 0.6470423 ,\n",
       "        0.6528443 , 0.65946733, 0.65343658, 0.65289098, 0.65310263,\n",
       "        0.65453447, 0.65802588, 0.65482925, 0.65105834, 0.6532687 ,\n",
       "        0.65655937, 0.65260432, 0.65539002, 0.6536153 , 0.65101195,\n",
       "        0.64923602, 0.65600246, 0.65731073, 0.65823941, 0.65716426,\n",
       "        0.65458873, 0.65780901, 0.65311956, 0.65876065, 0.65039187,\n",
       "        0.65834297, 0.65734913, 0.65575398]),\n",
       " 'std_test_jaccard': array([0.01013971, 0.02316159, 0.01961593, 0.01996485, 0.0223136 ,\n",
       "        0.00702859, 0.01966859, 0.01178747, 0.02309319, 0.00801982,\n",
       "        0.01293851, 0.00343379, 0.0124675 , 0.01520748, 0.01153851,\n",
       "        0.01421159, 0.01852241, 0.01792478, 0.01681828, 0.0054317 ,\n",
       "        0.01950355, 0.01731077, 0.01193136, 0.00858483, 0.01654231,\n",
       "        0.01485071, 0.01601661, 0.01886902, 0.01308148, 0.00914138,\n",
       "        0.01535809, 0.01083563, 0.01625579, 0.00948418, 0.01865628,\n",
       "        0.00641523, 0.01936275, 0.01960901, 0.01571854, 0.01974472,\n",
       "        0.00684864, 0.00468671, 0.01099114, 0.01357358, 0.01367539,\n",
       "        0.02310678, 0.01758414, 0.00521369, 0.01804039, 0.00942241,\n",
       "        0.01424912, 0.01091913, 0.01266215, 0.00770262, 0.00752293,\n",
       "        0.01195066, 0.01421404, 0.00684499, 0.0146486 , 0.01050836,\n",
       "        0.0183902 , 0.01836588, 0.01757773, 0.01005948, 0.01359526,\n",
       "        0.01481931, 0.00887267, 0.01888587, 0.01791371, 0.00793841,\n",
       "        0.02039495, 0.01052244, 0.01794156, 0.01631819, 0.00805798,\n",
       "        0.01832472, 0.00968965, 0.0094521 , 0.00876089, 0.02257338,\n",
       "        0.01397998, 0.02283944, 0.00986371, 0.00977011, 0.02411572,\n",
       "        0.01451398, 0.01452773, 0.011422  , 0.00745754, 0.00761013,\n",
       "        0.01511228, 0.00970512, 0.01582131, 0.01149214, 0.01831582,\n",
       "        0.02209902, 0.02157864, 0.1697514 , 0.01798276, 0.02039419,\n",
       "        0.24049177, 0.0185406 , 0.02128139, 0.19929951, 0.0183173 ,\n",
       "        0.01923049, 0.24710214, 0.01981466, 0.01805424, 0.20921082,\n",
       "        0.0180354 , 0.01995336, 0.16767763, 0.01794887, 0.01875391,\n",
       "        0.21246409, 0.01932214, 0.01748228, 0.19197827, 0.0201445 ,\n",
       "        0.01934424, 0.23787425, 0.01444541, 0.01989388, 0.19869764,\n",
       "        0.0177658 , 0.01976864, 0.20195812, 0.0186374 , 0.02167003,\n",
       "        0.12309359, 0.01965031, 0.01940789, 0.19646058, 0.01579605,\n",
       "        0.01616088, 0.23327634, 0.01893222, 0.01826482, 0.05901035,\n",
       "        0.02068904, 0.01859263, 0.226744  , 0.0151882 , 0.00919365,\n",
       "        0.04907901, 0.01401333, 0.01790439, 0.06588326, 0.01715517,\n",
       "        0.01567431, 0.11621399, 0.01746097, 0.01627644, 0.1231316 ,\n",
       "        0.01696127, 0.01787109, 0.07107178, 0.01786483, 0.01559666,\n",
       "        0.07687287, 0.01666943, 0.01596232, 0.06769514, 0.01210104,\n",
       "        0.02034417, 0.0738735 , 0.01619982, 0.01888194, 0.08814301,\n",
       "        0.01450797, 0.01360015, 0.08383948, 0.01182986, 0.01902312,\n",
       "        0.14077348, 0.0191546 , 0.01852409, 0.1778967 , 0.01570045,\n",
       "        0.01519243, 0.17585922, 0.01605847, 0.01637724, 0.1372527 ,\n",
       "        0.01817186, 0.01448719, 0.15120257, 0.01727193, 0.01823336,\n",
       "        0.15672587, 0.01316088, 0.01651515, 0.01736708, 0.0178266 ,\n",
       "        0.01774484, 0.01930553, 0.01865002, 0.01587043, 0.01717103,\n",
       "        0.01713001, 0.01720148, 0.01787316, 0.01703974, 0.01487368,\n",
       "        0.0165938 , 0.01665569, 0.01553239, 0.01772   , 0.01704129,\n",
       "        0.01669146, 0.01594138, 0.01748596, 0.01748298, 0.01749773,\n",
       "        0.01779336, 0.01607695, 0.0195146 , 0.01930275, 0.01689831,\n",
       "        0.01659957, 0.01565169, 0.01625065, 0.01920845, 0.0166885 ,\n",
       "        0.01629485, 0.01780994, 0.01773083, 0.01716982, 0.01734686,\n",
       "        0.01747558, 0.01786363, 0.01830326, 0.01743684, 0.01815811,\n",
       "        0.01862072, 0.01758205, 0.01740584, 0.01819762, 0.01765327,\n",
       "        0.0152982 , 0.01787536, 0.01429992, 0.01246539, 0.01466583,\n",
       "        0.01916169, 0.01182516, 0.02244793, 0.01180319, 0.01488155,\n",
       "        0.01109058, 0.01962821, 0.01319688, 0.01425858, 0.01099743,\n",
       "        0.01262309, 0.01773366, 0.01779779, 0.0165421 , 0.01638617,\n",
       "        0.01956046, 0.01614206, 0.01813326, 0.01230637, 0.0176329 ,\n",
       "        0.01605014, 0.01338753, 0.01348969, 0.01727793, 0.01701386,\n",
       "        0.02016203, 0.01714856, 0.01597219, 0.01744074, 0.01560938,\n",
       "        0.01603741, 0.01504133, 0.01492029, 0.01568397, 0.01605933,\n",
       "        0.01425617, 0.01826235, 0.0133791 , 0.0162832 , 0.01292604,\n",
       "        0.01171485, 0.01332445, 0.01577344]),\n",
       " 'rank_test_jaccard': array([180, 182, 176, 208, 205, 199, 247, 211, 233, 222, 238, 235, 169,\n",
       "        178, 177, 220, 209, 194, 216, 231, 195, 228, 239, 253, 183, 157,\n",
       "        171, 201, 189, 196, 219, 215, 229, 241, 226, 243, 185, 159, 161,\n",
       "        187, 198, 206, 202, 207, 214, 204, 234, 242,  51, 105, 186, 193,\n",
       "        190, 188, 236, 213, 230, 251, 256, 255, 181, 148, 173, 224, 184,\n",
       "        197, 245, 240, 218, 254, 248, 237, 146, 162, 167, 200, 212, 170,\n",
       "        232, 221, 223, 244, 246, 249, 168, 175, 174, 203, 192, 191, 210,\n",
       "        217, 225, 252, 227, 250,   2, 273,   1,  13, 279,  66,  28, 286,\n",
       "         47,  65, 280,   3,  84, 271,  24,  70, 288,  27, 103, 278,  34,\n",
       "         12, 285,  20,  21, 283,  95,  16, 287,  46,   9, 272,  61,  44,\n",
       "        275,   5,   7, 284,  97, 125, 281,   8,  36, 258,  11,  22, 282,\n",
       "        112,  45, 257,  91,  55, 259, 111, 151, 266,   6,  52, 267, 140,\n",
       "        153, 268, 136,  26, 260,  48, 143, 269,   4, 126, 270, 154, 129,\n",
       "        274, 166,  33, 261, 124,  41, 276, 101, 150, 262, 123, 152, 277,\n",
       "         94, 149, 264,  64,  38, 263, 104,  67, 265, 133, 116,  74, 131,\n",
       "         17,  49, 106,  78, 108,  73,  50, 113, 109,  32,  76,  89, 107,\n",
       "         98,  72, 114,  92,  39,  42,  79,  68, 117,  87,  83, 120, 119,\n",
       "         71,  69,  15, 115,  77,  37,  63,  62,  86,  75, 102,  30,  82,\n",
       "         88,  59,  57,  56,  43,  81,  60, 139,  85, 164,  93,  99,  10,\n",
       "        163, 179, 132,  19,  23, 158, 141, 127,  14, 128, 147,  90, 172,\n",
       "        144,  18, 134, 142, 138, 122,  35, 118, 155, 135,  80, 145, 110,\n",
       "        130, 156, 165,  96,  54,  31,  58, 121,  40, 137,  25, 160,  29,\n",
       "         53, 100]),\n",
       " 'split0_test_neg_log_loss': array([-0.45821415, -0.44421112, -0.45771827, -0.49587255, -0.50033637,\n",
       "        -0.51406465, -0.59982833, -0.53492735, -0.55819806, -0.58451857,\n",
       "        -0.63635821, -0.63464416, -0.45079534, -0.45684408, -0.45468243,\n",
       "        -0.49563931, -0.45973538, -0.51268481, -0.5250336 , -0.57525132,\n",
       "        -0.49592518, -0.56632303, -0.61908991, -0.66047964, -0.46069245,\n",
       "        -0.45260311, -0.44684596, -0.4741878 , -0.50780822, -0.48061285,\n",
       "        -0.53682412, -0.55047284, -0.54833953, -0.54000137, -0.5651985 ,\n",
       "        -0.61947436, -0.44482392, -0.45826333, -0.44462683, -0.46610498,\n",
       "        -0.48883329, -0.49021688, -0.52866126, -0.52619108, -0.51739429,\n",
       "        -0.53037739, -0.54767743, -0.554797  , -0.45674351, -0.44982126,\n",
       "        -0.44530115, -0.4998034 , -0.49004157, -0.49232949, -0.56054614,\n",
       "        -0.62087302, -0.53157981, -0.66520287, -0.67003464, -0.67194717,\n",
       "        -0.46422501, -0.45995941, -0.47857378, -0.53031013, -0.48508485,\n",
       "        -0.47023403, -0.56876513, -0.57151963, -0.59217979, -0.62964066,\n",
       "        -0.58693189, -0.63759211, -0.4497354 , -0.43820281, -0.45100655,\n",
       "        -0.49064594, -0.49722648, -0.50143388, -0.53949157, -0.54335955,\n",
       "        -0.54237264, -0.65147946, -0.61900464, -0.64741279, -0.45136397,\n",
       "        -0.4530651 , -0.46498302, -0.48512592, -0.48669963, -0.49277327,\n",
       "        -0.5088628 , -0.52384911, -0.51945877, -0.59522854, -0.5593628 ,\n",
       "        -0.59521256, -0.44865348, -0.66653787, -0.44558918, -0.43855   ,\n",
       "        -0.72640887, -0.44159194, -0.43949863, -0.68392813, -0.44001802,\n",
       "        -0.43888347, -0.68334515, -0.4376553 , -0.44516555, -0.65617317,\n",
       "        -0.44309665, -0.44184422, -0.68553474, -0.44507435, -0.43887327,\n",
       "        -0.68469439, -0.43970799, -0.43817755, -0.69437725, -0.43888379,\n",
       "        -0.44305807, -0.78004028, -0.4427173 , -0.4393908 , -0.68516386,\n",
       "        -0.44265005, -0.43966544, -0.6941874 , -0.44009243, -0.43972643,\n",
       "        -0.70689004, -0.43811039, -0.44623223, -0.70326233, -0.44447961,\n",
       "        -0.44337754, -0.7348723 , -0.44268572, -0.44151089, -0.69384581,\n",
       "        -0.43827102, -0.44000626, -0.70125266, -0.43975557, -0.44202927,\n",
       "        -0.75276752, -0.45194523, -0.44087461, -0.7050833 , -0.43982101,\n",
       "        -0.4411468 , -0.63724873, -0.4363136 , -0.43949072, -0.71734745,\n",
       "        -0.43659733, -0.43772657, -0.66866237, -0.44940754, -0.44225465,\n",
       "        -0.79420624, -0.45092943, -0.44444531, -0.69447788, -0.4418647 ,\n",
       "        -0.43583855, -0.58561599, -0.44393847, -0.44465899, -0.66528588,\n",
       "        -0.44466604, -0.44145924, -0.80915751, -0.44335857, -0.44131419,\n",
       "        -0.71310348, -0.43903714, -0.43762026, -0.61812989, -0.43844399,\n",
       "        -0.44617454, -0.80292354, -0.44577261, -0.43918667, -0.58168832,\n",
       "        -0.44130576, -0.44020425, -0.54931812, -0.4384995 , -0.44071993,\n",
       "        -0.69833628, -0.43814517, -0.44049468, -0.43666082, -0.43703571,\n",
       "        -0.43496701, -0.43386772, -0.43462145, -0.43409334, -0.4350868 ,\n",
       "        -0.43532902, -0.43387482, -0.43590938, -0.43454772, -0.44512174,\n",
       "        -0.43711394, -0.43484634, -0.43542395, -0.43547546, -0.43503339,\n",
       "        -0.43435412, -0.43467329, -0.43350939, -0.43426861, -0.43422816,\n",
       "        -0.43399381, -0.43613248, -0.43520678, -0.43664393, -0.43526903,\n",
       "        -0.43571263, -0.43594075, -0.43526766, -0.43531649, -0.43377234,\n",
       "        -0.43402164, -0.43356149, -0.4345077 , -0.43724689, -0.43624824,\n",
       "        -0.43687387, -0.43450075, -0.43525394, -0.43522574, -0.43469453,\n",
       "        -0.43535232, -0.43544519, -0.43447576, -0.4342944 , -0.43469418,\n",
       "        -0.44311832, -0.43152484, -0.43560139, -0.4434553 , -0.44295033,\n",
       "        -0.43649449, -0.44828772, -0.4456021 , -0.45065146, -0.43992137,\n",
       "        -0.44286612, -0.44822597, -0.43619554, -0.4379785 , -0.43589647,\n",
       "        -0.44486898, -0.44322172, -0.43937509, -0.44082401, -0.44418997,\n",
       "        -0.44406458, -0.4541209 , -0.44727368, -0.44530216, -0.44144872,\n",
       "        -0.4400693 , -0.4375089 , -0.44639765, -0.44511769, -0.44230127,\n",
       "        -0.44588777, -0.44485968, -0.44206549, -0.44633151, -0.44545811,\n",
       "        -0.44399596, -0.43606579, -0.43721938, -0.43555649, -0.44102018,\n",
       "        -0.43888677, -0.43335374, -0.44215876, -0.43471172, -0.44028246,\n",
       "        -0.44327806, -0.44139648, -0.44389608]),\n",
       " 'split1_test_neg_log_loss': array([-0.46837561, -0.46215607, -0.46305765, -0.50005264, -0.47583236,\n",
       "        -0.48168668, -0.5477813 , -0.52171945, -0.53289165, -0.57930965,\n",
       "        -0.58478348, -0.55270075, -0.46161978, -0.44539998, -0.45094843,\n",
       "        -0.47486648, -0.49732391, -0.47874701, -0.52871732, -0.52600645,\n",
       "        -0.48943567, -0.55342779, -0.55843546, -0.5793798 , -0.44910497,\n",
       "        -0.46079481, -0.46230778, -0.45148389, -0.48866775, -0.48487976,\n",
       "        -0.5259026 , -0.52162519, -0.5114828 , -0.58770341, -0.54183322,\n",
       "        -0.58228153, -0.44930285, -0.4577133 , -0.45404052, -0.48950037,\n",
       "        -0.48814284, -0.48547874, -0.47269557, -0.51231286, -0.48389965,\n",
       "        -0.56082415, -0.55806622, -0.55727957, -0.46690965, -0.46334302,\n",
       "        -0.46821841, -0.46969476, -0.46669545, -0.47161959, -0.5937509 ,\n",
       "        -0.52050383, -0.66642663, -0.63310011, -0.65237672, -0.56452442,\n",
       "        -0.46851914, -0.4573788 , -0.46114261, -0.51300029, -0.48870571,\n",
       "        -0.50345299, -0.617033  , -0.51135067, -0.48901828, -0.62539361,\n",
       "        -0.64032762, -0.57543299, -0.4727607 , -0.45907261, -0.45688073,\n",
       "        -0.47753849, -0.46869492, -0.47948236, -0.53313077, -0.53025527,\n",
       "        -0.55486713, -0.65300334, -0.58592505, -0.546285  , -0.47449146,\n",
       "        -0.45707532, -0.45084847, -0.48967903, -0.47545457, -0.4798029 ,\n",
       "        -0.51510537, -0.50704684, -0.53726541, -0.55303425, -0.56418331,\n",
       "        -0.63852843, -0.4558832 , -0.7209904 , -0.45417476, -0.44808301,\n",
       "        -0.66996931, -0.4505502 , -0.44681007, -0.70211003, -0.44906322,\n",
       "        -0.44597887, -0.71607413, -0.44729175, -0.45030719, -0.77728186,\n",
       "        -0.45305171, -0.448121  , -0.71845177, -0.4519641 , -0.44782873,\n",
       "        -0.69465886, -0.4511304 , -0.447702  , -0.72966586, -0.44602192,\n",
       "        -0.45530069, -0.7193823 , -0.45508663, -0.4470409 , -0.72767687,\n",
       "        -0.44823562, -0.44777737, -0.68914316, -0.4451726 , -0.44872906,\n",
       "        -0.70619115, -0.44889165, -0.45159795, -0.74611292, -0.45622917,\n",
       "        -0.45101873, -0.74279746, -0.44855788, -0.44928797, -0.71956148,\n",
       "        -0.44847396, -0.44769077, -0.70136149, -0.44939307, -0.44869504,\n",
       "        -0.58850932, -0.44940394, -0.44659985, -0.70133562, -0.44537826,\n",
       "        -0.44222091, -0.67709623, -0.44117861, -0.4413205 , -0.82606461,\n",
       "        -0.4403689 , -0.454559  , -0.67355675, -0.44701848, -0.44580814,\n",
       "        -0.77388169, -0.44880481, -0.44594393, -0.70764692, -0.44916911,\n",
       "        -0.43978279, -0.65844066, -0.4429845 , -0.45123883, -0.68352358,\n",
       "        -0.44484896, -0.44832039, -0.74648092, -0.44656353, -0.44455852,\n",
       "        -0.81112935, -0.44668288, -0.44305726, -0.63405232, -0.44115338,\n",
       "        -0.44537911, -1.02388101, -0.45096423, -0.44632707, -0.72869027,\n",
       "        -0.45174844, -0.44096528, -0.68926048, -0.4473877 , -0.4343319 ,\n",
       "        -0.62603589, -0.44285232, -0.43868279, -0.4401034 , -0.44090605,\n",
       "        -0.43814596, -0.43881732, -0.43795739, -0.43773276, -0.43751409,\n",
       "        -0.43731223, -0.43780588, -0.438129  , -0.43810718, -0.44045121,\n",
       "        -0.43962511, -0.44087056, -0.43851762, -0.43880317, -0.43850602,\n",
       "        -0.43804497, -0.43835687, -0.43754635, -0.4371211 , -0.43780926,\n",
       "        -0.43887149, -0.43929957, -0.43988648, -0.44085537, -0.43837472,\n",
       "        -0.43857841, -0.43910061, -0.43836576, -0.43809685, -0.43840842,\n",
       "        -0.43767609, -0.43861682, -0.4385045 , -0.44157237, -0.44192623,\n",
       "        -0.44154307, -0.4406923 , -0.43901422, -0.43878731, -0.43733763,\n",
       "        -0.43936513, -0.43943317, -0.43988192, -0.43786462, -0.43793177,\n",
       "        -0.43996445, -0.43912943, -0.44125801, -0.44065023, -0.43936002,\n",
       "        -0.43997143, -0.44728381, -0.44130628, -0.44195184, -0.452316  ,\n",
       "        -0.44638986, -0.44853149, -0.44130166, -0.4400454 , -0.43925422,\n",
       "        -0.43504711, -0.44344411, -0.44699822, -0.44462837, -0.44713846,\n",
       "        -0.44940662, -0.44103018, -0.4447696 , -0.44832396, -0.43884362,\n",
       "        -0.44000177, -0.43353917, -0.43767625, -0.44409216, -0.43745872,\n",
       "        -0.44804876, -0.4494423 , -0.44176837, -0.44392708, -0.44415633,\n",
       "        -0.45117022, -0.44075917, -0.43741769, -0.43920809, -0.4362232 ,\n",
       "        -0.44038317, -0.43929546, -0.43934637, -0.44169474, -0.43971447,\n",
       "        -0.43918566, -0.44001154, -0.44094182]),\n",
       " 'split2_test_neg_log_loss': array([-0.42401261, -0.42190793, -0.42195389, -0.47007727, -0.44317377,\n",
       "        -0.45348709, -0.47900401, -0.48231452, -0.49662429, -0.55426468,\n",
       "        -0.504545  , -0.53017899, -0.43263684, -0.42523399, -0.42230385,\n",
       "        -0.47363691, -0.46895553, -0.4474485 , -0.47840875, -0.52514756,\n",
       "        -0.47011506, -0.554209  , -0.60500013, -0.6189409 , -0.42313316,\n",
       "        -0.42935201, -0.42405108, -0.45827037, -0.45804995, -0.43865132,\n",
       "        -0.45932205, -0.48714618, -0.47121451, -0.56146051, -0.55271324,\n",
       "        -0.59151654, -0.43053413, -0.42013648, -0.42181865, -0.43971892,\n",
       "        -0.45843652, -0.45547686, -0.46963143, -0.49266759, -0.463506  ,\n",
       "        -0.49836797, -0.48974278, -0.5173982 , -0.4219296 , -0.43636361,\n",
       "        -0.43752829, -0.47733633, -0.45248385, -0.45387787, -0.50981821,\n",
       "        -0.54359152, -0.505047  , -0.60389579, -0.62861201, -0.60817117,\n",
       "        -0.42599922, -0.42011716, -0.43348501, -0.45789519, -0.445831  ,\n",
       "        -0.45290235, -0.52283883, -0.54384812, -0.49082332, -0.65236276,\n",
       "        -0.5578439 , -0.58026394, -0.42558311, -0.42688551, -0.43254932,\n",
       "        -0.4499794 , -0.46048758, -0.43424958, -0.54649919, -0.48375698,\n",
       "        -0.49430498, -0.56836619, -0.58946717, -0.59344945, -0.41583697,\n",
       "        -0.42076116, -0.44441583, -0.47096792, -0.45496467, -0.44891124,\n",
       "        -0.498682  , -0.52466621, -0.4850137 , -0.56812869, -0.51384432,\n",
       "        -0.55073775, -0.42551399, -0.7423367 , -0.42600975, -0.42196411,\n",
       "        -0.67594054, -0.42188687, -0.41888456, -0.68637591, -0.41805928,\n",
       "        -0.42013495, -0.70470626, -0.41963615, -0.42963619, -0.6565647 ,\n",
       "        -0.42578135, -0.42207511, -0.71035808, -0.42285342, -0.41927813,\n",
       "        -0.72661551, -0.4210569 , -0.4193613 , -0.69107454, -0.41866365,\n",
       "        -0.42898336, -0.70459377, -0.42628506, -0.42136756, -0.75248833,\n",
       "        -0.42214183, -0.41922842, -0.71325049, -0.42021841, -0.41783298,\n",
       "        -0.66718437, -0.41878031, -0.42651264, -0.70243797, -0.42712085,\n",
       "        -0.42210163, -0.72852813, -0.42109452, -0.42062688, -0.64965249,\n",
       "        -0.41907329, -0.42096684, -0.70600995, -0.41972375, -0.42428071,\n",
       "        -0.59422905, -0.42470648, -0.41941956, -0.51467198, -0.41676686,\n",
       "        -0.41804424, -0.6633386 , -0.41931047, -0.41804724, -0.6071392 ,\n",
       "        -0.42170989, -0.4257528 , -0.60756348, -0.42437898, -0.41895352,\n",
       "        -0.67290626, -0.41557157, -0.41507623, -0.73895676, -0.41555608,\n",
       "        -0.41401201, -0.62253475, -0.41793091, -0.42410535, -0.66633568,\n",
       "        -0.41772234, -0.42079612, -0.56385256, -0.41831848, -0.42050157,\n",
       "        -0.7241348 , -0.4154983 , -0.41505912, -0.55954113, -0.41678581,\n",
       "        -0.421917  , -0.54904558, -0.42128893, -0.4200864 , -0.5858024 ,\n",
       "        -0.41752852, -0.41548392, -0.65946143, -0.42045438, -0.41359415,\n",
       "        -0.57      , -0.41843187, -0.41719479, -0.41707357, -0.41578789,\n",
       "        -0.41603882, -0.41555017, -0.41403476, -0.41473948, -0.41464296,\n",
       "        -0.41461653, -0.41287678, -0.41357752, -0.4136751 , -0.41604701,\n",
       "        -0.4176858 , -0.41707114, -0.4163182 , -0.41695613, -0.41358431,\n",
       "        -0.41417148, -0.41435276, -0.41463649, -0.41382333, -0.41391496,\n",
       "        -0.41492269, -0.41895806, -0.4173858 , -0.4154257 , -0.41435601,\n",
       "        -0.41504973, -0.41453425, -0.4136518 , -0.41481497, -0.41425979,\n",
       "        -0.41372541, -0.41527974, -0.41421269, -0.41805257, -0.42241418,\n",
       "        -0.421275  , -0.41623565, -0.41565908, -0.41607904, -0.41481133,\n",
       "        -0.41545117, -0.41527975, -0.41417865, -0.41383548, -0.41480476,\n",
       "        -0.41433482, -0.41600778, -0.41921275, -0.41616568, -0.4158807 ,\n",
       "        -0.41452337, -0.41602817, -0.42106112, -0.42154399, -0.41534226,\n",
       "        -0.41365062, -0.41592892, -0.41878989, -0.41554931, -0.41769447,\n",
       "        -0.42030873, -0.41445581, -0.41670711, -0.4171787 , -0.4186207 ,\n",
       "        -0.41655968, -0.42136752, -0.41988043, -0.41924944, -0.41219436,\n",
       "        -0.41627222, -0.4165512 , -0.41846512, -0.41760341, -0.416202  ,\n",
       "        -0.41701042, -0.41484049, -0.41603855, -0.42137355, -0.42145355,\n",
       "        -0.42570833, -0.41513061, -0.41603591, -0.41389486, -0.412295  ,\n",
       "        -0.41355147, -0.41538155, -0.41081407, -0.4119772 , -0.41121105,\n",
       "        -0.41460215, -0.41259397, -0.41470908]),\n",
       " 'split3_test_neg_log_loss': array([-0.4440396 , -0.43626015, -0.45601941, -0.47177807, -0.52291808,\n",
       "        -0.4776666 , -0.52826868, -0.5625862 , -0.57307463, -0.56975443,\n",
       "        -0.57837544, -0.64517586, -0.44103815, -0.45454744, -0.45117678,\n",
       "        -0.48122113, -0.49604091, -0.49593161, -0.56943597, -0.57674774,\n",
       "        -0.53054217, -0.60692937, -0.5870317 , -0.59306556, -0.44260254,\n",
       "        -0.44838227, -0.44428398, -0.51120043, -0.47894005, -0.47725375,\n",
       "        -0.53647413, -0.51885811, -0.5412568 , -0.58945743, -0.54754408,\n",
       "        -0.57817341, -0.46353112, -0.44838847, -0.44595256, -0.48970988,\n",
       "        -0.4736065 , -0.47557297, -0.50383522, -0.51307164, -0.50657578,\n",
       "        -0.52528492, -0.54282295, -0.5496859 , -0.44126123, -0.4445503 ,\n",
       "        -0.45423651, -0.52137564, -0.48310187, -0.46592459, -0.57147696,\n",
       "        -0.56932654, -0.49164973, -0.68051559, -0.60827431, -0.68242473,\n",
       "        -0.45640412, -0.44738843, -0.45602812, -0.51660135, -0.50134194,\n",
       "        -0.49923484, -0.60968115, -0.56602014, -0.56040018, -0.72308412,\n",
       "        -0.64520714, -0.56223383, -0.45508043, -0.46024384, -0.44751208,\n",
       "        -0.49760083, -0.49009475, -0.46275213, -0.58191456, -0.60076576,\n",
       "        -0.57794103, -0.622859  , -0.57927151, -0.63314852, -0.43822875,\n",
       "        -0.44474172, -0.45401341, -0.4795145 , -0.48690404, -0.47835709,\n",
       "        -0.51739413, -0.53448091, -0.52189913, -0.58029201, -0.62768821,\n",
       "        -0.56634604, -0.45450578, -0.67328118, -0.45265921, -0.44548712,\n",
       "        -0.70311565, -0.44790149, -0.44483233, -0.71627573, -0.44566133,\n",
       "        -0.44413217, -0.68639206, -0.44240209, -0.45534799, -0.75698209,\n",
       "        -0.45475695, -0.44761571, -0.71516002, -0.44494393, -0.44471127,\n",
       "        -0.64799651, -0.44322286, -0.44461813, -0.70719086, -0.44462484,\n",
       "        -0.45095049, -0.69836445, -0.45246857, -0.44808865, -0.72666841,\n",
       "        -0.44662982, -0.44507363, -0.65426721, -0.44194173, -0.44231775,\n",
       "        -0.70124491, -0.44217076, -0.45701562, -0.74952456, -0.45444662,\n",
       "        -0.44752459, -0.68036647, -0.45085768, -0.44522103, -0.72453285,\n",
       "        -0.44515235, -0.44453766, -0.69553877, -0.44336495, -0.45146959,\n",
       "        -0.62367966, -0.45458462, -0.44826068, -0.6364137 , -0.44898585,\n",
       "        -0.44858323, -0.55895955, -0.44507677, -0.44689046, -0.54748092,\n",
       "        -0.44520711, -0.46083429, -0.65435607, -0.44986416, -0.44896539,\n",
       "        -0.54825719, -0.45004802, -0.44137965, -0.65788136, -0.44524705,\n",
       "        -0.44191767, -0.69724724, -0.44744558, -0.45491482, -0.71819916,\n",
       "        -0.44857391, -0.45133613, -0.67651996, -0.44846356, -0.44101704,\n",
       "        -0.66679104, -0.44408135, -0.45086526, -0.5321114 , -0.44365059,\n",
       "        -0.45511369, -0.92546694, -0.45466729, -0.44509983, -0.67234923,\n",
       "        -0.44661607, -0.44742842, -0.57710582, -0.44560001, -0.44025906,\n",
       "        -0.6012527 , -0.44989359, -0.44628469, -0.44422906, -0.44571248,\n",
       "        -0.44177421, -0.44383159, -0.44161488, -0.44052265, -0.44095607,\n",
       "        -0.44088315, -0.44134838, -0.44028333, -0.44168929, -0.44289958,\n",
       "        -0.44455107, -0.44455781, -0.4428297 , -0.44159525, -0.44165178,\n",
       "        -0.44072885, -0.44155349, -0.44210395, -0.4406548 , -0.44010006,\n",
       "        -0.44080669, -0.44421278, -0.44494138, -0.44819045, -0.44165546,\n",
       "        -0.4430095 , -0.4418797 , -0.44134975, -0.44155869, -0.44116213,\n",
       "        -0.43953225, -0.43982929, -0.44041313, -0.44520861, -0.44616301,\n",
       "        -0.44461457, -0.4427707 , -0.44086992, -0.44247401, -0.44090194,\n",
       "        -0.44035966, -0.44126856, -0.43956454, -0.43988089, -0.43904617,\n",
       "        -0.44283581, -0.44012003, -0.44268526, -0.44835129, -0.44466124,\n",
       "        -0.43966033, -0.44071604, -0.4403917 , -0.44048051, -0.44966362,\n",
       "        -0.44977664, -0.44743245, -0.44294842, -0.43950114, -0.44115145,\n",
       "        -0.44054699, -0.44918272, -0.44338115, -0.44454095, -0.45128585,\n",
       "        -0.44053259, -0.44435493, -0.43838028, -0.44744607, -0.44535283,\n",
       "        -0.44611607, -0.43938284, -0.44568649, -0.44327789, -0.44011019,\n",
       "        -0.44727203, -0.45132137, -0.45053221, -0.44000309, -0.44236352,\n",
       "        -0.44965649, -0.44334416, -0.44112266, -0.43845779, -0.44352969,\n",
       "        -0.44312383, -0.43981804, -0.44215623, -0.43819987, -0.44165173,\n",
       "        -0.43936943, -0.43888428, -0.43705824]),\n",
       " 'split4_test_neg_log_loss': array([-0.4637373 , -0.47141968, -0.47261317, -0.48754466, -0.57802607,\n",
       "        -0.49976864, -0.60149328, -0.59094054, -0.55468937, -0.59671859,\n",
       "        -0.62705356, -0.59084139, -0.46487033, -0.4608527 , -0.45900044,\n",
       "        -0.49395068, -0.52879128, -0.51970849, -0.57950285, -0.5791694 ,\n",
       "        -0.55315508, -0.5949776 , -0.54088998, -0.67875202, -0.47494882,\n",
       "        -0.46463191, -0.47073206, -0.50783662, -0.51956066, -0.51996291,\n",
       "        -0.54424421, -0.56303674, -0.54326971, -0.59940688, -0.65877589,\n",
       "        -0.61748166, -0.46111497, -0.4579794 , -0.46240114, -0.49845007,\n",
       "        -0.48709501, -0.49257976, -0.51884178, -0.53236689, -0.55086869,\n",
       "        -0.54005204, -0.54706798, -0.57350616, -0.4723075 , -0.45569565,\n",
       "        -0.47218389, -0.51179325, -0.49977738, -0.51260982, -0.57824407,\n",
       "        -0.57819205, -0.56728281, -0.73122364, -0.67371164, -0.69357756,\n",
       "        -0.47559114, -0.45915365, -0.46991516, -0.51200737, -0.49535529,\n",
       "        -0.54945255, -0.54174315, -0.62491185, -0.59042887, -0.60150094,\n",
       "        -0.65520537, -0.69068029, -0.4584496 , -0.46781777, -0.46720139,\n",
       "        -0.48840295, -0.50908465, -0.50865807, -0.5895327 , -0.53696159,\n",
       "        -0.56881456, -0.60747291, -0.62326961, -0.68748311, -0.4716117 ,\n",
       "        -0.47166528, -0.4736898 , -0.50889987, -0.49809295, -0.4936435 ,\n",
       "        -0.54962757, -0.55597796, -0.5443678 , -0.62391787, -0.62786526,\n",
       "        -0.61451119, -0.45991011, -0.69787616, -0.45944362, -0.45393945,\n",
       "        -0.71684612, -0.4584607 , -0.45716182, -0.69196893, -0.45613872,\n",
       "        -0.45698199, -0.7229927 , -0.45356669, -0.45860304, -0.73454204,\n",
       "        -0.46366636, -0.4558955 , -0.7269175 , -0.46134361, -0.45657765,\n",
       "        -0.70221613, -0.45489347, -0.45436614, -0.7244867 , -0.45442941,\n",
       "        -0.45697861, -0.82671026, -0.46400708, -0.45849567, -0.72073631,\n",
       "        -0.4580917 , -0.45561065, -0.69176697, -0.45670016, -0.45474642,\n",
       "        -0.72014926, -0.45385706, -0.46498871, -0.69054231, -0.46318482,\n",
       "        -0.45929351, -0.66869191, -0.45603665, -0.45478155, -0.70233325,\n",
       "        -0.45736849, -0.4545744 , -0.6884683 , -0.45572397, -0.46365209,\n",
       "        -0.70822349, -0.46095026, -0.46188578, -0.71861443, -0.45812523,\n",
       "        -0.46273309, -0.64215397, -0.4600813 , -0.45872445, -0.62212185,\n",
       "        -0.45410143, -0.46718151, -0.66170089, -0.46441866, -0.46133265,\n",
       "        -0.5102234 , -0.45460368, -0.46084227, -0.71897596, -0.4574727 ,\n",
       "        -0.46072966, -0.69335233, -0.45539497, -0.46898617, -0.69697628,\n",
       "        -0.46527538, -0.46191491, -0.64273036, -0.45544566, -0.46157277,\n",
       "        -0.66986075, -0.46175948, -0.46090484, -0.78015318, -0.45553085,\n",
       "        -0.46737096, -0.62152011, -0.46849766, -0.45948868, -0.63979551,\n",
       "        -0.45803228, -0.46840596, -0.75513915, -0.45562503, -0.46187212,\n",
       "        -0.69196203, -0.45842409, -0.45686206, -0.45614089, -0.4556967 ,\n",
       "        -0.45343149, -0.45372683, -0.4533715 , -0.45360699, -0.45372607,\n",
       "        -0.45442608, -0.4543522 , -0.45445684, -0.45387306, -0.45433818,\n",
       "        -0.45784486, -0.45746658, -0.45465259, -0.45487194, -0.45461763,\n",
       "        -0.45404975, -0.45437769, -0.45264409, -0.45369449, -0.45197858,\n",
       "        -0.45221182, -0.45542205, -0.45595989, -0.45656473, -0.45456072,\n",
       "        -0.45541201, -0.45446319, -0.45351117, -0.45346389, -0.45488187,\n",
       "        -0.45193556, -0.4533796 , -0.45405309, -0.45868939, -0.45498626,\n",
       "        -0.45711754, -0.4542844 , -0.45480259, -0.45435121, -0.45257402,\n",
       "        -0.45323592, -0.45377435, -0.45273459, -0.45343896, -0.45135715,\n",
       "        -0.46072306, -0.45520717, -0.45424895, -0.45265969, -0.46074147,\n",
       "        -0.45970899, -0.45991506, -0.46486323, -0.45705499, -0.45988496,\n",
       "        -0.45583168, -0.46087501, -0.45358633, -0.46053457, -0.45695374,\n",
       "        -0.45145664, -0.46442592, -0.45770468, -0.4612141 , -0.45432252,\n",
       "        -0.46662106, -0.46227042, -0.46014388, -0.4513688 , -0.45539613,\n",
       "        -0.45621388, -0.45085946, -0.45758261, -0.45493798, -0.46185344,\n",
       "        -0.45773999, -0.4637159 , -0.46331115, -0.45803025, -0.46245996,\n",
       "        -0.46306079, -0.45387215, -0.45675765, -0.45778391, -0.45226772,\n",
       "        -0.45137188, -0.45733616, -0.45658488, -0.4547025 , -0.45473634,\n",
       "        -0.45425244, -0.45692218, -0.45711548]),\n",
       " 'mean_test_neg_log_loss': array([-0.45167585, -0.44719099, -0.45427248, -0.48506504, -0.50405733,\n",
       "        -0.48533473, -0.55127512, -0.53849761, -0.5430956 , -0.57691318,\n",
       "        -0.58622314, -0.59070823, -0.45019209, -0.44857564, -0.44762239,\n",
       "        -0.4838629 , -0.4901694 , -0.49090408, -0.5362197 , -0.55646449,\n",
       "        -0.50783463, -0.57517336, -0.58208943, -0.62612359, -0.45009639,\n",
       "        -0.45115282, -0.44964417, -0.48059582, -0.49060532, -0.48027212,\n",
       "        -0.52055342, -0.52822781, -0.52311267, -0.57560592, -0.57321298,\n",
       "        -0.5977855 , -0.4498614 , -0.4484962 , -0.44576794, -0.47669685,\n",
       "        -0.47922283, -0.47986504, -0.49873305, -0.51532201, -0.50444888,\n",
       "        -0.53098129, -0.53707547, -0.55053337, -0.4518303 , -0.44995477,\n",
       "        -0.45549365, -0.49600068, -0.47842002, -0.47927227, -0.56276725,\n",
       "        -0.56649739, -0.5523972 , -0.6627876 , -0.64660187, -0.64412901,\n",
       "        -0.45814773, -0.44879949, -0.45982894, -0.50596287, -0.48326376,\n",
       "        -0.49505535, -0.57201225, -0.56353008, -0.54457009, -0.64639642,\n",
       "        -0.61710318, -0.60924063, -0.45232185, -0.45044451, -0.45103001,\n",
       "        -0.48083352, -0.48511768, -0.47731521, -0.55811376, -0.53901983,\n",
       "        -0.54766007, -0.62063618, -0.5993876 , -0.62155577, -0.45030657,\n",
       "        -0.44946171, -0.45759011, -0.48683745, -0.48042317, -0.4786976 ,\n",
       "        -0.51793437, -0.52920421, -0.52160096, -0.58412027, -0.57858878,\n",
       "        -0.59306719, -0.44889331, -0.70020446, -0.44757531, -0.44160474,\n",
       "        -0.6984561 , -0.44407824, -0.44143748, -0.69613175, -0.44178812,\n",
       "        -0.44122229, -0.70270206, -0.4401104 , -0.44781199, -0.71630877,\n",
       "        -0.4480706 , -0.44311031, -0.71128442, -0.44523588, -0.44145381,\n",
       "        -0.69123628, -0.44200232, -0.44084503, -0.70935904, -0.44052472,\n",
       "        -0.44705425, -0.74581821, -0.44811293, -0.44287672, -0.72254676,\n",
       "        -0.44354981, -0.4414711 , -0.68852305, -0.44082506, -0.44067053,\n",
       "        -0.70033195, -0.44036204, -0.44926943, -0.71837602, -0.44909222,\n",
       "        -0.4446632 , -0.71105125, -0.44384649, -0.44228566, -0.69798517,\n",
       "        -0.44166782, -0.44155518, -0.69852623, -0.44159226, -0.44602534,\n",
       "        -0.65348181, -0.44831811, -0.4434081 , -0.65522381, -0.44181544,\n",
       "        -0.44254565, -0.63575942, -0.44039215, -0.44089467, -0.66403081,\n",
       "        -0.43959693, -0.44921084, -0.65316791, -0.44701756, -0.44346287,\n",
       "        -0.65989496, -0.4439915 , -0.44153748, -0.70358778, -0.44186193,\n",
       "        -0.43845614, -0.65143819, -0.44153889, -0.44878083, -0.68606411,\n",
       "        -0.44421733, -0.44476536, -0.68774826, -0.44242996, -0.44179282,\n",
       "        -0.71700388, -0.44141183, -0.44150135, -0.62479758, -0.43911293,\n",
       "        -0.44719106, -0.78456744, -0.44823815, -0.44203773, -0.64166515,\n",
       "        -0.44304622, -0.44249756, -0.646057  , -0.44151332, -0.43815543,\n",
       "        -0.63751738, -0.44154941, -0.4399038 , -0.43884155, -0.43902776,\n",
       "        -0.4368715 , -0.43715872, -0.43631999, -0.43613905, -0.4363852 ,\n",
       "        -0.4365134 , -0.43605161, -0.43647122, -0.43637847, -0.43977154,\n",
       "        -0.43936416, -0.43896249, -0.43754841, -0.43754039, -0.43667862,\n",
       "        -0.43626983, -0.43666282, -0.43608806, -0.43591247, -0.4356062 ,\n",
       "        -0.4361613 , -0.43880499, -0.43867607, -0.43953604, -0.43684319,\n",
       "        -0.43755246, -0.4371837 , -0.43642923, -0.43665018, -0.43649691,\n",
       "        -0.43537819, -0.43613339, -0.43633822, -0.44015397, -0.44034758,\n",
       "        -0.44028481, -0.43769676, -0.43711995, -0.43738346, -0.43606389,\n",
       "        -0.43675284, -0.43704021, -0.43616709, -0.43586287, -0.43556681,\n",
       "        -0.44019529, -0.43639785, -0.43860128, -0.44025644, -0.44071875,\n",
       "        -0.43807172, -0.44244616, -0.44264489, -0.44233656, -0.44342564,\n",
       "        -0.44170298, -0.44419877, -0.43856437, -0.43872178, -0.43819007,\n",
       "        -0.43844569, -0.44294605, -0.44083325, -0.44167723, -0.4431115 ,\n",
       "        -0.44343691, -0.44462879, -0.44208957, -0.44233809, -0.43864713,\n",
       "        -0.43973465, -0.43556831, -0.44116162, -0.44100582, -0.43958513,\n",
       "        -0.4431918 , -0.44483595, -0.44274315, -0.4419331 , -0.44317829,\n",
       "        -0.44671836, -0.43783438, -0.43771066, -0.43698023, -0.43706716,\n",
       "        -0.43746342, -0.43703699, -0.43821206, -0.4362572 , -0.43751921,\n",
       "        -0.43813755, -0.43796169, -0.43874414]),\n",
       " 'std_test_neg_log_loss': array([0.01606659, 0.01776839, 0.01716378, 0.01223729, 0.04548495,\n",
       "        0.02059188, 0.04613869, 0.03682747, 0.02655163, 0.01428315,\n",
       "        0.04672393, 0.04473358, 0.01215227, 0.01272536, 0.01299253,\n",
       "        0.00930519, 0.0243046 , 0.02594437, 0.03605627, 0.02525197,\n",
       "        0.02990885, 0.0218694 , 0.02889729, 0.03815625, 0.01740159,\n",
       "        0.01232646, 0.01610363, 0.02476165, 0.02160572, 0.02583837,\n",
       "        0.03116842, 0.02655406, 0.02898477, 0.02178291, 0.04347198,\n",
       "        0.0174508 , 0.01193931, 0.01465993, 0.01356469, 0.02138236,\n",
       "        0.01181112, 0.01351698, 0.02387863, 0.01368215, 0.02975062,\n",
       "        0.02034363, 0.02419207, 0.0183853 , 0.01831174, 0.00923199,\n",
       "        0.01318109, 0.01973887, 0.01687168, 0.02080136, 0.02857941,\n",
       "        0.03388677, 0.06259385, 0.0432509 , 0.02494001, 0.04962948,\n",
       "        0.01723565, 0.01503346, 0.01525304, 0.02490996, 0.01953033,\n",
       "        0.0329849 , 0.03685268, 0.03727687, 0.04603264, 0.04160307,\n",
       "        0.03795511, 0.04822545, 0.01539191, 0.01534397, 0.01139874,\n",
       "        0.01671859, 0.01801199, 0.02696484, 0.02306279, 0.03732674,\n",
       "        0.02929544, 0.03133466, 0.01810796, 0.04817685, 0.02178974,\n",
       "        0.01679256, 0.01044662, 0.01267456, 0.01460437, 0.01618684,\n",
       "        0.01712134, 0.01603058, 0.02052808, 0.02426507, 0.04383589,\n",
       "        0.03173948, 0.0122342 , 0.02853385, 0.01165567, 0.01099338,\n",
       "        0.02217976, 0.01234502, 0.01264941, 0.0118571 , 0.01296194,\n",
       "        0.01207979, 0.01571779, 0.01151623, 0.01016688, 0.05077427,\n",
       "        0.01291741, 0.01142759, 0.01396201, 0.01269499, 0.01247719,\n",
       "        0.02567658, 0.01178955, 0.01193935, 0.01552219, 0.01200808,\n",
       "        0.01023918, 0.04972896, 0.01285144, 0.01235518, 0.02163591,\n",
       "        0.01184923, 0.01225293, 0.01912015, 0.01180964, 0.01255571,\n",
       "        0.01771813, 0.01207462, 0.01295853, 0.02448153, 0.01250779,\n",
       "        0.01243627, 0.0303861 , 0.01215692, 0.01169187, 0.0266173 ,\n",
       "        0.01286101, 0.01133419, 0.00602603, 0.01220906, 0.01293265,\n",
       "        0.06553592, 0.01241519, 0.01383494, 0.07581461, 0.01386742,\n",
       "        0.01446628, 0.04101383, 0.01320075, 0.01325148, 0.09764682,\n",
       "        0.01069013, 0.01528935, 0.02370557, 0.01287726, 0.01383673,\n",
       "        0.11494899, 0.01434082, 0.01483759, 0.02711569, 0.01414601,\n",
       "        0.01492795, 0.04259641, 0.01258622, 0.01468191, 0.01989541,\n",
       "        0.01526438, 0.01367754, 0.08448972, 0.01268956, 0.01306173,\n",
       "        0.05228666, 0.01500725, 0.01536784, 0.08615637, 0.01259563,\n",
       "        0.01492039, 0.17861443, 0.01543731, 0.01282419, 0.0552019 ,\n",
       "        0.01390853, 0.01692869, 0.07491036, 0.01185674, 0.01543606,\n",
       "        0.05033628, 0.01343196, 0.01300644, 0.01271601, 0.01319187,\n",
       "        0.01214514, 0.01264548, 0.01281396, 0.01256194, 0.01262762,\n",
       "        0.01281774, 0.01347483, 0.01315447, 0.01308471, 0.01275862,\n",
       "        0.01298863, 0.01321759, 0.0124629 , 0.01221798, 0.01331115,\n",
       "        0.01289315, 0.01297923, 0.0124839 , 0.01289018, 0.01237346,\n",
       "        0.01218459, 0.0118883 , 0.01268669, 0.0138296 , 0.01301972,\n",
       "        0.01311147, 0.01295261, 0.01296083, 0.01254944, 0.01315551,\n",
       "        0.01238843, 0.01231867, 0.0128686 , 0.01317608, 0.0108537 ,\n",
       "        0.01163339, 0.01249529, 0.01260458, 0.01244446, 0.01225691,\n",
       "        0.01222495, 0.01248758, 0.01253935, 0.01278074, 0.0118225 ,\n",
       "        0.01487093, 0.01276407, 0.01143242, 0.01272886, 0.01442364,\n",
       "        0.01436187, 0.01458487, 0.01396516, 0.0120141 , 0.01542606,\n",
       "        0.01466328, 0.01498485, 0.0113931 , 0.01425253, 0.01254837,\n",
       "        0.01054001, 0.01620061, 0.01351548, 0.01413864, 0.01272545,\n",
       "        0.01615151, 0.01382345, 0.0131683 , 0.01170767, 0.0143745 ,\n",
       "        0.0131374 , 0.01111669, 0.0129986 , 0.01243683, 0.01452462,\n",
       "        0.01374497, 0.01624475, 0.01548068, 0.01190681, 0.01304274,\n",
       "        0.0122013 , 0.01276488, 0.01299655, 0.01396353, 0.01343741,\n",
       "        0.01271077, 0.01347157, 0.01497084, 0.01389588, 0.01426942,\n",
       "        0.01298188, 0.01428196, 0.01377973]),\n",
       " 'rank_test_neg_log_loss': array([177, 150, 180, 198, 208, 200, 228, 222, 224, 239, 243, 244, 172,\n",
       "        160, 153, 197, 202, 204, 220, 230, 211, 237, 241, 253, 171, 176,\n",
       "        168, 194, 203, 192, 214, 217, 216, 238, 236, 246, 169, 159, 145,\n",
       "        185, 189, 191, 207, 212, 209, 219, 221, 227, 178, 170, 181, 206,\n",
       "        187, 190, 232, 234, 229, 266, 260, 257, 183, 162, 184, 210, 196,\n",
       "        205, 235, 233, 225, 259, 249, 248, 179, 174, 175, 195, 199, 186,\n",
       "        231, 223, 226, 250, 247, 251, 173, 167, 182, 201, 193, 188, 213,\n",
       "        218, 215, 242, 240, 245, 163, 276, 152, 102, 274, 137,  92, 272,\n",
       "        106,  90, 278,  73, 154, 283, 155, 126, 282, 144,  93, 271, 111,\n",
       "         86, 280,  81, 149, 287, 156, 123, 286, 134,  94, 270,  84,  82,\n",
       "        277,  79, 166, 285, 164, 141, 281, 135, 114, 273, 103, 100, 275,\n",
       "        101, 146, 263, 158, 130, 264, 108, 120, 254,  80,  87, 267,  69,\n",
       "        165, 262, 148, 133, 265, 136,  97, 279, 109,  54, 261,  98, 161,\n",
       "        268, 139, 142, 269, 117, 107, 284,  91,  95, 252,  65, 151, 288,\n",
       "        157, 112, 256, 125, 119, 258,  96,  50, 255,  99,  72,  62,  64,\n",
       "         30,  36,  16,  11,  19,  24,   7,  22,  18,  71,  66,  63,  42,\n",
       "         41,  27,  15,  26,   9,   6,   4,  12,  61,  58,  67,  29,  43,\n",
       "         37,  21,  25,  23,   1,  10,  17,  74,  78,  77,  44,  35,  38,\n",
       "          8,  28,  33,  13,   5,   2,  75,  20,  56,  76,  83,  48, 118,\n",
       "        121, 115, 131, 105, 138,  55,  59,  51,  53, 124,  85, 104, 127,\n",
       "        132, 140, 113, 116,  57,  70,   3,  89,  88,  68, 129, 143, 122,\n",
       "        110, 128, 147,  46,  45,  31,  34,  39,  32,  52,  14,  40,  49,\n",
       "         47,  60])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRIAL_1_MLP.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL ONE NEG LOG LOSS RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.01,\n",
       " 'classifier__hidden_layer_sizes': (20,),\n",
       " 'classifier__learning_rate': 'constant',\n",
       " 'classifier__solver': 'adam'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL ONE NEG LOG LOSS RESULTS ---------------------------------------------\")\n",
    "TRIAL_1_MLP.cv_results_['rank_test_neg_log_loss']\n",
    "TRIAL_1_MLP.cv_results_['params'][ np.argmin(TRIAL_1_MLP.cv_results_['rank_test_neg_log_loss']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL ONE F1 RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.0001,\n",
       " 'classifier__hidden_layer_sizes': (5,),\n",
       " 'classifier__learning_rate': 'adaptive',\n",
       " 'classifier__solver': 'sgd'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL ONE F1 RESULTS ---------------------------------------------\")\n",
    "TRIAL_1_MLP.cv_results_['rank_test_f1_micro']\n",
    "TRIAL_1_MLP.cv_results_['params'][ np.argmin(TRIAL_1_MLP.cv_results_['rank_test_f1_micro']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL ONE ROC AUC RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.001,\n",
       " 'classifier__hidden_layer_sizes': (5,),\n",
       " 'classifier__learning_rate': 'constant',\n",
       " 'classifier__solver': 'adam'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL ONE ROC AUC RESULTS ---------------------------------------------\")\n",
    "TRIAL_1_MLP.cv_results_['rank_test_roc_auc_ovo']\n",
    "TRIAL_1_MLP.cv_results_['params'][ np.argmin(TRIAL_1_MLP.cv_results_['rank_test_roc_auc_ovo']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL ONE JACCARD SCORE RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.0001,\n",
       " 'classifier__hidden_layer_sizes': (5,),\n",
       " 'classifier__learning_rate': 'adaptive',\n",
       " 'classifier__solver': 'sgd'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL ONE JACCARD SCORE RESULTS ---------------------------------------------\")\n",
    "TRIAL_1_MLP.cv_results_['rank_test_jaccard']\n",
    "TRIAL_1_MLP.cv_results_['params'][ np.argmin(TRIAL_1_MLP.cv_results_['rank_test_jaccard']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 1 Multilayered Perceptrons using best NEG LOG LOSS hyperperameters :0.7780112364682775\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 1 Multilayered Perceptrons using best F1 hyperperameters :0.761110857351665\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 1 Multilayered Perceptrons using best ROC_AUC hyperperameters :0.7680993924998858\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 1 Multilayered Perceptrons using best JACCARD hyperperameters :0.5058237792901841\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best NEG LOG LOSS results to train on test set, \n",
    "bestMPLT1_1 = MLPClassifier(activation = 'logistic', alpha = .01, hidden_layer_sizes = (20,), \n",
    "                          learning_rate = 'constant',solver = 'adam')\n",
    "bestMPLT1_1.fit(X_train,y_train)\n",
    "y_pred1_0 = bestMPLT1_1.predict(X_test)\n",
    "print('Accuracy of Trial 1 Multilayered Perceptrons using best NEG LOG LOSS hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred1_0)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best F1 results to train on test set, \n",
    "bestMPLT1_2 = MLPClassifier(activation = 'logistic', alpha = 0.0001, hidden_layer_sizes = (5,), \n",
    "                          learning_rate = 'constant',solver = 'adam')\n",
    "bestMPLT1_2.fit(X_train,y_train)\n",
    "y_pred1_1 = bestMPLT1_2.predict(X_test)\n",
    "print('Accuracy of Trial 1 Multilayered Perceptrons using best F1 hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred1_1)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best ROC_AUC results to train on test set,\n",
    "bestMPLT1_3 = MLPClassifier(activation = 'logistic', alpha = 0.0001, hidden_layer_sizes = (20,), \n",
    "                          learning_rate = 'constant',solver = 'adam')\n",
    "bestMPLT1_3.fit(X_train,y_train)\n",
    "y_pred1_2 = bestMPLT1_3.predict(X_test)\n",
    "print('Accuracy of Trial 1 Multilayered Perceptrons using best ROC_AUC hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred1_2)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best JACCARD results to train on test set, \n",
    "bestMPLT1_4 = MLPClassifier(activation = 'logistic', alpha = 0.0001, hidden_layer_sizes = (5,), \n",
    "                          learning_rate = 'adaptive',solver = 'sgd')\n",
    "bestMPLT1_4.fit(X_train,y_train)\n",
    "y_pred1_3 = bestMPLT1_4.predict(X_test)\n",
    "print('Accuracy of Trial 1 Multilayered Perceptrons using best JACCARD hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred1_3)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTI LAYERED PERCEPTRON TRIAL TWO ON LEAGUE OF LEGENDS DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    9.3s\n",
      "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed:   10.6s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   12.9s\n",
      "[Parallel(n_jobs=-1)]: Done 189 tasks      | elapsed:   13.7s\n",
      "[Parallel(n_jobs=-1)]: Done 210 tasks      | elapsed:   14.8s\n",
      "[Parallel(n_jobs=-1)]: Done 233 tasks      | elapsed:   17.4s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:   18.7s\n",
      "[Parallel(n_jobs=-1)]: Done 281 tasks      | elapsed:   20.8s\n",
      "[Parallel(n_jobs=-1)]: Done 306 tasks      | elapsed:   22.9s\n",
      "[Parallel(n_jobs=-1)]: Done 333 tasks      | elapsed:   25.3s\n",
      "[Parallel(n_jobs=-1)]: Done 360 tasks      | elapsed:   27.7s\n",
      "[Parallel(n_jobs=-1)]: Done 389 tasks      | elapsed:   29.3s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:   32.6s\n",
      "[Parallel(n_jobs=-1)]: Done 449 tasks      | elapsed:   34.3s\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:   37.7s\n",
      "[Parallel(n_jobs=-1)]: Done 513 tasks      | elapsed:   40.8s\n",
      "[Parallel(n_jobs=-1)]: Done 546 tasks      | elapsed:   44.1s\n",
      "[Parallel(n_jobs=-1)]: Done 581 tasks      | elapsed:   47.0s\n",
      "[Parallel(n_jobs=-1)]: Done 616 tasks      | elapsed:   50.6s\n",
      "[Parallel(n_jobs=-1)]: Done 653 tasks      | elapsed:   54.1s\n",
      "[Parallel(n_jobs=-1)]: Done 690 tasks      | elapsed:   57.3s\n",
      "[Parallel(n_jobs=-1)]: Done 729 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 809 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 850 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 893 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 936 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 981 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1026 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1073 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1120 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1169 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1269 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1320 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1373 tasks      | elapsed:  2.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL TWO RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1440 out of 1440 | elapsed:  2.4min finished\n"
     ]
    }
   ],
   "source": [
    "ITERATIONS = 1\n",
    "for i in range(1):\n",
    "    #For reach \"trial\", randomly select 5000 data points\n",
    "    random5000DataPoints = leagueData.sample(n = 5000, replace = True)\n",
    "    #From that 5000, create x set and y set\n",
    "    xSet = random5000DataPoints.iloc[:,1:17].values\n",
    "    ySet = random5000DataPoints.iloc[:,0].values\n",
    "    # Run GridSearch to find best hyperparameters per 5000 data points \n",
    "    \n",
    "    pipe = Pipeline([('std', StandardScaler()),('classifier', MLPClassifier(max_iter = 250))])\n",
    "\n",
    "    search_space = [\n",
    "                {\n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                {\n",
    "                 \n",
    "                  \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['sgd'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                \n",
    "                {\n",
    "                \n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['adam'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                ]\n",
    "    \n",
    "    runHyperParamSearch = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=5), \n",
    "                          scoring=['recall_micro', 'f1_micro','precision_micro','roc_auc_ovo','jaccard','neg_log_loss'], \n",
    "                          refit=False, verbose=10, n_jobs = -1)\n",
    "    TRIAL_2_MLP = runHyperParamSearch.fit(xSet, ySet)\n",
    "    print(\"---------------------------------TRIAL TWO RESULTS ---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.60372047, 0.61973314, 0.64665713, 0.86314282, 0.86154079,\n",
       "        0.84712787, 1.32123752, 1.37198071, 1.38318815, 1.6846499 ,\n",
       "        1.67133536, 1.70576577, 0.78037214, 0.64835653, 0.62603803,\n",
       "        0.81640253, 0.86514492, 0.89527183, 1.2960146 , 1.33704762,\n",
       "        1.39429984, 1.72908626, 1.64541659, 1.6113852 , 0.7340323 ,\n",
       "        0.64625301, 0.635147  , 0.83321681, 0.84282789, 0.88946447,\n",
       "        1.31703296, 1.34225426, 1.39540272, 1.71807661, 1.71697674,\n",
       "        1.6976573 , 0.70710254, 0.59480958, 0.52955518, 0.83291712,\n",
       "        0.85473747, 0.89607186, 1.31513071, 1.37117882, 1.3621726 ,\n",
       "        1.69325595, 1.63960876, 1.62359419, 0.80389209, 0.70030193,\n",
       "        0.68718972, 0.95992565, 0.95772414, 1.04449883, 1.46856575,\n",
       "        1.584864  , 1.58296037, 1.99191241, 1.9182467 , 1.91094184,\n",
       "        0.78086815, 0.71841793, 0.70410571, 0.97033377, 0.97464013,\n",
       "        1.04259763, 1.48187418, 1.54813166, 1.59336958, 1.97659926,\n",
       "        2.05476809, 1.96078668, 0.76275544, 0.70540648, 0.70560713,\n",
       "        0.9680336 , 0.98774891, 1.01797614, 1.48748069, 1.56824865,\n",
       "        1.59587317, 2.03945322, 1.93996739, 1.92795749, 0.76916151,\n",
       "        0.71691718, 0.67688279, 0.97363839, 0.98795071, 1.00096226,\n",
       "        1.50999942, 1.58456402, 1.59667377, 1.91114202, 1.95027566,\n",
       "        1.79294047, 2.03124552, 0.19606709, 2.28436413, 1.66993656,\n",
       "        0.1541327 , 2.28356428, 1.62539802, 0.19686971, 2.48673911,\n",
       "        1.98270516, 0.32017579, 2.51115928, 1.96999402, 0.17445045,\n",
       "        2.27615685, 1.68825188, 0.16794457, 2.31859369, 1.66893511,\n",
       "        0.16914539, 2.300879  , 1.74269919, 0.49392471, 2.45300941,\n",
       "        1.91764951, 0.17495041, 2.62435665, 1.88151855, 0.21618609,\n",
       "        2.44340119, 1.82887244, 0.20968037, 2.59563231, 1.73098898,\n",
       "        0.22489319, 2.5213686 , 2.09349995, 0.21618547, 2.49844885,\n",
       "        1.82326794, 0.20147333, 2.44259996, 1.83798132, 0.20317464,\n",
       "        2.5198669 , 1.8029501 , 0.22249045, 2.54638977, 1.09083824,\n",
       "        1.12326603, 1.73959627, 1.0329885 , 1.22455311, 1.60748196,\n",
       "        0.86584463, 1.61668963, 1.52401061, 0.88245916, 1.90013428,\n",
       "        1.67714229, 1.14648595, 1.22935729, 1.6812458 , 0.87465243,\n",
       "        0.85003138, 1.5170043 , 0.93660541, 1.6004766 , 1.65932693,\n",
       "        0.8389215 , 1.09293985, 1.48737926, 1.1041492 , 1.34765887,\n",
       "        1.57675581, 0.93960757, 1.37738452, 1.48197556, 0.96813245,\n",
       "        1.29751496, 1.58646426, 0.93920746, 1.33234563, 1.51950693,\n",
       "        1.00646515, 0.50973821, 1.41701856, 0.93830671, 1.63320451,\n",
       "        1.3990037 , 0.8612412 , 1.44194012, 1.50859723, 0.83671942,\n",
       "        2.05656829, 1.60387955, 1.84999051, 1.63000154, 1.92435503,\n",
       "        1.37508211, 1.36207204, 1.42302394, 1.41771936, 1.52651348,\n",
       "        1.35886922, 1.49038124, 1.21964912, 1.23396101, 1.818363  ,\n",
       "        1.94577265, 1.82937303, 1.47947259, 1.42572632, 1.44304056,\n",
       "        1.42402482, 1.47636967, 1.65021968, 1.52971582, 1.43303204,\n",
       "        1.4841764 , 1.99071221, 1.61458869, 1.83677964, 1.48998132,\n",
       "        1.39189725, 1.45014715, 1.37518291, 1.18001494, 1.33765049,\n",
       "        1.64291329, 1.37468219, 1.04519858, 1.82807202, 1.60798254,\n",
       "        1.50899739, 1.2606843 , 1.39339905, 1.32523971, 1.35976982,\n",
       "        1.33584828, 1.35136237, 1.34725885, 1.4513483 , 1.28890834,\n",
       "        1.98500705, 1.80415111, 2.11501913, 2.57031059, 2.67640147,\n",
       "        2.84474664, 3.23708425, 3.26771026, 3.38711329, 3.43935781,\n",
       "        3.51322098, 3.47448773, 1.86780648, 2.16356077, 2.13463573,\n",
       "        2.64777699, 2.35892911, 2.96765237, 3.26360664, 3.26931176,\n",
       "        3.24338965, 3.43095021, 3.42864804, 3.41884017, 1.88962483,\n",
       "        1.95998521, 2.06777749, 2.49824862, 2.77788863, 2.64967885,\n",
       "        3.18634028, 3.24549141, 3.28452439, 3.39431887, 3.42664661,\n",
       "        3.5132215 , 1.95327997, 1.88372025, 1.84178405, 2.55790014,\n",
       "        2.51586404, 2.23572268, 3.13619695, 3.02450089, 2.88838468,\n",
       "        3.30534239, 3.1459053 , 2.86836629]),\n",
       " 'std_fit_time': array([0.00727699, 0.02940285, 0.03037874, 0.03789969, 0.01379842,\n",
       "        0.03129325, 0.03796912, 0.01655053, 0.02812038, 0.06908695,\n",
       "        0.03695639, 0.02723807, 0.06306915, 0.03314801, 0.00723095,\n",
       "        0.01461824, 0.0359448 , 0.02818492, 0.03844234, 0.04908693,\n",
       "        0.05454336, 0.07149811, 0.05734306, 0.03433776, 0.07375418,\n",
       "        0.01944165, 0.01829265, 0.01314212, 0.02779626, 0.04413467,\n",
       "        0.05799261, 0.03022244, 0.0754616 , 0.02597993, 0.03218092,\n",
       "        0.05825569, 0.06258224, 0.05336273, 0.10280818, 0.02223282,\n",
       "        0.01728261, 0.03247594, 0.04261702, 0.04337276, 0.03384208,\n",
       "        0.02838475, 0.02346126, 0.02651464, 0.09927403, 0.03161293,\n",
       "        0.01809656, 0.02157885, 0.03753851, 0.02451526, 0.04365087,\n",
       "        0.04263111, 0.0398175 , 0.15775047, 0.07684749, 0.14196033,\n",
       "        0.04257401, 0.024591  , 0.02358985, 0.01934207, 0.03199914,\n",
       "        0.05508632, 0.05793508, 0.06466967, 0.02150361, 0.12450312,\n",
       "        0.16604592, 0.10157777, 0.06698645, 0.02439039, 0.02990131,\n",
       "        0.01413469, 0.02679353, 0.0339662 , 0.05066425, 0.04449479,\n",
       "        0.02635566, 0.15496994, 0.03628466, 0.12782443, 0.04719032,\n",
       "        0.01704606, 0.01673165, 0.02917726, 0.02868388, 0.02349912,\n",
       "        0.07022069, 0.10356802, 0.03094438, 0.05910004, 0.14400784,\n",
       "        0.11396682, 0.1653933 , 0.06143818, 0.05551789, 0.08455204,\n",
       "        0.00348152, 0.11841709, 0.09156937, 0.02690937, 0.19597073,\n",
       "        0.11410673, 0.10724289, 0.14341962, 0.10921079, 0.01440557,\n",
       "        0.16545196, 0.08561315, 0.0130445 , 0.1434546 , 0.077232  ,\n",
       "        0.00746961, 0.10522739, 0.1007691 , 0.56619131, 0.06246198,\n",
       "        0.10650874, 0.02452715, 0.11325537, 0.11726889, 0.07882354,\n",
       "        0.17934236, 0.18189488, 0.02884839, 0.10191952, 0.04000056,\n",
       "        0.01432523, 0.19043371, 0.16559302, 0.11541462, 0.06858428,\n",
       "        0.12034988, 0.04531629, 0.14984963, 0.10832776, 0.02410883,\n",
       "        0.09710183, 0.13008172, 0.02681386, 0.08223886, 0.25032742,\n",
       "        0.73719194, 0.22207948, 0.17793048, 0.80842511, 0.16280216,\n",
       "        0.14754411, 0.77472239, 0.13039449, 0.19377653, 0.66450743,\n",
       "        0.22067409, 0.27212676, 0.66768794, 0.193116  , 0.1134226 ,\n",
       "        0.66092168, 0.10054413, 0.14417065, 0.39972319, 0.15061797,\n",
       "        0.20926612, 0.42262122, 0.07974713, 0.16028555, 0.77782135,\n",
       "        0.09556881, 0.12195989, 0.93654042, 0.12512943, 0.09982181,\n",
       "        0.36804304, 0.19093744, 0.18108524, 0.36397534, 0.06502865,\n",
       "        0.07948619, 0.28638013, 0.14797518, 0.07204715, 0.87326775,\n",
       "        0.09762407, 0.09551599, 0.85305215, 0.11352572, 0.11214522,\n",
       "        0.88378679, 0.15043101, 0.37595715, 0.37359147, 0.38713586,\n",
       "        0.27875722, 0.20186889, 0.14716863, 0.18526986, 0.18428088,\n",
       "        0.21368008, 0.31803028, 0.18134091, 0.23933581, 0.09349924,\n",
       "        0.28201015, 0.23810791, 0.16676454, 0.30797287, 0.0964553 ,\n",
       "        0.2549295 , 0.33370951, 0.24669793, 0.19777576, 0.36423849,\n",
       "        0.12489513, 0.36812957, 0.31347146, 0.25598519, 0.2373528 ,\n",
       "        0.16323564, 0.21031354, 0.22492624, 0.12136028, 0.36180372,\n",
       "        0.22485067, 0.32199597, 0.30884384, 0.46867321, 0.09578559,\n",
       "        0.25771472, 0.09857765, 0.24739061, 0.18040702, 0.14096632,\n",
       "        0.20132456, 0.15580795, 0.38916383, 0.13854824, 0.19353733,\n",
       "        0.41525042, 0.29890178, 0.40547316, 0.47881681, 0.28435044,\n",
       "        0.29045991, 0.09768041, 0.04926969, 0.05214453, 0.07731385,\n",
       "        0.0261979 , 0.09381835, 0.44514182, 0.38570502, 0.37932034,\n",
       "        0.41641212, 0.73056755, 0.06250461, 0.10386295, 0.03524288,\n",
       "        0.04519974, 0.05725308, 0.07726703, 0.10339789, 0.23490073,\n",
       "        0.08333235, 0.35400382, 0.47796403, 0.20136708, 0.4789082 ,\n",
       "        0.04696739, 0.07174322, 0.04206765, 0.04825218, 0.04861429,\n",
       "        0.10180823, 0.29145236, 0.4584285 , 0.2922839 , 0.26987689,\n",
       "        0.39950469, 0.47912992, 0.13125802, 0.4694209 , 0.7074632 ,\n",
       "        0.03639894, 0.04876433, 0.16396837]),\n",
       " 'mean_score_time': array([0.01020861, 0.01110997, 0.01321077, 0.01871619, 0.01221066,\n",
       "        0.01231222, 0.01711493, 0.01521282, 0.01551466, 0.01481233,\n",
       "        0.01231203, 0.01231041, 0.01150784, 0.01020951, 0.01181016,\n",
       "        0.01241107, 0.01391168, 0.01331058, 0.01551309, 0.01621566,\n",
       "        0.01611309, 0.0139122 , 0.01551123, 0.01181002, 0.01121011,\n",
       "        0.01080928, 0.01120963, 0.0111083 , 0.01320977, 0.01301136,\n",
       "        0.01521378, 0.01611528, 0.01731534, 0.0142118 , 0.01200933,\n",
       "        0.01261096, 0.01131177, 0.01091061, 0.01090975, 0.01341219,\n",
       "        0.01200881, 0.01301007, 0.01431184, 0.01491394, 0.01591311,\n",
       "        0.01671429, 0.01351213, 0.01321192, 0.01290941, 0.0115098 ,\n",
       "        0.01111012, 0.01581302, 0.01401024, 0.01441278, 0.01601052,\n",
       "        0.01561365, 0.01681547, 0.01491175, 0.01411233, 0.01250992,\n",
       "        0.01201057, 0.01070962, 0.01281185, 0.01321211, 0.01280947,\n",
       "        0.0170146 , 0.01411309, 0.01501169, 0.01501188, 0.02081881,\n",
       "        0.0143115 , 0.01271019, 0.01171036, 0.01090913, 0.01131024,\n",
       "        0.01201029, 0.01291184, 0.01381073, 0.01471043, 0.01751695,\n",
       "        0.01401267, 0.0144125 , 0.01461329, 0.01241035, 0.011409  ,\n",
       "        0.0105092 , 0.01180954, 0.01170912, 0.01290984, 0.01301169,\n",
       "        0.01501169, 0.01641307, 0.01411219, 0.0135119 , 0.0123105 ,\n",
       "        0.01151071, 0.00900812, 0.00960903, 0.00930824, 0.0098083 ,\n",
       "        0.00950799, 0.01040926, 0.0110096 , 0.00990877, 0.01000824,\n",
       "        0.00982003, 0.00980783, 0.00970836, 0.01090946, 0.01060929,\n",
       "        0.01090975, 0.01201081, 0.00910773, 0.00960851, 0.00980816,\n",
       "        0.00940833, 0.01050887, 0.00990829, 0.01070948, 0.0133111 ,\n",
       "        0.01010847, 0.00950823, 0.01131001, 0.01160984, 0.01070886,\n",
       "        0.00970855, 0.01050925, 0.01100936, 0.01160975, 0.01050925,\n",
       "        0.01171026, 0.01090903, 0.00910835, 0.01040936, 0.00900888,\n",
       "        0.01020851, 0.00970802, 0.00990882, 0.01020823, 0.00960808,\n",
       "        0.01000938, 0.01110997, 0.0100091 , 0.01231074, 0.00970807,\n",
       "        0.0097084 , 0.0103086 , 0.01130929, 0.01090894, 0.01020885,\n",
       "        0.01010866, 0.01551375, 0.01030869, 0.01080933, 0.01120963,\n",
       "        0.01070948, 0.01000838, 0.01000876, 0.00960813, 0.00970821,\n",
       "        0.00980844, 0.0119103 , 0.0100091 , 0.01030874, 0.01191001,\n",
       "        0.01070957, 0.01070962, 0.01080947, 0.00980878, 0.00950837,\n",
       "        0.01010861, 0.00940824, 0.01231012, 0.00980787, 0.01040921,\n",
       "        0.01040955, 0.01010861, 0.01110997, 0.0106092 , 0.01000881,\n",
       "        0.00930805, 0.00910773, 0.00970845, 0.01100926, 0.00990891,\n",
       "        0.00960851, 0.01010876, 0.01000814, 0.0097084 , 0.01171012,\n",
       "        0.01020899, 0.01501236, 0.00930834, 0.00940795, 0.01000881,\n",
       "        0.00930829, 0.01030865, 0.00940824, 0.00990887, 0.01010885,\n",
       "        0.010709  , 0.01191034, 0.01090922, 0.00990872, 0.00920815,\n",
       "        0.00920811, 0.00980816, 0.00970864, 0.01030893, 0.01080914,\n",
       "        0.01040926, 0.01070933, 0.01010928, 0.00990853, 0.01010919,\n",
       "        0.01030884, 0.00980806, 0.00940809, 0.00900774, 0.00960832,\n",
       "        0.00960855, 0.01020875, 0.0098084 , 0.0098084 , 0.00980854,\n",
       "        0.00980868, 0.01050897, 0.01020904, 0.00920787, 0.00930796,\n",
       "        0.00920782, 0.00930777, 0.01020856, 0.01030898, 0.00970788,\n",
       "        0.00960851, 0.01000867, 0.01020827, 0.01110959, 0.01000867,\n",
       "        0.00980873, 0.01020865, 0.01060925, 0.01080904, 0.00980878,\n",
       "        0.01020875, 0.0119102 , 0.01100936, 0.0119101 , 0.01090937,\n",
       "        0.01271086, 0.01160994, 0.0096086 , 0.01100903, 0.01010895,\n",
       "        0.01000872, 0.01030869, 0.0102088 , 0.01271114, 0.01040931,\n",
       "        0.01201034, 0.0113101 , 0.01241078, 0.01110983, 0.01000843,\n",
       "        0.00920792, 0.00950875, 0.0097086 , 0.01120954, 0.00970817,\n",
       "        0.01080937, 0.01020861, 0.01080928, 0.01140966, 0.01531339,\n",
       "        0.01100965, 0.00960851, 0.00920792, 0.00970831, 0.01020832,\n",
       "        0.01010885, 0.01030917, 0.01100969, 0.01050944, 0.01110892,\n",
       "        0.00980854, 0.00800643, 0.0059051 ]),\n",
       " 'std_score_time': array([6.00139829e-04, 4.89561156e-04, 3.69783432e-03, 7.27629961e-03,\n",
       "        1.03143834e-03, 6.79555311e-04, 1.98756511e-03, 2.25248513e-03,\n",
       "        2.32547915e-03, 1.56703912e-03, 9.29080299e-04, 1.07920905e-03,\n",
       "        1.44844233e-03, 6.79625471e-04, 1.60079927e-03, 1.59583936e-03,\n",
       "        2.10919855e-03, 6.01142737e-04, 1.45148412e-03, 3.23802582e-03,\n",
       "        2.71225275e-03, 1.59526125e-03, 2.28224915e-03, 1.36464939e-03,\n",
       "        8.12973215e-04, 4.01187322e-04, 5.09698833e-04, 9.69291648e-04,\n",
       "        1.75215981e-03, 5.48685862e-04, 6.77807162e-04, 2.61701863e-03,\n",
       "        3.29522936e-03, 1.53859175e-03, 4.47767723e-04, 1.49764126e-03,\n",
       "        1.47232243e-03, 8.60996543e-04, 9.17159372e-04, 2.49989143e-03,\n",
       "        5.48903626e-04, 1.00317093e-03, 1.28942843e-03, 1.24187666e-03,\n",
       "        1.49888541e-03, 4.92990855e-03, 1.14113857e-03, 1.75029022e-03,\n",
       "        3.05576012e-03, 5.47727271e-04, 1.02109111e-03, 5.06934873e-03,\n",
       "        1.81930690e-03, 2.37569059e-03, 3.83701502e-03, 1.20210022e-03,\n",
       "        1.69293601e-03, 2.65491218e-03, 1.24262678e-03, 8.95562535e-04,\n",
       "        9.49092720e-04, 7.49163632e-04, 3.61539613e-03, 2.46500020e-03,\n",
       "        1.07831206e-03, 5.38119038e-03, 1.23943334e-03, 6.32938646e-04,\n",
       "        1.38031162e-03, 9.42879539e-03, 2.13640253e-03, 1.03231021e-03,\n",
       "        8.72400829e-04, 1.11534832e-03, 5.10186359e-04, 3.18774874e-04,\n",
       "        1.39315945e-03, 1.02986826e-03, 2.42443626e-03, 2.45201831e-03,\n",
       "        8.38937959e-04, 1.02068423e-03, 2.59823865e-03, 4.89727394e-04,\n",
       "        6.66379807e-04, 4.47928436e-04, 2.85804377e-03, 8.72028481e-04,\n",
       "        1.96272547e-03, 7.72879622e-04, 3.01996778e-03, 4.09481048e-03,\n",
       "        4.90039620e-04, 7.75187321e-04, 6.00417835e-04, 1.26598871e-03,\n",
       "        3.81469727e-07, 5.83961452e-04, 2.45496155e-04, 8.71951969e-04,\n",
       "        4.47341460e-04, 8.60531026e-04, 1.37970602e-03, 4.89706877e-04,\n",
       "        5.48249328e-04, 4.21054485e-04, 2.45281716e-04, 2.45223651e-04,\n",
       "        3.07525423e-03, 2.01152338e-03, 2.35557780e-03, 2.68505079e-03,\n",
       "        2.00200102e-04, 4.90125975e-04, 2.45554359e-04, 2.00320567e-04,\n",
       "        6.32862250e-04, 5.83903905e-04, 1.69273688e-03, 4.53816098e-03,\n",
       "        8.00788489e-04, 5.48379433e-04, 3.12653539e-03, 3.36971993e-03,\n",
       "        6.78472730e-04, 4.00948563e-04, 7.08471806e-04, 1.26610122e-03,\n",
       "        2.41840330e-03, 1.04939226e-03, 3.44735715e-03, 1.82914591e-03,\n",
       "        2.00486318e-04, 1.59548324e-03, 2.41733396e-06, 8.72438713e-04,\n",
       "        7.48813110e-04, 5.83421119e-04, 4.01151587e-04, 4.90797102e-04,\n",
       "        7.74694625e-04, 8.60525068e-04, 4.47448130e-04, 2.33886834e-03,\n",
       "        3.99995543e-04, 7.48869911e-04, 1.03065556e-03, 1.43657496e-03,\n",
       "        1.11485968e-03, 1.16697690e-03, 7.35462253e-04, 7.08380334e-03,\n",
       "        4.00639659e-04, 9.28086691e-04, 1.47069696e-03, 1.03108631e-03,\n",
       "        3.16581896e-04, 8.37653844e-04, 4.90252272e-04, 4.00877467e-04,\n",
       "        2.44912291e-04, 4.80408673e-03, 3.16205148e-04, 2.45673057e-04,\n",
       "        3.80358698e-03, 6.78823767e-04, 2.44912616e-04, 1.28998205e-03,\n",
       "        6.00457445e-04, 1.00088130e-03, 1.11458146e-03, 2.00152418e-04,\n",
       "        5.60517313e-03, 4.00806335e-04, 8.60552758e-04, 1.06868444e-03,\n",
       "        3.75273043e-04, 4.90923425e-04, 9.70429697e-04, 3.37174788e-07,\n",
       "        2.45048346e-04, 1.99961797e-04, 4.00340858e-04, 2.51229635e-03,\n",
       "        1.06866656e-03, 2.00271947e-04, 4.90067180e-04, 4.47607815e-04,\n",
       "        2.45963311e-04, 1.69273684e-03, 4.00114216e-04, 8.55694173e-03,\n",
       "        4.00007037e-04, 3.74534277e-04, 1.55058930e-03, 4.00758056e-04,\n",
       "        9.28215083e-04, 3.74801650e-04, 8.00871925e-04, 7.35046894e-04,\n",
       "        9.28143219e-04, 2.22484218e-03, 2.31267819e-03, 1.99580328e-04,\n",
       "        2.45320618e-04, 2.45165193e-04, 6.78795785e-04, 5.10407728e-04,\n",
       "        2.11383161e-03, 1.94096273e-03, 3.74584789e-04, 1.36513884e-03,\n",
       "        9.70769085e-04, 3.74508906e-04, 3.74355496e-04, 1.36496780e-03,\n",
       "        1.40135658e-03, 5.83535453e-04, 4.10190833e-07, 9.70282193e-04,\n",
       "        4.90504874e-04, 1.40147220e-03, 4.00292926e-04, 2.44873064e-04,\n",
       "        4.00603125e-04, 4.00495714e-04, 4.47714477e-04, 6.78640892e-04,\n",
       "        2.45457901e-04, 2.44775734e-04, 4.00138168e-04, 2.44912431e-04,\n",
       "        1.03071578e-03, 8.13184548e-04, 4.00686534e-04, 2.00271777e-04,\n",
       "        5.47987739e-04, 5.10258018e-04, 1.82929202e-03, 3.16431114e-04,\n",
       "        5.10248656e-04, 1.50452435e-03, 2.26927452e-03, 6.79041723e-04,\n",
       "        4.00543298e-04, 6.79182393e-04, 9.17083503e-04, 8.37283370e-04,\n",
       "        2.33424594e-03, 3.74304486e-04, 1.80709583e-03, 1.32013009e-03,\n",
       "        5.83952695e-04, 2.28244396e-03, 5.83723514e-04, 1.05000592e-03,\n",
       "        9.80289426e-04, 4.00567406e-04, 2.31726312e-03, 2.00129493e-04,\n",
       "        1.95116228e-03, 8.12832249e-04, 2.22452031e-03, 1.02111878e-03,\n",
       "        8.37397309e-04, 4.00209469e-04, 4.47287834e-04, 5.10613193e-04,\n",
       "        2.35988939e-03, 5.10781478e-04, 1.07782284e-03, 4.00662858e-04,\n",
       "        6.78619955e-04, 1.39371359e-03, 5.89217149e-03, 7.07999960e-04,\n",
       "        8.00490555e-04, 2.45125950e-04, 6.79006530e-04, 7.48564167e-04,\n",
       "        8.61162437e-04, 1.12329109e-03, 7.07696242e-04, 3.16280390e-04,\n",
       "        7.35416697e-04, 6.79013501e-04, 1.26581837e-03, 2.00486318e-04]),\n",
       " 'param_classifier': masked_array(data=[MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__activation': masked_array(data=['logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__alpha': masked_array(data=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__hidden_layer_sizes': masked_array(data=[(5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__learning_rate': masked_array(data=['constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'}],\n",
       " 'split0_test_recall_micro': array([0.763, 0.767, 0.76 , 0.762, 0.754, 0.755, 0.731, 0.753, 0.715,\n",
       "        0.715, 0.749, 0.741, 0.766, 0.765, 0.76 , 0.741, 0.748, 0.748,\n",
       "        0.743, 0.755, 0.722, 0.728, 0.755, 0.733, 0.765, 0.78 , 0.775,\n",
       "        0.761, 0.761, 0.772, 0.746, 0.749, 0.741, 0.747, 0.762, 0.743,\n",
       "        0.775, 0.773, 0.766, 0.763, 0.765, 0.779, 0.744, 0.755, 0.757,\n",
       "        0.745, 0.747, 0.748, 0.765, 0.77 , 0.782, 0.743, 0.762, 0.759,\n",
       "        0.734, 0.717, 0.742, 0.721, 0.714, 0.725, 0.775, 0.774, 0.76 ,\n",
       "        0.749, 0.757, 0.758, 0.749, 0.744, 0.727, 0.733, 0.746, 0.722,\n",
       "        0.768, 0.758, 0.763, 0.736, 0.748, 0.762, 0.753, 0.754, 0.745,\n",
       "        0.721, 0.734, 0.746, 0.771, 0.77 , 0.78 , 0.766, 0.754, 0.741,\n",
       "        0.748, 0.752, 0.741, 0.726, 0.751, 0.752, 0.77 , 0.758, 0.774,\n",
       "        0.777, 0.505, 0.771, 0.766, 0.495, 0.766, 0.766, 0.496, 0.769,\n",
       "        0.773, 0.48 , 0.767, 0.775, 0.646, 0.775, 0.775, 0.519, 0.774,\n",
       "        0.775, 0.498, 0.771, 0.769, 0.495, 0.771, 0.776, 0.495, 0.768,\n",
       "        0.767, 0.519, 0.77 , 0.772, 0.505, 0.772, 0.77 , 0.495, 0.771,\n",
       "        0.772, 0.495, 0.77 , 0.772, 0.328, 0.77 , 0.776, 0.495, 0.771,\n",
       "        0.772, 0.602, 0.769, 0.776, 0.661, 0.776, 0.76 , 0.495, 0.773,\n",
       "        0.766, 0.581, 0.766, 0.774, 0.574, 0.773, 0.769, 0.519, 0.766,\n",
       "        0.767, 0.548, 0.77 , 0.768, 0.722, 0.773, 0.763, 0.487, 0.765,\n",
       "        0.764, 0.592, 0.774, 0.776, 0.614, 0.771, 0.767, 0.649, 0.763,\n",
       "        0.77 , 0.519, 0.77 , 0.771, 0.519, 0.777, 0.779, 0.692, 0.768,\n",
       "        0.77 , 0.318, 0.771, 0.775, 0.776, 0.774, 0.772, 0.774, 0.776,\n",
       "        0.773, 0.774, 0.773, 0.776, 0.771, 0.771, 0.772, 0.772, 0.776,\n",
       "        0.775, 0.774, 0.775, 0.773, 0.772, 0.774, 0.771, 0.775, 0.776,\n",
       "        0.779, 0.773, 0.773, 0.774, 0.772, 0.776, 0.772, 0.77 , 0.771,\n",
       "        0.773, 0.774, 0.772, 0.774, 0.774, 0.773, 0.773, 0.772, 0.775,\n",
       "        0.773, 0.775, 0.775, 0.774, 0.773, 0.772, 0.778, 0.781, 0.782,\n",
       "        0.774, 0.777, 0.78 , 0.775, 0.778, 0.771, 0.777, 0.775, 0.782,\n",
       "        0.78 , 0.772, 0.773, 0.782, 0.78 , 0.78 , 0.771, 0.774, 0.771,\n",
       "        0.769, 0.773, 0.766, 0.774, 0.774, 0.775, 0.774, 0.773, 0.776,\n",
       "        0.778, 0.781, 0.772, 0.772, 0.775, 0.773, 0.775, 0.776, 0.767,\n",
       "        0.778, 0.774, 0.781, 0.774, 0.783, 0.777, 0.777, 0.78 , 0.781]),\n",
       " 'split1_test_recall_micro': array([0.76 , 0.778, 0.77 , 0.772, 0.769, 0.745, 0.74 , 0.731, 0.746,\n",
       "        0.716, 0.724, 0.733, 0.771, 0.76 , 0.769, 0.763, 0.753, 0.762,\n",
       "        0.751, 0.747, 0.747, 0.737, 0.732, 0.724, 0.771, 0.768, 0.774,\n",
       "        0.755, 0.772, 0.736, 0.739, 0.748, 0.753, 0.733, 0.735, 0.727,\n",
       "        0.766, 0.768, 0.766, 0.768, 0.752, 0.75 , 0.735, 0.752, 0.753,\n",
       "        0.738, 0.744, 0.753, 0.767, 0.769, 0.766, 0.75 , 0.747, 0.762,\n",
       "        0.758, 0.751, 0.731, 0.713, 0.728, 0.746, 0.765, 0.769, 0.764,\n",
       "        0.75 , 0.749, 0.768, 0.76 , 0.744, 0.732, 0.735, 0.717, 0.736,\n",
       "        0.781, 0.764, 0.776, 0.763, 0.758, 0.762, 0.757, 0.727, 0.745,\n",
       "        0.751, 0.717, 0.731, 0.777, 0.769, 0.767, 0.76 , 0.772, 0.743,\n",
       "        0.761, 0.739, 0.751, 0.745, 0.728, 0.732, 0.78 , 0.496, 0.78 ,\n",
       "        0.775, 0.587, 0.778, 0.783, 0.506, 0.775, 0.777, 0.494, 0.778,\n",
       "        0.778, 0.525, 0.785, 0.781, 0.508, 0.775, 0.783, 0.616, 0.782,\n",
       "        0.785, 0.494, 0.781, 0.783, 0.488, 0.781, 0.784, 0.506, 0.782,\n",
       "        0.778, 0.494, 0.782, 0.782, 0.535, 0.787, 0.775, 0.506, 0.787,\n",
       "        0.784, 0.493, 0.784, 0.782, 0.568, 0.783, 0.781, 0.618, 0.786,\n",
       "        0.785, 0.6  , 0.774, 0.783, 0.495, 0.78 , 0.776, 0.647, 0.786,\n",
       "        0.78 , 0.662, 0.786, 0.78 , 0.581, 0.781, 0.774, 0.632, 0.783,\n",
       "        0.776, 0.533, 0.781, 0.784, 0.727, 0.785, 0.784, 0.572, 0.777,\n",
       "        0.783, 0.467, 0.779, 0.781, 0.657, 0.782, 0.786, 0.655, 0.777,\n",
       "        0.776, 0.648, 0.777, 0.779, 0.509, 0.781, 0.777, 0.547, 0.783,\n",
       "        0.783, 0.59 , 0.784, 0.785, 0.784, 0.787, 0.786, 0.784, 0.786,\n",
       "        0.786, 0.786, 0.785, 0.784, 0.784, 0.787, 0.784, 0.786, 0.785,\n",
       "        0.784, 0.784, 0.787, 0.784, 0.786, 0.786, 0.786, 0.785, 0.786,\n",
       "        0.789, 0.787, 0.786, 0.784, 0.785, 0.785, 0.785, 0.787, 0.788,\n",
       "        0.786, 0.784, 0.785, 0.786, 0.782, 0.782, 0.784, 0.785, 0.787,\n",
       "        0.786, 0.785, 0.786, 0.785, 0.785, 0.785, 0.777, 0.784, 0.775,\n",
       "        0.775, 0.778, 0.77 , 0.78 , 0.775, 0.774, 0.76 , 0.777, 0.771,\n",
       "        0.775, 0.778, 0.783, 0.782, 0.779, 0.775, 0.771, 0.778, 0.777,\n",
       "        0.767, 0.768, 0.762, 0.784, 0.779, 0.784, 0.775, 0.778, 0.782,\n",
       "        0.776, 0.773, 0.777, 0.774, 0.771, 0.769, 0.778, 0.778, 0.779,\n",
       "        0.78 , 0.779, 0.777, 0.776, 0.787, 0.777, 0.774, 0.773, 0.766]),\n",
       " 'split2_test_recall_micro': array([0.784, 0.795, 0.786, 0.768, 0.769, 0.786, 0.765, 0.753, 0.767,\n",
       "        0.747, 0.742, 0.749, 0.786, 0.779, 0.787, 0.755, 0.758, 0.772,\n",
       "        0.752, 0.758, 0.753, 0.749, 0.733, 0.761, 0.793, 0.787, 0.771,\n",
       "        0.752, 0.758, 0.777, 0.782, 0.755, 0.741, 0.764, 0.734, 0.749,\n",
       "        0.803, 0.789, 0.789, 0.772, 0.788, 0.77 , 0.753, 0.759, 0.769,\n",
       "        0.765, 0.768, 0.759, 0.77 , 0.793, 0.792, 0.759, 0.755, 0.775,\n",
       "        0.753, 0.769, 0.762, 0.741, 0.736, 0.767, 0.778, 0.773, 0.773,\n",
       "        0.738, 0.769, 0.766, 0.775, 0.761, 0.749, 0.734, 0.752, 0.756,\n",
       "        0.792, 0.78 , 0.784, 0.761, 0.774, 0.757, 0.766, 0.749, 0.76 ,\n",
       "        0.737, 0.752, 0.738, 0.791, 0.771, 0.783, 0.781, 0.772, 0.784,\n",
       "        0.767, 0.764, 0.761, 0.743, 0.735, 0.763, 0.786, 0.477, 0.785,\n",
       "        0.791, 0.502, 0.791, 0.788, 0.586, 0.789, 0.786, 0.506, 0.79 ,\n",
       "        0.783, 0.494, 0.786, 0.786, 0.511, 0.785, 0.784, 0.401, 0.794,\n",
       "        0.789, 0.506, 0.791, 0.788, 0.358, 0.788, 0.791, 0.506, 0.788,\n",
       "        0.792, 0.494, 0.786, 0.789, 0.505, 0.786, 0.788, 0.526, 0.791,\n",
       "        0.789, 0.494, 0.791, 0.795, 0.494, 0.791, 0.791, 0.579, 0.791,\n",
       "        0.78 , 0.585, 0.803, 0.79 , 0.625, 0.791, 0.789, 0.616, 0.786,\n",
       "        0.784, 0.694, 0.787, 0.78 , 0.469, 0.785, 0.782, 0.656, 0.787,\n",
       "        0.782, 0.638, 0.789, 0.791, 0.665, 0.79 , 0.783, 0.469, 0.788,\n",
       "        0.786, 0.685, 0.797, 0.772, 0.619, 0.781, 0.795, 0.719, 0.796,\n",
       "        0.781, 0.765, 0.788, 0.797, 0.48 , 0.794, 0.783, 0.677, 0.789,\n",
       "        0.791, 0.641, 0.782, 0.794, 0.795, 0.796, 0.796, 0.796, 0.796,\n",
       "        0.796, 0.797, 0.797, 0.797, 0.796, 0.798, 0.793, 0.796, 0.798,\n",
       "        0.796, 0.793, 0.793, 0.792, 0.796, 0.797, 0.797, 0.793, 0.797,\n",
       "        0.796, 0.792, 0.793, 0.795, 0.795, 0.793, 0.795, 0.796, 0.798,\n",
       "        0.795, 0.797, 0.792, 0.796, 0.795, 0.791, 0.796, 0.795, 0.795,\n",
       "        0.795, 0.798, 0.794, 0.796, 0.794, 0.794, 0.795, 0.794, 0.791,\n",
       "        0.793, 0.797, 0.791, 0.793, 0.788, 0.784, 0.78 , 0.785, 0.79 ,\n",
       "        0.796, 0.795, 0.791, 0.795, 0.797, 0.792, 0.786, 0.786, 0.792,\n",
       "        0.787, 0.784, 0.782, 0.794, 0.794, 0.79 , 0.793, 0.788, 0.796,\n",
       "        0.786, 0.79 , 0.796, 0.788, 0.783, 0.785, 0.792, 0.793, 0.797,\n",
       "        0.789, 0.798, 0.787, 0.786, 0.793, 0.793, 0.796, 0.79 , 0.786]),\n",
       " 'split3_test_recall_micro': array([0.732, 0.757, 0.749, 0.745, 0.746, 0.757, 0.74 , 0.751, 0.728,\n",
       "        0.734, 0.748, 0.728, 0.753, 0.743, 0.752, 0.732, 0.755, 0.743,\n",
       "        0.731, 0.743, 0.748, 0.729, 0.734, 0.74 , 0.749, 0.746, 0.748,\n",
       "        0.749, 0.748, 0.753, 0.748, 0.74 , 0.728, 0.739, 0.736, 0.733,\n",
       "        0.758, 0.753, 0.752, 0.755, 0.764, 0.759, 0.758, 0.744, 0.76 ,\n",
       "        0.745, 0.753, 0.745, 0.757, 0.746, 0.756, 0.745, 0.765, 0.753,\n",
       "        0.745, 0.734, 0.765, 0.742, 0.746, 0.748, 0.76 , 0.751, 0.765,\n",
       "        0.75 , 0.74 , 0.752, 0.744, 0.746, 0.755, 0.719, 0.728, 0.719,\n",
       "        0.767, 0.764, 0.751, 0.749, 0.746, 0.764, 0.734, 0.746, 0.762,\n",
       "        0.745, 0.736, 0.743, 0.776, 0.764, 0.753, 0.751, 0.755, 0.738,\n",
       "        0.76 , 0.738, 0.768, 0.731, 0.739, 0.75 , 0.754, 0.494, 0.76 ,\n",
       "        0.767, 0.532, 0.766, 0.761, 0.516, 0.757, 0.759, 0.493, 0.767,\n",
       "        0.762, 0.527, 0.764, 0.764, 0.601, 0.763, 0.762, 0.505, 0.763,\n",
       "        0.761, 0.583, 0.763, 0.755, 0.494, 0.764, 0.76 , 0.653, 0.759,\n",
       "        0.759, 0.506, 0.766, 0.763, 0.574, 0.758, 0.76 , 0.516, 0.759,\n",
       "        0.759, 0.5  , 0.757, 0.763, 0.506, 0.768, 0.762, 0.484, 0.761,\n",
       "        0.76 , 0.666, 0.756, 0.764, 0.711, 0.761, 0.763, 0.672, 0.759,\n",
       "        0.756, 0.639, 0.752, 0.748, 0.585, 0.752, 0.754, 0.605, 0.759,\n",
       "        0.755, 0.661, 0.754, 0.757, 0.653, 0.753, 0.747, 0.315, 0.755,\n",
       "        0.755, 0.399, 0.748, 0.76 , 0.609, 0.747, 0.754, 0.686, 0.748,\n",
       "        0.758, 0.586, 0.753, 0.758, 0.556, 0.75 , 0.759, 0.564, 0.761,\n",
       "        0.762, 0.433, 0.754, 0.75 , 0.752, 0.752, 0.752, 0.752, 0.75 ,\n",
       "        0.749, 0.749, 0.75 , 0.753, 0.751, 0.751, 0.751, 0.746, 0.749,\n",
       "        0.752, 0.753, 0.751, 0.749, 0.752, 0.75 , 0.752, 0.751, 0.753,\n",
       "        0.757, 0.75 , 0.756, 0.752, 0.751, 0.753, 0.752, 0.752, 0.749,\n",
       "        0.752, 0.751, 0.751, 0.752, 0.754, 0.754, 0.754, 0.752, 0.753,\n",
       "        0.752, 0.749, 0.751, 0.751, 0.748, 0.754, 0.758, 0.754, 0.755,\n",
       "        0.761, 0.749, 0.753, 0.753, 0.763, 0.756, 0.754, 0.754, 0.759,\n",
       "        0.755, 0.751, 0.752, 0.752, 0.747, 0.757, 0.756, 0.758, 0.748,\n",
       "        0.757, 0.756, 0.757, 0.759, 0.754, 0.748, 0.75 , 0.754, 0.758,\n",
       "        0.756, 0.762, 0.754, 0.759, 0.76 , 0.757, 0.755, 0.749, 0.751,\n",
       "        0.758, 0.757, 0.754, 0.758, 0.755, 0.763, 0.756, 0.759, 0.754]),\n",
       " 'split4_test_recall_micro': array([0.744, 0.768, 0.764, 0.75 , 0.764, 0.78 , 0.752, 0.738, 0.735,\n",
       "        0.73 , 0.744, 0.731, 0.775, 0.768, 0.775, 0.741, 0.744, 0.759,\n",
       "        0.74 , 0.726, 0.725, 0.728, 0.724, 0.729, 0.774, 0.769, 0.77 ,\n",
       "        0.749, 0.771, 0.752, 0.736, 0.728, 0.732, 0.73 , 0.723, 0.721,\n",
       "        0.787, 0.777, 0.775, 0.747, 0.746, 0.771, 0.762, 0.739, 0.758,\n",
       "        0.756, 0.729, 0.736, 0.763, 0.752, 0.774, 0.726, 0.754, 0.762,\n",
       "        0.729, 0.732, 0.743, 0.725, 0.712, 0.743, 0.762, 0.769, 0.781,\n",
       "        0.761, 0.735, 0.734, 0.743, 0.747, 0.719, 0.747, 0.74 , 0.7  ,\n",
       "        0.77 , 0.763, 0.769, 0.744, 0.754, 0.761, 0.744, 0.731, 0.736,\n",
       "        0.735, 0.707, 0.734, 0.777, 0.761, 0.773, 0.744, 0.753, 0.757,\n",
       "        0.723, 0.735, 0.749, 0.736, 0.731, 0.736, 0.784, 0.506, 0.783,\n",
       "        0.777, 0.386, 0.776, 0.789, 0.472, 0.784, 0.78 , 0.492, 0.781,\n",
       "        0.787, 0.667, 0.785, 0.778, 0.494, 0.774, 0.779, 0.495, 0.776,\n",
       "        0.785, 0.494, 0.781, 0.779, 0.506, 0.788, 0.778, 0.494, 0.783,\n",
       "        0.784, 0.621, 0.778, 0.784, 0.565, 0.777, 0.787, 0.612, 0.776,\n",
       "        0.777, 0.494, 0.776, 0.788, 0.494, 0.779, 0.781, 0.529, 0.782,\n",
       "        0.78 , 0.54 , 0.774, 0.776, 0.484, 0.782, 0.777, 0.484, 0.79 ,\n",
       "        0.781, 0.577, 0.779, 0.784, 0.532, 0.778, 0.785, 0.694, 0.786,\n",
       "        0.78 , 0.606, 0.782, 0.785, 0.666, 0.783, 0.777, 0.706, 0.766,\n",
       "        0.786, 0.605, 0.783, 0.779, 0.587, 0.781, 0.785, 0.52 , 0.783,\n",
       "        0.788, 0.529, 0.771, 0.777, 0.539, 0.788, 0.772, 0.441, 0.784,\n",
       "        0.782, 0.589, 0.773, 0.782, 0.784, 0.79 , 0.785, 0.785, 0.786,\n",
       "        0.787, 0.784, 0.781, 0.786, 0.785, 0.786, 0.784, 0.784, 0.783,\n",
       "        0.788, 0.784, 0.784, 0.785, 0.784, 0.783, 0.784, 0.783, 0.785,\n",
       "        0.785, 0.782, 0.786, 0.783, 0.782, 0.785, 0.787, 0.787, 0.784,\n",
       "        0.783, 0.785, 0.783, 0.785, 0.784, 0.786, 0.784, 0.784, 0.785,\n",
       "        0.785, 0.785, 0.784, 0.784, 0.785, 0.785, 0.784, 0.773, 0.78 ,\n",
       "        0.779, 0.78 , 0.787, 0.772, 0.777, 0.773, 0.769, 0.775, 0.781,\n",
       "        0.786, 0.789, 0.778, 0.779, 0.783, 0.781, 0.776, 0.768, 0.778,\n",
       "        0.76 , 0.775, 0.774, 0.786, 0.786, 0.784, 0.78 , 0.789, 0.782,\n",
       "        0.772, 0.766, 0.775, 0.772, 0.77 , 0.774, 0.787, 0.784, 0.791,\n",
       "        0.789, 0.784, 0.781, 0.779, 0.777, 0.775, 0.779, 0.785, 0.779]),\n",
       " 'mean_test_recall_micro': array([0.7566, 0.773 , 0.7658, 0.7594, 0.7604, 0.7646, 0.7456, 0.7452,\n",
       "        0.7382, 0.7284, 0.7414, 0.7364, 0.7702, 0.763 , 0.7686, 0.7464,\n",
       "        0.7516, 0.7568, 0.7434, 0.7458, 0.739 , 0.7342, 0.7356, 0.7374,\n",
       "        0.7704, 0.77  , 0.7676, 0.7532, 0.762 , 0.758 , 0.7502, 0.744 ,\n",
       "        0.739 , 0.7426, 0.738 , 0.7346, 0.7778, 0.772 , 0.7696, 0.761 ,\n",
       "        0.763 , 0.7658, 0.7504, 0.7498, 0.7594, 0.7498, 0.7482, 0.7482,\n",
       "        0.7644, 0.766 , 0.774 , 0.7446, 0.7566, 0.7622, 0.7438, 0.7406,\n",
       "        0.7486, 0.7284, 0.7272, 0.7458, 0.768 , 0.7672, 0.7686, 0.7496,\n",
       "        0.75  , 0.7556, 0.7542, 0.7484, 0.7364, 0.7336, 0.7366, 0.7266,\n",
       "        0.7756, 0.7658, 0.7686, 0.7506, 0.756 , 0.7612, 0.7508, 0.7414,\n",
       "        0.7496, 0.7378, 0.7292, 0.7384, 0.7784, 0.767 , 0.7712, 0.7604,\n",
       "        0.7612, 0.7526, 0.7518, 0.7456, 0.754 , 0.7362, 0.7368, 0.7466,\n",
       "        0.7748, 0.5462, 0.7764, 0.7774, 0.5024, 0.7764, 0.7774, 0.515 ,\n",
       "        0.7742, 0.7736, 0.4962, 0.777 , 0.7766, 0.5386, 0.7774, 0.7768,\n",
       "        0.552 , 0.7744, 0.7766, 0.5072, 0.7778, 0.779 , 0.515 , 0.7774,\n",
       "        0.7748, 0.4682, 0.7784, 0.7778, 0.5308, 0.776 , 0.776 , 0.5268,\n",
       "        0.7764, 0.778 , 0.5368, 0.776 , 0.776 , 0.531 , 0.7768, 0.7762,\n",
       "        0.4952, 0.7756, 0.78  , 0.478 , 0.7782, 0.7782, 0.541 , 0.7782,\n",
       "        0.7754, 0.5986, 0.7752, 0.7778, 0.5952, 0.778 , 0.773 , 0.5828,\n",
       "        0.7788, 0.7734, 0.6306, 0.774 , 0.7732, 0.5482, 0.7738, 0.7728,\n",
       "        0.6212, 0.7762, 0.772 , 0.5972, 0.7752, 0.777 , 0.6866, 0.7768,\n",
       "        0.7708, 0.5098, 0.7702, 0.7748, 0.5496, 0.7762, 0.7736, 0.6172,\n",
       "        0.7724, 0.7774, 0.6458, 0.7734, 0.7746, 0.6094, 0.7718, 0.7764,\n",
       "        0.5206, 0.778 , 0.774 , 0.5842, 0.777 , 0.7776, 0.5142, 0.7728,\n",
       "        0.7772, 0.7782, 0.7798, 0.7782, 0.7782, 0.7788, 0.7782, 0.778 ,\n",
       "        0.7772, 0.7792, 0.7774, 0.7786, 0.7768, 0.7768, 0.7782, 0.779 ,\n",
       "        0.7776, 0.778 , 0.7766, 0.778 , 0.778 , 0.778 , 0.7774, 0.7794,\n",
       "        0.7812, 0.7768, 0.7788, 0.7776, 0.777 , 0.7784, 0.7782, 0.7784,\n",
       "        0.778 , 0.7778, 0.7782, 0.7766, 0.7786, 0.7778, 0.7772, 0.7782,\n",
       "        0.7776, 0.779 , 0.7782, 0.7784, 0.778 , 0.778 , 0.777 , 0.778 ,\n",
       "        0.7784, 0.7772, 0.7766, 0.7764, 0.7762, 0.7762, 0.7746, 0.7762,\n",
       "        0.7716, 0.768 , 0.7732, 0.7766, 0.7784, 0.777 , 0.7754, 0.778 ,\n",
       "        0.7772, 0.777 , 0.772 , 0.7728, 0.7732, 0.768 , 0.7712, 0.7682,\n",
       "        0.7794, 0.7774, 0.7762, 0.7744, 0.7764, 0.7788, 0.7736, 0.7744,\n",
       "        0.7748, 0.773 , 0.7718, 0.7716, 0.7774, 0.776 , 0.777 , 0.7788,\n",
       "        0.7784, 0.776 , 0.7746, 0.779 , 0.777 , 0.7764, 0.7774, 0.7732]),\n",
       " 'std_test_recall_micro': array([0.01770424, 0.01285302, 0.01220492, 0.01034601, 0.00904655,\n",
       "        0.01567929, 0.01177455, 0.00904212, 0.01756588, 0.0119432 ,\n",
       "        0.00906863, 0.00763151, 0.01083328, 0.01178134, 0.01207642,\n",
       "        0.01109234, 0.005004  , 0.0103034 , 0.00770973, 0.01126765,\n",
       "        0.01285302, 0.00813388, 0.01032666, 0.01290891, 0.01422111,\n",
       "        0.01392839, 0.00997196, 0.00448999, 0.00887694, 0.01484587,\n",
       "        0.01649727, 0.00931665, 0.0086487 , 0.01217539, 0.0128841 ,\n",
       "        0.01022937, 0.01586695, 0.01176435, 0.01217539, 0.0090111 ,\n",
       "        0.01442221, 0.01014692, 0.00976934, 0.00730479, 0.00531413,\n",
       "        0.0095373 , 0.01267123, 0.00773046, 0.00436348, 0.01643168,\n",
       "        0.01245793, 0.0108185 , 0.0063435 , 0.00719444, 0.01097998,\n",
       "        0.01782807, 0.01290891, 0.01137717, 0.01293677, 0.01337759,\n",
       "        0.00718331, 0.00835225, 0.00749933, 0.00728286, 0.0121326 ,\n",
       "        0.01222457, 0.01202331, 0.00640625, 0.01352923, 0.00889044,\n",
       "        0.01261111, 0.01865047, 0.00960417, 0.00744043, 0.01125344,\n",
       "        0.0102098 , 0.00995992, 0.00231517, 0.01097998, 0.01051856,\n",
       "        0.00989141, 0.01016661, 0.01568949, 0.00553534, 0.00668132,\n",
       "        0.00384708, 0.01066583, 0.01275304, 0.00884081, 0.01700118,\n",
       "        0.01566397, 0.0108922 , 0.00946573, 0.00713863, 0.00800999,\n",
       "        0.0112712 , 0.01177115, 0.10630974, 0.00900222, 0.00773563,\n",
       "        0.06571027, 0.00840476, 0.01163787, 0.03839792, 0.01165161,\n",
       "        0.00976934, 0.00507543, 0.0083666 , 0.00868562, 0.06667713,\n",
       "        0.00976934, 0.00735935, 0.06036224, 0.00697424, 0.00796492,\n",
       "        0.06842339, 0.01016661, 0.01011929, 0.03428119, 0.00958332,\n",
       "        0.01170299, 0.05540541, 0.0095205 , 0.01032279, 0.06131688,\n",
       "        0.01078888, 0.01178134, 0.0479975 , 0.00741889, 0.00931665,\n",
       "        0.02899931, 0.01060189, 0.01056409, 0.04178995, 0.01146124,\n",
       "        0.01038075, 0.00248193, 0.01170641, 0.01136662, 0.07986989,\n",
       "        0.00847113, 0.00945304, 0.05075825, 0.01083328, 0.00875443,\n",
       "        0.04043563, 0.01538051, 0.00863481, 0.09059007, 0.00981835,\n",
       "        0.01048809, 0.07829534, 0.01144378, 0.01068831, 0.04562718,\n",
       "        0.01331165, 0.01299846, 0.04388804, 0.01158275, 0.01097998,\n",
       "        0.0588809 , 0.01147868, 0.00993982, 0.04970875, 0.01222129,\n",
       "        0.01256981, 0.03132156, 0.01312098, 0.01406272, 0.12844828,\n",
       "        0.0113031 , 0.01285924, 0.10266762, 0.01604244, 0.0074458 ,\n",
       "        0.02270154, 0.01332066, 0.01481351, 0.06766801, 0.01654811,\n",
       "        0.01019019, 0.09042699, 0.01137365, 0.01264278, 0.02598923,\n",
       "        0.01516575, 0.00829458, 0.09222885, 0.01063955, 0.01028786,\n",
       "        0.12042159, 0.01064707, 0.01490503, 0.01442775, 0.0156512 ,\n",
       "        0.01515784, 0.01483779, 0.01572768, 0.01633891, 0.01623576,\n",
       "        0.0156512 , 0.01471598, 0.0153961 , 0.01625546, 0.01452446,\n",
       "        0.01718604, 0.01624069, 0.01509967, 0.01369087, 0.01469694,\n",
       "        0.01508111, 0.01507315, 0.01581139, 0.0154013 , 0.01438888,\n",
       "        0.01478648, 0.01330263, 0.0147973 , 0.01310572, 0.01443052,\n",
       "        0.01492649, 0.01379275, 0.01503862, 0.01565375, 0.01688787,\n",
       "        0.01468877, 0.01543243, 0.01431922, 0.01501466, 0.0136587 ,\n",
       "        0.01301384, 0.01411949, 0.01473228, 0.01447757, 0.01485126,\n",
       "        0.01641463, 0.01479189, 0.0151921 , 0.01596246, 0.01389964,\n",
       "        0.01204326, 0.01340746, 0.01197664, 0.0102684 , 0.01543243,\n",
       "        0.0136147 , 0.01297074, 0.00798499, 0.00900222, 0.00985901,\n",
       "        0.01028397, 0.01066958, 0.01363231, 0.01529706, 0.01312402,\n",
       "        0.01412799, 0.01642437, 0.01143678, 0.00969536, 0.00943186,\n",
       "        0.01435827, 0.010469  , 0.00919565, 0.00886341, 0.01202664,\n",
       "        0.01349963, 0.01489161, 0.01395134, 0.01272164, 0.01230285,\n",
       "        0.00991161, 0.01013114, 0.01337759, 0.00920869, 0.00746726,\n",
       "        0.00902441, 0.01275304, 0.01473771, 0.01658915, 0.01133843,\n",
       "        0.01336563, 0.01145426, 0.00924338, 0.01308434, 0.00954987,\n",
       "        0.01275304, 0.01078147, 0.01165161]),\n",
       " 'rank_test_recall_micro': array([198, 147, 180, 194, 192, 183, 225, 227, 239, 253, 233, 245, 165,\n",
       "        186, 171, 222, 207, 197, 231, 223, 237, 250, 248, 242, 164, 167,\n",
       "        176, 204, 188, 196, 211, 229, 236, 232, 240, 249,  54, 154, 168,\n",
       "        191, 185, 180, 210, 214, 195, 213, 219, 220, 184, 179, 134, 228,\n",
       "        199, 187, 230, 235, 217, 253, 255, 223, 173, 177, 169, 215, 212,\n",
       "        201, 202, 218, 245, 251, 244, 256, 117, 180, 169, 209, 200, 189,\n",
       "        208, 233, 215, 241, 252, 238,  18, 178, 161, 193, 189, 205, 206,\n",
       "        225, 203, 247, 243, 221, 123, 271,  99,  62, 284,  99,  62, 280,\n",
       "        133, 138, 285,  79,  92, 273,  61,  89, 268, 131,  92, 283,  51,\n",
       "          7, 279,  62, 125, 288,  18,  54, 276, 111, 111, 277,  99,  45,\n",
       "        274, 111, 111, 275,  85, 104, 286, 117,   2, 287,  27,  27, 272,\n",
       "         27, 120, 263, 121,  51, 265,  42, 147, 267,  12, 141, 259, 134,\n",
       "        146, 270, 137, 150, 260, 104, 154, 264, 121,  79, 257,  89, 163,\n",
       "        282, 165, 124, 269, 104, 140, 261, 153,  62, 258, 141, 127, 262,\n",
       "        157,  99, 278,  38, 134, 266,  76,  58, 281, 150,  71,  27,   3,\n",
       "         27,  27,  12,  26,  38,  71,   6,  62,  16,  85,  85,  27,   7,\n",
       "         58,  45,  91,  45,  42,  45,  62,   4,   1,  85,  11,  57,  82,\n",
       "         18,  27,  25,  38,  54,  27,  92,  16,  51,  71,  27,  58,   7,\n",
       "         27,  18,  45,  38,  76,  42,  18,  71,  92,  99, 104, 104, 127,\n",
       "        104, 159, 173, 143,  92,  24,  79, 119,  45,  71,  76, 154, 152,\n",
       "        143, 173, 161, 172,   4,  62, 104, 131,  97,  12, 138, 130, 125,\n",
       "        147, 157, 159,  70, 111,  82,  12,  18, 111, 127,  10,  82,  97,\n",
       "         62, 143]),\n",
       " 'split0_test_f1_micro': array([0.763, 0.767, 0.76 , 0.762, 0.754, 0.755, 0.731, 0.753, 0.715,\n",
       "        0.715, 0.749, 0.741, 0.766, 0.765, 0.76 , 0.741, 0.748, 0.748,\n",
       "        0.743, 0.755, 0.722, 0.728, 0.755, 0.733, 0.765, 0.78 , 0.775,\n",
       "        0.761, 0.761, 0.772, 0.746, 0.749, 0.741, 0.747, 0.762, 0.743,\n",
       "        0.775, 0.773, 0.766, 0.763, 0.765, 0.779, 0.744, 0.755, 0.757,\n",
       "        0.745, 0.747, 0.748, 0.765, 0.77 , 0.782, 0.743, 0.762, 0.759,\n",
       "        0.734, 0.717, 0.742, 0.721, 0.714, 0.725, 0.775, 0.774, 0.76 ,\n",
       "        0.749, 0.757, 0.758, 0.749, 0.744, 0.727, 0.733, 0.746, 0.722,\n",
       "        0.768, 0.758, 0.763, 0.736, 0.748, 0.762, 0.753, 0.754, 0.745,\n",
       "        0.721, 0.734, 0.746, 0.771, 0.77 , 0.78 , 0.766, 0.754, 0.741,\n",
       "        0.748, 0.752, 0.741, 0.726, 0.751, 0.752, 0.77 , 0.758, 0.774,\n",
       "        0.777, 0.505, 0.771, 0.766, 0.495, 0.766, 0.766, 0.496, 0.769,\n",
       "        0.773, 0.48 , 0.767, 0.775, 0.646, 0.775, 0.775, 0.519, 0.774,\n",
       "        0.775, 0.498, 0.771, 0.769, 0.495, 0.771, 0.776, 0.495, 0.768,\n",
       "        0.767, 0.519, 0.77 , 0.772, 0.505, 0.772, 0.77 , 0.495, 0.771,\n",
       "        0.772, 0.495, 0.77 , 0.772, 0.328, 0.77 , 0.776, 0.495, 0.771,\n",
       "        0.772, 0.602, 0.769, 0.776, 0.661, 0.776, 0.76 , 0.495, 0.773,\n",
       "        0.766, 0.581, 0.766, 0.774, 0.574, 0.773, 0.769, 0.519, 0.766,\n",
       "        0.767, 0.548, 0.77 , 0.768, 0.722, 0.773, 0.763, 0.487, 0.765,\n",
       "        0.764, 0.592, 0.774, 0.776, 0.614, 0.771, 0.767, 0.649, 0.763,\n",
       "        0.77 , 0.519, 0.77 , 0.771, 0.519, 0.777, 0.779, 0.692, 0.768,\n",
       "        0.77 , 0.318, 0.771, 0.775, 0.776, 0.774, 0.772, 0.774, 0.776,\n",
       "        0.773, 0.774, 0.773, 0.776, 0.771, 0.771, 0.772, 0.772, 0.776,\n",
       "        0.775, 0.774, 0.775, 0.773, 0.772, 0.774, 0.771, 0.775, 0.776,\n",
       "        0.779, 0.773, 0.773, 0.774, 0.772, 0.776, 0.772, 0.77 , 0.771,\n",
       "        0.773, 0.774, 0.772, 0.774, 0.774, 0.773, 0.773, 0.772, 0.775,\n",
       "        0.773, 0.775, 0.775, 0.774, 0.773, 0.772, 0.778, 0.781, 0.782,\n",
       "        0.774, 0.777, 0.78 , 0.775, 0.778, 0.771, 0.777, 0.775, 0.782,\n",
       "        0.78 , 0.772, 0.773, 0.782, 0.78 , 0.78 , 0.771, 0.774, 0.771,\n",
       "        0.769, 0.773, 0.766, 0.774, 0.774, 0.775, 0.774, 0.773, 0.776,\n",
       "        0.778, 0.781, 0.772, 0.772, 0.775, 0.773, 0.775, 0.776, 0.767,\n",
       "        0.778, 0.774, 0.781, 0.774, 0.783, 0.777, 0.777, 0.78 , 0.781]),\n",
       " 'split1_test_f1_micro': array([0.76 , 0.778, 0.77 , 0.772, 0.769, 0.745, 0.74 , 0.731, 0.746,\n",
       "        0.716, 0.724, 0.733, 0.771, 0.76 , 0.769, 0.763, 0.753, 0.762,\n",
       "        0.751, 0.747, 0.747, 0.737, 0.732, 0.724, 0.771, 0.768, 0.774,\n",
       "        0.755, 0.772, 0.736, 0.739, 0.748, 0.753, 0.733, 0.735, 0.727,\n",
       "        0.766, 0.768, 0.766, 0.768, 0.752, 0.75 , 0.735, 0.752, 0.753,\n",
       "        0.738, 0.744, 0.753, 0.767, 0.769, 0.766, 0.75 , 0.747, 0.762,\n",
       "        0.758, 0.751, 0.731, 0.713, 0.728, 0.746, 0.765, 0.769, 0.764,\n",
       "        0.75 , 0.749, 0.768, 0.76 , 0.744, 0.732, 0.735, 0.717, 0.736,\n",
       "        0.781, 0.764, 0.776, 0.763, 0.758, 0.762, 0.757, 0.727, 0.745,\n",
       "        0.751, 0.717, 0.731, 0.777, 0.769, 0.767, 0.76 , 0.772, 0.743,\n",
       "        0.761, 0.739, 0.751, 0.745, 0.728, 0.732, 0.78 , 0.496, 0.78 ,\n",
       "        0.775, 0.587, 0.778, 0.783, 0.506, 0.775, 0.777, 0.494, 0.778,\n",
       "        0.778, 0.525, 0.785, 0.781, 0.508, 0.775, 0.783, 0.616, 0.782,\n",
       "        0.785, 0.494, 0.781, 0.783, 0.488, 0.781, 0.784, 0.506, 0.782,\n",
       "        0.778, 0.494, 0.782, 0.782, 0.535, 0.787, 0.775, 0.506, 0.787,\n",
       "        0.784, 0.493, 0.784, 0.782, 0.568, 0.783, 0.781, 0.618, 0.786,\n",
       "        0.785, 0.6  , 0.774, 0.783, 0.495, 0.78 , 0.776, 0.647, 0.786,\n",
       "        0.78 , 0.662, 0.786, 0.78 , 0.581, 0.781, 0.774, 0.632, 0.783,\n",
       "        0.776, 0.533, 0.781, 0.784, 0.727, 0.785, 0.784, 0.572, 0.777,\n",
       "        0.783, 0.467, 0.779, 0.781, 0.657, 0.782, 0.786, 0.655, 0.777,\n",
       "        0.776, 0.648, 0.777, 0.779, 0.509, 0.781, 0.777, 0.547, 0.783,\n",
       "        0.783, 0.59 , 0.784, 0.785, 0.784, 0.787, 0.786, 0.784, 0.786,\n",
       "        0.786, 0.786, 0.785, 0.784, 0.784, 0.787, 0.784, 0.786, 0.785,\n",
       "        0.784, 0.784, 0.787, 0.784, 0.786, 0.786, 0.786, 0.785, 0.786,\n",
       "        0.789, 0.787, 0.786, 0.784, 0.785, 0.785, 0.785, 0.787, 0.788,\n",
       "        0.786, 0.784, 0.785, 0.786, 0.782, 0.782, 0.784, 0.785, 0.787,\n",
       "        0.786, 0.785, 0.786, 0.785, 0.785, 0.785, 0.777, 0.784, 0.775,\n",
       "        0.775, 0.778, 0.77 , 0.78 , 0.775, 0.774, 0.76 , 0.777, 0.771,\n",
       "        0.775, 0.778, 0.783, 0.782, 0.779, 0.775, 0.771, 0.778, 0.777,\n",
       "        0.767, 0.768, 0.762, 0.784, 0.779, 0.784, 0.775, 0.778, 0.782,\n",
       "        0.776, 0.773, 0.777, 0.774, 0.771, 0.769, 0.778, 0.778, 0.779,\n",
       "        0.78 , 0.779, 0.777, 0.776, 0.787, 0.777, 0.774, 0.773, 0.766]),\n",
       " 'split2_test_f1_micro': array([0.784, 0.795, 0.786, 0.768, 0.769, 0.786, 0.765, 0.753, 0.767,\n",
       "        0.747, 0.742, 0.749, 0.786, 0.779, 0.787, 0.755, 0.758, 0.772,\n",
       "        0.752, 0.758, 0.753, 0.749, 0.733, 0.761, 0.793, 0.787, 0.771,\n",
       "        0.752, 0.758, 0.777, 0.782, 0.755, 0.741, 0.764, 0.734, 0.749,\n",
       "        0.803, 0.789, 0.789, 0.772, 0.788, 0.77 , 0.753, 0.759, 0.769,\n",
       "        0.765, 0.768, 0.759, 0.77 , 0.793, 0.792, 0.759, 0.755, 0.775,\n",
       "        0.753, 0.769, 0.762, 0.741, 0.736, 0.767, 0.778, 0.773, 0.773,\n",
       "        0.738, 0.769, 0.766, 0.775, 0.761, 0.749, 0.734, 0.752, 0.756,\n",
       "        0.792, 0.78 , 0.784, 0.761, 0.774, 0.757, 0.766, 0.749, 0.76 ,\n",
       "        0.737, 0.752, 0.738, 0.791, 0.771, 0.783, 0.781, 0.772, 0.784,\n",
       "        0.767, 0.764, 0.761, 0.743, 0.735, 0.763, 0.786, 0.477, 0.785,\n",
       "        0.791, 0.502, 0.791, 0.788, 0.586, 0.789, 0.786, 0.506, 0.79 ,\n",
       "        0.783, 0.494, 0.786, 0.786, 0.511, 0.785, 0.784, 0.401, 0.794,\n",
       "        0.789, 0.506, 0.791, 0.788, 0.358, 0.788, 0.791, 0.506, 0.788,\n",
       "        0.792, 0.494, 0.786, 0.789, 0.505, 0.786, 0.788, 0.526, 0.791,\n",
       "        0.789, 0.494, 0.791, 0.795, 0.494, 0.791, 0.791, 0.579, 0.791,\n",
       "        0.78 , 0.585, 0.803, 0.79 , 0.625, 0.791, 0.789, 0.616, 0.786,\n",
       "        0.784, 0.694, 0.787, 0.78 , 0.469, 0.785, 0.782, 0.656, 0.787,\n",
       "        0.782, 0.638, 0.789, 0.791, 0.665, 0.79 , 0.783, 0.469, 0.788,\n",
       "        0.786, 0.685, 0.797, 0.772, 0.619, 0.781, 0.795, 0.719, 0.796,\n",
       "        0.781, 0.765, 0.788, 0.797, 0.48 , 0.794, 0.783, 0.677, 0.789,\n",
       "        0.791, 0.641, 0.782, 0.794, 0.795, 0.796, 0.796, 0.796, 0.796,\n",
       "        0.796, 0.797, 0.797, 0.797, 0.796, 0.798, 0.793, 0.796, 0.798,\n",
       "        0.796, 0.793, 0.793, 0.792, 0.796, 0.797, 0.797, 0.793, 0.797,\n",
       "        0.796, 0.792, 0.793, 0.795, 0.795, 0.793, 0.795, 0.796, 0.798,\n",
       "        0.795, 0.797, 0.792, 0.796, 0.795, 0.791, 0.796, 0.795, 0.795,\n",
       "        0.795, 0.798, 0.794, 0.796, 0.794, 0.794, 0.795, 0.794, 0.791,\n",
       "        0.793, 0.797, 0.791, 0.793, 0.788, 0.784, 0.78 , 0.785, 0.79 ,\n",
       "        0.796, 0.795, 0.791, 0.795, 0.797, 0.792, 0.786, 0.786, 0.792,\n",
       "        0.787, 0.784, 0.782, 0.794, 0.794, 0.79 , 0.793, 0.788, 0.796,\n",
       "        0.786, 0.79 , 0.796, 0.788, 0.783, 0.785, 0.792, 0.793, 0.797,\n",
       "        0.789, 0.798, 0.787, 0.786, 0.793, 0.793, 0.796, 0.79 , 0.786]),\n",
       " 'split3_test_f1_micro': array([0.732, 0.757, 0.749, 0.745, 0.746, 0.757, 0.74 , 0.751, 0.728,\n",
       "        0.734, 0.748, 0.728, 0.753, 0.743, 0.752, 0.732, 0.755, 0.743,\n",
       "        0.731, 0.743, 0.748, 0.729, 0.734, 0.74 , 0.749, 0.746, 0.748,\n",
       "        0.749, 0.748, 0.753, 0.748, 0.74 , 0.728, 0.739, 0.736, 0.733,\n",
       "        0.758, 0.753, 0.752, 0.755, 0.764, 0.759, 0.758, 0.744, 0.76 ,\n",
       "        0.745, 0.753, 0.745, 0.757, 0.746, 0.756, 0.745, 0.765, 0.753,\n",
       "        0.745, 0.734, 0.765, 0.742, 0.746, 0.748, 0.76 , 0.751, 0.765,\n",
       "        0.75 , 0.74 , 0.752, 0.744, 0.746, 0.755, 0.719, 0.728, 0.719,\n",
       "        0.767, 0.764, 0.751, 0.749, 0.746, 0.764, 0.734, 0.746, 0.762,\n",
       "        0.745, 0.736, 0.743, 0.776, 0.764, 0.753, 0.751, 0.755, 0.738,\n",
       "        0.76 , 0.738, 0.768, 0.731, 0.739, 0.75 , 0.754, 0.494, 0.76 ,\n",
       "        0.767, 0.532, 0.766, 0.761, 0.516, 0.757, 0.759, 0.493, 0.767,\n",
       "        0.762, 0.527, 0.764, 0.764, 0.601, 0.763, 0.762, 0.505, 0.763,\n",
       "        0.761, 0.583, 0.763, 0.755, 0.494, 0.764, 0.76 , 0.653, 0.759,\n",
       "        0.759, 0.506, 0.766, 0.763, 0.574, 0.758, 0.76 , 0.516, 0.759,\n",
       "        0.759, 0.5  , 0.757, 0.763, 0.506, 0.768, 0.762, 0.484, 0.761,\n",
       "        0.76 , 0.666, 0.756, 0.764, 0.711, 0.761, 0.763, 0.672, 0.759,\n",
       "        0.756, 0.639, 0.752, 0.748, 0.585, 0.752, 0.754, 0.605, 0.759,\n",
       "        0.755, 0.661, 0.754, 0.757, 0.653, 0.753, 0.747, 0.315, 0.755,\n",
       "        0.755, 0.399, 0.748, 0.76 , 0.609, 0.747, 0.754, 0.686, 0.748,\n",
       "        0.758, 0.586, 0.753, 0.758, 0.556, 0.75 , 0.759, 0.564, 0.761,\n",
       "        0.762, 0.433, 0.754, 0.75 , 0.752, 0.752, 0.752, 0.752, 0.75 ,\n",
       "        0.749, 0.749, 0.75 , 0.753, 0.751, 0.751, 0.751, 0.746, 0.749,\n",
       "        0.752, 0.753, 0.751, 0.749, 0.752, 0.75 , 0.752, 0.751, 0.753,\n",
       "        0.757, 0.75 , 0.756, 0.752, 0.751, 0.753, 0.752, 0.752, 0.749,\n",
       "        0.752, 0.751, 0.751, 0.752, 0.754, 0.754, 0.754, 0.752, 0.753,\n",
       "        0.752, 0.749, 0.751, 0.751, 0.748, 0.754, 0.758, 0.754, 0.755,\n",
       "        0.761, 0.749, 0.753, 0.753, 0.763, 0.756, 0.754, 0.754, 0.759,\n",
       "        0.755, 0.751, 0.752, 0.752, 0.747, 0.757, 0.756, 0.758, 0.748,\n",
       "        0.757, 0.756, 0.757, 0.759, 0.754, 0.748, 0.75 , 0.754, 0.758,\n",
       "        0.756, 0.762, 0.754, 0.759, 0.76 , 0.757, 0.755, 0.749, 0.751,\n",
       "        0.758, 0.757, 0.754, 0.758, 0.755, 0.763, 0.756, 0.759, 0.754]),\n",
       " 'split4_test_f1_micro': array([0.744, 0.768, 0.764, 0.75 , 0.764, 0.78 , 0.752, 0.738, 0.735,\n",
       "        0.73 , 0.744, 0.731, 0.775, 0.768, 0.775, 0.741, 0.744, 0.759,\n",
       "        0.74 , 0.726, 0.725, 0.728, 0.724, 0.729, 0.774, 0.769, 0.77 ,\n",
       "        0.749, 0.771, 0.752, 0.736, 0.728, 0.732, 0.73 , 0.723, 0.721,\n",
       "        0.787, 0.777, 0.775, 0.747, 0.746, 0.771, 0.762, 0.739, 0.758,\n",
       "        0.756, 0.729, 0.736, 0.763, 0.752, 0.774, 0.726, 0.754, 0.762,\n",
       "        0.729, 0.732, 0.743, 0.725, 0.712, 0.743, 0.762, 0.769, 0.781,\n",
       "        0.761, 0.735, 0.734, 0.743, 0.747, 0.719, 0.747, 0.74 , 0.7  ,\n",
       "        0.77 , 0.763, 0.769, 0.744, 0.754, 0.761, 0.744, 0.731, 0.736,\n",
       "        0.735, 0.707, 0.734, 0.777, 0.761, 0.773, 0.744, 0.753, 0.757,\n",
       "        0.723, 0.735, 0.749, 0.736, 0.731, 0.736, 0.784, 0.506, 0.783,\n",
       "        0.777, 0.386, 0.776, 0.789, 0.472, 0.784, 0.78 , 0.492, 0.781,\n",
       "        0.787, 0.667, 0.785, 0.778, 0.494, 0.774, 0.779, 0.495, 0.776,\n",
       "        0.785, 0.494, 0.781, 0.779, 0.506, 0.788, 0.778, 0.494, 0.783,\n",
       "        0.784, 0.621, 0.778, 0.784, 0.565, 0.777, 0.787, 0.612, 0.776,\n",
       "        0.777, 0.494, 0.776, 0.788, 0.494, 0.779, 0.781, 0.529, 0.782,\n",
       "        0.78 , 0.54 , 0.774, 0.776, 0.484, 0.782, 0.777, 0.484, 0.79 ,\n",
       "        0.781, 0.577, 0.779, 0.784, 0.532, 0.778, 0.785, 0.694, 0.786,\n",
       "        0.78 , 0.606, 0.782, 0.785, 0.666, 0.783, 0.777, 0.706, 0.766,\n",
       "        0.786, 0.605, 0.783, 0.779, 0.587, 0.781, 0.785, 0.52 , 0.783,\n",
       "        0.788, 0.529, 0.771, 0.777, 0.539, 0.788, 0.772, 0.441, 0.784,\n",
       "        0.782, 0.589, 0.773, 0.782, 0.784, 0.79 , 0.785, 0.785, 0.786,\n",
       "        0.787, 0.784, 0.781, 0.786, 0.785, 0.786, 0.784, 0.784, 0.783,\n",
       "        0.788, 0.784, 0.784, 0.785, 0.784, 0.783, 0.784, 0.783, 0.785,\n",
       "        0.785, 0.782, 0.786, 0.783, 0.782, 0.785, 0.787, 0.787, 0.784,\n",
       "        0.783, 0.785, 0.783, 0.785, 0.784, 0.786, 0.784, 0.784, 0.785,\n",
       "        0.785, 0.785, 0.784, 0.784, 0.785, 0.785, 0.784, 0.773, 0.78 ,\n",
       "        0.779, 0.78 , 0.787, 0.772, 0.777, 0.773, 0.769, 0.775, 0.781,\n",
       "        0.786, 0.789, 0.778, 0.779, 0.783, 0.781, 0.776, 0.768, 0.778,\n",
       "        0.76 , 0.775, 0.774, 0.786, 0.786, 0.784, 0.78 , 0.789, 0.782,\n",
       "        0.772, 0.766, 0.775, 0.772, 0.77 , 0.774, 0.787, 0.784, 0.791,\n",
       "        0.789, 0.784, 0.781, 0.779, 0.777, 0.775, 0.779, 0.785, 0.779]),\n",
       " 'mean_test_f1_micro': array([0.7566, 0.773 , 0.7658, 0.7594, 0.7604, 0.7646, 0.7456, 0.7452,\n",
       "        0.7382, 0.7284, 0.7414, 0.7364, 0.7702, 0.763 , 0.7686, 0.7464,\n",
       "        0.7516, 0.7568, 0.7434, 0.7458, 0.739 , 0.7342, 0.7356, 0.7374,\n",
       "        0.7704, 0.77  , 0.7676, 0.7532, 0.762 , 0.758 , 0.7502, 0.744 ,\n",
       "        0.739 , 0.7426, 0.738 , 0.7346, 0.7778, 0.772 , 0.7696, 0.761 ,\n",
       "        0.763 , 0.7658, 0.7504, 0.7498, 0.7594, 0.7498, 0.7482, 0.7482,\n",
       "        0.7644, 0.766 , 0.774 , 0.7446, 0.7566, 0.7622, 0.7438, 0.7406,\n",
       "        0.7486, 0.7284, 0.7272, 0.7458, 0.768 , 0.7672, 0.7686, 0.7496,\n",
       "        0.75  , 0.7556, 0.7542, 0.7484, 0.7364, 0.7336, 0.7366, 0.7266,\n",
       "        0.7756, 0.7658, 0.7686, 0.7506, 0.756 , 0.7612, 0.7508, 0.7414,\n",
       "        0.7496, 0.7378, 0.7292, 0.7384, 0.7784, 0.767 , 0.7712, 0.7604,\n",
       "        0.7612, 0.7526, 0.7518, 0.7456, 0.754 , 0.7362, 0.7368, 0.7466,\n",
       "        0.7748, 0.5462, 0.7764, 0.7774, 0.5024, 0.7764, 0.7774, 0.515 ,\n",
       "        0.7742, 0.7736, 0.4962, 0.777 , 0.7766, 0.5386, 0.7774, 0.7768,\n",
       "        0.552 , 0.7744, 0.7766, 0.5072, 0.7778, 0.779 , 0.515 , 0.7774,\n",
       "        0.7748, 0.4682, 0.7784, 0.7778, 0.5308, 0.776 , 0.776 , 0.5268,\n",
       "        0.7764, 0.778 , 0.5368, 0.776 , 0.776 , 0.531 , 0.7768, 0.7762,\n",
       "        0.4952, 0.7756, 0.78  , 0.478 , 0.7782, 0.7782, 0.541 , 0.7782,\n",
       "        0.7754, 0.5986, 0.7752, 0.7778, 0.5952, 0.778 , 0.773 , 0.5828,\n",
       "        0.7788, 0.7734, 0.6306, 0.774 , 0.7732, 0.5482, 0.7738, 0.7728,\n",
       "        0.6212, 0.7762, 0.772 , 0.5972, 0.7752, 0.777 , 0.6866, 0.7768,\n",
       "        0.7708, 0.5098, 0.7702, 0.7748, 0.5496, 0.7762, 0.7736, 0.6172,\n",
       "        0.7724, 0.7774, 0.6458, 0.7734, 0.7746, 0.6094, 0.7718, 0.7764,\n",
       "        0.5206, 0.778 , 0.774 , 0.5842, 0.777 , 0.7776, 0.5142, 0.7728,\n",
       "        0.7772, 0.7782, 0.7798, 0.7782, 0.7782, 0.7788, 0.7782, 0.778 ,\n",
       "        0.7772, 0.7792, 0.7774, 0.7786, 0.7768, 0.7768, 0.7782, 0.779 ,\n",
       "        0.7776, 0.778 , 0.7766, 0.778 , 0.778 , 0.778 , 0.7774, 0.7794,\n",
       "        0.7812, 0.7768, 0.7788, 0.7776, 0.777 , 0.7784, 0.7782, 0.7784,\n",
       "        0.778 , 0.7778, 0.7782, 0.7766, 0.7786, 0.7778, 0.7772, 0.7782,\n",
       "        0.7776, 0.779 , 0.7782, 0.7784, 0.778 , 0.778 , 0.777 , 0.778 ,\n",
       "        0.7784, 0.7772, 0.7766, 0.7764, 0.7762, 0.7762, 0.7746, 0.7762,\n",
       "        0.7716, 0.768 , 0.7732, 0.7766, 0.7784, 0.777 , 0.7754, 0.778 ,\n",
       "        0.7772, 0.777 , 0.772 , 0.7728, 0.7732, 0.768 , 0.7712, 0.7682,\n",
       "        0.7794, 0.7774, 0.7762, 0.7744, 0.7764, 0.7788, 0.7736, 0.7744,\n",
       "        0.7748, 0.773 , 0.7718, 0.7716, 0.7774, 0.776 , 0.777 , 0.7788,\n",
       "        0.7784, 0.776 , 0.7746, 0.779 , 0.777 , 0.7764, 0.7774, 0.7732]),\n",
       " 'std_test_f1_micro': array([0.01770424, 0.01285302, 0.01220492, 0.01034601, 0.00904655,\n",
       "        0.01567929, 0.01177455, 0.00904212, 0.01756588, 0.0119432 ,\n",
       "        0.00906863, 0.00763151, 0.01083328, 0.01178134, 0.01207642,\n",
       "        0.01109234, 0.005004  , 0.0103034 , 0.00770973, 0.01126765,\n",
       "        0.01285302, 0.00813388, 0.01032666, 0.01290891, 0.01422111,\n",
       "        0.01392839, 0.00997196, 0.00448999, 0.00887694, 0.01484587,\n",
       "        0.01649727, 0.00931665, 0.0086487 , 0.01217539, 0.0128841 ,\n",
       "        0.01022937, 0.01586695, 0.01176435, 0.01217539, 0.0090111 ,\n",
       "        0.01442221, 0.01014692, 0.00976934, 0.00730479, 0.00531413,\n",
       "        0.0095373 , 0.01267123, 0.00773046, 0.00436348, 0.01643168,\n",
       "        0.01245793, 0.0108185 , 0.0063435 , 0.00719444, 0.01097998,\n",
       "        0.01782807, 0.01290891, 0.01137717, 0.01293677, 0.01337759,\n",
       "        0.00718331, 0.00835225, 0.00749933, 0.00728286, 0.0121326 ,\n",
       "        0.01222457, 0.01202331, 0.00640625, 0.01352923, 0.00889044,\n",
       "        0.01261111, 0.01865047, 0.00960417, 0.00744043, 0.01125344,\n",
       "        0.0102098 , 0.00995992, 0.00231517, 0.01097998, 0.01051856,\n",
       "        0.00989141, 0.01016661, 0.01568949, 0.00553534, 0.00668132,\n",
       "        0.00384708, 0.01066583, 0.01275304, 0.00884081, 0.01700118,\n",
       "        0.01566397, 0.0108922 , 0.00946573, 0.00713863, 0.00800999,\n",
       "        0.0112712 , 0.01177115, 0.10630974, 0.00900222, 0.00773563,\n",
       "        0.06571027, 0.00840476, 0.01163787, 0.03839792, 0.01165161,\n",
       "        0.00976934, 0.00507543, 0.0083666 , 0.00868562, 0.06667713,\n",
       "        0.00976934, 0.00735935, 0.06036224, 0.00697424, 0.00796492,\n",
       "        0.06842339, 0.01016661, 0.01011929, 0.03428119, 0.00958332,\n",
       "        0.01170299, 0.05540541, 0.0095205 , 0.01032279, 0.06131688,\n",
       "        0.01078888, 0.01178134, 0.0479975 , 0.00741889, 0.00931665,\n",
       "        0.02899931, 0.01060189, 0.01056409, 0.04178995, 0.01146124,\n",
       "        0.01038075, 0.00248193, 0.01170641, 0.01136662, 0.07986989,\n",
       "        0.00847113, 0.00945304, 0.05075825, 0.01083328, 0.00875443,\n",
       "        0.04043563, 0.01538051, 0.00863481, 0.09059007, 0.00981835,\n",
       "        0.01048809, 0.07829534, 0.01144378, 0.01068831, 0.04562718,\n",
       "        0.01331165, 0.01299846, 0.04388804, 0.01158275, 0.01097998,\n",
       "        0.0588809 , 0.01147868, 0.00993982, 0.04970875, 0.01222129,\n",
       "        0.01256981, 0.03132156, 0.01312098, 0.01406272, 0.12844828,\n",
       "        0.0113031 , 0.01285924, 0.10266762, 0.01604244, 0.0074458 ,\n",
       "        0.02270154, 0.01332066, 0.01481351, 0.06766801, 0.01654811,\n",
       "        0.01019019, 0.09042699, 0.01137365, 0.01264278, 0.02598923,\n",
       "        0.01516575, 0.00829458, 0.09222885, 0.01063955, 0.01028786,\n",
       "        0.12042159, 0.01064707, 0.01490503, 0.01442775, 0.0156512 ,\n",
       "        0.01515784, 0.01483779, 0.01572768, 0.01633891, 0.01623576,\n",
       "        0.0156512 , 0.01471598, 0.0153961 , 0.01625546, 0.01452446,\n",
       "        0.01718604, 0.01624069, 0.01509967, 0.01369087, 0.01469694,\n",
       "        0.01508111, 0.01507315, 0.01581139, 0.0154013 , 0.01438888,\n",
       "        0.01478648, 0.01330263, 0.0147973 , 0.01310572, 0.01443052,\n",
       "        0.01492649, 0.01379275, 0.01503862, 0.01565375, 0.01688787,\n",
       "        0.01468877, 0.01543243, 0.01431922, 0.01501466, 0.0136587 ,\n",
       "        0.01301384, 0.01411949, 0.01473228, 0.01447757, 0.01485126,\n",
       "        0.01641463, 0.01479189, 0.0151921 , 0.01596246, 0.01389964,\n",
       "        0.01204326, 0.01340746, 0.01197664, 0.0102684 , 0.01543243,\n",
       "        0.0136147 , 0.01297074, 0.00798499, 0.00900222, 0.00985901,\n",
       "        0.01028397, 0.01066958, 0.01363231, 0.01529706, 0.01312402,\n",
       "        0.01412799, 0.01642437, 0.01143678, 0.00969536, 0.00943186,\n",
       "        0.01435827, 0.010469  , 0.00919565, 0.00886341, 0.01202664,\n",
       "        0.01349963, 0.01489161, 0.01395134, 0.01272164, 0.01230285,\n",
       "        0.00991161, 0.01013114, 0.01337759, 0.00920869, 0.00746726,\n",
       "        0.00902441, 0.01275304, 0.01473771, 0.01658915, 0.01133843,\n",
       "        0.01336563, 0.01145426, 0.00924338, 0.01308434, 0.00954987,\n",
       "        0.01275304, 0.01078147, 0.01165161]),\n",
       " 'rank_test_f1_micro': array([198, 147, 180, 194, 192, 183, 225, 227, 239, 253, 233, 245, 165,\n",
       "        186, 170, 222, 207, 197, 231, 223, 237, 250, 248, 242, 164, 167,\n",
       "        176, 204, 188, 196, 211, 229, 236, 232, 240, 249,  54, 154, 168,\n",
       "        191, 185, 180, 210, 214, 195, 213, 219, 220, 184, 179, 134, 228,\n",
       "        199, 187, 230, 235, 217, 253, 255, 223, 173, 177, 170, 215, 212,\n",
       "        201, 202, 218, 246, 251, 244, 256, 117, 180, 169, 209, 200, 189,\n",
       "        208, 233, 215, 241, 252, 238,  18, 178, 162, 192, 189, 205, 206,\n",
       "        225, 203, 247, 243, 221, 123, 271,  98,  63, 284,  98,  63, 280,\n",
       "        133, 138, 285,  77,  93, 273,  61,  89, 268, 131,  93, 283,  51,\n",
       "          7, 279,  63, 125, 288,  18,  54, 276, 111, 111, 277,  98,  42,\n",
       "        274, 111, 111, 275,  85, 105, 286, 117,   2, 287,  29,  26, 272,\n",
       "         29, 120, 263, 121,  51, 265,  42, 147, 267,  12, 141, 259, 134,\n",
       "        143, 270, 137, 150, 260, 105, 154, 264, 121,  77, 257,  89, 163,\n",
       "        282, 165, 123, 269, 105, 138, 261, 153,  63, 258, 141, 127, 262,\n",
       "        157,  98, 278,  38, 134, 266,  77,  60, 281, 151,  71,  26,   3,\n",
       "         29,  29,  12,  29,  38,  71,   6,  63,  16,  85,  85,  29,   7,\n",
       "         57,  42,  91,  42,  42,  42,  63,   4,   1,  85,  15,  58,  81,\n",
       "         18,  29,  25,  38,  54,  26,  93,  16,  51,  71,  29,  58,   7,\n",
       "         29,  18,  42,  38,  81,  42,  18,  71,  93,  98, 105, 105, 127,\n",
       "        105, 160, 173, 143,  91,  24,  77, 119,  50,  71,  76, 154, 151,\n",
       "        143, 173, 161, 172,   4,  63, 104, 131,  98,  12, 138, 130, 125,\n",
       "        147, 158, 159,  70, 111,  81,  11,  18, 111, 127,  10,  81,  97,\n",
       "         61, 143]),\n",
       " 'split0_test_precision_micro': array([0.763, 0.767, 0.76 , 0.762, 0.754, 0.755, 0.731, 0.753, 0.715,\n",
       "        0.715, 0.749, 0.741, 0.766, 0.765, 0.76 , 0.741, 0.748, 0.748,\n",
       "        0.743, 0.755, 0.722, 0.728, 0.755, 0.733, 0.765, 0.78 , 0.775,\n",
       "        0.761, 0.761, 0.772, 0.746, 0.749, 0.741, 0.747, 0.762, 0.743,\n",
       "        0.775, 0.773, 0.766, 0.763, 0.765, 0.779, 0.744, 0.755, 0.757,\n",
       "        0.745, 0.747, 0.748, 0.765, 0.77 , 0.782, 0.743, 0.762, 0.759,\n",
       "        0.734, 0.717, 0.742, 0.721, 0.714, 0.725, 0.775, 0.774, 0.76 ,\n",
       "        0.749, 0.757, 0.758, 0.749, 0.744, 0.727, 0.733, 0.746, 0.722,\n",
       "        0.768, 0.758, 0.763, 0.736, 0.748, 0.762, 0.753, 0.754, 0.745,\n",
       "        0.721, 0.734, 0.746, 0.771, 0.77 , 0.78 , 0.766, 0.754, 0.741,\n",
       "        0.748, 0.752, 0.741, 0.726, 0.751, 0.752, 0.77 , 0.758, 0.774,\n",
       "        0.777, 0.505, 0.771, 0.766, 0.495, 0.766, 0.766, 0.496, 0.769,\n",
       "        0.773, 0.48 , 0.767, 0.775, 0.646, 0.775, 0.775, 0.519, 0.774,\n",
       "        0.775, 0.498, 0.771, 0.769, 0.495, 0.771, 0.776, 0.495, 0.768,\n",
       "        0.767, 0.519, 0.77 , 0.772, 0.505, 0.772, 0.77 , 0.495, 0.771,\n",
       "        0.772, 0.495, 0.77 , 0.772, 0.328, 0.77 , 0.776, 0.495, 0.771,\n",
       "        0.772, 0.602, 0.769, 0.776, 0.661, 0.776, 0.76 , 0.495, 0.773,\n",
       "        0.766, 0.581, 0.766, 0.774, 0.574, 0.773, 0.769, 0.519, 0.766,\n",
       "        0.767, 0.548, 0.77 , 0.768, 0.722, 0.773, 0.763, 0.487, 0.765,\n",
       "        0.764, 0.592, 0.774, 0.776, 0.614, 0.771, 0.767, 0.649, 0.763,\n",
       "        0.77 , 0.519, 0.77 , 0.771, 0.519, 0.777, 0.779, 0.692, 0.768,\n",
       "        0.77 , 0.318, 0.771, 0.775, 0.776, 0.774, 0.772, 0.774, 0.776,\n",
       "        0.773, 0.774, 0.773, 0.776, 0.771, 0.771, 0.772, 0.772, 0.776,\n",
       "        0.775, 0.774, 0.775, 0.773, 0.772, 0.774, 0.771, 0.775, 0.776,\n",
       "        0.779, 0.773, 0.773, 0.774, 0.772, 0.776, 0.772, 0.77 , 0.771,\n",
       "        0.773, 0.774, 0.772, 0.774, 0.774, 0.773, 0.773, 0.772, 0.775,\n",
       "        0.773, 0.775, 0.775, 0.774, 0.773, 0.772, 0.778, 0.781, 0.782,\n",
       "        0.774, 0.777, 0.78 , 0.775, 0.778, 0.771, 0.777, 0.775, 0.782,\n",
       "        0.78 , 0.772, 0.773, 0.782, 0.78 , 0.78 , 0.771, 0.774, 0.771,\n",
       "        0.769, 0.773, 0.766, 0.774, 0.774, 0.775, 0.774, 0.773, 0.776,\n",
       "        0.778, 0.781, 0.772, 0.772, 0.775, 0.773, 0.775, 0.776, 0.767,\n",
       "        0.778, 0.774, 0.781, 0.774, 0.783, 0.777, 0.777, 0.78 , 0.781]),\n",
       " 'split1_test_precision_micro': array([0.76 , 0.778, 0.77 , 0.772, 0.769, 0.745, 0.74 , 0.731, 0.746,\n",
       "        0.716, 0.724, 0.733, 0.771, 0.76 , 0.769, 0.763, 0.753, 0.762,\n",
       "        0.751, 0.747, 0.747, 0.737, 0.732, 0.724, 0.771, 0.768, 0.774,\n",
       "        0.755, 0.772, 0.736, 0.739, 0.748, 0.753, 0.733, 0.735, 0.727,\n",
       "        0.766, 0.768, 0.766, 0.768, 0.752, 0.75 , 0.735, 0.752, 0.753,\n",
       "        0.738, 0.744, 0.753, 0.767, 0.769, 0.766, 0.75 , 0.747, 0.762,\n",
       "        0.758, 0.751, 0.731, 0.713, 0.728, 0.746, 0.765, 0.769, 0.764,\n",
       "        0.75 , 0.749, 0.768, 0.76 , 0.744, 0.732, 0.735, 0.717, 0.736,\n",
       "        0.781, 0.764, 0.776, 0.763, 0.758, 0.762, 0.757, 0.727, 0.745,\n",
       "        0.751, 0.717, 0.731, 0.777, 0.769, 0.767, 0.76 , 0.772, 0.743,\n",
       "        0.761, 0.739, 0.751, 0.745, 0.728, 0.732, 0.78 , 0.496, 0.78 ,\n",
       "        0.775, 0.587, 0.778, 0.783, 0.506, 0.775, 0.777, 0.494, 0.778,\n",
       "        0.778, 0.525, 0.785, 0.781, 0.508, 0.775, 0.783, 0.616, 0.782,\n",
       "        0.785, 0.494, 0.781, 0.783, 0.488, 0.781, 0.784, 0.506, 0.782,\n",
       "        0.778, 0.494, 0.782, 0.782, 0.535, 0.787, 0.775, 0.506, 0.787,\n",
       "        0.784, 0.493, 0.784, 0.782, 0.568, 0.783, 0.781, 0.618, 0.786,\n",
       "        0.785, 0.6  , 0.774, 0.783, 0.495, 0.78 , 0.776, 0.647, 0.786,\n",
       "        0.78 , 0.662, 0.786, 0.78 , 0.581, 0.781, 0.774, 0.632, 0.783,\n",
       "        0.776, 0.533, 0.781, 0.784, 0.727, 0.785, 0.784, 0.572, 0.777,\n",
       "        0.783, 0.467, 0.779, 0.781, 0.657, 0.782, 0.786, 0.655, 0.777,\n",
       "        0.776, 0.648, 0.777, 0.779, 0.509, 0.781, 0.777, 0.547, 0.783,\n",
       "        0.783, 0.59 , 0.784, 0.785, 0.784, 0.787, 0.786, 0.784, 0.786,\n",
       "        0.786, 0.786, 0.785, 0.784, 0.784, 0.787, 0.784, 0.786, 0.785,\n",
       "        0.784, 0.784, 0.787, 0.784, 0.786, 0.786, 0.786, 0.785, 0.786,\n",
       "        0.789, 0.787, 0.786, 0.784, 0.785, 0.785, 0.785, 0.787, 0.788,\n",
       "        0.786, 0.784, 0.785, 0.786, 0.782, 0.782, 0.784, 0.785, 0.787,\n",
       "        0.786, 0.785, 0.786, 0.785, 0.785, 0.785, 0.777, 0.784, 0.775,\n",
       "        0.775, 0.778, 0.77 , 0.78 , 0.775, 0.774, 0.76 , 0.777, 0.771,\n",
       "        0.775, 0.778, 0.783, 0.782, 0.779, 0.775, 0.771, 0.778, 0.777,\n",
       "        0.767, 0.768, 0.762, 0.784, 0.779, 0.784, 0.775, 0.778, 0.782,\n",
       "        0.776, 0.773, 0.777, 0.774, 0.771, 0.769, 0.778, 0.778, 0.779,\n",
       "        0.78 , 0.779, 0.777, 0.776, 0.787, 0.777, 0.774, 0.773, 0.766]),\n",
       " 'split2_test_precision_micro': array([0.784, 0.795, 0.786, 0.768, 0.769, 0.786, 0.765, 0.753, 0.767,\n",
       "        0.747, 0.742, 0.749, 0.786, 0.779, 0.787, 0.755, 0.758, 0.772,\n",
       "        0.752, 0.758, 0.753, 0.749, 0.733, 0.761, 0.793, 0.787, 0.771,\n",
       "        0.752, 0.758, 0.777, 0.782, 0.755, 0.741, 0.764, 0.734, 0.749,\n",
       "        0.803, 0.789, 0.789, 0.772, 0.788, 0.77 , 0.753, 0.759, 0.769,\n",
       "        0.765, 0.768, 0.759, 0.77 , 0.793, 0.792, 0.759, 0.755, 0.775,\n",
       "        0.753, 0.769, 0.762, 0.741, 0.736, 0.767, 0.778, 0.773, 0.773,\n",
       "        0.738, 0.769, 0.766, 0.775, 0.761, 0.749, 0.734, 0.752, 0.756,\n",
       "        0.792, 0.78 , 0.784, 0.761, 0.774, 0.757, 0.766, 0.749, 0.76 ,\n",
       "        0.737, 0.752, 0.738, 0.791, 0.771, 0.783, 0.781, 0.772, 0.784,\n",
       "        0.767, 0.764, 0.761, 0.743, 0.735, 0.763, 0.786, 0.477, 0.785,\n",
       "        0.791, 0.502, 0.791, 0.788, 0.586, 0.789, 0.786, 0.506, 0.79 ,\n",
       "        0.783, 0.494, 0.786, 0.786, 0.511, 0.785, 0.784, 0.401, 0.794,\n",
       "        0.789, 0.506, 0.791, 0.788, 0.358, 0.788, 0.791, 0.506, 0.788,\n",
       "        0.792, 0.494, 0.786, 0.789, 0.505, 0.786, 0.788, 0.526, 0.791,\n",
       "        0.789, 0.494, 0.791, 0.795, 0.494, 0.791, 0.791, 0.579, 0.791,\n",
       "        0.78 , 0.585, 0.803, 0.79 , 0.625, 0.791, 0.789, 0.616, 0.786,\n",
       "        0.784, 0.694, 0.787, 0.78 , 0.469, 0.785, 0.782, 0.656, 0.787,\n",
       "        0.782, 0.638, 0.789, 0.791, 0.665, 0.79 , 0.783, 0.469, 0.788,\n",
       "        0.786, 0.685, 0.797, 0.772, 0.619, 0.781, 0.795, 0.719, 0.796,\n",
       "        0.781, 0.765, 0.788, 0.797, 0.48 , 0.794, 0.783, 0.677, 0.789,\n",
       "        0.791, 0.641, 0.782, 0.794, 0.795, 0.796, 0.796, 0.796, 0.796,\n",
       "        0.796, 0.797, 0.797, 0.797, 0.796, 0.798, 0.793, 0.796, 0.798,\n",
       "        0.796, 0.793, 0.793, 0.792, 0.796, 0.797, 0.797, 0.793, 0.797,\n",
       "        0.796, 0.792, 0.793, 0.795, 0.795, 0.793, 0.795, 0.796, 0.798,\n",
       "        0.795, 0.797, 0.792, 0.796, 0.795, 0.791, 0.796, 0.795, 0.795,\n",
       "        0.795, 0.798, 0.794, 0.796, 0.794, 0.794, 0.795, 0.794, 0.791,\n",
       "        0.793, 0.797, 0.791, 0.793, 0.788, 0.784, 0.78 , 0.785, 0.79 ,\n",
       "        0.796, 0.795, 0.791, 0.795, 0.797, 0.792, 0.786, 0.786, 0.792,\n",
       "        0.787, 0.784, 0.782, 0.794, 0.794, 0.79 , 0.793, 0.788, 0.796,\n",
       "        0.786, 0.79 , 0.796, 0.788, 0.783, 0.785, 0.792, 0.793, 0.797,\n",
       "        0.789, 0.798, 0.787, 0.786, 0.793, 0.793, 0.796, 0.79 , 0.786]),\n",
       " 'split3_test_precision_micro': array([0.732, 0.757, 0.749, 0.745, 0.746, 0.757, 0.74 , 0.751, 0.728,\n",
       "        0.734, 0.748, 0.728, 0.753, 0.743, 0.752, 0.732, 0.755, 0.743,\n",
       "        0.731, 0.743, 0.748, 0.729, 0.734, 0.74 , 0.749, 0.746, 0.748,\n",
       "        0.749, 0.748, 0.753, 0.748, 0.74 , 0.728, 0.739, 0.736, 0.733,\n",
       "        0.758, 0.753, 0.752, 0.755, 0.764, 0.759, 0.758, 0.744, 0.76 ,\n",
       "        0.745, 0.753, 0.745, 0.757, 0.746, 0.756, 0.745, 0.765, 0.753,\n",
       "        0.745, 0.734, 0.765, 0.742, 0.746, 0.748, 0.76 , 0.751, 0.765,\n",
       "        0.75 , 0.74 , 0.752, 0.744, 0.746, 0.755, 0.719, 0.728, 0.719,\n",
       "        0.767, 0.764, 0.751, 0.749, 0.746, 0.764, 0.734, 0.746, 0.762,\n",
       "        0.745, 0.736, 0.743, 0.776, 0.764, 0.753, 0.751, 0.755, 0.738,\n",
       "        0.76 , 0.738, 0.768, 0.731, 0.739, 0.75 , 0.754, 0.494, 0.76 ,\n",
       "        0.767, 0.532, 0.766, 0.761, 0.516, 0.757, 0.759, 0.493, 0.767,\n",
       "        0.762, 0.527, 0.764, 0.764, 0.601, 0.763, 0.762, 0.505, 0.763,\n",
       "        0.761, 0.583, 0.763, 0.755, 0.494, 0.764, 0.76 , 0.653, 0.759,\n",
       "        0.759, 0.506, 0.766, 0.763, 0.574, 0.758, 0.76 , 0.516, 0.759,\n",
       "        0.759, 0.5  , 0.757, 0.763, 0.506, 0.768, 0.762, 0.484, 0.761,\n",
       "        0.76 , 0.666, 0.756, 0.764, 0.711, 0.761, 0.763, 0.672, 0.759,\n",
       "        0.756, 0.639, 0.752, 0.748, 0.585, 0.752, 0.754, 0.605, 0.759,\n",
       "        0.755, 0.661, 0.754, 0.757, 0.653, 0.753, 0.747, 0.315, 0.755,\n",
       "        0.755, 0.399, 0.748, 0.76 , 0.609, 0.747, 0.754, 0.686, 0.748,\n",
       "        0.758, 0.586, 0.753, 0.758, 0.556, 0.75 , 0.759, 0.564, 0.761,\n",
       "        0.762, 0.433, 0.754, 0.75 , 0.752, 0.752, 0.752, 0.752, 0.75 ,\n",
       "        0.749, 0.749, 0.75 , 0.753, 0.751, 0.751, 0.751, 0.746, 0.749,\n",
       "        0.752, 0.753, 0.751, 0.749, 0.752, 0.75 , 0.752, 0.751, 0.753,\n",
       "        0.757, 0.75 , 0.756, 0.752, 0.751, 0.753, 0.752, 0.752, 0.749,\n",
       "        0.752, 0.751, 0.751, 0.752, 0.754, 0.754, 0.754, 0.752, 0.753,\n",
       "        0.752, 0.749, 0.751, 0.751, 0.748, 0.754, 0.758, 0.754, 0.755,\n",
       "        0.761, 0.749, 0.753, 0.753, 0.763, 0.756, 0.754, 0.754, 0.759,\n",
       "        0.755, 0.751, 0.752, 0.752, 0.747, 0.757, 0.756, 0.758, 0.748,\n",
       "        0.757, 0.756, 0.757, 0.759, 0.754, 0.748, 0.75 , 0.754, 0.758,\n",
       "        0.756, 0.762, 0.754, 0.759, 0.76 , 0.757, 0.755, 0.749, 0.751,\n",
       "        0.758, 0.757, 0.754, 0.758, 0.755, 0.763, 0.756, 0.759, 0.754]),\n",
       " 'split4_test_precision_micro': array([0.744, 0.768, 0.764, 0.75 , 0.764, 0.78 , 0.752, 0.738, 0.735,\n",
       "        0.73 , 0.744, 0.731, 0.775, 0.768, 0.775, 0.741, 0.744, 0.759,\n",
       "        0.74 , 0.726, 0.725, 0.728, 0.724, 0.729, 0.774, 0.769, 0.77 ,\n",
       "        0.749, 0.771, 0.752, 0.736, 0.728, 0.732, 0.73 , 0.723, 0.721,\n",
       "        0.787, 0.777, 0.775, 0.747, 0.746, 0.771, 0.762, 0.739, 0.758,\n",
       "        0.756, 0.729, 0.736, 0.763, 0.752, 0.774, 0.726, 0.754, 0.762,\n",
       "        0.729, 0.732, 0.743, 0.725, 0.712, 0.743, 0.762, 0.769, 0.781,\n",
       "        0.761, 0.735, 0.734, 0.743, 0.747, 0.719, 0.747, 0.74 , 0.7  ,\n",
       "        0.77 , 0.763, 0.769, 0.744, 0.754, 0.761, 0.744, 0.731, 0.736,\n",
       "        0.735, 0.707, 0.734, 0.777, 0.761, 0.773, 0.744, 0.753, 0.757,\n",
       "        0.723, 0.735, 0.749, 0.736, 0.731, 0.736, 0.784, 0.506, 0.783,\n",
       "        0.777, 0.386, 0.776, 0.789, 0.472, 0.784, 0.78 , 0.492, 0.781,\n",
       "        0.787, 0.667, 0.785, 0.778, 0.494, 0.774, 0.779, 0.495, 0.776,\n",
       "        0.785, 0.494, 0.781, 0.779, 0.506, 0.788, 0.778, 0.494, 0.783,\n",
       "        0.784, 0.621, 0.778, 0.784, 0.565, 0.777, 0.787, 0.612, 0.776,\n",
       "        0.777, 0.494, 0.776, 0.788, 0.494, 0.779, 0.781, 0.529, 0.782,\n",
       "        0.78 , 0.54 , 0.774, 0.776, 0.484, 0.782, 0.777, 0.484, 0.79 ,\n",
       "        0.781, 0.577, 0.779, 0.784, 0.532, 0.778, 0.785, 0.694, 0.786,\n",
       "        0.78 , 0.606, 0.782, 0.785, 0.666, 0.783, 0.777, 0.706, 0.766,\n",
       "        0.786, 0.605, 0.783, 0.779, 0.587, 0.781, 0.785, 0.52 , 0.783,\n",
       "        0.788, 0.529, 0.771, 0.777, 0.539, 0.788, 0.772, 0.441, 0.784,\n",
       "        0.782, 0.589, 0.773, 0.782, 0.784, 0.79 , 0.785, 0.785, 0.786,\n",
       "        0.787, 0.784, 0.781, 0.786, 0.785, 0.786, 0.784, 0.784, 0.783,\n",
       "        0.788, 0.784, 0.784, 0.785, 0.784, 0.783, 0.784, 0.783, 0.785,\n",
       "        0.785, 0.782, 0.786, 0.783, 0.782, 0.785, 0.787, 0.787, 0.784,\n",
       "        0.783, 0.785, 0.783, 0.785, 0.784, 0.786, 0.784, 0.784, 0.785,\n",
       "        0.785, 0.785, 0.784, 0.784, 0.785, 0.785, 0.784, 0.773, 0.78 ,\n",
       "        0.779, 0.78 , 0.787, 0.772, 0.777, 0.773, 0.769, 0.775, 0.781,\n",
       "        0.786, 0.789, 0.778, 0.779, 0.783, 0.781, 0.776, 0.768, 0.778,\n",
       "        0.76 , 0.775, 0.774, 0.786, 0.786, 0.784, 0.78 , 0.789, 0.782,\n",
       "        0.772, 0.766, 0.775, 0.772, 0.77 , 0.774, 0.787, 0.784, 0.791,\n",
       "        0.789, 0.784, 0.781, 0.779, 0.777, 0.775, 0.779, 0.785, 0.779]),\n",
       " 'mean_test_precision_micro': array([0.7566, 0.773 , 0.7658, 0.7594, 0.7604, 0.7646, 0.7456, 0.7452,\n",
       "        0.7382, 0.7284, 0.7414, 0.7364, 0.7702, 0.763 , 0.7686, 0.7464,\n",
       "        0.7516, 0.7568, 0.7434, 0.7458, 0.739 , 0.7342, 0.7356, 0.7374,\n",
       "        0.7704, 0.77  , 0.7676, 0.7532, 0.762 , 0.758 , 0.7502, 0.744 ,\n",
       "        0.739 , 0.7426, 0.738 , 0.7346, 0.7778, 0.772 , 0.7696, 0.761 ,\n",
       "        0.763 , 0.7658, 0.7504, 0.7498, 0.7594, 0.7498, 0.7482, 0.7482,\n",
       "        0.7644, 0.766 , 0.774 , 0.7446, 0.7566, 0.7622, 0.7438, 0.7406,\n",
       "        0.7486, 0.7284, 0.7272, 0.7458, 0.768 , 0.7672, 0.7686, 0.7496,\n",
       "        0.75  , 0.7556, 0.7542, 0.7484, 0.7364, 0.7336, 0.7366, 0.7266,\n",
       "        0.7756, 0.7658, 0.7686, 0.7506, 0.756 , 0.7612, 0.7508, 0.7414,\n",
       "        0.7496, 0.7378, 0.7292, 0.7384, 0.7784, 0.767 , 0.7712, 0.7604,\n",
       "        0.7612, 0.7526, 0.7518, 0.7456, 0.754 , 0.7362, 0.7368, 0.7466,\n",
       "        0.7748, 0.5462, 0.7764, 0.7774, 0.5024, 0.7764, 0.7774, 0.515 ,\n",
       "        0.7742, 0.7736, 0.4962, 0.777 , 0.7766, 0.5386, 0.7774, 0.7768,\n",
       "        0.552 , 0.7744, 0.7766, 0.5072, 0.7778, 0.779 , 0.515 , 0.7774,\n",
       "        0.7748, 0.4682, 0.7784, 0.7778, 0.5308, 0.776 , 0.776 , 0.5268,\n",
       "        0.7764, 0.778 , 0.5368, 0.776 , 0.776 , 0.531 , 0.7768, 0.7762,\n",
       "        0.4952, 0.7756, 0.78  , 0.478 , 0.7782, 0.7782, 0.541 , 0.7782,\n",
       "        0.7754, 0.5986, 0.7752, 0.7778, 0.5952, 0.778 , 0.773 , 0.5828,\n",
       "        0.7788, 0.7734, 0.6306, 0.774 , 0.7732, 0.5482, 0.7738, 0.7728,\n",
       "        0.6212, 0.7762, 0.772 , 0.5972, 0.7752, 0.777 , 0.6866, 0.7768,\n",
       "        0.7708, 0.5098, 0.7702, 0.7748, 0.5496, 0.7762, 0.7736, 0.6172,\n",
       "        0.7724, 0.7774, 0.6458, 0.7734, 0.7746, 0.6094, 0.7718, 0.7764,\n",
       "        0.5206, 0.778 , 0.774 , 0.5842, 0.777 , 0.7776, 0.5142, 0.7728,\n",
       "        0.7772, 0.7782, 0.7798, 0.7782, 0.7782, 0.7788, 0.7782, 0.778 ,\n",
       "        0.7772, 0.7792, 0.7774, 0.7786, 0.7768, 0.7768, 0.7782, 0.779 ,\n",
       "        0.7776, 0.778 , 0.7766, 0.778 , 0.778 , 0.778 , 0.7774, 0.7794,\n",
       "        0.7812, 0.7768, 0.7788, 0.7776, 0.777 , 0.7784, 0.7782, 0.7784,\n",
       "        0.778 , 0.7778, 0.7782, 0.7766, 0.7786, 0.7778, 0.7772, 0.7782,\n",
       "        0.7776, 0.779 , 0.7782, 0.7784, 0.778 , 0.778 , 0.777 , 0.778 ,\n",
       "        0.7784, 0.7772, 0.7766, 0.7764, 0.7762, 0.7762, 0.7746, 0.7762,\n",
       "        0.7716, 0.768 , 0.7732, 0.7766, 0.7784, 0.777 , 0.7754, 0.778 ,\n",
       "        0.7772, 0.777 , 0.772 , 0.7728, 0.7732, 0.768 , 0.7712, 0.7682,\n",
       "        0.7794, 0.7774, 0.7762, 0.7744, 0.7764, 0.7788, 0.7736, 0.7744,\n",
       "        0.7748, 0.773 , 0.7718, 0.7716, 0.7774, 0.776 , 0.777 , 0.7788,\n",
       "        0.7784, 0.776 , 0.7746, 0.779 , 0.777 , 0.7764, 0.7774, 0.7732]),\n",
       " 'std_test_precision_micro': array([0.01770424, 0.01285302, 0.01220492, 0.01034601, 0.00904655,\n",
       "        0.01567929, 0.01177455, 0.00904212, 0.01756588, 0.0119432 ,\n",
       "        0.00906863, 0.00763151, 0.01083328, 0.01178134, 0.01207642,\n",
       "        0.01109234, 0.005004  , 0.0103034 , 0.00770973, 0.01126765,\n",
       "        0.01285302, 0.00813388, 0.01032666, 0.01290891, 0.01422111,\n",
       "        0.01392839, 0.00997196, 0.00448999, 0.00887694, 0.01484587,\n",
       "        0.01649727, 0.00931665, 0.0086487 , 0.01217539, 0.0128841 ,\n",
       "        0.01022937, 0.01586695, 0.01176435, 0.01217539, 0.0090111 ,\n",
       "        0.01442221, 0.01014692, 0.00976934, 0.00730479, 0.00531413,\n",
       "        0.0095373 , 0.01267123, 0.00773046, 0.00436348, 0.01643168,\n",
       "        0.01245793, 0.0108185 , 0.0063435 , 0.00719444, 0.01097998,\n",
       "        0.01782807, 0.01290891, 0.01137717, 0.01293677, 0.01337759,\n",
       "        0.00718331, 0.00835225, 0.00749933, 0.00728286, 0.0121326 ,\n",
       "        0.01222457, 0.01202331, 0.00640625, 0.01352923, 0.00889044,\n",
       "        0.01261111, 0.01865047, 0.00960417, 0.00744043, 0.01125344,\n",
       "        0.0102098 , 0.00995992, 0.00231517, 0.01097998, 0.01051856,\n",
       "        0.00989141, 0.01016661, 0.01568949, 0.00553534, 0.00668132,\n",
       "        0.00384708, 0.01066583, 0.01275304, 0.00884081, 0.01700118,\n",
       "        0.01566397, 0.0108922 , 0.00946573, 0.00713863, 0.00800999,\n",
       "        0.0112712 , 0.01177115, 0.10630974, 0.00900222, 0.00773563,\n",
       "        0.06571027, 0.00840476, 0.01163787, 0.03839792, 0.01165161,\n",
       "        0.00976934, 0.00507543, 0.0083666 , 0.00868562, 0.06667713,\n",
       "        0.00976934, 0.00735935, 0.06036224, 0.00697424, 0.00796492,\n",
       "        0.06842339, 0.01016661, 0.01011929, 0.03428119, 0.00958332,\n",
       "        0.01170299, 0.05540541, 0.0095205 , 0.01032279, 0.06131688,\n",
       "        0.01078888, 0.01178134, 0.0479975 , 0.00741889, 0.00931665,\n",
       "        0.02899931, 0.01060189, 0.01056409, 0.04178995, 0.01146124,\n",
       "        0.01038075, 0.00248193, 0.01170641, 0.01136662, 0.07986989,\n",
       "        0.00847113, 0.00945304, 0.05075825, 0.01083328, 0.00875443,\n",
       "        0.04043563, 0.01538051, 0.00863481, 0.09059007, 0.00981835,\n",
       "        0.01048809, 0.07829534, 0.01144378, 0.01068831, 0.04562718,\n",
       "        0.01331165, 0.01299846, 0.04388804, 0.01158275, 0.01097998,\n",
       "        0.0588809 , 0.01147868, 0.00993982, 0.04970875, 0.01222129,\n",
       "        0.01256981, 0.03132156, 0.01312098, 0.01406272, 0.12844828,\n",
       "        0.0113031 , 0.01285924, 0.10266762, 0.01604244, 0.0074458 ,\n",
       "        0.02270154, 0.01332066, 0.01481351, 0.06766801, 0.01654811,\n",
       "        0.01019019, 0.09042699, 0.01137365, 0.01264278, 0.02598923,\n",
       "        0.01516575, 0.00829458, 0.09222885, 0.01063955, 0.01028786,\n",
       "        0.12042159, 0.01064707, 0.01490503, 0.01442775, 0.0156512 ,\n",
       "        0.01515784, 0.01483779, 0.01572768, 0.01633891, 0.01623576,\n",
       "        0.0156512 , 0.01471598, 0.0153961 , 0.01625546, 0.01452446,\n",
       "        0.01718604, 0.01624069, 0.01509967, 0.01369087, 0.01469694,\n",
       "        0.01508111, 0.01507315, 0.01581139, 0.0154013 , 0.01438888,\n",
       "        0.01478648, 0.01330263, 0.0147973 , 0.01310572, 0.01443052,\n",
       "        0.01492649, 0.01379275, 0.01503862, 0.01565375, 0.01688787,\n",
       "        0.01468877, 0.01543243, 0.01431922, 0.01501466, 0.0136587 ,\n",
       "        0.01301384, 0.01411949, 0.01473228, 0.01447757, 0.01485126,\n",
       "        0.01641463, 0.01479189, 0.0151921 , 0.01596246, 0.01389964,\n",
       "        0.01204326, 0.01340746, 0.01197664, 0.0102684 , 0.01543243,\n",
       "        0.0136147 , 0.01297074, 0.00798499, 0.00900222, 0.00985901,\n",
       "        0.01028397, 0.01066958, 0.01363231, 0.01529706, 0.01312402,\n",
       "        0.01412799, 0.01642437, 0.01143678, 0.00969536, 0.00943186,\n",
       "        0.01435827, 0.010469  , 0.00919565, 0.00886341, 0.01202664,\n",
       "        0.01349963, 0.01489161, 0.01395134, 0.01272164, 0.01230285,\n",
       "        0.00991161, 0.01013114, 0.01337759, 0.00920869, 0.00746726,\n",
       "        0.00902441, 0.01275304, 0.01473771, 0.01658915, 0.01133843,\n",
       "        0.01336563, 0.01145426, 0.00924338, 0.01308434, 0.00954987,\n",
       "        0.01275304, 0.01078147, 0.01165161]),\n",
       " 'rank_test_precision_micro': array([198, 147, 180, 194, 192, 183, 225, 227, 239, 253, 233, 245, 165,\n",
       "        186, 171, 222, 207, 197, 231, 223, 237, 250, 248, 242, 164, 167,\n",
       "        176, 204, 188, 196, 211, 229, 236, 232, 240, 249,  54, 154, 168,\n",
       "        191, 185, 180, 210, 214, 195, 213, 219, 220, 184, 179, 134, 228,\n",
       "        199, 187, 230, 235, 217, 253, 255, 223, 173, 177, 169, 215, 212,\n",
       "        201, 202, 218, 245, 251, 244, 256, 117, 180, 169, 209, 200, 189,\n",
       "        208, 233, 215, 241, 252, 238,  18, 178, 161, 193, 189, 205, 206,\n",
       "        225, 203, 247, 243, 221, 123, 271,  99,  62, 284,  99,  62, 280,\n",
       "        133, 138, 285,  79,  92, 273,  61,  89, 268, 131,  92, 283,  51,\n",
       "          7, 279,  62, 125, 288,  18,  54, 276, 111, 111, 277,  99,  45,\n",
       "        274, 111, 111, 275,  85, 104, 286, 117,   2, 287,  27,  27, 272,\n",
       "         27, 120, 263, 121,  51, 265,  42, 147, 267,  12, 141, 259, 134,\n",
       "        146, 270, 137, 150, 260, 104, 154, 264, 121,  79, 257,  89, 163,\n",
       "        282, 165, 124, 269, 104, 140, 261, 153,  62, 258, 141, 127, 262,\n",
       "        157,  99, 278,  38, 134, 266,  76,  58, 281, 150,  71,  27,   3,\n",
       "         27,  27,  12,  26,  38,  71,   6,  62,  16,  85,  85,  27,   7,\n",
       "         58,  45,  91,  45,  42,  45,  62,   4,   1,  85,  11,  57,  82,\n",
       "         18,  27,  25,  38,  54,  27,  92,  16,  51,  71,  27,  58,   7,\n",
       "         27,  18,  45,  38,  76,  42,  18,  71,  92,  99, 104, 104, 127,\n",
       "        104, 159, 173, 143,  92,  24,  79, 119,  45,  71,  76, 154, 152,\n",
       "        143, 173, 161, 172,   4,  62, 104, 131,  97,  12, 138, 130, 125,\n",
       "        147, 157, 159,  70, 111,  82,  12,  18, 111, 127,  10,  82,  97,\n",
       "         62, 143]),\n",
       " 'split0_test_roc_auc_ovo': array([0.84638464, 0.85372937, 0.84906491, 0.83807981, 0.8290029 ,\n",
       "        0.8340394 , 0.8229503 , 0.82554655, 0.79535154, 0.78819882,\n",
       "        0.82093009, 0.8150135 , 0.85535754, 0.85759376, 0.84840084,\n",
       "        0.82790679, 0.82262226, 0.83071507, 0.81641364, 0.82993099,\n",
       "        0.80660466, 0.80748875, 0.81514551, 0.82140614, 0.84350835,\n",
       "        0.85253725, 0.84578458, 0.84086409, 0.84386039, 0.84214421,\n",
       "        0.81926993, 0.82257426, 0.81749775, 0.82185819, 0.83235124,\n",
       "        0.83329933, 0.85354535, 0.84570857, 0.85083708, 0.84442844,\n",
       "        0.84456046, 0.85120512, 0.82611461, 0.830047  , 0.82982298,\n",
       "        0.8270347 , 0.82492249, 0.82858286, 0.85317732, 0.85114911,\n",
       "        0.85632563, 0.82817082, 0.83170717, 0.8360196 , 0.81822182,\n",
       "        0.79510551, 0.81521352, 0.79648365, 0.79216722, 0.78669467,\n",
       "        0.85472947, 0.85677768, 0.85327333, 0.82947495, 0.83190719,\n",
       "        0.83107111, 0.82912691, 0.83075108, 0.79935994, 0.81150915,\n",
       "        0.81708571, 0.79479948, 0.85365337, 0.84993699, 0.85441344,\n",
       "        0.82571057, 0.8390239 , 0.84455246, 0.82244224, 0.81515752,\n",
       "        0.81971397, 0.7889509 , 0.81930993, 0.81240124, 0.85619762,\n",
       "        0.8560136 , 0.85751775, 0.84653665, 0.82919892, 0.82584658,\n",
       "        0.83211121, 0.83027103, 0.81919792, 0.80544054, 0.81882588,\n",
       "        0.81989399, 0.86279828, 0.84171217, 0.86184618, 0.8619542 ,\n",
       "        0.22543054, 0.86309831, 0.86162216, 0.25674967, 0.86420242,\n",
       "        0.86315432, 0.70957096, 0.86189019, 0.86338234, 0.20174017,\n",
       "        0.86208221, 0.86126213, 0.8159856 , 0.86188219, 0.86346635,\n",
       "        0.73112111, 0.86093009, 0.86279028, 0.61843384, 0.86312231,\n",
       "        0.86219022, 0.45788979, 0.86164616, 0.86087009, 0.77152915,\n",
       "        0.86363436, 0.86141414, 0.8059966 , 0.86405441, 0.86273827,\n",
       "        0.69676968, 0.8629623 , 0.86293429, 0.23470747, 0.86143414,\n",
       "        0.85865387, 0.51383538, 0.86316632, 0.86084608, 0.25731373,\n",
       "        0.86317832, 0.86290229, 0.34279028, 0.86305431, 0.85587759,\n",
       "        0.63566757, 0.85992599, 0.85962996, 0.73334933, 0.860034  ,\n",
       "        0.86049805, 0.60583258, 0.86128613, 0.85993399, 0.61530153,\n",
       "        0.85650565, 0.86073007, 0.6150095 , 0.85134113, 0.85479748,\n",
       "        0.52234423, 0.86217822, 0.85860586, 0.60865287, 0.85835384,\n",
       "        0.86155816, 0.78818682, 0.86335834, 0.85411341, 0.43635964,\n",
       "        0.85612961, 0.85344934, 0.66807481, 0.85874987, 0.85834183,\n",
       "        0.66613861, 0.8570217 , 0.85759776, 0.6960496 , 0.85746975,\n",
       "        0.85476548, 0.50585459, 0.85828183, 0.86167417, 0.49122512,\n",
       "        0.85924592, 0.8620142 , 0.76716872, 0.86010601, 0.85965397,\n",
       "        0.16266427, 0.86093009, 0.8630103 , 0.86311031, 0.86361836,\n",
       "        0.86333433, 0.86333833, 0.86357436, 0.86344634, 0.86376238,\n",
       "        0.86340234, 0.86363436, 0.8639824 , 0.86364636, 0.86283028,\n",
       "        0.86156216, 0.86281428, 0.86335034, 0.86383038, 0.86342234,\n",
       "        0.86334633, 0.86315832, 0.86353835, 0.86381038, 0.8639864 ,\n",
       "        0.86360236, 0.86279428, 0.86326233, 0.86370637, 0.86345835,\n",
       "        0.86315032, 0.86318632, 0.86332633, 0.86345035, 0.86331433,\n",
       "        0.86329033, 0.86336234, 0.86357436, 0.8640224 , 0.86378638,\n",
       "        0.8630183 , 0.86364636, 0.86390639, 0.86352635, 0.8639864 ,\n",
       "        0.86372237, 0.86387039, 0.86389839, 0.86393839, 0.86391039,\n",
       "        0.85830183, 0.86278628, 0.86325033, 0.86281428, 0.85641364,\n",
       "        0.86129813, 0.8559976 , 0.8589779 , 0.85550955, 0.85488149,\n",
       "        0.85278928, 0.85677768, 0.86130213, 0.860042  , 0.86057806,\n",
       "        0.85606161, 0.86009401, 0.86075008, 0.85725773, 0.85462146,\n",
       "        0.85562156, 0.8560456 , 0.85675768, 0.85066907, 0.85968997,\n",
       "        0.8619542 , 0.86026603, 0.85660166, 0.85588959, 0.85592159,\n",
       "        0.85388539, 0.85386939, 0.85210121, 0.85741374, 0.8549975 ,\n",
       "        0.85830183, 0.86060206, 0.86137414, 0.86219822, 0.86123012,\n",
       "        0.86041804, 0.8630223 , 0.85625363, 0.86170617, 0.86216622,\n",
       "        0.85784578, 0.85706571, 0.8580418 ]),\n",
       " 'split1_test_roc_auc_ovo': array([0.847198  , 0.85081052, 0.85102655, 0.84679394, 0.83351603,\n",
       "        0.83301995, 0.8097566 , 0.80959658, 0.82526684, 0.79151798,\n",
       "        0.80658015, 0.80049527, 0.84985038, 0.84959434, 0.85620729,\n",
       "        0.8389008 , 0.83227985, 0.8367885 , 0.82252244, 0.81574147,\n",
       "        0.82402666, 0.81116881, 0.81056072, 0.8187459 , 0.847238  ,\n",
       "        0.84400554, 0.8499664 , 0.83374006, 0.83847674, 0.82939943,\n",
       "        0.8138972 , 0.81933798, 0.82880335, 0.81342513, 0.81620953,\n",
       "        0.8020955 , 0.84389352, 0.85005441, 0.85522715, 0.8410011 ,\n",
       "        0.84324143, 0.83704453, 0.82368661, 0.83079964, 0.83449617,\n",
       "        0.81057272, 0.81861388, 0.82866333, 0.85053448, 0.8527948 ,\n",
       "        0.84194124, 0.83134771, 0.82406867, 0.83790466, 0.82547887,\n",
       "        0.82174233, 0.80608808, 0.80258757, 0.80750428, 0.81480533,\n",
       "        0.84806212, 0.84689795, 0.85167464, 0.8298955 , 0.81715167,\n",
       "        0.84334144, 0.83080364, 0.8118129 , 0.80284361, 0.82039414,\n",
       "        0.80218752, 0.81380519, 0.85215471, 0.84946232, 0.84840217,\n",
       "        0.84220528, 0.83921685, 0.8396089 , 0.81973804, 0.82912339,\n",
       "        0.82557488, 0.81907795, 0.80126338, 0.79104591, 0.84826615,\n",
       "        0.85437503, 0.84630587, 0.83736058, 0.83992495, 0.8277952 ,\n",
       "        0.82931142, 0.81590949, 0.82616297, 0.81904594, 0.82204237,\n",
       "        0.80829639, 0.86092397, 0.51355795, 0.85886768, 0.85837161,\n",
       "        0.68938327, 0.85824359, 0.86022387, 0.63170297, 0.85886368,\n",
       "        0.8597398 , 0.76083356, 0.85829559, 0.85720744, 0.57059817,\n",
       "        0.86064793, 0.86062393, 0.81290506, 0.85741947, 0.86019187,\n",
       "        0.76054952, 0.861132  , 0.86019587, 0.750024  , 0.85962379,\n",
       "        0.86016786, 0.33128771, 0.85993183, 0.85980381, 0.65504233,\n",
       "        0.86132003, 0.8597398 , 0.38134691, 0.86046391, 0.85943576,\n",
       "        0.79633467, 0.8604279 , 0.86101599, 0.78107648, 0.85975981,\n",
       "        0.86106399, 0.30942456, 0.85995183, 0.8603879 , 0.63341121,\n",
       "        0.86124802, 0.85953577, 0.68534269, 0.86057592, 0.86097198,\n",
       "        0.64569298, 0.854147  , 0.85859564, 0.49779968, 0.85808756,\n",
       "        0.8562513 , 0.70035285, 0.86021987, 0.85753549, 0.73740219,\n",
       "        0.86262822, 0.85219472, 0.64729321, 0.86295227, 0.86404042,\n",
       "        0.81186891, 0.8590557 , 0.8569354 , 0.53422493, 0.86054792,\n",
       "        0.85989983, 0.80740027, 0.86222816, 0.85822358, 0.60303884,\n",
       "        0.85547919, 0.86329631, 0.41477973, 0.85844362, 0.8569594 ,\n",
       "        0.78735738, 0.85793154, 0.85897169, 0.74817174, 0.8583436 ,\n",
       "        0.85541518, 0.72042774, 0.85720344, 0.85588725, 0.50709302,\n",
       "        0.861104  , 0.85957178, 0.61499256, 0.85749148, 0.8604399 ,\n",
       "        0.72660863, 0.85671537, 0.8604039 , 0.86089997, 0.86065193,\n",
       "        0.86097198, 0.86094398, 0.8618041 , 0.86165208, 0.86156807,\n",
       "        0.8618361 , 0.86115201, 0.86194812, 0.86215215, 0.86062393,\n",
       "        0.86055592, 0.86138804, 0.86112   , 0.86139204, 0.86202413,\n",
       "        0.86148005, 0.86131203, 0.86143605, 0.86134803, 0.86131603,\n",
       "        0.86198013, 0.86137204, 0.86191612, 0.86238818, 0.8618041 ,\n",
       "        0.861096  , 0.86147205, 0.861084  , 0.86212815, 0.86153206,\n",
       "        0.861084  , 0.86143205, 0.86140004, 0.86192412, 0.86158407,\n",
       "        0.86152406, 0.86201613, 0.8618241 , 0.86205614, 0.86218015,\n",
       "        0.86198013, 0.86237618, 0.86171209, 0.8618201 , 0.8617881 ,\n",
       "        0.85629931, 0.85953177, 0.85473908, 0.85715943, 0.85736346,\n",
       "        0.85158263, 0.85473908, 0.8534829 , 0.8548831 , 0.85193068,\n",
       "        0.85273479, 0.85429502, 0.85465907, 0.85911171, 0.85593926,\n",
       "        0.85799955, 0.85801155, 0.85596726, 0.85570322, 0.85358692,\n",
       "        0.85819958, 0.85182266, 0.85199469, 0.85169464, 0.85837961,\n",
       "        0.85923173, 0.8569474 , 0.85165464, 0.85453505, 0.8590197 ,\n",
       "        0.8548431 , 0.85244275, 0.85547519, 0.85144661, 0.85096654,\n",
       "        0.8548391 , 0.85701141, 0.86011186, 0.85814357, 0.8576315 ,\n",
       "        0.85388296, 0.8548951 , 0.85641932, 0.85719144, 0.85440703,\n",
       "        0.85573523, 0.8555312 , 0.85374294]),\n",
       " 'split2_test_roc_auc_ovo': array([0.86620873, 0.86801299, 0.86578067, 0.84577379, 0.83733458,\n",
       "        0.85939175, 0.84345746, 0.82873534, 0.83690851, 0.83238786,\n",
       "        0.81973004, 0.81744171, 0.86618473, 0.86563665, 0.86560065,\n",
       "        0.84229729, 0.84342545, 0.85720344, 0.83147573, 0.83801667,\n",
       "        0.83719256, 0.83786065, 0.81372518, 0.84229329, 0.86434447,\n",
       "        0.86732489, 0.86752892, 0.84141716, 0.85061449, 0.85265078,\n",
       "        0.85669136, 0.83502424, 0.83182778, 0.83653246, 0.81785777,\n",
       "        0.83307196, 0.86864508, 0.87007729, 0.86768895, 0.85595126,\n",
       "        0.85936375, 0.85685139, 0.8409811 , 0.83882879, 0.84496968,\n",
       "        0.84944232, 0.84819814, 0.8368325 , 0.8638604 , 0.87145749,\n",
       "        0.86158007, 0.84436159, 0.8375246 , 0.85439903, 0.8305356 ,\n",
       "        0.83904482, 0.82857732, 0.82835528, 0.82921541, 0.84764606,\n",
       "        0.86142004, 0.86278824, 0.86581668, 0.83288394, 0.8492943 ,\n",
       "        0.84496968, 0.84042502, 0.82982349, 0.82366261, 0.82158631,\n",
       "        0.83463219, 0.83082364, 0.86627274, 0.86029188, 0.86377238,\n",
       "        0.85457506, 0.84571378, 0.84586981, 0.84060505, 0.83003553,\n",
       "        0.82836728, 0.8145613 , 0.83616441, 0.81638956, 0.87303372,\n",
       "        0.86212815, 0.86890512, 0.84686995, 0.85124258, 0.85729545,\n",
       "        0.84287737, 0.84872622, 0.83640444, 0.83190779, 0.82832728,\n",
       "        0.82186235, 0.87127746, 0.49830776, 0.87048935, 0.87130947,\n",
       "        0.3028036 , 0.87191756, 0.87170553, 0.75879727, 0.8722136 ,\n",
       "        0.87186555, 0.28731337, 0.87165752, 0.87134547, 0.75200029,\n",
       "        0.87236962, 0.87233362, 0.72376822, 0.8728937 , 0.87128547,\n",
       "        0.30938455, 0.87488598, 0.87263766, 0.54465843, 0.87260966,\n",
       "        0.87235762, 0.31078075, 0.87200557, 0.87232161, 0.68453857,\n",
       "        0.87115345, 0.87349378, 0.59779408, 0.87142949, 0.87248964,\n",
       "        0.54842697, 0.8708214 , 0.87017731, 0.5326047 , 0.87079339,\n",
       "        0.87104143, 0.62795042, 0.87255365, 0.87309373, 0.55392377,\n",
       "        0.87244963, 0.87296571, 0.60039046, 0.87298971, 0.86966523,\n",
       "        0.63676769, 0.87334576, 0.86978525, 0.67026052, 0.86960122,\n",
       "        0.8722456 , 0.63689571, 0.87322174, 0.87066938, 0.784721  ,\n",
       "        0.87324175, 0.86463251, 0.42250084, 0.87077739, 0.8701293 ,\n",
       "        0.71751932, 0.87026932, 0.86850106, 0.74114673, 0.86987726,\n",
       "        0.87093341, 0.72176793, 0.87136548, 0.86420845, 0.45925413,\n",
       "        0.87113744, 0.86791298, 0.73376566, 0.87260165, 0.86650078,\n",
       "        0.66270743, 0.87031333, 0.87265366, 0.80364372, 0.87067738,\n",
       "        0.86421245, 0.84739002, 0.86794898, 0.87177754, 0.46920757,\n",
       "        0.8708334 , 0.87071338, 0.7340537 , 0.87022931, 0.87304572,\n",
       "        0.71377078, 0.86756893, 0.87482197, 0.87412587, 0.87457794,\n",
       "        0.87388584, 0.87522603, 0.87454193, 0.87472596, 0.87485798,\n",
       "        0.87391384, 0.87419788, 0.87454193, 0.87470596, 0.87404186,\n",
       "        0.87480997, 0.87536205, 0.87443392, 0.87370581, 0.87436191,\n",
       "        0.87402986, 0.87453393, 0.87493399, 0.87476597, 0.87461794,\n",
       "        0.87447792, 0.8757261 , 0.87475396, 0.87486598, 0.87451393,\n",
       "        0.87388184, 0.87420989, 0.87419788, 0.87413388, 0.87448593,\n",
       "        0.87490599, 0.87473396, 0.87374182, 0.87543806, 0.87517002,\n",
       "        0.87450593, 0.8743179 , 0.87426989, 0.87424189, 0.87391384,\n",
       "        0.87418588, 0.87412187, 0.87472996, 0.875002  , 0.87396985,\n",
       "        0.87226161, 0.87484998, 0.87326975, 0.87300971, 0.87559409,\n",
       "        0.87226961, 0.87213359, 0.87247364, 0.87130547, 0.86828903,\n",
       "        0.87163352, 0.87034933, 0.87613816, 0.87280968, 0.87370981,\n",
       "        0.87480197, 0.87480597, 0.87105343, 0.87253364, 0.87474596,\n",
       "        0.87367381, 0.86891712, 0.86754093, 0.86835704, 0.87462995,\n",
       "        0.87536605, 0.87540206, 0.87509401, 0.87092541, 0.87627418,\n",
       "        0.87555808, 0.87424189, 0.87238162, 0.87178954, 0.87035333,\n",
       "        0.87118945, 0.87495399, 0.87443792, 0.87484998, 0.87469396,\n",
       "        0.87447792, 0.8736338 , 0.87542206, 0.87585412, 0.87414588,\n",
       "        0.87416988, 0.87628619, 0.87283369]),\n",
       " 'split3_test_roc_auc_ovo': array([0.84065705, 0.84248532, 0.85185467, 0.8298635 , 0.83432814,\n",
       "        0.84680194, 0.8243147 , 0.82586692, 0.8187179 , 0.819478  ,\n",
       "        0.81392521, 0.81789778, 0.85116257, 0.84267334, 0.84892224,\n",
       "        0.82603495, 0.833356  , 0.8222144 , 0.81118881, 0.82811525,\n",
       "        0.83035557, 0.82021011, 0.81202893, 0.82028612, 0.83881279,\n",
       "        0.84457762, 0.84895025, 0.82453473, 0.840293  , 0.84063705,\n",
       "        0.81846986, 0.8305596 , 0.812493  , 0.81164488, 0.819438  ,\n",
       "        0.8069642 , 0.85051447, 0.84866621, 0.84592581, 0.8361324 ,\n",
       "        0.8382047 , 0.83896081, 0.83060761, 0.82864332, 0.83607239,\n",
       "        0.82025812, 0.83089165, 0.82675905, 0.85272679, 0.84809012,\n",
       "        0.85184667, 0.83565633, 0.84700197, 0.84202525, 0.81587749,\n",
       "        0.80233954, 0.83952089, 0.82367061, 0.8187419 , 0.82434671,\n",
       "        0.85179066, 0.85319086, 0.85412699, 0.84038502, 0.82558688,\n",
       "        0.84207726, 0.82067818, 0.81810581, 0.83095966, 0.80159143,\n",
       "        0.80159543, 0.80103535, 0.85248276, 0.84969836, 0.85144261,\n",
       "        0.83309997, 0.8312837 , 0.84706198, 0.82526684, 0.83759261,\n",
       "        0.84084108, 0.81815381, 0.81164088, 0.81270103, 0.8604239 ,\n",
       "        0.86009185, 0.84925029, 0.83566834, 0.8368085 , 0.82795923,\n",
       "        0.82812725, 0.81844986, 0.84143317, 0.81630155, 0.81119281,\n",
       "        0.82725112, 0.85440303, 0.44658431, 0.85527916, 0.85670337,\n",
       "        0.76354195, 0.8576475 , 0.85400698, 0.76561025, 0.85641132,\n",
       "        0.85645933, 0.58426013, 0.85708342, 0.8562593 , 0.54815893,\n",
       "        0.85774752, 0.85826759, 0.80810837, 0.85567922, 0.85669936,\n",
       "        0.39674513, 0.85439503, 0.85791554, 0.68479461, 0.85818758,\n",
       "        0.85665936, 0.71772735, 0.8576515 , 0.85607127, 0.78140852,\n",
       "        0.85634731, 0.85675537, 0.75428462, 0.85648333, 0.85644733,\n",
       "        0.59795811, 0.85735546, 0.85599126, 0.57787121, 0.85671537,\n",
       "        0.85801555, 0.72148789, 0.85616329, 0.85715943, 0.58881679,\n",
       "        0.85784353, 0.85737946, 0.31166088, 0.85758349, 0.85009841,\n",
       "        0.72841689, 0.85465507, 0.85402698, 0.77139508, 0.8562673 ,\n",
       "        0.85592325, 0.76168568, 0.85586324, 0.85259477, 0.6896113 ,\n",
       "        0.85584724, 0.85505513, 0.66287945, 0.85011042, 0.85575923,\n",
       "        0.62286169, 0.85675937, 0.85096654, 0.72237602, 0.85717543,\n",
       "        0.8534829 , 0.71105839, 0.85449505, 0.85244675, 0.26642637,\n",
       "        0.85568722, 0.8576155 , 0.34533773, 0.85087853, 0.85302284,\n",
       "        0.70224512, 0.85237474, 0.8555392 , 0.73359364, 0.85533917,\n",
       "        0.85612328, 0.71345474, 0.85099854, 0.85443904, 0.68013394,\n",
       "        0.85065849, 0.85548319, 0.61471252, 0.85565521, 0.85295483,\n",
       "        0.40858284, 0.85482309, 0.85705542, 0.85757149, 0.85827959,\n",
       "        0.85789154, 0.85825159, 0.85790354, 0.85772351, 0.85783553,\n",
       "        0.85784753, 0.85786753, 0.85797955, 0.85793954, 0.85808756,\n",
       "        0.8576635 , 0.85718343, 0.8576155 , 0.8576395 , 0.85788754,\n",
       "        0.85839961, 0.85804356, 0.85798755, 0.85780752, 0.85803156,\n",
       "        0.85790754, 0.8576275 , 0.85789954, 0.85771151, 0.85736346,\n",
       "        0.85781953, 0.85760349, 0.85746347, 0.85776352, 0.85776352,\n",
       "        0.85785953, 0.85778752, 0.85795955, 0.85728345, 0.85793954,\n",
       "        0.8576195 , 0.85785953, 0.85795155, 0.85795155, 0.85795555,\n",
       "        0.85818358, 0.85803956, 0.85778752, 0.85791554, 0.85798755,\n",
       "        0.85430302, 0.85706742, 0.8583116 , 0.85481909, 0.85309485,\n",
       "        0.85796355, 0.85411099, 0.85536317, 0.85856363, 0.85182266,\n",
       "        0.85543918, 0.85587925, 0.85608728, 0.85638732, 0.85895969,\n",
       "        0.85650334, 0.8576475 , 0.8548831 , 0.85404698, 0.85749148,\n",
       "        0.85171465, 0.85819558, 0.85792754, 0.85789554, 0.85939975,\n",
       "        0.85701141, 0.85535517, 0.85738746, 0.85593926, 0.8583516 ,\n",
       "        0.85640332, 0.85505513, 0.8603959 , 0.85865165, 0.85882767,\n",
       "        0.85413099, 0.85937975, 0.85877966, 0.85809557, 0.85912771,\n",
       "        0.86057992, 0.85678338, 0.8583196 , 0.85989983, 0.8618081 ,\n",
       "        0.85506713, 0.85650334, 0.8583076 ]),\n",
       " 'split4_test_roc_auc_ovo': array([0.8396129 , 0.84545775, 0.84416556, 0.82761118, 0.83657247,\n",
       "        0.83945288, 0.82891136, 0.80391176, 0.7951705 , 0.80479189,\n",
       "        0.81586548, 0.79462643, 0.83976893, 0.84642188, 0.84590581,\n",
       "        0.81672961, 0.82610696, 0.82858332, 0.80084332, 0.80501992,\n",
       "        0.80457586, 0.80155542, 0.79822694, 0.79268215, 0.84537773,\n",
       "        0.84395353, 0.8416412 , 0.82454673, 0.84371349, 0.83147173,\n",
       "        0.81930998, 0.79712679, 0.8076603 , 0.79565857, 0.8076403 ,\n",
       "        0.7958386 , 0.85343489, 0.84983038, 0.83846474, 0.82191836,\n",
       "        0.82687507, 0.82942344, 0.82375862, 0.81998208, 0.82762318,\n",
       "        0.81196492, 0.81000464, 0.80605607, 0.84953033, 0.8388728 ,\n",
       "        0.84968235, 0.79509449, 0.83428814, 0.82955146, 0.80424581,\n",
       "        0.78913364, 0.80931454, 0.79741883, 0.80089133, 0.80615209,\n",
       "        0.84384951, 0.85177466, 0.85538318, 0.82551087, 0.8222064 ,\n",
       "        0.81307308, 0.81202893, 0.81567346, 0.78660527, 0.81728169,\n",
       "        0.79969116, 0.78230465, 0.84882223, 0.84767006, 0.84876222,\n",
       "        0.82029012, 0.82342657, 0.8375046 , 0.80659615, 0.8110808 ,\n",
       "        0.80613208, 0.80355971, 0.78145253, 0.79849498, 0.85106255,\n",
       "        0.84477365, 0.85272679, 0.82913139, 0.82853931, 0.8249948 ,\n",
       "        0.80855243, 0.81090877, 0.81045671, 0.79676273, 0.80059529,\n",
       "        0.80782033, 0.86101599, 0.53640524, 0.85960378, 0.85891168,\n",
       "        0.32053816, 0.85674737, 0.86087197, 0.18088605, 0.86003985,\n",
       "        0.85815557, 0.42096862, 0.85843561, 0.85803556, 0.736078  ,\n",
       "        0.85943976, 0.85733146, 0.68368645, 0.85820758, 0.85792354,\n",
       "        0.46576707, 0.85866765, 0.86017186, 0.66265942, 0.85958378,\n",
       "        0.85951177, 0.48035717, 0.85811157, 0.8590037 , 0.40860284,\n",
       "        0.85977981, 0.85827159, 0.74743163, 0.85979981, 0.85939975,\n",
       "        0.80047527, 0.85918772, 0.86035189, 0.76740651, 0.8596958 ,\n",
       "        0.85729145, 0.33493223, 0.85853563, 0.85977181, 0.65560641,\n",
       "        0.85949177, 0.85877166, 0.53483702, 0.8597398 , 0.85617129,\n",
       "        0.51505817, 0.85397897, 0.8527828 , 0.4708358 , 0.85599126,\n",
       "        0.85472708, 0.48513786, 0.85771151, 0.85811157, 0.64077627,\n",
       "        0.85844362, 0.8548911 , 0.53132451, 0.85507113, 0.8569634 ,\n",
       "        0.76725048, 0.85743947, 0.85746748, 0.65821878, 0.85389496,\n",
       "        0.85865565, 0.73553792, 0.8507265 , 0.85527516, 0.81425725,\n",
       "        0.85021043, 0.85748348, 0.70004881, 0.85609928, 0.8527908 ,\n",
       "        0.65643453, 0.85729145, 0.85777152, 0.68689491, 0.85962779,\n",
       "        0.85879967, 0.60406699, 0.85581524, 0.85288282, 0.60397897,\n",
       "        0.85519115, 0.85226273, 0.40517435, 0.85400298, 0.85771551,\n",
       "        0.61562865, 0.85569122, 0.85756749, 0.85717143, 0.85799955,\n",
       "        0.85841561, 0.85785553, 0.85770751, 0.85786753, 0.85896369,\n",
       "        0.85855563, 0.85855563, 0.8583596 , 0.85839961, 0.85740347,\n",
       "        0.85781953, 0.85727945, 0.85811557, 0.8583396 , 0.8583156 ,\n",
       "        0.85737546, 0.85875966, 0.85846362, 0.85888768, 0.85805956,\n",
       "        0.85861964, 0.85784353, 0.85779952, 0.85751148, 0.85863164,\n",
       "        0.85841961, 0.85877166, 0.85845962, 0.85809557, 0.85829559,\n",
       "        0.85896769, 0.85883567, 0.85827559, 0.85816758, 0.85852363,\n",
       "        0.85812757, 0.85855563, 0.85883967, 0.85856363, 0.85878767,\n",
       "        0.85853163, 0.85884767, 0.85864765, 0.85909971, 0.85918372,\n",
       "        0.85559521, 0.85405498, 0.85423101, 0.85251076, 0.85615929,\n",
       "        0.85449105, 0.85089853, 0.85672737, 0.85283881, 0.85007441,\n",
       "        0.84855019, 0.85200269, 0.85669936, 0.85528316, 0.85618729,\n",
       "        0.85201869, 0.85319086, 0.85200669, 0.85103055, 0.84629787,\n",
       "        0.85085452, 0.84505369, 0.84921029, 0.85107455, 0.85637132,\n",
       "        0.85339489, 0.85608728, 0.85252676, 0.85648733, 0.8520627 ,\n",
       "        0.85238274, 0.85126658, 0.85038246, 0.84961835, 0.85117057,\n",
       "        0.85183066, 0.8555632 , 0.85631931, 0.85827959, 0.85687939,\n",
       "        0.85282681, 0.85427102, 0.85313485, 0.85199469, 0.85447904,\n",
       "        0.8513986 , 0.85534317, 0.85413099]),\n",
       " 'mean_test_roc_auc_ovo': array([0.84801227, 0.85209919, 0.85237847, 0.83762444, 0.83415082,\n",
       "        0.84254119, 0.82587808, 0.81873143, 0.81428306, 0.80727491,\n",
       "        0.81540619, 0.80909494, 0.85246483, 0.852384  , 0.85300737,\n",
       "        0.83037389, 0.83155811, 0.83510094, 0.81648879, 0.82336486,\n",
       "        0.82055106, 0.81565675, 0.80993746, 0.81908272, 0.84785627,\n",
       "        0.85047977, 0.85077427, 0.83302056, 0.84339162, 0.83926064,\n",
       "        0.82552767, 0.82092457, 0.81965644, 0.81582385, 0.81869937,\n",
       "        0.81425392, 0.85400667, 0.85286737, 0.85162875, 0.83988631,\n",
       "        0.84244908, 0.84269706, 0.82902971, 0.82966017, 0.83459688,\n",
       "        0.82385456, 0.82652616, 0.82537876, 0.85396586, 0.85247287,\n",
       "        0.85227519, 0.82692619, 0.83491811, 0.83998   , 0.81887192,\n",
       "        0.80947317, 0.81974287, 0.80970319, 0.80970403, 0.81592897,\n",
       "        0.85197036, 0.85428588, 0.85605496, 0.83163006, 0.82922929,\n",
       "        0.83490651, 0.82661254, 0.82123335, 0.80868622, 0.81447254,\n",
       "        0.8110384 , 0.80455366, 0.85467716, 0.85141192, 0.85335856,\n",
       "        0.8351762 , 0.83573296, 0.84291955, 0.82292966, 0.82459797,\n",
       "        0.82412586, 0.80886073, 0.80996623, 0.80620654, 0.85779679,\n",
       "        0.85547646, 0.85494117, 0.83911338, 0.83714285, 0.83277825,\n",
       "        0.82819594, 0.82485307, 0.82673104, 0.81389171, 0.81619673,\n",
       "        0.81702484, 0.86208375, 0.56731349, 0.86121723, 0.86145006,\n",
       "        0.46033951, 0.86153087, 0.8616861 , 0.51874924, 0.86234617,\n",
       "        0.86187491, 0.55258933, 0.86147247, 0.86124602, 0.56171511,\n",
       "        0.86245741, 0.86196374, 0.76889074, 0.86121643, 0.86191332,\n",
       "        0.53271348, 0.86200215, 0.86274224, 0.65211406, 0.86262542,\n",
       "        0.86217737, 0.45960855, 0.86186933, 0.8616141 , 0.66022428,\n",
       "        0.86244699, 0.86193494, 0.65737077, 0.86244619, 0.86210215,\n",
       "        0.68799294, 0.86215096, 0.86209415, 0.57873327, 0.8616797 ,\n",
       "        0.86121326, 0.5015261 , 0.86207414, 0.86225179, 0.53781438,\n",
       "        0.86284225, 0.86231098, 0.49500426, 0.86278865, 0.8585569 ,\n",
       "        0.63232066, 0.85921056, 0.85896413, 0.62872808, 0.85999627,\n",
       "        0.85992906, 0.63798094, 0.8616605 , 0.85976904, 0.69356246,\n",
       "        0.86133329, 0.85750071, 0.5758015 , 0.85805047, 0.86033797,\n",
       "        0.68836893, 0.86114042, 0.85849527, 0.65292386, 0.85996988,\n",
       "        0.86090599, 0.75279027, 0.86043471, 0.85685347, 0.51586724,\n",
       "        0.85772878, 0.85995152, 0.57240135, 0.85935459, 0.85752313,\n",
       "        0.69497661, 0.85898655, 0.86050677, 0.73367072, 0.86029154,\n",
       "        0.85786321, 0.67823881, 0.85804961, 0.85933216, 0.55032772,\n",
       "        0.85940659, 0.86000906, 0.62722037, 0.859497  , 0.86076198,\n",
       "        0.52545103, 0.85914574, 0.86257182, 0.86257582, 0.86302548,\n",
       "        0.86289986, 0.86312309, 0.86310629, 0.86308309, 0.86339753,\n",
       "        0.86311109, 0.86308148, 0.86336232, 0.86336873, 0.86259742,\n",
       "        0.86248222, 0.86280545, 0.86292706, 0.86298147, 0.8632023 ,\n",
       "        0.86292626, 0.8631615 , 0.86327191, 0.86332392, 0.8632023 ,\n",
       "        0.86331752, 0.86307269, 0.86312629, 0.86323671, 0.8631543 ,\n",
       "        0.86287346, 0.86304868, 0.86290626, 0.86311429, 0.86307829,\n",
       "        0.86322151, 0.86323031, 0.86299027, 0.86336712, 0.86340073,\n",
       "        0.86295907, 0.86327911, 0.86335832, 0.86326791, 0.86336472,\n",
       "        0.86332072, 0.86345113, 0.86335512, 0.86355515, 0.86336792,\n",
       "        0.85935219, 0.86165809, 0.86076035, 0.86006266, 0.85972506,\n",
       "        0.85952099, 0.85757596, 0.859405  , 0.85862011, 0.85539965,\n",
       "        0.85622939, 0.85786079, 0.8609772 , 0.86072678, 0.86107482,\n",
       "        0.85947703, 0.86074998, 0.85893211, 0.85811442, 0.85734874,\n",
       "        0.85801282, 0.85600693, 0.85668622, 0.85593817, 0.86169412,\n",
       "        0.86139166, 0.86081159, 0.85865291, 0.85875533, 0.86032595,\n",
       "        0.85861453, 0.85737515, 0.85814727, 0.85778398, 0.85726312,\n",
       "        0.85805841, 0.86150208, 0.86220458, 0.86231339, 0.86191254,\n",
       "        0.86043713, 0.86052112, 0.85990989, 0.86132925, 0.86140125,\n",
       "        0.85884332, 0.86014592, 0.8594114 ]),\n",
       " 'std_test_roc_auc_ovo': array([0.00958198, 0.00887809, 0.00721322, 0.00788919, 0.0029304 ,\n",
       "        0.00974235, 0.01085498, 0.01000507, 0.01658864, 0.01672076,\n",
       "        0.00508945, 0.00964863, 0.00855945, 0.00825106, 0.00717176,\n",
       "        0.0092299 , 0.00712954, 0.01199288, 0.01033334, 0.01161903,\n",
       "        0.01292174, 0.01264477, 0.0060559 , 0.01576503, 0.00870718,\n",
       "        0.0090259 , 0.00886576, 0.00743447, 0.00415333, 0.00833566,\n",
       "        0.01571035, 0.01313931, 0.00929182, 0.01337995, 0.00795147,\n",
       "        0.0158552 , 0.00811535, 0.00874318, 0.00977057, 0.01110761,\n",
       "        0.01050482, 0.00994818, 0.00648315, 0.00600352, 0.00602005,\n",
       "        0.01411065, 0.01285977, 0.01027146, 0.00512849, 0.0106437 ,\n",
       "        0.00658307, 0.01682001, 0.00749974, 0.00825637, 0.00897781,\n",
       "        0.01841755, 0.01252945, 0.01355937, 0.01305695, 0.02014113,\n",
       "        0.00596932, 0.00530257, 0.00502761, 0.00496627, 0.01111965,\n",
       "        0.0119603 , 0.00961842, 0.00766572, 0.01630388, 0.00732485,\n",
       "        0.01333795, 0.01660267, 0.00601605, 0.00451149, 0.00563779,\n",
       "        0.01217561, 0.00766587, 0.00370977, 0.01090953, 0.009907  ,\n",
       "        0.0113378 , 0.01138202, 0.01824952, 0.009732  , 0.00869259,\n",
       "        0.00602659, 0.00791819, 0.00678012, 0.00829194, 0.01231078,\n",
       "        0.01111747, 0.01352563, 0.01123968, 0.0120148 , 0.00955697,\n",
       "        0.00770869, 0.0054132 , 0.14034026, 0.00509416, 0.0052137 ,\n",
       "        0.22087868, 0.00564108, 0.00569496, 0.25064607, 0.00554042,\n",
       "        0.00546206, 0.17703518, 0.00533832, 0.00562223, 0.19821983,\n",
       "        0.00515667, 0.00538407, 0.05475221, 0.00617952, 0.00522142,\n",
       "        0.18117388, 0.00688312, 0.00518285, 0.06847868, 0.00525143,\n",
       "        0.00538943, 0.14537863, 0.00526228, 0.00558603, 0.13491166,\n",
       "        0.00495585, 0.00598239, 0.15448718, 0.00509459, 0.00556227,\n",
       "        0.10203153, 0.00470279, 0.00463652, 0.19850577, 0.00480388,\n",
       "        0.00507487, 0.16073229, 0.00570923, 0.0055692 , 0.14460642,\n",
       "        0.0051219 , 0.00562888, 0.1453998 , 0.00539291, 0.00653672,\n",
       "        0.06810987, 0.00740375, 0.00600433, 0.12255337, 0.00501687,\n",
       "        0.00645994, 0.09336897, 0.00608409, 0.00596742, 0.06190084,\n",
       "        0.00640689, 0.00452185, 0.08911812, 0.00778381, 0.00587585,\n",
       "        0.10409581, 0.00493255, 0.00566215, 0.07574202, 0.00540029,\n",
       "        0.00569479, 0.03804404, 0.00722184, 0.00413258, 0.18351171,\n",
       "        0.0070442 , 0.00506752, 0.1599301 , 0.0071989 , 0.00498465,\n",
       "        0.04887834, 0.00599765, 0.00617291, 0.04173687, 0.00537753,\n",
       "        0.00345849, 0.1156287 , 0.00554035, 0.00689603, 0.07958762,\n",
       "        0.00674949, 0.00631322, 0.12697189, 0.00573619, 0.00667015,\n",
       "        0.21416495, 0.00470446, 0.00648741, 0.00617655, 0.00611928,\n",
       "        0.00582733, 0.00636796, 0.00614661, 0.00622356, 0.00608961,\n",
       "        0.00577697, 0.00592067, 0.00602278, 0.00607055, 0.00603722,\n",
       "        0.00634809, 0.00666065, 0.00611908, 0.00580245, 0.00596825,\n",
       "        0.00594743, 0.00597197, 0.00617235, 0.00608559, 0.00612657,\n",
       "        0.00596259, 0.00663435, 0.00620289, 0.00631643, 0.00608186,\n",
       "        0.00582751, 0.00591642, 0.00600657, 0.0059386 , 0.00606138,\n",
       "        0.00613101, 0.0060749 , 0.00576198, 0.00651561, 0.00625421,\n",
       "        0.00611959, 0.00592061, 0.00585437, 0.00587124, 0.00571458,\n",
       "        0.00581955, 0.00575627, 0.00609164, 0.00609662, 0.00568589,\n",
       "        0.00658309, 0.00719307, 0.00703661, 0.00732328, 0.00806279,\n",
       "        0.00716141, 0.00747071, 0.00677568, 0.00660281, 0.00662709,\n",
       "        0.00801159, 0.00645128, 0.00790051, 0.0062855 , 0.00655173,\n",
       "        0.00791418, 0.00737924, 0.00668398, 0.00749916, 0.00944876,\n",
       "        0.00826956, 0.00786062, 0.00628125, 0.00674515, 0.00657168,\n",
       "        0.00752862, 0.00748603, 0.00851615, 0.00611896, 0.00833823,\n",
       "        0.00857201, 0.00852998, 0.00789507, 0.00779597, 0.00715123,\n",
       "        0.00688559, 0.00695276, 0.00634177, 0.00645954, 0.00656118,\n",
       "        0.00772061, 0.007251  , 0.00793234, 0.00796759, 0.00721127,\n",
       "        0.00794031, 0.00809477, 0.00697519]),\n",
       " 'rank_test_roc_auc_ovo': array([183, 177, 175, 194, 202, 188, 216, 232, 242, 254, 240, 251, 173,\n",
       "        174, 170, 207, 206, 198, 235, 223, 227, 239, 247, 230, 184, 182,\n",
       "        181, 203, 185, 192, 217, 226, 229, 238, 233, 243, 167, 171, 179,\n",
       "        191, 189, 187, 210, 208, 201, 222, 215, 218, 168, 172, 176, 212,\n",
       "        199, 190, 231, 250, 228, 249, 248, 237, 178, 166, 159, 205, 209,\n",
       "        200, 214, 225, 253, 241, 245, 256, 165, 180, 169, 197, 196, 186,\n",
       "        224, 220, 221, 252, 246, 255, 147, 162, 164, 193, 195, 204, 211,\n",
       "        219, 213, 244, 236, 234,  65, 276,  89,  83, 287,  80,  75, 283,\n",
       "         56,  72, 278,  82,  88, 277,  53,  68, 257,  90,  70, 281,  67,\n",
       "         47, 268,  48,  61, 288,  73,  79, 265,  54,  69, 266,  55,  63,\n",
       "        263,  62,  64, 273,  76,  91, 285,  66,  59, 280,  44,  58, 286,\n",
       "         46, 137, 270, 127, 130, 271, 111, 114, 269,  77, 116, 261,  86,\n",
       "        152, 274, 142, 105, 262,  92, 138, 267, 112,  95, 258, 104, 156,\n",
       "        284, 149, 113, 275, 124, 151, 260, 129, 102, 259, 107, 145, 264,\n",
       "        143, 126, 279, 122, 110, 272, 119,  97, 282, 128,  51,  50,  35,\n",
       "         42,  26,  29,  30,   4,  28,  31,   9,   5,  49,  52,  45,  39,\n",
       "         37,  21,  40,  23,  16,  12,  22,  14,  33,  25,  18,  24,  43,\n",
       "         34,  41,  27,  32,  20,  19,  36,   7,   3,  38,  15,  10,  17,\n",
       "          8,  13,   2,  11,   1,   6, 125,  78,  98, 109, 117, 118, 150,\n",
       "        123, 135, 163, 158, 146,  94, 100,  93, 120,  99, 131, 140, 154,\n",
       "        144, 160, 157, 161,  74,  85,  96, 134, 133, 106, 136, 153, 139,\n",
       "        148, 155, 141,  81,  60,  57,  71, 103, 101, 115,  87,  84, 132,\n",
       "        108, 121]),\n",
       " 'split0_test_jaccard': array([0.61897106, 0.62957075, 0.616     , 0.61980831, 0.60952381,\n",
       "        0.608     , 0.58615385, 0.60606061, 0.55813953, 0.56355283,\n",
       "        0.61443932, 0.59404389, 0.62619808, 0.62579618, 0.61661342,\n",
       "        0.59083728, 0.60189573, 0.60377358, 0.59780908, 0.61234177,\n",
       "        0.56965944, 0.57232704, 0.60610932, 0.57619048, 0.62698413,\n",
       "        0.64573269, 0.63942308, 0.62063492, 0.6200318 , 0.63225806,\n",
       "        0.60620155, 0.59904153, 0.5856    , 0.58995138, 0.62222222,\n",
       "        0.60031104, 0.63533225, 0.63327948, 0.625     , 0.62200957,\n",
       "        0.6263911 , 0.64065041, 0.59300477, 0.61356467, 0.6136725 ,\n",
       "        0.60218409, 0.60094637, 0.60377358, 0.62519936, 0.62962963,\n",
       "        0.65064103, 0.59335443, 0.62341772, 0.61562998, 0.58176101,\n",
       "        0.5619195 , 0.59875583, 0.56474259, 0.56666667, 0.57298137,\n",
       "        0.64057508, 0.64012739, 0.61965135, 0.60472441, 0.61489699,\n",
       "        0.60967742, 0.60596546, 0.59365079, 0.57410296, 0.58084772,\n",
       "        0.5955414 , 0.57230769, 0.63057325, 0.6128    , 0.62380952,\n",
       "        0.59196291, 0.60625   , 0.6192    , 0.6066879 , 0.60450161,\n",
       "        0.59968603, 0.56677019, 0.5945122 , 0.59873618, 0.63123994,\n",
       "        0.63949843, 0.64573269, 0.62619808, 0.6076555 , 0.59148265,\n",
       "        0.6       , 0.61006289, 0.58954041, 0.571875  , 0.60910518,\n",
       "        0.6032    , 0.62962963, 0.60586319, 0.63723917, 0.6414791 ,\n",
       "        0.505     , 0.6336    , 0.62798092, 0.        , 0.6256    ,\n",
       "        0.62798092, 0.0019802 , 0.6304    , 0.6368    , 0.00191939,\n",
       "        0.62779553, 0.63768116, 0.32954545, 0.63826367, 0.63826367,\n",
       "        0.51217039, 0.63548387, 0.6388443 , 0.01181102, 0.63242376,\n",
       "        0.62980769, 0.        , 0.6318328 , 0.63812601, 0.        ,\n",
       "        0.6288    , 0.62660256, 0.04752475, 0.63258786, 0.63461538,\n",
       "        0.505     , 0.63344051, 0.632     , 0.        , 0.6341853 ,\n",
       "        0.63461538, 0.        , 0.632     , 0.63344051, 0.11462451,\n",
       "        0.632     , 0.63929147, 0.        , 0.63123994, 0.63285024,\n",
       "        0.47424042, 0.62801932, 0.63812601, 0.48558422, 0.63929147,\n",
       "        0.61722488, 0.00980392, 0.6368    , 0.625     , 0.3503876 ,\n",
       "        0.62857143, 0.63897764, 0.29934211, 0.63387097, 0.6327504 ,\n",
       "        0.22042139, 0.62258065, 0.62838915, 0.46698113, 0.63081862,\n",
       "        0.62820513, 0.53433836, 0.63795853, 0.62559242, 0.        ,\n",
       "        0.62816456, 0.62300319, 0.25137615, 0.63192182, 0.64044944,\n",
       "        0.46013986, 0.6341853 , 0.62600321, 0.4761194 , 0.62200957,\n",
       "        0.63375796, 0.4239521 , 0.62962963, 0.63301282, 0.42669845,\n",
       "        0.64376997, 0.64583333, 0.5       , 0.62939297, 0.63081862,\n",
       "        0.29618163, 0.63593005, 0.63826367, 0.6433121 , 0.63955343,\n",
       "        0.63578275, 0.63782051, 0.63929147, 0.6368    , 0.6384    ,\n",
       "        0.63504823, 0.63929147, 0.6318328 , 0.6336    , 0.63285024,\n",
       "        0.63225806, 0.63929147, 0.63709677, 0.63723917, 0.63826367,\n",
       "        0.6368    , 0.63578275, 0.63782051, 0.6336    , 0.63826367,\n",
       "        0.63929147, 0.64412238, 0.63387097, 0.6391097 , 0.63548387,\n",
       "        0.63285024, 0.63987138, 0.63402889, 0.63081862, 0.6336    ,\n",
       "        0.6368    , 0.63723917, 0.63402889, 0.63897764, 0.63723917,\n",
       "        0.63387097, 0.63563403, 0.63402889, 0.63826367, 0.63621795,\n",
       "        0.64      , 0.63768116, 0.6384    , 0.63621795, 0.6352    ,\n",
       "        0.64193548, 0.64790997, 0.64951768, 0.63665595, 0.6414791 ,\n",
       "        0.648     , 0.63414634, 0.64423077, 0.63123994, 0.6432    ,\n",
       "        0.64057508, 0.64724919, 0.648     , 0.63285024, 0.63621795,\n",
       "        0.65008026, 0.64573269, 0.64686998, 0.63476874, 0.64126984,\n",
       "        0.6341853 , 0.6327504 , 0.63795853, 0.6256    , 0.63489499,\n",
       "        0.64069952, 0.6388443 , 0.63723917, 0.63621795, 0.64044944,\n",
       "        0.64365971, 0.64620355, 0.63809524, 0.63344051, 0.6365105 ,\n",
       "        0.63446055, 0.63709677, 0.64102564, 0.62419355, 0.64423077,\n",
       "        0.63489499, 0.647343  , 0.63782051, 0.6511254 , 0.6432    ,\n",
       "        0.64205457, 0.64630225, 0.64790997]),\n",
       " 'split1_test_jaccard': array([0.61414791, 0.63784666, 0.62903226, 0.62684124, 0.625     ,\n",
       "        0.59651899, 0.58795563, 0.57770801, 0.59229535, 0.55968992,\n",
       "        0.56535433, 0.58411215, 0.62944984, 0.61538462, 0.62439024,\n",
       "        0.62019231, 0.60416667, 0.61858974, 0.59577922, 0.5984127 ,\n",
       "        0.59584665, 0.58712716, 0.57861635, 0.566719  , 0.62764228,\n",
       "        0.62337662, 0.63311688, 0.60610932, 0.63106796, 0.58161648,\n",
       "        0.58832808, 0.60063391, 0.6048    , 0.58215962, 0.58135861,\n",
       "        0.57276995, 0.62074554, 0.6202946 , 0.62258065, 0.62398703,\n",
       "        0.60634921, 0.60127592, 0.58398744, 0.60128617, 0.60793651,\n",
       "        0.58146965, 0.59365079, 0.60289389, 0.62236629, 0.62621359,\n",
       "        0.62258065, 0.60127592, 0.61136713, 0.61550889, 0.61217949,\n",
       "        0.60663507, 0.57436709, 0.54874214, 0.57165354, 0.58699187,\n",
       "        0.6197411 , 0.62741935, 0.6187399 , 0.60567823, 0.60472441,\n",
       "        0.62520194, 0.61352657, 0.59557662, 0.58059468, 0.5846395 ,\n",
       "        0.55850234, 0.57894737, 0.64098361, 0.6211878 , 0.63278689,\n",
       "        0.61712439, 0.60967742, 0.61550889, 0.60358891, 0.57410296,\n",
       "        0.59330144, 0.60971787, 0.56595092, 0.57369255, 0.63621533,\n",
       "        0.625     , 0.62358643, 0.61352657, 0.62987013, 0.59011164,\n",
       "        0.6176    , 0.5947205 , 0.60223642, 0.58804523, 0.56893819,\n",
       "        0.57728707, 0.64516129, 0.03632887, 0.64227642, 0.63414634,\n",
       "        0.28670121, 0.64019449, 0.64772727, 0.506     , 0.63709677,\n",
       "        0.63974152, 0.        , 0.63961039, 0.63784666, 0.12844037,\n",
       "        0.64926591, 0.64215686, 0.50701403, 0.63709677, 0.64829822,\n",
       "        0.56114286, 0.64552846, 0.64926591, 0.        , 0.64563107,\n",
       "        0.64829822, 0.00582524, 0.64448052, 0.64820847, 0.506     ,\n",
       "        0.64552846, 0.64135703, 0.        , 0.64667747, 0.64724919,\n",
       "        0.52012384, 0.65252855, 0.63474026, 0.506     , 0.65024631,\n",
       "        0.64878049, 0.0019685 , 0.64878049, 0.6461039 , 0.20147874,\n",
       "        0.6465798 , 0.64332248, 0.37377049, 0.65203252, 0.65097403,\n",
       "        0.47712418, 0.63192182, 0.64772727, 0.30440771, 0.64343598,\n",
       "        0.63754045, 0.48542274, 0.6503268 , 0.64458805, 0.55231788,\n",
       "        0.6514658 , 0.63993453, 0.47820672, 0.64215686, 0.63430421,\n",
       "        0.57109557, 0.64715447, 0.63754045, 0.45823666, 0.64215686,\n",
       "        0.65048544, 0.53962901, 0.64696223, 0.64763458, 0.45822785,\n",
       "        0.63382594, 0.64715447, 0.43597884, 0.64469453, 0.64448052,\n",
       "        0.36481481, 0.64262295, 0.6514658 , 0.56658291, 0.63915858,\n",
       "        0.64102564, 0.4340836 , 0.64090177, 0.64354839, 0.16920474,\n",
       "        0.64390244, 0.63915858, 0.15168539, 0.64772727, 0.6465798 ,\n",
       "        0.53409091, 0.64991896, 0.64521452, 0.64763458, 0.65196078,\n",
       "        0.65089723, 0.64820847, 0.65316045, 0.65203252, 0.65316045,\n",
       "        0.65097403, 0.64991896, 0.65048544, 0.65422078, 0.64878049,\n",
       "        0.6525974 , 0.64926591, 0.64705882, 0.64820847, 0.65196078,\n",
       "        0.64935065, 0.6525974 , 0.65089723, 0.6514658 , 0.65097403,\n",
       "        0.65316045, 0.65691057, 0.6547812 , 0.6525974 , 0.64991896,\n",
       "        0.64869281, 0.6504065 , 0.65097403, 0.65422078, 0.65415987,\n",
       "        0.6525974 , 0.64763458, 0.65097403, 0.65203252, 0.64552846,\n",
       "        0.64320786, 0.64991896, 0.65097403, 0.65422078, 0.6525974 ,\n",
       "        0.65097403, 0.65316045, 0.65153971, 0.64926591, 0.65097403,\n",
       "        0.63857374, 0.64991896, 0.63474026, 0.63355049, 0.63961039,\n",
       "        0.62662338, 0.6411093 , 0.63295269, 0.63071895, 0.61352657,\n",
       "        0.63680782, 0.62944984, 0.63355049, 0.63902439, 0.64600326,\n",
       "        0.64552846, 0.64065041, 0.63533225, 0.62764228, 0.6372549 ,\n",
       "        0.63915858, 0.62052117, 0.62459547, 0.61550889, 0.64705882,\n",
       "        0.63947798, 0.64705882, 0.63414634, 0.6372549 , 0.64552846,\n",
       "        0.63577236, 0.63327948, 0.63739837, 0.63430421, 0.63004847,\n",
       "        0.62621359, 0.63961039, 0.64135703, 0.64065041, 0.6411093 ,\n",
       "        0.64006515, 0.63562092, 0.63398693, 0.65081967, 0.63562092,\n",
       "        0.63371151, 0.63149351, 0.62197092]),\n",
       " 'split2_test_jaccard': array([0.65105008, 0.66227348, 0.64860427, 0.63291139, 0.62861736,\n",
       "        0.64802632, 0.62279294, 0.60855784, 0.62540193, 0.60344828,\n",
       "        0.59937888, 0.60031847, 0.64569536, 0.64006515, 0.65645161,\n",
       "        0.61172742, 0.61030596, 0.63285024, 0.5993538 , 0.61464968,\n",
       "        0.6073132 , 0.6078125 , 0.58604651, 0.62123613, 0.65671642,\n",
       "        0.65139116, 0.63301282, 0.60446571, 0.61526232, 0.64433812,\n",
       "        0.64667747, 0.61538462, 0.59148265, 0.62179487, 0.58372457,\n",
       "        0.60410095, 0.67275748, 0.65353038, 0.65066225, 0.63924051,\n",
       "        0.65584416, 0.63022508, 0.61041009, 0.62461059, 0.63216561,\n",
       "        0.6263911 , 0.62640902, 0.61685215, 0.63022508, 0.66231648,\n",
       "        0.65957447, 0.61806656, 0.61356467, 0.6365105 , 0.61102362,\n",
       "        0.62861736, 0.62401264, 0.59404389, 0.58878505, 0.63422292,\n",
       "        0.6407767 , 0.63621795, 0.62969005, 0.59567901, 0.6304    ,\n",
       "        0.62197092, 0.6388443 , 0.62183544, 0.59904153, 0.58823529,\n",
       "        0.60821485, 0.61208267, 0.65789474, 0.64169381, 0.64763458,\n",
       "        0.62063492, 0.63782051, 0.61732283, 0.63207547, 0.6009539 ,\n",
       "        0.61783439, 0.59969559, 0.60697306, 0.59190031, 0.65681445,\n",
       "        0.63301282, 0.64600326, 0.64215686, 0.63285024, 0.64763458,\n",
       "        0.63479624, 0.6224    , 0.61942675, 0.60522273, 0.58267717,\n",
       "        0.6208    , 0.6503268 , 0.15508885, 0.65097403, 0.65849673,\n",
       "        0.502     , 0.65737705, 0.65640194, 0.5389755 , 0.65691057,\n",
       "        0.6514658 , 0.506     , 0.65798046, 0.64943457, 0.        ,\n",
       "        0.65372168, 0.65089723, 0.50854271, 0.65153971, 0.64991896,\n",
       "        0.33738938, 0.66284779, 0.65746753, 0.506     , 0.65960912,\n",
       "        0.65528455, 0.2189781 , 0.65359477, 0.65905383, 0.506     ,\n",
       "        0.65695793, 0.65845649, 0.        , 0.6514658 , 0.65579119,\n",
       "        0.505     , 0.65316045, 0.65640194, 0.24038462, 0.65960912,\n",
       "        0.65353038, 0.        , 0.65793781, 0.66448445, 0.        ,\n",
       "        0.65960912, 0.65960912, 0.42249657, 0.65737705, 0.64285714,\n",
       "        0.39942113, 0.67704918, 0.65909091, 0.4740533 , 0.65737705,\n",
       "        0.65746753, 0.44588745, 0.6514658 , 0.64878049, 0.50961538,\n",
       "        0.65422078, 0.6474359 , 0.40868597, 0.65153971, 0.65064103,\n",
       "        0.44065041, 0.65252855, 0.64552846, 0.33577982, 0.65802269,\n",
       "        0.66071429, 0.51589595, 0.65853659, 0.64484452, 0.292     ,\n",
       "        0.65806452, 0.65089723, 0.50393701, 0.66721311, 0.63578275,\n",
       "        0.37128713, 0.64677419, 0.66393443, 0.5008881 , 0.66447368,\n",
       "        0.64215686, 0.64012251, 0.65359477, 0.66556837, 0.31847969,\n",
       "        0.66229508, 0.65056361, 0.5611413 , 0.65409836, 0.65960912,\n",
       "        0.40562914, 0.6461039 , 0.6589404 , 0.66338259, 0.66392092,\n",
       "        0.66392092, 0.66447368, 0.66502463, 0.66280992, 0.66611842,\n",
       "        0.6650165 , 0.6650165 , 0.66392092, 0.66556291, 0.66286645,\n",
       "        0.66280992, 0.66556291, 0.66225166, 0.66176471, 0.66176471,\n",
       "        0.65901639, 0.66336634, 0.66446281, 0.66446281, 0.66176471,\n",
       "        0.66556837, 0.66392092, 0.66013072, 0.66176471, 0.66338259,\n",
       "        0.66282895, 0.66121113, 0.66227348, 0.66392092, 0.66666667,\n",
       "        0.66338259, 0.66446281, 0.66013072, 0.66      , 0.65946844,\n",
       "        0.66071429, 0.66392092, 0.66338259, 0.66338259, 0.66282895,\n",
       "        0.6661157 , 0.66118421, 0.66336634, 0.66284779, 0.66174056,\n",
       "        0.66227348, 0.66118421, 0.65625   , 0.66009852, 0.6650165 ,\n",
       "        0.6601626 , 0.66286645, 0.65359477, 0.64935065, 0.64573269,\n",
       "        0.64983713, 0.65460526, 0.66502463, 0.66503268, 0.65960912,\n",
       "        0.66282895, 0.66775777, 0.65957447, 0.6525974 , 0.6514658 ,\n",
       "        0.66013072, 0.65309446, 0.64991896, 0.64320786, 0.66229508,\n",
       "        0.66284779, 0.65686275, 0.66009852, 0.65584416, 0.66612111,\n",
       "        0.65203252, 0.65630115, 0.66557377, 0.65074135, 0.65056361,\n",
       "        0.64869281, 0.66068515, 0.66231648, 0.66611842, 0.65522876,\n",
       "        0.66776316, 0.65309446, 0.6514658 , 0.65841584, 0.66176471,\n",
       "        0.66447368, 0.65630115, 0.6514658 ]),\n",
       " 'split3_test_jaccard': array([0.58255452, 0.61732283, 0.6015873 , 0.5977918 , 0.60436137,\n",
       "        0.60932476, 0.59055118, 0.60725552, 0.575     , 0.58566978,\n",
       "        0.59936407, 0.57632399, 0.61041009, 0.59206349, 0.61189358,\n",
       "        0.57527734, 0.61295419, 0.59011164, 0.57436709, 0.59206349,\n",
       "        0.59808612, 0.5765625 , 0.58631415, 0.59247649, 0.6028481 ,\n",
       "        0.59936909, 0.6       , 0.6022187 , 0.6       , 0.6140625 ,\n",
       "        0.60189573, 0.59752322, 0.57698289, 0.58571429, 0.57961783,\n",
       "        0.58475894, 0.61526232, 0.60917722, 0.60821485, 0.61356467,\n",
       "        0.62420382, 0.61746032, 0.61403509, 0.59557662, 0.62025316,\n",
       "        0.592     , 0.6066879 , 0.60093897, 0.61611374, 0.60188088,\n",
       "        0.61392405, 0.60031348, 0.62579618, 0.61163522, 0.59968603,\n",
       "        0.58044164, 0.62035541, 0.59813084, 0.60250391, 0.6       ,\n",
       "        0.61904762, 0.60350318, 0.62460064, 0.6031746 , 0.59119497,\n",
       "        0.60383387, 0.59874608, 0.603125  , 0.61049285, 0.5609375 ,\n",
       "        0.58787879, 0.56568779, 0.62660256, 0.62420382, 0.61032864,\n",
       "        0.59968102, 0.60125589, 0.6224    , 0.584375  , 0.59873618,\n",
       "        0.61612903, 0.5977918 , 0.59006211, 0.59141494, 0.63517915,\n",
       "        0.6211878 , 0.6122449 , 0.60538827, 0.60291734, 0.59190031,\n",
       "        0.61352657, 0.58869702, 0.62939297, 0.57836991, 0.58897638,\n",
       "        0.60629921, 0.60952381, 0.        , 0.616     , 0.6272    ,\n",
       "        0.51851852, 0.62379421, 0.62123613, 0.51111111, 0.61305732,\n",
       "        0.61685215, 0.0019685 , 0.6272    , 0.61736334, 0.44023669,\n",
       "        0.62179487, 0.62300319, 0.22823985, 0.62321145, 0.62041467,\n",
       "        0.505     , 0.6208    , 0.6200318 , 0.52233677, 0.62380952,\n",
       "        0.61049285, 0.        , 0.62300319, 0.616     , 0.56840796,\n",
       "        0.61624204, 0.61501597, 0.506     , 0.62619808, 0.62019231,\n",
       "        0.38171263, 0.61587302, 0.61722488, 0.5076297 , 0.61501597,\n",
       "        0.61685215, 0.01185771, 0.61550633, 0.6208    , 0.506     ,\n",
       "        0.62700965, 0.6192    , 0.48244734, 0.61942675, 0.61661342,\n",
       "        0.45065789, 0.61392405, 0.62599049, 0.5159129 , 0.61942675,\n",
       "        0.62200957, 0.56898817, 0.61987382, 0.61695447, 0.45550528,\n",
       "        0.60759494, 0.60063391, 0.51405152, 0.61006289, 0.60386473,\n",
       "        0.45667125, 0.61987382, 0.61172742, 0.5058309 , 0.61259843,\n",
       "        0.61550633, 0.45525903, 0.6122449 , 0.60094637, 0.16156671,\n",
       "        0.61172742, 0.61111111, 0.2995338 , 0.6056338 , 0.616     ,\n",
       "        0.51368159, 0.6015748 , 0.60952381, 0.53891336, 0.60625   ,\n",
       "        0.61526232, 0.25      , 0.60416667, 0.61708861, 0.18232044,\n",
       "        0.60443038, 0.61806656, 0.50622877, 0.61881978, 0.62162162,\n",
       "        0.25882353, 0.61198738, 0.60753532, 0.61067504, 0.60759494,\n",
       "        0.61006289, 0.60883281, 0.60815047, 0.60596546, 0.60596546,\n",
       "        0.60753532, 0.61041009, 0.60725552, 0.60787402, 0.60663507,\n",
       "        0.603125  , 0.60658307, 0.60944882, 0.61102362, 0.60601266,\n",
       "        0.60534591, 0.61006289, 0.60629921, 0.60944882, 0.60663507,\n",
       "        0.6073132 , 0.61244019, 0.60567823, 0.6096    , 0.60697306,\n",
       "        0.60725552, 0.60979463, 0.61006289, 0.60572337, 0.60596546,\n",
       "        0.60883281, 0.60787402, 0.60725552, 0.60697306, 0.61198738,\n",
       "        0.61198738, 0.61137441, 0.61006289, 0.61102362, 0.61067504,\n",
       "        0.60596546, 0.60849057, 0.60725552, 0.60501567, 0.61014263,\n",
       "        0.61587302, 0.61014263, 0.61295419, 0.61698718, 0.60596546,\n",
       "        0.6073132 , 0.60543131, 0.62200957, 0.6108453 , 0.60890302,\n",
       "        0.60828025, 0.61254019, 0.61295419, 0.60849057, 0.60759494,\n",
       "        0.60572337, 0.59904913, 0.61182109, 0.61269841, 0.61708861,\n",
       "        0.60189573, 0.6112    , 0.60897436, 0.60932476, 0.61562998,\n",
       "        0.61014263, 0.60501567, 0.60567823, 0.6064    , 0.61217949,\n",
       "        0.60897436, 0.61797753, 0.61014263, 0.61624204, 0.616     ,\n",
       "        0.61550633, 0.61295419, 0.60658307, 0.60849057, 0.61526232,\n",
       "        0.61428571, 0.61014263, 0.61464968, 0.61111111, 0.61835749,\n",
       "        0.61146497, 0.61378205, 0.60890302]),\n",
       " 'split4_test_jaccard': array([0.58776167, 0.61967213, 0.61374795, 0.59216966, 0.61500816,\n",
       "        0.63333333, 0.60064412, 0.58280255, 0.58528951, 0.57480315,\n",
       "        0.58032787, 0.57097289, 0.62624585, 0.61716172, 0.62932455,\n",
       "        0.5927673 , 0.59493671, 0.60876623, 0.58466454, 0.56782334,\n",
       "        0.56487342, 0.56549521, 0.56050955, 0.56915739, 0.63011457,\n",
       "        0.61881188, 0.62962963, 0.59319287, 0.62086093, 0.60128617,\n",
       "        0.57961783, 0.55915721, 0.57392687, 0.5686901 , 0.56514914,\n",
       "        0.5640625 , 0.64321608, 0.63018242, 0.62809917, 0.59455128,\n",
       "        0.59229535, 0.62884927, 0.6091954 , 0.58105939, 0.60777958,\n",
       "        0.60517799, 0.57389937, 0.57894737, 0.61463415, 0.59277504,\n",
       "        0.63071895, 0.56369427, 0.60450161, 0.60983607, 0.56360709,\n",
       "        0.57728707, 0.59463722, 0.56964006, 0.5399361 , 0.59399684,\n",
       "        0.60790774, 0.61691542, 0.63801653, 0.61575563, 0.58267717,\n",
       "        0.57371795, 0.59399684, 0.5984127 , 0.56025039, 0.59649123,\n",
       "        0.58132045, 0.53846154, 0.62108731, 0.6127451 , 0.61943987,\n",
       "        0.58842444, 0.59738134, 0.61138211, 0.58908507, 0.57570978,\n",
       "        0.57350565, 0.57119741, 0.54783951, 0.57507987, 0.63382594,\n",
       "        0.61389338, 0.62786885, 0.58908507, 0.6016129 , 0.59967051,\n",
       "        0.56240126, 0.58267717, 0.58986928, 0.57827476, 0.57233704,\n",
       "        0.57419355, 0.64297521, 0.506     , 0.63893511, 0.63140496,\n",
       "        0.12535613, 0.63097199, 0.65008292, 0.        , 0.64119601,\n",
       "        0.63636364, 0.        , 0.63801653, 0.6461794 , 0.51173021,\n",
       "        0.64521452, 0.63426689, 0.        , 0.62582781, 0.63591433,\n",
       "        0.20597484, 0.63036304, 0.64344942, 0.        , 0.63741722,\n",
       "        0.63531353, 0.506     , 0.65074135, 0.63486842, 0.00393701,\n",
       "        0.64013267, 0.64415157, 0.53836784, 0.63546798, 0.64119601,\n",
       "        0.53624733, 0.63140496, 0.64321608, 0.27746741, 0.63157895,\n",
       "        0.6307947 , 0.        , 0.63036304, 0.64666667, 0.        ,\n",
       "        0.63289037, 0.63681592, 0.47433036, 0.63727121, 0.63333333,\n",
       "        0.47786606, 0.63132137, 0.63636364, 0.15548282, 0.64320786,\n",
       "        0.63382594, 0.17307692, 0.65116279, 0.63920923, 0.30541872,\n",
       "        0.63531353, 0.64238411, 0.44019139, 0.62626263, 0.6422629 ,\n",
       "        0.48484848, 0.6427379 , 0.63875205, 0.47255689, 0.63907285,\n",
       "        0.64166667, 0.47401575, 0.64013267, 0.63857374, 0.46642468,\n",
       "        0.6195122 , 0.64451827, 0.28442029, 0.64250412, 0.63711002,\n",
       "        0.47984887, 0.63980263, 0.64166667, 0.07692308, 0.64367816,\n",
       "        0.65245902, 0.48580786, 0.61960133, 0.63018242, 0.22521008,\n",
       "        0.64607679, 0.62805873, 0.37191011, 0.64415157, 0.64144737,\n",
       "        0.37059724, 0.62786885, 0.63907285, 0.640599  , 0.65630115,\n",
       "        0.64403974, 0.64403974, 0.64569536, 0.64793388, 0.64473684,\n",
       "        0.63861386, 0.64628099, 0.6446281 , 0.64628099, 0.64179104,\n",
       "        0.64179104, 0.63953488, 0.65016502, 0.64179104, 0.64297521,\n",
       "        0.6446281 , 0.64473684, 0.64132231, 0.64415157, 0.64132231,\n",
       "        0.64403974, 0.6446281 , 0.63907285, 0.64686469, 0.64072848,\n",
       "        0.63787375, 0.64344942, 0.64851485, 0.64851485, 0.64297521,\n",
       "        0.64132231, 0.6446281 , 0.64132231, 0.6446281 , 0.64119601,\n",
       "        0.64628099, 0.64297521, 0.64297521, 0.64403974, 0.6446281 ,\n",
       "        0.64403974, 0.64297521, 0.64297521, 0.64521452, 0.64521452,\n",
       "        0.64119601, 0.6272578 , 0.63756178, 0.63410596, 0.6369637 ,\n",
       "        0.64676617, 0.62438221, 0.6320132 , 0.62786885, 0.62006579,\n",
       "        0.62624585, 0.63801653, 0.64628099, 0.64833333, 0.63486842,\n",
       "        0.63410596, 0.64132231, 0.63920923, 0.63278689, 0.61842105,\n",
       "        0.63122924, 0.6097561 , 0.62932455, 0.6295082 , 0.64744646,\n",
       "        0.64392679, 0.64356436, 0.63515755, 0.64891847, 0.64026403,\n",
       "        0.62622951, 0.61702128, 0.62748344, 0.625     , 0.6192053 ,\n",
       "        0.62889984, 0.64793388, 0.64119601, 0.65397351, 0.64774624,\n",
       "        0.64179104, 0.63741722, 0.63531353, 0.63261944, 0.62932455,\n",
       "        0.63166667, 0.64344942, 0.63471074]),\n",
       " 'mean_test_jaccard': array([0.61089705, 0.63333717, 0.62179436, 0.61390448, 0.61650214,\n",
       "        0.61904068, 0.59761954, 0.59647691, 0.58722526, 0.57743279,\n",
       "        0.5917729 , 0.58515428, 0.62759985, 0.61809423, 0.62773468,\n",
       "        0.59816033, 0.60485185, 0.61081829, 0.59039474, 0.5970582 ,\n",
       "        0.58715577, 0.58186488, 0.58351918, 0.5851559 , 0.6288611 ,\n",
       "        0.62773629, 0.62703648, 0.6053243 , 0.6174446 , 0.61471227,\n",
       "        0.60454413, 0.5943481 , 0.58655848, 0.58966205, 0.58641447,\n",
       "        0.58520068, 0.63746273, 0.62929282, 0.62691138, 0.61867061,\n",
       "        0.62101673, 0.6236922 , 0.60212656, 0.60321949, 0.61636147,\n",
       "        0.60144457, 0.60031869, 0.60068119, 0.62170772, 0.62256312,\n",
       "        0.63548783, 0.59534093, 0.61572946, 0.61782413, 0.59365145,\n",
       "        0.59098013, 0.60242564, 0.5750599 , 0.57390905, 0.5976386 ,\n",
       "        0.62560965, 0.62483666, 0.62613969, 0.60500238, 0.60477871,\n",
       "        0.60688042, 0.61021585, 0.60252011, 0.58489648, 0.58223025,\n",
       "        0.58629157, 0.57349741, 0.63542829, 0.62252611, 0.6267999 ,\n",
       "        0.60356554, 0.61047703, 0.61716277, 0.60316247, 0.59080088,\n",
       "        0.60009131, 0.58903457, 0.58106756, 0.58616477, 0.63865496,\n",
       "        0.62651849, 0.63108723, 0.61527097, 0.61498122, 0.60415994,\n",
       "        0.60566481, 0.59971151, 0.60609317, 0.58435753, 0.58440679,\n",
       "        0.59635597, 0.63552335, 0.26065618, 0.63708494, 0.63854543,\n",
       "        0.38751517, 0.63718755, 0.64068584, 0.31121732, 0.63477214,\n",
       "        0.6344808 , 0.10198974, 0.63864147, 0.63752479, 0.21646533,\n",
       "        0.6395585 , 0.63760107, 0.31466841, 0.63518788, 0.63856197,\n",
       "        0.42433549, 0.63900463, 0.64181179, 0.20802956, 0.63977814,\n",
       "        0.63583937, 0.14616067, 0.64073053, 0.63925135, 0.31686899,\n",
       "        0.63753222, 0.63711672, 0.21837852, 0.63847944, 0.63980882,\n",
       "        0.48961676, 0.6372815 , 0.63671663, 0.30629635, 0.63812713,\n",
       "        0.63691462, 0.00276524, 0.63691753, 0.64229911, 0.16442065,\n",
       "        0.63961779, 0.6396478 , 0.35060895, 0.63946949, 0.63532563,\n",
       "        0.45586194, 0.63644715, 0.64145966, 0.38708819, 0.64054782,\n",
       "        0.63361368, 0.33663584, 0.64192584, 0.63490645, 0.43464897,\n",
       "        0.63543329, 0.63387322, 0.42809554, 0.63277861, 0.63276465,\n",
       "        0.43473742, 0.63697508, 0.63238751, 0.44787708, 0.63653389,\n",
       "        0.63931557, 0.50382762, 0.63916698, 0.63151833, 0.27564385,\n",
       "        0.63025893, 0.63533686, 0.35504922, 0.63839348, 0.63476454,\n",
       "        0.43795445, 0.63299198, 0.63851878, 0.43188537, 0.635114  ,\n",
       "        0.63693236, 0.44679321, 0.62957883, 0.63788012, 0.26438268,\n",
       "        0.64009493, 0.63633616, 0.41819312, 0.63883799, 0.64001531,\n",
       "        0.37306449, 0.63436183, 0.63780535, 0.64112066, 0.64386624,\n",
       "        0.64094071, 0.64067504, 0.64226448, 0.64110836, 0.64167624,\n",
       "        0.63943759, 0.6421836 , 0.63962456, 0.64150774, 0.63858466,\n",
       "        0.63851629, 0.64004765, 0.64120422, 0.6400054 , 0.6401954 ,\n",
       "        0.63902821, 0.64130924, 0.64016042, 0.6406258 , 0.63979196,\n",
       "        0.64187464, 0.64440443, 0.63870679, 0.6419873 , 0.63929739,\n",
       "        0.63790025, 0.64094661, 0.64117083, 0.64063971, 0.64067344,\n",
       "        0.64058702, 0.64036773, 0.63874229, 0.64052226, 0.63908389,\n",
       "        0.6392123 , 0.64076471, 0.64028472, 0.64218608, 0.64138949,\n",
       "        0.64141899, 0.64069832, 0.64070735, 0.63971237, 0.64065435,\n",
       "        0.63997035, 0.63928271, 0.63820478, 0.63627962, 0.63780703,\n",
       "        0.63777307, 0.63358712, 0.6369602 , 0.63000474, 0.62628561,\n",
       "        0.63234923, 0.6363722 , 0.64116206, 0.63874624, 0.63685874,\n",
       "        0.6396534 , 0.63890246, 0.6385614 , 0.63209874, 0.63310004,\n",
       "        0.63331991, 0.62546443, 0.63015437, 0.62462994, 0.64146507,\n",
       "        0.63941894, 0.63826918, 0.63446396, 0.6369271 , 0.6409085 ,\n",
       "        0.63333369, 0.6341566 , 0.63573869, 0.63194562, 0.63046557,\n",
       "        0.63075462, 0.63965608, 0.63849565, 0.63868529, 0.64071548,\n",
       "        0.63976001, 0.63672364, 0.63464729, 0.64081829, 0.63765353,\n",
       "        0.63667428, 0.63826567, 0.63299209]),\n",
       " 'std_test_jaccard': array([0.02460312, 0.01621742, 0.01598626, 0.01609658, 0.00913586,\n",
       "        0.01880409, 0.01354742, 0.01336589, 0.02228159, 0.0158684 ,\n",
       "        0.01707731, 0.01085495, 0.01122636, 0.01567069, 0.01557561,\n",
       "        0.01597384, 0.00637045, 0.01434159, 0.00952838, 0.01687902,\n",
       "        0.01675679, 0.01475142, 0.01468329, 0.0201575 , 0.01708318,\n",
       "        0.01890383, 0.01388389, 0.00886098, 0.01012953, 0.0221881 ,\n",
       "        0.02310524, 0.01872194, 0.01103783, 0.01757439, 0.01904416,\n",
       "        0.01540833, 0.0202765 , 0.01476313, 0.01369294, 0.01463111,\n",
       "        0.02141049, 0.01340284, 0.01159856, 0.01495208, 0.0091299 ,\n",
       "        0.01500008, 0.01711203, 0.01223425, 0.00577059, 0.02432561,\n",
       "        0.0171121 , 0.01778686, 0.00787692, 0.00960694, 0.0185771 ,\n",
       "        0.02368425, 0.01815976, 0.0185532 , 0.02121632, 0.02038913,\n",
       "        0.01299893, 0.01332572, 0.0071118 , 0.00642545, 0.01692235,\n",
       "        0.0183317 , 0.01576849, 0.0101705 , 0.01787564, 0.01183666,\n",
       "        0.01652672, 0.02369997, 0.01298254, 0.01060445, 0.0126834 ,\n",
       "        0.01307035, 0.01430175, 0.00368119, 0.01673149, 0.01311735,\n",
       "        0.01627937, 0.01692343, 0.02128554, 0.00996928, 0.00923149,\n",
       "        0.00895289, 0.01310443, 0.01804807, 0.01355652, 0.02199348,\n",
       "        0.02431757, 0.01454866, 0.01595461, 0.01164149, 0.01427473,\n",
       "        0.01787745, 0.01468519, 0.24850365, 0.01155751, 0.01100652,\n",
       "        0.15681974, 0.01138018, 0.01359845, 0.25435568, 0.01478135,\n",
       "        0.01159938, 0.20200706, 0.01071592, 0.01117012, 0.21812408,\n",
       "        0.01249587, 0.00918438, 0.19041613, 0.01011467, 0.01058489,\n",
       "        0.13279984, 0.01434948, 0.01254211, 0.25005188, 0.01218345,\n",
       "        0.01557204, 0.19859841, 0.01160341, 0.01437686, 0.25812606,\n",
       "        0.01398273, 0.01497895, 0.24887292, 0.00927863, 0.01203743,\n",
       "        0.05517551, 0.01408704, 0.01292931, 0.18942523, 0.01549769,\n",
       "        0.01313107, 0.00460972, 0.01488734, 0.0146043 , 0.18686771,\n",
       "        0.01191954, 0.01294771, 0.17962142, 0.0138042 , 0.01150513,\n",
       "        0.02995557, 0.02132414, 0.01119394, 0.13735888, 0.01221735,\n",
       "        0.01405503, 0.21053821, 0.01232456, 0.01204282, 0.09348942,\n",
       "        0.01692386, 0.01687618, 0.07348476, 0.0141392 , 0.01579466,\n",
       "        0.11623431, 0.01325438, 0.01168393, 0.0583195 , 0.01486878,\n",
       "        0.01599075, 0.03348416, 0.01525214, 0.01706963, 0.17832268,\n",
       "        0.01580614, 0.01552473, 0.09750455, 0.02001125, 0.00985256,\n",
       "        0.05962528, 0.01623301, 0.01908317, 0.18017473, 0.01978156,\n",
       "        0.01236766, 0.12514726, 0.01703118, 0.01621111, 0.0965406 ,\n",
       "        0.01912377, 0.01185645, 0.14700021, 0.01288464, 0.01305349,\n",
       "        0.09587286, 0.01360246, 0.01685038, 0.01714663, 0.01978154,\n",
       "        0.01798261, 0.01820179, 0.01908466, 0.01944463, 0.02012296,\n",
       "        0.01910534, 0.01797713, 0.01919257, 0.0197829 , 0.01874834,\n",
       "        0.02044941, 0.01927028, 0.01779238, 0.01668258, 0.01888692,\n",
       "        0.01831339, 0.01806966, 0.01928363, 0.01854457, 0.0185035 ,\n",
       "        0.01946595, 0.01765865, 0.01914041, 0.01780296, 0.01873162,\n",
       "        0.01845276, 0.01719306, 0.01796703, 0.02051381, 0.02057915,\n",
       "        0.01836899, 0.01853192, 0.0180409 , 0.01820253, 0.01548643,\n",
       "        0.01611206, 0.01740696, 0.01793045, 0.01779914, 0.01769677,\n",
       "        0.01983462, 0.01803665, 0.01876306, 0.01935388, 0.01750791,\n",
       "        0.01473381, 0.01822623, 0.01485679, 0.01379748, 0.01882386,\n",
       "        0.01864874, 0.01892963, 0.01089609, 0.01223067, 0.01528345,\n",
       "        0.01421223, 0.01462423, 0.01729742, 0.01861792, 0.01709938,\n",
       "        0.01929955, 0.02225525, 0.0157256 , 0.01283768, 0.01336524,\n",
       "        0.01868426, 0.01606963, 0.01365084, 0.0117245 , 0.01556646,\n",
       "        0.01689957, 0.01764734, 0.01728602, 0.01693613, 0.01722209,\n",
       "        0.01487324, 0.01543801, 0.0180097 , 0.01145357, 0.01246349,\n",
       "        0.01088195, 0.01568622, 0.01793167, 0.02055189, 0.01356792,\n",
       "        0.01709387, 0.0147582 , 0.01177893, 0.01712119, 0.01454615,\n",
       "        0.01715295, 0.01457427, 0.01592811]),\n",
       " 'rank_test_jaccard': array([198, 144, 182, 197, 191, 185, 224, 226, 237, 253, 231, 245, 168,\n",
       "        187, 167, 222, 207, 199, 234, 225, 238, 251, 249, 244, 165, 166,\n",
       "        169, 205, 189, 196, 209, 229, 239, 235, 240, 243, 103, 164, 170,\n",
       "        186, 184, 179, 216, 212, 192, 217, 219, 218, 183, 180, 126, 228,\n",
       "        193, 188, 230, 232, 215, 254, 255, 223, 175, 177, 174, 206, 208,\n",
       "        202, 201, 214, 246, 250, 241, 256, 128, 181, 171, 211, 200, 190,\n",
       "        213, 233, 220, 236, 252, 242,  79, 172, 157, 194, 195, 210, 204,\n",
       "        221, 203, 248, 247, 227, 125, 281, 107,  84, 269, 105,  32, 277,\n",
       "        134, 137, 287,  80, 102, 283,  60, 100, 276, 131,  82, 267,  72,\n",
       "         10, 284,  52, 123, 286,  28,  67, 275, 101, 106, 282,  88,  50,\n",
       "        258, 104, 116, 278,  93, 113, 288, 112,   3, 285,  59,  57, 273,\n",
       "         61, 130, 259, 119,  14, 270,  39, 142, 274,   8, 133, 264, 127,\n",
       "        141, 266, 150, 151, 263, 108, 152, 260, 118,  64, 257,  69, 156,\n",
       "        279, 160, 129, 272,  89, 135, 262, 149,  85, 265, 132, 110, 261,\n",
       "        163,  95, 280,  45, 121, 268,  74,  47, 271, 139,  97,  21,   2,\n",
       "         24,  33,   4,  22,  11,  62,   6,  58,  12,  81,  86,  46,  18,\n",
       "         48,  43,  71,  17,  44,  37,  51,   9,   1,  77,   7,  65,  94,\n",
       "         23,  19,  36,  34,  38,  41,  76,  40,  70,  68,  27,  42,   5,\n",
       "         16,  15,  31,  30,  54,  35,  49,  66,  92, 122,  96,  98, 143,\n",
       "        109, 162, 173, 153, 120,  20,  75, 114,  56,  73,  83, 154, 147,\n",
       "        146, 176, 161, 178,  13,  63,  90, 138, 111,  25, 145, 140, 124,\n",
       "        155, 159, 158,  55,  87,  78,  29,  53, 115, 136,  26,  99, 117,\n",
       "         91, 148]),\n",
       " 'split0_test_neg_log_loss': array([-0.48452451, -0.47287293, -0.48403131, -0.51529947, -0.52855604,\n",
       "        -0.51361344, -0.56321759, -0.5482716 , -0.63447286, -0.71876317,\n",
       "        -0.57837009, -0.60652403, -0.47158898, -0.46641579, -0.48719916,\n",
       "        -0.5339195 , -0.54432226, -0.52837015, -0.58552265, -0.54246474,\n",
       "        -0.59084675, -0.60410222, -0.60941101, -0.57765783, -0.49398139,\n",
       "        -0.48030303, -0.48903326, -0.50283733, -0.49804964, -0.50969411,\n",
       "        -0.57145828, -0.55696575, -0.57742614, -0.58662795, -0.56248246,\n",
       "        -0.55304337, -0.47574725, -0.48810918, -0.47938389, -0.49439556,\n",
       "        -0.49482092, -0.48440106, -0.53544834, -0.53144692, -0.53346083,\n",
       "        -0.5460981 , -0.54644424, -0.54667348, -0.47353991, -0.47989645,\n",
       "        -0.47375246, -0.53337871, -0.52801491, -0.51321947, -0.5963313 ,\n",
       "        -0.65553672, -0.58014681, -0.70064769, -0.74475706, -0.72077163,\n",
       "        -0.47154746, -0.46941955, -0.47579153, -0.53155975, -0.52782531,\n",
       "        -0.53425632, -0.56174734, -0.5633624 , -0.64440053, -0.67444879,\n",
       "        -0.60800496, -0.70411776, -0.48008189, -0.48025284, -0.472345  ,\n",
       "        -0.53822333, -0.52141062, -0.50281327, -0.56813217, -0.58849819,\n",
       "        -0.58059602, -0.69409423, -0.60098474, -0.62877193, -0.46961992,\n",
       "        -0.46974862, -0.46895627, -0.48966399, -0.52873271, -0.53051192,\n",
       "        -0.54205591, -0.54032311, -0.56051985, -0.62633018, -0.58276198,\n",
       "        -0.58947315, -0.46820105, -0.66002495, -0.46946627, -0.46664691,\n",
       "        -0.74173904, -0.46561   , -0.46576889, -0.73900564, -0.4621629 ,\n",
       "        -0.46290501, -0.68929092, -0.46365655, -0.46723835, -0.7492584 ,\n",
       "        -0.47227686, -0.4673741 , -0.66285627, -0.46722429, -0.46316405,\n",
       "        -0.6783462 , -0.46610626, -0.46297716, -0.69054106, -0.46237898,\n",
       "        -0.47040561, -0.74686805, -0.47084307, -0.46796768, -0.74737828,\n",
       "        -0.46467555, -0.46560942, -0.6679087 , -0.4623311 , -0.46278329,\n",
       "        -0.69687141, -0.46321305, -0.47043271, -0.75587996, -0.47335671,\n",
       "        -0.47040396, -0.71242167, -0.46520968, -0.46683927, -0.71248344,\n",
       "        -0.46278713, -0.46236583, -0.71404022, -0.4632855 , -0.47403673,\n",
       "        -0.69224545, -0.47029977, -0.46710129, -0.61307741, -0.46883061,\n",
       "        -0.46596643, -0.74848155, -0.46362954, -0.46634286, -0.67175952,\n",
       "        -0.47101713, -0.46871168, -0.69529715, -0.48073977, -0.47572414,\n",
       "        -0.75763006, -0.46498226, -0.46859612, -0.68985222, -0.46946805,\n",
       "        -0.4647191 , -0.59840393, -0.46214833, -0.48055861, -0.80638818,\n",
       "        -0.47370832, -0.47789742, -0.67673718, -0.46798291, -0.46939099,\n",
       "        -0.64891086, -0.4720254 , -0.470005  , -0.63403705, -0.46893603,\n",
       "        -0.47747933, -0.76118555, -0.47245813, -0.46511184, -0.76471068,\n",
       "        -0.46978829, -0.46341783, -0.59739826, -0.46413804, -0.46585637,\n",
       "        -0.87333374, -0.46651521, -0.46324114, -0.46465098, -0.46399132,\n",
       "        -0.46162465, -0.46135367, -0.46054938, -0.46086048, -0.46052979,\n",
       "        -0.46071022, -0.45977253, -0.45977199, -0.46067242, -0.46327201,\n",
       "        -0.46473141, -0.46321079, -0.46135539, -0.46088655, -0.46075968,\n",
       "        -0.46121076, -0.46210638, -0.46026295, -0.45982993, -0.45890089,\n",
       "        -0.4594777 , -0.46287665, -0.46334142, -0.46422148, -0.46173897,\n",
       "        -0.46214707, -0.4617287 , -0.46095661, -0.4604937 , -0.46131596,\n",
       "        -0.46133208, -0.46053272, -0.46037557, -0.46337505, -0.46335288,\n",
       "        -0.46408997, -0.46117338, -0.46084681, -0.46129598, -0.46009596,\n",
       "        -0.45983289, -0.45988388, -0.45964518, -0.45975831, -0.4594051 ,\n",
       "        -0.46821726, -0.46075583, -0.45998882, -0.46062307, -0.47046486,\n",
       "        -0.46196912, -0.46920033, -0.46704176, -0.47192495, -0.474378  ,\n",
       "        -0.47358582, -0.46915408, -0.46382101, -0.46597131, -0.46424026,\n",
       "        -0.47082546, -0.46429593, -0.46369216, -0.46958208, -0.47256095,\n",
       "        -0.47167433, -0.47152855, -0.46850376, -0.47980141, -0.46656262,\n",
       "        -0.46296176, -0.46527039, -0.47031887, -0.47026445, -0.47013358,\n",
       "        -0.47301898, -0.4744366 , -0.47560101, -0.46984781, -0.47136412,\n",
       "        -0.46806324, -0.46430922, -0.46359502, -0.46182475, -0.46269085,\n",
       "        -0.4632352 , -0.46019926, -0.47036497, -0.46179408, -0.46147344,\n",
       "        -0.46778658, -0.46878516, -0.46633724]),\n",
       " 'split1_test_neg_log_loss': array([-0.49411012, -0.48497741, -0.48404366, -0.50595817, -0.53440536,\n",
       "        -0.57190077, -0.61201404, -0.59660737, -0.57765735, -0.69441616,\n",
       "        -0.67704185, -0.68490558, -0.48627251, -0.48728928, -0.47581815,\n",
       "        -0.53461531, -0.53072244, -0.52500964, -0.58173282, -0.60025806,\n",
       "        -0.60983553, -0.6522666 , -0.6654996 , -0.60015874, -0.48934189,\n",
       "        -0.49763843, -0.48598475, -0.53016786, -0.51942339, -0.53441806,\n",
       "        -0.57682666, -0.57872247, -0.57221101, -0.61772001, -0.61269523,\n",
       "        -0.64908989, -0.49689953, -0.48401331, -0.47824054, -0.51190524,\n",
       "        -0.50321708, -0.51520739, -0.55818585, -0.54975066, -0.53557376,\n",
       "        -0.60995642, -0.58694219, -0.56505223, -0.48353271, -0.48178462,\n",
       "        -0.50011544, -0.55202665, -0.56104789, -0.52219553, -0.58092687,\n",
       "        -0.61456097, -0.66730986, -0.68331796, -0.68260579, -0.64555873,\n",
       "        -0.49162082, -0.48917341, -0.48268467, -0.54075533, -0.63157701,\n",
       "        -0.51389205, -0.57026094, -0.61475461, -0.64926852, -0.61741454,\n",
       "        -0.72606135, -0.66616285, -0.48224086, -0.48570486, -0.49012808,\n",
       "        -0.5081271 , -0.52445363, -0.51312062, -0.61114387, -0.55474354,\n",
       "        -0.57994087, -0.64952182, -0.69493314, -0.731821  , -0.49003957,\n",
       "        -0.47849578, -0.49206537, -0.52238451, -0.51415655, -0.53858375,\n",
       "        -0.55618668, -0.60537613, -0.57027651, -0.61783591, -0.59476504,\n",
       "        -0.64651153, -0.47301762, -0.69549802, -0.47608047, -0.47099997,\n",
       "        -0.6846492 , -0.47112099, -0.4660524 , -0.70996495, -0.46994191,\n",
       "        -0.46772199, -0.69453305, -0.46921273, -0.47698884, -0.6907599 ,\n",
       "        -0.47078724, -0.46800427, -0.67643839, -0.47298707, -0.46762775,\n",
       "        -0.67204789, -0.46663906, -0.46660811, -0.71006577, -0.4671142 ,\n",
       "        -0.47184523, -0.71305223, -0.4744275 , -0.46937867, -0.70066672,\n",
       "        -0.46682693, -0.46726397, -0.71179087, -0.46524027, -0.46711498,\n",
       "        -0.67344596, -0.46644228, -0.47100625, -0.83646504, -0.47482569,\n",
       "        -0.46746249, -0.72176645, -0.46892505, -0.46681356, -0.68731681,\n",
       "        -0.46628367, -0.4675427 , -0.67979445, -0.46516965, -0.46989979,\n",
       "        -0.68483294, -0.47991463, -0.46827049, -0.7356397 , -0.47216354,\n",
       "        -0.47346765, -0.62997388, -0.46689326, -0.47134021, -0.60962811,\n",
       "        -0.46126341, -0.48193005, -0.67445095, -0.46409127, -0.46130469,\n",
       "        -0.63471108, -0.46816616, -0.47235755, -0.73526625, -0.46735561,\n",
       "        -0.46719167, -0.55591397, -0.46299722, -0.47115257, -0.70157356,\n",
       "        -0.47949402, -0.46542325, -0.83766633, -0.46871796, -0.47261823,\n",
       "        -0.62923843, -0.47073972, -0.46916343, -0.62669891, -0.47005418,\n",
       "        -0.47758588, -0.62545571, -0.47272266, -0.47609263, -0.80303237,\n",
       "        -0.46587421, -0.46833409, -0.71263664, -0.47025821, -0.46673169,\n",
       "        -0.65672264, -0.47191571, -0.46957964, -0.46642548, -0.46679429,\n",
       "        -0.46503462, -0.46519993, -0.46462319, -0.46423818, -0.46425629,\n",
       "        -0.46431288, -0.4639852 , -0.46319169, -0.46283942, -0.46722291,\n",
       "        -0.46726879, -0.46577239, -0.46548326, -0.46475331, -0.46421047,\n",
       "        -0.46416364, -0.46409939, -0.46415656, -0.46395997, -0.46383654,\n",
       "        -0.46323495, -0.46734649, -0.46620602, -0.46483445, -0.46469719,\n",
       "        -0.4651278 , -0.46445317, -0.46448751, -0.46334431, -0.46372402,\n",
       "        -0.4642924 , -0.46390362, -0.46372046, -0.46539523, -0.46623292,\n",
       "        -0.4665794 , -0.46410618, -0.46384296, -0.46395346, -0.46304002,\n",
       "        -0.46315944, -0.46281055, -0.46290303, -0.46328146, -0.46305144,\n",
       "        -0.4727042 , -0.46689427, -0.47390597, -0.47035376, -0.47106509,\n",
       "        -0.47902776, -0.47545949, -0.47827874, -0.47443164, -0.47974107,\n",
       "        -0.47852292, -0.47612236, -0.47624972, -0.46797072, -0.47284317,\n",
       "        -0.46945503, -0.46948566, -0.47252589, -0.47339891, -0.47725945,\n",
       "        -0.46815161, -0.4820374 , -0.47861306, -0.48033993, -0.46973677,\n",
       "        -0.46731004, -0.47084788, -0.47961076, -0.47353782, -0.46848729,\n",
       "        -0.47354003, -0.47731731, -0.4729715 , -0.47998555, -0.47992536,\n",
       "        -0.47597494, -0.47053791, -0.4662568 , -0.46833528, -0.46957103,\n",
       "        -0.47550423, -0.47356046, -0.47246797, -0.46997757, -0.47433996,\n",
       "        -0.47173101, -0.47279494, -0.47532509]),\n",
       " 'split2_test_neg_log_loss': array([-0.45923728, -0.45482828, -0.45841616, -0.52056364, -0.52533453,\n",
       "        -0.47773833, -0.52399618, -0.563943  , -0.53035139, -0.56870144,\n",
       "        -0.60362629, -0.58753097, -0.45667352, -0.45713826, -0.4592382 ,\n",
       "        -0.51285651, -0.51444943, -0.4844484 , -0.55705412, -0.54014825,\n",
       "        -0.53587184, -0.55140017, -0.62833226, -0.54238725, -0.45751   ,\n",
       "        -0.45554763, -0.45411862, -0.50748775, -0.48934999, -0.49363706,\n",
       "        -0.49671783, -0.53276588, -0.5364814 , -0.54823933, -0.61009864,\n",
       "        -0.55741232, -0.45289907, -0.45184654, -0.45603984, -0.48077988,\n",
       "        -0.48465928, -0.4752092 , -0.51524152, -0.51659559, -0.50624132,\n",
       "        -0.51498125, -0.5155005 , -0.54294366, -0.45934142, -0.44964937,\n",
       "        -0.46352622, -0.49915838, -0.52574379, -0.48648389, -0.56900367,\n",
       "        -0.54373435, -0.58642489, -0.58773697, -0.5888939 , -0.5414459 ,\n",
       "        -0.46418863, -0.46388092, -0.46734858, -0.530904  , -0.49556625,\n",
       "        -0.50432123, -0.53840426, -0.5653649 , -0.5938986 , -0.62387106,\n",
       "        -0.57687419, -0.5945086 , -0.45766766, -0.46490523, -0.46526643,\n",
       "        -0.48335237, -0.50408056, -0.50485945, -0.51748397, -0.56030222,\n",
       "        -0.56354291, -0.61170107, -0.55477112, -0.62078499, -0.44615547,\n",
       "        -0.46338308, -0.45462728, -0.49740806, -0.48464549, -0.47816043,\n",
       "        -0.52239672, -0.50983828, -0.53915932, -0.56931671, -0.56619134,\n",
       "        -0.59385337, -0.45945506, -0.69899841, -0.45816402, -0.45504599,\n",
       "        -0.71816976, -0.45390648, -0.45170024, -0.66800931, -0.45090522,\n",
       "        -0.4515148 , -0.72543665, -0.45080059, -0.46020846, -0.71195856,\n",
       "        -0.45601319, -0.45222588, -0.68237782, -0.45182039, -0.45184084,\n",
       "        -0.70707507, -0.44644237, -0.449679  , -0.71325492, -0.44931929,\n",
       "        -0.45600818, -0.70567151, -0.45644177, -0.45384456, -0.69954241,\n",
       "        -0.45289504, -0.44799477, -0.69823516, -0.4511249 , -0.45050863,\n",
       "        -0.70536252, -0.45173352, -0.46169308, -0.69203296, -0.46301291,\n",
       "        -0.45595846, -0.69466723, -0.45386617, -0.45024633, -0.69826381,\n",
       "        -0.45058096, -0.44985108, -0.68657355, -0.44887921, -0.45346809,\n",
       "        -0.66800286, -0.44810779, -0.45410297, -0.65079728, -0.45296948,\n",
       "        -0.44740951, -0.66811363, -0.44676499, -0.45176943, -0.58323383,\n",
       "        -0.44470167, -0.46479445, -0.84840637, -0.45278821, -0.45267419,\n",
       "        -0.62915998, -0.45375694, -0.45435812, -0.65052605, -0.45215285,\n",
       "        -0.44864326, -0.6219609 , -0.44903869, -0.46636126, -0.77081582,\n",
       "        -0.45636306, -0.45848196, -0.61232048, -0.44826788, -0.45658459,\n",
       "        -0.67296594, -0.4517343 , -0.44722065, -0.58671091, -0.44959069,\n",
       "        -0.46654   , -0.50113022, -0.45647041, -0.45051949, -0.78289755,\n",
       "        -0.45131911, -0.45107303, -0.6234991 , -0.45076289, -0.44674292,\n",
       "        -0.64068635, -0.4558874 , -0.44705362, -0.448096  , -0.44669655,\n",
       "        -0.44638974, -0.44392532, -0.44538725, -0.44435003, -0.44394338,\n",
       "        -0.4452454 , -0.44407679, -0.44367718, -0.44356514, -0.44944666,\n",
       "        -0.44691779, -0.44511063, -0.44617905, -0.44688229, -0.44542622,\n",
       "        -0.44555948, -0.44454392, -0.44353426, -0.44354714, -0.44394976,\n",
       "        -0.44431716, -0.44504721, -0.44836843, -0.44700002, -0.44566152,\n",
       "        -0.44617397, -0.44558335, -0.4446818 , -0.44474958, -0.44497959,\n",
       "        -0.44333426, -0.44355419, -0.44567529, -0.45106117, -0.44889928,\n",
       "        -0.44790563, -0.44607416, -0.44610607, -0.44537419, -0.44464477,\n",
       "        -0.44420542, -0.44444339, -0.44361824, -0.44350462, -0.44445361,\n",
       "        -0.44712811, -0.4431554 , -0.44525794, -0.44512293, -0.44187436,\n",
       "        -0.44699491, -0.44705782, -0.44754924, -0.44969096, -0.45446392,\n",
       "        -0.4477502 , -0.45145966, -0.44222943, -0.44602188, -0.445487  ,\n",
       "        -0.4430253 , -0.44507178, -0.4492444 , -0.4461668 , -0.44415577,\n",
       "        -0.44497976, -0.45395161, -0.45413727, -0.45505744, -0.44446109,\n",
       "        -0.44328867, -0.44319977, -0.44272022, -0.4490384 , -0.44082501,\n",
       "        -0.44149908, -0.44459503, -0.44739395, -0.44785318, -0.45118592,\n",
       "        -0.44910353, -0.44330062, -0.44392653, -0.44314991, -0.44357821,\n",
       "        -0.44364756, -0.44446226, -0.4408086 , -0.44126018, -0.44422314,\n",
       "        -0.44380127, -0.44127128, -0.44661561]),\n",
       " 'split3_test_neg_log_loss': array([-0.50062302, -0.49644709, -0.47599375, -0.54047907, -0.52233266,\n",
       "        -0.50534347, -0.61004703, -0.56415299, -0.56181483, -0.60567436,\n",
       "        -0.64575993, -0.62491859, -0.48089167, -0.49546894, -0.48431848,\n",
       "        -0.54197231, -0.52862285, -0.56271719, -0.58110175, -0.55978355,\n",
       "        -0.56112556, -0.60395646, -0.70364533, -0.59946321, -0.50192159,\n",
       "        -0.49139882, -0.4832087 , -0.55331262, -0.51179733, -0.50880786,\n",
       "        -0.58323505, -0.54969307, -0.59733426, -0.62637437, -0.60813727,\n",
       "        -0.65849607, -0.48199894, -0.48527449, -0.49247243, -0.51778739,\n",
       "        -0.51540163, -0.51585421, -0.53729886, -0.54941703, -0.52296275,\n",
       "        -0.58217222, -0.56069524, -0.57395437, -0.47587177, -0.49501385,\n",
       "        -0.47878437, -0.5303896 , -0.49843667, -0.50930854, -0.59910644,\n",
       "        -0.63187152, -0.54903234, -0.62409007, -0.59892519, -0.64226351,\n",
       "        -0.50339743, -0.47851804, -0.47482327, -0.51828783, -0.55187265,\n",
       "        -0.50729255, -0.606412  , -0.60301806, -0.54999949, -0.68170533,\n",
       "        -0.67490735, -0.75260572, -0.47799944, -0.48327217, -0.48157962,\n",
       "        -0.53284608, -0.54437451, -0.50104423, -0.58866003, -0.52643674,\n",
       "        -0.53358802, -0.62144641, -0.62981114, -0.63343188, -0.47543827,\n",
       "        -0.46520376, -0.48306047, -0.51824237, -0.51945747, -0.53259157,\n",
       "        -0.55916391, -0.5782898 , -0.52116613, -0.6231909 , -0.63780319,\n",
       "        -0.58186219, -0.47981912, -0.72645255, -0.48014991, -0.47349328,\n",
       "        -0.6689377 , -0.47151431, -0.47543892, -0.67880808, -0.47063914,\n",
       "        -0.47009941, -0.69344806, -0.46965505, -0.47798856, -0.69021688,\n",
       "        -0.47697074, -0.4706362 , -0.65282051, -0.47477998, -0.47128052,\n",
       "        -0.7125921 , -0.47455592, -0.4686237 , -0.67814617, -0.46766135,\n",
       "        -0.47729502, -0.70618943, -0.47812854, -0.4753131 , -0.66071494,\n",
       "        -0.47349834, -0.47077009, -0.70146324, -0.47086308, -0.47042087,\n",
       "        -0.68870594, -0.4694396 , -0.4795387 , -0.69410309, -0.47571943,\n",
       "        -0.47154929, -0.68384578, -0.47461433, -0.47160549, -0.71315666,\n",
       "        -0.46919935, -0.46941743, -0.7077738 , -0.46933145, -0.4854653 ,\n",
       "        -0.6310036 , -0.47736855, -0.47693103, -0.59016398, -0.47440677,\n",
       "        -0.47271156, -0.60292129, -0.47296373, -0.47720894, -0.6341509 ,\n",
       "        -0.4727871 , -0.47831384, -0.68122267, -0.48311807, -0.47455309,\n",
       "        -0.67250159, -0.47048894, -0.47878484, -0.61869646, -0.47412916,\n",
       "        -0.47670252, -0.62326741, -0.47407472, -0.48170112, -1.0017559 ,\n",
       "        -0.47523092, -0.4703542 , -0.89459983, -0.48056485, -0.47711457,\n",
       "        -0.65357646, -0.48005478, -0.47152092, -0.61416009, -0.47493813,\n",
       "        -0.47990798, -0.72642475, -0.48159682, -0.47493808, -0.68990283,\n",
       "        -0.48194542, -0.47206983, -0.69904408, -0.47266406, -0.47621536,\n",
       "        -0.78766258, -0.47553971, -0.47442919, -0.47343474, -0.47078146,\n",
       "        -0.47041827, -0.46933327, -0.47077797, -0.46880565, -0.46826176,\n",
       "        -0.4693552 , -0.46809859, -0.46896005, -0.46846019, -0.47147434,\n",
       "        -0.47418994, -0.47337737, -0.47178347, -0.47106923, -0.46890631,\n",
       "        -0.46802969, -0.46920649, -0.46792381, -0.46830466, -0.46776605,\n",
       "        -0.46770982, -0.47209563, -0.47225889, -0.47218967, -0.46986229,\n",
       "        -0.46998409, -0.47048843, -0.4705127 , -0.46819729, -0.46894074,\n",
       "        -0.46822309, -0.46907349, -0.46877873, -0.47357571, -0.47268245,\n",
       "        -0.47445089, -0.46976701, -0.46967027, -0.4702208 , -0.46922936,\n",
       "        -0.46802215, -0.4687029 , -0.46865406, -0.46779554, -0.46768156,\n",
       "        -0.47473668, -0.46944916, -0.46968691, -0.47133816, -0.4751556 ,\n",
       "        -0.46859334, -0.47407164, -0.47295059, -0.46761784, -0.47858735,\n",
       "        -0.47148049, -0.47336015, -0.47098642, -0.47163491, -0.46660331,\n",
       "        -0.46941675, -0.46817102, -0.47288046, -0.4739461 , -0.47030768,\n",
       "        -0.47654006, -0.46804582, -0.46710604, -0.46754366, -0.46647484,\n",
       "        -0.46931657, -0.47276491, -0.46808886, -0.46946439, -0.46698501,\n",
       "        -0.47047797, -0.47094613, -0.46430321, -0.46797128, -0.46558924,\n",
       "        -0.47519142, -0.46679605, -0.46836631, -0.46936895, -0.46579717,\n",
       "        -0.46372335, -0.46891621, -0.46733933, -0.46416921, -0.45981361,\n",
       "        -0.47298094, -0.4689976 , -0.46651302]),\n",
       " 'split4_test_neg_log_loss': array([-0.49883411, -0.4885406 , -0.49092435, -0.52934338, -0.51168741,\n",
       "        -0.50609675, -0.54383019, -0.76074665, -0.65004822, -0.61216164,\n",
       "        -0.60402149, -0.65282765, -0.49891745, -0.48622817, -0.48764825,\n",
       "        -0.5630593 , -0.5285254 , -0.52493743, -0.59727761, -0.58321004,\n",
       "        -0.5947356 , -0.6235169 , -0.6373706 , -0.67300559, -0.49006779,\n",
       "        -0.49356143, -0.4963879 , -0.53129236, -0.49982832, -0.52411514,\n",
       "        -0.56248499, -0.60859352, -0.57319936, -0.65027389, -0.61683255,\n",
       "        -0.63051277, -0.4771265 , -0.48174792, -0.49880184, -0.53813102,\n",
       "        -0.52755131, -0.52619999, -0.55047596, -0.55032744, -0.53634448,\n",
       "        -0.59062017, -0.5816838 , -0.60509094, -0.47951902, -0.5069284 ,\n",
       "        -0.48211211, -0.6109002 , -0.51215001, -0.53346066, -0.64857725,\n",
       "        -0.68004906, -0.60938636, -0.69970218, -0.67534895, -0.64260363,\n",
       "        -0.49090817, -0.47892099, -0.47225122, -0.53681257, -0.54593331,\n",
       "        -0.56495061, -0.57953957, -0.58338858, -0.6859926 , -0.58357275,\n",
       "        -0.68374744, -0.72365445, -0.48618172, -0.48501696, -0.48512103,\n",
       "        -0.55066914, -0.53938501, -0.51128719, -0.61474503, -0.58468037,\n",
       "        -0.60146454, -0.66568075, -0.72369254, -0.64386351, -0.48073178,\n",
       "        -0.48875812, -0.47441889, -0.52335797, -0.52949218, -0.53369174,\n",
       "        -0.56861582, -0.57321047, -0.59410986, -0.63478145, -0.6559875 ,\n",
       "        -0.61147954, -0.47112631, -0.77380131, -0.47312043, -0.47162771,\n",
       "        -0.71756719, -0.47364813, -0.46794013, -0.74046839, -0.4687517 ,\n",
       "        -0.47033064, -0.72109915, -0.47022456, -0.47522093, -0.67270633,\n",
       "        -0.47372322, -0.47295868, -0.70418762, -0.47200725, -0.47187281,\n",
       "        -0.69748092, -0.47014891, -0.46768996, -0.78702582, -0.46827968,\n",
       "        -0.47385036, -0.72593913, -0.47717581, -0.47200983, -0.71442666,\n",
       "        -0.47032983, -0.47054629, -0.66946869, -0.46962352, -0.46935029,\n",
       "        -0.67032128, -0.46945474, -0.47581504, -0.66690606, -0.47598617,\n",
       "        -0.47454207, -0.74450311, -0.47214343, -0.46934233, -0.71736871,\n",
       "        -0.47019664, -0.47071538, -0.69121416, -0.46855506, -0.47408601,\n",
       "        -0.77846954, -0.4769706 , -0.47817187, -0.79464131, -0.47420116,\n",
       "        -0.47495713, -0.75089305, -0.4707795 , -0.46994647, -0.67663872,\n",
       "        -0.46916366, -0.47787349, -0.75595866, -0.47702601, -0.47110776,\n",
       "        -0.59569523, -0.47162238, -0.47013722, -0.65459924, -0.47626574,\n",
       "        -0.46827247, -0.60886315, -0.47974438, -0.47739532, -0.58529519,\n",
       "        -0.48204449, -0.47153169, -0.67755657, -0.47278202, -0.47729734,\n",
       "        -0.6802519 , -0.47067264, -0.47111489, -0.69658787, -0.46786153,\n",
       "        -0.47232114, -0.73064235, -0.4741445 , -0.47722448, -0.70535552,\n",
       "        -0.47462446, -0.47786916, -0.77230724, -0.47563347, -0.46956515,\n",
       "        -0.69105098, -0.47252351, -0.47185065, -0.47301257, -0.47547807,\n",
       "        -0.46894404, -0.47050138, -0.47011248, -0.46960393, -0.46783878,\n",
       "        -0.46841752, -0.46851578, -0.46888596, -0.46872025, -0.4713177 ,\n",
       "        -0.47150288, -0.47267491, -0.46950815, -0.46931147, -0.46973137,\n",
       "        -0.47070663, -0.46840992, -0.46883805, -0.46756881, -0.46959562,\n",
       "        -0.46816867, -0.47097755, -0.47136372, -0.47163575, -0.46932161,\n",
       "        -0.47072945, -0.46905371, -0.46902848, -0.46945862, -0.46914208,\n",
       "        -0.46738545, -0.46767269, -0.46929661, -0.47112016, -0.47093163,\n",
       "        -0.47175709, -0.46962918, -0.4687243 , -0.46916394, -0.46834826,\n",
       "        -0.46855949, -0.4682061 , -0.46847952, -0.46753028, -0.46746841,\n",
       "        -0.47318198, -0.47422024, -0.47432887, -0.47765681, -0.47050505,\n",
       "        -0.47437207, -0.47996558, -0.47171366, -0.47483593, -0.48184549,\n",
       "        -0.48323017, -0.4784364 , -0.47218477, -0.47388909, -0.47192881,\n",
       "        -0.47786117, -0.4756732 , -0.47866171, -0.47960763, -0.4860217 ,\n",
       "        -0.47953049, -0.4879387 , -0.4824715 , -0.47959674, -0.47218602,\n",
       "        -0.47637468, -0.47257419, -0.47776592, -0.47103453, -0.47859765,\n",
       "        -0.47661106, -0.47905377, -0.48124002, -0.48327296, -0.48018307,\n",
       "        -0.47734946, -0.47203075, -0.47216572, -0.46802733, -0.47156428,\n",
       "        -0.47591631, -0.47432645, -0.4761173 , -0.478952  , -0.47340115,\n",
       "        -0.47910125, -0.47295966, -0.47407662]),\n",
       " 'mean_test_neg_log_loss': array([-0.48746581, -0.47953326, -0.47868185, -0.52232875, -0.5244632 ,\n",
       "        -0.51493855, -0.570621  , -0.60674432, -0.59086893, -0.63994335,\n",
       "        -0.62176393, -0.63134137, -0.47886883, -0.47850809, -0.47884445,\n",
       "        -0.53728459, -0.52932848, -0.52509656, -0.58053779, -0.56517293,\n",
       "        -0.57848305, -0.60704847, -0.64885176, -0.59853453, -0.48656453,\n",
       "        -0.48368987, -0.48174665, -0.52501958, -0.50368973, -0.51413445,\n",
       "        -0.55814456, -0.56534814, -0.57133043, -0.60584711, -0.60204923,\n",
       "        -0.60971088, -0.47693426, -0.47819829, -0.48098771, -0.50859982,\n",
       "        -0.50513004, -0.50337437, -0.53933011, -0.53950753, -0.52691663,\n",
       "        -0.56876563, -0.55825319, -0.56674294, -0.47436097, -0.48265454,\n",
       "        -0.47965812, -0.54517071, -0.52507865, -0.51293362, -0.59878911,\n",
       "        -0.62515052, -0.59846005, -0.65909897, -0.65810618, -0.63852868,\n",
       "        -0.4843325 , -0.47598258, -0.47457985, -0.5316639 , -0.55055491,\n",
       "        -0.52494255, -0.57127282, -0.58597771, -0.62471195, -0.63620249,\n",
       "        -0.65391906, -0.68820988, -0.47683432, -0.47983041, -0.47888804,\n",
       "        -0.5226436 , -0.52674087, -0.50662495, -0.58003301, -0.56293221,\n",
       "        -0.57182647, -0.64848886, -0.64083854, -0.65173466, -0.472397  ,\n",
       "        -0.47311787, -0.47462565, -0.51021138, -0.51529688, -0.52270788,\n",
       "        -0.54968381, -0.56140756, -0.55704634, -0.61429103, -0.60750181,\n",
       "        -0.60463596, -0.47032383, -0.71095505, -0.47139622, -0.46756277,\n",
       "        -0.70621258, -0.46715998, -0.46538011, -0.70725127, -0.46448017,\n",
       "        -0.46451437, -0.70476156, -0.4647099 , -0.47152903, -0.70298001,\n",
       "        -0.46995425, -0.46623983, -0.67573612, -0.4677638 , -0.46515719,\n",
       "        -0.69350844, -0.4647785 , -0.46311559, -0.71580675, -0.4629507 ,\n",
       "        -0.46988088, -0.71954407, -0.47140334, -0.46770277, -0.7045458 ,\n",
       "        -0.46564514, -0.46443691, -0.68977333, -0.46383658, -0.46403561,\n",
       "        -0.68694142, -0.46405664, -0.47169716, -0.72907742, -0.47258018,\n",
       "        -0.46798326, -0.71144085, -0.46695173, -0.4649694 , -0.70571789,\n",
       "        -0.46380955, -0.46397848, -0.69587924, -0.46304418, -0.47139118,\n",
       "        -0.69091088, -0.47053227, -0.46891553, -0.67686394, -0.46851431,\n",
       "        -0.46690246, -0.68007668, -0.4642062 , -0.46732158, -0.63508222,\n",
       "        -0.46378659, -0.4743247 , -0.73106716, -0.47155267, -0.46707277,\n",
       "        -0.65793959, -0.46580334, -0.46884677, -0.66978805, -0.46787428,\n",
       "        -0.4651058 , -0.60168187, -0.46560067, -0.47543378, -0.77316573,\n",
       "        -0.47336816, -0.4687377 , -0.73977608, -0.46766312, -0.47060114,\n",
       "        -0.65698872, -0.46904537, -0.46580498, -0.63163897, -0.46627611,\n",
       "        -0.47476687, -0.66896772, -0.4714785 , -0.4687773 , -0.74917979,\n",
       "        -0.4687103 , -0.46655279, -0.68097706, -0.46669133, -0.4650223 ,\n",
       "        -0.72989126, -0.46847631, -0.46523085, -0.46512395, -0.46474834,\n",
       "        -0.46248226, -0.46206271, -0.46229005, -0.46157165, -0.460966  ,\n",
       "        -0.46160824, -0.46088978, -0.46089737, -0.46085148, -0.46454672,\n",
       "        -0.46492216, -0.46402922, -0.46286186, -0.46258057, -0.46180681,\n",
       "        -0.46193404, -0.46167322, -0.46094313, -0.4606421 , -0.46080977,\n",
       "        -0.46058166, -0.46366871, -0.4643077 , -0.46397627, -0.46225632,\n",
       "        -0.46283247, -0.46226147, -0.46193342, -0.4612487 , -0.46162048,\n",
       "        -0.46091345, -0.46094734, -0.46156933, -0.46490546, -0.46441983,\n",
       "        -0.4649566 , -0.46214998, -0.46183808, -0.46200167, -0.46107167,\n",
       "        -0.46075588, -0.46080936, -0.46066001, -0.46037404, -0.46041203,\n",
       "        -0.46719365, -0.46289498, -0.4646337 , -0.46501894, -0.46581299,\n",
       "        -0.46619144, -0.46915097, -0.4675068 , -0.46770026, -0.47380317,\n",
       "        -0.47091392, -0.46970653, -0.46509427, -0.46509758, -0.46422051,\n",
       "        -0.46611674, -0.46453952, -0.46740092, -0.46854031, -0.47006111,\n",
       "        -0.46817525, -0.47270042, -0.47016633, -0.47246784, -0.46388427,\n",
       "        -0.46385034, -0.46493143, -0.46770093, -0.46666792, -0.46500571,\n",
       "        -0.46702942, -0.46926977, -0.46830194, -0.46978616, -0.46964954,\n",
       "        -0.46913652, -0.46339491, -0.46286208, -0.46214124, -0.46264031,\n",
       "        -0.46440533, -0.46429293, -0.46541964, -0.46323061, -0.46265026,\n",
       "        -0.46708021, -0.46496173, -0.46577352]),\n",
       " 'std_test_neg_log_loss': array([0.01517893, 0.01450203, 0.01118185, 0.01182507, 0.00753813,\n",
       "        0.03098564, 0.03525401, 0.07859327, 0.04491002, 0.05692354,\n",
       "        0.03509487, 0.03437234, 0.01418789, 0.01433368, 0.01068491,\n",
       "        0.01613376, 0.0094793 , 0.02481264, 0.0131019 , 0.02335485,\n",
       "        0.02652162, 0.03295713, 0.03282659, 0.04274995, 0.01519869,\n",
       "        0.01519799, 0.01449737, 0.01825248, 0.01063607, 0.01399479,\n",
       "        0.03145756, 0.02617627, 0.01965747, 0.03528202, 0.01999635,\n",
       "        0.04540873, 0.01416615, 0.01333512, 0.01470889, 0.01971364,\n",
       "        0.01508117, 0.01984963, 0.01469165, 0.01349163, 0.01139963,\n",
       "        0.03394294, 0.02586604, 0.02233661, 0.00823735, 0.01918165,\n",
       "        0.01200335, 0.03699199, 0.02087963, 0.0156202 , 0.02717069,\n",
       "        0.04630998, 0.03945377, 0.04534233, 0.05779039, 0.05704199,\n",
       "        0.01434617, 0.00870154, 0.00499858, 0.0075983 , 0.04500681,\n",
       "        0.02256664, 0.0222495 , 0.0203053 , 0.04749189, 0.03690048,\n",
       "        0.05399772, 0.05463004, 0.00995861, 0.00769668, 0.00895313,\n",
       "        0.02402823, 0.01426851, 0.00474806, 0.03549999, 0.02250552,\n",
       "        0.02259186, 0.0298701 , 0.0614956 , 0.04073312, 0.0147398 ,\n",
       "        0.00940522, 0.01270755, 0.01394012, 0.0163716 , 0.02243102,\n",
       "        0.01608323, 0.03304996, 0.02517198, 0.02314849, 0.03391128,\n",
       "        0.02308774, 0.00664375, 0.03785652, 0.0074881 , 0.00664857,\n",
       "        0.02603239, 0.00713868, 0.00768733, 0.02989324, 0.00742507,\n",
       "        0.00702694, 0.01527308, 0.00734466, 0.0068078 , 0.02627177,\n",
       "        0.00726468, 0.0072837 , 0.01757901, 0.00835463, 0.00734842,\n",
       "        0.01583963, 0.00965275, 0.00698603, 0.0378637 , 0.00712833,\n",
       "        0.0073118 , 0.01549474, 0.00789757, 0.00736745, 0.02790512,\n",
       "        0.00705012, 0.00845157, 0.01779533, 0.00705243, 0.00725324,\n",
       "        0.01341202, 0.00657842, 0.00601007, 0.06117393, 0.00487101,\n",
       "        0.00642591, 0.02118271, 0.00726402, 0.00757399, 0.01123399,\n",
       "        0.0070994 , 0.00761388, 0.01294898, 0.00741784, 0.01035184,\n",
       "        0.04861872, 0.01165441, 0.00863768, 0.07693006, 0.00802646,\n",
       "        0.01022327, 0.06049883, 0.00929141, 0.00852792, 0.0358028 ,\n",
       "        0.01032282, 0.00646034, 0.06535662, 0.01145335, 0.00880536,\n",
       "        0.05548411, 0.00643722, 0.00803418, 0.03975435, 0.00847954,\n",
       "        0.00916545, 0.02463409, 0.01062873, 0.00583147, 0.1369551 ,\n",
       "        0.00900626, 0.00648823, 0.10737131, 0.01067753, 0.00760606,\n",
       "        0.01812965, 0.00933038, 0.00932913, 0.03625619, 0.00868673,\n",
       "        0.00480413, 0.09557397, 0.00821265, 0.01009562, 0.04407385,\n",
       "        0.01021572, 0.00906777, 0.06318082, 0.00881581, 0.0098354 ,\n",
       "        0.08801477, 0.00693437, 0.00981544, 0.00919965, 0.0098199 ,\n",
       "        0.00861341, 0.00962686, 0.00924568, 0.00917659, 0.00895822,\n",
       "        0.00874352, 0.00898476, 0.00929397, 0.0091936 , 0.00813188,\n",
       "        0.00958017, 0.01023482, 0.00906891, 0.00861845, 0.00881362,\n",
       "        0.00880609, 0.00896237, 0.00921865, 0.009062  , 0.00919398,\n",
       "        0.00873116, 0.00985237, 0.00862098, 0.00911172, 0.00882476,\n",
       "        0.00890788, 0.00891139, 0.00926326, 0.00886418, 0.00884983,\n",
       "        0.00912073, 0.00919454, 0.0086065 , 0.0078491 , 0.00843786,\n",
       "        0.0092791 , 0.00868527, 0.00850013, 0.00894016, 0.00887881,\n",
       "        0.00887966, 0.00882866, 0.00918186, 0.00894092, 0.0085474 ,\n",
       "        0.01026447, 0.01078369, 0.01097726, 0.0113427 , 0.01209613,\n",
       "        0.01116958, 0.01156785, 0.01060083, 0.00936396, 0.00997159,\n",
       "        0.01227685, 0.00963303, 0.01211494, 0.00992903, 0.00990344,\n",
       "        0.01196121, 0.01039868, 0.01026168, 0.01163592, 0.01402821,\n",
       "        0.0122408 , 0.01178084, 0.00991791, 0.00993923, 0.00994299,\n",
       "        0.01115598, 0.01119964, 0.01322261, 0.00891984, 0.01274191,\n",
       "        0.01291326, 0.01263993, 0.01179499, 0.01241189, 0.01074131,\n",
       "        0.01051999, 0.01040946, 0.00987391, 0.00985924, 0.01001004,\n",
       "        0.01173378, 0.01111708, 0.012633  , 0.0124766 , 0.01096559,\n",
       "        0.01219313, 0.0119789 , 0.01027681]),\n",
       " 'rank_test_neg_log_loss': array([184, 175, 171, 195, 198, 193, 222, 238, 230, 251, 243, 246, 173,\n",
       "        170, 172, 207, 205, 202, 228, 218, 226, 239, 254, 232, 183, 181,\n",
       "        179, 200, 186, 192, 214, 219, 224, 237, 235, 241, 168, 169, 178,\n",
       "        189, 187, 185, 208, 209, 204, 221, 215, 220, 161, 180, 176, 210,\n",
       "        201, 191, 233, 245, 231, 260, 259, 250, 182, 166, 162, 206, 212,\n",
       "        199, 223, 229, 244, 249, 256, 268, 167, 177, 174, 196, 203, 188,\n",
       "        227, 217, 225, 253, 252, 255, 153, 157, 163, 190, 194, 197, 211,\n",
       "        216, 213, 242, 240, 236, 142, 279, 147, 113, 277, 108,  88, 278,\n",
       "         65,  66, 275,  70, 150, 273, 139,  98, 263, 118,  86, 271,  72,\n",
       "         44, 281,  42, 138, 282, 148, 117, 274,  91,  64, 269,  50,  56,\n",
       "        267,  57, 152, 283, 155, 120, 280, 104,  78, 276,  49,  54, 272,\n",
       "         43, 146, 270, 143, 130, 264, 124, 103, 265,  58, 110, 248,  48,\n",
       "        160, 285, 151, 106, 258,  93, 129, 262, 119,  84, 234,  90, 165,\n",
       "        288, 158, 127, 286, 114, 144, 257, 131,  94, 247,  99, 164, 261,\n",
       "        149, 128, 287, 126, 100, 266, 102,  81, 284, 123,  87,  85,  71,\n",
       "         34,  28,  33,  19,  15,  20,  10,  11,   9,  68,  74,  55,  39,\n",
       "         35,  23,  26,  22,  13,   4,   8,   3,  47,  61,  53,  31,  38,\n",
       "         32,  25,  17,  21,  12,  14,  18,  73,  63,  76,  30,  24,  27,\n",
       "         16,   6,   7,   5,   1,   2, 109,  41,  69,  80,  95,  97, 133,\n",
       "        112, 115, 159, 145, 136,  82,  83,  59,  96,  67, 111, 125, 140,\n",
       "        121, 156, 141, 154,  52,  51,  75, 116, 101,  79, 105, 134, 122,\n",
       "        137, 135, 132,  46,  40,  29,  36,  62,  60,  89,  45,  37, 107,\n",
       "         77,  92])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRIAL_2_MLP.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL TWO NEG LOG LOSS RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.1,\n",
       " 'classifier__hidden_layer_sizes': (20,),\n",
       " 'classifier__learning_rate': 'invscaling',\n",
       " 'classifier__solver': 'adam'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL TWO NEG LOG LOSS RESULTS ---------------------------------------------\")\n",
    "TRIAL_2_MLP.cv_results_['rank_test_neg_log_loss']\n",
    "TRIAL_2_MLP.cv_results_['params'][ np.argmin(TRIAL_2_MLP.cv_results_['rank_test_neg_log_loss']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL TWO F1 RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.01,\n",
       " 'classifier__hidden_layer_sizes': (5,),\n",
       " 'classifier__learning_rate': 'constant',\n",
       " 'classifier__solver': 'adam'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL TWO F1 RESULTS ---------------------------------------------\")\n",
    "TRIAL_2_MLP.cv_results_['rank_test_f1_micro']\n",
    "TRIAL_2_MLP.cv_results_['params'][ np.argmin(TRIAL_2_MLP.cv_results_['rank_test_f1_micro']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL TWO ROC AUC RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.1,\n",
       " 'classifier__hidden_layer_sizes': (20,),\n",
       " 'classifier__learning_rate': 'invscaling',\n",
       " 'classifier__solver': 'adam'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL TWO ROC AUC RESULTS ---------------------------------------------\")\n",
    "TRIAL_2_MLP.cv_results_['rank_test_roc_auc_ovo']\n",
    "TRIAL_2_MLP.cv_results_['params'][ np.argmin(TRIAL_2_MLP.cv_results_['rank_test_roc_auc_ovo']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL TWO JACCARD SCORE RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.01,\n",
       " 'classifier__hidden_layer_sizes': (5,),\n",
       " 'classifier__learning_rate': 'constant',\n",
       " 'classifier__solver': 'adam'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL TWO JACCARD SCORE RESULTS ---------------------------------------------\")\n",
    "TRIAL_2_MLP.cv_results_['rank_test_jaccard']\n",
    "TRIAL_2_MLP.cv_results_['params'][ np.argmin(TRIAL_2_MLP.cv_results_['rank_test_jaccard']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 2 Multilayered Perceptrons using best NEG LOG LOSS hyperperameters :0.7780112364682775\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 2 Multilayered Perceptrons using best F1 hyperperameters :0.4941762207098159\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 2 Multilayered Perceptrons using best ROC_AUC hyperperameters :0.7781254282190655\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 2 Multilayered Perceptrons using best JACCARD hyperperameters :0.5036769743753712\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best NEG LOG LOSS results to train on test set, \n",
    "bestMPLT2_1 = MLPClassifier(activation = 'logistic', alpha = .01, hidden_layer_sizes = (20,), \n",
    "                          learning_rate = 'invscaling',solver = 'adam')\n",
    "bestMPLT2_1.fit(X_train,y_train)\n",
    "y_pred2_1 = bestMPLT1_1.predict(X_test)\n",
    "print('Accuracy of Trial 2 Multilayered Perceptrons using best NEG LOG LOSS hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred1_0)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best F1 results to train on test set, \n",
    "bestMPLT2_2 = MLPClassifier(activation = 'logistic', alpha = 0.01, hidden_layer_sizes = (5,), \n",
    "                          learning_rate = 'constant',solver = 'adam')\n",
    "bestMPLT2_2.fit(X_train,y_train)\n",
    "y_pred2_2 = bestMPLT2_2.predict(X_test)\n",
    "print('Accuracy of Trial 2 Multilayered Perceptrons using best F1 hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred2_2)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best ROC_AUC results to train on test set,\n",
    "bestMPLT2_3 = MLPClassifier(activation = 'logistic', alpha = 0.1, hidden_layer_sizes = (20,), \n",
    "                          learning_rate = 'invscaling',solver = 'adam')\n",
    "bestMPLT2_3.fit(X_train,y_train)\n",
    "y_pred2_3 = bestMPLT2_3.predict(X_test)\n",
    "print('Accuracy of Trial 2 Multilayered Perceptrons using best ROC_AUC hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred2_3)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best JACCARD results to train on test set, \n",
    "bestMPLT2_4 = MLPClassifier(activation = 'logistic', alpha = 0.01, hidden_layer_sizes = (5,), \n",
    "                          learning_rate = 'constant',solver = 'adam')\n",
    "bestMPLT2_4.fit(X_train,y_train)\n",
    "y_pred2_4 = bestMPLT2_4.predict(X_test)\n",
    "print('Accuracy of Trial 2 Multilayered Perceptrons using best JACCARD hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred2_4)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTI LAYERED PERCEPTRON TRIAL THREE ON LEAGUE OF LEGENDS DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:    9.4s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:   10.3s\n",
      "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed:   11.6s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   14.2s\n",
      "[Parallel(n_jobs=-1)]: Done 189 tasks      | elapsed:   15.1s\n",
      "[Parallel(n_jobs=-1)]: Done 210 tasks      | elapsed:   16.3s\n",
      "[Parallel(n_jobs=-1)]: Done 233 tasks      | elapsed:   19.0s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:   20.2s\n",
      "[Parallel(n_jobs=-1)]: Done 281 tasks      | elapsed:   22.5s\n",
      "[Parallel(n_jobs=-1)]: Done 306 tasks      | elapsed:   24.8s\n",
      "[Parallel(n_jobs=-1)]: Done 333 tasks      | elapsed:   27.1s\n",
      "[Parallel(n_jobs=-1)]: Done 360 tasks      | elapsed:   29.6s\n",
      "[Parallel(n_jobs=-1)]: Done 389 tasks      | elapsed:   31.5s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:   35.1s\n",
      "[Parallel(n_jobs=-1)]: Done 449 tasks      | elapsed:   36.9s\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:   40.3s\n",
      "[Parallel(n_jobs=-1)]: Done 513 tasks      | elapsed:   43.7s\n",
      "[Parallel(n_jobs=-1)]: Done 546 tasks      | elapsed:   46.8s\n",
      "[Parallel(n_jobs=-1)]: Done 581 tasks      | elapsed:   49.6s\n",
      "[Parallel(n_jobs=-1)]: Done 616 tasks      | elapsed:   53.2s\n",
      "[Parallel(n_jobs=-1)]: Done 653 tasks      | elapsed:   56.6s\n",
      "[Parallel(n_jobs=-1)]: Done 690 tasks      | elapsed:   59.6s\n",
      "[Parallel(n_jobs=-1)]: Done 729 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 809 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 850 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 893 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 936 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 981 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1026 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1073 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1120 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1169 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1269 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1320 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1373 tasks      | elapsed:  2.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL THREE RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1440 out of 1440 | elapsed:  2.5min finished\n"
     ]
    }
   ],
   "source": [
    "ITERATIONS = 1\n",
    "for i in range(1):\n",
    "    #For reach \"trial\", randomly select 5000 data points\n",
    "    random5000DataPoints = leagueData.sample(n = 5000, replace = True)\n",
    "    #From that 5000, create x set and y set\n",
    "    xSet = random5000DataPoints.iloc[:,1:17].values\n",
    "    ySet = random5000DataPoints.iloc[:,0].values\n",
    "    # Run GridSearch to find best hyperparameters per 5000 data points \n",
    "    \n",
    "    pipe = Pipeline([('std', StandardScaler()),('classifier', MLPClassifier(max_iter = 250))])\n",
    "\n",
    "    search_space = [\n",
    "                {\n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                {\n",
    "                 \n",
    "                  \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['sgd'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                \n",
    "                {\n",
    "                \n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['adam'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                ]\n",
    "    \n",
    "    runHyperParamSearch = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=5), \n",
    "                          scoring=['recall_micro', 'f1_micro','precision_micro','roc_auc_ovo','jaccard','neg_log_loss'], \n",
    "                          refit=False, verbose=10, n_jobs = -1)\n",
    "    TRIAL_3_MLP = runHyperParamSearch.fit(xSet, ySet)\n",
    "    print(\"---------------------------------TRIAL THREE RESULTS ---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.643255  , 0.64115262, 0.68338757, 0.92709785, 0.95161915,\n",
       "        0.94241343, 1.34635801, 1.51119866, 1.48337584, 1.77332449,\n",
       "        1.74840288, 1.73378997, 0.81379681, 0.70280294, 0.691995  ,\n",
       "        0.92149215, 0.96012645, 1.02338095, 1.3558661 , 1.50259252,\n",
       "        1.54062586, 1.84058294, 1.81025624, 1.71567492, 0.81870255,\n",
       "        0.78928018, 0.73943653, 0.90387936, 0.93720627, 1.02658391,\n",
       "        1.482375  , 1.36387382, 1.45665274, 1.87681465, 1.87541442,\n",
       "        1.73779202, 0.72632384, 0.68098526, 0.66417222, 0.97474008,\n",
       "        0.98925261, 0.93220134, 1.39229789, 1.34745955, 1.40250664,\n",
       "        1.73649092, 1.76341648, 1.72087898, 0.88245854, 0.7306284 ,\n",
       "        0.71010985, 0.98054557, 1.04790211, 1.05620875, 1.53712249,\n",
       "        1.55533614, 1.64271121, 2.07378211, 2.058673  , 1.93426428,\n",
       "        0.83771987, 0.71751561, 0.70730658, 0.97964258, 1.00236344,\n",
       "        1.02748413, 1.51149955, 1.55723791, 1.62159472, 2.05126081,\n",
       "        1.97089615, 2.01052809, 0.86123986, 0.77116375, 0.72392268,\n",
       "        1.05390644, 1.03719268, 1.15839581, 1.58826575, 1.69055467,\n",
       "        1.77552705, 2.13693647, 2.17757192, 2.00642548, 0.87795568,\n",
       "        0.74624233, 0.70740867, 1.00916815, 0.99565654, 1.09544325,\n",
       "        1.5757565 , 1.55153394, 1.68675094, 2.05026245, 2.04255466,\n",
       "        1.90243464, 2.31459007, 0.22098961, 2.45440974, 1.85439467,\n",
       "        0.19656906, 2.3753437 , 1.74620166, 0.19756966, 2.37994623,\n",
       "        1.76672034, 0.17925386, 2.36022964, 1.87361059, 0.1821568 ,\n",
       "        2.39756174, 1.62279572, 0.18135624, 2.14744697, 1.77382579,\n",
       "        0.17485051, 2.46281824, 1.71747704, 0.26342649, 2.29737573,\n",
       "        2.06687708, 0.19346647, 2.62015333, 1.81496062, 0.33538837,\n",
       "        2.28036146, 1.71357341, 0.25882292, 2.30678449, 1.63660741,\n",
       "        0.19266582, 2.38204813, 1.86360254, 0.15052981, 2.3569273 ,\n",
       "        1.67934489, 0.17855382, 2.31599226, 1.73178954, 0.20207448,\n",
       "        2.3560257 , 1.79694505, 0.24611168, 2.35462499, 0.9046782 ,\n",
       "        0.84472656, 1.42592645, 0.86414294, 1.85939889, 1.50369306,\n",
       "        0.789079  , 1.70326519, 1.53702221, 0.73553243, 1.13297381,\n",
       "        1.37097855, 0.88856359, 1.01727524, 1.55173488, 0.94861555,\n",
       "        1.00296259, 1.49468584, 0.90067496, 0.63724828, 1.61308737,\n",
       "        0.78467512, 1.32704115, 1.61678982, 1.13897891, 0.98885002,\n",
       "        1.55593786, 0.91889048, 0.96523051, 1.58226128, 0.9405087 ,\n",
       "        1.23406134, 1.56454563, 0.83311687, 2.60694175, 1.57105155,\n",
       "        1.21574626, 1.15259104, 1.5914691 , 0.75604997, 1.2678905 ,\n",
       "        1.49188333, 0.7945837 , 1.61198673, 1.55713959, 0.94631395,\n",
       "        2.41037202, 1.53231711, 1.91684794, 1.87761483, 1.76521826,\n",
       "        1.50509448, 1.49598675, 1.43243151, 1.17370987, 1.43333282,\n",
       "        1.1285706 , 1.32594085, 1.02508187, 1.09354019, 1.92275329,\n",
       "        1.74530106, 1.76001339, 1.52561212, 1.28500552, 1.57435379,\n",
       "        1.38118811, 1.34575715, 1.40190592, 1.28500566, 1.1708065 ,\n",
       "        1.31112776, 1.70886893, 1.74480038, 1.66012836, 1.4713654 ,\n",
       "        1.19442739, 1.36637487, 1.38729291, 1.3600697 , 1.2552803 ,\n",
       "        1.22035046, 1.22715583, 1.12366681, 1.68975267, 1.90804062,\n",
       "        1.60457973, 1.21044126, 1.13237429, 1.49058208, 1.26488791,\n",
       "        1.34435601, 1.11936269, 1.26538839, 1.28740716, 1.16119857,\n",
       "        2.02594247, 2.03364892, 1.69786043, 2.92761803, 2.97445831,\n",
       "        2.91871014, 3.37290101, 3.39251719, 3.28282313, 3.58908644,\n",
       "        3.48649864, 3.62892065, 2.05796981, 2.17947369, 1.95438051,\n",
       "        2.98196468, 2.93292217, 2.95834384, 3.17661667, 3.19644895,\n",
       "        3.23538237, 3.3876133 , 3.37370157, 3.40132446, 1.84018278,\n",
       "        1.717277  , 1.95077739, 2.86085982, 2.63806901, 2.86956735,\n",
       "        3.34837971, 3.41944108, 3.37620368, 3.45617213, 3.41043229,\n",
       "        3.3899148 , 1.79244151, 1.66503186, 1.56454535, 2.62295642,\n",
       "        2.78979964, 2.78489523, 3.37470226, 3.14030056, 3.415237  ,\n",
       "        3.50601501, 3.29022913, 3.04962196]),\n",
       " 'std_fit_time': array([0.08317248, 0.01928897, 0.05574282, 0.04726359, 0.04524337,\n",
       "        0.07758514, 0.07725017, 0.08108393, 0.07555332, 0.06058064,\n",
       "        0.057364  , 0.02951623, 0.06940499, 0.06975752, 0.04273265,\n",
       "        0.0702457 , 0.05638396, 0.06774322, 0.0409274 , 0.04324584,\n",
       "        0.04825039, 0.072414  , 0.06487318, 0.04948697, 0.08154826,\n",
       "        0.03651934, 0.03849487, 0.06478893, 0.05046748, 0.10108154,\n",
       "        0.06587959, 0.05963943, 0.08538304, 0.12019272, 0.13024532,\n",
       "        0.09995185, 0.10001595, 0.02671172, 0.07657986, 0.03300629,\n",
       "        0.07548498, 0.07108239, 0.03511121, 0.04246283, 0.03790593,\n",
       "        0.09684852, 0.05981612, 0.03359516, 0.13751489, 0.01282875,\n",
       "        0.01302328, 0.0428275 , 0.04775742, 0.05644933, 0.08330278,\n",
       "        0.07038499, 0.07054626, 0.05579802, 0.14418189, 0.05684874,\n",
       "        0.05327356, 0.02971059, 0.02279275, 0.01027764, 0.00783348,\n",
       "        0.03052522, 0.06310332, 0.02983584, 0.0261967 , 0.12225928,\n",
       "        0.03384402, 0.11170683, 0.03488399, 0.06558262, 0.02397286,\n",
       "        0.06093699, 0.04645097, 0.09434645, 0.07196993, 0.08803793,\n",
       "        0.11057359, 0.21905121, 0.13580236, 0.115917  , 0.07611292,\n",
       "        0.01849111, 0.02224959, 0.02737866, 0.08228708, 0.02801798,\n",
       "        0.08942104, 0.06030966, 0.06465497, 0.11918082, 0.07182851,\n",
       "        0.07223021, 0.25610821, 0.02010355, 0.144224  , 0.15224932,\n",
       "        0.0286992 , 0.20941225, 0.13859075, 0.01872254, 0.23712423,\n",
       "        0.03204619, 0.00534692, 0.07112981, 0.27052851, 0.02161699,\n",
       "        0.19814693, 0.11376709, 0.02042977, 0.09606765, 0.01343021,\n",
       "        0.01070739, 0.14188257, 0.083221  , 0.05730544, 0.13493294,\n",
       "        0.1700129 , 0.03480352, 0.13485338, 0.11669723, 0.28558106,\n",
       "        0.11302573, 0.09398155, 0.07190085, 0.09563824, 0.04676522,\n",
       "        0.01528727, 0.07363979, 0.12598388, 0.00822518, 0.07409265,\n",
       "        0.04310094, 0.02099465, 0.15733273, 0.0509402 , 0.02351191,\n",
       "        0.16684653, 0.05029067, 0.06749746, 0.12861499, 0.16791742,\n",
       "        0.90983903, 0.19359198, 0.17544153, 1.09253436, 0.20248732,\n",
       "        0.0780736 , 0.6240706 , 0.14947744, 0.13155319, 0.96876471,\n",
       "        0.03325489, 0.09669202, 0.73229364, 0.13611587, 0.2808607 ,\n",
       "        0.71153876, 0.13554958, 0.14416005, 0.12731304, 0.20467437,\n",
       "        0.10072336, 1.16768337, 0.13232191, 0.23905819, 0.64139508,\n",
       "        0.21785277, 0.12329007, 0.36458092, 0.11826337, 0.13628895,\n",
       "        1.08249174, 0.15151359, 0.19336651, 0.62723306, 0.07106267,\n",
       "        0.19552556, 0.79593554, 0.17235012, 0.11372832, 0.82889501,\n",
       "        0.18483655, 0.04100874, 1.0089584 , 0.15050905, 0.1094769 ,\n",
       "        0.81910759, 0.13628513, 0.31259968, 0.37627843, 0.22881601,\n",
       "        0.22211586, 0.39658867, 0.30732636, 0.19696781, 0.41813584,\n",
       "        0.20176183, 0.37276703, 0.25997526, 0.25412208, 0.37167857,\n",
       "        0.28865931, 0.1775793 , 0.36841824, 0.44179711, 0.323392  ,\n",
       "        0.24279943, 0.37858333, 0.35555629, 0.51908486, 0.0726113 ,\n",
       "        0.44766381, 0.26718787, 0.33549974, 0.37761811, 0.47810961,\n",
       "        0.19741995, 0.23102922, 0.29173657, 0.32928311, 0.20229468,\n",
       "        0.21250159, 0.16309877, 0.16930573, 0.22257518, 0.27254971,\n",
       "        0.09216206, 0.13992299, 0.16832445, 0.1948442 , 0.27572009,\n",
       "        0.16706194, 0.25761161, 0.18123384, 0.25267454, 0.24431972,\n",
       "        0.34200214, 0.28405467, 0.3035565 , 0.04287526, 0.10106912,\n",
       "        0.14918156, 0.12008846, 0.10850213, 0.15126898, 0.12575279,\n",
       "        0.05348516, 0.15917256, 0.32092681, 0.33046292, 0.29692124,\n",
       "        0.05629027, 0.02595571, 0.05045325, 0.06266244, 0.0455382 ,\n",
       "        0.04006226, 0.03554758, 0.06223601, 0.03606197, 0.41143593,\n",
       "        0.37762177, 0.30860546, 0.07111414, 0.30994922, 0.24730271,\n",
       "        0.09929581, 0.11424868, 0.06826867, 0.05934933, 0.07117938,\n",
       "        0.05123213, 0.25833439, 0.17806248, 0.26261352, 0.27671765,\n",
       "        0.22044016, 0.43251857, 0.16376349, 0.32254742, 0.16139751,\n",
       "        0.21425485, 0.11785128, 0.062454  ]),\n",
       " 'mean_score_time': array([0.01181026, 0.01651449, 0.01181016, 0.01581326, 0.01401267,\n",
       "        0.01981449, 0.01671505, 0.01801734, 0.0170155 , 0.0173161 ,\n",
       "        0.01381364, 0.01491294, 0.01611481, 0.01341166, 0.01111107,\n",
       "        0.01381316, 0.01511326, 0.01611652, 0.01741595, 0.01661439,\n",
       "        0.01921539, 0.01561236, 0.01671653, 0.01681418, 0.01221147,\n",
       "        0.01441116, 0.01781549, 0.01281018, 0.01211066, 0.01351156,\n",
       "        0.01631556, 0.0152143 , 0.02001772, 0.0177134 , 0.01641321,\n",
       "        0.01161098, 0.01180921, 0.01391239, 0.01221156, 0.01421275,\n",
       "        0.01461172, 0.01361251, 0.01491265, 0.0155127 , 0.02031956,\n",
       "        0.01601477, 0.01481256, 0.0122107 , 0.01291122, 0.01301184,\n",
       "        0.01271176, 0.01591249, 0.01261215, 0.01351285, 0.01971464,\n",
       "        0.0198195 , 0.01691465, 0.01591415, 0.01350951, 0.01220894,\n",
       "        0.01231122, 0.01121082, 0.01371231, 0.01241121, 0.0133122 ,\n",
       "        0.01341248, 0.01631479, 0.01811643, 0.01661506, 0.01821551,\n",
       "        0.01341023, 0.01391263, 0.01331158, 0.01341209, 0.01431117,\n",
       "        0.0141118 , 0.01391249, 0.012711  , 0.01501222, 0.01571159,\n",
       "        0.01821494, 0.01551504, 0.01611371, 0.015413  , 0.01381068,\n",
       "        0.01231127, 0.01231132, 0.01210985, 0.01481338, 0.0126112 ,\n",
       "        0.01661487, 0.01561413, 0.01851678, 0.02131796, 0.01581445,\n",
       "        0.01281109, 0.0101089 , 0.01231008, 0.01020913, 0.00990858,\n",
       "        0.01120954, 0.01100912, 0.01020932, 0.01231103, 0.00970907,\n",
       "        0.01030879, 0.0106091 , 0.01000862, 0.01000867, 0.01000848,\n",
       "        0.00940819, 0.01060925, 0.01331177, 0.00950842, 0.01020942,\n",
       "        0.01070919, 0.01020861, 0.01070933, 0.01090941, 0.01090937,\n",
       "        0.01251106, 0.01251087, 0.0102087 , 0.00980868, 0.01060939,\n",
       "        0.00980825, 0.00990925, 0.00980902, 0.0106091 , 0.00980821,\n",
       "        0.01181068, 0.00980849, 0.00930805, 0.00970807, 0.00950804,\n",
       "        0.00990853, 0.00960789, 0.00990825, 0.01000862, 0.0104084 ,\n",
       "        0.01050906, 0.01060939, 0.01040897, 0.00980864, 0.00950794,\n",
       "        0.01080956, 0.00940809, 0.01040874, 0.01030893, 0.01060915,\n",
       "        0.01241059, 0.01040897, 0.01040854, 0.01150999, 0.0105094 ,\n",
       "        0.01020899, 0.0097084 , 0.00920792, 0.00940804, 0.0096086 ,\n",
       "        0.00970831, 0.00960832, 0.01110997, 0.01050887, 0.01261086,\n",
       "        0.0106091 , 0.0111095 , 0.01060953, 0.01000867, 0.00980873,\n",
       "        0.01160998, 0.0122107 , 0.01110969, 0.01040845, 0.01020918,\n",
       "        0.01020889, 0.01040893, 0.01110926, 0.01030936, 0.01080909,\n",
       "        0.00960803, 0.01040888, 0.00940757, 0.01000929, 0.00990863,\n",
       "        0.01140943, 0.01441255, 0.01070919, 0.01130967, 0.01591411,\n",
       "        0.01010885, 0.00990872, 0.01080914, 0.00940814, 0.00910769,\n",
       "        0.0098084 , 0.01020885, 0.00980897, 0.01020875, 0.00990887,\n",
       "        0.01190991, 0.01010876, 0.01040888, 0.01070919, 0.00900779,\n",
       "        0.00920796, 0.01090941, 0.00930791, 0.00970845, 0.01060915,\n",
       "        0.01040921, 0.01000886, 0.01130962, 0.01170969, 0.01060944,\n",
       "        0.0101089 , 0.00910821, 0.00930786, 0.0096076 , 0.00920806,\n",
       "        0.00970845, 0.00950828, 0.01000857, 0.01010876, 0.01040878,\n",
       "        0.01060882, 0.01461248, 0.01060901, 0.01180992, 0.00990877,\n",
       "        0.01000829, 0.00990872, 0.00980802, 0.00970855, 0.01000814,\n",
       "        0.01090999, 0.01030898, 0.01120944, 0.01080952, 0.01020842,\n",
       "        0.00940771, 0.00950799, 0.00950813, 0.01040893, 0.01241064,\n",
       "        0.01171031, 0.01471195, 0.01221032, 0.0115097 , 0.01181064,\n",
       "        0.01171007, 0.01271076, 0.010109  , 0.00920825, 0.00940828,\n",
       "        0.01231079, 0.01010895, 0.00990887, 0.01141009, 0.01060939,\n",
       "        0.01130996, 0.01081033, 0.01080971, 0.01090946, 0.00930791,\n",
       "        0.0097085 , 0.00930824, 0.00980864, 0.01010852, 0.01100979,\n",
       "        0.01301112, 0.01371193, 0.0110096 , 0.01261115, 0.01070933,\n",
       "        0.01070929, 0.01070938, 0.00950851, 0.01140995, 0.0107089 ,\n",
       "        0.01050897, 0.01020851, 0.01070957, 0.0109097 , 0.01191039,\n",
       "        0.01020885, 0.00750651, 0.00600529]),\n",
       " 'std_score_time': array([1.88913894e-03, 4.74824985e-03, 1.69268651e-03, 9.28551324e-04,\n",
       "        2.19205930e-03, 8.58017615e-03, 1.96668916e-03, 4.33958678e-03,\n",
       "        1.48530293e-03, 4.21926617e-03, 2.87770262e-03, 4.36699783e-03,\n",
       "        4.65678246e-03, 2.95925599e-03, 3.75499783e-04, 1.88657002e-03,\n",
       "        1.15825817e-03, 4.06962866e-03, 2.03659347e-03, 4.14525218e-03,\n",
       "        6.39654749e-03, 2.08594777e-03, 4.89889116e-03, 6.04305396e-03,\n",
       "        2.04152219e-03, 2.06149192e-03, 1.00614733e-02, 2.25099042e-03,\n",
       "        8.61048339e-04, 8.95513153e-04, 2.56216622e-03, 1.77984679e-03,\n",
       "        6.78632419e-03, 4.49672067e-03, 4.51413671e-03, 3.75440697e-04,\n",
       "        8.72630937e-04, 2.22546659e-03, 2.11441361e-03, 3.57498198e-03,\n",
       "        1.65557009e-03, 1.59397374e-03, 1.88215838e-03, 1.51707986e-03,\n",
       "        7.08361095e-03, 7.73926413e-04, 2.06624703e-03, 1.03119766e-03,\n",
       "        1.59561235e-03, 1.04780220e-03, 1.63164665e-03, 6.60519766e-03,\n",
       "        1.53070737e-03, 8.93244541e-04, 7.76386308e-03, 4.52531209e-03,\n",
       "        2.22392629e-03, 2.51891120e-03, 1.18676097e-03, 6.79437167e-04,\n",
       "        9.82539031e-04, 8.13737226e-04, 2.29558383e-03, 5.83372090e-04,\n",
       "        5.04604766e-04, 1.24234643e-03, 5.63184825e-03, 6.26106967e-03,\n",
       "        3.10681280e-03, 5.34865710e-03, 1.53071799e-03, 1.35779691e-03,\n",
       "        1.25131926e-03, 2.22447563e-03, 3.23685810e-03, 2.71253383e-03,\n",
       "        2.90630059e-03, 1.50398381e-03, 2.49027843e-03, 1.96535287e-03,\n",
       "        7.37250752e-03, 1.84623268e-03, 1.90924743e-03, 1.59582471e-03,\n",
       "        2.04016219e-03, 1.40117624e-03, 2.20711082e-03, 1.46399091e-03,\n",
       "        3.48999245e-03, 7.33960347e-04, 3.21501097e-03, 1.53056214e-03,\n",
       "        4.49783430e-03, 3.26807783e-03, 4.09792706e-03, 1.63256917e-03,\n",
       "        1.39432303e-03, 2.20698746e-03, 9.27762569e-04, 1.99747171e-04,\n",
       "        1.72183984e-03, 3.05250417e-03, 7.49220300e-04, 2.56408943e-03,\n",
       "        4.00209725e-04, 4.00150038e-04, 7.35202484e-04, 3.16808125e-04,\n",
       "        1.51789585e-03, 1.30464209e-03, 5.83551950e-04, 4.90155098e-04,\n",
       "        7.86526683e-03, 5.48814927e-04, 5.10276599e-04, 1.28923066e-03,\n",
       "        5.10126890e-04, 1.69245773e-03, 8.61218713e-04, 1.62618078e-03,\n",
       "        2.81302647e-03, 2.59044048e-03, 8.13659837e-04, 6.00743509e-04,\n",
       "        1.02062781e-03, 7.48506787e-04, 5.83903905e-04, 3.99887906e-04,\n",
       "        1.02074966e-03, 4.00734714e-04, 3.60255246e-03, 2.45145753e-04,\n",
       "        4.00281134e-04, 1.16693189e-03, 5.47900847e-04, 4.90485525e-04,\n",
       "        4.90592529e-04, 4.89648506e-04, 5.47639482e-04, 1.32019516e-03,\n",
       "        1.14147293e-03, 1.02027743e-03, 1.31989539e-03, 2.45067608e-04,\n",
       "        5.48118359e-04, 2.04087213e-03, 4.90485525e-04, 8.00949445e-04,\n",
       "        9.28174024e-04, 9.69967386e-04, 3.83011790e-03, 5.84148846e-04,\n",
       "        5.83707399e-04, 1.58298453e-03, 4.47980925e-04, 4.00352677e-04,\n",
       "        6.78626880e-04, 2.44639370e-04, 3.74686745e-04, 3.74648974e-04,\n",
       "        5.10491729e-04, 4.89872230e-04, 8.00532402e-04, 8.37198041e-04,\n",
       "        4.03297809e-03, 5.84263462e-04, 9.70444531e-04, 4.90534244e-04,\n",
       "        4.47874555e-04, 9.28379677e-04, 2.95872842e-03, 4.43765412e-03,\n",
       "        2.24663714e-03, 1.11444894e-03, 5.10183440e-04, 5.10108937e-04,\n",
       "        5.83731994e-04, 8.60913235e-04, 4.00197573e-04, 8.71902769e-04,\n",
       "        2.00867844e-04, 1.68699514e-03, 2.00057222e-04, 1.00100068e-03,\n",
       "        8.61090364e-04, 1.90962720e-03, 7.85115293e-03, 6.78514388e-04,\n",
       "        1.16698504e-03, 6.39593775e-03, 4.90582772e-04, 1.99938076e-04,\n",
       "        3.12681314e-03, 5.83911829e-04, 3.74584910e-04, 2.45651658e-04,\n",
       "        1.12312973e-03, 6.00242766e-04, 6.00886390e-04, 3.74852375e-04,\n",
       "        2.57910526e-03, 2.00248013e-04, 6.63866453e-04, 8.13119797e-04,\n",
       "        2.43140197e-07, 4.00304809e-04, 2.33463234e-03, 2.45418798e-04,\n",
       "        6.78788580e-04, 1.71642914e-03, 3.74635864e-04, 4.47234908e-04,\n",
       "        1.88818341e-03, 1.91476366e-03, 4.90602187e-04, 3.74878184e-04,\n",
       "        2.00438634e-04, 4.00185823e-04, 5.83388260e-04, 2.45204049e-04,\n",
       "        5.10388737e-04, 5.95569420e-07, 7.75033037e-04, 1.02081017e-03,\n",
       "        1.56274594e-03, 8.01008973e-04, 7.76143116e-03, 8.61223452e-04,\n",
       "        2.36012402e-03, 3.74444484e-04, 1.30489809e-03, 5.83527520e-04,\n",
       "        6.78648112e-04, 4.00710315e-04, 3.16506511e-04, 5.83838225e-04,\n",
       "        6.79119292e-04, 1.86163900e-03, 9.27844828e-04, 6.00679954e-04,\n",
       "        5.83306525e-04, 3.16657235e-04, 7.75217722e-04, 1.80158616e-03,\n",
       "        3.77646157e-03, 1.28967488e-03, 2.54395285e-03, 9.81087562e-04,\n",
       "        1.14092937e-03, 1.63218328e-03, 2.11351772e-03, 1.50456875e-03,\n",
       "        1.32029999e-03, 2.45145661e-04, 3.74177111e-04, 3.69867550e-03,\n",
       "        3.74546619e-04, 3.74279184e-04, 2.08584866e-03, 5.83519193e-04,\n",
       "        1.20936942e-03, 4.00281872e-04, 6.00314318e-04, 5.83584683e-04,\n",
       "        4.00269659e-04, 1.16698102e-03, 4.00257196e-04, 4.00353018e-04,\n",
       "        9.70306774e-04, 1.58244166e-03, 2.36866838e-03, 5.46912875e-03,\n",
       "        7.07696204e-04, 3.50170726e-03, 2.45242937e-04, 2.45281808e-04,\n",
       "        2.42315656e-03, 3.16582765e-04, 4.07021063e-03, 9.28369565e-04,\n",
       "        1.09645400e-03, 1.16748380e-03, 6.78612748e-04, 6.63816097e-04,\n",
       "        1.77353388e-03, 5.10734780e-04, 9.49419132e-04, 3.69356475e-07]),\n",
       " 'param_classifier': masked_array(data=[MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__activation': masked_array(data=['logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__alpha': masked_array(data=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__hidden_layer_sizes': masked_array(data=[(5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__learning_rate': masked_array(data=['constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'}],\n",
       " 'split0_test_recall_micro': array([0.758, 0.767, 0.758, 0.762, 0.76 , 0.737, 0.739, 0.732, 0.753,\n",
       "        0.746, 0.736, 0.741, 0.765, 0.76 , 0.767, 0.775, 0.767, 0.757,\n",
       "        0.742, 0.756, 0.739, 0.745, 0.712, 0.742, 0.769, 0.773, 0.775,\n",
       "        0.764, 0.761, 0.745, 0.763, 0.735, 0.751, 0.729, 0.734, 0.739,\n",
       "        0.77 , 0.752, 0.747, 0.76 , 0.744, 0.757, 0.76 , 0.752, 0.757,\n",
       "        0.735, 0.753, 0.751, 0.767, 0.783, 0.768, 0.749, 0.753, 0.756,\n",
       "        0.739, 0.738, 0.736, 0.738, 0.73 , 0.729, 0.772, 0.763, 0.77 ,\n",
       "        0.772, 0.775, 0.739, 0.742, 0.737, 0.724, 0.734, 0.731, 0.726,\n",
       "        0.775, 0.768, 0.756, 0.755, 0.743, 0.758, 0.771, 0.731, 0.745,\n",
       "        0.729, 0.724, 0.723, 0.762, 0.777, 0.758, 0.767, 0.752, 0.768,\n",
       "        0.737, 0.734, 0.752, 0.737, 0.74 , 0.745, 0.776, 0.501, 0.782,\n",
       "        0.783, 0.521, 0.779, 0.777, 0.502, 0.782, 0.78 , 0.535, 0.78 ,\n",
       "        0.781, 0.502, 0.779, 0.781, 0.726, 0.781, 0.782, 0.47 , 0.78 ,\n",
       "        0.779, 0.502, 0.781, 0.778, 0.502, 0.784, 0.777, 0.52 , 0.78 ,\n",
       "        0.786, 0.502, 0.78 , 0.78 , 0.324, 0.781, 0.779, 0.502, 0.779,\n",
       "        0.783, 0.502, 0.778, 0.782, 0.502, 0.781, 0.777, 0.501, 0.775,\n",
       "        0.781, 0.694, 0.775, 0.775, 0.35 , 0.787, 0.78 , 0.71 , 0.777,\n",
       "        0.767, 0.658, 0.786, 0.782, 0.721, 0.775, 0.774, 0.692, 0.772,\n",
       "        0.779, 0.707, 0.772, 0.777, 0.702, 0.779, 0.786, 0.574, 0.779,\n",
       "        0.776, 0.697, 0.784, 0.793, 0.708, 0.787, 0.79 , 0.589, 0.78 ,\n",
       "        0.784, 0.543, 0.773, 0.773, 0.568, 0.774, 0.782, 0.497, 0.775,\n",
       "        0.777, 0.568, 0.776, 0.783, 0.774, 0.782, 0.78 , 0.78 , 0.78 ,\n",
       "        0.782, 0.778, 0.779, 0.779, 0.784, 0.782, 0.777, 0.782, 0.779,\n",
       "        0.784, 0.781, 0.781, 0.778, 0.781, 0.779, 0.779, 0.781, 0.782,\n",
       "        0.784, 0.78 , 0.784, 0.78 , 0.779, 0.78 , 0.784, 0.781, 0.778,\n",
       "        0.78 , 0.782, 0.781, 0.78 , 0.777, 0.784, 0.778, 0.785, 0.785,\n",
       "        0.782, 0.783, 0.778, 0.778, 0.781, 0.778, 0.781, 0.778, 0.781,\n",
       "        0.784, 0.78 , 0.775, 0.769, 0.783, 0.773, 0.77 , 0.778, 0.77 ,\n",
       "        0.777, 0.779, 0.779, 0.778, 0.782, 0.766, 0.785, 0.774, 0.779,\n",
       "        0.767, 0.777, 0.78 , 0.784, 0.781, 0.782, 0.771, 0.781, 0.775,\n",
       "        0.775, 0.78 , 0.77 , 0.778, 0.77 , 0.774, 0.778, 0.773, 0.776,\n",
       "        0.783, 0.782, 0.784, 0.774, 0.775, 0.783, 0.776, 0.787, 0.778]),\n",
       " 'split1_test_recall_micro': array([0.774, 0.769, 0.76 , 0.765, 0.758, 0.76 , 0.745, 0.748, 0.736,\n",
       "        0.729, 0.76 , 0.746, 0.774, 0.76 , 0.763, 0.758, 0.761, 0.757,\n",
       "        0.748, 0.752, 0.749, 0.75 , 0.733, 0.747, 0.763, 0.763, 0.765,\n",
       "        0.741, 0.782, 0.761, 0.751, 0.739, 0.755, 0.746, 0.76 , 0.751,\n",
       "        0.777, 0.769, 0.773, 0.768, 0.762, 0.755, 0.754, 0.746, 0.74 ,\n",
       "        0.759, 0.764, 0.744, 0.776, 0.771, 0.78 , 0.758, 0.757, 0.768,\n",
       "        0.765, 0.762, 0.743, 0.755, 0.729, 0.745, 0.773, 0.769, 0.767,\n",
       "        0.769, 0.76 , 0.756, 0.735, 0.76 , 0.739, 0.743, 0.752, 0.72 ,\n",
       "        0.768, 0.772, 0.773, 0.767, 0.742, 0.756, 0.734, 0.742, 0.742,\n",
       "        0.736, 0.752, 0.768, 0.76 , 0.77 , 0.781, 0.767, 0.755, 0.763,\n",
       "        0.757, 0.756, 0.754, 0.734, 0.739, 0.745, 0.785, 0.597, 0.78 ,\n",
       "        0.785, 0.533, 0.775, 0.781, 0.502, 0.786, 0.785, 0.451, 0.78 ,\n",
       "        0.781, 0.582, 0.776, 0.782, 0.503, 0.777, 0.784, 0.502, 0.778,\n",
       "        0.779, 0.5  , 0.784, 0.778, 0.502, 0.778, 0.776, 0.498, 0.785,\n",
       "        0.782, 0.502, 0.775, 0.78 , 0.505, 0.777, 0.772, 0.499, 0.78 ,\n",
       "        0.775, 0.441, 0.779, 0.784, 0.48 , 0.784, 0.778, 0.491, 0.786,\n",
       "        0.776, 0.411, 0.784, 0.784, 0.375, 0.768, 0.783, 0.503, 0.777,\n",
       "        0.773, 0.538, 0.772, 0.788, 0.464, 0.787, 0.779, 0.57 , 0.78 ,\n",
       "        0.774, 0.684, 0.773, 0.773, 0.518, 0.78 , 0.772, 0.512, 0.78 ,\n",
       "        0.789, 0.587, 0.773, 0.789, 0.449, 0.773, 0.771, 0.456, 0.777,\n",
       "        0.771, 0.521, 0.781, 0.78 , 0.462, 0.78 , 0.776, 0.573, 0.785,\n",
       "        0.772, 0.644, 0.776, 0.784, 0.779, 0.777, 0.781, 0.782, 0.782,\n",
       "        0.784, 0.779, 0.782, 0.779, 0.781, 0.782, 0.781, 0.778, 0.781,\n",
       "        0.783, 0.784, 0.778, 0.781, 0.782, 0.78 , 0.78 , 0.782, 0.783,\n",
       "        0.782, 0.782, 0.78 , 0.782, 0.781, 0.781, 0.782, 0.783, 0.78 ,\n",
       "        0.782, 0.782, 0.781, 0.779, 0.78 , 0.78 , 0.782, 0.782, 0.784,\n",
       "        0.782, 0.784, 0.78 , 0.78 , 0.78 , 0.784, 0.776, 0.779, 0.78 ,\n",
       "        0.778, 0.77 , 0.768, 0.766, 0.78 , 0.773, 0.77 , 0.779, 0.775,\n",
       "        0.779, 0.779, 0.772, 0.781, 0.781, 0.784, 0.776, 0.771, 0.768,\n",
       "        0.767, 0.773, 0.774, 0.771, 0.779, 0.777, 0.77 , 0.778, 0.774,\n",
       "        0.767, 0.771, 0.766, 0.774, 0.774, 0.773, 0.78 , 0.784, 0.789,\n",
       "        0.781, 0.778, 0.771, 0.771, 0.785, 0.764, 0.775, 0.775, 0.774]),\n",
       " 'split2_test_recall_micro': array([0.763, 0.777, 0.773, 0.759, 0.767, 0.761, 0.746, 0.748, 0.752,\n",
       "        0.743, 0.75 , 0.766, 0.768, 0.763, 0.76 , 0.764, 0.762, 0.76 ,\n",
       "        0.757, 0.745, 0.761, 0.744, 0.728, 0.764, 0.774, 0.773, 0.778,\n",
       "        0.756, 0.763, 0.752, 0.743, 0.768, 0.765, 0.749, 0.765, 0.75 ,\n",
       "        0.779, 0.769, 0.77 , 0.775, 0.764, 0.763, 0.752, 0.754, 0.765,\n",
       "        0.766, 0.742, 0.764, 0.77 , 0.775, 0.795, 0.761, 0.758, 0.77 ,\n",
       "        0.725, 0.741, 0.741, 0.732, 0.74 , 0.731, 0.774, 0.772, 0.768,\n",
       "        0.764, 0.766, 0.761, 0.75 , 0.768, 0.746, 0.732, 0.729, 0.749,\n",
       "        0.766, 0.773, 0.774, 0.757, 0.768, 0.767, 0.78 , 0.747, 0.759,\n",
       "        0.744, 0.745, 0.743, 0.758, 0.766, 0.777, 0.771, 0.755, 0.763,\n",
       "        0.775, 0.765, 0.755, 0.75 , 0.769, 0.76 , 0.78 , 0.503, 0.781,\n",
       "        0.778, 0.497, 0.781, 0.785, 0.45 , 0.776, 0.779, 0.504, 0.78 ,\n",
       "        0.787, 0.335, 0.776, 0.779, 0.497, 0.783, 0.778, 0.497, 0.775,\n",
       "        0.782, 0.497, 0.781, 0.773, 0.497, 0.783, 0.783, 0.733, 0.776,\n",
       "        0.78 , 0.495, 0.776, 0.778, 0.74 , 0.78 , 0.774, 0.485, 0.78 ,\n",
       "        0.786, 0.497, 0.774, 0.78 , 0.514, 0.775, 0.779, 0.548, 0.782,\n",
       "        0.772, 0.55 , 0.786, 0.766, 0.471, 0.77 , 0.765, 0.579, 0.773,\n",
       "        0.776, 0.663, 0.777, 0.776, 0.532, 0.778, 0.781, 0.585, 0.77 ,\n",
       "        0.779, 0.651, 0.77 , 0.776, 0.593, 0.783, 0.769, 0.476, 0.774,\n",
       "        0.777, 0.552, 0.774, 0.782, 0.771, 0.781, 0.785, 0.618, 0.775,\n",
       "        0.773, 0.57 , 0.772, 0.777, 0.64 , 0.779, 0.779, 0.698, 0.778,\n",
       "        0.778, 0.502, 0.775, 0.779, 0.78 , 0.779, 0.78 , 0.778, 0.781,\n",
       "        0.779, 0.78 , 0.779, 0.783, 0.779, 0.777, 0.78 , 0.782, 0.782,\n",
       "        0.78 , 0.779, 0.776, 0.78 , 0.781, 0.78 , 0.78 , 0.78 , 0.778,\n",
       "        0.78 , 0.778, 0.777, 0.78 , 0.779, 0.779, 0.778, 0.777, 0.776,\n",
       "        0.778, 0.779, 0.779, 0.779, 0.778, 0.777, 0.776, 0.781, 0.781,\n",
       "        0.779, 0.779, 0.78 , 0.781, 0.779, 0.78 , 0.774, 0.778, 0.782,\n",
       "        0.772, 0.772, 0.785, 0.768, 0.782, 0.771, 0.771, 0.774, 0.78 ,\n",
       "        0.778, 0.782, 0.78 , 0.779, 0.772, 0.777, 0.775, 0.766, 0.776,\n",
       "        0.776, 0.767, 0.772, 0.773, 0.782, 0.771, 0.776, 0.782, 0.779,\n",
       "        0.775, 0.772, 0.765, 0.776, 0.769, 0.778, 0.775, 0.778, 0.78 ,\n",
       "        0.773, 0.781, 0.785, 0.774, 0.778, 0.771, 0.771, 0.772, 0.776]),\n",
       " 'split3_test_recall_micro': array([0.759, 0.77 , 0.763, 0.737, 0.734, 0.749, 0.748, 0.738, 0.736,\n",
       "        0.713, 0.715, 0.737, 0.763, 0.763, 0.758, 0.745, 0.743, 0.75 ,\n",
       "        0.74 , 0.746, 0.748, 0.719, 0.733, 0.757, 0.751, 0.755, 0.752,\n",
       "        0.76 , 0.757, 0.754, 0.739, 0.753, 0.739, 0.712, 0.722, 0.733,\n",
       "        0.761, 0.758, 0.765, 0.749, 0.755, 0.763, 0.741, 0.741, 0.733,\n",
       "        0.736, 0.746, 0.732, 0.76 , 0.762, 0.754, 0.741, 0.737, 0.754,\n",
       "        0.739, 0.74 , 0.759, 0.755, 0.74 , 0.742, 0.76 , 0.759, 0.754,\n",
       "        0.744, 0.761, 0.748, 0.723, 0.715, 0.727, 0.731, 0.734, 0.719,\n",
       "        0.762, 0.755, 0.764, 0.749, 0.755, 0.739, 0.741, 0.73 , 0.738,\n",
       "        0.732, 0.746, 0.731, 0.753, 0.766, 0.76 , 0.753, 0.746, 0.746,\n",
       "        0.755, 0.759, 0.743, 0.723, 0.726, 0.744, 0.776, 0.497, 0.772,\n",
       "        0.771, 0.558, 0.764, 0.786, 0.446, 0.782, 0.779, 0.515, 0.776,\n",
       "        0.774, 0.497, 0.776, 0.767, 0.395, 0.777, 0.773, 0.672, 0.778,\n",
       "        0.784, 0.497, 0.776, 0.778, 0.503, 0.779, 0.779, 0.497, 0.779,\n",
       "        0.778, 0.503, 0.779, 0.774, 0.507, 0.778, 0.773, 0.503, 0.778,\n",
       "        0.779, 0.446, 0.771, 0.774, 0.503, 0.776, 0.78 , 0.503, 0.77 ,\n",
       "        0.765, 0.66 , 0.767, 0.77 , 0.686, 0.771, 0.763, 0.57 , 0.764,\n",
       "        0.772, 0.723, 0.772, 0.776, 0.614, 0.769, 0.779, 0.545, 0.769,\n",
       "        0.762, 0.694, 0.773, 0.771, 0.722, 0.763, 0.767, 0.555, 0.772,\n",
       "        0.778, 0.7  , 0.77 , 0.773, 0.63 , 0.769, 0.768, 0.614, 0.778,\n",
       "        0.77 , 0.46 , 0.775, 0.768, 0.651, 0.763, 0.774, 0.665, 0.766,\n",
       "        0.769, 0.698, 0.774, 0.773, 0.773, 0.77 , 0.768, 0.775, 0.774,\n",
       "        0.768, 0.771, 0.766, 0.766, 0.766, 0.771, 0.775, 0.771, 0.769,\n",
       "        0.77 , 0.768, 0.771, 0.77 , 0.765, 0.771, 0.77 , 0.767, 0.77 ,\n",
       "        0.767, 0.767, 0.774, 0.767, 0.768, 0.77 , 0.772, 0.769, 0.768,\n",
       "        0.769, 0.768, 0.769, 0.77 , 0.768, 0.775, 0.766, 0.766, 0.773,\n",
       "        0.768, 0.765, 0.774, 0.768, 0.766, 0.768, 0.766, 0.76 , 0.773,\n",
       "        0.774, 0.767, 0.767, 0.764, 0.774, 0.777, 0.758, 0.758, 0.777,\n",
       "        0.776, 0.772, 0.773, 0.751, 0.765, 0.77 , 0.774, 0.757, 0.77 ,\n",
       "        0.777, 0.775, 0.758, 0.768, 0.773, 0.774, 0.77 , 0.775, 0.77 ,\n",
       "        0.763, 0.761, 0.761, 0.753, 0.755, 0.771, 0.774, 0.773, 0.769,\n",
       "        0.774, 0.764, 0.772, 0.768, 0.766, 0.773, 0.759, 0.754, 0.766]),\n",
       " 'split4_test_recall_micro': array([0.785, 0.787, 0.774, 0.771, 0.773, 0.763, 0.745, 0.749, 0.774,\n",
       "        0.742, 0.745, 0.748, 0.784, 0.779, 0.778, 0.765, 0.778, 0.768,\n",
       "        0.758, 0.756, 0.777, 0.748, 0.763, 0.74 , 0.79 , 0.782, 0.774,\n",
       "        0.759, 0.762, 0.777, 0.746, 0.759, 0.775, 0.737, 0.76 , 0.753,\n",
       "        0.772, 0.788, 0.766, 0.781, 0.779, 0.769, 0.777, 0.776, 0.763,\n",
       "        0.745, 0.762, 0.765, 0.791, 0.775, 0.78 , 0.785, 0.767, 0.757,\n",
       "        0.764, 0.75 , 0.759, 0.738, 0.731, 0.75 , 0.784, 0.78 , 0.78 ,\n",
       "        0.752, 0.769, 0.773, 0.771, 0.76 , 0.753, 0.753, 0.756, 0.732,\n",
       "        0.778, 0.777, 0.768, 0.767, 0.764, 0.771, 0.76 , 0.767, 0.744,\n",
       "        0.734, 0.756, 0.749, 0.784, 0.785, 0.764, 0.776, 0.774, 0.77 ,\n",
       "        0.761, 0.753, 0.765, 0.754, 0.752, 0.745, 0.788, 0.497, 0.795,\n",
       "        0.785, 0.485, 0.788, 0.787, 0.501, 0.791, 0.789, 0.655, 0.796,\n",
       "        0.789, 0.566, 0.789, 0.794, 0.503, 0.787, 0.785, 0.497, 0.785,\n",
       "        0.793, 0.623, 0.786, 0.787, 0.476, 0.791, 0.784, 0.503, 0.79 ,\n",
       "        0.793, 0.497, 0.79 , 0.786, 0.503, 0.786, 0.785, 0.502, 0.786,\n",
       "        0.793, 0.511, 0.785, 0.786, 0.647, 0.79 , 0.786, 0.497, 0.791,\n",
       "        0.787, 0.676, 0.792, 0.789, 0.652, 0.785, 0.791, 0.577, 0.795,\n",
       "        0.792, 0.738, 0.786, 0.786, 0.659, 0.781, 0.794, 0.694, 0.792,\n",
       "        0.791, 0.747, 0.784, 0.79 , 0.729, 0.791, 0.787, 0.444, 0.789,\n",
       "        0.785, 0.595, 0.791, 0.789, 0.661, 0.794, 0.789, 0.455, 0.783,\n",
       "        0.787, 0.673, 0.79 , 0.792, 0.576, 0.789, 0.799, 0.641, 0.785,\n",
       "        0.777, 0.557, 0.793, 0.798, 0.794, 0.799, 0.797, 0.797, 0.798,\n",
       "        0.8  , 0.797, 0.795, 0.798, 0.798, 0.797, 0.796, 0.794, 0.799,\n",
       "        0.796, 0.794, 0.795, 0.796, 0.795, 0.796, 0.797, 0.796, 0.8  ,\n",
       "        0.793, 0.793, 0.797, 0.794, 0.798, 0.798, 0.795, 0.8  , 0.799,\n",
       "        0.796, 0.797, 0.796, 0.794, 0.798, 0.796, 0.795, 0.796, 0.798,\n",
       "        0.798, 0.799, 0.798, 0.795, 0.798, 0.799, 0.791, 0.795, 0.794,\n",
       "        0.797, 0.78 , 0.799, 0.798, 0.796, 0.789, 0.794, 0.796, 0.789,\n",
       "        0.788, 0.797, 0.794, 0.787, 0.786, 0.793, 0.784, 0.795, 0.789,\n",
       "        0.787, 0.79 , 0.792, 0.791, 0.794, 0.791, 0.797, 0.781, 0.793,\n",
       "        0.799, 0.795, 0.794, 0.776, 0.791, 0.789, 0.793, 0.793, 0.79 ,\n",
       "        0.797, 0.789, 0.799, 0.785, 0.798, 0.795, 0.796, 0.791, 0.792]),\n",
       " 'mean_test_recall_micro': array([0.7678, 0.774 , 0.7656, 0.7588, 0.7584, 0.754 , 0.7446, 0.743 ,\n",
       "        0.7502, 0.7346, 0.7412, 0.7476, 0.7708, 0.765 , 0.7652, 0.7614,\n",
       "        0.7622, 0.7584, 0.749 , 0.751 , 0.7548, 0.7412, 0.7338, 0.75  ,\n",
       "        0.7694, 0.7692, 0.7688, 0.756 , 0.765 , 0.7578, 0.7484, 0.7508,\n",
       "        0.757 , 0.7346, 0.7482, 0.7452, 0.7718, 0.7672, 0.7642, 0.7666,\n",
       "        0.7608, 0.7614, 0.7568, 0.7538, 0.7516, 0.7482, 0.7534, 0.7512,\n",
       "        0.7728, 0.7732, 0.7754, 0.7588, 0.7544, 0.761 , 0.7464, 0.7462,\n",
       "        0.7476, 0.7436, 0.734 , 0.7394, 0.7726, 0.7686, 0.7678, 0.7602,\n",
       "        0.7662, 0.7554, 0.7442, 0.748 , 0.7378, 0.7386, 0.7404, 0.7292,\n",
       "        0.7698, 0.769 , 0.767 , 0.759 , 0.7544, 0.7582, 0.7572, 0.7434,\n",
       "        0.7456, 0.735 , 0.7446, 0.7428, 0.7634, 0.7728, 0.768 , 0.7668,\n",
       "        0.7564, 0.762 , 0.757 , 0.7534, 0.7538, 0.7396, 0.7452, 0.7478,\n",
       "        0.781 , 0.519 , 0.782 , 0.7804, 0.5188, 0.7774, 0.7832, 0.4802,\n",
       "        0.7834, 0.7824, 0.532 , 0.7824, 0.7824, 0.4964, 0.7792, 0.7806,\n",
       "        0.5248, 0.781 , 0.7804, 0.5276, 0.7792, 0.7834, 0.5238, 0.7816,\n",
       "        0.7788, 0.496 , 0.783 , 0.7798, 0.5502, 0.782 , 0.7838, 0.4998,\n",
       "        0.78  , 0.7796, 0.5158, 0.7804, 0.7766, 0.4982, 0.7806, 0.7832,\n",
       "        0.4794, 0.7774, 0.7812, 0.5292, 0.7812, 0.78  , 0.508 , 0.7808,\n",
       "        0.7762, 0.5982, 0.7808, 0.7768, 0.5068, 0.7762, 0.7764, 0.5878,\n",
       "        0.7772, 0.776 , 0.664 , 0.7786, 0.7816, 0.598 , 0.778 , 0.7814,\n",
       "        0.6172, 0.7766, 0.777 , 0.6966, 0.7744, 0.7774, 0.6528, 0.7792,\n",
       "        0.7762, 0.5122, 0.7788, 0.781 , 0.6262, 0.7784, 0.7852, 0.6438,\n",
       "        0.7808, 0.7806, 0.5464, 0.7786, 0.777 , 0.5534, 0.7782, 0.778 ,\n",
       "        0.5794, 0.777 , 0.782 , 0.6148, 0.7778, 0.7746, 0.5938, 0.7788,\n",
       "        0.7834, 0.78  , 0.7814, 0.7812, 0.7824, 0.783 , 0.7826, 0.781 ,\n",
       "        0.7802, 0.781 , 0.7816, 0.7818, 0.7818, 0.7814, 0.782 , 0.7826,\n",
       "        0.7812, 0.7802, 0.781 , 0.7808, 0.7812, 0.7812, 0.7812, 0.7826,\n",
       "        0.7812, 0.78  , 0.7824, 0.7806, 0.781 , 0.7816, 0.7822, 0.782 ,\n",
       "        0.7802, 0.781 , 0.7816, 0.7812, 0.7804, 0.7802, 0.7824, 0.7794,\n",
       "        0.782 , 0.7842, 0.7818, 0.782 , 0.782 , 0.7804, 0.7808, 0.7818,\n",
       "        0.7776, 0.778 , 0.782 , 0.781 , 0.7738, 0.7788, 0.773 , 0.783 ,\n",
       "        0.7766, 0.7726, 0.777 , 0.7782, 0.7796, 0.7818, 0.7796, 0.7752,\n",
       "        0.7772, 0.778 , 0.7788, 0.7726, 0.7764, 0.7748, 0.7764, 0.7752,\n",
       "        0.7774, 0.7818, 0.779 , 0.7768, 0.7794, 0.7782, 0.7758, 0.7758,\n",
       "        0.7712, 0.7714, 0.7718, 0.777 , 0.78  , 0.7802, 0.7808, 0.7816,\n",
       "        0.7788, 0.7822, 0.7744, 0.7804, 0.7772, 0.7754, 0.7758, 0.7772]),\n",
       " 'std_test_recall_micro': array([0.0103034 , 0.0073212 , 0.00665132, 0.0116    , 0.01330564,\n",
       "        0.00979796, 0.00300666, 0.00681175, 0.01400571, 0.01227355,\n",
       "        0.01522367, 0.00997196, 0.00757364, 0.00712741, 0.00708237,\n",
       "        0.00985089, 0.01133843, 0.00581722, 0.00742967, 0.00473286,\n",
       "        0.01312098, 0.0113031 , 0.01650939, 0.0091433 , 0.01284679,\n",
       "        0.00930376, 0.00945304, 0.00792465, 0.00874071, 0.01087014,\n",
       "        0.00828493, 0.01230285, 0.01226377, 0.01330564, 0.01702234,\n",
       "        0.00780769, 0.00630555, 0.01228658, 0.00906422, 0.01125344,\n",
       "        0.01147868, 0.00496387, 0.01182201, 0.01200666, 0.0128    ,\n",
       "        0.01238386, 0.00861626, 0.01244829, 0.01045753, 0.00682349,\n",
       "        0.01370547, 0.01486472, 0.00983056, 0.00663325, 0.01564097,\n",
       "        0.00890842, 0.00958332, 0.00956243, 0.00493964, 0.00811419,\n",
       "        0.00763151, 0.00728286, 0.00830422, 0.01059056, 0.00549181,\n",
       "        0.0115343 , 0.01606736, 0.01948333, 0.01101635, 0.00835703,\n",
       "        0.01128893, 0.01094349, 0.00587878, 0.00756307, 0.00657267,\n",
       "        0.00704273, 0.01059434, 0.01108873, 0.01742871, 0.01345511,\n",
       "        0.00711618, 0.00505964, 0.01105622, 0.01552289, 0.01072567,\n",
       "        0.00730479, 0.00927362, 0.00765245, 0.00939361, 0.00846168,\n",
       "        0.01219836, 0.01048046, 0.00702567, 0.01121784, 0.01446928,\n",
       "        0.00611228, 0.00481664, 0.03906917, 0.0074027 , 0.00535164,\n",
       "        0.02592605, 0.00791454, 0.00370945, 0.02632413, 0.00496387,\n",
       "        0.00397995, 0.06748629, 0.00697424, 0.00527636, 0.08746794,\n",
       "        0.00503587, 0.00859302, 0.1086764 , 0.00379473, 0.00440908,\n",
       "        0.07307147, 0.00331059, 0.0051614 , 0.04963628, 0.00338231,\n",
       "        0.00453431, 0.01021763, 0.00460435, 0.00318748, 0.09177233,\n",
       "        0.00493964, 0.0053066 , 0.00318748, 0.00532917, 0.00387814,\n",
       "        0.13222012, 0.00313688, 0.00484149, 0.00673498, 0.0028    ,\n",
       "        0.00614492, 0.02969579, 0.00475815, 0.00411825, 0.05992462,\n",
       "        0.00549181, 0.00316228, 0.02041568, 0.00752064, 0.00752064,\n",
       "        0.10622316, 0.00879545, 0.00856505, 0.1388775 , 0.00808455,\n",
       "        0.0107629 , 0.06728269, 0.01008762, 0.00850882, 0.07052659,\n",
       "        0.00631189, 0.00496387, 0.09101428, 0.006     , 0.00671118,\n",
       "        0.06319937, 0.00861626, 0.00935949, 0.03128322, 0.00492341,\n",
       "        0.00665132, 0.08338201, 0.00913017, 0.00856505, 0.04819295,\n",
       "        0.0059127 , 0.00509902, 0.06078618, 0.00786384, 0.00705408,\n",
       "        0.10838339, 0.00908625, 0.00926499, 0.07488284, 0.00272764,\n",
       "        0.00707107, 0.06994455, 0.00667533, 0.00807465, 0.06741395,\n",
       "        0.00850882, 0.00892188, 0.0717674 , 0.00708237, 0.00349857,\n",
       "        0.06902869, 0.00713863, 0.00826075, 0.00750999, 0.00964572,\n",
       "        0.00923905, 0.00765768, 0.008     , 0.01030728, 0.00860233,\n",
       "        0.00923905, 0.01025671, 0.0102489 , 0.00861162, 0.0074135 ,\n",
       "        0.00747262, 0.00967471, 0.00833307, 0.00837616, 0.00808455,\n",
       "        0.00843801, 0.0095163 , 0.00813388, 0.00874986, 0.00919565,\n",
       "        0.00983056, 0.00837616, 0.00831865, 0.00801499, 0.00856971,\n",
       "        0.00965401, 0.00909065, 0.0076    , 0.01019804, 0.010245  ,\n",
       "        0.0087178 , 0.00926499, 0.00863481, 0.00770973, 0.00980612,\n",
       "        0.0074458 , 0.00941488, 0.00961249, 0.00808455, 0.0096    ,\n",
       "        0.01088118, 0.00829458, 0.00863944, 0.01018627, 0.01008762,\n",
       "        0.00826075, 0.01108152, 0.00678233, 0.00898888, 0.0053066 ,\n",
       "        0.0119733 , 0.01261745, 0.0072111 , 0.00649923, 0.01172348,\n",
       "        0.0121326 , 0.00630555, 0.00431741, 0.0082801 , 0.00786384,\n",
       "        0.0124964 , 0.00762627, 0.00969536, 0.00470744, 0.01259524,\n",
       "        0.0074458 , 0.00744043, 0.00757892, 0.01107068, 0.00868562,\n",
       "        0.00685274, 0.00701427, 0.01034215, 0.00257682, 0.00793473,\n",
       "        0.0124964 , 0.01133843, 0.01175415, 0.00928655, 0.01154816,\n",
       "        0.00641872, 0.00684105, 0.00757364, 0.00793473, 0.00861626,\n",
       "        0.00823165, 0.01022546, 0.00574804, 0.01070701, 0.01077775,\n",
       "        0.0119432 , 0.01301384, 0.00844748]),\n",
       " 'rank_test_recall_micro': array([176, 155, 183, 198, 199, 213, 237, 242, 222, 252, 244, 230, 168,\n",
       "        186, 184, 191, 189, 200, 224, 220, 210, 245, 255, 223, 170, 171,\n",
       "        173, 208, 185, 202, 225, 221, 205, 252, 226, 235, 164, 178, 187,\n",
       "        181, 194, 191, 206, 214, 218, 226, 216, 219, 159, 157, 147, 197,\n",
       "        212, 193, 232, 233, 230, 240, 254, 248, 161, 174, 177, 195, 182,\n",
       "        209, 239, 228, 250, 249, 246, 256, 169, 172, 179, 196, 211, 201,\n",
       "        203, 241, 234, 251, 237, 243, 188, 159, 175, 180, 207, 190, 204,\n",
       "        216, 214, 247, 236, 229,  56, 277,  23,  76, 278, 119,   8, 287,\n",
       "          4,  17, 272,  17,  15, 285,  97,  72, 275,  56,  75, 274,  97,\n",
       "          6, 276,  39, 103, 286,   9,  91, 270,  23,   3, 283,  86,  92,\n",
       "        279,  76, 134, 284,  71,   7, 288, 120,  50, 273,  54,  86, 281,\n",
       "         67, 140, 264,  67, 132, 282, 140, 138, 267, 123, 143, 258, 107,\n",
       "         39, 265, 113,  44, 262, 136, 130, 257, 153, 120, 259,  99, 140,\n",
       "        280, 101,  56, 261, 109,   1, 260,  65,  72, 271, 108, 127, 269,\n",
       "        111, 115, 268, 127,  23, 263, 117, 152, 266, 103,   4,  86,  44,\n",
       "         50,  17,   9,  12,  64,  81,  56,  39,  32,  36,  44,  23,  12,\n",
       "         50,  81,  56,  65,  47,  47,  54,  12,  50,  86,  15,  72,  56,\n",
       "         39,  21,  23,  81,  56,  39,  47,  76,  81,  17,  95,  23,   2,\n",
       "         36,  23,  23,  76,  67,  32, 118, 115,  23,  56, 156, 103, 158,\n",
       "          9, 134, 161, 130, 110,  92,  32,  92, 150, 123, 113, 103, 161,\n",
       "        137, 151, 138, 149, 120,  32, 100, 132,  95, 111, 144, 144, 167,\n",
       "        166, 164, 127,  86,  81,  67,  38, 101,  22, 153,  76, 123, 147,\n",
       "        144, 123]),\n",
       " 'split0_test_f1_micro': array([0.758, 0.767, 0.758, 0.762, 0.76 , 0.737, 0.739, 0.732, 0.753,\n",
       "        0.746, 0.736, 0.741, 0.765, 0.76 , 0.767, 0.775, 0.767, 0.757,\n",
       "        0.742, 0.756, 0.739, 0.745, 0.712, 0.742, 0.769, 0.773, 0.775,\n",
       "        0.764, 0.761, 0.745, 0.763, 0.735, 0.751, 0.729, 0.734, 0.739,\n",
       "        0.77 , 0.752, 0.747, 0.76 , 0.744, 0.757, 0.76 , 0.752, 0.757,\n",
       "        0.735, 0.753, 0.751, 0.767, 0.783, 0.768, 0.749, 0.753, 0.756,\n",
       "        0.739, 0.738, 0.736, 0.738, 0.73 , 0.729, 0.772, 0.763, 0.77 ,\n",
       "        0.772, 0.775, 0.739, 0.742, 0.737, 0.724, 0.734, 0.731, 0.726,\n",
       "        0.775, 0.768, 0.756, 0.755, 0.743, 0.758, 0.771, 0.731, 0.745,\n",
       "        0.729, 0.724, 0.723, 0.762, 0.777, 0.758, 0.767, 0.752, 0.768,\n",
       "        0.737, 0.734, 0.752, 0.737, 0.74 , 0.745, 0.776, 0.501, 0.782,\n",
       "        0.783, 0.521, 0.779, 0.777, 0.502, 0.782, 0.78 , 0.535, 0.78 ,\n",
       "        0.781, 0.502, 0.779, 0.781, 0.726, 0.781, 0.782, 0.47 , 0.78 ,\n",
       "        0.779, 0.502, 0.781, 0.778, 0.502, 0.784, 0.777, 0.52 , 0.78 ,\n",
       "        0.786, 0.502, 0.78 , 0.78 , 0.324, 0.781, 0.779, 0.502, 0.779,\n",
       "        0.783, 0.502, 0.778, 0.782, 0.502, 0.781, 0.777, 0.501, 0.775,\n",
       "        0.781, 0.694, 0.775, 0.775, 0.35 , 0.787, 0.78 , 0.71 , 0.777,\n",
       "        0.767, 0.658, 0.786, 0.782, 0.721, 0.775, 0.774, 0.692, 0.772,\n",
       "        0.779, 0.707, 0.772, 0.777, 0.702, 0.779, 0.786, 0.574, 0.779,\n",
       "        0.776, 0.697, 0.784, 0.793, 0.708, 0.787, 0.79 , 0.589, 0.78 ,\n",
       "        0.784, 0.543, 0.773, 0.773, 0.568, 0.774, 0.782, 0.497, 0.775,\n",
       "        0.777, 0.568, 0.776, 0.783, 0.774, 0.782, 0.78 , 0.78 , 0.78 ,\n",
       "        0.782, 0.778, 0.779, 0.779, 0.784, 0.782, 0.777, 0.782, 0.779,\n",
       "        0.784, 0.781, 0.781, 0.778, 0.781, 0.779, 0.779, 0.781, 0.782,\n",
       "        0.784, 0.78 , 0.784, 0.78 , 0.779, 0.78 , 0.784, 0.781, 0.778,\n",
       "        0.78 , 0.782, 0.781, 0.78 , 0.777, 0.784, 0.778, 0.785, 0.785,\n",
       "        0.782, 0.783, 0.778, 0.778, 0.781, 0.778, 0.781, 0.778, 0.781,\n",
       "        0.784, 0.78 , 0.775, 0.769, 0.783, 0.773, 0.77 , 0.778, 0.77 ,\n",
       "        0.777, 0.779, 0.779, 0.778, 0.782, 0.766, 0.785, 0.774, 0.779,\n",
       "        0.767, 0.777, 0.78 , 0.784, 0.781, 0.782, 0.771, 0.781, 0.775,\n",
       "        0.775, 0.78 , 0.77 , 0.778, 0.77 , 0.774, 0.778, 0.773, 0.776,\n",
       "        0.783, 0.782, 0.784, 0.774, 0.775, 0.783, 0.776, 0.787, 0.778]),\n",
       " 'split1_test_f1_micro': array([0.774, 0.769, 0.76 , 0.765, 0.758, 0.76 , 0.745, 0.748, 0.736,\n",
       "        0.729, 0.76 , 0.746, 0.774, 0.76 , 0.763, 0.758, 0.761, 0.757,\n",
       "        0.748, 0.752, 0.749, 0.75 , 0.733, 0.747, 0.763, 0.763, 0.765,\n",
       "        0.741, 0.782, 0.761, 0.751, 0.739, 0.755, 0.746, 0.76 , 0.751,\n",
       "        0.777, 0.769, 0.773, 0.768, 0.762, 0.755, 0.754, 0.746, 0.74 ,\n",
       "        0.759, 0.764, 0.744, 0.776, 0.771, 0.78 , 0.758, 0.757, 0.768,\n",
       "        0.765, 0.762, 0.743, 0.755, 0.729, 0.745, 0.773, 0.769, 0.767,\n",
       "        0.769, 0.76 , 0.756, 0.735, 0.76 , 0.739, 0.743, 0.752, 0.72 ,\n",
       "        0.768, 0.772, 0.773, 0.767, 0.742, 0.756, 0.734, 0.742, 0.742,\n",
       "        0.736, 0.752, 0.768, 0.76 , 0.77 , 0.781, 0.767, 0.755, 0.763,\n",
       "        0.757, 0.756, 0.754, 0.734, 0.739, 0.745, 0.785, 0.597, 0.78 ,\n",
       "        0.785, 0.533, 0.775, 0.781, 0.502, 0.786, 0.785, 0.451, 0.78 ,\n",
       "        0.781, 0.582, 0.776, 0.782, 0.503, 0.777, 0.784, 0.502, 0.778,\n",
       "        0.779, 0.5  , 0.784, 0.778, 0.502, 0.778, 0.776, 0.498, 0.785,\n",
       "        0.782, 0.502, 0.775, 0.78 , 0.505, 0.777, 0.772, 0.499, 0.78 ,\n",
       "        0.775, 0.441, 0.779, 0.784, 0.48 , 0.784, 0.778, 0.491, 0.786,\n",
       "        0.776, 0.411, 0.784, 0.784, 0.375, 0.768, 0.783, 0.503, 0.777,\n",
       "        0.773, 0.538, 0.772, 0.788, 0.464, 0.787, 0.779, 0.57 , 0.78 ,\n",
       "        0.774, 0.684, 0.773, 0.773, 0.518, 0.78 , 0.772, 0.512, 0.78 ,\n",
       "        0.789, 0.587, 0.773, 0.789, 0.449, 0.773, 0.771, 0.456, 0.777,\n",
       "        0.771, 0.521, 0.781, 0.78 , 0.462, 0.78 , 0.776, 0.573, 0.785,\n",
       "        0.772, 0.644, 0.776, 0.784, 0.779, 0.777, 0.781, 0.782, 0.782,\n",
       "        0.784, 0.779, 0.782, 0.779, 0.781, 0.782, 0.781, 0.778, 0.781,\n",
       "        0.783, 0.784, 0.778, 0.781, 0.782, 0.78 , 0.78 , 0.782, 0.783,\n",
       "        0.782, 0.782, 0.78 , 0.782, 0.781, 0.781, 0.782, 0.783, 0.78 ,\n",
       "        0.782, 0.782, 0.781, 0.779, 0.78 , 0.78 , 0.782, 0.782, 0.784,\n",
       "        0.782, 0.784, 0.78 , 0.78 , 0.78 , 0.784, 0.776, 0.779, 0.78 ,\n",
       "        0.778, 0.77 , 0.768, 0.766, 0.78 , 0.773, 0.77 , 0.779, 0.775,\n",
       "        0.779, 0.779, 0.772, 0.781, 0.781, 0.784, 0.776, 0.771, 0.768,\n",
       "        0.767, 0.773, 0.774, 0.771, 0.779, 0.777, 0.77 , 0.778, 0.774,\n",
       "        0.767, 0.771, 0.766, 0.774, 0.774, 0.773, 0.78 , 0.784, 0.789,\n",
       "        0.781, 0.778, 0.771, 0.771, 0.785, 0.764, 0.775, 0.775, 0.774]),\n",
       " 'split2_test_f1_micro': array([0.763, 0.777, 0.773, 0.759, 0.767, 0.761, 0.746, 0.748, 0.752,\n",
       "        0.743, 0.75 , 0.766, 0.768, 0.763, 0.76 , 0.764, 0.762, 0.76 ,\n",
       "        0.757, 0.745, 0.761, 0.744, 0.728, 0.764, 0.774, 0.773, 0.778,\n",
       "        0.756, 0.763, 0.752, 0.743, 0.768, 0.765, 0.749, 0.765, 0.75 ,\n",
       "        0.779, 0.769, 0.77 , 0.775, 0.764, 0.763, 0.752, 0.754, 0.765,\n",
       "        0.766, 0.742, 0.764, 0.77 , 0.775, 0.795, 0.761, 0.758, 0.77 ,\n",
       "        0.725, 0.741, 0.741, 0.732, 0.74 , 0.731, 0.774, 0.772, 0.768,\n",
       "        0.764, 0.766, 0.761, 0.75 , 0.768, 0.746, 0.732, 0.729, 0.749,\n",
       "        0.766, 0.773, 0.774, 0.757, 0.768, 0.767, 0.78 , 0.747, 0.759,\n",
       "        0.744, 0.745, 0.743, 0.758, 0.766, 0.777, 0.771, 0.755, 0.763,\n",
       "        0.775, 0.765, 0.755, 0.75 , 0.769, 0.76 , 0.78 , 0.503, 0.781,\n",
       "        0.778, 0.497, 0.781, 0.785, 0.45 , 0.776, 0.779, 0.504, 0.78 ,\n",
       "        0.787, 0.335, 0.776, 0.779, 0.497, 0.783, 0.778, 0.497, 0.775,\n",
       "        0.782, 0.497, 0.781, 0.773, 0.497, 0.783, 0.783, 0.733, 0.776,\n",
       "        0.78 , 0.495, 0.776, 0.778, 0.74 , 0.78 , 0.774, 0.485, 0.78 ,\n",
       "        0.786, 0.497, 0.774, 0.78 , 0.514, 0.775, 0.779, 0.548, 0.782,\n",
       "        0.772, 0.55 , 0.786, 0.766, 0.471, 0.77 , 0.765, 0.579, 0.773,\n",
       "        0.776, 0.663, 0.777, 0.776, 0.532, 0.778, 0.781, 0.585, 0.77 ,\n",
       "        0.779, 0.651, 0.77 , 0.776, 0.593, 0.783, 0.769, 0.476, 0.774,\n",
       "        0.777, 0.552, 0.774, 0.782, 0.771, 0.781, 0.785, 0.618, 0.775,\n",
       "        0.773, 0.57 , 0.772, 0.777, 0.64 , 0.779, 0.779, 0.698, 0.778,\n",
       "        0.778, 0.502, 0.775, 0.779, 0.78 , 0.779, 0.78 , 0.778, 0.781,\n",
       "        0.779, 0.78 , 0.779, 0.783, 0.779, 0.777, 0.78 , 0.782, 0.782,\n",
       "        0.78 , 0.779, 0.776, 0.78 , 0.781, 0.78 , 0.78 , 0.78 , 0.778,\n",
       "        0.78 , 0.778, 0.777, 0.78 , 0.779, 0.779, 0.778, 0.777, 0.776,\n",
       "        0.778, 0.779, 0.779, 0.779, 0.778, 0.777, 0.776, 0.781, 0.781,\n",
       "        0.779, 0.779, 0.78 , 0.781, 0.779, 0.78 , 0.774, 0.778, 0.782,\n",
       "        0.772, 0.772, 0.785, 0.768, 0.782, 0.771, 0.771, 0.774, 0.78 ,\n",
       "        0.778, 0.782, 0.78 , 0.779, 0.772, 0.777, 0.775, 0.766, 0.776,\n",
       "        0.776, 0.767, 0.772, 0.773, 0.782, 0.771, 0.776, 0.782, 0.779,\n",
       "        0.775, 0.772, 0.765, 0.776, 0.769, 0.778, 0.775, 0.778, 0.78 ,\n",
       "        0.773, 0.781, 0.785, 0.774, 0.778, 0.771, 0.771, 0.772, 0.776]),\n",
       " 'split3_test_f1_micro': array([0.759, 0.77 , 0.763, 0.737, 0.734, 0.749, 0.748, 0.738, 0.736,\n",
       "        0.713, 0.715, 0.737, 0.763, 0.763, 0.758, 0.745, 0.743, 0.75 ,\n",
       "        0.74 , 0.746, 0.748, 0.719, 0.733, 0.757, 0.751, 0.755, 0.752,\n",
       "        0.76 , 0.757, 0.754, 0.739, 0.753, 0.739, 0.712, 0.722, 0.733,\n",
       "        0.761, 0.758, 0.765, 0.749, 0.755, 0.763, 0.741, 0.741, 0.733,\n",
       "        0.736, 0.746, 0.732, 0.76 , 0.762, 0.754, 0.741, 0.737, 0.754,\n",
       "        0.739, 0.74 , 0.759, 0.755, 0.74 , 0.742, 0.76 , 0.759, 0.754,\n",
       "        0.744, 0.761, 0.748, 0.723, 0.715, 0.727, 0.731, 0.734, 0.719,\n",
       "        0.762, 0.755, 0.764, 0.749, 0.755, 0.739, 0.741, 0.73 , 0.738,\n",
       "        0.732, 0.746, 0.731, 0.753, 0.766, 0.76 , 0.753, 0.746, 0.746,\n",
       "        0.755, 0.759, 0.743, 0.723, 0.726, 0.744, 0.776, 0.497, 0.772,\n",
       "        0.771, 0.558, 0.764, 0.786, 0.446, 0.782, 0.779, 0.515, 0.776,\n",
       "        0.774, 0.497, 0.776, 0.767, 0.395, 0.777, 0.773, 0.672, 0.778,\n",
       "        0.784, 0.497, 0.776, 0.778, 0.503, 0.779, 0.779, 0.497, 0.779,\n",
       "        0.778, 0.503, 0.779, 0.774, 0.507, 0.778, 0.773, 0.503, 0.778,\n",
       "        0.779, 0.446, 0.771, 0.774, 0.503, 0.776, 0.78 , 0.503, 0.77 ,\n",
       "        0.765, 0.66 , 0.767, 0.77 , 0.686, 0.771, 0.763, 0.57 , 0.764,\n",
       "        0.772, 0.723, 0.772, 0.776, 0.614, 0.769, 0.779, 0.545, 0.769,\n",
       "        0.762, 0.694, 0.773, 0.771, 0.722, 0.763, 0.767, 0.555, 0.772,\n",
       "        0.778, 0.7  , 0.77 , 0.773, 0.63 , 0.769, 0.768, 0.614, 0.778,\n",
       "        0.77 , 0.46 , 0.775, 0.768, 0.651, 0.763, 0.774, 0.665, 0.766,\n",
       "        0.769, 0.698, 0.774, 0.773, 0.773, 0.77 , 0.768, 0.775, 0.774,\n",
       "        0.768, 0.771, 0.766, 0.766, 0.766, 0.771, 0.775, 0.771, 0.769,\n",
       "        0.77 , 0.768, 0.771, 0.77 , 0.765, 0.771, 0.77 , 0.767, 0.77 ,\n",
       "        0.767, 0.767, 0.774, 0.767, 0.768, 0.77 , 0.772, 0.769, 0.768,\n",
       "        0.769, 0.768, 0.769, 0.77 , 0.768, 0.775, 0.766, 0.766, 0.773,\n",
       "        0.768, 0.765, 0.774, 0.768, 0.766, 0.768, 0.766, 0.76 , 0.773,\n",
       "        0.774, 0.767, 0.767, 0.764, 0.774, 0.777, 0.758, 0.758, 0.777,\n",
       "        0.776, 0.772, 0.773, 0.751, 0.765, 0.77 , 0.774, 0.757, 0.77 ,\n",
       "        0.777, 0.775, 0.758, 0.768, 0.773, 0.774, 0.77 , 0.775, 0.77 ,\n",
       "        0.763, 0.761, 0.761, 0.753, 0.755, 0.771, 0.774, 0.773, 0.769,\n",
       "        0.774, 0.764, 0.772, 0.768, 0.766, 0.773, 0.759, 0.754, 0.766]),\n",
       " 'split4_test_f1_micro': array([0.785, 0.787, 0.774, 0.771, 0.773, 0.763, 0.745, 0.749, 0.774,\n",
       "        0.742, 0.745, 0.748, 0.784, 0.779, 0.778, 0.765, 0.778, 0.768,\n",
       "        0.758, 0.756, 0.777, 0.748, 0.763, 0.74 , 0.79 , 0.782, 0.774,\n",
       "        0.759, 0.762, 0.777, 0.746, 0.759, 0.775, 0.737, 0.76 , 0.753,\n",
       "        0.772, 0.788, 0.766, 0.781, 0.779, 0.769, 0.777, 0.776, 0.763,\n",
       "        0.745, 0.762, 0.765, 0.791, 0.775, 0.78 , 0.785, 0.767, 0.757,\n",
       "        0.764, 0.75 , 0.759, 0.738, 0.731, 0.75 , 0.784, 0.78 , 0.78 ,\n",
       "        0.752, 0.769, 0.773, 0.771, 0.76 , 0.753, 0.753, 0.756, 0.732,\n",
       "        0.778, 0.777, 0.768, 0.767, 0.764, 0.771, 0.76 , 0.767, 0.744,\n",
       "        0.734, 0.756, 0.749, 0.784, 0.785, 0.764, 0.776, 0.774, 0.77 ,\n",
       "        0.761, 0.753, 0.765, 0.754, 0.752, 0.745, 0.788, 0.497, 0.795,\n",
       "        0.785, 0.485, 0.788, 0.787, 0.501, 0.791, 0.789, 0.655, 0.796,\n",
       "        0.789, 0.566, 0.789, 0.794, 0.503, 0.787, 0.785, 0.497, 0.785,\n",
       "        0.793, 0.623, 0.786, 0.787, 0.476, 0.791, 0.784, 0.503, 0.79 ,\n",
       "        0.793, 0.497, 0.79 , 0.786, 0.503, 0.786, 0.785, 0.502, 0.786,\n",
       "        0.793, 0.511, 0.785, 0.786, 0.647, 0.79 , 0.786, 0.497, 0.791,\n",
       "        0.787, 0.676, 0.792, 0.789, 0.652, 0.785, 0.791, 0.577, 0.795,\n",
       "        0.792, 0.738, 0.786, 0.786, 0.659, 0.781, 0.794, 0.694, 0.792,\n",
       "        0.791, 0.747, 0.784, 0.79 , 0.729, 0.791, 0.787, 0.444, 0.789,\n",
       "        0.785, 0.595, 0.791, 0.789, 0.661, 0.794, 0.789, 0.455, 0.783,\n",
       "        0.787, 0.673, 0.79 , 0.792, 0.576, 0.789, 0.799, 0.641, 0.785,\n",
       "        0.777, 0.557, 0.793, 0.798, 0.794, 0.799, 0.797, 0.797, 0.798,\n",
       "        0.8  , 0.797, 0.795, 0.798, 0.798, 0.797, 0.796, 0.794, 0.799,\n",
       "        0.796, 0.794, 0.795, 0.796, 0.795, 0.796, 0.797, 0.796, 0.8  ,\n",
       "        0.793, 0.793, 0.797, 0.794, 0.798, 0.798, 0.795, 0.8  , 0.799,\n",
       "        0.796, 0.797, 0.796, 0.794, 0.798, 0.796, 0.795, 0.796, 0.798,\n",
       "        0.798, 0.799, 0.798, 0.795, 0.798, 0.799, 0.791, 0.795, 0.794,\n",
       "        0.797, 0.78 , 0.799, 0.798, 0.796, 0.789, 0.794, 0.796, 0.789,\n",
       "        0.788, 0.797, 0.794, 0.787, 0.786, 0.793, 0.784, 0.795, 0.789,\n",
       "        0.787, 0.79 , 0.792, 0.791, 0.794, 0.791, 0.797, 0.781, 0.793,\n",
       "        0.799, 0.795, 0.794, 0.776, 0.791, 0.789, 0.793, 0.793, 0.79 ,\n",
       "        0.797, 0.789, 0.799, 0.785, 0.798, 0.795, 0.796, 0.791, 0.792]),\n",
       " 'mean_test_f1_micro': array([0.7678, 0.774 , 0.7656, 0.7588, 0.7584, 0.754 , 0.7446, 0.743 ,\n",
       "        0.7502, 0.7346, 0.7412, 0.7476, 0.7708, 0.765 , 0.7652, 0.7614,\n",
       "        0.7622, 0.7584, 0.749 , 0.751 , 0.7548, 0.7412, 0.7338, 0.75  ,\n",
       "        0.7694, 0.7692, 0.7688, 0.756 , 0.765 , 0.7578, 0.7484, 0.7508,\n",
       "        0.757 , 0.7346, 0.7482, 0.7452, 0.7718, 0.7672, 0.7642, 0.7666,\n",
       "        0.7608, 0.7614, 0.7568, 0.7538, 0.7516, 0.7482, 0.7534, 0.7512,\n",
       "        0.7728, 0.7732, 0.7754, 0.7588, 0.7544, 0.761 , 0.7464, 0.7462,\n",
       "        0.7476, 0.7436, 0.734 , 0.7394, 0.7726, 0.7686, 0.7678, 0.7602,\n",
       "        0.7662, 0.7554, 0.7442, 0.748 , 0.7378, 0.7386, 0.7404, 0.7292,\n",
       "        0.7698, 0.769 , 0.767 , 0.759 , 0.7544, 0.7582, 0.7572, 0.7434,\n",
       "        0.7456, 0.735 , 0.7446, 0.7428, 0.7634, 0.7728, 0.768 , 0.7668,\n",
       "        0.7564, 0.762 , 0.757 , 0.7534, 0.7538, 0.7396, 0.7452, 0.7478,\n",
       "        0.781 , 0.519 , 0.782 , 0.7804, 0.5188, 0.7774, 0.7832, 0.4802,\n",
       "        0.7834, 0.7824, 0.532 , 0.7824, 0.7824, 0.4964, 0.7792, 0.7806,\n",
       "        0.5248, 0.781 , 0.7804, 0.5276, 0.7792, 0.7834, 0.5238, 0.7816,\n",
       "        0.7788, 0.496 , 0.783 , 0.7798, 0.5502, 0.782 , 0.7838, 0.4998,\n",
       "        0.78  , 0.7796, 0.5158, 0.7804, 0.7766, 0.4982, 0.7806, 0.7832,\n",
       "        0.4794, 0.7774, 0.7812, 0.5292, 0.7812, 0.78  , 0.508 , 0.7808,\n",
       "        0.7762, 0.5982, 0.7808, 0.7768, 0.5068, 0.7762, 0.7764, 0.5878,\n",
       "        0.7772, 0.776 , 0.664 , 0.7786, 0.7816, 0.598 , 0.778 , 0.7814,\n",
       "        0.6172, 0.7766, 0.777 , 0.6966, 0.7744, 0.7774, 0.6528, 0.7792,\n",
       "        0.7762, 0.5122, 0.7788, 0.781 , 0.6262, 0.7784, 0.7852, 0.6438,\n",
       "        0.7808, 0.7806, 0.5464, 0.7786, 0.777 , 0.5534, 0.7782, 0.778 ,\n",
       "        0.5794, 0.777 , 0.782 , 0.6148, 0.7778, 0.7746, 0.5938, 0.7788,\n",
       "        0.7834, 0.78  , 0.7814, 0.7812, 0.7824, 0.783 , 0.7826, 0.781 ,\n",
       "        0.7802, 0.781 , 0.7816, 0.7818, 0.7818, 0.7814, 0.782 , 0.7826,\n",
       "        0.7812, 0.7802, 0.781 , 0.7808, 0.7812, 0.7812, 0.7812, 0.7826,\n",
       "        0.7812, 0.78  , 0.7824, 0.7806, 0.781 , 0.7816, 0.7822, 0.782 ,\n",
       "        0.7802, 0.781 , 0.7816, 0.7812, 0.7804, 0.7802, 0.7824, 0.7794,\n",
       "        0.782 , 0.7842, 0.7818, 0.782 , 0.782 , 0.7804, 0.7808, 0.7818,\n",
       "        0.7776, 0.778 , 0.782 , 0.781 , 0.7738, 0.7788, 0.773 , 0.783 ,\n",
       "        0.7766, 0.7726, 0.777 , 0.7782, 0.7796, 0.7818, 0.7796, 0.7752,\n",
       "        0.7772, 0.778 , 0.7788, 0.7726, 0.7764, 0.7748, 0.7764, 0.7752,\n",
       "        0.7774, 0.7818, 0.779 , 0.7768, 0.7794, 0.7782, 0.7758, 0.7758,\n",
       "        0.7712, 0.7714, 0.7718, 0.777 , 0.78  , 0.7802, 0.7808, 0.7816,\n",
       "        0.7788, 0.7822, 0.7744, 0.7804, 0.7772, 0.7754, 0.7758, 0.7772]),\n",
       " 'std_test_f1_micro': array([0.0103034 , 0.0073212 , 0.00665132, 0.0116    , 0.01330564,\n",
       "        0.00979796, 0.00300666, 0.00681175, 0.01400571, 0.01227355,\n",
       "        0.01522367, 0.00997196, 0.00757364, 0.00712741, 0.00708237,\n",
       "        0.00985089, 0.01133843, 0.00581722, 0.00742967, 0.00473286,\n",
       "        0.01312098, 0.0113031 , 0.01650939, 0.0091433 , 0.01284679,\n",
       "        0.00930376, 0.00945304, 0.00792465, 0.00874071, 0.01087014,\n",
       "        0.00828493, 0.01230285, 0.01226377, 0.01330564, 0.01702234,\n",
       "        0.00780769, 0.00630555, 0.01228658, 0.00906422, 0.01125344,\n",
       "        0.01147868, 0.00496387, 0.01182201, 0.01200666, 0.0128    ,\n",
       "        0.01238386, 0.00861626, 0.01244829, 0.01045753, 0.00682349,\n",
       "        0.01370547, 0.01486472, 0.00983056, 0.00663325, 0.01564097,\n",
       "        0.00890842, 0.00958332, 0.00956243, 0.00493964, 0.00811419,\n",
       "        0.00763151, 0.00728286, 0.00830422, 0.01059056, 0.00549181,\n",
       "        0.0115343 , 0.01606736, 0.01948333, 0.01101635, 0.00835703,\n",
       "        0.01128893, 0.01094349, 0.00587878, 0.00756307, 0.00657267,\n",
       "        0.00704273, 0.01059434, 0.01108873, 0.01742871, 0.01345511,\n",
       "        0.00711618, 0.00505964, 0.01105622, 0.01552289, 0.01072567,\n",
       "        0.00730479, 0.00927362, 0.00765245, 0.00939361, 0.00846168,\n",
       "        0.01219836, 0.01048046, 0.00702567, 0.01121784, 0.01446928,\n",
       "        0.00611228, 0.00481664, 0.03906917, 0.0074027 , 0.00535164,\n",
       "        0.02592605, 0.00791454, 0.00370945, 0.02632413, 0.00496387,\n",
       "        0.00397995, 0.06748629, 0.00697424, 0.00527636, 0.08746794,\n",
       "        0.00503587, 0.00859302, 0.1086764 , 0.00379473, 0.00440908,\n",
       "        0.07307147, 0.00331059, 0.0051614 , 0.04963628, 0.00338231,\n",
       "        0.00453431, 0.01021763, 0.00460435, 0.00318748, 0.09177233,\n",
       "        0.00493964, 0.0053066 , 0.00318748, 0.00532917, 0.00387814,\n",
       "        0.13222012, 0.00313688, 0.00484149, 0.00673498, 0.0028    ,\n",
       "        0.00614492, 0.02969579, 0.00475815, 0.00411825, 0.05992462,\n",
       "        0.00549181, 0.00316228, 0.02041568, 0.00752064, 0.00752064,\n",
       "        0.10622316, 0.00879545, 0.00856505, 0.1388775 , 0.00808455,\n",
       "        0.0107629 , 0.06728269, 0.01008762, 0.00850882, 0.07052659,\n",
       "        0.00631189, 0.00496387, 0.09101428, 0.006     , 0.00671118,\n",
       "        0.06319937, 0.00861626, 0.00935949, 0.03128322, 0.00492341,\n",
       "        0.00665132, 0.08338201, 0.00913017, 0.00856505, 0.04819295,\n",
       "        0.0059127 , 0.00509902, 0.06078618, 0.00786384, 0.00705408,\n",
       "        0.10838339, 0.00908625, 0.00926499, 0.07488284, 0.00272764,\n",
       "        0.00707107, 0.06994455, 0.00667533, 0.00807465, 0.06741395,\n",
       "        0.00850882, 0.00892188, 0.0717674 , 0.00708237, 0.00349857,\n",
       "        0.06902869, 0.00713863, 0.00826075, 0.00750999, 0.00964572,\n",
       "        0.00923905, 0.00765768, 0.008     , 0.01030728, 0.00860233,\n",
       "        0.00923905, 0.01025671, 0.0102489 , 0.00861162, 0.0074135 ,\n",
       "        0.00747262, 0.00967471, 0.00833307, 0.00837616, 0.00808455,\n",
       "        0.00843801, 0.0095163 , 0.00813388, 0.00874986, 0.00919565,\n",
       "        0.00983056, 0.00837616, 0.00831865, 0.00801499, 0.00856971,\n",
       "        0.00965401, 0.00909065, 0.0076    , 0.01019804, 0.010245  ,\n",
       "        0.0087178 , 0.00926499, 0.00863481, 0.00770973, 0.00980612,\n",
       "        0.0074458 , 0.00941488, 0.00961249, 0.00808455, 0.0096    ,\n",
       "        0.01088118, 0.00829458, 0.00863944, 0.01018627, 0.01008762,\n",
       "        0.00826075, 0.01108152, 0.00678233, 0.00898888, 0.0053066 ,\n",
       "        0.0119733 , 0.01261745, 0.0072111 , 0.00649923, 0.01172348,\n",
       "        0.0121326 , 0.00630555, 0.00431741, 0.0082801 , 0.00786384,\n",
       "        0.0124964 , 0.00762627, 0.00969536, 0.00470744, 0.01259524,\n",
       "        0.0074458 , 0.00744043, 0.00757892, 0.01107068, 0.00868562,\n",
       "        0.00685274, 0.00701427, 0.01034215, 0.00257682, 0.00793473,\n",
       "        0.0124964 , 0.01133843, 0.01175415, 0.00928655, 0.01154816,\n",
       "        0.00641872, 0.00684105, 0.00757364, 0.00793473, 0.00861626,\n",
       "        0.00823165, 0.01022546, 0.00574804, 0.01070701, 0.01077775,\n",
       "        0.0119432 , 0.01301384, 0.00844748]),\n",
       " 'rank_test_f1_micro': array([176, 155, 183, 197, 199, 213, 237, 242, 222, 252, 245, 230, 168,\n",
       "        186, 184, 191, 189, 199, 224, 220, 210, 244, 255, 223, 170, 171,\n",
       "        173, 208, 185, 202, 225, 221, 205, 252, 227, 235, 164, 178, 187,\n",
       "        181, 194, 191, 206, 214, 218, 226, 217, 219, 159, 157, 148, 198,\n",
       "        212, 193, 232, 233, 230, 240, 254, 248, 161, 174, 177, 195, 182,\n",
       "        209, 239, 228, 250, 249, 246, 256, 169, 172, 179, 196, 211, 201,\n",
       "        203, 241, 234, 251, 237, 243, 188, 160, 175, 180, 207, 190, 204,\n",
       "        216, 214, 247, 236, 229,  57, 277,  24,  75, 278, 119,   7, 287,\n",
       "          4,  17, 272,  17,  15, 285,  97,  72, 275,  57,  75, 274,  97,\n",
       "          4, 276,  38, 106, 286,   9,  91, 270,  24,   3, 283,  86,  92,\n",
       "        279,  75, 134, 284,  71,   8, 288, 120,  51, 273,  47,  86, 281,\n",
       "         69, 140, 264,  69, 132, 282, 140, 138, 267, 123, 143, 258, 107,\n",
       "         41, 265, 114,  44, 262, 134, 130, 257, 153, 122, 259,  99, 140,\n",
       "        280, 101,  57, 261, 109,   1, 260,  65,  72, 271, 108, 127, 269,\n",
       "        111, 115, 268, 127,  24, 263, 117, 152, 266, 103,   6,  86,  44,\n",
       "         51,  17,   9,  12,  64,  81,  57,  39,  32,  36,  44,  24,  13,\n",
       "         51,  81,  57,  65,  47,  47,  55,  13,  51,  86,  15,  72,  57,\n",
       "         39,  21,  23,  81,  57,  41,  47,  75,  81,  17,  95,  24,   2,\n",
       "         36,  24,  24,  75,  65,  32, 118, 115,  31,  56, 156, 103, 158,\n",
       "          9, 134, 161, 130, 110,  92,  32,  92, 150, 123, 113, 103, 161,\n",
       "        137, 151, 138, 149, 120,  32, 100, 132,  95, 111, 144, 144, 167,\n",
       "        166, 164, 127,  86,  81,  65,  41, 101,  22, 154,  75, 123, 147,\n",
       "        144, 123]),\n",
       " 'split0_test_precision_micro': array([0.758, 0.767, 0.758, 0.762, 0.76 , 0.737, 0.739, 0.732, 0.753,\n",
       "        0.746, 0.736, 0.741, 0.765, 0.76 , 0.767, 0.775, 0.767, 0.757,\n",
       "        0.742, 0.756, 0.739, 0.745, 0.712, 0.742, 0.769, 0.773, 0.775,\n",
       "        0.764, 0.761, 0.745, 0.763, 0.735, 0.751, 0.729, 0.734, 0.739,\n",
       "        0.77 , 0.752, 0.747, 0.76 , 0.744, 0.757, 0.76 , 0.752, 0.757,\n",
       "        0.735, 0.753, 0.751, 0.767, 0.783, 0.768, 0.749, 0.753, 0.756,\n",
       "        0.739, 0.738, 0.736, 0.738, 0.73 , 0.729, 0.772, 0.763, 0.77 ,\n",
       "        0.772, 0.775, 0.739, 0.742, 0.737, 0.724, 0.734, 0.731, 0.726,\n",
       "        0.775, 0.768, 0.756, 0.755, 0.743, 0.758, 0.771, 0.731, 0.745,\n",
       "        0.729, 0.724, 0.723, 0.762, 0.777, 0.758, 0.767, 0.752, 0.768,\n",
       "        0.737, 0.734, 0.752, 0.737, 0.74 , 0.745, 0.776, 0.501, 0.782,\n",
       "        0.783, 0.521, 0.779, 0.777, 0.502, 0.782, 0.78 , 0.535, 0.78 ,\n",
       "        0.781, 0.502, 0.779, 0.781, 0.726, 0.781, 0.782, 0.47 , 0.78 ,\n",
       "        0.779, 0.502, 0.781, 0.778, 0.502, 0.784, 0.777, 0.52 , 0.78 ,\n",
       "        0.786, 0.502, 0.78 , 0.78 , 0.324, 0.781, 0.779, 0.502, 0.779,\n",
       "        0.783, 0.502, 0.778, 0.782, 0.502, 0.781, 0.777, 0.501, 0.775,\n",
       "        0.781, 0.694, 0.775, 0.775, 0.35 , 0.787, 0.78 , 0.71 , 0.777,\n",
       "        0.767, 0.658, 0.786, 0.782, 0.721, 0.775, 0.774, 0.692, 0.772,\n",
       "        0.779, 0.707, 0.772, 0.777, 0.702, 0.779, 0.786, 0.574, 0.779,\n",
       "        0.776, 0.697, 0.784, 0.793, 0.708, 0.787, 0.79 , 0.589, 0.78 ,\n",
       "        0.784, 0.543, 0.773, 0.773, 0.568, 0.774, 0.782, 0.497, 0.775,\n",
       "        0.777, 0.568, 0.776, 0.783, 0.774, 0.782, 0.78 , 0.78 , 0.78 ,\n",
       "        0.782, 0.778, 0.779, 0.779, 0.784, 0.782, 0.777, 0.782, 0.779,\n",
       "        0.784, 0.781, 0.781, 0.778, 0.781, 0.779, 0.779, 0.781, 0.782,\n",
       "        0.784, 0.78 , 0.784, 0.78 , 0.779, 0.78 , 0.784, 0.781, 0.778,\n",
       "        0.78 , 0.782, 0.781, 0.78 , 0.777, 0.784, 0.778, 0.785, 0.785,\n",
       "        0.782, 0.783, 0.778, 0.778, 0.781, 0.778, 0.781, 0.778, 0.781,\n",
       "        0.784, 0.78 , 0.775, 0.769, 0.783, 0.773, 0.77 , 0.778, 0.77 ,\n",
       "        0.777, 0.779, 0.779, 0.778, 0.782, 0.766, 0.785, 0.774, 0.779,\n",
       "        0.767, 0.777, 0.78 , 0.784, 0.781, 0.782, 0.771, 0.781, 0.775,\n",
       "        0.775, 0.78 , 0.77 , 0.778, 0.77 , 0.774, 0.778, 0.773, 0.776,\n",
       "        0.783, 0.782, 0.784, 0.774, 0.775, 0.783, 0.776, 0.787, 0.778]),\n",
       " 'split1_test_precision_micro': array([0.774, 0.769, 0.76 , 0.765, 0.758, 0.76 , 0.745, 0.748, 0.736,\n",
       "        0.729, 0.76 , 0.746, 0.774, 0.76 , 0.763, 0.758, 0.761, 0.757,\n",
       "        0.748, 0.752, 0.749, 0.75 , 0.733, 0.747, 0.763, 0.763, 0.765,\n",
       "        0.741, 0.782, 0.761, 0.751, 0.739, 0.755, 0.746, 0.76 , 0.751,\n",
       "        0.777, 0.769, 0.773, 0.768, 0.762, 0.755, 0.754, 0.746, 0.74 ,\n",
       "        0.759, 0.764, 0.744, 0.776, 0.771, 0.78 , 0.758, 0.757, 0.768,\n",
       "        0.765, 0.762, 0.743, 0.755, 0.729, 0.745, 0.773, 0.769, 0.767,\n",
       "        0.769, 0.76 , 0.756, 0.735, 0.76 , 0.739, 0.743, 0.752, 0.72 ,\n",
       "        0.768, 0.772, 0.773, 0.767, 0.742, 0.756, 0.734, 0.742, 0.742,\n",
       "        0.736, 0.752, 0.768, 0.76 , 0.77 , 0.781, 0.767, 0.755, 0.763,\n",
       "        0.757, 0.756, 0.754, 0.734, 0.739, 0.745, 0.785, 0.597, 0.78 ,\n",
       "        0.785, 0.533, 0.775, 0.781, 0.502, 0.786, 0.785, 0.451, 0.78 ,\n",
       "        0.781, 0.582, 0.776, 0.782, 0.503, 0.777, 0.784, 0.502, 0.778,\n",
       "        0.779, 0.5  , 0.784, 0.778, 0.502, 0.778, 0.776, 0.498, 0.785,\n",
       "        0.782, 0.502, 0.775, 0.78 , 0.505, 0.777, 0.772, 0.499, 0.78 ,\n",
       "        0.775, 0.441, 0.779, 0.784, 0.48 , 0.784, 0.778, 0.491, 0.786,\n",
       "        0.776, 0.411, 0.784, 0.784, 0.375, 0.768, 0.783, 0.503, 0.777,\n",
       "        0.773, 0.538, 0.772, 0.788, 0.464, 0.787, 0.779, 0.57 , 0.78 ,\n",
       "        0.774, 0.684, 0.773, 0.773, 0.518, 0.78 , 0.772, 0.512, 0.78 ,\n",
       "        0.789, 0.587, 0.773, 0.789, 0.449, 0.773, 0.771, 0.456, 0.777,\n",
       "        0.771, 0.521, 0.781, 0.78 , 0.462, 0.78 , 0.776, 0.573, 0.785,\n",
       "        0.772, 0.644, 0.776, 0.784, 0.779, 0.777, 0.781, 0.782, 0.782,\n",
       "        0.784, 0.779, 0.782, 0.779, 0.781, 0.782, 0.781, 0.778, 0.781,\n",
       "        0.783, 0.784, 0.778, 0.781, 0.782, 0.78 , 0.78 , 0.782, 0.783,\n",
       "        0.782, 0.782, 0.78 , 0.782, 0.781, 0.781, 0.782, 0.783, 0.78 ,\n",
       "        0.782, 0.782, 0.781, 0.779, 0.78 , 0.78 , 0.782, 0.782, 0.784,\n",
       "        0.782, 0.784, 0.78 , 0.78 , 0.78 , 0.784, 0.776, 0.779, 0.78 ,\n",
       "        0.778, 0.77 , 0.768, 0.766, 0.78 , 0.773, 0.77 , 0.779, 0.775,\n",
       "        0.779, 0.779, 0.772, 0.781, 0.781, 0.784, 0.776, 0.771, 0.768,\n",
       "        0.767, 0.773, 0.774, 0.771, 0.779, 0.777, 0.77 , 0.778, 0.774,\n",
       "        0.767, 0.771, 0.766, 0.774, 0.774, 0.773, 0.78 , 0.784, 0.789,\n",
       "        0.781, 0.778, 0.771, 0.771, 0.785, 0.764, 0.775, 0.775, 0.774]),\n",
       " 'split2_test_precision_micro': array([0.763, 0.777, 0.773, 0.759, 0.767, 0.761, 0.746, 0.748, 0.752,\n",
       "        0.743, 0.75 , 0.766, 0.768, 0.763, 0.76 , 0.764, 0.762, 0.76 ,\n",
       "        0.757, 0.745, 0.761, 0.744, 0.728, 0.764, 0.774, 0.773, 0.778,\n",
       "        0.756, 0.763, 0.752, 0.743, 0.768, 0.765, 0.749, 0.765, 0.75 ,\n",
       "        0.779, 0.769, 0.77 , 0.775, 0.764, 0.763, 0.752, 0.754, 0.765,\n",
       "        0.766, 0.742, 0.764, 0.77 , 0.775, 0.795, 0.761, 0.758, 0.77 ,\n",
       "        0.725, 0.741, 0.741, 0.732, 0.74 , 0.731, 0.774, 0.772, 0.768,\n",
       "        0.764, 0.766, 0.761, 0.75 , 0.768, 0.746, 0.732, 0.729, 0.749,\n",
       "        0.766, 0.773, 0.774, 0.757, 0.768, 0.767, 0.78 , 0.747, 0.759,\n",
       "        0.744, 0.745, 0.743, 0.758, 0.766, 0.777, 0.771, 0.755, 0.763,\n",
       "        0.775, 0.765, 0.755, 0.75 , 0.769, 0.76 , 0.78 , 0.503, 0.781,\n",
       "        0.778, 0.497, 0.781, 0.785, 0.45 , 0.776, 0.779, 0.504, 0.78 ,\n",
       "        0.787, 0.335, 0.776, 0.779, 0.497, 0.783, 0.778, 0.497, 0.775,\n",
       "        0.782, 0.497, 0.781, 0.773, 0.497, 0.783, 0.783, 0.733, 0.776,\n",
       "        0.78 , 0.495, 0.776, 0.778, 0.74 , 0.78 , 0.774, 0.485, 0.78 ,\n",
       "        0.786, 0.497, 0.774, 0.78 , 0.514, 0.775, 0.779, 0.548, 0.782,\n",
       "        0.772, 0.55 , 0.786, 0.766, 0.471, 0.77 , 0.765, 0.579, 0.773,\n",
       "        0.776, 0.663, 0.777, 0.776, 0.532, 0.778, 0.781, 0.585, 0.77 ,\n",
       "        0.779, 0.651, 0.77 , 0.776, 0.593, 0.783, 0.769, 0.476, 0.774,\n",
       "        0.777, 0.552, 0.774, 0.782, 0.771, 0.781, 0.785, 0.618, 0.775,\n",
       "        0.773, 0.57 , 0.772, 0.777, 0.64 , 0.779, 0.779, 0.698, 0.778,\n",
       "        0.778, 0.502, 0.775, 0.779, 0.78 , 0.779, 0.78 , 0.778, 0.781,\n",
       "        0.779, 0.78 , 0.779, 0.783, 0.779, 0.777, 0.78 , 0.782, 0.782,\n",
       "        0.78 , 0.779, 0.776, 0.78 , 0.781, 0.78 , 0.78 , 0.78 , 0.778,\n",
       "        0.78 , 0.778, 0.777, 0.78 , 0.779, 0.779, 0.778, 0.777, 0.776,\n",
       "        0.778, 0.779, 0.779, 0.779, 0.778, 0.777, 0.776, 0.781, 0.781,\n",
       "        0.779, 0.779, 0.78 , 0.781, 0.779, 0.78 , 0.774, 0.778, 0.782,\n",
       "        0.772, 0.772, 0.785, 0.768, 0.782, 0.771, 0.771, 0.774, 0.78 ,\n",
       "        0.778, 0.782, 0.78 , 0.779, 0.772, 0.777, 0.775, 0.766, 0.776,\n",
       "        0.776, 0.767, 0.772, 0.773, 0.782, 0.771, 0.776, 0.782, 0.779,\n",
       "        0.775, 0.772, 0.765, 0.776, 0.769, 0.778, 0.775, 0.778, 0.78 ,\n",
       "        0.773, 0.781, 0.785, 0.774, 0.778, 0.771, 0.771, 0.772, 0.776]),\n",
       " 'split3_test_precision_micro': array([0.759, 0.77 , 0.763, 0.737, 0.734, 0.749, 0.748, 0.738, 0.736,\n",
       "        0.713, 0.715, 0.737, 0.763, 0.763, 0.758, 0.745, 0.743, 0.75 ,\n",
       "        0.74 , 0.746, 0.748, 0.719, 0.733, 0.757, 0.751, 0.755, 0.752,\n",
       "        0.76 , 0.757, 0.754, 0.739, 0.753, 0.739, 0.712, 0.722, 0.733,\n",
       "        0.761, 0.758, 0.765, 0.749, 0.755, 0.763, 0.741, 0.741, 0.733,\n",
       "        0.736, 0.746, 0.732, 0.76 , 0.762, 0.754, 0.741, 0.737, 0.754,\n",
       "        0.739, 0.74 , 0.759, 0.755, 0.74 , 0.742, 0.76 , 0.759, 0.754,\n",
       "        0.744, 0.761, 0.748, 0.723, 0.715, 0.727, 0.731, 0.734, 0.719,\n",
       "        0.762, 0.755, 0.764, 0.749, 0.755, 0.739, 0.741, 0.73 , 0.738,\n",
       "        0.732, 0.746, 0.731, 0.753, 0.766, 0.76 , 0.753, 0.746, 0.746,\n",
       "        0.755, 0.759, 0.743, 0.723, 0.726, 0.744, 0.776, 0.497, 0.772,\n",
       "        0.771, 0.558, 0.764, 0.786, 0.446, 0.782, 0.779, 0.515, 0.776,\n",
       "        0.774, 0.497, 0.776, 0.767, 0.395, 0.777, 0.773, 0.672, 0.778,\n",
       "        0.784, 0.497, 0.776, 0.778, 0.503, 0.779, 0.779, 0.497, 0.779,\n",
       "        0.778, 0.503, 0.779, 0.774, 0.507, 0.778, 0.773, 0.503, 0.778,\n",
       "        0.779, 0.446, 0.771, 0.774, 0.503, 0.776, 0.78 , 0.503, 0.77 ,\n",
       "        0.765, 0.66 , 0.767, 0.77 , 0.686, 0.771, 0.763, 0.57 , 0.764,\n",
       "        0.772, 0.723, 0.772, 0.776, 0.614, 0.769, 0.779, 0.545, 0.769,\n",
       "        0.762, 0.694, 0.773, 0.771, 0.722, 0.763, 0.767, 0.555, 0.772,\n",
       "        0.778, 0.7  , 0.77 , 0.773, 0.63 , 0.769, 0.768, 0.614, 0.778,\n",
       "        0.77 , 0.46 , 0.775, 0.768, 0.651, 0.763, 0.774, 0.665, 0.766,\n",
       "        0.769, 0.698, 0.774, 0.773, 0.773, 0.77 , 0.768, 0.775, 0.774,\n",
       "        0.768, 0.771, 0.766, 0.766, 0.766, 0.771, 0.775, 0.771, 0.769,\n",
       "        0.77 , 0.768, 0.771, 0.77 , 0.765, 0.771, 0.77 , 0.767, 0.77 ,\n",
       "        0.767, 0.767, 0.774, 0.767, 0.768, 0.77 , 0.772, 0.769, 0.768,\n",
       "        0.769, 0.768, 0.769, 0.77 , 0.768, 0.775, 0.766, 0.766, 0.773,\n",
       "        0.768, 0.765, 0.774, 0.768, 0.766, 0.768, 0.766, 0.76 , 0.773,\n",
       "        0.774, 0.767, 0.767, 0.764, 0.774, 0.777, 0.758, 0.758, 0.777,\n",
       "        0.776, 0.772, 0.773, 0.751, 0.765, 0.77 , 0.774, 0.757, 0.77 ,\n",
       "        0.777, 0.775, 0.758, 0.768, 0.773, 0.774, 0.77 , 0.775, 0.77 ,\n",
       "        0.763, 0.761, 0.761, 0.753, 0.755, 0.771, 0.774, 0.773, 0.769,\n",
       "        0.774, 0.764, 0.772, 0.768, 0.766, 0.773, 0.759, 0.754, 0.766]),\n",
       " 'split4_test_precision_micro': array([0.785, 0.787, 0.774, 0.771, 0.773, 0.763, 0.745, 0.749, 0.774,\n",
       "        0.742, 0.745, 0.748, 0.784, 0.779, 0.778, 0.765, 0.778, 0.768,\n",
       "        0.758, 0.756, 0.777, 0.748, 0.763, 0.74 , 0.79 , 0.782, 0.774,\n",
       "        0.759, 0.762, 0.777, 0.746, 0.759, 0.775, 0.737, 0.76 , 0.753,\n",
       "        0.772, 0.788, 0.766, 0.781, 0.779, 0.769, 0.777, 0.776, 0.763,\n",
       "        0.745, 0.762, 0.765, 0.791, 0.775, 0.78 , 0.785, 0.767, 0.757,\n",
       "        0.764, 0.75 , 0.759, 0.738, 0.731, 0.75 , 0.784, 0.78 , 0.78 ,\n",
       "        0.752, 0.769, 0.773, 0.771, 0.76 , 0.753, 0.753, 0.756, 0.732,\n",
       "        0.778, 0.777, 0.768, 0.767, 0.764, 0.771, 0.76 , 0.767, 0.744,\n",
       "        0.734, 0.756, 0.749, 0.784, 0.785, 0.764, 0.776, 0.774, 0.77 ,\n",
       "        0.761, 0.753, 0.765, 0.754, 0.752, 0.745, 0.788, 0.497, 0.795,\n",
       "        0.785, 0.485, 0.788, 0.787, 0.501, 0.791, 0.789, 0.655, 0.796,\n",
       "        0.789, 0.566, 0.789, 0.794, 0.503, 0.787, 0.785, 0.497, 0.785,\n",
       "        0.793, 0.623, 0.786, 0.787, 0.476, 0.791, 0.784, 0.503, 0.79 ,\n",
       "        0.793, 0.497, 0.79 , 0.786, 0.503, 0.786, 0.785, 0.502, 0.786,\n",
       "        0.793, 0.511, 0.785, 0.786, 0.647, 0.79 , 0.786, 0.497, 0.791,\n",
       "        0.787, 0.676, 0.792, 0.789, 0.652, 0.785, 0.791, 0.577, 0.795,\n",
       "        0.792, 0.738, 0.786, 0.786, 0.659, 0.781, 0.794, 0.694, 0.792,\n",
       "        0.791, 0.747, 0.784, 0.79 , 0.729, 0.791, 0.787, 0.444, 0.789,\n",
       "        0.785, 0.595, 0.791, 0.789, 0.661, 0.794, 0.789, 0.455, 0.783,\n",
       "        0.787, 0.673, 0.79 , 0.792, 0.576, 0.789, 0.799, 0.641, 0.785,\n",
       "        0.777, 0.557, 0.793, 0.798, 0.794, 0.799, 0.797, 0.797, 0.798,\n",
       "        0.8  , 0.797, 0.795, 0.798, 0.798, 0.797, 0.796, 0.794, 0.799,\n",
       "        0.796, 0.794, 0.795, 0.796, 0.795, 0.796, 0.797, 0.796, 0.8  ,\n",
       "        0.793, 0.793, 0.797, 0.794, 0.798, 0.798, 0.795, 0.8  , 0.799,\n",
       "        0.796, 0.797, 0.796, 0.794, 0.798, 0.796, 0.795, 0.796, 0.798,\n",
       "        0.798, 0.799, 0.798, 0.795, 0.798, 0.799, 0.791, 0.795, 0.794,\n",
       "        0.797, 0.78 , 0.799, 0.798, 0.796, 0.789, 0.794, 0.796, 0.789,\n",
       "        0.788, 0.797, 0.794, 0.787, 0.786, 0.793, 0.784, 0.795, 0.789,\n",
       "        0.787, 0.79 , 0.792, 0.791, 0.794, 0.791, 0.797, 0.781, 0.793,\n",
       "        0.799, 0.795, 0.794, 0.776, 0.791, 0.789, 0.793, 0.793, 0.79 ,\n",
       "        0.797, 0.789, 0.799, 0.785, 0.798, 0.795, 0.796, 0.791, 0.792]),\n",
       " 'mean_test_precision_micro': array([0.7678, 0.774 , 0.7656, 0.7588, 0.7584, 0.754 , 0.7446, 0.743 ,\n",
       "        0.7502, 0.7346, 0.7412, 0.7476, 0.7708, 0.765 , 0.7652, 0.7614,\n",
       "        0.7622, 0.7584, 0.749 , 0.751 , 0.7548, 0.7412, 0.7338, 0.75  ,\n",
       "        0.7694, 0.7692, 0.7688, 0.756 , 0.765 , 0.7578, 0.7484, 0.7508,\n",
       "        0.757 , 0.7346, 0.7482, 0.7452, 0.7718, 0.7672, 0.7642, 0.7666,\n",
       "        0.7608, 0.7614, 0.7568, 0.7538, 0.7516, 0.7482, 0.7534, 0.7512,\n",
       "        0.7728, 0.7732, 0.7754, 0.7588, 0.7544, 0.761 , 0.7464, 0.7462,\n",
       "        0.7476, 0.7436, 0.734 , 0.7394, 0.7726, 0.7686, 0.7678, 0.7602,\n",
       "        0.7662, 0.7554, 0.7442, 0.748 , 0.7378, 0.7386, 0.7404, 0.7292,\n",
       "        0.7698, 0.769 , 0.767 , 0.759 , 0.7544, 0.7582, 0.7572, 0.7434,\n",
       "        0.7456, 0.735 , 0.7446, 0.7428, 0.7634, 0.7728, 0.768 , 0.7668,\n",
       "        0.7564, 0.762 , 0.757 , 0.7534, 0.7538, 0.7396, 0.7452, 0.7478,\n",
       "        0.781 , 0.519 , 0.782 , 0.7804, 0.5188, 0.7774, 0.7832, 0.4802,\n",
       "        0.7834, 0.7824, 0.532 , 0.7824, 0.7824, 0.4964, 0.7792, 0.7806,\n",
       "        0.5248, 0.781 , 0.7804, 0.5276, 0.7792, 0.7834, 0.5238, 0.7816,\n",
       "        0.7788, 0.496 , 0.783 , 0.7798, 0.5502, 0.782 , 0.7838, 0.4998,\n",
       "        0.78  , 0.7796, 0.5158, 0.7804, 0.7766, 0.4982, 0.7806, 0.7832,\n",
       "        0.4794, 0.7774, 0.7812, 0.5292, 0.7812, 0.78  , 0.508 , 0.7808,\n",
       "        0.7762, 0.5982, 0.7808, 0.7768, 0.5068, 0.7762, 0.7764, 0.5878,\n",
       "        0.7772, 0.776 , 0.664 , 0.7786, 0.7816, 0.598 , 0.778 , 0.7814,\n",
       "        0.6172, 0.7766, 0.777 , 0.6966, 0.7744, 0.7774, 0.6528, 0.7792,\n",
       "        0.7762, 0.5122, 0.7788, 0.781 , 0.6262, 0.7784, 0.7852, 0.6438,\n",
       "        0.7808, 0.7806, 0.5464, 0.7786, 0.777 , 0.5534, 0.7782, 0.778 ,\n",
       "        0.5794, 0.777 , 0.782 , 0.6148, 0.7778, 0.7746, 0.5938, 0.7788,\n",
       "        0.7834, 0.78  , 0.7814, 0.7812, 0.7824, 0.783 , 0.7826, 0.781 ,\n",
       "        0.7802, 0.781 , 0.7816, 0.7818, 0.7818, 0.7814, 0.782 , 0.7826,\n",
       "        0.7812, 0.7802, 0.781 , 0.7808, 0.7812, 0.7812, 0.7812, 0.7826,\n",
       "        0.7812, 0.78  , 0.7824, 0.7806, 0.781 , 0.7816, 0.7822, 0.782 ,\n",
       "        0.7802, 0.781 , 0.7816, 0.7812, 0.7804, 0.7802, 0.7824, 0.7794,\n",
       "        0.782 , 0.7842, 0.7818, 0.782 , 0.782 , 0.7804, 0.7808, 0.7818,\n",
       "        0.7776, 0.778 , 0.782 , 0.781 , 0.7738, 0.7788, 0.773 , 0.783 ,\n",
       "        0.7766, 0.7726, 0.777 , 0.7782, 0.7796, 0.7818, 0.7796, 0.7752,\n",
       "        0.7772, 0.778 , 0.7788, 0.7726, 0.7764, 0.7748, 0.7764, 0.7752,\n",
       "        0.7774, 0.7818, 0.779 , 0.7768, 0.7794, 0.7782, 0.7758, 0.7758,\n",
       "        0.7712, 0.7714, 0.7718, 0.777 , 0.78  , 0.7802, 0.7808, 0.7816,\n",
       "        0.7788, 0.7822, 0.7744, 0.7804, 0.7772, 0.7754, 0.7758, 0.7772]),\n",
       " 'std_test_precision_micro': array([0.0103034 , 0.0073212 , 0.00665132, 0.0116    , 0.01330564,\n",
       "        0.00979796, 0.00300666, 0.00681175, 0.01400571, 0.01227355,\n",
       "        0.01522367, 0.00997196, 0.00757364, 0.00712741, 0.00708237,\n",
       "        0.00985089, 0.01133843, 0.00581722, 0.00742967, 0.00473286,\n",
       "        0.01312098, 0.0113031 , 0.01650939, 0.0091433 , 0.01284679,\n",
       "        0.00930376, 0.00945304, 0.00792465, 0.00874071, 0.01087014,\n",
       "        0.00828493, 0.01230285, 0.01226377, 0.01330564, 0.01702234,\n",
       "        0.00780769, 0.00630555, 0.01228658, 0.00906422, 0.01125344,\n",
       "        0.01147868, 0.00496387, 0.01182201, 0.01200666, 0.0128    ,\n",
       "        0.01238386, 0.00861626, 0.01244829, 0.01045753, 0.00682349,\n",
       "        0.01370547, 0.01486472, 0.00983056, 0.00663325, 0.01564097,\n",
       "        0.00890842, 0.00958332, 0.00956243, 0.00493964, 0.00811419,\n",
       "        0.00763151, 0.00728286, 0.00830422, 0.01059056, 0.00549181,\n",
       "        0.0115343 , 0.01606736, 0.01948333, 0.01101635, 0.00835703,\n",
       "        0.01128893, 0.01094349, 0.00587878, 0.00756307, 0.00657267,\n",
       "        0.00704273, 0.01059434, 0.01108873, 0.01742871, 0.01345511,\n",
       "        0.00711618, 0.00505964, 0.01105622, 0.01552289, 0.01072567,\n",
       "        0.00730479, 0.00927362, 0.00765245, 0.00939361, 0.00846168,\n",
       "        0.01219836, 0.01048046, 0.00702567, 0.01121784, 0.01446928,\n",
       "        0.00611228, 0.00481664, 0.03906917, 0.0074027 , 0.00535164,\n",
       "        0.02592605, 0.00791454, 0.00370945, 0.02632413, 0.00496387,\n",
       "        0.00397995, 0.06748629, 0.00697424, 0.00527636, 0.08746794,\n",
       "        0.00503587, 0.00859302, 0.1086764 , 0.00379473, 0.00440908,\n",
       "        0.07307147, 0.00331059, 0.0051614 , 0.04963628, 0.00338231,\n",
       "        0.00453431, 0.01021763, 0.00460435, 0.00318748, 0.09177233,\n",
       "        0.00493964, 0.0053066 , 0.00318748, 0.00532917, 0.00387814,\n",
       "        0.13222012, 0.00313688, 0.00484149, 0.00673498, 0.0028    ,\n",
       "        0.00614492, 0.02969579, 0.00475815, 0.00411825, 0.05992462,\n",
       "        0.00549181, 0.00316228, 0.02041568, 0.00752064, 0.00752064,\n",
       "        0.10622316, 0.00879545, 0.00856505, 0.1388775 , 0.00808455,\n",
       "        0.0107629 , 0.06728269, 0.01008762, 0.00850882, 0.07052659,\n",
       "        0.00631189, 0.00496387, 0.09101428, 0.006     , 0.00671118,\n",
       "        0.06319937, 0.00861626, 0.00935949, 0.03128322, 0.00492341,\n",
       "        0.00665132, 0.08338201, 0.00913017, 0.00856505, 0.04819295,\n",
       "        0.0059127 , 0.00509902, 0.06078618, 0.00786384, 0.00705408,\n",
       "        0.10838339, 0.00908625, 0.00926499, 0.07488284, 0.00272764,\n",
       "        0.00707107, 0.06994455, 0.00667533, 0.00807465, 0.06741395,\n",
       "        0.00850882, 0.00892188, 0.0717674 , 0.00708237, 0.00349857,\n",
       "        0.06902869, 0.00713863, 0.00826075, 0.00750999, 0.00964572,\n",
       "        0.00923905, 0.00765768, 0.008     , 0.01030728, 0.00860233,\n",
       "        0.00923905, 0.01025671, 0.0102489 , 0.00861162, 0.0074135 ,\n",
       "        0.00747262, 0.00967471, 0.00833307, 0.00837616, 0.00808455,\n",
       "        0.00843801, 0.0095163 , 0.00813388, 0.00874986, 0.00919565,\n",
       "        0.00983056, 0.00837616, 0.00831865, 0.00801499, 0.00856971,\n",
       "        0.00965401, 0.00909065, 0.0076    , 0.01019804, 0.010245  ,\n",
       "        0.0087178 , 0.00926499, 0.00863481, 0.00770973, 0.00980612,\n",
       "        0.0074458 , 0.00941488, 0.00961249, 0.00808455, 0.0096    ,\n",
       "        0.01088118, 0.00829458, 0.00863944, 0.01018627, 0.01008762,\n",
       "        0.00826075, 0.01108152, 0.00678233, 0.00898888, 0.0053066 ,\n",
       "        0.0119733 , 0.01261745, 0.0072111 , 0.00649923, 0.01172348,\n",
       "        0.0121326 , 0.00630555, 0.00431741, 0.0082801 , 0.00786384,\n",
       "        0.0124964 , 0.00762627, 0.00969536, 0.00470744, 0.01259524,\n",
       "        0.0074458 , 0.00744043, 0.00757892, 0.01107068, 0.00868562,\n",
       "        0.00685274, 0.00701427, 0.01034215, 0.00257682, 0.00793473,\n",
       "        0.0124964 , 0.01133843, 0.01175415, 0.00928655, 0.01154816,\n",
       "        0.00641872, 0.00684105, 0.00757364, 0.00793473, 0.00861626,\n",
       "        0.00823165, 0.01022546, 0.00574804, 0.01070701, 0.01077775,\n",
       "        0.0119432 , 0.01301384, 0.00844748]),\n",
       " 'rank_test_precision_micro': array([176, 155, 183, 198, 199, 213, 237, 242, 222, 252, 244, 230, 168,\n",
       "        186, 184, 191, 189, 200, 224, 220, 210, 245, 255, 223, 170, 171,\n",
       "        173, 208, 185, 202, 225, 221, 205, 252, 226, 235, 164, 178, 187,\n",
       "        181, 194, 191, 206, 214, 218, 226, 216, 219, 159, 157, 147, 197,\n",
       "        212, 193, 232, 233, 230, 240, 254, 248, 161, 174, 177, 195, 182,\n",
       "        209, 239, 228, 250, 249, 246, 256, 169, 172, 179, 196, 211, 201,\n",
       "        203, 241, 234, 251, 237, 243, 188, 159, 175, 180, 207, 190, 204,\n",
       "        216, 214, 247, 236, 229,  56, 277,  23,  76, 278, 119,   8, 287,\n",
       "          4,  17, 272,  17,  15, 285,  97,  72, 275,  56,  75, 274,  97,\n",
       "          6, 276,  39, 103, 286,   9,  91, 270,  23,   3, 283,  86,  92,\n",
       "        279,  76, 134, 284,  71,   7, 288, 120,  50, 273,  54,  86, 281,\n",
       "         67, 140, 264,  67, 132, 282, 140, 138, 267, 123, 143, 258, 107,\n",
       "         39, 265, 113,  44, 262, 136, 130, 257, 153, 120, 259,  99, 140,\n",
       "        280, 101,  56, 261, 109,   1, 260,  65,  72, 271, 108, 127, 269,\n",
       "        111, 115, 268, 127,  23, 263, 117, 152, 266, 103,   4,  86,  44,\n",
       "         50,  17,   9,  12,  64,  81,  56,  39,  32,  36,  44,  23,  12,\n",
       "         50,  81,  56,  65,  47,  47,  54,  12,  50,  86,  15,  72,  56,\n",
       "         39,  21,  23,  81,  56,  39,  47,  76,  81,  17,  95,  23,   2,\n",
       "         36,  23,  23,  76,  67,  32, 118, 115,  23,  56, 156, 103, 158,\n",
       "          9, 134, 161, 130, 110,  92,  32,  92, 150, 123, 113, 103, 161,\n",
       "        137, 151, 138, 149, 120,  32, 100, 132,  95, 111, 144, 144, 167,\n",
       "        166, 164, 127,  86,  81,  67,  38, 101,  22, 153,  76, 123, 147,\n",
       "        144, 123]),\n",
       " 'split0_test_roc_auc_ovo': array([0.84751756, 0.84712155, 0.8498336 , 0.84132546, 0.84240948,\n",
       "        0.82286917, 0.81984912, 0.82558121, 0.81647706, 0.81335701,\n",
       "        0.80693291, 0.81345302, 0.8501376 , 0.84869758, 0.85276564,\n",
       "        0.84182547, 0.83282933, 0.8373934 , 0.81788109, 0.83351334,\n",
       "        0.8249932 , 0.81138498, 0.79823277, 0.82063313, 0.84315749,\n",
       "        0.84920159, 0.85056961, 0.8313133 , 0.83587337, 0.83406535,\n",
       "        0.83421335, 0.82197315, 0.82901726, 0.80885294, 0.80850494,\n",
       "        0.81142498, 0.85358566, 0.84445351, 0.8434975 , 0.84858158,\n",
       "        0.83929343, 0.8435735 , 0.8377454 , 0.8374534 , 0.83403734,\n",
       "        0.81674507, 0.82764124, 0.83160531, 0.84810957, 0.85464567,\n",
       "        0.84690955, 0.82318917, 0.83381934, 0.83433735, 0.8247292 ,\n",
       "        0.81716107, 0.81174899, 0.80322285, 0.80405686, 0.81192499,\n",
       "        0.85058161, 0.84530152, 0.84886958, 0.84083345, 0.84250548,\n",
       "        0.82462319, 0.82152314, 0.81614506, 0.80391286, 0.80754492,\n",
       "        0.80726492, 0.8002968 , 0.84922959, 0.84884958, 0.84468151,\n",
       "        0.83272132, 0.82957327, 0.83913343, 0.83315333, 0.81469304,\n",
       "        0.82136914, 0.81526104, 0.79702875, 0.81314901, 0.84483752,\n",
       "        0.85194963, 0.84860158, 0.83936143, 0.83604138, 0.84492552,\n",
       "        0.82194915, 0.81963711, 0.82727324, 0.8190171 , 0.82832525,\n",
       "        0.82152514, 0.85867774, 0.4315109 , 0.85914975, 0.85952975,\n",
       "        0.54644074, 0.86040577, 0.85984176, 0.44820717, 0.86076977,\n",
       "        0.85958575, 0.66334261, 0.85820573, 0.85952175, 0.75996016,\n",
       "        0.85877374, 0.85980176, 0.80682891, 0.86024576, 0.85976176,\n",
       "        0.2810525 , 0.85964975, 0.85947775, 0.41565465, 0.86026576,\n",
       "        0.86065377, 0.58292133, 0.85968976, 0.85966575, 0.63905022,\n",
       "        0.86141378, 0.86088577, 0.53558057, 0.86095378, 0.85919775,\n",
       "        0.25840413, 0.86106178, 0.85980176, 0.5434727 , 0.86046977,\n",
       "        0.86044177, 0.79268468, 0.85848574, 0.86085377, 0.48519576,\n",
       "        0.86011776, 0.85979376, 0.37760204, 0.85812173, 0.85841773,\n",
       "        0.76199219, 0.85722572, 0.85738972, 0.32873326, 0.85849374,\n",
       "        0.85992976, 0.77013632, 0.85960975, 0.86043777, 0.73399974,\n",
       "        0.86085777, 0.86081777, 0.82653322, 0.8627858 , 0.8564457 ,\n",
       "        0.78188451, 0.8563737 , 0.85866574, 0.78949263, 0.8562537 ,\n",
       "        0.85816173, 0.80784093, 0.85924975, 0.85948575, 0.73168771,\n",
       "        0.85742572, 0.85782973, 0.76708427, 0.85907775, 0.86127778,\n",
       "        0.8184611 , 0.86032977, 0.86132978, 0.63401814, 0.86146578,\n",
       "        0.85877374, 0.59708555, 0.85453767, 0.85769372, 0.64674635,\n",
       "        0.85810173, 0.85878174, 0.5125002 , 0.85761772, 0.85675371,\n",
       "        0.59932559, 0.86026976, 0.86083777, 0.85975376, 0.86042977,\n",
       "        0.86106578, 0.86106978, 0.86158579, 0.86091777, 0.86054177,\n",
       "        0.86082577, 0.86080977, 0.86038577, 0.86052577, 0.86003376,\n",
       "        0.86050177, 0.86015776, 0.86063777, 0.86032177, 0.86070977,\n",
       "        0.86097378, 0.86077377, 0.86099778, 0.86085377, 0.86091377,\n",
       "        0.86068577, 0.85939775, 0.86043377, 0.86058177, 0.86116978,\n",
       "        0.86105778, 0.86064977, 0.86066577, 0.86109378, 0.86118578,\n",
       "        0.86068577, 0.86110978, 0.86072577, 0.86078177, 0.86080177,\n",
       "        0.86106578, 0.86072577, 0.86064177, 0.86087777, 0.86050577,\n",
       "        0.86054177, 0.86054177, 0.86078577, 0.86073777, 0.86041777,\n",
       "        0.86119378, 0.85774572, 0.85911775, 0.85342565, 0.85433367,\n",
       "        0.8564817 , 0.85395766, 0.85783373, 0.85472568, 0.85166163,\n",
       "        0.85402566, 0.85085761, 0.86014576, 0.85768172, 0.85840573,\n",
       "        0.85568169, 0.85413767, 0.8564217 , 0.85380166, 0.85769772,\n",
       "        0.85707371, 0.8560417 , 0.85658571, 0.8565457 , 0.8561937 ,\n",
       "        0.85814973, 0.85696571, 0.85388166, 0.85750572, 0.85330565,\n",
       "        0.85552569, 0.8563017 , 0.85414967, 0.8561417 , 0.85357366,\n",
       "        0.85570169, 0.85713371, 0.85901374, 0.85698571, 0.85581369,\n",
       "        0.85831773, 0.85680571, 0.85699371, 0.85794973, 0.85790173,\n",
       "        0.85934575, 0.85771372, 0.85421367]),\n",
       " 'split1_test_roc_auc_ovo': array([0.85882574, 0.85456967, 0.85432567, 0.84178547, 0.84032945,\n",
       "        0.84938159, 0.82363718, 0.82074913, 0.81184899, 0.81085097,\n",
       "        0.83881342, 0.81560105, 0.85819773, 0.85237764, 0.85370566,\n",
       "        0.84772556, 0.84618554, 0.84030944, 0.82425719, 0.83327333,\n",
       "        0.83186931, 0.83337333, 0.80453687, 0.82185315, 0.85081361,\n",
       "        0.86045777, 0.86116978, 0.83661339, 0.84308949, 0.83859342,\n",
       "        0.83700139, 0.81746908, 0.83844142, 0.83244132, 0.8377814 ,\n",
       "        0.83282133, 0.85592569, 0.85729372, 0.8563577 , 0.84686555,\n",
       "        0.85243764, 0.84512552, 0.8438095 , 0.84026944, 0.83046529,\n",
       "        0.83578937, 0.83507336, 0.82953727, 0.85873374, 0.85257764,\n",
       "        0.85120162, 0.83656539, 0.83658139, 0.84406951, 0.84688155,\n",
       "        0.84826157, 0.82272916, 0.8310453 , 0.80853294, 0.82553921,\n",
       "        0.84956559, 0.86139378, 0.85096562, 0.84049345, 0.83401734,\n",
       "        0.84048545, 0.80371686, 0.84456151, 0.82228916, 0.81177299,\n",
       "        0.82038113, 0.78801661, 0.85920975, 0.85681371, 0.85672171,\n",
       "        0.84063345, 0.8189691 , 0.8439255 , 0.82424119, 0.83453335,\n",
       "        0.82650522, 0.81604106, 0.81807309, 0.83978144, 0.85532569,\n",
       "        0.85880174, 0.85764972, 0.8502896 , 0.83610138, 0.84613354,\n",
       "        0.83238932, 0.83085729, 0.83326133, 0.81700907, 0.82774524,\n",
       "        0.82248116, 0.86416583, 0.65211843, 0.86516184, 0.86463383,\n",
       "        0.58808141, 0.86598586, 0.86720588, 0.54434071, 0.86520584,\n",
       "        0.86643386, 0.24513992, 0.86340581, 0.86484584, 0.76979232,\n",
       "        0.86436183, 0.86352582, 0.50231604, 0.86477384, 0.86315381,\n",
       "        0.57026912, 0.86330581, 0.86475784, 0.26555225, 0.86484184,\n",
       "        0.86364982, 0.70964335, 0.86350982, 0.86198579, 0.63317813,\n",
       "        0.86609386, 0.86408983, 0.71811549, 0.86424583, 0.86496584,\n",
       "        0.55364886, 0.8624378 , 0.8622058 , 0.47982368, 0.86459783,\n",
       "        0.86402982, 0.35743372, 0.86322981, 0.86526184, 0.46737948,\n",
       "        0.8622458 , 0.86370982, 0.44879518, 0.86576585, 0.85955775,\n",
       "        0.34517752, 0.86292981, 0.86399382, 0.30484088, 0.86421783,\n",
       "        0.86574985, 0.46969552, 0.86423383, 0.8622938 , 0.54699675,\n",
       "        0.85802173, 0.86404982, 0.45101122, 0.86197379, 0.86195779,\n",
       "        0.60308965, 0.86084577, 0.85903374, 0.77566441, 0.8627978 ,\n",
       "        0.86401782, 0.49311189, 0.86176579, 0.85838973, 0.6064537 ,\n",
       "        0.86154978, 0.86410983, 0.60982976, 0.86402582, 0.86667787,\n",
       "        0.43014288, 0.86558185, 0.86083377, 0.41395862, 0.86104578,\n",
       "        0.86092577, 0.50519608, 0.86414583, 0.86081777, 0.43323093,\n",
       "        0.86179779, 0.8623418 , 0.6060337 , 0.86340181, 0.86059377,\n",
       "        0.67765484, 0.86496584, 0.86636186, 0.86501784, 0.86568585,\n",
       "        0.86638186, 0.86654586, 0.86585385, 0.86666587, 0.86640186,\n",
       "        0.86605386, 0.86608986, 0.86620586, 0.86648186, 0.86561785,\n",
       "        0.86491784, 0.86661387, 0.86652586, 0.86615386, 0.86492584,\n",
       "        0.86615786, 0.86629386, 0.86637386, 0.86636986, 0.86665787,\n",
       "        0.86602186, 0.86627786, 0.86473784, 0.86492984, 0.86658187,\n",
       "        0.86612586, 0.86644186, 0.86610586, 0.86648186, 0.86602586,\n",
       "        0.86662587, 0.86661387, 0.86658987, 0.86541785, 0.86611786,\n",
       "        0.86614186, 0.86631386, 0.86640586, 0.86623386, 0.86622986,\n",
       "        0.86626986, 0.86669787, 0.86654986, 0.86645386, 0.86647786,\n",
       "        0.86514584, 0.8624938 , 0.86595386, 0.85858974, 0.85857374,\n",
       "        0.85850174, 0.86289381, 0.86143778, 0.85902574, 0.85866974,\n",
       "        0.86036977, 0.8561257 , 0.8626698 , 0.86118578, 0.86453383,\n",
       "        0.86080577, 0.86374182, 0.86072977, 0.86084577, 0.85965775,\n",
       "        0.86292981, 0.8563257 , 0.85734972, 0.86098578, 0.86402182,\n",
       "        0.86485784, 0.85952575, 0.86288181, 0.86627786, 0.86117778,\n",
       "        0.8622818 , 0.8622018 , 0.86029776, 0.85696571, 0.85521768,\n",
       "        0.85889774, 0.8622858 , 0.86339381, 0.86755388, 0.86356182,\n",
       "        0.86390582, 0.86356182, 0.86194979, 0.86465783, 0.86061377,\n",
       "        0.86024976, 0.86428183, 0.8625498 ]),\n",
       " 'split2_test_roc_auc_ovo': array([0.85868691, 0.86257905, 0.86499514, 0.84382238, 0.85569481,\n",
       "        0.85344272, 0.82272162, 0.8305459 , 0.84105828, 0.82189759,\n",
       "        0.8248977 , 0.82528971, 0.86407911, 0.86199503, 0.86412311,\n",
       "        0.8417863 , 0.8500106 , 0.86351509, 0.83815417, 0.82868583,\n",
       "        0.84181431, 0.82028553, 0.80612102, 0.84124628, 0.86025497,\n",
       "        0.86403111, 0.86311907, 0.84655048, 0.85180667, 0.84615846,\n",
       "        0.83102192, 0.83677812, 0.84279034, 0.83127393, 0.83841418,\n",
       "        0.82901384, 0.86183103, 0.86267506, 0.86153102, 0.85301471,\n",
       "        0.85752287, 0.8498626 , 0.8359861 , 0.8444824 , 0.83905421,\n",
       "        0.83397802, 0.82698977, 0.83830618, 0.85991096, 0.86465113,\n",
       "        0.86753523, 0.84182631, 0.84339836, 0.84584845, 0.81294327,\n",
       "        0.81968351, 0.81348129, 0.81346128, 0.8220976 , 0.8137613 ,\n",
       "        0.8554508 , 0.85330672, 0.85740287, 0.83457004, 0.85721086,\n",
       "        0.8444304 , 0.83109792, 0.84488242, 0.82760979, 0.80621702,\n",
       "        0.80522099, 0.81651339, 0.86440712, 0.8584429 , 0.86416711,\n",
       "        0.84122628, 0.85991896, 0.8529107 , 0.84971459, 0.83952222,\n",
       "        0.83280598, 0.81626939, 0.82198959, 0.83290998, 0.8527787 ,\n",
       "        0.861043  , 0.85681885, 0.84826654, 0.8388782 , 0.8528307 ,\n",
       "        0.8473345 , 0.84631047, 0.82996588, 0.82362165, 0.83977023,\n",
       "        0.83105792, 0.87434748, 0.4860215 , 0.87381546, 0.87329544,\n",
       "        0.63365081, 0.87236341, 0.87285142, 0.31970351, 0.87316743,\n",
       "        0.87379146, 0.51493054, 0.87415147, 0.8749195 , 0.22386806,\n",
       "        0.87165938, 0.87460349, 0.71303367, 0.8749155 , 0.8749395 ,\n",
       "        0.49198971, 0.87360745, 0.87412347, 0.86821926, 0.87330744,\n",
       "        0.87093935, 0.20533939, 0.87342744, 0.87399946, 0.81962151,\n",
       "        0.87365145, 0.87339144, 0.42881144, 0.87188739, 0.8750115 ,\n",
       "        0.83140993, 0.87438348, 0.87343144, 0.45853251, 0.87376346,\n",
       "        0.87475549, 0.24976899, 0.87251141, 0.87472749, 0.62203839,\n",
       "        0.87267942, 0.87315943, 0.86272306, 0.87263141, 0.86753123,\n",
       "        0.73236637, 0.87305143, 0.87313543, 0.40180246, 0.86700321,\n",
       "        0.86981131, 0.61127001, 0.87197939, 0.87149937, 0.73643851,\n",
       "        0.87239141, 0.86611918, 0.53236717, 0.87084335, 0.87313943,\n",
       "        0.63722694, 0.86904329, 0.87134737, 0.70358133, 0.87546752,\n",
       "        0.87317543, 0.66360389, 0.87394746, 0.8666392 , 0.49285374,\n",
       "        0.86411511, 0.87057934, 0.5305791 , 0.87137537, 0.87171138,\n",
       "        0.83231396, 0.87122736, 0.87078735, 0.69182091, 0.87125937,\n",
       "        0.86583117, 0.63649491, 0.86510314, 0.87014733, 0.68221656,\n",
       "        0.87185139, 0.87427547, 0.74661488, 0.87391946, 0.87105536,\n",
       "        0.48838958, 0.87409947, 0.87459149, 0.87352745, 0.87391146,\n",
       "        0.87379146, 0.87350745, 0.87431948, 0.87304743, 0.87380746,\n",
       "        0.87340744, 0.87453548, 0.87371945, 0.87373145, 0.87381546,\n",
       "        0.87347545, 0.87388746, 0.87429547, 0.87389546, 0.87401946,\n",
       "        0.87384746, 0.87369145, 0.87356745, 0.87368745, 0.87363545,\n",
       "        0.87329144, 0.87371145, 0.87367945, 0.87390346, 0.87417547,\n",
       "        0.87377146, 0.87420347, 0.87361545, 0.87348745, 0.87350745,\n",
       "        0.87395946, 0.87364745, 0.87367945, 0.87385146, 0.87382346,\n",
       "        0.87354345, 0.87376346, 0.87365945, 0.87396746, 0.87379546,\n",
       "        0.87372745, 0.87381546, 0.87386746, 0.87375946, 0.87379546,\n",
       "        0.87159938, 0.8721874 , 0.87319944, 0.87014733, 0.86833126,\n",
       "        0.8750915 , 0.86300707, 0.87145137, 0.86433112, 0.86552316,\n",
       "        0.86373109, 0.8666632 , 0.87184339, 0.87358745, 0.87262341,\n",
       "        0.87075135, 0.87202739, 0.87133537, 0.86854727, 0.86305907,\n",
       "        0.86541916, 0.86775124, 0.86871927, 0.86651919, 0.87167538,\n",
       "        0.87269142, 0.87189539, 0.86902328, 0.8695353 , 0.87069535,\n",
       "        0.86961931, 0.8693113 , 0.8666472 , 0.86796725, 0.86818325,\n",
       "        0.86737923, 0.87154338, 0.87279142, 0.87400346, 0.87361945,\n",
       "        0.87127137, 0.87442748, 0.8723514 , 0.86816325, 0.87098736,\n",
       "        0.8666712 , 0.86765524, 0.86845126]),\n",
       " 'split3_test_roc_auc_ovo': array([0.84955458, 0.8499546 , 0.85067062, 0.81361729, 0.83257797,\n",
       "        0.82922585, 0.82145357, 0.80744907, 0.81868147, 0.79846474,\n",
       "        0.8026489 , 0.81838946, 0.84631447, 0.85261869, 0.85222668,\n",
       "        0.83154594, 0.82129357, 0.83515407, 0.81833346, 0.82727778,\n",
       "        0.82650575, 0.80019681, 0.81608938, 0.83215396, 0.84757451,\n",
       "        0.84846254, 0.84552644, 0.83688213, 0.83282198, 0.83188595,\n",
       "        0.83287798, 0.83163394, 0.82655376, 0.79543264, 0.79862075,\n",
       "        0.80297691, 0.85254269, 0.85070263, 0.84931458, 0.82960187,\n",
       "        0.84053826, 0.84427039, 0.82832982, 0.82579773, 0.82445368,\n",
       "        0.8194775 , 0.82864983, 0.81140521, 0.85101064, 0.84560244,\n",
       "        0.85223068, 0.82542972, 0.82364565, 0.82141157, 0.81730142,\n",
       "        0.81744143, 0.8249977 , 0.81688141, 0.81792945, 0.81840146,\n",
       "        0.84979059, 0.85087463, 0.84564244, 0.81930149, 0.84500642,\n",
       "        0.82922185, 0.80590101, 0.80797709, 0.805453  , 0.80528899,\n",
       "        0.81335328, 0.79338456, 0.85441876, 0.8472705 , 0.85053862,\n",
       "        0.82186559, 0.83920221, 0.82874983, 0.83004588, 0.81802145,\n",
       "        0.81568936, 0.80719706, 0.81597738, 0.81058118, 0.84827854,\n",
       "        0.84578245, 0.84850655, 0.83006588, 0.83298999, 0.83784616,\n",
       "        0.82666576, 0.83713014, 0.81344928, 0.80524499, 0.80088483,\n",
       "        0.81514935, 0.86630319, 0.66537195, 0.86479113, 0.86508314,\n",
       "        0.611106  , 0.86470713, 0.86689521, 0.42060314, 0.86762723,\n",
       "        0.86619918, 0.75129105, 0.86651919, 0.86464313, 0.51038237,\n",
       "        0.86724322, 0.86735122, 0.19637907, 0.86625919, 0.86587117,\n",
       "        0.7278022 , 0.86746723, 0.86795525, 0.30631503, 0.86729922,\n",
       "        0.86686321, 0.76412751, 0.86553516, 0.86861527, 0.25054102,\n",
       "        0.86724722, 0.86721922, 0.69811713, 0.86741523, 0.86628719,\n",
       "        0.60320972, 0.86783124, 0.86642719, 0.54000744, 0.86702721,\n",
       "        0.8667112 , 0.41254685, 0.8667672 , 0.86467513, 0.76877568,\n",
       "        0.86739923, 0.86694321, 0.68410063, 0.86734722, 0.86206303,\n",
       "        0.77873603, 0.86077899, 0.86263906, 0.75170306, 0.861231  ,\n",
       "        0.86077099, 0.59323336, 0.86330308, 0.85995496, 0.80340492,\n",
       "        0.861147  , 0.86371109, 0.6860727 , 0.85799089, 0.86095899,\n",
       "        0.74567884, 0.86514715, 0.86256305, 0.75194307, 0.86222704,\n",
       "        0.86331908, 0.81221324, 0.86189103, 0.86278306, 0.73641851,\n",
       "        0.86487514, 0.86409111, 0.80576901, 0.86244305, 0.86359109,\n",
       "        0.69484901, 0.8637791 , 0.86347909, 0.66622798, 0.86435112,\n",
       "        0.85900692, 0.44241593, 0.85976695, 0.86227104, 0.77323584,\n",
       "        0.86047498, 0.86449912, 0.75617122, 0.85942294, 0.85853891,\n",
       "        0.78974043, 0.86433112, 0.86178302, 0.86450712, 0.86422711,\n",
       "        0.8639071 , 0.86311907, 0.86372709, 0.86460313, 0.86363109,\n",
       "        0.86434312, 0.86409511, 0.86405111, 0.86365909, 0.86340708,\n",
       "        0.86265906, 0.86359509, 0.86424311, 0.86323508, 0.86317907,\n",
       "        0.8638031 , 0.8639511 , 0.86338708, 0.86323908, 0.86431512,\n",
       "        0.86337108, 0.8639791 , 0.86295107, 0.86255905, 0.86410711,\n",
       "        0.86451112, 0.86340708, 0.86405511, 0.8639911 , 0.86441512,\n",
       "        0.86446712, 0.86437512, 0.86482713, 0.86415911, 0.86371109,\n",
       "        0.86453512, 0.86411111, 0.86449912, 0.86443112, 0.86414311,\n",
       "        0.86434712, 0.86440712, 0.86441512, 0.86415911, 0.86467913,\n",
       "        0.85702685, 0.85623082, 0.85964295, 0.85953894, 0.85527479,\n",
       "        0.85602682, 0.8527907 , 0.85720286, 0.85494278, 0.85354273,\n",
       "        0.8556108 , 0.85608282, 0.85662284, 0.86078299, 0.85859091,\n",
       "        0.84964259, 0.85533879, 0.85637483, 0.85433476, 0.85313071,\n",
       "        0.8500346 , 0.85444676, 0.85307871, 0.8501346 , 0.85436676,\n",
       "        0.86274706, 0.85931894, 0.85709486, 0.85718686, 0.85725486,\n",
       "        0.85334272, 0.85622682, 0.85355873, 0.85093063, 0.84773852,\n",
       "        0.85379074, 0.85977495, 0.86043898, 0.8583149 , 0.85955094,\n",
       "        0.85744287, 0.85670684, 0.85796689, 0.85576281, 0.85685485,\n",
       "        0.85485477, 0.85142665, 0.85511078]),\n",
       " 'split4_test_roc_auc_ovo': array([0.86493514, 0.86859927, 0.86081499, 0.84985059, 0.85617482,\n",
       "        0.85252669, 0.83514207, 0.83018589, 0.85056262, 0.82337364,\n",
       "        0.82845382, 0.83433404, 0.87182339, 0.86171502, 0.86207503,\n",
       "        0.85221868, 0.85115864, 0.84759451, 0.83219796, 0.84463041,\n",
       "        0.8388182 , 0.83707013, 0.84410239, 0.82758179, 0.8693473 ,\n",
       "        0.86289906, 0.86922329, 0.8471025 , 0.83812217, 0.85177066,\n",
       "        0.83075391, 0.84047826, 0.85228268, 0.82887784, 0.84357837,\n",
       "        0.83488206, 0.86289506, 0.86595917, 0.86250305, 0.86043898,\n",
       "        0.85953494, 0.85338272, 0.8584309 , 0.85527079, 0.8473065 ,\n",
       "        0.83076191, 0.83815017, 0.83649811, 0.86890728, 0.86428711,\n",
       "        0.87199939, 0.85453476, 0.84329436, 0.85094263, 0.83653812,\n",
       "        0.83737815, 0.84123028, 0.82146157, 0.80468497, 0.83226996,\n",
       "        0.86791925, 0.86638319, 0.86731522, 0.84331436, 0.85307471,\n",
       "        0.84625047, 0.84511842, 0.84919457, 0.83739815, 0.81971351,\n",
       "        0.82642175, 0.80858511, 0.860999  , 0.85981095, 0.86367909,\n",
       "        0.85106664, 0.85065062, 0.85432676, 0.83256197, 0.84908257,\n",
       "        0.82612174, 0.82380566, 0.83671012, 0.82182559, 0.87008732,\n",
       "        0.86159902, 0.85991896, 0.85798289, 0.85364673, 0.84366237,\n",
       "        0.84891056, 0.84773052, 0.84822254, 0.83192595, 0.83707013,\n",
       "        0.83130193, 0.87573953, 0.30779108, 0.87539151, 0.87597554,\n",
       "        0.34816053, 0.87528751, 0.87542752, 0.43829578, 0.8776516 ,\n",
       "        0.87514751, 0.71598178, 0.87742759, 0.87662756, 0.57132057,\n",
       "        0.87738359, 0.87642755, 0.583465  , 0.87687557, 0.87647955,\n",
       "        0.32578773, 0.87874363, 0.87676756, 0.68124052, 0.87600754,\n",
       "        0.87700357, 0.44659208, 0.87666756, 0.87736359, 0.34904057,\n",
       "        0.87725558, 0.87730358, 0.70781348, 0.87592753, 0.87714358,\n",
       "        0.34930057, 0.87557552, 0.87559952, 0.49300575, 0.87696757,\n",
       "        0.87719158, 0.79547264, 0.87641555, 0.87423547, 0.7750519 ,\n",
       "        0.87672356, 0.87585553, 0.46320068, 0.8779156 , 0.8749955 ,\n",
       "        0.74049066, 0.87680356, 0.87363945, 0.72344204, 0.87303543,\n",
       "        0.87738759, 0.59785352, 0.87674356, 0.87861563, 0.81534535,\n",
       "        0.87597153, 0.87479549, 0.77479989, 0.8720954 , 0.8749235 ,\n",
       "        0.77380786, 0.87856763, 0.87809161, 0.81775344, 0.87282742,\n",
       "        0.8776716 , 0.79759271, 0.87699157, 0.87044734, 0.27329784,\n",
       "        0.87327544, 0.87525151, 0.67088815, 0.87805161, 0.87661956,\n",
       "        0.73535047, 0.87302743, 0.87668756, 0.29030645, 0.87554352,\n",
       "        0.87376746, 0.70740947, 0.87532351, 0.87611954, 0.66079979,\n",
       "        0.87193539, 0.87716758, 0.69341696, 0.8777356 , 0.87293143,\n",
       "        0.58188895, 0.87281542, 0.87672356, 0.87573553, 0.87715158,\n",
       "        0.87746359, 0.87717158, 0.87746359, 0.87759559, 0.87745159,\n",
       "        0.8777116 , 0.87748359, 0.87757959, 0.87711958, 0.87662756,\n",
       "        0.87800761, 0.87762359, 0.87760359, 0.8778236 , 0.87747559,\n",
       "        0.87750359, 0.87803161, 0.87735558, 0.87727158, 0.87792761,\n",
       "        0.87742759, 0.87668756, 0.87715158, 0.87697557, 0.8776436 ,\n",
       "        0.8777276 , 0.87729558, 0.87752759, 0.87831562, 0.8777796 ,\n",
       "        0.87750359, 0.87734358, 0.87746759, 0.8779036 , 0.87697957,\n",
       "        0.87759559, 0.87747559, 0.87753159, 0.8777396 , 0.87741959,\n",
       "        0.87798761, 0.87732358, 0.8776676 , 0.87746359, 0.8776956 ,\n",
       "        0.87607554, 0.87544752, 0.87637955, 0.87626755, 0.87266342,\n",
       "        0.87801561, 0.87267942, 0.87291943, 0.87426347, 0.87386346,\n",
       "        0.87753959, 0.86966731, 0.87467949, 0.87653156, 0.87521151,\n",
       "        0.87384346, 0.8693713 , 0.87389546, 0.86961931, 0.87248741,\n",
       "        0.87398346, 0.86984731, 0.86977931, 0.87461149, 0.87533551,\n",
       "        0.87880764, 0.87369945, 0.87628355, 0.86985531, 0.87095535,\n",
       "        0.87529551, 0.87325944, 0.87392746, 0.86897128, 0.87251541,\n",
       "        0.87192339, 0.8749715 , 0.87660356, 0.87587953, 0.87613154,\n",
       "        0.87612354, 0.87809961, 0.87187139, 0.87742759, 0.87549552,\n",
       "        0.87326344, 0.87517151, 0.87429547]),\n",
       " 'mean_test_roc_auc_ovo': array([0.85590399, 0.85656483, 0.856128  , 0.83808024, 0.8454373 ,\n",
       "        0.8414892 , 0.82456071, 0.82290224, 0.82772568, 0.81358879,\n",
       "        0.82034935, 0.82141345, 0.85811046, 0.85548079, 0.85697923,\n",
       "        0.84302039, 0.84029553, 0.8447933 , 0.82616477, 0.83347614,\n",
       "        0.83280015, 0.82046216, 0.81381649, 0.82869366, 0.85422958,\n",
       "        0.85701041, 0.85792164, 0.83969236, 0.84034274, 0.84049477,\n",
       "        0.83317371, 0.82966651, 0.83781709, 0.81937573, 0.82537993,\n",
       "        0.82222382, 0.85735603, 0.85621682, 0.85464077, 0.84770054,\n",
       "        0.84986543, 0.84724295, 0.84086034, 0.84065475, 0.8350634 ,\n",
       "        0.82735037, 0.83130088, 0.82947042, 0.85733444, 0.8563528 ,\n",
       "        0.85797529, 0.83630907, 0.83614782, 0.8393219 , 0.82767871,\n",
       "        0.82798515, 0.82283748, 0.81721448, 0.81146036, 0.82037938,\n",
       "        0.85466157, 0.85545197, 0.85403915, 0.83570256, 0.84636296,\n",
       "        0.83700227, 0.82147147, 0.83255213, 0.81933259, 0.81010749,\n",
       "        0.81452841, 0.8013593 , 0.85765284, 0.85423753, 0.85595761,\n",
       "        0.83750266, 0.83966283, 0.84380924, 0.83394339, 0.83117053,\n",
       "        0.82449829, 0.81571484, 0.81795579, 0.82364944, 0.85426155,\n",
       "        0.85583517, 0.85429913, 0.84519327, 0.83953153, 0.84507966,\n",
       "        0.83544986, 0.83633311, 0.83043445, 0.81936375, 0.82675914,\n",
       "        0.8243031 , 0.86784675, 0.50856277, 0.86766194, 0.86770354,\n",
       "        0.5454879 , 0.86774993, 0.86844436, 0.43423006, 0.86888438,\n",
       "        0.86823155, 0.57813718, 0.86794196, 0.86811155, 0.5670647 ,\n",
       "        0.86788435, 0.86834197, 0.56040454, 0.86861397, 0.86804116,\n",
       "        0.47938025, 0.86855478, 0.86861637, 0.50739634, 0.86834436,\n",
       "        0.86782194, 0.54172473, 0.86776595, 0.86832597, 0.53828629,\n",
       "        0.86913238, 0.86857797, 0.61768762, 0.86808595, 0.86852117,\n",
       "        0.51919464, 0.86825796, 0.86749314, 0.50296841, 0.86856517,\n",
       "        0.86862597, 0.52158138, 0.86748194, 0.86795074, 0.62368824,\n",
       "        0.86783315, 0.86789235, 0.56728432, 0.86835637, 0.86451305,\n",
       "        0.67175255, 0.8661579 , 0.8661595 , 0.50210434, 0.86479624,\n",
       "        0.8667299 , 0.60843774, 0.86717392, 0.86656031, 0.72723706,\n",
       "        0.86567789, 0.86589867, 0.65415684, 0.86513785, 0.86548508,\n",
       "        0.70833756, 0.86599551, 0.8659403 , 0.76768698, 0.8659147 ,\n",
       "        0.86726913, 0.71487253, 0.86676912, 0.86354902, 0.5681423 ,\n",
       "        0.86424824, 0.8663723 , 0.67683006, 0.86699472, 0.86797554,\n",
       "        0.70222349, 0.8667891 , 0.86662351, 0.53926642, 0.86673311,\n",
       "        0.86366101, 0.57772039, 0.86377542, 0.86540988, 0.63924589,\n",
       "        0.86483225, 0.86741314, 0.66294739, 0.86641951, 0.86397463,\n",
       "        0.62739988, 0.86729632, 0.86805954, 0.86770834, 0.86828115,\n",
       "        0.86852196, 0.86828275, 0.86858996, 0.86856596, 0.86836675,\n",
       "        0.86846836, 0.86860276, 0.86838836, 0.86830355, 0.86790034,\n",
       "        0.86791234, 0.86837555, 0.86866116, 0.86828595, 0.86806195,\n",
       "        0.86845716, 0.86854836, 0.86833635, 0.86828435, 0.86868996,\n",
       "        0.86815955, 0.86801075, 0.86779074, 0.86778994, 0.86873556,\n",
       "        0.86863876, 0.86839955, 0.86839396, 0.86867396, 0.86858276,\n",
       "        0.86864836, 0.86861796, 0.86865796, 0.86842276, 0.86828675,\n",
       "        0.86857636, 0.86847796, 0.86854756, 0.86864996, 0.86841876,\n",
       "        0.86857476, 0.86855716, 0.86865716, 0.86851476, 0.86861316,\n",
       "        0.86620828, 0.86482105, 0.86685871, 0.86359384, 0.86183537,\n",
       "        0.86482347, 0.86106573, 0.86416903, 0.86145776, 0.86065214,\n",
       "        0.86225538, 0.85987933, 0.86519226, 0.8659539 , 0.86587308,\n",
       "        0.86214497, 0.86292339, 0.86375143, 0.86142975, 0.86120653,\n",
       "        0.86188815, 0.86088254, 0.86110254, 0.86175935, 0.86431863,\n",
       "        0.86745074, 0.86428105, 0.86383303, 0.86407221, 0.8626778 ,\n",
       "        0.863213  , 0.86346021, 0.86171616, 0.86019531, 0.8594457 ,\n",
       "        0.86153856, 0.86514187, 0.8664483 , 0.8665475 , 0.86573549,\n",
       "        0.86541227, 0.86592029, 0.86422664, 0.86479224, 0.86437064,\n",
       "        0.86287699, 0.86324979, 0.8629242 ]),\n",
       " 'std_test_roc_auc_ovo': array([0.0064574 , 0.00797308, 0.00588634, 0.01260325, 0.00917748,\n",
       "        0.01283821, 0.00544157, 0.00850988, 0.01522946, 0.00895618,\n",
       "        0.01356907, 0.00759294, 0.00923459, 0.00538765, 0.00506082,\n",
       "        0.00694782, 0.01152303, 0.01025798, 0.00792127, 0.00609634,\n",
       "        0.00661674, 0.01367943, 0.01619003, 0.00751722, 0.00941701,\n",
       "        0.00678077, 0.00863695, 0.00615678, 0.00663979, 0.0074604 ,\n",
       "        0.00229369, 0.00870982, 0.00936671, 0.01474156, 0.0181972 ,\n",
       "        0.0126941 , 0.00424585, 0.00783025, 0.00727708, 0.01019356,\n",
       "        0.00845595, 0.0037774 , 0.01007866, 0.00958549, 0.00775347,\n",
       "        0.0077619 , 0.00447551, 0.00957469, 0.00731595, 0.00727395,\n",
       "        0.00989345, 0.01142955, 0.00728776, 0.01044799, 0.01249607,\n",
       "        0.01262228, 0.01052109, 0.00915743, 0.00727031, 0.00757889,\n",
       "        0.0069675 , 0.00752838, 0.0076713 , 0.00868902, 0.00814689,\n",
       "        0.00856289, 0.01555323, 0.01700798, 0.01291533, 0.00529093,\n",
       "        0.00795579, 0.01024316, 0.0053038 , 0.00515663, 0.00753813,\n",
       "        0.00974708, 0.01456904, 0.00940337, 0.00849219, 0.01301005,\n",
       "        0.00571086, 0.00526562, 0.01272686, 0.01122821, 0.00869937,\n",
       "        0.00608767, 0.00479946, 0.00961083, 0.00729938, 0.00480719,\n",
       "        0.01087411, 0.01038099, 0.01116348, 0.0087286 , 0.01377324,\n",
       "        0.0061553 , 0.00639622, 0.135611  , 0.00607555, 0.00604641,\n",
       "        0.10279264, 0.00537413, 0.00540602, 0.07160764, 0.00593578,\n",
       "        0.00567195, 0.18500033, 0.00701328, 0.00656277, 0.19966075,\n",
       "        0.00632943, 0.0063513 , 0.20997201, 0.00629733, 0.00657171,\n",
       "        0.16313051, 0.00689023, 0.00625476, 0.23141036, 0.00569175,\n",
       "        0.0057224 , 0.20077955, 0.00631834, 0.00676975, 0.2082876 ,\n",
       "        0.0056351 , 0.00600775, 0.11583878, 0.00532927, 0.00664872,\n",
       "        0.20129565, 0.00594828, 0.00615185, 0.03353198, 0.00602102,\n",
       "        0.0063673 , 0.22857313, 0.00639731, 0.00554525, 0.13234549,\n",
       "        0.00622029, 0.00591838, 0.17987915, 0.00666531, 0.00611067,\n",
       "        0.16409684, 0.00748316, 0.00630248, 0.19510037, 0.00501043,\n",
       "        0.00641549, 0.09565256, 0.00625324, 0.00733545, 0.09610576,\n",
       "        0.00711913, 0.0047586 , 0.14239934, 0.00543291, 0.00724307,\n",
       "        0.07378852, 0.00757847, 0.00760153, 0.03847904, 0.0071511 ,\n",
       "        0.00710231, 0.12390902, 0.00723045, 0.00449113, 0.17270401,\n",
       "        0.00521017, 0.0059973 , 0.10068244, 0.00683601, 0.00555775,\n",
       "        0.14533709, 0.00470862, 0.0061653 , 0.15877853, 0.00572494,\n",
       "        0.00565468, 0.09403806, 0.00687986, 0.00674713, 0.11203819,\n",
       "        0.00588586, 0.00708451, 0.09218769, 0.00799791, 0.00668551,\n",
       "        0.1010577 , 0.00529791, 0.00651402, 0.00598431, 0.0062482 ,\n",
       "        0.00615129, 0.00613215, 0.00619291, 0.00599044, 0.00631907,\n",
       "        0.00618205, 0.00634713, 0.00633271, 0.00620593, 0.00629976,\n",
       "        0.00669841, 0.00647141, 0.00632718, 0.00657143, 0.00650908,\n",
       "        0.00622642, 0.00637252, 0.0061792 , 0.00623374, 0.00622141,\n",
       "        0.00625443, 0.00634391, 0.00646792, 0.00646917, 0.00620077,\n",
       "        0.00616076, 0.00635045, 0.00623656, 0.00632969, 0.00612302,\n",
       "        0.0061903 , 0.00599705, 0.00607778, 0.00640234, 0.00613002,\n",
       "        0.00607759, 0.00620875, 0.00616964, 0.00624404, 0.00625478,\n",
       "        0.00636972, 0.0061551 , 0.00620853, 0.00618529, 0.00626973,\n",
       "        0.00688476, 0.00770004, 0.00698083, 0.00834801, 0.00734027,\n",
       "        0.00965811, 0.00722481, 0.00671901, 0.00729278, 0.00816785,\n",
       "        0.00838116, 0.00709535, 0.00692113, 0.00759026, 0.0069765 ,\n",
       "        0.00906423, 0.00720893, 0.00745234, 0.00673195, 0.00649029,\n",
       "        0.00804407, 0.00652938, 0.00681423, 0.00837554, 0.00825392,\n",
       "        0.00737293, 0.00703469, 0.00809063, 0.0056335 , 0.00710344,\n",
       "        0.00829954, 0.00686163, 0.00773613, 0.00707272, 0.00934427,\n",
       "        0.0069718 , 0.0069093 , 0.00698683, 0.00778325, 0.00789494,\n",
       "        0.00728249, 0.00887957, 0.00665038, 0.00773802, 0.00748321,\n",
       "        0.00641876, 0.00816131, 0.00770607]),\n",
       " 'rank_test_roc_auc_ovo': array([174, 169, 172, 205, 189, 195, 233, 237, 227, 253, 244, 241, 161,\n",
       "        176, 168, 194, 200, 192, 231, 216, 218, 242, 252, 225, 183, 167,\n",
       "        163, 201, 199, 198, 217, 223, 206, 245, 232, 239, 165, 171, 179,\n",
       "        186, 185, 187, 196, 197, 214, 229, 220, 224, 166, 170, 162, 210,\n",
       "        211, 204, 228, 226, 238, 249, 254, 243, 178, 177, 184, 212, 188,\n",
       "        208, 240, 219, 247, 255, 251, 256, 164, 182, 173, 207, 202, 193,\n",
       "        215, 221, 234, 250, 248, 236, 181, 175, 180, 190, 203, 191, 213,\n",
       "        209, 222, 246, 230, 235,  70, 283,  79,  78, 277,  76,  35, 288,\n",
       "          2,  55, 271,  65,  57, 275,  69,  45, 276,  15,  61, 287,  26,\n",
       "         14, 284,  44,  72, 278,  75,  47, 280,   1,  20, 269,  58,  30,\n",
       "        282,  54,  80, 285,  24,  12, 281,  81,  64, 268,  71,  68, 274,\n",
       "         43, 122, 263, 101, 100, 286, 120,  92, 270,  86,  94, 258, 110,\n",
       "        107, 265, 116, 111, 260, 102, 104, 257, 106,  85, 259,  90, 136,\n",
       "        273, 126,  98, 262,  87,  63, 261,  89,  93, 279,  91, 134, 272,\n",
       "        132, 113, 266, 117,  83, 264,  97, 130, 267,  84,  60,  77,  53,\n",
       "         29,  52,  18,  23,  42,  33,  17,  40,  48,  67,  66,  41,   6,\n",
       "         50,  59,  34,  27,  46,  51,   4,  56,  62,  73,  74,   3,  11,\n",
       "         38,  39,   5,  19,  10,  13,   7,  36,  49,  21,  32,  28,   9,\n",
       "         37,  22,  25,   8,  31,  16,  99, 119,  88, 135, 147, 118, 155,\n",
       "        128, 151, 157, 144, 159, 114, 103, 108, 145, 141, 133, 152, 153,\n",
       "        146, 156, 154, 148, 124,  82, 125, 131, 129, 143, 139, 137, 149,\n",
       "        158, 160, 150, 115,  96,  95, 109, 112, 105, 127, 121, 123, 142,\n",
       "        138, 140]),\n",
       " 'split0_test_jaccard': array([0.60841424, 0.61928105, 0.61093248, 0.61550889, 0.61165049,\n",
       "        0.58777429, 0.58571429, 0.56634304, 0.59308072, 0.58699187,\n",
       "        0.57624398, 0.59018987, 0.61412151, 0.61414791, 0.62236629,\n",
       "        0.63235294, 0.61865794, 0.60995185, 0.5872    , 0.61022364,\n",
       "        0.5843949 , 0.5952381 , 0.55070203, 0.58653846, 0.62254902,\n",
       "        0.63029316, 0.62932455, 0.61750405, 0.61264182, 0.59265176,\n",
       "        0.60826446, 0.58135861, 0.59967846, 0.57788162, 0.57507987,\n",
       "        0.58637084, 0.62479608, 0.60572337, 0.59259259, 0.61290323,\n",
       "        0.58642973, 0.61057692, 0.61844197, 0.6       , 0.60806452,\n",
       "        0.58267717, 0.59574468, 0.60096154, 0.62419355, 0.64072848,\n",
       "        0.62214984, 0.59775641, 0.61041009, 0.60517799, 0.57698541,\n",
       "        0.590625  , 0.58293839, 0.57809984, 0.57074722, 0.57120253,\n",
       "        0.62684124, 0.61835749, 0.62108731, 0.62926829, 0.63295269,\n",
       "        0.58038585, 0.59433962, 0.57512116, 0.56259905, 0.57643312,\n",
       "        0.57369255, 0.57320872, 0.63533225, 0.61967213, 0.60260586,\n",
       "        0.60032626, 0.59076433, 0.60650407, 0.63301282, 0.57233704,\n",
       "        0.59459459, 0.57052298, 0.56120827, 0.56446541, 0.61111111,\n",
       "        0.63140496, 0.60777958, 0.61928105, 0.59477124, 0.62520194,\n",
       "        0.58121019, 0.58176101, 0.59674797, 0.58320127, 0.57860616,\n",
       "        0.59134615, 0.63097199, 0.49442756, 0.63666667, 0.64191419,\n",
       "        0.12750455, 0.63410596, 0.63322368, 0.502     , 0.63907285,\n",
       "        0.63576159, 0.09708738, 0.63636364, 0.63681592, 0.502     ,\n",
       "        0.63349917, 0.63741722, 0.58610272, 0.63741722, 0.63966942,\n",
       "        0.        , 0.63576159, 0.63410596, 0.502     , 0.63741722,\n",
       "        0.63486842, 0.502     , 0.64238411, 0.63261944, 0.06066536,\n",
       "        0.63756178, 0.64451827, 0.502     , 0.63515755, 0.63576159,\n",
       "        0.26681128, 0.63681592, 0.63410596, 0.502     , 0.63471074,\n",
       "        0.63833333, 0.502     , 0.63245033, 0.63907285, 0.502     ,\n",
       "        0.63681592, 0.6307947 , 0.501     , 0.62932455, 0.63861386,\n",
       "        0.49588138, 0.63235294, 0.63054187, 0.22619048, 0.64201681,\n",
       "        0.63756178, 0.54968944, 0.63382594, 0.62419355, 0.4375    ,\n",
       "        0.64154104, 0.63727121, 0.59682081, 0.63295269, 0.6276771 ,\n",
       "        0.57575758, 0.62926829, 0.63227953, 0.50254669, 0.63047002,\n",
       "        0.62956811, 0.47902098, 0.63591433, 0.64860427, 0.530837  ,\n",
       "        0.63531353, 0.63398693, 0.51904762, 0.64238411, 0.655     ,\n",
       "        0.48042705, 0.64676617, 0.65      , 0.39380531, 0.63756178,\n",
       "        0.64119601, 0.18827709, 0.62602965, 0.63387097, 0.18181818,\n",
       "        0.62889984, 0.6384743 , 0.14745763, 0.62748344, 0.63140496,\n",
       "        0.38896747, 0.62975207, 0.63772955, 0.62706271, 0.63545151,\n",
       "        0.63515755, 0.63515755, 0.63515755, 0.6384743 , 0.63245033,\n",
       "        0.63410596, 0.63349917, 0.64      , 0.6384743 , 0.63621533,\n",
       "        0.63727121, 0.63471074, 0.63879599, 0.63681592, 0.63681592,\n",
       "        0.63245033, 0.63621262, 0.63471074, 0.63227953, 0.63621262,\n",
       "        0.63666667, 0.63879599, 0.63394343, 0.64      , 0.63515755,\n",
       "        0.63410596, 0.63636364, 0.63819095, 0.63681592, 0.63366337,\n",
       "        0.63515755, 0.63787375, 0.63621262, 0.63636364, 0.6320132 ,\n",
       "        0.63758389, 0.63305785, 0.64166667, 0.63926174, 0.63666667,\n",
       "        0.63893511, 0.63305785, 0.63305785, 0.63801653, 0.6318408 ,\n",
       "        0.63621262, 0.6293823 , 0.63681592, 0.64238411, 0.63815789,\n",
       "        0.63235294, 0.62006579, 0.63953488, 0.62908497, 0.62601626,\n",
       "        0.63305785, 0.62295082, 0.63322368, 0.63651316, 0.63711002,\n",
       "        0.6293823 , 0.63787375, 0.61889251, 0.64285714, 0.62582781,\n",
       "        0.63531353, 0.61928105, 0.63261944, 0.6369637 , 0.639399  ,\n",
       "        0.63741722, 0.63966942, 0.62581699, 0.63681592, 0.62932455,\n",
       "        0.63054187, 0.63576159, 0.62418301, 0.63606557, 0.62540717,\n",
       "        0.62828947, 0.63366337, 0.62664474, 0.63157895, 0.63833333,\n",
       "        0.63787375, 0.64297521, 0.63132137, 0.62871287, 0.63953488,\n",
       "        0.63097199, 0.64676617, 0.63061564]),\n",
       " 'split1_test_jaccard': array([0.62644628, 0.62254902, 0.60720131, 0.61285008, 0.62128326,\n",
       "        0.60912052, 0.59069021, 0.59550562, 0.5776    , 0.57722309,\n",
       "        0.60526316, 0.59618442, 0.62828947, 0.60591133, 0.61525974,\n",
       "        0.60327869, 0.61575563, 0.60615883, 0.60252366, 0.6032    ,\n",
       "        0.60534591, 0.60629921, 0.57753165, 0.59968354, 0.61400651,\n",
       "        0.61400651, 0.61285008, 0.58426966, 0.6384743 , 0.60883797,\n",
       "        0.60287081, 0.58038585, 0.60674157, 0.60125589, 0.61290323,\n",
       "        0.60032103, 0.6320132 , 0.62068966, 0.62479339, 0.61589404,\n",
       "        0.61426256, 0.60483871, 0.6012966 , 0.5955414 , 0.58532695,\n",
       "        0.61191626, 0.61437908, 0.59105431, 0.62913907, 0.62211221,\n",
       "        0.63756178, 0.59933775, 0.60995185, 0.61716172, 0.62519936,\n",
       "        0.61300813, 0.59463722, 0.6092504 , 0.57984496, 0.60031348,\n",
       "        0.62847791, 0.62131148, 0.61677632, 0.62131148, 0.61352657,\n",
       "        0.60260586, 0.58201893, 0.6214511 , 0.59345794, 0.59076433,\n",
       "        0.60128617, 0.56181534, 0.62153344, 0.62684124, 0.62602965,\n",
       "        0.61990212, 0.57980456, 0.60581583, 0.58372457, 0.58982512,\n",
       "        0.59624413, 0.58814353, 0.60128617, 0.62091503, 0.60848287,\n",
       "        0.62356792, 0.63801653, 0.62175325, 0.60737179, 0.61712439,\n",
       "        0.61057692, 0.61022364, 0.60576923, 0.57844691, 0.58897638,\n",
       "        0.59587956, 0.6422629 , 0.43319269, 0.6369637 , 0.64285714,\n",
       "        0.15703971, 0.62748344, 0.63560732, 0.502     , 0.64214047,\n",
       "        0.64285714, 0.01081081, 0.63756178, 0.64039409, 0.16898608,\n",
       "        0.62728785, 0.6384743 , 0.03868472, 0.6320132 , 0.640599  ,\n",
       "        0.5015015 , 0.63305785, 0.63105175, 0.5       , 0.64415157,\n",
       "        0.63061564, 0.502     , 0.6318408 , 0.63036304, 0.        ,\n",
       "        0.64344942, 0.6384743 , 0.502     , 0.62686567, 0.6369637 ,\n",
       "        0.47284345, 0.63140496, 0.625     , 0.00988142, 0.6327212 ,\n",
       "        0.62932455, 0.12106918, 0.63651316, 0.64119601, 0.43044907,\n",
       "        0.64297521, 0.63305785, 0.491     , 0.6427379 , 0.62852405,\n",
       "        0.33596392, 0.63202726, 0.64297521, 0.27325581, 0.61904762,\n",
       "        0.63893511, 0.16047297, 0.62833333, 0.62541254, 0.45901639,\n",
       "        0.62438221, 0.64489112, 0.14513557, 0.645     , 0.63289037,\n",
       "        0.45500634, 0.63934426, 0.62207358, 0.45138889, 0.62602965,\n",
       "        0.62354892, 0.18855219, 0.6327212 , 0.62251656, 0.48414376,\n",
       "        0.63210702, 0.64597315, 0.41912799, 0.62479339, 0.64833333,\n",
       "        0.20260492, 0.62040134, 0.62520458, 0.17325228, 0.63261944,\n",
       "        0.62520458, 0.2336    , 0.63439065, 0.6369637 , 0.34068627,\n",
       "        0.63934426, 0.63218391, 0.31897927, 0.64344942, 0.62251656,\n",
       "        0.44375   , 0.63097199, 0.63758389, 0.63166667, 0.63018242,\n",
       "        0.635     , 0.63484087, 0.63545151, 0.63879599, 0.63227953,\n",
       "        0.63727121, 0.63227953, 0.635     , 0.63727121, 0.63621262,\n",
       "        0.6293823 , 0.63377926, 0.63712375, 0.63879599, 0.6293823 ,\n",
       "        0.63621262, 0.63727121, 0.63210702, 0.63394343, 0.63666667,\n",
       "        0.63772955, 0.63361345, 0.64085667, 0.6345515 , 0.6360601 ,\n",
       "        0.635     , 0.63560732, 0.63545151, 0.63833333, 0.63515755,\n",
       "        0.63666667, 0.63545151, 0.635     , 0.63410596, 0.63394343,\n",
       "        0.6327212 , 0.6360601 , 0.6360601 , 0.64      , 0.6360601 ,\n",
       "        0.63758389, 0.63394343, 0.63394343, 0.63394343, 0.64      ,\n",
       "        0.62728785, 0.63227953, 0.6327212 , 0.6318408 , 0.62356792,\n",
       "        0.61842105, 0.62135922, 0.63934426, 0.62969005, 0.62356792,\n",
       "        0.63711002, 0.63114754, 0.63410596, 0.63289037, 0.62438221,\n",
       "        0.63741722, 0.64039409, 0.64473684, 0.62852405, 0.62520458,\n",
       "        0.62091503, 0.62052117, 0.6272578 , 0.63011457, 0.61769616,\n",
       "        0.63651316, 0.62895175, 0.62356792, 0.63305785, 0.6295082 ,\n",
       "        0.61990212, 0.62211221, 0.61576355, 0.63011457, 0.63252033,\n",
       "        0.6272578 , 0.6327212 , 0.64179104, 0.64891847, 0.63621262,\n",
       "        0.63245033, 0.62397373, 0.62703583, 0.64403974, 0.61563518,\n",
       "        0.62809917, 0.62993421, 0.63011457]),\n",
       " 'split2_test_jaccard': array([0.61712439, 0.63442623, 0.63387097, 0.61254019, 0.62297735,\n",
       "        0.61698718, 0.59489633, 0.59872611, 0.60192616, 0.58814103,\n",
       "        0.6       , 0.62074554, 0.62520194, 0.62019231, 0.61290323,\n",
       "        0.61750405, 0.61797753, 0.61965135, 0.60358891, 0.59265176,\n",
       "        0.61698718, 0.59429477, 0.57698289, 0.6187399 , 0.63311688,\n",
       "        0.63327948, 0.64193548, 0.61208267, 0.61712439, 0.5974026 ,\n",
       "        0.59076433, 0.62520194, 0.6215781 , 0.5984    , 0.61788618,\n",
       "        0.60191083, 0.63711002, 0.62439024, 0.62479608, 0.63592233,\n",
       "        0.6224    , 0.61463415, 0.6       , 0.60952381, 0.6191248 ,\n",
       "        0.62439807, 0.59241706, 0.6187399 , 0.62540717, 0.63295269,\n",
       "        0.65661642, 0.61698718, 0.60967742, 0.6185738 , 0.57692308,\n",
       "        0.5849359 , 0.58954041, 0.57661927, 0.58598726, 0.57770801,\n",
       "        0.63192182, 0.64485981, 0.62580645, 0.61935484, 0.62318841,\n",
       "        0.61326861, 0.6       , 0.61589404, 0.5936    , 0.57392687,\n",
       "        0.56847134, 0.60031847, 0.62258065, 0.63209076, 0.63371151,\n",
       "        0.60806452, 0.62214984, 0.62419355, 0.63934426, 0.5984127 ,\n",
       "        0.61003236, 0.58974359, 0.5952381 , 0.59399684, 0.61093248,\n",
       "        0.62197092, 0.63915858, 0.62581699, 0.6086262 , 0.62200957,\n",
       "        0.63474026, 0.62460064, 0.60162602, 0.59742351, 0.62801932,\n",
       "        0.60784314, 0.64285714, 0.503     , 0.64620355, 0.63961039,\n",
       "        0.        , 0.64620355, 0.65153971, 0.43647541, 0.63636364,\n",
       "        0.64297254, 0.50250752, 0.64516129, 0.64967105, 0.08275862,\n",
       "        0.63636364, 0.64065041, 0.        , 0.64772727, 0.64019449,\n",
       "        0.        , 0.63768116, 0.6461039 , 0.        , 0.64390244,\n",
       "        0.63268608, 0.        , 0.64886731, 0.64715447, 0.6090776 ,\n",
       "        0.63870968, 0.64343598, 0.        , 0.63812601, 0.63961039,\n",
       "        0.58199357, 0.64285714, 0.63607085, 0.47502548, 0.64686998,\n",
       "        0.6514658 , 0.        , 0.63548387, 0.64285714, 0.50709939,\n",
       "        0.6365105 , 0.64065041, 0.52670157, 0.64667747, 0.62987013,\n",
       "        0.11591356, 0.6542811 , 0.62379421, 0.06205674, 0.62903226,\n",
       "        0.62460064, 0.33386076, 0.63089431, 0.63636364, 0.4502447 ,\n",
       "        0.63915858, 0.63929147, 0.45454545, 0.64308682, 0.64677419,\n",
       "        0.31967213, 0.62418301, 0.63888889, 0.5206044 , 0.63081862,\n",
       "        0.636953  , 0.32279534, 0.64829822, 0.63157895, 0.20965309,\n",
       "        0.63548387, 0.63915858, 0.45498783, 0.63955343, 0.6461039 ,\n",
       "        0.64330218, 0.64390244, 0.64869281, 0.49934469, 0.63826367,\n",
       "        0.63089431, 0.26495726, 0.63047002, 0.63739837, 0.43127962,\n",
       "        0.64297254, 0.64065041, 0.5       , 0.6407767 , 0.64251208,\n",
       "        0.39636364, 0.63709677, 0.64297254, 0.6411093 , 0.64181524,\n",
       "        0.64227642, 0.64019449, 0.64563107, 0.64181524, 0.64285714,\n",
       "        0.64239482, 0.64829822, 0.64239482, 0.63974152, 0.64285714,\n",
       "        0.6461039 , 0.64552846, 0.64343598, 0.64239482, 0.63987138,\n",
       "        0.64285714, 0.64448052, 0.64401294, 0.64343598, 0.64458805,\n",
       "        0.64135703, 0.64285714, 0.64135703, 0.63915858, 0.64401294,\n",
       "        0.64297254, 0.64297254, 0.6407767 , 0.64090177, 0.63929147,\n",
       "        0.64135703, 0.64181524, 0.64354839, 0.64181524, 0.64251208,\n",
       "        0.6414791 , 0.63987138, 0.64563107, 0.64448052, 0.64123377,\n",
       "        0.64181524, 0.64458805, 0.64448052, 0.64181524, 0.64343598,\n",
       "        0.63192182, 0.64193548, 0.64495114, 0.63047002, 0.62805873,\n",
       "        0.64869281, 0.62580645, 0.64552846, 0.63123994, 0.62824675,\n",
       "        0.63192182, 0.63815789, 0.64193548, 0.6461039 , 0.63993453,\n",
       "        0.63888889, 0.63047002, 0.63621533, 0.63533225, 0.62318841,\n",
       "        0.63218391, 0.63398693, 0.62479871, 0.62987013, 0.63504823,\n",
       "        0.64495114, 0.63123994, 0.63338789, 0.64379085, 0.63770492,\n",
       "        0.63054187, 0.63106796, 0.62339744, 0.63577236, 0.62681745,\n",
       "        0.63666121, 0.63414634, 0.6407767 , 0.64343598, 0.63089431,\n",
       "        0.64098361, 0.64811784, 0.63371151, 0.63784666, 0.63004847,\n",
       "        0.62884927, 0.62987013, 0.63398693]),\n",
       " 'split3_test_jaccard': array([0.61867089, 0.63434022, 0.62321145, 0.58777429, 0.58631415,\n",
       "        0.60347551, 0.60189573, 0.58934169, 0.58942457, 0.55641422,\n",
       "        0.56221198, 0.5851735 , 0.62618297, 0.625     , 0.6128    ,\n",
       "        0.60280374, 0.5984375 , 0.60567823, 0.59311424, 0.5968254 ,\n",
       "        0.6043956 , 0.56501548, 0.57952756, 0.61550633, 0.60725552,\n",
       "        0.61417323, 0.60944882, 0.61783439, 0.61550633, 0.61075949,\n",
       "        0.58897638, 0.61102362, 0.5843949 , 0.55348837, 0.56765163,\n",
       "        0.58215962, 0.62480377, 0.62128326, 0.62816456, 0.6078125 ,\n",
       "        0.61598746, 0.62140575, 0.59594384, 0.59018987, 0.5815047 ,\n",
       "        0.58685446, 0.6       , 0.58059468, 0.62085308, 0.62637363,\n",
       "        0.61502347, 0.59148265, 0.58777429, 0.60576923, 0.59090909,\n",
       "        0.5892575 , 0.61129032, 0.60610932, 0.58730159, 0.60185185,\n",
       "        0.62085308, 0.61867089, 0.61381476, 0.59621451, 0.61821086,\n",
       "        0.60747664, 0.57450077, 0.56422018, 0.5787037 , 0.5696    ,\n",
       "        0.584375  , 0.5636646 , 0.62222222, 0.61897356, 0.62834646,\n",
       "        0.60347551, 0.60737179, 0.5915493 , 0.59594384, 0.58461538,\n",
       "        0.58675079, 0.57527734, 0.59746434, 0.57503949, 0.61163522,\n",
       "        0.63033175, 0.6214511 , 0.60289389, 0.59936909, 0.60436137,\n",
       "        0.61049285, 0.6144    , 0.59591195, 0.57187017, 0.5725429 ,\n",
       "        0.59429477, 0.64044944, 0.        , 0.6352    , 0.6318328 ,\n",
       "        0.27422003, 0.62420382, 0.65316045, 0.26912929, 0.6483871 ,\n",
       "        0.64354839, 0.03578529, 0.63987138, 0.63665595, 0.        ,\n",
       "        0.6416    , 0.62957075, 0.38888889, 0.64090177, 0.63563403,\n",
       "        0.48508634, 0.64308682, 0.64991896, 0.        , 0.64044944,\n",
       "        0.64193548, 0.503     , 0.6464    , 0.64583333, 0.        ,\n",
       "        0.64297254, 0.64365971, 0.503     , 0.64412238, 0.63782051,\n",
       "        0.50352467, 0.64365971, 0.6391097 , 0.503     , 0.64308682,\n",
       "        0.64239482, 0.36248562, 0.63301282, 0.63665595, 0.503     ,\n",
       "        0.64217252, 0.64686998, 0.503     , 0.632     , 0.63338534,\n",
       "        0.56852792, 0.63074485, 0.63722397, 0.509375  , 0.63937008,\n",
       "        0.62261146, 0.3494705 , 0.62834646, 0.63402889, 0.56786271,\n",
       "        0.63578275, 0.63812601, 0.32399299, 0.63333333, 0.64006515,\n",
       "        0.50597177, 0.63679245, 0.62637363, 0.58082192, 0.63621795,\n",
       "        0.63880126, 0.55591054, 0.62618297, 0.63422292, 0.15559772,\n",
       "        0.63809524, 0.64135703, 0.43820225, 0.63549921, 0.63968254,\n",
       "        0.50201884, 0.63391442, 0.63291139, 0.47123288, 0.64423077,\n",
       "        0.63836478, 0.33415536, 0.64057508, 0.63057325, 0.57645631,\n",
       "        0.62321145, 0.64069952, 0.55333333, 0.62974684, 0.63507109,\n",
       "        0.46071429, 0.63665595, 0.63738019, 0.63738019, 0.63375796,\n",
       "        0.63232964, 0.6388443 , 0.64012739, 0.63232964, 0.63535032,\n",
       "        0.62974684, 0.63091483, 0.63091483, 0.63593005, 0.64      ,\n",
       "        0.63476874, 0.6327504 , 0.63607595, 0.63291139, 0.6341853 ,\n",
       "        0.63492063, 0.62933754, 0.63593005, 0.63492063, 0.63074485,\n",
       "        0.63665087, 0.63249211, 0.63191153, 0.63665595, 0.63191153,\n",
       "        0.63349131, 0.63375796, 0.63694268, 0.63449367, 0.63116057,\n",
       "        0.63507109, 0.63232964, 0.63449367, 0.63549921, 0.63349131,\n",
       "        0.6388443 , 0.62974684, 0.62974684, 0.63621795, 0.63291139,\n",
       "        0.62933754, 0.63723917, 0.63464567, 0.63091483, 0.63349131,\n",
       "        0.62798092, 0.62085308, 0.6368    , 0.63955343, 0.63015873,\n",
       "        0.63015873, 0.62480127, 0.63955343, 0.64090177, 0.61587302,\n",
       "        0.61889764, 0.64376997, 0.64217252, 0.63924051, 0.6391097 ,\n",
       "        0.6109375 , 0.6310832 , 0.63607595, 0.63897764, 0.61732283,\n",
       "        0.62903226, 0.64490446, 0.63768116, 0.61829653, 0.63116057,\n",
       "        0.63795853, 0.64012739, 0.63665087, 0.64114833, 0.63665087,\n",
       "        0.62261146, 0.62421384, 0.62243286, 0.61345853, 0.61295419,\n",
       "        0.6336    , 0.64012739, 0.63853503, 0.63564669, 0.64069952,\n",
       "        0.62599049, 0.63924051, 0.63464567, 0.62916006, 0.6391097 ,\n",
       "        0.61806656, 0.61320755, 0.62916006]),\n",
       " 'split4_test_jaccard': array([0.65097403, 0.64851485, 0.63311688, 0.63880126, 0.63563403,\n",
       "        0.61835749, 0.5971564 , 0.60410095, 0.63897764, 0.59305994,\n",
       "        0.60093897, 0.60314961, 0.65217391, 0.64181524, 0.63961039,\n",
       "        0.62519936, 0.63902439, 0.62760835, 0.61587302, 0.61695447,\n",
       "        0.63974152, 0.60377358, 0.6208    , 0.58664547, 0.65853659,\n",
       "        0.64379085, 0.63252033, 0.61987382, 0.61736334, 0.63798701,\n",
       "        0.60188088, 0.61987382, 0.64057508, 0.58712716, 0.61538462,\n",
       "        0.60917722, 0.63344051, 0.65188834, 0.62379421, 0.63861386,\n",
       "        0.63947798, 0.62621359, 0.64205457, 0.63636364, 0.62019231,\n",
       "        0.60031348, 0.62341772, 0.6197411 , 0.6618123 , 0.63768116,\n",
       "        0.64401294, 0.65544872, 0.61677632, 0.61305732, 0.62480127,\n",
       "        0.6       , 0.60940032, 0.59253499, 0.58423493, 0.60254372,\n",
       "        0.65105008, 0.64516129, 0.64285714, 0.6025641 , 0.63099042,\n",
       "        0.63795853, 0.63765823, 0.62025316, 0.60096931, 0.6066879 ,\n",
       "        0.61574803, 0.5799373 , 0.63902439, 0.63857374, 0.62760835,\n",
       "        0.62479871, 0.62599049, 0.63123994, 0.61476726, 0.62779553,\n",
       "        0.60062402, 0.58372457, 0.61453397, 0.6028481 , 0.64991896,\n",
       "        0.65378422, 0.62179487, 0.63929147, 0.63782051, 0.62662338,\n",
       "        0.61389338, 0.6048    , 0.62875197, 0.61259843, 0.60697306,\n",
       "        0.59968603, 0.64958678, 0.        , 0.66557912, 0.64754098,\n",
       "        0.        , 0.65415987, 0.65139116, 0.5005005 , 0.65793781,\n",
       "        0.65579119, 0.47247706, 0.66557377, 0.65522876, 0.40302613,\n",
       "        0.65296053, 0.6639478 , 0.503     , 0.65252855, 0.64811784,\n",
       "        0.        , 0.64983713, 0.65953947, 0.52096569, 0.6525974 ,\n",
       "        0.65196078, 0.21321321, 0.65849673, 0.64590164, 0.503     ,\n",
       "        0.65517241, 0.66121113, 0.        , 0.65686275, 0.65089723,\n",
       "        0.503     , 0.65089723, 0.64926591, 0.5015015 , 0.6503268 ,\n",
       "        0.66176471, 0.50705645, 0.64926591, 0.65372168, 0.37076649,\n",
       "        0.65346535, 0.6503268 , 0.        , 0.65960912, 0.65422078,\n",
       "        0.50153846, 0.65789474, 0.65466448, 0.41216216, 0.65153971,\n",
       "        0.65905383, 0.37794118, 0.66882068, 0.66013072, 0.58012821,\n",
       "        0.65316045, 0.65483871, 0.57428215, 0.64448052, 0.66118421,\n",
       "        0.59145527, 0.66013072, 0.65511551, 0.59324759, 0.64763458,\n",
       "        0.65517241, 0.59612519, 0.65849673, 0.65139116, 0.44008056,\n",
       "        0.65691057, 0.65153971, 0.34146341, 0.65793781, 0.65409836,\n",
       "        0.50073638, 0.66229508, 0.65857605, 0.05052265, 0.64715447,\n",
       "        0.65024631, 0.50454545, 0.65909091, 0.66178862, 0.22201835,\n",
       "        0.65466448, 0.66940789, 0.48121387, 0.64811784, 0.63680782,\n",
       "        0.38896552, 0.66231648, 0.66776316, 0.65551839, 0.66886326,\n",
       "        0.66721311, 0.66666667, 0.66885246, 0.67159278, 0.66666667,\n",
       "        0.66338259, 0.66885246, 0.66939444, 0.66666667, 0.66447368,\n",
       "        0.66006601, 0.66995074, 0.66447368, 0.66229508, 0.66448445,\n",
       "        0.66336634, 0.66227348, 0.66502463, 0.66611842, 0.66447368,\n",
       "        0.67213115, 0.65897858, 0.66065574, 0.66721311, 0.66229508,\n",
       "        0.6683087 , 0.66666667, 0.66227348, 0.67105263, 0.66995074,\n",
       "        0.66392092, 0.66666667, 0.66392092, 0.66229508, 0.66939444,\n",
       "        0.66612111, 0.66227348, 0.66502463, 0.66939444, 0.66885246,\n",
       "        0.66940789, 0.6683087 , 0.66448445, 0.6683087 , 0.66940789,\n",
       "        0.65737705, 0.66282895, 0.66339869, 0.66721311, 0.63934426,\n",
       "        0.66995074, 0.66721582, 0.66666667, 0.65802269, 0.66006601,\n",
       "        0.66666667, 0.65466448, 0.65415987, 0.66556837, 0.66229508,\n",
       "        0.65533981, 0.6503268 , 0.66396104, 0.64705882, 0.66448445,\n",
       "        0.65691057, 0.65309446, 0.65403624, 0.66343042, 0.65681445,\n",
       "        0.66284779, 0.65960912, 0.66556837, 0.64332248, 0.66121113,\n",
       "        0.66940789, 0.66666667, 0.66229508, 0.63754045, 0.65793781,\n",
       "        0.65579119, 0.66176471, 0.65728477, 0.65630115, 0.6650165 ,\n",
       "        0.65579119, 0.66831683, 0.64926591, 0.66776316, 0.66448445,\n",
       "        0.66280992, 0.6601626 , 0.65845649]),\n",
       " 'mean_test_jaccard': array([0.62432597, 0.63182227, 0.62166662, 0.61349494, 0.61557185,\n",
       "        0.607143  , 0.59407059, 0.59080348, 0.60020182, 0.58036603,\n",
       "        0.58893162, 0.59908859, 0.62919396, 0.62141336, 0.62058793,\n",
       "        0.61622776, 0.6179706 , 0.61380972, 0.60045996, 0.60397105,\n",
       "        0.61017302, 0.59292423, 0.58110883, 0.60142274, 0.6270929 ,\n",
       "        0.62710865, 0.62521585, 0.61031292, 0.62022204, 0.60952777,\n",
       "        0.59855137, 0.60356877, 0.61059362, 0.58363061, 0.59778111,\n",
       "        0.59598791, 0.63043272, 0.62479497, 0.61882817, 0.62222919,\n",
       "        0.61571154, 0.61553382, 0.6115474 , 0.60632374, 0.60284266,\n",
       "        0.60123189, 0.60519171, 0.60221831, 0.63228103, 0.63196963,\n",
       "        0.63507289, 0.61220254, 0.60691799, 0.61194801, 0.59896364,\n",
       "        0.59556531, 0.59756133, 0.59252277, 0.58162319, 0.59072392,\n",
       "        0.63182883, 0.62967219, 0.6240684 , 0.61374264, 0.62377379,\n",
       "        0.6083391 , 0.59770351, 0.59938793, 0.585866  , 0.58348244,\n",
       "        0.58871462, 0.57578889, 0.62813859, 0.62723029, 0.62366037,\n",
       "        0.61131343, 0.6052162 , 0.61186054, 0.61335855, 0.59459715,\n",
       "        0.59764918, 0.5814824 , 0.59394617, 0.59145298, 0.61841613,\n",
       "        0.63221195, 0.62564013, 0.62180733, 0.60959177, 0.61906413,\n",
       "        0.61018272, 0.60715706, 0.60576143, 0.58870806, 0.59502356,\n",
       "        0.59780993, 0.64122565, 0.28612405, 0.64412261, 0.6407511 ,\n",
       "        0.11175286, 0.63723133, 0.64498447, 0.44202104, 0.64478037,\n",
       "        0.64418617, 0.22373361, 0.64490637, 0.64375315, 0.23135417,\n",
       "        0.63834224, 0.64201209, 0.30333527, 0.6421176 , 0.64084296,\n",
       "        0.19731757, 0.63988491, 0.64414401, 0.30459314, 0.64370361,\n",
       "        0.63841328, 0.34404264, 0.64559779, 0.64037438, 0.23454859,\n",
       "        0.64357317, 0.64625988, 0.3014    , 0.64022687, 0.64021068,\n",
       "        0.46563459, 0.64112699, 0.63671048, 0.39828168, 0.64154311,\n",
       "        0.64465664, 0.29852225, 0.63734522, 0.64270073, 0.46266299,\n",
       "        0.6423879 , 0.64033995, 0.40434031, 0.64206981, 0.63692283,\n",
       "        0.40356505, 0.64146018, 0.63783995, 0.29660804, 0.63620129,\n",
       "        0.63655256, 0.35428697, 0.63804414, 0.63602587, 0.4989504 ,\n",
       "        0.638805  , 0.6428837 , 0.41895539, 0.63977067, 0.6417182 ,\n",
       "        0.48957262, 0.63794375, 0.63494623, 0.5297219 , 0.63423416,\n",
       "        0.63680874, 0.42848085, 0.64032269, 0.63766277, 0.36406243,\n",
       "        0.63958205, 0.64240308, 0.43456582, 0.64003359, 0.64864363,\n",
       "        0.46581787, 0.64145589, 0.64307697, 0.31763156, 0.63996603,\n",
       "        0.6371812 , 0.30510703, 0.63811126, 0.64011898, 0.35045175,\n",
       "        0.63781851, 0.64428321, 0.40019682, 0.63791485, 0.6336625 ,\n",
       "        0.41575218, 0.63935865, 0.64468587, 0.63854745, 0.64201408,\n",
       "        0.64239534, 0.64314077, 0.64504399, 0.64460159, 0.6419208 ,\n",
       "        0.64138029, 0.64276884, 0.64354082, 0.64361675, 0.64395176,\n",
       "        0.64151843, 0.64334392, 0.64398107, 0.64264264, 0.64094787,\n",
       "        0.64196141, 0.64191507, 0.64235708, 0.6421396 , 0.64253717,\n",
       "        0.64490705, 0.64134745, 0.64174488, 0.64351583, 0.64188744,\n",
       "        0.6427757 , 0.64307362, 0.64272706, 0.64431947, 0.64184474,\n",
       "        0.64243465, 0.64282736, 0.64263512, 0.64201582, 0.64227089,\n",
       "        0.64334992, 0.64020193, 0.64362586, 0.64587093, 0.64314488,\n",
       "        0.64341593, 0.64342744, 0.64212238, 0.64259974, 0.6436352 ,\n",
       "        0.63615605, 0.63745587, 0.64293739, 0.64229229, 0.63185751,\n",
       "        0.63991525, 0.63184971, 0.64612554, 0.63778788, 0.63075399,\n",
       "        0.6375308 , 0.63813814, 0.6411195 , 0.64406326, 0.64056631,\n",
       "        0.63439314, 0.63802957, 0.63997633, 0.63854998, 0.63120562,\n",
       "        0.63487106, 0.63435761, 0.63527867, 0.63573507, 0.63602368,\n",
       "        0.64393757, 0.63991952, 0.63699841, 0.63962708, 0.63887993,\n",
       "        0.63460104, 0.63596445, 0.62961439, 0.6305903 , 0.63112739,\n",
       "        0.63631994, 0.6404846 , 0.64100646, 0.64317625, 0.64223126,\n",
       "        0.63861787, 0.64452482, 0.63519606, 0.6415045 , 0.63776254,\n",
       "        0.63375938, 0.63598813, 0.63646674]),\n",
       " 'std_test_jaccard': array([0.01450269, 0.01034445, 0.01101733, 0.01617   , 0.01650056,\n",
       "        0.01109216, 0.00552615, 0.01313127, 0.0209009 , 0.01303115,\n",
       "        0.01678285, 0.01238359, 0.01249862, 0.01203072, 0.01013028,\n",
       "        0.01174753, 0.01288552, 0.00853308, 0.00981221, 0.00880279,\n",
       "        0.01812137, 0.0147155 , 0.02250459, 0.01371981, 0.01794667,\n",
       "        0.01153696, 0.01225801, 0.01327463, 0.0092802 , 0.01577175,\n",
       "        0.00743513, 0.01907963, 0.0191828 , 0.01721985, 0.02175273,\n",
       "        0.01011459, 0.00489056, 0.01501417, 0.01320102, 0.01257703,\n",
       "        0.01714164, 0.00759345, 0.01708868, 0.01630451, 0.01646508,\n",
       "        0.01550909, 0.0117982 , 0.01532193, 0.01500149, 0.00691071,\n",
       "        0.01496146, 0.0232234 , 0.00992654, 0.00559151, 0.02186139,\n",
       "        0.01001097, 0.01109381, 0.01360405, 0.00599265, 0.01346103,\n",
       "        0.01025707, 0.01256606, 0.01022972, 0.01234524, 0.00738412,\n",
       "        0.01852894, 0.02189677, 0.02457739, 0.01369826, 0.01360326,\n",
       "        0.01756778, 0.01391277, 0.00748026, 0.00745317, 0.01083953,\n",
       "        0.00946916, 0.01776274, 0.01418056, 0.02118951, 0.01862585,\n",
       "        0.00764555, 0.00743313, 0.01768361, 0.02001032, 0.0157889 ,\n",
       "        0.01139427, 0.01172397, 0.01171218, 0.01501249, 0.00804269,\n",
       "        0.01706752, 0.01425944, 0.01203264, 0.01460027, 0.02021381,\n",
       "        0.00569294, 0.00598976, 0.23485683, 0.01141198, 0.00515224,\n",
       "        0.10360695, 0.01132768, 0.00868459, 0.09004122, 0.00770082,\n",
       "        0.00646987, 0.2173882 , 0.01076549, 0.00743626, 0.19090081,\n",
       "        0.00864728, 0.01158832, 0.24049742, 0.00728734, 0.00404757,\n",
       "        0.24171942, 0.00596265, 0.01045353, 0.24880689, 0.00508818,\n",
       "        0.00777317, 0.20525602, 0.00868676, 0.00730324, 0.26555587,\n",
       "        0.00624034, 0.00777076, 0.24609234, 0.0100056 , 0.0054884 ,\n",
       "        0.1057865 , 0.00660256, 0.00784504, 0.19448502, 0.0068179 ,\n",
       "        0.01112855, 0.20473698, 0.006148  , 0.00588984, 0.05409579,\n",
       "        0.00614316, 0.0075712 , 0.20250931, 0.01089171, 0.00932601,\n",
       "        0.16287857, 0.01201009, 0.01058641, 0.15436373, 0.0111783 ,\n",
       "        0.01303998, 0.12389184, 0.01552065, 0.01294444, 0.0617768 ,\n",
       "        0.00928268, 0.00654204, 0.16794303, 0.00544884, 0.01168165,\n",
       "        0.09812019, 0.01233021, 0.01156417, 0.05216207, 0.00743849,\n",
       "        0.01067207, 0.15207396, 0.01158493, 0.01082994, 0.15186298,\n",
       "        0.00886994, 0.00597789, 0.05740007, 0.01076235, 0.00560366,\n",
       "        0.14389231, 0.01391851, 0.01218564, 0.17584539, 0.00514806,\n",
       "        0.00861604, 0.11044816, 0.01152747, 0.01110936, 0.14327525,\n",
       "        0.01100534, 0.01294161, 0.14862612, 0.00798064, 0.00662602,\n",
       "        0.03038529, 0.01184994, 0.01172773, 0.00974867, 0.01394372,\n",
       "        0.01284003, 0.01194344, 0.01249895, 0.01384296, 0.01295415,\n",
       "        0.01174962, 0.01447342, 0.01352495, 0.01159407, 0.01056135,\n",
       "        0.0107324 , 0.01407787, 0.01055124, 0.01029295, 0.0122613 ,\n",
       "        0.01124326, 0.01125454, 0.01201163, 0.01259505, 0.01182318,\n",
       "        0.01372107, 0.00956951, 0.01015989, 0.01200271, 0.01095224,\n",
       "        0.01321967, 0.01219994, 0.00992809, 0.01352794, 0.01429875,\n",
       "        0.01098425, 0.0123165 , 0.01113325, 0.01047156, 0.01405271,\n",
       "        0.01173494, 0.01153052, 0.01196357, 0.01205502, 0.01312568,\n",
       "        0.01364138, 0.01308582, 0.01192129, 0.01337192, 0.01356092,\n",
       "        0.01107917, 0.01436307, 0.01097479, 0.01324807, 0.0060296 ,\n",
       "        0.01785248, 0.0178094 , 0.01053479, 0.01098485, 0.01523817,\n",
       "        0.01579708, 0.01081075, 0.00752808, 0.01159078, 0.01223545,\n",
       "        0.01444573, 0.00724204, 0.01464114, 0.00635623, 0.01690811,\n",
       "        0.01201681, 0.01327549, 0.01038127, 0.01509061, 0.01268186,\n",
       "        0.00991977, 0.01080198, 0.0150652 , 0.00410766, 0.01169758,\n",
       "        0.01791245, 0.01610344, 0.01661083, 0.00893153, 0.0148479 ,\n",
       "        0.0103272 , 0.01095394, 0.00977772, 0.008905  , 0.01184447,\n",
       "        0.00998562, 0.01436414, 0.00751092, 0.01432016, 0.01592573,\n",
       "        0.01519286, 0.01608481, 0.0111146 ]),\n",
       " 'rank_test_jaccard': array([177, 161, 183, 197, 193, 212, 239, 244, 225, 255, 246, 227, 169,\n",
       "        184, 185, 191, 190, 195, 224, 218, 207, 241, 254, 222, 173, 172,\n",
       "        175, 205, 186, 209, 229, 219, 204, 250, 231, 235, 166, 176, 188,\n",
       "        181, 192, 194, 202, 214, 220, 223, 217, 221, 155, 157, 146, 199,\n",
       "        213, 200, 228, 236, 234, 242, 252, 245, 160, 167, 178, 196, 179,\n",
       "        210, 232, 226, 249, 251, 247, 256, 170, 171, 180, 203, 216, 201,\n",
       "        198, 238, 233, 253, 240, 243, 189, 156, 174, 182, 208, 187, 206,\n",
       "        211, 215, 248, 237, 230,  81, 283,  19,  87, 288, 128,   7, 263,\n",
       "         10,  17, 286,   9,  24, 285, 113,  66, 279,  62,  86, 287, 102,\n",
       "         18, 278,  25, 112, 275,   5,  90, 284,  29,   2, 280,  93,  94,\n",
       "        261,  82, 133, 271,  74,  12, 281, 127,  47, 262,  55,  91, 268,\n",
       "         63, 131, 269,  77, 120, 282, 137, 134, 273, 116, 139, 258, 108,\n",
       "         42, 266, 103,  73, 259, 118, 147, 257, 152, 132, 265,  92, 124,\n",
       "        272, 105,  53, 264,  97,   1, 260,  78,  39, 276,  99, 129, 277,\n",
       "        115,  96, 274, 121,  16, 270, 119, 154, 267, 106,  11, 111,  65,\n",
       "         54,  38,   6,  13,  68,  79,  45,  30,  28,  22,  75,  35,  21,\n",
       "         48,  85,  67,  69,  56,  60,  51,   8,  80,  72,  31,  70,  44,\n",
       "         40,  46,  15,  71,  52,  43,  49,  64,  58,  34,  95,  27,   4,\n",
       "         37,  33,  32,  61,  50,  26, 138, 126,  41,  57, 158, 101, 159,\n",
       "          3, 122, 164, 125, 114,  83,  20,  88, 150, 117,  98, 110, 162,\n",
       "        148, 151, 144, 143, 140,  23, 100, 130, 104, 107, 149, 142, 168,\n",
       "        165, 163, 136,  89,  84,  36,  59, 109,  14, 145,  76, 123, 153,\n",
       "        141, 135]),\n",
       " 'split0_test_neg_log_loss': array([-0.48670555, -0.49014375, -0.48493827, -0.51622217, -0.51725211,\n",
       "        -0.56164246, -0.57811978, -0.60057567, -0.63098832, -0.65567437,\n",
       "        -0.66798943, -0.63079569, -0.48269828, -0.48420602, -0.48082381,\n",
       "        -0.51004222, -0.54429539, -0.52772929, -0.58428522, -0.55735118,\n",
       "        -0.5538151 , -0.65515942, -0.6797551 , -0.60585981, -0.49715401,\n",
       "        -0.49150174, -0.48609497, -0.54213426, -0.52163718, -0.52903062,\n",
       "        -0.55989192, -0.58106879, -0.54802942, -0.65133929, -0.64477544,\n",
       "        -0.64640828, -0.48014133, -0.49272135, -0.49113489, -0.496048  ,\n",
       "        -0.50797866, -0.51208811, -0.53500212, -0.52550753, -0.54039533,\n",
       "        -0.58970508, -0.56842199, -0.55962524, -0.49001916, -0.47817591,\n",
       "        -0.49766297, -0.56375452, -0.54933055, -0.51892788, -0.59491272,\n",
       "        -0.62011067, -0.61159801, -0.74497753, -0.72686148, -0.72438283,\n",
       "        -0.48643075, -0.49295882, -0.48753607, -0.52719844, -0.51608713,\n",
       "        -0.55350149, -0.6026142 , -0.62131805, -0.64219554, -0.72429523,\n",
       "        -0.69497495, -0.69645768, -0.48453062, -0.4880889 , -0.49264949,\n",
       "        -0.53824456, -0.54224773, -0.52180175, -0.56565792, -0.62390923,\n",
       "        -0.59209526, -0.65884748, -0.71067767, -0.64955244, -0.49727021,\n",
       "        -0.48404437, -0.48532469, -0.51394891, -0.52997171, -0.52013553,\n",
       "        -0.58010212, -0.5788396 , -0.56973035, -0.61282613, -0.59825814,\n",
       "        -0.59700847, -0.47187676, -0.70538727, -0.47285185, -0.46805836,\n",
       "        -0.69252435, -0.46661425, -0.46745265, -0.70815016, -0.46534972,\n",
       "        -0.46683763, -0.68613727, -0.46900346, -0.47206222, -0.74418869,\n",
       "        -0.47223958, -0.46788153, -0.64813147, -0.467618  , -0.46709972,\n",
       "        -0.71483827, -0.46743015, -0.46685786, -0.7081056 , -0.46528222,\n",
       "        -0.47059203, -0.73878575, -0.47018904, -0.46820521, -0.68665483,\n",
       "        -0.4652943 , -0.4653439 , -0.73410521, -0.46565982, -0.46692105,\n",
       "        -0.71240077, -0.46454645, -0.47037374, -0.70268499, -0.47083791,\n",
       "        -0.46727269, -0.68887593, -0.47004988, -0.46606655, -0.70557418,\n",
       "        -0.46614607, -0.46592771, -0.70066204, -0.4686333 , -0.47077969,\n",
       "        -0.59394767, -0.47473012, -0.47247241, -0.84693063, -0.47015063,\n",
       "        -0.4678794 , -0.58018903, -0.46949225, -0.46727547, -0.61837358,\n",
       "        -0.46531665, -0.46865478, -0.54169965, -0.46628164, -0.47289713,\n",
       "        -0.60175287, -0.47444587, -0.46968189, -0.56949968, -0.47344502,\n",
       "        -0.46996496, -0.56541959, -0.46878861, -0.47169856, -0.72176043,\n",
       "        -0.47445321, -0.47163884, -0.58595488, -0.47041062, -0.46567183,\n",
       "        -0.56286783, -0.46746157, -0.4655    , -0.66497268, -0.46478069,\n",
       "        -0.47003091, -0.70279663, -0.47699884, -0.47180953, -0.73101385,\n",
       "        -0.47070127, -0.46908972, -0.75730939, -0.47057482, -0.47118598,\n",
       "        -0.69686854, -0.46699142, -0.46590421, -0.46753957, -0.46646332,\n",
       "        -0.4644372 , -0.46471351, -0.46423024, -0.46494438, -0.46500255,\n",
       "        -0.46471998, -0.46482767, -0.46599738, -0.46506238, -0.47021689,\n",
       "        -0.46616045, -0.46674195, -0.46574957, -0.46603974, -0.4651435 ,\n",
       "        -0.46459612, -0.46452216, -0.46486598, -0.46510819, -0.46456717,\n",
       "        -0.46477885, -0.46912549, -0.46647943, -0.4658141 , -0.4647932 ,\n",
       "        -0.46506592, -0.46515334, -0.46536009, -0.464328  , -0.46451225,\n",
       "        -0.46448605, -0.46402669, -0.46480224, -0.46679459, -0.46642408,\n",
       "        -0.46597511, -0.46511869, -0.46568673, -0.46503027, -0.46490765,\n",
       "        -0.46483973, -0.46463775, -0.46420253, -0.46458921, -0.46433655,\n",
       "        -0.46441871, -0.47119154, -0.46927562, -0.47855826, -0.47633535,\n",
       "        -0.47265939, -0.47650789, -0.47132317, -0.4772774 , -0.48410664,\n",
       "        -0.47801814, -0.48780966, -0.46677488, -0.47012653, -0.46847521,\n",
       "        -0.47462238, -0.47791813, -0.47408414, -0.4803325 , -0.47041399,\n",
       "        -0.4733543 , -0.47452334, -0.47466344, -0.47286445, -0.47399373,\n",
       "        -0.46910175, -0.47180096, -0.47691527, -0.47234667, -0.47768216,\n",
       "        -0.47376624, -0.47382029, -0.47533503, -0.47570742, -0.4770649 ,\n",
       "        -0.476088  , -0.4714299 , -0.46766267, -0.47069083, -0.47261475,\n",
       "        -0.47031611, -0.47147409, -0.47116437, -0.46822822, -0.46951053,\n",
       "        -0.46768918, -0.47174135, -0.47634025]),\n",
       " 'split1_test_neg_log_loss': array([-0.4678622 , -0.47470911, -0.47308605, -0.51640031, -0.52968032,\n",
       "        -0.49349563, -0.58313516, -0.59578511, -0.59511031, -0.66813539,\n",
       "        -0.56992836, -0.6254704 , -0.46764596, -0.47836118, -0.4747485 ,\n",
       "        -0.49488387, -0.50998081, -0.51792743, -0.56639127, -0.54163597,\n",
       "        -0.54588902, -0.58836638, -0.66232638, -0.60410908, -0.48171647,\n",
       "        -0.46599293, -0.46102824, -0.5113551 , -0.51325303, -0.51977838,\n",
       "        -0.53654704, -0.59217283, -0.53243795, -0.56933593, -0.57352722,\n",
       "        -0.56507371, -0.47409844, -0.46890106, -0.47165979, -0.49787129,\n",
       "        -0.48850022, -0.50201683, -0.51206149, -0.51971888, -0.53201511,\n",
       "        -0.53913828, -0.54675626, -0.55737702, -0.46899258, -0.4790373 ,\n",
       "        -0.48010986, -0.52919372, -0.52359127, -0.51901734, -0.52358229,\n",
       "        -0.51343651, -0.58851214, -0.60628647, -0.68749438, -0.64903086,\n",
       "        -0.48167172, -0.46134857, -0.4822223 , -0.52059902, -0.54048605,\n",
       "        -0.51788695, -0.63605546, -0.53014841, -0.5954395 , -0.69525133,\n",
       "        -0.65801037, -0.76621639, -0.46641452, -0.47406318, -0.4716471 ,\n",
       "        -0.50886965, -0.55961732, -0.50463303, -0.57724158, -0.54452383,\n",
       "        -0.59770251, -0.63972736, -0.64410673, -0.57272331, -0.47257657,\n",
       "        -0.46673009, -0.47005924, -0.49051797, -0.52175159, -0.49739773,\n",
       "        -0.53924105, -0.56309595, -0.54236926, -0.60281705, -0.58233409,\n",
       "        -0.59312354, -0.46563772, -0.67661571, -0.46569203, -0.46010034,\n",
       "        -0.68912534, -0.45824125, -0.45623136, -0.69404353, -0.45868166,\n",
       "        -0.45720409, -0.71409907, -0.4600471 , -0.46420648, -0.66685726,\n",
       "        -0.46752997, -0.46227524, -0.69565524, -0.46090283, -0.46148674,\n",
       "        -0.69371342, -0.46107907, -0.45869618, -0.72182554, -0.45803571,\n",
       "        -0.46703823, -0.7127383 , -0.46733409, -0.4652325 , -0.69565863,\n",
       "        -0.45956397, -0.46189372, -0.72844545, -0.46026661, -0.45878998,\n",
       "        -0.69100214, -0.46224082, -0.46922417, -0.69819408, -0.46457603,\n",
       "        -0.46165965, -0.70142405, -0.46422124, -0.45977745, -0.69908539,\n",
       "        -0.46234633, -0.46031068, -0.69665109, -0.45748039, -0.46870015,\n",
       "        -0.80268449, -0.4657343 , -0.4585555 , -0.91866113, -0.45938513,\n",
       "        -0.45661808, -0.75231778, -0.4586098 , -0.46077189, -0.70724652,\n",
       "        -0.46803519, -0.45997924, -0.74863471, -0.4670282 , -0.46196683,\n",
       "        -0.70899588, -0.46668142, -0.46775609, -0.60832649, -0.46198495,\n",
       "        -0.45904455, -0.8093193 , -0.46339596, -0.47234748, -0.76531229,\n",
       "        -0.46710714, -0.46111436, -0.6896833 , -0.46149543, -0.45656445,\n",
       "        -0.74770576, -0.45765609, -0.46422222, -0.74631562, -0.46334614,\n",
       "        -0.46778392, -0.72806889, -0.46379737, -0.46639193, -0.77612512,\n",
       "        -0.46401669, -0.46243001, -0.67856828, -0.45975141, -0.46353914,\n",
       "        -0.64596683, -0.45656961, -0.45842826, -0.45893438, -0.4580052 ,\n",
       "        -0.45656999, -0.45594382, -0.4566594 , -0.45494681, -0.45517822,\n",
       "        -0.45632778, -0.45552158, -0.45536037, -0.45459567, -0.4576951 ,\n",
       "        -0.45900493, -0.4569831 , -0.45517939, -0.45711896, -0.45822165,\n",
       "        -0.45551138, -0.45472047, -0.45500641, -0.45499292, -0.4543944 ,\n",
       "        -0.45632938, -0.45810839, -0.46303509, -0.45898297, -0.45632502,\n",
       "        -0.45612779, -0.45656566, -0.45528477, -0.45530995, -0.4562845 ,\n",
       "        -0.45475285, -0.45432621, -0.45489864, -0.46069988, -0.45827364,\n",
       "        -0.4583752 , -0.45691479, -0.4565506 , -0.45627383, -0.45567905,\n",
       "        -0.45548754, -0.45495336, -0.45444491, -0.4550688 , -0.45448208,\n",
       "        -0.45712569, -0.46000226, -0.45518954, -0.46676556, -0.46624178,\n",
       "        -0.46613924, -0.46059694, -0.46223864, -0.46554117, -0.46759003,\n",
       "        -0.463691  , -0.47035706, -0.46037075, -0.46371121, -0.45719152,\n",
       "        -0.46255466, -0.45815256, -0.46403715, -0.4631742 , -0.4670122 ,\n",
       "        -0.45962479, -0.47101436, -0.47069329, -0.46413915, -0.45782035,\n",
       "        -0.45712847, -0.46494521, -0.46012102, -0.45384857, -0.46111997,\n",
       "        -0.46061058, -0.45941613, -0.46484869, -0.4710954 , -0.47392188,\n",
       "        -0.46507375, -0.46117259, -0.4594395 , -0.45235616, -0.45932214,\n",
       "        -0.45739935, -0.45780166, -0.46080933, -0.45761238, -0.46341569,\n",
       "        -0.46350634, -0.45751476, -0.46047717]),\n",
       " 'split2_test_neg_log_loss': array([-0.46519936, -0.46128599, -0.45511983, -0.50764956, -0.47988579,\n",
       "        -0.4852759 , -0.57871657, -0.55149154, -0.54230139, -0.61779682,\n",
       "        -0.61467987, -0.59902171, -0.45804996, -0.45814633, -0.45601127,\n",
       "        -0.51136687, -0.49382504, -0.46785234, -0.53257845, -0.57062104,\n",
       "        -0.52702981, -0.62278183, -0.66092665, -0.57622705, -0.4654247 ,\n",
       "        -0.45611161, -0.46027939, -0.50248165, -0.49064011, -0.5044424 ,\n",
       "        -0.56150567, -0.55385272, -0.53459773, -0.58026731, -0.55923087,\n",
       "        -0.5884824 , -0.46006215, -0.45902006, -0.46106208, -0.48391034,\n",
       "        -0.47814186, -0.49209853, -0.5300571 , -0.51555129, -0.52350098,\n",
       "        -0.55477619, -0.56942772, -0.54267029, -0.46760156, -0.4572674 ,\n",
       "        -0.45225577, -0.51224348, -0.50906698, -0.50286754, -0.62573966,\n",
       "        -0.59224349, -0.62670344, -0.68616452, -0.59511563, -0.69169505,\n",
       "        -0.47249932, -0.48166615, -0.46901132, -0.54190351, -0.48153265,\n",
       "        -0.5148832 , -0.57495293, -0.55081114, -0.58340015, -0.72122631,\n",
       "        -0.69100373, -0.66326872, -0.45780365, -0.4682597 , -0.45622464,\n",
       "        -0.51977724, -0.47698398, -0.48919697, -0.51596717, -0.53669068,\n",
       "        -0.56261072, -0.63734945, -0.63388464, -0.58282026, -0.47625268,\n",
       "        -0.46182226, -0.47017456, -0.49549744, -0.51558717, -0.4836128 ,\n",
       "        -0.51100501, -0.51198406, -0.55243953, -0.61008728, -0.56521567,\n",
       "        -0.58961211, -0.45531537, -0.71352138, -0.45733709, -0.44915171,\n",
       "        -0.69561112, -0.4518461 , -0.44634587, -0.71064862, -0.44667524,\n",
       "        -0.44480137, -0.69703012, -0.44534529, -0.45235298, -0.72983601,\n",
       "        -0.45563793, -0.4502355 , -0.70459659, -0.44756084, -0.44459754,\n",
       "        -0.70944162, -0.44654631, -0.44509467, -0.67696133, -0.44526336,\n",
       "        -0.4588428 , -0.73941493, -0.45835781, -0.44892758, -0.66439073,\n",
       "        -0.4483436 , -0.44856106, -0.72102591, -0.44872558, -0.4449589 ,\n",
       "        -0.65252202, -0.44458441, -0.45355735, -0.70429746, -0.4523999 ,\n",
       "        -0.44851749, -0.75889849, -0.45047251, -0.44781003, -0.68865792,\n",
       "        -0.44849067, -0.44662465, -0.66023757, -0.44563183, -0.46117045,\n",
       "        -0.71623181, -0.45127603, -0.44714468, -0.80379394, -0.45423301,\n",
       "        -0.45106096, -0.6824758 , -0.44922318, -0.4445969 , -0.61732426,\n",
       "        -0.44726296, -0.46079171, -0.84236624, -0.45733231, -0.44850491,\n",
       "        -0.71026825, -0.45208138, -0.45016351, -0.63420553, -0.44133975,\n",
       "        -0.44380753, -0.66162496, -0.44419457, -0.45823165, -0.74443346,\n",
       "        -0.46247249, -0.45064089, -0.73873109, -0.44973163, -0.44959848,\n",
       "        -0.51994607, -0.44981542, -0.44853422, -0.64030032, -0.44767833,\n",
       "        -0.46122385, -0.70087564, -0.46682622, -0.45113147, -0.6827482 ,\n",
       "        -0.44882676, -0.44374842, -0.60450259, -0.44485477, -0.44970192,\n",
       "        -0.73759084, -0.4440468 , -0.44630281, -0.4491677 , -0.4476613 ,\n",
       "        -0.44703056, -0.44716296, -0.44501485, -0.44583537, -0.44377445,\n",
       "        -0.44485961, -0.4423364 , -0.4442318 , -0.44446152, -0.44871899,\n",
       "        -0.44864356, -0.4458616 , -0.44502903, -0.44457531, -0.44683257,\n",
       "        -0.44466073, -0.44504904, -0.44455011, -0.444732  , -0.44444156,\n",
       "        -0.4460806 , -0.44895137, -0.44990863, -0.44939752, -0.44463061,\n",
       "        -0.4471843 , -0.44472874, -0.44444195, -0.44577443, -0.44529804,\n",
       "        -0.44380729, -0.44364452, -0.44461777, -0.44828033, -0.44925072,\n",
       "        -0.44985647, -0.44622675, -0.44663826, -0.44458425, -0.44522315,\n",
       "        -0.44533215, -0.444721  , -0.44285954, -0.44361357, -0.44301448,\n",
       "        -0.44569324, -0.44596404, -0.44386884, -0.44755067, -0.44994896,\n",
       "        -0.43967673, -0.4572679 , -0.44636737, -0.45602551, -0.45711076,\n",
       "        -0.45847794, -0.45341643, -0.44612969, -0.44265828, -0.44471491,\n",
       "        -0.44549471, -0.44472884, -0.44591134, -0.45001691, -0.46051494,\n",
       "        -0.45390624, -0.45161555, -0.45037401, -0.45188695, -0.44655697,\n",
       "        -0.44378792, -0.44758577, -0.44884892, -0.44878407, -0.44641294,\n",
       "        -0.44734091, -0.44895504, -0.45182993, -0.45022864, -0.45072733,\n",
       "        -0.45240957, -0.44595902, -0.44516499, -0.4425758 , -0.44226323,\n",
       "        -0.44430769, -0.44018299, -0.44344948, -0.4514075 , -0.4444606 ,\n",
       "        -0.45253403, -0.45097091, -0.45031777]),\n",
       " 'split3_test_neg_log_loss': array([-0.48217206, -0.48296382, -0.48235855, -0.56519545, -0.52181582,\n",
       "        -0.53473227, -0.57126602, -0.61013021, -0.56170305, -0.66415369,\n",
       "        -0.66551574, -0.60939614, -0.49557174, -0.47675297, -0.47630425,\n",
       "        -0.53143831, -0.55596618, -0.51840498, -0.58721802, -0.54744731,\n",
       "        -0.57765328, -0.68194074, -0.60897726, -0.58101207, -0.48582943,\n",
       "        -0.48209567, -0.4909591 , -0.51884557, -0.52581112, -0.52655271,\n",
       "        -0.54363357, -0.54699062, -0.54711257, -0.65398306, -0.64482232,\n",
       "        -0.66304185, -0.47739942, -0.47844445, -0.48465443, -0.52984327,\n",
       "        -0.50708444, -0.50113079, -0.55861725, -0.55290006, -0.55727466,\n",
       "        -0.59566736, -0.57576045, -0.60652246, -0.48082953, -0.48732937,\n",
       "        -0.48007265, -0.55795995, -0.61387234, -0.57199108, -0.61335499,\n",
       "        -0.6306096 , -0.58933021, -0.64260609, -0.62756073, -0.64823292,\n",
       "        -0.48376544, -0.48052267, -0.48883002, -0.57292181, -0.50729973,\n",
       "        -0.53363112, -0.6164871 , -0.63260253, -0.63770817, -0.70661069,\n",
       "        -0.64878471, -0.72739314, -0.47486639, -0.4874757 , -0.47985727,\n",
       "        -0.56149701, -0.52050266, -0.54490822, -0.54777584, -0.60281116,\n",
       "        -0.59524974, -0.67941885, -0.63550822, -0.65688508, -0.48461715,\n",
       "        -0.49192328, -0.48472003, -0.53439849, -0.52229156, -0.51476577,\n",
       "        -0.5611441 , -0.54430732, -0.59337916, -0.62744616, -0.67449315,\n",
       "        -0.62343403, -0.46279739, -0.71053865, -0.4670074 , -0.46171916,\n",
       "        -0.68582156, -0.46033566, -0.45698153, -0.69960037, -0.45652537,\n",
       "        -0.45661293, -0.67974335, -0.45569998, -0.46433255, -0.7421277 ,\n",
       "        -0.4614811 , -0.45730499, -0.72675394, -0.45791115, -0.45799382,\n",
       "        -0.68155054, -0.45726332, -0.45439788, -0.73463027, -0.45469492,\n",
       "        -0.46435423, -0.74677897, -0.46475963, -0.45577804, -0.82870245,\n",
       "        -0.45707658, -0.45576177, -0.69309218, -0.45583022, -0.45666588,\n",
       "        -0.68982202, -0.45568957, -0.46544827, -0.75521427, -0.46076392,\n",
       "        -0.45917737, -0.70529529, -0.4595812 , -0.45946077, -0.68571456,\n",
       "        -0.45607786, -0.45670581, -0.72001651, -0.45515356, -0.46760162,\n",
       "        -0.6216616 , -0.46655328, -0.46205191, -0.59530312, -0.46778905,\n",
       "        -0.46494984, -0.68830285, -0.46125115, -0.46489623, -0.55592233,\n",
       "        -0.46468273, -0.46485222, -0.68143693, -0.47434897, -0.46559859,\n",
       "        -0.65209085, -0.45869596, -0.4632375 , -0.60598289, -0.46107305,\n",
       "        -0.46258388, -0.53133295, -0.46415862, -0.46651332, -0.69169229,\n",
       "        -0.46056319, -0.46220892, -0.58558231, -0.46335463, -0.46074429,\n",
       "        -0.64355717, -0.46082476, -0.45973469, -0.6564698 , -0.45897575,\n",
       "        -0.47199465, -0.90643107, -0.47121838, -0.46342109, -0.63121001,\n",
       "        -0.46505227, -0.45875012, -0.60389735, -0.46624025, -0.46682918,\n",
       "        -0.58477777, -0.45827428, -0.46400604, -0.46048512, -0.461064  ,\n",
       "        -0.45926063, -0.46086281, -0.45927018, -0.45867634, -0.45887067,\n",
       "        -0.45843393, -0.4585048 , -0.45858278, -0.45846155, -0.46198102,\n",
       "        -0.46253519, -0.46124625, -0.45886377, -0.46043603, -0.46015583,\n",
       "        -0.45890582, -0.45897625, -0.45912151, -0.45898885, -0.458068  ,\n",
       "        -0.45926381, -0.46098205, -0.46197534, -0.46319106, -0.45989869,\n",
       "        -0.459499  , -0.46026654, -0.45845624, -0.45878165, -0.4585461 ,\n",
       "        -0.45835496, -0.45774701, -0.45795607, -0.46322015, -0.46093968,\n",
       "        -0.46076049, -0.45981947, -0.45902015, -0.45923792, -0.45834676,\n",
       "        -0.45818572, -0.45776817, -0.45813261, -0.45800944, -0.45799972,\n",
       "        -0.46797521, -0.46964662, -0.4653385 , -0.46309368, -0.47354647,\n",
       "        -0.47051628, -0.47484426, -0.46863201, -0.4734027 , -0.47345616,\n",
       "        -0.4744    , -0.47150086, -0.47035833, -0.46386938, -0.4669041 ,\n",
       "        -0.4824399 , -0.47197596, -0.47018351, -0.47500503, -0.47581055,\n",
       "        -0.48187409, -0.47589856, -0.47746688, -0.48048517, -0.47370441,\n",
       "        -0.46097936, -0.46516424, -0.46983341, -0.46943831, -0.47013171,\n",
       "        -0.47494967, -0.47148498, -0.47523003, -0.48055128, -0.48591884,\n",
       "        -0.4778396 , -0.46547733, -0.46409907, -0.46919045, -0.46527112,\n",
       "        -0.46776919, -0.47039805, -0.46729743, -0.47083459, -0.47027176,\n",
       "        -0.47145463, -0.47912141, -0.47098606]),\n",
       " 'split4_test_neg_log_loss': array([-0.45945593, -0.45188043, -0.46448213, -0.48958417, -0.48050946,\n",
       "        -0.48349719, -0.56489083, -0.55308447, -0.51563108, -0.60903697,\n",
       "        -0.59832321, -0.59100999, -0.44399714, -0.46233461, -0.48510407,\n",
       "        -0.48455786, -0.49847819, -0.50077685, -0.55357856, -0.52624871,\n",
       "        -0.53573382, -0.56443773, -0.54962182, -0.58783976, -0.45120498,\n",
       "        -0.46102089, -0.44897585, -0.49746326, -0.51783499, -0.48852038,\n",
       "        -0.55351462, -0.53722727, -0.50335497, -0.57288458, -0.55308658,\n",
       "        -0.56127876, -0.46053891, -0.45845645, -0.46094988, -0.47136841,\n",
       "        -0.4711223 , -0.4895027 , -0.48465831, -0.48522891, -0.50391844,\n",
       "        -0.56472202, -0.55198406, -0.5480078 , -0.45160915, -0.45758721,\n",
       "        -0.44703863, -0.49352141, -0.50767704, -0.48794485, -0.56273491,\n",
       "        -0.53740212, -0.53681509, -0.60482594, -0.73827036, -0.57715013,\n",
       "        -0.45233152, -0.45518615, -0.45187524, -0.51327755, -0.48724896,\n",
       "        -0.51338971, -0.51844162, -0.52687599, -0.54976427, -0.64461967,\n",
       "        -0.59366582, -0.68648211, -0.46251796, -0.47004728, -0.46179314,\n",
       "        -0.49058026, -0.4983953 , -0.48630209, -0.55219554, -0.52273836,\n",
       "        -0.56568237, -0.61070611, -0.5834319 , -0.6249493 , -0.45030063,\n",
       "        -0.46400466, -0.46478107, -0.4791573 , -0.48403489, -0.50264064,\n",
       "        -0.50005896, -0.52171515, -0.50419262, -0.56377595, -0.56308859,\n",
       "        -0.57871746, -0.45891469, -0.7302478 , -0.44980279, -0.44565858,\n",
       "        -0.70989228, -0.44556621, -0.44427452, -0.70852681, -0.44090454,\n",
       "        -0.4440709 , -0.66703433, -0.44139039, -0.4480792 , -0.68887402,\n",
       "        -0.45001203, -0.44612696, -0.71879313, -0.44429939, -0.44246512,\n",
       "        -0.71448485, -0.44027212, -0.44217427, -0.68190622, -0.44253338,\n",
       "        -0.44774733, -0.69860099, -0.44962233, -0.44282283, -0.7156053 ,\n",
       "        -0.44433049, -0.44242373, -0.69816658, -0.44335855, -0.44087758,\n",
       "        -0.71587788, -0.44256652, -0.45120423, -0.70275138, -0.45042428,\n",
       "        -0.44480189, -0.67108793, -0.44533006, -0.44678012, -0.67356388,\n",
       "        -0.44341997, -0.44287153, -0.7235051 , -0.44031017, -0.44908407,\n",
       "        -0.60449234, -0.44329006, -0.44492053, -0.63812953, -0.44646531,\n",
       "        -0.43877098, -0.68934029, -0.43945832, -0.43589284, -0.53567724,\n",
       "        -0.44074316, -0.4469051 , -0.61973236, -0.44992487, -0.44555953,\n",
       "        -0.60561023, -0.43797181, -0.43893034, -0.54735894, -0.4463706 ,\n",
       "        -0.43849556, -0.56699104, -0.43936957, -0.45257188, -0.82332863,\n",
       "        -0.45154346, -0.44531747, -0.66448032, -0.44028558, -0.43970341,\n",
       "        -0.60934556, -0.44624459, -0.4398827 , -0.80407858, -0.44167534,\n",
       "        -0.44847368, -0.65344421, -0.44877187, -0.442061  , -0.7094596 ,\n",
       "        -0.44818268, -0.43950671, -0.63328239, -0.44178943, -0.4448243 ,\n",
       "        -0.71069373, -0.44626702, -0.44115641, -0.44693091, -0.44114975,\n",
       "        -0.43882168, -0.44030846, -0.4390955 , -0.43822513, -0.43955737,\n",
       "        -0.43887384, -0.43816746, -0.43898245, -0.43896378, -0.4418555 ,\n",
       "        -0.44227134, -0.4397215 , -0.43830189, -0.44027476, -0.43944308,\n",
       "        -0.43881284, -0.43830678, -0.43914333, -0.43840988, -0.43783731,\n",
       "        -0.43862092, -0.44250917, -0.44213515, -0.44283035, -0.43925379,\n",
       "        -0.43846448, -0.4396842 , -0.43884801, -0.43709004, -0.43786797,\n",
       "        -0.43821957, -0.43824986, -0.43825384, -0.44239246, -0.44320413,\n",
       "        -0.44266627, -0.43985838, -0.43906844, -0.43963612, -0.43884769,\n",
       "        -0.43754802, -0.43886725, -0.43801655, -0.43788319, -0.43788993,\n",
       "        -0.4407385 , -0.44135588, -0.43965769, -0.43976161, -0.44467516,\n",
       "        -0.43644447, -0.44471681, -0.44505796, -0.44404084, -0.44295324,\n",
       "        -0.43789291, -0.4510887 , -0.44311213, -0.43895744, -0.44205274,\n",
       "        -0.44302864, -0.45061204, -0.4424483 , -0.45084316, -0.44512026,\n",
       "        -0.44372301, -0.45029047, -0.45052762, -0.44167246, -0.44184538,\n",
       "        -0.43535061, -0.44474712, -0.43936339, -0.44978019, -0.44799274,\n",
       "        -0.4412807 , -0.44316941, -0.44341307, -0.44906033, -0.44626461,\n",
       "        -0.44675138, -0.44286064, -0.43948972, -0.44019448, -0.43949718,\n",
       "        -0.43910414, -0.43600831, -0.44582786, -0.43762044, -0.4402791 ,\n",
       "        -0.44514222, -0.4412209 , -0.44164648]),\n",
       " 'mean_test_neg_log_loss': array([-0.47227902, -0.47219662, -0.47199697, -0.51901033, -0.5058287 ,\n",
       "        -0.51172869, -0.57522567, -0.5822134 , -0.56914683, -0.64295945,\n",
       "        -0.62328732, -0.61113878, -0.46959262, -0.47196022, -0.47459838,\n",
       "        -0.50645783, -0.52050912, -0.50653818, -0.5648103 , -0.54866084,\n",
       "        -0.5480242 , -0.62253722, -0.63232144, -0.59100955, -0.47626592,\n",
       "        -0.47134457, -0.46946751, -0.51445597, -0.51383529, -0.5136649 ,\n",
       "        -0.55101856, -0.56226245, -0.53310653, -0.60556203, -0.59508849,\n",
       "        -0.604857  , -0.47044805, -0.47150867, -0.47389221, -0.49580826,\n",
       "        -0.4905655 , -0.49936739, -0.52407926, -0.51978134, -0.53142091,\n",
       "        -0.56880178, -0.5624701 , -0.56284056, -0.4718104 , -0.47187944,\n",
       "        -0.47142798, -0.53133461, -0.54070764, -0.52014974, -0.58406492,\n",
       "        -0.57876048, -0.59059178, -0.65697211, -0.67506052, -0.65809836,\n",
       "        -0.47533975, -0.47433647, -0.47589499, -0.53518007, -0.5065309 ,\n",
       "        -0.52665849, -0.58971026, -0.57235122, -0.60170153, -0.69840065,\n",
       "        -0.65728791, -0.70796361, -0.46922663, -0.47758695, -0.47243433,\n",
       "        -0.52379374, -0.5195494 , -0.50936841, -0.55176761, -0.56613465,\n",
       "        -0.58266812, -0.64520985, -0.64152183, -0.61738608, -0.47620345,\n",
       "        -0.47370493, -0.47501192, -0.50270402, -0.51472738, -0.50371049,\n",
       "        -0.53831025, -0.54398842, -0.55242218, -0.60339052, -0.59667793,\n",
       "        -0.59637912, -0.46290839, -0.70726216, -0.46253823, -0.45693763,\n",
       "        -0.69459493, -0.45652069, -0.45425719, -0.7041939 , -0.45362731,\n",
       "        -0.45390538, -0.68880883, -0.45429724, -0.46020669, -0.71437674,\n",
       "        -0.46138012, -0.45676484, -0.69878607, -0.45565844, -0.45472859,\n",
       "        -0.70280574, -0.4545182 , -0.45344417, -0.70468579, -0.45316192,\n",
       "        -0.46171492, -0.72726379, -0.46205258, -0.45619323, -0.71820239,\n",
       "        -0.45492179, -0.45479684, -0.71496707, -0.45476816, -0.45364268,\n",
       "        -0.69232497, -0.45392556, -0.46196155, -0.71262844, -0.45980041,\n",
       "        -0.45628582, -0.70511634, -0.45793098, -0.45597898, -0.69051919,\n",
       "        -0.45529618, -0.45448807, -0.70021446, -0.45344185, -0.4634672 ,\n",
       "        -0.66780358, -0.46031676, -0.45702901, -0.76056367, -0.45960463,\n",
       "        -0.45585585, -0.67852515, -0.45560694, -0.45468667, -0.60690879,\n",
       "        -0.45720814, -0.46023661, -0.68677398, -0.4629832 , -0.4589054 ,\n",
       "        -0.65574362, -0.45797529, -0.45795387, -0.59307471, -0.45684267,\n",
       "        -0.4547793 , -0.62693757, -0.45598147, -0.46427258, -0.74930542,\n",
       "        -0.4632279 , -0.4581841 , -0.65288638, -0.45705558, -0.45445649,\n",
       "        -0.61668448, -0.45640049, -0.45557477, -0.7024274 , -0.45529125,\n",
       "        -0.4639014 , -0.73832329, -0.46552254, -0.458963  , -0.70611136,\n",
       "        -0.45935593, -0.454705  , -0.655512  , -0.45664213, -0.4592161 ,\n",
       "        -0.67517954, -0.45442983, -0.45515954, -0.45661154, -0.45486871,\n",
       "        -0.45322401, -0.45379832, -0.45285403, -0.4525256 , -0.45247665,\n",
       "        -0.45264303, -0.45187158, -0.45263096, -0.45230898, -0.4560935 ,\n",
       "        -0.45572309, -0.45411088, -0.45262473, -0.45368896, -0.45395932,\n",
       "        -0.45249738, -0.45231494, -0.45253747, -0.45244637, -0.45186169,\n",
       "        -0.45301471, -0.45593529, -0.45670673, -0.4560432 , -0.45298026,\n",
       "        -0.4532683 , -0.4532797 , -0.45247821, -0.45225682, -0.45250177,\n",
       "        -0.45192415, -0.45159886, -0.45210571, -0.45627748, -0.45561845,\n",
       "        -0.45552671, -0.45358762, -0.45339284, -0.45295248, -0.45260086,\n",
       "        -0.45227863, -0.45218951, -0.45153123, -0.45183284, -0.45154455,\n",
       "        -0.45519027, -0.45763207, -0.45466604, -0.45914595, -0.46214954,\n",
       "        -0.45708722, -0.46278676, -0.45872383, -0.46325752, -0.46504337,\n",
       "        -0.462496  , -0.46683454, -0.45734916, -0.45586457, -0.45586769,\n",
       "        -0.46162806, -0.46067751, -0.45933289, -0.46387436, -0.46377439,\n",
       "        -0.46249649, -0.46466845, -0.46474505, -0.46220964, -0.45878417,\n",
       "        -0.45326962, -0.45884866, -0.4590164 , -0.45883956, -0.46066791,\n",
       "        -0.45958962, -0.45936917, -0.46213135, -0.46532861, -0.46677951,\n",
       "        -0.46363246, -0.4573799 , -0.45517119, -0.45500154, -0.45579368,\n",
       "        -0.4557793 , -0.45517302, -0.4577097 , -0.45714063, -0.45758754,\n",
       "        -0.46006528, -0.46011387, -0.45995355]),\n",
       " 'std_test_neg_log_loss': array([0.01039287, 0.01397236, 0.01111158, 0.02507037, 0.02130296,\n",
       "        0.03114406, 0.00641264, 0.02487183, 0.04034475, 0.02461132,\n",
       "        0.03827791, 0.01514475, 0.01810428, 0.00997375, 0.00997536,\n",
       "        0.01596581, 0.02502494, 0.02121031, 0.02024313, 0.01490569,\n",
       "        0.01736545, 0.04274633, 0.04765015, 0.0120054 , 0.01614351,\n",
       "        0.01333536, 0.01621056, 0.01566594, 0.01231782, 0.01521103,\n",
       "        0.00958028, 0.02087593, 0.01616594, 0.03862673, 0.04112679,\n",
       "        0.04209961, 0.00850487, 0.01289347, 0.01224937, 0.0195016 ,\n",
       "        0.01491807, 0.00802614, 0.02468341, 0.02163981, 0.01771113,\n",
       "        0.02122259, 0.0111117 , 0.02269426, 0.01301267, 0.01222588,\n",
       "        0.01897785, 0.02667903, 0.0395286 , 0.0283765 , 0.03694722,\n",
       "        0.04595149, 0.03047347, 0.05308821, 0.05564322, 0.04950663,\n",
       "        0.0124221 , 0.01395855, 0.01390826, 0.02109824, 0.02117343,\n",
       "        0.01523694, 0.04081271, 0.04547751, 0.03466374, 0.02884877,\n",
       "        0.03654495, 0.03567175, 0.00948069, 0.00853627, 0.01296427,\n",
       "        0.02438179, 0.02960024, 0.02181299, 0.02068648, 0.03974992,\n",
       "        0.01525789, 0.02297212, 0.04064023, 0.03418072, 0.01548362,\n",
       "        0.01202292, 0.00840476, 0.01942096, 0.01601071, 0.01293765,\n",
       "        0.02992507, 0.02490053, 0.02967895, 0.02136443, 0.04094281,\n",
       "        0.01483705, 0.00568325, 0.01744118, 0.00806888, 0.00829908,\n",
       "        0.00832126, 0.00722837, 0.00833885, 0.00632804, 0.00873568,\n",
       "        0.00854465, 0.01594136, 0.00998024, 0.0087445 , 0.03100434,\n",
       "        0.00797452, 0.00787444, 0.02753883, 0.00860475, 0.00961676,\n",
       "        0.01311238, 0.00983668, 0.00900161, 0.02231454, 0.00834696,\n",
       "        0.00796805, 0.0184054 , 0.00734285, 0.00956792, 0.0576475 ,\n",
       "        0.00760566, 0.0084204 , 0.01640373, 0.00795841, 0.00949005,\n",
       "        0.02258348, 0.00895928, 0.00802519, 0.02139036, 0.00759264,\n",
       "        0.00836873, 0.02942063, 0.0089841 , 0.00747878, 0.01108209,\n",
       "        0.00843083, 0.00856112, 0.0225638 , 0.00983318, 0.00787527,\n",
       "        0.08016264, 0.01138097, 0.0101032 , 0.12379344, 0.00871489,\n",
       "        0.01041855, 0.05539589, 0.01034607, 0.01228621, 0.05996855,\n",
       "        0.0110349 , 0.00735218, 0.10354257, 0.00847201, 0.01035589,\n",
       "        0.04743509, 0.01250978, 0.01169727, 0.03077888, 0.01157568,\n",
       "        0.01179193, 0.1009798 , 0.01183849, 0.00773006, 0.04435626,\n",
       "        0.00755319, 0.0092549 , 0.05977861, 0.01070159, 0.00906661,\n",
       "        0.07771374, 0.00761582, 0.0098631 , 0.06266398, 0.00907586,\n",
       "        0.00852443, 0.08744633, 0.0094806 , 0.01083612, 0.04835147,\n",
       "        0.00914978, 0.0112605 , 0.05771393, 0.01144951, 0.01017418,\n",
       "        0.05413447, 0.00838559, 0.00978695, 0.00760194, 0.00919577,\n",
       "        0.00915596, 0.00894042, 0.00933612, 0.00945709, 0.00946273,\n",
       "        0.00941421, 0.01003897, 0.00978123, 0.00943832, 0.00993783,\n",
       "        0.00891272, 0.00993793, 0.00980359, 0.00972449, 0.00941448,\n",
       "        0.00943622, 0.00946782, 0.00943015, 0.00965552, 0.00957187,\n",
       "        0.00942015, 0.00930816, 0.00918539, 0.00864811, 0.00955751,\n",
       "        0.00940585, 0.00957657, 0.00959083, 0.00969186, 0.0096035 ,\n",
       "        0.0096006 , 0.00939315, 0.00949919, 0.00932862, 0.00832815,\n",
       "        0.00827306, 0.00922546, 0.00941903, 0.00942087, 0.00935444,\n",
       "        0.00967695, 0.009241  , 0.00970025, 0.00973135, 0.00972774,\n",
       "        0.01050154, 0.0121234 , 0.01157105, 0.01386134, 0.01266556,\n",
       "        0.01570999, 0.01179346, 0.01103365, 0.01205222, 0.01408239,\n",
       "        0.01418116, 0.01343217, 0.01091588, 0.01256429, 0.01093381,\n",
       "        0.01555035, 0.01254546, 0.01282726, 0.01230523, 0.01056444,\n",
       "        0.01362252, 0.01131909, 0.01186813, 0.01399697, 0.01335141,\n",
       "        0.01213244, 0.01068197, 0.01362509, 0.01002867, 0.01219112,\n",
       "        0.01358821, 0.01205418, 0.01272627, 0.01315577, 0.01550258,\n",
       "        0.01240955, 0.01112296, 0.0109544 , 0.01286954, 0.0129141 ,\n",
       "        0.01238838, 0.01480826, 0.01119904, 0.01203818, 0.01271938,\n",
       "        0.00978876, 0.01373884, 0.01279462]),\n",
       " 'rank_test_neg_log_loss': array([173, 172, 171, 200, 190, 195, 228, 230, 226, 253, 249, 245, 163,\n",
       "        170, 178, 191, 204, 193, 223, 216, 215, 248, 251, 235, 183, 165,\n",
       "        162, 198, 197, 196, 217, 220, 210, 243, 237, 242, 164, 167, 176,\n",
       "        186, 185, 187, 206, 202, 209, 225, 221, 222, 168, 169, 166, 208,\n",
       "        213, 203, 232, 229, 234, 258, 262, 260, 180, 177, 181, 211, 192,\n",
       "        207, 233, 227, 240, 270, 259, 280, 161, 184, 174, 205, 201, 194,\n",
       "        218, 224, 231, 254, 252, 247, 182, 175, 179, 188, 199, 189, 212,\n",
       "        214, 219, 241, 239, 238, 144, 279, 142,  94, 269,  88,  46, 275,\n",
       "         38,  42, 266,  47, 127, 282, 132,  92, 271,  72,  55, 274,  51,\n",
       "         36, 276,  29, 134, 285, 136,  84, 284,  60,  58, 283,  56,  39,\n",
       "        268,  43, 135, 281, 123,  86, 277, 105,  80, 267,  67,  50, 272,\n",
       "         35, 148, 261, 129,  95, 288, 122,  76, 264,  70,  53, 244,  99,\n",
       "        128, 265, 145, 113, 257, 107, 106, 236,  93,  57, 250,  81, 153,\n",
       "        287, 146, 108, 255,  96,  49, 246,  87,  69, 273,  66, 152, 286,\n",
       "        158, 114, 278, 119,  54, 256,  90, 117, 263,  48,  62,  89,  59,\n",
       "         30,  41,  25,  19,  15,  24,   6,  23,  12,  83,  73,  45,  22,\n",
       "         40,  44,  17,  13,  20,  14,   5,  28,  79,  91,  82,  27,  31,\n",
       "         33,  16,  10,  18,   7,   3,   8,  85,  71,  68,  37,  34,  26,\n",
       "         21,  11,   9,   1,   4,   2,  65, 103,  52, 116, 138,  97, 143,\n",
       "        109, 147, 156, 140, 160, 100,  77,  78, 133, 131, 118, 151, 150,\n",
       "        141, 154, 155, 139, 110,  32, 112, 115, 111, 130, 121, 120, 137,\n",
       "        157, 159, 149, 101,  63,  61,  75,  74,  64, 104,  98, 102, 125,\n",
       "        126, 124])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRIAL_3_MLP.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL THREE NEG LOG LOSS RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.1,\n",
       " 'classifier__hidden_layer_sizes': (20,),\n",
       " 'classifier__learning_rate': 'constant',\n",
       " 'classifier__solver': 'adam'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL THREE NEG LOG LOSS RESULTS ---------------------------------------------\")\n",
    "TRIAL_3_MLP.cv_results_['rank_test_neg_log_loss']\n",
    "TRIAL_3_MLP.cv_results_['params'][ np.argmin(TRIAL_3_MLP.cv_results_['rank_test_neg_log_loss']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL THREE F1 RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'tanh',\n",
       " 'classifier__alpha': 0.01,\n",
       " 'classifier__hidden_layer_sizes': (15,),\n",
       " 'classifier__learning_rate': 'constant',\n",
       " 'classifier__solver': 'sgd'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL THREE F1 RESULTS ---------------------------------------------\")\n",
    "TRIAL_3_MLP.cv_results_['rank_test_f1_micro']\n",
    "TRIAL_3_MLP.cv_results_['params'][ np.argmin(TRIAL_3_MLP.cv_results_['rank_test_f1_micro']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL THREE ROC AUC RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.01,\n",
       " 'classifier__hidden_layer_sizes': (10,),\n",
       " 'classifier__learning_rate': 'adaptive',\n",
       " 'classifier__solver': 'sgd'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL THREE ROC AUC RESULTS ---------------------------------------------\")\n",
    "TRIAL_3_MLP.cv_results_['rank_test_roc_auc_ovo']\n",
    "TRIAL_3_MLP.cv_results_['params'][ np.argmin(TRIAL_3_MLP.cv_results_['rank_test_roc_auc_ovo']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL THREE JACCARD SCORE RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'tanh',\n",
       " 'classifier__alpha': 0.01,\n",
       " 'classifier__hidden_layer_sizes': (15,),\n",
       " 'classifier__learning_rate': 'constant',\n",
       " 'classifier__solver': 'sgd'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL THREE JACCARD SCORE RESULTS ---------------------------------------------\")\n",
    "TRIAL_3_MLP.cv_results_['rank_test_jaccard']\n",
    "TRIAL_3_MLP.cv_results_['params'][ np.argmin(TRIAL_3_MLP.cv_results_['rank_test_jaccard']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 3 Multilayered Perceptrons using best NEG LOG LOSS hyperperameters :0.7760014616544101\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 3 Multilayered Perceptrons using best F1 hyperperameters :0.5058237792901841\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 3 Multilayered Perceptrons using best ROC_AUC hyperperameters :0.5059608093911296\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 3 Multilayered Perceptrons using best JACCARD hyperperameters :0.5058922943406569\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best NEG LOG LOSS results to train on test set, \n",
    "bestMPLT3_1 = MLPClassifier(activation = 'logistic', alpha = .01, hidden_layer_sizes = (20,), \n",
    "                          learning_rate = 'constant',solver = 'adam')\n",
    "bestMPLT3_1.fit(X_train,y_train)\n",
    "y_pred3_1 = bestMPLT3_1.predict(X_test)\n",
    "print('Accuracy of Trial 3 Multilayered Perceptrons using best NEG LOG LOSS hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred3_1)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best F1 results to train on test set, \n",
    "bestMPLT3_2 = MLPClassifier(activation = 'tanh', alpha = 0.01, hidden_layer_sizes = (15,), \n",
    "                          learning_rate = 'constant',solver = 'sgd')\n",
    "bestMPLT3_2.fit(X_train,y_train)\n",
    "y_pred3_2 = bestMPLT3_2.predict(X_test)\n",
    "print('Accuracy of Trial 3 Multilayered Perceptrons using best F1 hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred3_2)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best ROC_AUC results to train on test set,\n",
    "bestMPLT3_3 = MLPClassifier(activation = 'logistic', alpha = 0.01, hidden_layer_sizes = (10,), \n",
    "                          learning_rate = 'adaptive',solver = 'sgd')\n",
    "bestMPLT3_3.fit(X_train,y_train)\n",
    "y_pred3_3 = bestMPLT3_3.predict(X_test)\n",
    "print('Accuracy of Trial 3 Multilayered Perceptrons using best ROC_AUC hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred3_3)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best JACCARD results to train on test set, \n",
    "bestMPLT3_4 = MLPClassifier(activation = 'tanh', alpha = 0.01, hidden_layer_sizes = (15,), \n",
    "                          learning_rate = 'constant',solver = 'sgd')\n",
    "bestMPLT3_4.fit(X_train,y_train)\n",
    "y_pred3_4 = bestMPLT3_4.predict(X_test)\n",
    "print('Accuracy of Trial 3 Multilayered Perceptrons using best JACCARD hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred3_4)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTI LAYERED PERCEPTRON TRIAL FOUR ON LEAGUE OF LEGENDS DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:    9.0s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    9.8s\n",
      "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed:   10.9s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=-1)]: Done 189 tasks      | elapsed:   14.4s\n",
      "[Parallel(n_jobs=-1)]: Done 210 tasks      | elapsed:   15.5s\n",
      "[Parallel(n_jobs=-1)]: Done 233 tasks      | elapsed:   18.3s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:   19.6s\n",
      "[Parallel(n_jobs=-1)]: Done 281 tasks      | elapsed:   21.9s\n",
      "[Parallel(n_jobs=-1)]: Done 306 tasks      | elapsed:   24.2s\n",
      "[Parallel(n_jobs=-1)]: Done 333 tasks      | elapsed:   26.7s\n",
      "[Parallel(n_jobs=-1)]: Done 360 tasks      | elapsed:   29.3s\n",
      "[Parallel(n_jobs=-1)]: Done 389 tasks      | elapsed:   30.9s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:   34.5s\n",
      "[Parallel(n_jobs=-1)]: Done 449 tasks      | elapsed:   36.1s\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:   39.6s\n",
      "[Parallel(n_jobs=-1)]: Done 513 tasks      | elapsed:   43.1s\n",
      "[Parallel(n_jobs=-1)]: Done 546 tasks      | elapsed:   46.4s\n",
      "[Parallel(n_jobs=-1)]: Done 581 tasks      | elapsed:   49.6s\n",
      "[Parallel(n_jobs=-1)]: Done 616 tasks      | elapsed:   52.9s\n",
      "[Parallel(n_jobs=-1)]: Done 653 tasks      | elapsed:   56.8s\n",
      "[Parallel(n_jobs=-1)]: Done 690 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 729 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 809 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 850 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 893 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 936 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 981 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1026 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1073 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1120 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1169 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1269 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1320 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1373 tasks      | elapsed:  2.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FOUR RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1440 out of 1440 | elapsed:  2.5min finished\n"
     ]
    }
   ],
   "source": [
    "ITERATIONS = 1\n",
    "for i in range(1):\n",
    "    #For reach \"trial\", randomly select 5000 data points\n",
    "    random5000DataPoints = leagueData.sample(n = 5000, replace = True)\n",
    "    #From that 5000, create x set and y set\n",
    "    xSet = random5000DataPoints.iloc[:,1:17].values\n",
    "    ySet = random5000DataPoints.iloc[:,0].values\n",
    "    # Run GridSearch to find best hyperparameters per 5000 data points \n",
    "    \n",
    "    pipe = Pipeline([('std', StandardScaler()),('classifier', MLPClassifier(max_iter = 250))])\n",
    "\n",
    "    search_space = [\n",
    "                {\n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                {\n",
    "                 \n",
    "                  \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['sgd'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                \n",
    "                {\n",
    "                \n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['adam'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                ]\n",
    "    \n",
    "    runHyperParamSearch = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=5), \n",
    "                          scoring=['recall_micro', 'f1_micro','precision_micro','roc_auc_ovo','jaccard','neg_log_loss'], \n",
    "                          refit=False, verbose=10, n_jobs = -1)\n",
    "    TRIAL_4_MLP = runHyperParamSearch.fit(xSet, ySet)\n",
    "    print(\"---------------------------------TRIAL FOUR RESULTS ---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.62799587, 0.61683264, 0.63634758, 0.90698051, 0.88756371,\n",
       "        0.90858226, 1.34675922, 1.37808514, 1.40180454, 1.78203311,\n",
       "        1.77812929, 1.80124884, 0.81620131, 0.68708887, 0.65676303,\n",
       "        0.88255887, 0.92499704, 0.93230467, 1.3120307 , 1.40230598,\n",
       "        1.62629752, 1.75380692, 1.71257319, 1.60918479, 0.72872791,\n",
       "        0.67948246, 0.61532841, 0.88185787, 0.8971714 , 0.95441899,\n",
       "        1.36827693, 1.36387219, 1.43923874, 1.76021276, 1.73879299,\n",
       "        1.72628393, 0.71741295, 0.68148603, 0.66757493, 0.88716474,\n",
       "        0.90688095, 0.87064848, 1.3368525 , 1.43713794, 1.52751279,\n",
       "        1.7641171 , 1.76992273, 1.73318906, 0.85483298, 0.78707576,\n",
       "        0.74373765, 0.95962596, 1.05210485, 1.10235167, 1.58926649,\n",
       "        1.61208754, 1.67213593, 2.00782619, 2.13663664, 2.13033113,\n",
       "        0.85563731, 0.78747745, 0.71441488, 1.05437956, 1.01867733,\n",
       "        1.10985484, 1.57075167, 1.61228614, 1.5811594 , 2.03735108,\n",
       "        2.10831394, 2.04015441, 0.89937406, 0.74383774, 0.72172003,\n",
       "        1.03659167, 1.03458929, 1.04930158, 1.52731471, 1.59036741,\n",
       "        1.6683341 , 2.17086649, 2.16496305, 1.99221215, 0.84472656,\n",
       "        0.75695124, 0.63874974, 1.00976896, 1.00886812, 1.0530045 ,\n",
       "        1.56814847, 1.66683488, 1.65522361, 1.96889086, 1.91274381,\n",
       "        1.79974566, 2.29056945, 0.17885222, 2.46962409, 1.72638464,\n",
       "        0.17695265, 2.53998461, 1.87881589, 0.20627723, 2.59363003,\n",
       "        1.93786674, 0.29315262, 2.60153747, 2.13343506, 0.29925709,\n",
       "        2.46582026, 1.86080003, 0.16253958, 2.34541731, 1.81255884,\n",
       "        0.25091538, 2.42788773, 1.82787242, 0.205477  , 2.69872007,\n",
       "        2.03164759, 0.1542325 , 2.48083363, 1.86200166, 0.18295732,\n",
       "        2.51886644, 1.93556485, 0.25311761, 2.52487178, 1.75671067,\n",
       "        0.20047274, 2.56250381, 1.92625594, 0.19867115, 2.50535402,\n",
       "        1.97099476, 0.15853615, 2.42648726, 1.91034317, 0.247613  ,\n",
       "        2.533779  , 1.97900248, 0.22309194, 2.49094224, 1.07442389,\n",
       "        0.70300436, 1.82857242, 1.07662592, 1.32654057, 1.68074517,\n",
       "        0.96803241, 1.54933243, 1.5818604 , 1.02908525, 1.06331458,\n",
       "        1.59166861, 1.24066682, 0.84232483, 1.80445085, 1.17501054,\n",
       "        0.88556204, 1.67744236, 1.04850216, 1.79324217, 1.64731679,\n",
       "        0.93890758, 1.86170073, 1.78873858, 1.18601956, 0.92369423,\n",
       "        1.76161494, 0.98034282, 0.83711987, 1.78563528, 1.01767473,\n",
       "        1.64011011, 1.73879528, 1.08623447, 1.86520414, 1.73409171,\n",
       "        1.61909251, 0.91768951, 1.641712  , 1.14438457, 1.07432389,\n",
       "        1.82096601, 0.95842471, 1.65862632, 1.53532033, 0.93560448,\n",
       "        1.4613564 , 1.7051661 , 2.01793532, 1.98220453, 1.91494699,\n",
       "        1.51380186, 1.34375587, 1.36497378, 1.31342955, 1.27930002,\n",
       "        1.15028987, 1.03529029, 1.2149446 , 1.18381863, 2.06587625,\n",
       "        2.09770374, 1.71627607, 1.31002684, 1.23015857, 1.30141912,\n",
       "        1.1868206 , 1.18051515, 1.27599692, 1.00956841, 1.18341765,\n",
       "        1.30552278, 1.6842485 , 1.84638762, 1.69876094, 1.38088779,\n",
       "        1.23946567, 1.38529148, 1.08933682, 1.50689635, 1.14358406,\n",
       "        1.19182477, 1.17581134, 1.02147775, 1.67674193, 1.83658018,\n",
       "        1.38889441, 1.41511707, 1.24587164, 1.29391294, 1.16750383,\n",
       "        1.22775583, 1.26748972, 1.19662905, 1.20843959, 1.22555399,\n",
       "        2.07798676, 1.56934948, 1.85639648, 2.92571692, 2.92911954,\n",
       "        2.9399282 , 3.18013463, 3.19174461, 3.20705805, 3.50191164,\n",
       "        3.48229489, 3.4054286 , 1.83697972, 1.88562174, 1.88882437,\n",
       "        2.40336637, 2.89248738, 2.56340451, 3.31404986, 3.3119483 ,\n",
       "        3.34397559, 3.48309526, 3.54264636, 3.56086183, 2.09980583,\n",
       "        1.79214134, 1.82957335, 2.77558694, 2.50635562, 2.58752522,\n",
       "        3.29427691, 3.25329804, 3.30414143, 3.53383884, 3.48389611,\n",
       "        3.43775601, 1.53261781, 1.71467495, 1.97439795, 2.27465649,\n",
       "        2.15825596, 2.19448738, 3.26610899, 3.10687256, 3.1182817 ,\n",
       "        3.56756778, 3.40262542, 3.02840366]),\n",
       " 'std_fit_time': array([0.04771538, 0.04337482, 0.04176932, 0.03824729, 0.01588784,\n",
       "        0.01617095, 0.05690293, 0.0347719 , 0.07272415, 0.06131994,\n",
       "        0.14848902, 0.0817245 , 0.09125656, 0.03701502, 0.01702859,\n",
       "        0.02660592, 0.05642661, 0.03614571, 0.04074358, 0.07987651,\n",
       "        0.07259748, 0.15214218, 0.05174863, 0.07349431, 0.05686723,\n",
       "        0.02477385, 0.07407796, 0.02178489, 0.03253768, 0.03431363,\n",
       "        0.04257912, 0.03950755, 0.06052991, 0.03662344, 0.06464111,\n",
       "        0.03578764, 0.08424315, 0.04815461, 0.05827259, 0.07101154,\n",
       "        0.05974345, 0.10755974, 0.02464924, 0.07959809, 0.12367981,\n",
       "        0.07151437, 0.01676914, 0.03949314, 0.06652949, 0.03080044,\n",
       "        0.02307216, 0.03769643, 0.02750776, 0.05970058, 0.09876028,\n",
       "        0.05873507, 0.02815271, 0.11080233, 0.08804671, 0.23122941,\n",
       "        0.12464476, 0.05010288, 0.0421615 , 0.0340632 , 0.05117514,\n",
       "        0.06658613, 0.03681926, 0.03496784, 0.03115554, 0.13968783,\n",
       "        0.15740402, 0.18867652, 0.08695911, 0.04303051, 0.02672707,\n",
       "        0.02578411, 0.02168919, 0.03105509, 0.02915267, 0.03591425,\n",
       "        0.06705802, 0.11604842, 0.15273587, 0.1197958 , 0.05903186,\n",
       "        0.0618248 , 0.12357117, 0.09694521, 0.05770664, 0.02348605,\n",
       "        0.13792463, 0.06147096, 0.08639779, 0.05543122, 0.04317881,\n",
       "        0.05306854, 0.25125063, 0.01918318, 0.10716596, 0.02024459,\n",
       "        0.04380255, 0.10425116, 0.11075666, 0.04463144, 0.15612164,\n",
       "        0.1789343 , 0.18776808, 0.06365798, 0.15107328, 0.22431723,\n",
       "        0.0989214 , 0.168633  , 0.01398143, 0.17993081, 0.07494868,\n",
       "        0.07362095, 0.09380299, 0.12443361, 0.03467866, 0.13188727,\n",
       "        0.14729402, 0.00796556, 0.09764686, 0.1261886 , 0.01441886,\n",
       "        0.07527928, 0.16959117, 0.13670851, 0.0707811 , 0.09397046,\n",
       "        0.0210461 , 0.09870044, 0.20984046, 0.08294872, 0.08367435,\n",
       "        0.16494859, 0.01306662, 0.18296332, 0.12679746, 0.0990992 ,\n",
       "        0.04619546, 0.13086897, 0.04346907, 0.11513554, 0.19738483,\n",
       "        0.29628207, 0.28631457, 0.14428918, 0.57520138, 0.1675396 ,\n",
       "        0.11534088, 0.92094847, 0.2001858 , 0.06502709, 0.41542117,\n",
       "        0.09930086, 0.22521379, 0.30170371, 0.39418891, 0.15259527,\n",
       "        0.36659039, 0.12873268, 0.20744713, 0.89295232, 0.21992573,\n",
       "        0.13272318, 1.11877699, 0.20339949, 0.29058865, 0.62591007,\n",
       "        0.06885768, 0.15217114, 0.55466781, 0.25682004, 0.13295525,\n",
       "        0.85955017, 0.09587135, 0.25667814, 1.12478701, 0.10373796,\n",
       "        0.26118454, 0.75590603, 0.32841458, 0.23256047, 0.8086378 ,\n",
       "        0.27007302, 0.2127503 , 0.65910919, 0.11060535, 0.1259631 ,\n",
       "        1.04053208, 0.25959127, 0.12607721, 0.52526191, 0.20575798,\n",
       "        0.24768908, 0.24656793, 0.26343401, 0.22983073, 0.29297464,\n",
       "        0.29431134, 0.16261864, 0.30946557, 0.21782076, 0.23121787,\n",
       "        0.20710602, 0.12041254, 0.07401343, 0.32389844, 0.34925348,\n",
       "        0.30002019, 0.10939667, 0.35601611, 0.09992132, 0.43859418,\n",
       "        0.17964195, 0.21569013, 0.19836496, 0.4079944 , 0.33044219,\n",
       "        0.21207193, 0.34380869, 0.20242288, 0.23362647, 0.18951446,\n",
       "        0.34431194, 0.22147414, 0.14305356, 0.15970588, 0.42051766,\n",
       "        0.19998847, 0.07611733, 0.10720832, 0.16096377, 0.12632241,\n",
       "        0.16456151, 0.18916594, 0.15224114, 0.17622287, 0.13967468,\n",
       "        0.37819145, 0.12373938, 0.2858982 , 0.05022095, 0.11062536,\n",
       "        0.03078241, 0.06557413, 0.08749779, 0.05678725, 0.068686  ,\n",
       "        0.06948534, 0.01827748, 0.519176  , 0.24187582, 0.16152436,\n",
       "        0.45013537, 0.14971596, 0.52378371, 0.0730941 , 0.0504661 ,\n",
       "        0.04344853, 0.04317521, 0.06008236, 0.15091449, 0.67793061,\n",
       "        0.20585309, 0.28654373, 0.34807982, 0.67632165, 0.43833173,\n",
       "        0.09344276, 0.06156116, 0.08231329, 0.10486067, 0.06168358,\n",
       "        0.0726339 , 0.32975091, 0.18158975, 0.11231723, 0.38959087,\n",
       "        0.70933509, 0.58618115, 0.05553552, 0.4273112 , 0.51453476,\n",
       "        0.12378129, 0.11349182, 0.1282406 ]),\n",
       " 'mean_score_time': array([0.01075492, 0.01080818, 0.01211009, 0.01151009, 0.01151075,\n",
       "        0.01251187, 0.01611304, 0.0139122 , 0.01411481, 0.01951432,\n",
       "        0.01941757, 0.01301103, 0.01331124, 0.01281109, 0.01151032,\n",
       "        0.01251121, 0.01461172, 0.01391249, 0.01521111, 0.01711564,\n",
       "        0.01581249, 0.01581454, 0.01331115, 0.01651464, 0.01130795,\n",
       "        0.01471272, 0.01211081, 0.01251125, 0.01231098, 0.01911664,\n",
       "        0.01591511, 0.01401196, 0.01661363, 0.01481342, 0.01481347,\n",
       "        0.01391206, 0.013311  , 0.01120834, 0.01491241, 0.0123086 ,\n",
       "        0.01180954, 0.01301064, 0.01821628, 0.01741381, 0.01791816,\n",
       "        0.02402081, 0.01941638, 0.01941619, 0.01261182, 0.01060872,\n",
       "        0.01151004, 0.01551318, 0.01421232, 0.01561022, 0.01421442,\n",
       "        0.01811581, 0.01471248, 0.01301093, 0.01531324, 0.01421151,\n",
       "        0.0187149 , 0.01161046, 0.01281109, 0.01181016, 0.01581264,\n",
       "        0.01551361, 0.0143116 , 0.01341386, 0.01891556, 0.01831684,\n",
       "        0.01661396, 0.01521339, 0.01260986, 0.01311173, 0.01391187,\n",
       "        0.01251106, 0.01341138, 0.01491303, 0.01531367, 0.01541486,\n",
       "        0.01871576, 0.01461315, 0.01711445, 0.01311269, 0.01351271,\n",
       "        0.01131086, 0.01050739, 0.01160922, 0.0130125 , 0.01761508,\n",
       "        0.01711621, 0.01801615, 0.01711516, 0.01431265, 0.01391201,\n",
       "        0.01080956, 0.01020923, 0.00950804, 0.0098084 , 0.00960832,\n",
       "        0.01070895, 0.01030898, 0.0096087 , 0.01030874, 0.01020889,\n",
       "        0.0111093 , 0.01120949, 0.01000872, 0.00970788, 0.01080933,\n",
       "        0.00970864, 0.00970879, 0.00960817, 0.01010909, 0.00960889,\n",
       "        0.01050897, 0.00970855, 0.01030855, 0.01000843, 0.01181049,\n",
       "        0.01050911, 0.00950832, 0.00930796, 0.009308  , 0.01040878,\n",
       "        0.00960798, 0.00990844, 0.00960817, 0.00980825, 0.00990896,\n",
       "        0.01000857, 0.01020851, 0.00930772, 0.01100926, 0.01110964,\n",
       "        0.01181002, 0.00960817, 0.00960832, 0.01030922, 0.01020885,\n",
       "        0.01140995, 0.01171007, 0.01080899, 0.01050849, 0.00930805,\n",
       "        0.00960884, 0.00990839, 0.00970793, 0.0101089 , 0.00990858,\n",
       "        0.01151009, 0.01030884, 0.01141005, 0.01090956, 0.01060934,\n",
       "        0.01271076, 0.00930829, 0.00940824, 0.00960827, 0.00990849,\n",
       "        0.01010828, 0.01131001, 0.01060886, 0.01020865, 0.01010895,\n",
       "        0.01080933, 0.01060915, 0.01050873, 0.01040936, 0.00940838,\n",
       "        0.00940804, 0.01080894, 0.01090941, 0.01050935, 0.01241121,\n",
       "        0.01501255, 0.01050882, 0.01120939, 0.01241026, 0.01221037,\n",
       "        0.00950818, 0.00930834, 0.01501288, 0.00950804, 0.01000857,\n",
       "        0.01351175, 0.01100931, 0.01010895, 0.010009  , 0.01040869,\n",
       "        0.01040902, 0.01110959, 0.01040878, 0.0093082 , 0.00930762,\n",
       "        0.01020875, 0.01080961, 0.00940809, 0.00990849, 0.01010861,\n",
       "        0.01150999, 0.01080928, 0.01020927, 0.01030869, 0.00890799,\n",
       "        0.00910769, 0.01170974, 0.01331129, 0.01020846, 0.00960822,\n",
       "        0.01030869, 0.0102088 , 0.01090937, 0.01000824, 0.01030869,\n",
       "        0.01231046, 0.00920806, 0.00890789, 0.00910778, 0.00990839,\n",
       "        0.00930829, 0.00950794, 0.01080947, 0.01010852, 0.00980797,\n",
       "        0.01000834, 0.01611433, 0.01241069, 0.00920801, 0.00940728,\n",
       "        0.00910807, 0.00960851, 0.00940766, 0.00960822, 0.01010904,\n",
       "        0.00980811, 0.01010876, 0.01060901, 0.0102088 , 0.0115099 ,\n",
       "        0.00940819, 0.0102088 , 0.00930843, 0.01040845, 0.00990872,\n",
       "        0.00960855, 0.01020894, 0.01120973, 0.01060948, 0.0119103 ,\n",
       "        0.01080894, 0.01080894, 0.01010914, 0.00920782, 0.01030927,\n",
       "        0.00940847, 0.0106092 , 0.00990863, 0.01050949, 0.01050897,\n",
       "        0.01090956, 0.01120987, 0.01081004, 0.01100974, 0.010109  ,\n",
       "        0.01050911, 0.00940809, 0.01070957, 0.00980825, 0.01000857,\n",
       "        0.0105649 , 0.01080928, 0.01100969, 0.01091018, 0.01080976,\n",
       "        0.01120973, 0.00940804, 0.00900779, 0.01110988, 0.01020865,\n",
       "        0.01291137, 0.0098084 , 0.01070924, 0.01130891, 0.01090951,\n",
       "        0.00890851, 0.00730653, 0.00610528]),\n",
       " 'std_score_time': array([2.00435550e-03, 1.46967204e-03, 2.37788884e-03, 1.55068247e-03,\n",
       "        1.34308966e-03, 5.47728640e-04, 4.98760941e-03, 2.37669817e-03,\n",
       "        1.68980031e-03, 5.59985179e-03, 8.49947919e-03, 1.34404967e-03,\n",
       "        1.16591130e-03, 1.50501010e-03, 7.08606673e-04, 2.79461812e-03,\n",
       "        4.21377034e-03, 1.42827577e-03, 1.20988746e-03, 4.71036265e-03,\n",
       "        1.36536971e-03, 2.29616861e-03, 1.69405664e-03, 7.01339560e-03,\n",
       "        9.27286538e-04, 4.52684270e-03, 1.46491945e-03, 1.41620171e-03,\n",
       "        1.07654906e-03, 6.58856222e-03, 2.78367094e-03, 4.46436904e-04,\n",
       "        1.27907847e-03, 1.69233654e-03, 3.76734290e-03, 2.48089625e-03,\n",
       "        3.32648080e-03, 1.47061273e-03, 5.43883875e-03, 1.12404439e-03,\n",
       "        2.44660560e-04, 2.28150301e-03, 3.04616892e-03, 2.48077535e-03,\n",
       "        4.18134407e-03, 6.16908417e-03, 6.92315172e-03, 7.54588447e-03,\n",
       "        1.53250390e-03, 2.00605647e-04, 1.30555662e-03, 3.55315255e-03,\n",
       "        2.46369793e-03, 2.99093420e-03, 1.56928928e-03, 4.42422508e-03,\n",
       "        1.40176166e-03, 9.49645011e-04, 2.97810903e-03, 1.28857230e-03,\n",
       "        5.47751290e-03, 1.15994327e-03, 1.69210019e-03, 1.28797276e-03,\n",
       "        2.85846460e-03, 1.79047287e-03, 1.63251380e-03, 9.20879340e-04,\n",
       "        3.97129815e-03, 4.74333927e-03, 5.31912362e-03, 2.38095160e-03,\n",
       "        1.98584366e-03, 2.22467991e-03, 4.37958183e-03, 3.17713396e-04,\n",
       "        1.15813468e-03, 2.89054665e-03, 1.96688994e-03, 1.15774724e-03,\n",
       "        5.27288506e-03, 1.15837317e-03, 8.02715947e-03, 1.24235648e-03,\n",
       "        3.06665310e-03, 1.86138287e-03, 4.47878433e-04, 5.83985654e-04,\n",
       "        1.64580557e-03, 4.66706247e-03, 4.74195864e-03, 3.27567388e-03,\n",
       "        2.03699120e-03, 1.16620419e-03, 3.17120756e-03, 5.10463757e-04,\n",
       "        1.07829204e-03, 7.16843432e-07, 6.00290781e-04, 4.90018322e-04,\n",
       "        2.18376332e-03, 9.28055839e-04, 3.74623265e-04, 7.49029341e-04,\n",
       "        9.80581587e-04, 1.15880971e-03, 1.20953128e-03, 3.16657450e-04,\n",
       "        7.48882669e-04, 1.36471238e-03, 7.48869865e-04, 7.49405273e-04,\n",
       "        7.35306378e-04, 3.74636502e-04, 3.75018877e-04, 1.76216557e-03,\n",
       "        2.44971237e-04, 1.12377548e-03, 4.47128518e-04, 2.22922675e-03,\n",
       "        2.28256942e-03, 8.95242055e-04, 5.10351430e-04, 2.45301209e-04,\n",
       "        7.35215460e-04, 4.89356939e-04, 2.00152475e-04, 3.74189695e-04,\n",
       "        2.45145753e-04, 4.90573035e-04, 4.48141572e-04, 4.00829514e-04,\n",
       "        2.45067794e-04, 3.05253546e-03, 1.20095018e-03, 5.60464860e-03,\n",
       "        7.35273878e-04, 3.74279367e-04, 6.79083885e-04, 5.10174157e-04,\n",
       "        1.24197641e-03, 3.44665516e-03, 9.27834754e-04, 1.04980141e-03,\n",
       "        4.00043312e-04, 1.20086684e-03, 5.83347608e-04, 2.45476538e-04,\n",
       "        3.75069312e-04, 7.35605026e-04, 1.51764423e-03, 4.00293011e-04,\n",
       "        2.33401706e-03, 9.17057470e-04, 1.02060911e-03, 1.72189249e-03,\n",
       "        2.45340359e-04, 3.74547013e-04, 4.90514947e-04, 3.74431817e-04,\n",
       "        5.83429124e-04, 2.38087526e-03, 1.20093836e-03, 2.45145661e-04,\n",
       "        3.74037006e-04, 8.12808717e-04, 7.35650252e-04, 7.75741170e-04,\n",
       "        1.59527383e-03, 5.83748104e-04, 5.83649994e-04, 1.16716086e-03,\n",
       "        1.59489106e-03, 1.04950582e-03, 1.53090114e-03, 8.27162933e-03,\n",
       "        3.16883797e-04, 1.47108629e-03, 1.82908850e-03, 3.90966554e-03,\n",
       "        3.16355793e-04, 2.45281947e-04, 1.07704503e-02, 5.48466664e-04,\n",
       "        1.14084565e-03, 7.05557166e-03, 2.26023410e-03, 4.90076729e-04,\n",
       "        5.48640707e-04, 3.74177232e-04, 8.60835359e-04, 9.17161685e-04,\n",
       "        2.15599071e-03, 6.00568698e-04, 5.10407394e-04, 1.50441662e-03,\n",
       "        2.36059715e-03, 3.74444788e-04, 3.74559470e-04, 7.35338814e-04,\n",
       "        1.73312291e-03, 1.03047030e-03, 5.10426447e-04, 7.48570515e-04,\n",
       "        2.00248240e-04, 4.90543839e-04, 5.15811053e-03, 7.36531254e-03,\n",
       "        5.10557688e-04, 3.73858985e-04, 1.07835395e-03, 3.99852009e-04,\n",
       "        1.15878927e-03, 3.16657379e-04, 5.10201686e-04, 2.56321630e-03,\n",
       "        4.00614863e-04, 2.00319602e-04, 3.74431848e-04, 9.70355971e-04,\n",
       "        2.45437651e-04, 6.33088749e-04, 2.35957390e-03, 7.35838911e-04,\n",
       "        2.45593524e-04, 5.48161864e-04, 1.14611839e-02, 3.21821302e-03,\n",
       "        2.45243632e-04, 4.90174173e-04, 2.00152816e-04, 3.74674020e-04,\n",
       "        3.74457691e-04, 4.90037950e-04, 4.90398019e-04, 2.45612678e-04,\n",
       "        3.74598106e-04, 7.35020847e-04, 2.44639463e-04, 2.53144825e-03,\n",
       "        4.90505013e-04, 1.50487012e-03, 4.00292926e-04, 3.75132808e-04,\n",
       "        3.74751125e-04, 2.00129947e-04, 5.10323473e-04, 1.36498856e-03,\n",
       "        5.83380313e-04, 2.08525352e-03, 6.78803228e-04, 9.28287114e-04,\n",
       "        1.24231068e-03, 2.45301163e-04, 1.36443617e-03, 3.74546861e-04,\n",
       "        1.98626735e-03, 3.74215118e-04, 4.47821510e-04, 3.16356189e-04,\n",
       "        9.17291618e-04, 1.07791601e-03, 2.45593524e-04, 8.37311867e-04,\n",
       "        1.24232232e-03, 8.95108860e-04, 3.74508511e-04, 1.12333372e-03,\n",
       "        2.45437419e-04, 4.47714518e-04, 6.33439419e-04, 5.10538583e-04,\n",
       "        8.37226386e-04, 3.74840249e-04, 6.00767221e-04, 1.91452680e-03,\n",
       "        5.83568140e-04, 5.48074638e-04, 2.35566896e-03, 9.80484241e-04,\n",
       "        4.19150699e-03, 3.99995230e-04, 1.40156756e-03, 1.94109070e-03,\n",
       "        1.39417923e-03, 1.02090855e-03, 1.77898812e-03, 2.00224519e-04]),\n",
       " 'param_classifier': masked_array(data=[MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__activation': masked_array(data=['logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__alpha': masked_array(data=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__hidden_layer_sizes': masked_array(data=[(5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__learning_rate': masked_array(data=['constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'}],\n",
       " 'split0_test_recall_micro': array([0.763, 0.77 , 0.775, 0.758, 0.754, 0.732, 0.764, 0.763, 0.741,\n",
       "        0.727, 0.74 , 0.737, 0.763, 0.771, 0.78 , 0.763, 0.76 , 0.754,\n",
       "        0.731, 0.752, 0.756, 0.771, 0.749, 0.746, 0.765, 0.767, 0.766,\n",
       "        0.764, 0.762, 0.743, 0.754, 0.743, 0.752, 0.736, 0.748, 0.756,\n",
       "        0.763, 0.775, 0.756, 0.763, 0.76 , 0.749, 0.761, 0.759, 0.758,\n",
       "        0.757, 0.762, 0.749, 0.777, 0.776, 0.768, 0.749, 0.754, 0.755,\n",
       "        0.755, 0.75 , 0.751, 0.724, 0.738, 0.729, 0.76 , 0.77 , 0.77 ,\n",
       "        0.739, 0.768, 0.758, 0.742, 0.741, 0.747, 0.748, 0.733, 0.747,\n",
       "        0.764, 0.783, 0.773, 0.761, 0.753, 0.765, 0.747, 0.752, 0.748,\n",
       "        0.752, 0.741, 0.744, 0.774, 0.771, 0.76 , 0.766, 0.769, 0.756,\n",
       "        0.744, 0.768, 0.772, 0.744, 0.743, 0.763, 0.776, 0.502, 0.778,\n",
       "        0.776, 0.502, 0.777, 0.778, 0.507, 0.778, 0.776, 0.498, 0.778,\n",
       "        0.773, 0.502, 0.776, 0.777, 0.501, 0.772, 0.778, 0.498, 0.781,\n",
       "        0.774, 0.529, 0.78 , 0.782, 0.502, 0.78 , 0.778, 0.476, 0.776,\n",
       "        0.777, 0.498, 0.776, 0.782, 0.51 , 0.776, 0.776, 0.502, 0.776,\n",
       "        0.778, 0.5  , 0.775, 0.778, 0.498, 0.78 , 0.782, 0.498, 0.781,\n",
       "        0.767, 0.562, 0.774, 0.781, 0.612, 0.782, 0.777, 0.706, 0.773,\n",
       "        0.776, 0.635, 0.77 , 0.774, 0.654, 0.771, 0.775, 0.651, 0.774,\n",
       "        0.772, 0.62 , 0.773, 0.773, 0.733, 0.777, 0.777, 0.697, 0.783,\n",
       "        0.775, 0.615, 0.784, 0.779, 0.693, 0.779, 0.779, 0.648, 0.775,\n",
       "        0.773, 0.599, 0.782, 0.777, 0.535, 0.773, 0.776, 0.703, 0.776,\n",
       "        0.78 , 0.769, 0.779, 0.776, 0.78 , 0.779, 0.779, 0.78 , 0.78 ,\n",
       "        0.779, 0.779, 0.778, 0.774, 0.776, 0.776, 0.78 , 0.783, 0.778,\n",
       "        0.775, 0.775, 0.778, 0.775, 0.776, 0.776, 0.777, 0.776, 0.779,\n",
       "        0.777, 0.775, 0.774, 0.778, 0.779, 0.78 , 0.777, 0.778, 0.778,\n",
       "        0.777, 0.777, 0.778, 0.777, 0.778, 0.779, 0.777, 0.779, 0.779,\n",
       "        0.78 , 0.776, 0.776, 0.778, 0.778, 0.775, 0.779, 0.786, 0.779,\n",
       "        0.774, 0.771, 0.777, 0.775, 0.766, 0.765, 0.774, 0.766, 0.766,\n",
       "        0.781, 0.776, 0.778, 0.774, 0.775, 0.779, 0.779, 0.788, 0.767,\n",
       "        0.777, 0.776, 0.773, 0.781, 0.776, 0.777, 0.78 , 0.773, 0.773,\n",
       "        0.772, 0.77 , 0.782, 0.77 , 0.774, 0.77 , 0.774, 0.78 , 0.774,\n",
       "        0.78 , 0.775, 0.775, 0.777, 0.766, 0.778, 0.769, 0.777, 0.777]),\n",
       " 'split1_test_recall_micro': array([0.795, 0.795, 0.781, 0.786, 0.777, 0.761, 0.764, 0.757, 0.775,\n",
       "        0.748, 0.775, 0.76 , 0.792, 0.788, 0.787, 0.755, 0.759, 0.768,\n",
       "        0.767, 0.76 , 0.736, 0.748, 0.75 , 0.742, 0.779, 0.794, 0.778,\n",
       "        0.778, 0.77 , 0.762, 0.77 , 0.769, 0.772, 0.758, 0.748, 0.761,\n",
       "        0.782, 0.788, 0.789, 0.781, 0.775, 0.775, 0.765, 0.775, 0.781,\n",
       "        0.765, 0.75 , 0.754, 0.776, 0.78 , 0.784, 0.766, 0.785, 0.754,\n",
       "        0.755, 0.768, 0.77 , 0.755, 0.749, 0.739, 0.787, 0.785, 0.793,\n",
       "        0.775, 0.78 , 0.757, 0.761, 0.753, 0.761, 0.741, 0.735, 0.746,\n",
       "        0.785, 0.785, 0.782, 0.786, 0.764, 0.761, 0.768, 0.762, 0.746,\n",
       "        0.748, 0.759, 0.763, 0.783, 0.78 , 0.79 , 0.773, 0.772, 0.766,\n",
       "        0.772, 0.761, 0.761, 0.733, 0.751, 0.754, 0.795, 0.284, 0.797,\n",
       "        0.791, 0.502, 0.792, 0.792, 0.504, 0.796, 0.791, 0.498, 0.793,\n",
       "        0.791, 0.522, 0.792, 0.793, 0.695, 0.793, 0.792, 0.502, 0.791,\n",
       "        0.795, 0.522, 0.793, 0.791, 0.496, 0.793, 0.79 , 0.496, 0.799,\n",
       "        0.79 , 0.371, 0.791, 0.793, 0.504, 0.793, 0.792, 0.576, 0.793,\n",
       "        0.793, 0.498, 0.793, 0.794, 0.501, 0.791, 0.791, 0.496, 0.793,\n",
       "        0.791, 0.651, 0.791, 0.784, 0.596, 0.792, 0.801, 0.662, 0.789,\n",
       "        0.797, 0.688, 0.797, 0.793, 0.483, 0.79 , 0.792, 0.66 , 0.8  ,\n",
       "        0.784, 0.55 , 0.796, 0.787, 0.563, 0.793, 0.79 , 0.558, 0.795,\n",
       "        0.794, 0.637, 0.794, 0.793, 0.507, 0.788, 0.795, 0.692, 0.794,\n",
       "        0.794, 0.697, 0.791, 0.795, 0.664, 0.79 , 0.798, 0.472, 0.796,\n",
       "        0.794, 0.595, 0.79 , 0.793, 0.792, 0.791, 0.791, 0.792, 0.792,\n",
       "        0.791, 0.791, 0.793, 0.793, 0.793, 0.791, 0.799, 0.791, 0.791,\n",
       "        0.791, 0.793, 0.793, 0.793, 0.792, 0.79 , 0.791, 0.792, 0.791,\n",
       "        0.793, 0.792, 0.794, 0.791, 0.793, 0.793, 0.79 , 0.79 , 0.789,\n",
       "        0.789, 0.79 , 0.793, 0.792, 0.791, 0.796, 0.794, 0.792, 0.793,\n",
       "        0.791, 0.793, 0.792, 0.792, 0.794, 0.792, 0.787, 0.791, 0.794,\n",
       "        0.794, 0.796, 0.792, 0.785, 0.792, 0.793, 0.784, 0.791, 0.795,\n",
       "        0.792, 0.795, 0.789, 0.79 , 0.788, 0.787, 0.792, 0.794, 0.792,\n",
       "        0.794, 0.789, 0.787, 0.794, 0.801, 0.788, 0.795, 0.786, 0.788,\n",
       "        0.787, 0.792, 0.791, 0.793, 0.793, 0.788, 0.796, 0.794, 0.786,\n",
       "        0.791, 0.795, 0.792, 0.794, 0.789, 0.793, 0.792, 0.787, 0.794]),\n",
       " 'split2_test_recall_micro': array([0.796, 0.792, 0.796, 0.793, 0.777, 0.775, 0.759, 0.785, 0.788,\n",
       "        0.743, 0.762, 0.767, 0.804, 0.791, 0.796, 0.788, 0.773, 0.791,\n",
       "        0.784, 0.774, 0.777, 0.766, 0.78 , 0.777, 0.796, 0.792, 0.808,\n",
       "        0.784, 0.778, 0.782, 0.777, 0.771, 0.781, 0.762, 0.751, 0.76 ,\n",
       "        0.789, 0.791, 0.797, 0.782, 0.788, 0.784, 0.767, 0.778, 0.785,\n",
       "        0.78 , 0.772, 0.775, 0.802, 0.792, 0.797, 0.772, 0.781, 0.784,\n",
       "        0.774, 0.759, 0.783, 0.767, 0.758, 0.769, 0.783, 0.793, 0.785,\n",
       "        0.789, 0.791, 0.786, 0.785, 0.768, 0.756, 0.756, 0.743, 0.769,\n",
       "        0.779, 0.804, 0.796, 0.786, 0.783, 0.786, 0.753, 0.77 , 0.77 ,\n",
       "        0.776, 0.76 , 0.763, 0.783, 0.791, 0.799, 0.793, 0.804, 0.769,\n",
       "        0.77 , 0.782, 0.772, 0.76 , 0.775, 0.782, 0.802, 0.491, 0.806,\n",
       "        0.797, 0.5  , 0.798, 0.795, 0.5  , 0.794, 0.793, 0.711, 0.793,\n",
       "        0.797, 0.62 , 0.793, 0.802, 0.544, 0.799, 0.798, 0.498, 0.796,\n",
       "        0.8  , 0.502, 0.801, 0.8  , 0.709, 0.803, 0.798, 0.493, 0.799,\n",
       "        0.8  , 0.502, 0.797, 0.794, 0.502, 0.799, 0.792, 0.547, 0.804,\n",
       "        0.797, 0.516, 0.796, 0.798, 0.462, 0.795, 0.801, 0.502, 0.797,\n",
       "        0.807, 0.619, 0.8  , 0.8  , 0.482, 0.796, 0.811, 0.635, 0.799,\n",
       "        0.799, 0.703, 0.806, 0.791, 0.661, 0.805, 0.798, 0.703, 0.805,\n",
       "        0.799, 0.522, 0.802, 0.796, 0.722, 0.797, 0.802, 0.639, 0.81 ,\n",
       "        0.803, 0.738, 0.793, 0.796, 0.685, 0.803, 0.804, 0.328, 0.798,\n",
       "        0.803, 0.672, 0.806, 0.797, 0.514, 0.792, 0.805, 0.608, 0.8  ,\n",
       "        0.803, 0.768, 0.802, 0.798, 0.799, 0.801, 0.798, 0.801, 0.8  ,\n",
       "        0.8  , 0.8  , 0.8  , 0.8  , 0.804, 0.8  , 0.802, 0.806, 0.801,\n",
       "        0.801, 0.8  , 0.8  , 0.797, 0.802, 0.803, 0.801, 0.802, 0.8  ,\n",
       "        0.8  , 0.8  , 0.798, 0.8  , 0.801, 0.799, 0.802, 0.802, 0.8  ,\n",
       "        0.801, 0.803, 0.802, 0.796, 0.799, 0.799, 0.8  , 0.799, 0.8  ,\n",
       "        0.801, 0.801, 0.802, 0.801, 0.8  , 0.801, 0.8  , 0.803, 0.807,\n",
       "        0.806, 0.803, 0.807, 0.803, 0.808, 0.8  , 0.805, 0.817, 0.811,\n",
       "        0.799, 0.802, 0.796, 0.808, 0.803, 0.808, 0.799, 0.801, 0.8  ,\n",
       "        0.8  , 0.81 , 0.811, 0.804, 0.795, 0.801, 0.804, 0.807, 0.813,\n",
       "        0.809, 0.805, 0.812, 0.794, 0.807, 0.805, 0.805, 0.799, 0.805,\n",
       "        0.796, 0.801, 0.806, 0.8  , 0.806, 0.806, 0.808, 0.803, 0.8  ]),\n",
       " 'split3_test_recall_micro': array([0.786, 0.79 , 0.79 , 0.774, 0.787, 0.781, 0.768, 0.761, 0.77 ,\n",
       "        0.741, 0.758, 0.76 , 0.797, 0.787, 0.79 , 0.785, 0.765, 0.777,\n",
       "        0.756, 0.771, 0.773, 0.751, 0.756, 0.772, 0.792, 0.79 , 0.785,\n",
       "        0.779, 0.771, 0.78 , 0.771, 0.754, 0.775, 0.759, 0.76 , 0.763,\n",
       "        0.771, 0.789, 0.791, 0.787, 0.783, 0.791, 0.78 , 0.767, 0.763,\n",
       "        0.766, 0.774, 0.754, 0.795, 0.789, 0.79 , 0.74 , 0.776, 0.776,\n",
       "        0.761, 0.776, 0.758, 0.76 , 0.765, 0.724, 0.777, 0.777, 0.796,\n",
       "        0.779, 0.758, 0.785, 0.763, 0.764, 0.774, 0.754, 0.771, 0.775,\n",
       "        0.79 , 0.779, 0.795, 0.782, 0.785, 0.783, 0.788, 0.767, 0.77 ,\n",
       "        0.758, 0.756, 0.764, 0.786, 0.791, 0.789, 0.775, 0.774, 0.772,\n",
       "        0.752, 0.774, 0.758, 0.747, 0.769, 0.743, 0.796, 0.498, 0.799,\n",
       "        0.794, 0.314, 0.804, 0.799, 0.498, 0.802, 0.801, 0.503, 0.799,\n",
       "        0.802, 0.502, 0.798, 0.802, 0.496, 0.8  , 0.8  , 0.498, 0.797,\n",
       "        0.799, 0.555, 0.801, 0.801, 0.44 , 0.796, 0.8  , 0.284, 0.794,\n",
       "        0.8  , 0.499, 0.802, 0.802, 0.498, 0.795, 0.805, 0.498, 0.799,\n",
       "        0.801, 0.605, 0.802, 0.802, 0.55 , 0.795, 0.803, 0.482, 0.8  ,\n",
       "        0.796, 0.663, 0.788, 0.801, 0.532, 0.788, 0.8  , 0.354, 0.793,\n",
       "        0.791, 0.732, 0.789, 0.798, 0.321, 0.787, 0.789, 0.658, 0.799,\n",
       "        0.793, 0.686, 0.789, 0.789, 0.606, 0.796, 0.798, 0.728, 0.789,\n",
       "        0.793, 0.575, 0.796, 0.798, 0.589, 0.791, 0.798, 0.712, 0.789,\n",
       "        0.79 , 0.472, 0.795, 0.787, 0.718, 0.793, 0.801, 0.661, 0.792,\n",
       "        0.794, 0.603, 0.793, 0.793, 0.794, 0.795, 0.795, 0.792, 0.791,\n",
       "        0.793, 0.797, 0.796, 0.79 , 0.796, 0.793, 0.794, 0.795, 0.793,\n",
       "        0.792, 0.793, 0.791, 0.796, 0.793, 0.794, 0.794, 0.79 , 0.792,\n",
       "        0.791, 0.793, 0.796, 0.79 , 0.795, 0.796, 0.794, 0.793, 0.793,\n",
       "        0.796, 0.798, 0.797, 0.794, 0.795, 0.796, 0.794, 0.794, 0.796,\n",
       "        0.796, 0.797, 0.794, 0.796, 0.793, 0.79 , 0.787, 0.796, 0.789,\n",
       "        0.794, 0.792, 0.8  , 0.791, 0.791, 0.799, 0.792, 0.797, 0.792,\n",
       "        0.795, 0.796, 0.794, 0.793, 0.793, 0.797, 0.796, 0.785, 0.794,\n",
       "        0.786, 0.797, 0.796, 0.792, 0.785, 0.797, 0.789, 0.783, 0.794,\n",
       "        0.794, 0.793, 0.792, 0.802, 0.799, 0.792, 0.788, 0.786, 0.795,\n",
       "        0.789, 0.796, 0.786, 0.794, 0.792, 0.79 , 0.792, 0.797, 0.797]),\n",
       " 'split4_test_recall_micro': array([0.79 , 0.796, 0.797, 0.764, 0.777, 0.774, 0.758, 0.763, 0.752,\n",
       "        0.763, 0.76 , 0.764, 0.793, 0.79 , 0.792, 0.794, 0.77 , 0.787,\n",
       "        0.777, 0.763, 0.762, 0.744, 0.751, 0.758, 0.799, 0.791, 0.798,\n",
       "        0.767, 0.778, 0.775, 0.77 , 0.769, 0.759, 0.757, 0.759, 0.745,\n",
       "        0.796, 0.787, 0.789, 0.78 , 0.77 , 0.796, 0.771, 0.779, 0.772,\n",
       "        0.765, 0.755, 0.765, 0.784, 0.794, 0.783, 0.765, 0.781, 0.796,\n",
       "        0.773, 0.749, 0.767, 0.748, 0.732, 0.771, 0.793, 0.794, 0.793,\n",
       "        0.754, 0.771, 0.78 , 0.76 , 0.75 , 0.75 , 0.75 , 0.744, 0.736,\n",
       "        0.802, 0.786, 0.798, 0.785, 0.766, 0.777, 0.752, 0.773, 0.764,\n",
       "        0.766, 0.751, 0.746, 0.778, 0.806, 0.803, 0.779, 0.773, 0.802,\n",
       "        0.756, 0.761, 0.77 , 0.767, 0.762, 0.759, 0.795, 0.502, 0.798,\n",
       "        0.799, 0.566, 0.796, 0.8  , 0.622, 0.802, 0.801, 0.511, 0.8  ,\n",
       "        0.8  , 0.498, 0.798, 0.797, 0.386, 0.797, 0.8  , 0.536, 0.802,\n",
       "        0.797, 0.503, 0.801, 0.797, 0.498, 0.801, 0.799, 0.497, 0.798,\n",
       "        0.799, 0.513, 0.8  , 0.802, 0.502, 0.794, 0.795, 0.502, 0.795,\n",
       "        0.798, 0.47 , 0.792, 0.8  , 0.502, 0.802, 0.799, 0.498, 0.801,\n",
       "        0.796, 0.556, 0.799, 0.8  , 0.666, 0.794, 0.788, 0.561, 0.789,\n",
       "        0.797, 0.669, 0.796, 0.794, 0.483, 0.799, 0.792, 0.55 , 0.796,\n",
       "        0.796, 0.717, 0.796, 0.8  , 0.484, 0.798, 0.799, 0.533, 0.8  ,\n",
       "        0.795, 0.745, 0.798, 0.794, 0.689, 0.797, 0.794, 0.571, 0.797,\n",
       "        0.799, 0.622, 0.796, 0.789, 0.629, 0.797, 0.805, 0.718, 0.796,\n",
       "        0.804, 0.652, 0.796, 0.799, 0.8  , 0.798, 0.797, 0.801, 0.802,\n",
       "        0.799, 0.797, 0.8  , 0.801, 0.797, 0.799, 0.798, 0.795, 0.799,\n",
       "        0.799, 0.801, 0.801, 0.8  , 0.802, 0.799, 0.801, 0.8  , 0.801,\n",
       "        0.801, 0.798, 0.798, 0.8  , 0.8  , 0.799, 0.799, 0.799, 0.8  ,\n",
       "        0.801, 0.798, 0.798, 0.799, 0.801, 0.799, 0.801, 0.801, 0.798,\n",
       "        0.8  , 0.799, 0.801, 0.8  , 0.798, 0.798, 0.801, 0.797, 0.803,\n",
       "        0.796, 0.798, 0.796, 0.797, 0.795, 0.8  , 0.795, 0.795, 0.799,\n",
       "        0.803, 0.798, 0.796, 0.786, 0.799, 0.793, 0.791, 0.798, 0.792,\n",
       "        0.786, 0.793, 0.797, 0.796, 0.793, 0.798, 0.801, 0.799, 0.793,\n",
       "        0.794, 0.796, 0.803, 0.787, 0.801, 0.793, 0.804, 0.799, 0.793,\n",
       "        0.797, 0.802, 0.796, 0.799, 0.798, 0.801, 0.794, 0.798, 0.787]),\n",
       " 'mean_test_recall_micro': array([0.786 , 0.7886, 0.7878, 0.775 , 0.7744, 0.7646, 0.7626, 0.7658,\n",
       "        0.7652, 0.7444, 0.759 , 0.7576, 0.7898, 0.7854, 0.789 , 0.777 ,\n",
       "        0.7654, 0.7754, 0.763 , 0.764 , 0.7608, 0.756 , 0.7572, 0.759 ,\n",
       "        0.7862, 0.7868, 0.787 , 0.7744, 0.7718, 0.7684, 0.7684, 0.7612,\n",
       "        0.7678, 0.7544, 0.7532, 0.757 , 0.7802, 0.786 , 0.7844, 0.7786,\n",
       "        0.7752, 0.779 , 0.7688, 0.7716, 0.7718, 0.7666, 0.7626, 0.7594,\n",
       "        0.7868, 0.7862, 0.7844, 0.7584, 0.7754, 0.773 , 0.7636, 0.7604,\n",
       "        0.7658, 0.7508, 0.7484, 0.7464, 0.78  , 0.7838, 0.7874, 0.7672,\n",
       "        0.7736, 0.7732, 0.7622, 0.7552, 0.7576, 0.7498, 0.7452, 0.7546,\n",
       "        0.784 , 0.7874, 0.7888, 0.78  , 0.7702, 0.7744, 0.7616, 0.7648,\n",
       "        0.7596, 0.76  , 0.7534, 0.756 , 0.7808, 0.7878, 0.7882, 0.7772,\n",
       "        0.7784, 0.773 , 0.7588, 0.7692, 0.7666, 0.7502, 0.76  , 0.7602,\n",
       "        0.7928, 0.4554, 0.7956, 0.7914, 0.4768, 0.7934, 0.7928, 0.5262,\n",
       "        0.7944, 0.7924, 0.5442, 0.7926, 0.7926, 0.5288, 0.7914, 0.7942,\n",
       "        0.5244, 0.7922, 0.7936, 0.5064, 0.7934, 0.793 , 0.5222, 0.7952,\n",
       "        0.7942, 0.529 , 0.7946, 0.793 , 0.4492, 0.7932, 0.7932, 0.4766,\n",
       "        0.7932, 0.7946, 0.5032, 0.7914, 0.792 , 0.525 , 0.7934, 0.7934,\n",
       "        0.5178, 0.7916, 0.7944, 0.5026, 0.7926, 0.7952, 0.4952, 0.7944,\n",
       "        0.7914, 0.6102, 0.7904, 0.7932, 0.5776, 0.7904, 0.7954, 0.5836,\n",
       "        0.7886, 0.792 , 0.6854, 0.7916, 0.79  , 0.5204, 0.7904, 0.7892,\n",
       "        0.6444, 0.7948, 0.7888, 0.619 , 0.7912, 0.789 , 0.6216, 0.7922,\n",
       "        0.7932, 0.631 , 0.7954, 0.792 , 0.662 , 0.793 , 0.792 , 0.6326,\n",
       "        0.7916, 0.794 , 0.5902, 0.7906, 0.7918, 0.6124, 0.794 , 0.789 ,\n",
       "        0.612 , 0.789 , 0.797 , 0.6324, 0.792 , 0.795 , 0.6774, 0.792 ,\n",
       "        0.7918, 0.793 , 0.7928, 0.792 , 0.7932, 0.793 , 0.7924, 0.7928,\n",
       "        0.7934, 0.7916, 0.7932, 0.7918, 0.7946, 0.794 , 0.7924, 0.7916,\n",
       "        0.7924, 0.7926, 0.7922, 0.793 , 0.7924, 0.7928, 0.792 , 0.7926,\n",
       "        0.7924, 0.7916, 0.792 , 0.7918, 0.7936, 0.7934, 0.7924, 0.7924,\n",
       "        0.792 , 0.7928, 0.7932, 0.7936, 0.7916, 0.7928, 0.7938, 0.7932,\n",
       "        0.793 , 0.7932, 0.7936, 0.7932, 0.793 , 0.7934, 0.7926, 0.7912,\n",
       "        0.7908, 0.7946, 0.7944, 0.7928, 0.792 , 0.7944, 0.7902, 0.7904,\n",
       "        0.7914, 0.79  , 0.7932, 0.7926, 0.794 , 0.7934, 0.7906, 0.7902,\n",
       "        0.7916, 0.7928, 0.7914, 0.7932, 0.789 , 0.7886, 0.793 , 0.7928,\n",
       "        0.7934, 0.79  , 0.7922, 0.7938, 0.7896, 0.7922, 0.7912, 0.7912,\n",
       "        0.796 , 0.7892, 0.7948, 0.7896, 0.7934, 0.7916, 0.7906, 0.7906,\n",
       "        0.7938, 0.791 , 0.7928, 0.7902, 0.7936, 0.791 , 0.7924, 0.791 ]),\n",
       " 'std_test_recall_micro': array([0.0120499 , 0.00954149, 0.00856505, 0.01308434, 0.01091055,\n",
       "        0.01755677, 0.00366606, 0.00984683, 0.01672603, 0.01162067,\n",
       "        0.01120714, 0.01063203, 0.01404849, 0.00733757, 0.00536656,\n",
       "        0.0151921 , 0.0054626 , 0.01336563, 0.01857956, 0.00787401,\n",
       "        0.0144969 , 0.01056409, 0.01165161, 0.01379855, 0.01260793,\n",
       "        0.00998799, 0.01475127, 0.00760526, 0.00594643, 0.01448586,\n",
       "        0.00765768, 0.01096175, 0.01068457, 0.00935094, 0.00526878,\n",
       "        0.00641872, 0.01192309, 0.00565685, 0.01449966, 0.00816333,\n",
       "        0.00982649, 0.01657709, 0.0064622 , 0.00757892, 0.0102645 ,\n",
       "        0.0074458 , 0.00932952, 0.00939361, 0.01018627, 0.00699714,\n",
       "        0.00960417, 0.0119432 , 0.01107429, 0.01639512, 0.00838093,\n",
       "        0.01040385, 0.01090688, 0.01477024, 0.01220819, 0.01987561,\n",
       "        0.0112783 , 0.00923905, 0.0094361 , 0.01813725, 0.01118213,\n",
       "        0.01298307, 0.01367333, 0.00974474, 0.0095205 , 0.00523068,\n",
       "        0.0136    , 0.01484049, 0.01253794, 0.00863944, 0.00970361,\n",
       "        0.00961249, 0.01212271, 0.00983056, 0.01494791, 0.00735935,\n",
       "        0.01053755, 0.01003992, 0.0069455 , 0.0090111 , 0.00426146,\n",
       "        0.01178813, 0.01506519, 0.00895321, 0.01290891, 0.01546609,\n",
       "        0.01070327, 0.00803492, 0.00591946, 0.01202331, 0.0116619 ,\n",
       "        0.01279687, 0.00879545, 0.08579417, 0.00935094, 0.00816333,\n",
       "        0.08516901, 0.00906863, 0.00793473, 0.04800167, 0.0088    ,\n",
       "        0.00915642, 0.08353538, 0.00786384, 0.01048046, 0.0463655 ,\n",
       "        0.0080895 , 0.00923905, 0.0999972 , 0.01038075, 0.00833307,\n",
       "        0.01488086, 0.00711618, 0.00965401, 0.01948743, 0.00820731,\n",
       "        0.00702567, 0.09284396, 0.00811419, 0.00829458, 0.08294914,\n",
       "        0.00879545, 0.00893085, 0.05306826, 0.00936803, 0.00736478,\n",
       "        0.00391918, 0.00796492, 0.00931665, 0.03121538, 0.0094784 ,\n",
       "        0.00811419, 0.0460452 , 0.00900222, 0.00861626, 0.02799714,\n",
       "        0.00722772, 0.00775629, 0.00688186, 0.00725534, 0.01327554,\n",
       "        0.0442511 , 0.00939361, 0.00879545, 0.06411739, 0.00496387,\n",
       "        0.01174053, 0.12409448, 0.00861626, 0.00843801, 0.03254904,\n",
       "        0.01207642, 0.00831865, 0.12662954, 0.01162067, 0.00767854,\n",
       "        0.05063043, 0.0107963 , 0.0097857 , 0.0751851 , 0.00998799,\n",
       "        0.00927362, 0.09497494, 0.00778203, 0.00901998, 0.07584458,\n",
       "        0.00926499, 0.00920869, 0.06792349, 0.00481664, 0.00672309,\n",
       "        0.07382574, 0.0081388 , 0.00827043, 0.13975464, 0.00840476,\n",
       "        0.01038075, 0.07834947, 0.00777174, 0.00704273, 0.0771518 ,\n",
       "        0.00831865, 0.01082589, 0.08882927, 0.00839047, 0.00862554,\n",
       "        0.0769015 , 0.00761577, 0.0082801 , 0.00715542, 0.00765245,\n",
       "        0.0069282 , 0.00773046, 0.00779744, 0.00752596, 0.007494  ,\n",
       "        0.0081388 , 0.00972831, 0.00932523, 0.00861162, 0.00773563,\n",
       "        0.00742967, 0.0080895 , 0.00915642, 0.00932952, 0.00826075,\n",
       "        0.00888594, 0.00950789, 0.00930806, 0.00881816, 0.00920869,\n",
       "        0.00791454, 0.00861626, 0.0088227 , 0.0091214 , 0.00810925,\n",
       "        0.00788923, 0.00705975, 0.00873155, 0.00835703, 0.00817313,\n",
       "        0.00904212, 0.00910824, 0.00830903, 0.00765768, 0.00815843,\n",
       "        0.00752064, 0.00861162, 0.0077201 , 0.00746726, 0.00765768,\n",
       "        0.00899778, 0.00933809, 0.00833307, 0.00773563, 0.00901998,\n",
       "        0.00844748, 0.00574804, 0.009992  , 0.0104    , 0.01108152,\n",
       "        0.01001199, 0.00968297, 0.01363231, 0.01345511, 0.01044988,\n",
       "        0.01630215, 0.01478648, 0.00748331, 0.00902441, 0.0068    ,\n",
       "        0.01099818, 0.00974885, 0.0097242 , 0.00682935, 0.00597997,\n",
       "        0.0113842 , 0.00783837, 0.01104536, 0.01252837, 0.00741889,\n",
       "        0.00867179, 0.00874986, 0.00861162, 0.01202664, 0.01282809,\n",
       "        0.01199   , 0.01154816, 0.01041153, 0.01072194, 0.01132078,\n",
       "        0.01132431, 0.01148216, 0.00749933, 0.01028786, 0.00608605,\n",
       "        0.0097857 , 0.01031504, 0.0082801 , 0.01342237, 0.00964572,\n",
       "        0.01252198, 0.00928655, 0.00822192]),\n",
       " 'rank_test_recall_micro': array([175, 162, 166, 194, 195, 219, 223, 214, 217, 256, 235, 239, 150,\n",
       "        177, 156, 190, 216, 191, 222, 220, 228, 243, 241, 235, 174, 172,\n",
       "        170, 197, 202, 208, 208, 227, 210, 247, 249, 242, 183, 175, 178,\n",
       "        187, 193, 186, 207, 204, 202, 212, 223, 234, 171, 173, 178, 238,\n",
       "        191, 200, 221, 229, 215, 250, 253, 254, 184, 181, 168, 211, 198,\n",
       "        199, 225, 245, 239, 252, 255, 246, 180, 169, 160, 184, 205, 195,\n",
       "        226, 218, 233, 231, 248, 243, 182, 166, 165, 189, 188, 201, 237,\n",
       "        206, 213, 251, 231, 230,  66, 287,   3, 122, 285,  34,  66, 275,\n",
       "         15,  84, 272,  78,  78, 274, 122,  21, 277,  93,  31, 281,  34,\n",
       "         62, 278,   6,  20, 273,  14,  57, 288,  46,  55, 286,  46,  11,\n",
       "        282, 126,  98, 276,  41,  34, 280, 113,  15, 283,  78,   7, 284,\n",
       "         15, 122, 268, 140,  46, 271, 140,   4, 270, 162,  98, 257, 113,\n",
       "        147, 279, 142, 153, 260,   9, 160, 265, 128, 156, 264,  93,  46,\n",
       "        263,   4,  98, 259,  62,  98, 261, 113,  22, 269, 136, 109, 266,\n",
       "         23, 156, 267, 155,   1, 262,  98,   8, 258,  98, 109,  62,  74,\n",
       "         98,  46,  62,  86,  66,  34, 113,  44, 109,  11,  23,  86, 121,\n",
       "         84,  77,  93,  57,  86,  66,  98,  78,  86, 113,  98, 112,  31,\n",
       "         41,  86,  86,  98,  66,  46,  29, 113,  74,  27,  44,  57,  46,\n",
       "         31,  46,  57,  43,  78, 128, 135,  11,  19,  66,  98,  15, 145,\n",
       "        142, 126, 147,  55,  78,  23,  34, 136, 145, 113,  66, 122,  46,\n",
       "        156, 162,  57,  74,  34, 147,  93,  26, 152,  93, 128, 128,   2,\n",
       "        154,   9, 151,  34, 113, 136, 136,  27, 133,  66, 144,  29, 134,\n",
       "         86, 132]),\n",
       " 'split0_test_f1_micro': array([0.763, 0.77 , 0.775, 0.758, 0.754, 0.732, 0.764, 0.763, 0.741,\n",
       "        0.727, 0.74 , 0.737, 0.763, 0.771, 0.78 , 0.763, 0.76 , 0.754,\n",
       "        0.731, 0.752, 0.756, 0.771, 0.749, 0.746, 0.765, 0.767, 0.766,\n",
       "        0.764, 0.762, 0.743, 0.754, 0.743, 0.752, 0.736, 0.748, 0.756,\n",
       "        0.763, 0.775, 0.756, 0.763, 0.76 , 0.749, 0.761, 0.759, 0.758,\n",
       "        0.757, 0.762, 0.749, 0.777, 0.776, 0.768, 0.749, 0.754, 0.755,\n",
       "        0.755, 0.75 , 0.751, 0.724, 0.738, 0.729, 0.76 , 0.77 , 0.77 ,\n",
       "        0.739, 0.768, 0.758, 0.742, 0.741, 0.747, 0.748, 0.733, 0.747,\n",
       "        0.764, 0.783, 0.773, 0.761, 0.753, 0.765, 0.747, 0.752, 0.748,\n",
       "        0.752, 0.741, 0.744, 0.774, 0.771, 0.76 , 0.766, 0.769, 0.756,\n",
       "        0.744, 0.768, 0.772, 0.744, 0.743, 0.763, 0.776, 0.502, 0.778,\n",
       "        0.776, 0.502, 0.777, 0.778, 0.507, 0.778, 0.776, 0.498, 0.778,\n",
       "        0.773, 0.502, 0.776, 0.777, 0.501, 0.772, 0.778, 0.498, 0.781,\n",
       "        0.774, 0.529, 0.78 , 0.782, 0.502, 0.78 , 0.778, 0.476, 0.776,\n",
       "        0.777, 0.498, 0.776, 0.782, 0.51 , 0.776, 0.776, 0.502, 0.776,\n",
       "        0.778, 0.5  , 0.775, 0.778, 0.498, 0.78 , 0.782, 0.498, 0.781,\n",
       "        0.767, 0.562, 0.774, 0.781, 0.612, 0.782, 0.777, 0.706, 0.773,\n",
       "        0.776, 0.635, 0.77 , 0.774, 0.654, 0.771, 0.775, 0.651, 0.774,\n",
       "        0.772, 0.62 , 0.773, 0.773, 0.733, 0.777, 0.777, 0.697, 0.783,\n",
       "        0.775, 0.615, 0.784, 0.779, 0.693, 0.779, 0.779, 0.648, 0.775,\n",
       "        0.773, 0.599, 0.782, 0.777, 0.535, 0.773, 0.776, 0.703, 0.776,\n",
       "        0.78 , 0.769, 0.779, 0.776, 0.78 , 0.779, 0.779, 0.78 , 0.78 ,\n",
       "        0.779, 0.779, 0.778, 0.774, 0.776, 0.776, 0.78 , 0.783, 0.778,\n",
       "        0.775, 0.775, 0.778, 0.775, 0.776, 0.776, 0.777, 0.776, 0.779,\n",
       "        0.777, 0.775, 0.774, 0.778, 0.779, 0.78 , 0.777, 0.778, 0.778,\n",
       "        0.777, 0.777, 0.778, 0.777, 0.778, 0.779, 0.777, 0.779, 0.779,\n",
       "        0.78 , 0.776, 0.776, 0.778, 0.778, 0.775, 0.779, 0.786, 0.779,\n",
       "        0.774, 0.771, 0.777, 0.775, 0.766, 0.765, 0.774, 0.766, 0.766,\n",
       "        0.781, 0.776, 0.778, 0.774, 0.775, 0.779, 0.779, 0.788, 0.767,\n",
       "        0.777, 0.776, 0.773, 0.781, 0.776, 0.777, 0.78 , 0.773, 0.773,\n",
       "        0.772, 0.77 , 0.782, 0.77 , 0.774, 0.77 , 0.774, 0.78 , 0.774,\n",
       "        0.78 , 0.775, 0.775, 0.777, 0.766, 0.778, 0.769, 0.777, 0.777]),\n",
       " 'split1_test_f1_micro': array([0.795, 0.795, 0.781, 0.786, 0.777, 0.761, 0.764, 0.757, 0.775,\n",
       "        0.748, 0.775, 0.76 , 0.792, 0.788, 0.787, 0.755, 0.759, 0.768,\n",
       "        0.767, 0.76 , 0.736, 0.748, 0.75 , 0.742, 0.779, 0.794, 0.778,\n",
       "        0.778, 0.77 , 0.762, 0.77 , 0.769, 0.772, 0.758, 0.748, 0.761,\n",
       "        0.782, 0.788, 0.789, 0.781, 0.775, 0.775, 0.765, 0.775, 0.781,\n",
       "        0.765, 0.75 , 0.754, 0.776, 0.78 , 0.784, 0.766, 0.785, 0.754,\n",
       "        0.755, 0.768, 0.77 , 0.755, 0.749, 0.739, 0.787, 0.785, 0.793,\n",
       "        0.775, 0.78 , 0.757, 0.761, 0.753, 0.761, 0.741, 0.735, 0.746,\n",
       "        0.785, 0.785, 0.782, 0.786, 0.764, 0.761, 0.768, 0.762, 0.746,\n",
       "        0.748, 0.759, 0.763, 0.783, 0.78 , 0.79 , 0.773, 0.772, 0.766,\n",
       "        0.772, 0.761, 0.761, 0.733, 0.751, 0.754, 0.795, 0.284, 0.797,\n",
       "        0.791, 0.502, 0.792, 0.792, 0.504, 0.796, 0.791, 0.498, 0.793,\n",
       "        0.791, 0.522, 0.792, 0.793, 0.695, 0.793, 0.792, 0.502, 0.791,\n",
       "        0.795, 0.522, 0.793, 0.791, 0.496, 0.793, 0.79 , 0.496, 0.799,\n",
       "        0.79 , 0.371, 0.791, 0.793, 0.504, 0.793, 0.792, 0.576, 0.793,\n",
       "        0.793, 0.498, 0.793, 0.794, 0.501, 0.791, 0.791, 0.496, 0.793,\n",
       "        0.791, 0.651, 0.791, 0.784, 0.596, 0.792, 0.801, 0.662, 0.789,\n",
       "        0.797, 0.688, 0.797, 0.793, 0.483, 0.79 , 0.792, 0.66 , 0.8  ,\n",
       "        0.784, 0.55 , 0.796, 0.787, 0.563, 0.793, 0.79 , 0.558, 0.795,\n",
       "        0.794, 0.637, 0.794, 0.793, 0.507, 0.788, 0.795, 0.692, 0.794,\n",
       "        0.794, 0.697, 0.791, 0.795, 0.664, 0.79 , 0.798, 0.472, 0.796,\n",
       "        0.794, 0.595, 0.79 , 0.793, 0.792, 0.791, 0.791, 0.792, 0.792,\n",
       "        0.791, 0.791, 0.793, 0.793, 0.793, 0.791, 0.799, 0.791, 0.791,\n",
       "        0.791, 0.793, 0.793, 0.793, 0.792, 0.79 , 0.791, 0.792, 0.791,\n",
       "        0.793, 0.792, 0.794, 0.791, 0.793, 0.793, 0.79 , 0.79 , 0.789,\n",
       "        0.789, 0.79 , 0.793, 0.792, 0.791, 0.796, 0.794, 0.792, 0.793,\n",
       "        0.791, 0.793, 0.792, 0.792, 0.794, 0.792, 0.787, 0.791, 0.794,\n",
       "        0.794, 0.796, 0.792, 0.785, 0.792, 0.793, 0.784, 0.791, 0.795,\n",
       "        0.792, 0.795, 0.789, 0.79 , 0.788, 0.787, 0.792, 0.794, 0.792,\n",
       "        0.794, 0.789, 0.787, 0.794, 0.801, 0.788, 0.795, 0.786, 0.788,\n",
       "        0.787, 0.792, 0.791, 0.793, 0.793, 0.788, 0.796, 0.794, 0.786,\n",
       "        0.791, 0.795, 0.792, 0.794, 0.789, 0.793, 0.792, 0.787, 0.794]),\n",
       " 'split2_test_f1_micro': array([0.796, 0.792, 0.796, 0.793, 0.777, 0.775, 0.759, 0.785, 0.788,\n",
       "        0.743, 0.762, 0.767, 0.804, 0.791, 0.796, 0.788, 0.773, 0.791,\n",
       "        0.784, 0.774, 0.777, 0.766, 0.78 , 0.777, 0.796, 0.792, 0.808,\n",
       "        0.784, 0.778, 0.782, 0.777, 0.771, 0.781, 0.762, 0.751, 0.76 ,\n",
       "        0.789, 0.791, 0.797, 0.782, 0.788, 0.784, 0.767, 0.778, 0.785,\n",
       "        0.78 , 0.772, 0.775, 0.802, 0.792, 0.797, 0.772, 0.781, 0.784,\n",
       "        0.774, 0.759, 0.783, 0.767, 0.758, 0.769, 0.783, 0.793, 0.785,\n",
       "        0.789, 0.791, 0.786, 0.785, 0.768, 0.756, 0.756, 0.743, 0.769,\n",
       "        0.779, 0.804, 0.796, 0.786, 0.783, 0.786, 0.753, 0.77 , 0.77 ,\n",
       "        0.776, 0.76 , 0.763, 0.783, 0.791, 0.799, 0.793, 0.804, 0.769,\n",
       "        0.77 , 0.782, 0.772, 0.76 , 0.775, 0.782, 0.802, 0.491, 0.806,\n",
       "        0.797, 0.5  , 0.798, 0.795, 0.5  , 0.794, 0.793, 0.711, 0.793,\n",
       "        0.797, 0.62 , 0.793, 0.802, 0.544, 0.799, 0.798, 0.498, 0.796,\n",
       "        0.8  , 0.502, 0.801, 0.8  , 0.709, 0.803, 0.798, 0.493, 0.799,\n",
       "        0.8  , 0.502, 0.797, 0.794, 0.502, 0.799, 0.792, 0.547, 0.804,\n",
       "        0.797, 0.516, 0.796, 0.798, 0.462, 0.795, 0.801, 0.502, 0.797,\n",
       "        0.807, 0.619, 0.8  , 0.8  , 0.482, 0.796, 0.811, 0.635, 0.799,\n",
       "        0.799, 0.703, 0.806, 0.791, 0.661, 0.805, 0.798, 0.703, 0.805,\n",
       "        0.799, 0.522, 0.802, 0.796, 0.722, 0.797, 0.802, 0.639, 0.81 ,\n",
       "        0.803, 0.738, 0.793, 0.796, 0.685, 0.803, 0.804, 0.328, 0.798,\n",
       "        0.803, 0.672, 0.806, 0.797, 0.514, 0.792, 0.805, 0.608, 0.8  ,\n",
       "        0.803, 0.768, 0.802, 0.798, 0.799, 0.801, 0.798, 0.801, 0.8  ,\n",
       "        0.8  , 0.8  , 0.8  , 0.8  , 0.804, 0.8  , 0.802, 0.806, 0.801,\n",
       "        0.801, 0.8  , 0.8  , 0.797, 0.802, 0.803, 0.801, 0.802, 0.8  ,\n",
       "        0.8  , 0.8  , 0.798, 0.8  , 0.801, 0.799, 0.802, 0.802, 0.8  ,\n",
       "        0.801, 0.803, 0.802, 0.796, 0.799, 0.799, 0.8  , 0.799, 0.8  ,\n",
       "        0.801, 0.801, 0.802, 0.801, 0.8  , 0.801, 0.8  , 0.803, 0.807,\n",
       "        0.806, 0.803, 0.807, 0.803, 0.808, 0.8  , 0.805, 0.817, 0.811,\n",
       "        0.799, 0.802, 0.796, 0.808, 0.803, 0.808, 0.799, 0.801, 0.8  ,\n",
       "        0.8  , 0.81 , 0.811, 0.804, 0.795, 0.801, 0.804, 0.807, 0.813,\n",
       "        0.809, 0.805, 0.812, 0.794, 0.807, 0.805, 0.805, 0.799, 0.805,\n",
       "        0.796, 0.801, 0.806, 0.8  , 0.806, 0.806, 0.808, 0.803, 0.8  ]),\n",
       " 'split3_test_f1_micro': array([0.786, 0.79 , 0.79 , 0.774, 0.787, 0.781, 0.768, 0.761, 0.77 ,\n",
       "        0.741, 0.758, 0.76 , 0.797, 0.787, 0.79 , 0.785, 0.765, 0.777,\n",
       "        0.756, 0.771, 0.773, 0.751, 0.756, 0.772, 0.792, 0.79 , 0.785,\n",
       "        0.779, 0.771, 0.78 , 0.771, 0.754, 0.775, 0.759, 0.76 , 0.763,\n",
       "        0.771, 0.789, 0.791, 0.787, 0.783, 0.791, 0.78 , 0.767, 0.763,\n",
       "        0.766, 0.774, 0.754, 0.795, 0.789, 0.79 , 0.74 , 0.776, 0.776,\n",
       "        0.761, 0.776, 0.758, 0.76 , 0.765, 0.724, 0.777, 0.777, 0.796,\n",
       "        0.779, 0.758, 0.785, 0.763, 0.764, 0.774, 0.754, 0.771, 0.775,\n",
       "        0.79 , 0.779, 0.795, 0.782, 0.785, 0.783, 0.788, 0.767, 0.77 ,\n",
       "        0.758, 0.756, 0.764, 0.786, 0.791, 0.789, 0.775, 0.774, 0.772,\n",
       "        0.752, 0.774, 0.758, 0.747, 0.769, 0.743, 0.796, 0.498, 0.799,\n",
       "        0.794, 0.314, 0.804, 0.799, 0.498, 0.802, 0.801, 0.503, 0.799,\n",
       "        0.802, 0.502, 0.798, 0.802, 0.496, 0.8  , 0.8  , 0.498, 0.797,\n",
       "        0.799, 0.555, 0.801, 0.801, 0.44 , 0.796, 0.8  , 0.284, 0.794,\n",
       "        0.8  , 0.499, 0.802, 0.802, 0.498, 0.795, 0.805, 0.498, 0.799,\n",
       "        0.801, 0.605, 0.802, 0.802, 0.55 , 0.795, 0.803, 0.482, 0.8  ,\n",
       "        0.796, 0.663, 0.788, 0.801, 0.532, 0.788, 0.8  , 0.354, 0.793,\n",
       "        0.791, 0.732, 0.789, 0.798, 0.321, 0.787, 0.789, 0.658, 0.799,\n",
       "        0.793, 0.686, 0.789, 0.789, 0.606, 0.796, 0.798, 0.728, 0.789,\n",
       "        0.793, 0.575, 0.796, 0.798, 0.589, 0.791, 0.798, 0.712, 0.789,\n",
       "        0.79 , 0.472, 0.795, 0.787, 0.718, 0.793, 0.801, 0.661, 0.792,\n",
       "        0.794, 0.603, 0.793, 0.793, 0.794, 0.795, 0.795, 0.792, 0.791,\n",
       "        0.793, 0.797, 0.796, 0.79 , 0.796, 0.793, 0.794, 0.795, 0.793,\n",
       "        0.792, 0.793, 0.791, 0.796, 0.793, 0.794, 0.794, 0.79 , 0.792,\n",
       "        0.791, 0.793, 0.796, 0.79 , 0.795, 0.796, 0.794, 0.793, 0.793,\n",
       "        0.796, 0.798, 0.797, 0.794, 0.795, 0.796, 0.794, 0.794, 0.796,\n",
       "        0.796, 0.797, 0.794, 0.796, 0.793, 0.79 , 0.787, 0.796, 0.789,\n",
       "        0.794, 0.792, 0.8  , 0.791, 0.791, 0.799, 0.792, 0.797, 0.792,\n",
       "        0.795, 0.796, 0.794, 0.793, 0.793, 0.797, 0.796, 0.785, 0.794,\n",
       "        0.786, 0.797, 0.796, 0.792, 0.785, 0.797, 0.789, 0.783, 0.794,\n",
       "        0.794, 0.793, 0.792, 0.802, 0.799, 0.792, 0.788, 0.786, 0.795,\n",
       "        0.789, 0.796, 0.786, 0.794, 0.792, 0.79 , 0.792, 0.797, 0.797]),\n",
       " 'split4_test_f1_micro': array([0.79 , 0.796, 0.797, 0.764, 0.777, 0.774, 0.758, 0.763, 0.752,\n",
       "        0.763, 0.76 , 0.764, 0.793, 0.79 , 0.792, 0.794, 0.77 , 0.787,\n",
       "        0.777, 0.763, 0.762, 0.744, 0.751, 0.758, 0.799, 0.791, 0.798,\n",
       "        0.767, 0.778, 0.775, 0.77 , 0.769, 0.759, 0.757, 0.759, 0.745,\n",
       "        0.796, 0.787, 0.789, 0.78 , 0.77 , 0.796, 0.771, 0.779, 0.772,\n",
       "        0.765, 0.755, 0.765, 0.784, 0.794, 0.783, 0.765, 0.781, 0.796,\n",
       "        0.773, 0.749, 0.767, 0.748, 0.732, 0.771, 0.793, 0.794, 0.793,\n",
       "        0.754, 0.771, 0.78 , 0.76 , 0.75 , 0.75 , 0.75 , 0.744, 0.736,\n",
       "        0.802, 0.786, 0.798, 0.785, 0.766, 0.777, 0.752, 0.773, 0.764,\n",
       "        0.766, 0.751, 0.746, 0.778, 0.806, 0.803, 0.779, 0.773, 0.802,\n",
       "        0.756, 0.761, 0.77 , 0.767, 0.762, 0.759, 0.795, 0.502, 0.798,\n",
       "        0.799, 0.566, 0.796, 0.8  , 0.622, 0.802, 0.801, 0.511, 0.8  ,\n",
       "        0.8  , 0.498, 0.798, 0.797, 0.386, 0.797, 0.8  , 0.536, 0.802,\n",
       "        0.797, 0.503, 0.801, 0.797, 0.498, 0.801, 0.799, 0.497, 0.798,\n",
       "        0.799, 0.513, 0.8  , 0.802, 0.502, 0.794, 0.795, 0.502, 0.795,\n",
       "        0.798, 0.47 , 0.792, 0.8  , 0.502, 0.802, 0.799, 0.498, 0.801,\n",
       "        0.796, 0.556, 0.799, 0.8  , 0.666, 0.794, 0.788, 0.561, 0.789,\n",
       "        0.797, 0.669, 0.796, 0.794, 0.483, 0.799, 0.792, 0.55 , 0.796,\n",
       "        0.796, 0.717, 0.796, 0.8  , 0.484, 0.798, 0.799, 0.533, 0.8  ,\n",
       "        0.795, 0.745, 0.798, 0.794, 0.689, 0.797, 0.794, 0.571, 0.797,\n",
       "        0.799, 0.622, 0.796, 0.789, 0.629, 0.797, 0.805, 0.718, 0.796,\n",
       "        0.804, 0.652, 0.796, 0.799, 0.8  , 0.798, 0.797, 0.801, 0.802,\n",
       "        0.799, 0.797, 0.8  , 0.801, 0.797, 0.799, 0.798, 0.795, 0.799,\n",
       "        0.799, 0.801, 0.801, 0.8  , 0.802, 0.799, 0.801, 0.8  , 0.801,\n",
       "        0.801, 0.798, 0.798, 0.8  , 0.8  , 0.799, 0.799, 0.799, 0.8  ,\n",
       "        0.801, 0.798, 0.798, 0.799, 0.801, 0.799, 0.801, 0.801, 0.798,\n",
       "        0.8  , 0.799, 0.801, 0.8  , 0.798, 0.798, 0.801, 0.797, 0.803,\n",
       "        0.796, 0.798, 0.796, 0.797, 0.795, 0.8  , 0.795, 0.795, 0.799,\n",
       "        0.803, 0.798, 0.796, 0.786, 0.799, 0.793, 0.791, 0.798, 0.792,\n",
       "        0.786, 0.793, 0.797, 0.796, 0.793, 0.798, 0.801, 0.799, 0.793,\n",
       "        0.794, 0.796, 0.803, 0.787, 0.801, 0.793, 0.804, 0.799, 0.793,\n",
       "        0.797, 0.802, 0.796, 0.799, 0.798, 0.801, 0.794, 0.798, 0.787]),\n",
       " 'mean_test_f1_micro': array([0.786 , 0.7886, 0.7878, 0.775 , 0.7744, 0.7646, 0.7626, 0.7658,\n",
       "        0.7652, 0.7444, 0.759 , 0.7576, 0.7898, 0.7854, 0.789 , 0.777 ,\n",
       "        0.7654, 0.7754, 0.763 , 0.764 , 0.7608, 0.756 , 0.7572, 0.759 ,\n",
       "        0.7862, 0.7868, 0.787 , 0.7744, 0.7718, 0.7684, 0.7684, 0.7612,\n",
       "        0.7678, 0.7544, 0.7532, 0.757 , 0.7802, 0.786 , 0.7844, 0.7786,\n",
       "        0.7752, 0.779 , 0.7688, 0.7716, 0.7718, 0.7666, 0.7626, 0.7594,\n",
       "        0.7868, 0.7862, 0.7844, 0.7584, 0.7754, 0.773 , 0.7636, 0.7604,\n",
       "        0.7658, 0.7508, 0.7484, 0.7464, 0.78  , 0.7838, 0.7874, 0.7672,\n",
       "        0.7736, 0.7732, 0.7622, 0.7552, 0.7576, 0.7498, 0.7452, 0.7546,\n",
       "        0.784 , 0.7874, 0.7888, 0.78  , 0.7702, 0.7744, 0.7616, 0.7648,\n",
       "        0.7596, 0.76  , 0.7534, 0.756 , 0.7808, 0.7878, 0.7882, 0.7772,\n",
       "        0.7784, 0.773 , 0.7588, 0.7692, 0.7666, 0.7502, 0.76  , 0.7602,\n",
       "        0.7928, 0.4554, 0.7956, 0.7914, 0.4768, 0.7934, 0.7928, 0.5262,\n",
       "        0.7944, 0.7924, 0.5442, 0.7926, 0.7926, 0.5288, 0.7914, 0.7942,\n",
       "        0.5244, 0.7922, 0.7936, 0.5064, 0.7934, 0.793 , 0.5222, 0.7952,\n",
       "        0.7942, 0.529 , 0.7946, 0.793 , 0.4492, 0.7932, 0.7932, 0.4766,\n",
       "        0.7932, 0.7946, 0.5032, 0.7914, 0.792 , 0.525 , 0.7934, 0.7934,\n",
       "        0.5178, 0.7916, 0.7944, 0.5026, 0.7926, 0.7952, 0.4952, 0.7944,\n",
       "        0.7914, 0.6102, 0.7904, 0.7932, 0.5776, 0.7904, 0.7954, 0.5836,\n",
       "        0.7886, 0.792 , 0.6854, 0.7916, 0.79  , 0.5204, 0.7904, 0.7892,\n",
       "        0.6444, 0.7948, 0.7888, 0.619 , 0.7912, 0.789 , 0.6216, 0.7922,\n",
       "        0.7932, 0.631 , 0.7954, 0.792 , 0.662 , 0.793 , 0.792 , 0.6326,\n",
       "        0.7916, 0.794 , 0.5902, 0.7906, 0.7918, 0.6124, 0.794 , 0.789 ,\n",
       "        0.612 , 0.789 , 0.797 , 0.6324, 0.792 , 0.795 , 0.6774, 0.792 ,\n",
       "        0.7918, 0.793 , 0.7928, 0.792 , 0.7932, 0.793 , 0.7924, 0.7928,\n",
       "        0.7934, 0.7916, 0.7932, 0.7918, 0.7946, 0.794 , 0.7924, 0.7916,\n",
       "        0.7924, 0.7926, 0.7922, 0.793 , 0.7924, 0.7928, 0.792 , 0.7926,\n",
       "        0.7924, 0.7916, 0.792 , 0.7918, 0.7936, 0.7934, 0.7924, 0.7924,\n",
       "        0.792 , 0.7928, 0.7932, 0.7936, 0.7916, 0.7928, 0.7938, 0.7932,\n",
       "        0.793 , 0.7932, 0.7936, 0.7932, 0.793 , 0.7934, 0.7926, 0.7912,\n",
       "        0.7908, 0.7946, 0.7944, 0.7928, 0.792 , 0.7944, 0.7902, 0.7904,\n",
       "        0.7914, 0.79  , 0.7932, 0.7926, 0.794 , 0.7934, 0.7906, 0.7902,\n",
       "        0.7916, 0.7928, 0.7914, 0.7932, 0.789 , 0.7886, 0.793 , 0.7928,\n",
       "        0.7934, 0.79  , 0.7922, 0.7938, 0.7896, 0.7922, 0.7912, 0.7912,\n",
       "        0.796 , 0.7892, 0.7948, 0.7896, 0.7934, 0.7916, 0.7906, 0.7906,\n",
       "        0.7938, 0.791 , 0.7928, 0.7902, 0.7936, 0.791 , 0.7924, 0.791 ]),\n",
       " 'std_test_f1_micro': array([0.0120499 , 0.00954149, 0.00856505, 0.01308434, 0.01091055,\n",
       "        0.01755677, 0.00366606, 0.00984683, 0.01672603, 0.01162067,\n",
       "        0.01120714, 0.01063203, 0.01404849, 0.00733757, 0.00536656,\n",
       "        0.0151921 , 0.0054626 , 0.01336563, 0.01857956, 0.00787401,\n",
       "        0.0144969 , 0.01056409, 0.01165161, 0.01379855, 0.01260793,\n",
       "        0.00998799, 0.01475127, 0.00760526, 0.00594643, 0.01448586,\n",
       "        0.00765768, 0.01096175, 0.01068457, 0.00935094, 0.00526878,\n",
       "        0.00641872, 0.01192309, 0.00565685, 0.01449966, 0.00816333,\n",
       "        0.00982649, 0.01657709, 0.0064622 , 0.00757892, 0.0102645 ,\n",
       "        0.0074458 , 0.00932952, 0.00939361, 0.01018627, 0.00699714,\n",
       "        0.00960417, 0.0119432 , 0.01107429, 0.01639512, 0.00838093,\n",
       "        0.01040385, 0.01090688, 0.01477024, 0.01220819, 0.01987561,\n",
       "        0.0112783 , 0.00923905, 0.0094361 , 0.01813725, 0.01118213,\n",
       "        0.01298307, 0.01367333, 0.00974474, 0.0095205 , 0.00523068,\n",
       "        0.0136    , 0.01484049, 0.01253794, 0.00863944, 0.00970361,\n",
       "        0.00961249, 0.01212271, 0.00983056, 0.01494791, 0.00735935,\n",
       "        0.01053755, 0.01003992, 0.0069455 , 0.0090111 , 0.00426146,\n",
       "        0.01178813, 0.01506519, 0.00895321, 0.01290891, 0.01546609,\n",
       "        0.01070327, 0.00803492, 0.00591946, 0.01202331, 0.0116619 ,\n",
       "        0.01279687, 0.00879545, 0.08579417, 0.00935094, 0.00816333,\n",
       "        0.08516901, 0.00906863, 0.00793473, 0.04800167, 0.0088    ,\n",
       "        0.00915642, 0.08353538, 0.00786384, 0.01048046, 0.0463655 ,\n",
       "        0.0080895 , 0.00923905, 0.0999972 , 0.01038075, 0.00833307,\n",
       "        0.01488086, 0.00711618, 0.00965401, 0.01948743, 0.00820731,\n",
       "        0.00702567, 0.09284396, 0.00811419, 0.00829458, 0.08294914,\n",
       "        0.00879545, 0.00893085, 0.05306826, 0.00936803, 0.00736478,\n",
       "        0.00391918, 0.00796492, 0.00931665, 0.03121538, 0.0094784 ,\n",
       "        0.00811419, 0.0460452 , 0.00900222, 0.00861626, 0.02799714,\n",
       "        0.00722772, 0.00775629, 0.00688186, 0.00725534, 0.01327554,\n",
       "        0.0442511 , 0.00939361, 0.00879545, 0.06411739, 0.00496387,\n",
       "        0.01174053, 0.12409448, 0.00861626, 0.00843801, 0.03254904,\n",
       "        0.01207642, 0.00831865, 0.12662954, 0.01162067, 0.00767854,\n",
       "        0.05063043, 0.0107963 , 0.0097857 , 0.0751851 , 0.00998799,\n",
       "        0.00927362, 0.09497494, 0.00778203, 0.00901998, 0.07584458,\n",
       "        0.00926499, 0.00920869, 0.06792349, 0.00481664, 0.00672309,\n",
       "        0.07382574, 0.0081388 , 0.00827043, 0.13975464, 0.00840476,\n",
       "        0.01038075, 0.07834947, 0.00777174, 0.00704273, 0.0771518 ,\n",
       "        0.00831865, 0.01082589, 0.08882927, 0.00839047, 0.00862554,\n",
       "        0.0769015 , 0.00761577, 0.0082801 , 0.00715542, 0.00765245,\n",
       "        0.0069282 , 0.00773046, 0.00779744, 0.00752596, 0.007494  ,\n",
       "        0.0081388 , 0.00972831, 0.00932523, 0.00861162, 0.00773563,\n",
       "        0.00742967, 0.0080895 , 0.00915642, 0.00932952, 0.00826075,\n",
       "        0.00888594, 0.00950789, 0.00930806, 0.00881816, 0.00920869,\n",
       "        0.00791454, 0.00861626, 0.0088227 , 0.0091214 , 0.00810925,\n",
       "        0.00788923, 0.00705975, 0.00873155, 0.00835703, 0.00817313,\n",
       "        0.00904212, 0.00910824, 0.00830903, 0.00765768, 0.00815843,\n",
       "        0.00752064, 0.00861162, 0.0077201 , 0.00746726, 0.00765768,\n",
       "        0.00899778, 0.00933809, 0.00833307, 0.00773563, 0.00901998,\n",
       "        0.00844748, 0.00574804, 0.009992  , 0.0104    , 0.01108152,\n",
       "        0.01001199, 0.00968297, 0.01363231, 0.01345511, 0.01044988,\n",
       "        0.01630215, 0.01478648, 0.00748331, 0.00902441, 0.0068    ,\n",
       "        0.01099818, 0.00974885, 0.0097242 , 0.00682935, 0.00597997,\n",
       "        0.0113842 , 0.00783837, 0.01104536, 0.01252837, 0.00741889,\n",
       "        0.00867179, 0.00874986, 0.00861162, 0.01202664, 0.01282809,\n",
       "        0.01199   , 0.01154816, 0.01041153, 0.01072194, 0.01132078,\n",
       "        0.01132431, 0.01148216, 0.00749933, 0.01028786, 0.00608605,\n",
       "        0.0097857 , 0.01031504, 0.0082801 , 0.01342237, 0.00964572,\n",
       "        0.01252198, 0.00928655, 0.00822192]),\n",
       " 'rank_test_f1_micro': array([175, 162, 166, 194, 195, 219, 223, 214, 217, 256, 235, 239, 150,\n",
       "        177, 155, 190, 216, 191, 222, 220, 228, 243, 241, 235, 174, 171,\n",
       "        170, 197, 202, 208, 208, 227, 210, 247, 249, 242, 183, 175, 178,\n",
       "        187, 193, 186, 207, 204, 202, 212, 223, 234, 171, 173, 178, 238,\n",
       "        191, 200, 221, 229, 215, 250, 253, 254, 184, 181, 168, 211, 198,\n",
       "        199, 225, 245, 239, 252, 255, 246, 180, 169, 160, 184, 205, 195,\n",
       "        226, 218, 233, 231, 248, 243, 182, 166, 165, 189, 188, 201, 237,\n",
       "        206, 213, 251, 231, 230,  66, 287,   3, 122, 285,  35,  66, 275,\n",
       "         16,  84, 272,  77,  77, 274, 122,  21, 277,  94,  29, 281,  35,\n",
       "         58, 278,   6,  20, 273,  14,  58, 288,  50,  45, 286,  45,  11,\n",
       "        282, 127, 100, 276,  41,  35, 280, 113,  16, 283,  81,   7, 284,\n",
       "         15, 122, 268, 140,  44, 271, 140,   4, 270, 162, 100, 257, 113,\n",
       "        147, 279, 142, 153, 260,   9, 160, 265, 128, 155, 264,  94,  50,\n",
       "        263,   4, 100, 259,  57, 100, 261, 113,  22, 269, 136, 109, 266,\n",
       "         24, 155, 267, 155,   1, 262,  98,   8, 258, 100, 109,  58,  74,\n",
       "        100,  50,  58,  86,  66,  34, 113,  45, 109,  11,  24,  86, 121,\n",
       "         84,  77,  93,  58,  86,  66, 100,  81,  86, 113, 100, 109,  30,\n",
       "         41,  86,  86,  98,  66,  50,  30, 113,  74,  27,  45,  58,  50,\n",
       "         30,  50,  58,  41,  77, 128, 135,  11,  16,  66, 100,  16, 145,\n",
       "        142, 122, 147,  56,  81,  22,  35, 136, 145, 113,  66, 122,  45,\n",
       "        155, 162,  58,  74,  35, 147,  94,  26, 152,  94, 128, 128,   2,\n",
       "        154,   9, 151,  35, 113, 136, 136,  27, 133,  66, 144,  30, 134,\n",
       "         86, 132]),\n",
       " 'split0_test_precision_micro': array([0.763, 0.77 , 0.775, 0.758, 0.754, 0.732, 0.764, 0.763, 0.741,\n",
       "        0.727, 0.74 , 0.737, 0.763, 0.771, 0.78 , 0.763, 0.76 , 0.754,\n",
       "        0.731, 0.752, 0.756, 0.771, 0.749, 0.746, 0.765, 0.767, 0.766,\n",
       "        0.764, 0.762, 0.743, 0.754, 0.743, 0.752, 0.736, 0.748, 0.756,\n",
       "        0.763, 0.775, 0.756, 0.763, 0.76 , 0.749, 0.761, 0.759, 0.758,\n",
       "        0.757, 0.762, 0.749, 0.777, 0.776, 0.768, 0.749, 0.754, 0.755,\n",
       "        0.755, 0.75 , 0.751, 0.724, 0.738, 0.729, 0.76 , 0.77 , 0.77 ,\n",
       "        0.739, 0.768, 0.758, 0.742, 0.741, 0.747, 0.748, 0.733, 0.747,\n",
       "        0.764, 0.783, 0.773, 0.761, 0.753, 0.765, 0.747, 0.752, 0.748,\n",
       "        0.752, 0.741, 0.744, 0.774, 0.771, 0.76 , 0.766, 0.769, 0.756,\n",
       "        0.744, 0.768, 0.772, 0.744, 0.743, 0.763, 0.776, 0.502, 0.778,\n",
       "        0.776, 0.502, 0.777, 0.778, 0.507, 0.778, 0.776, 0.498, 0.778,\n",
       "        0.773, 0.502, 0.776, 0.777, 0.501, 0.772, 0.778, 0.498, 0.781,\n",
       "        0.774, 0.529, 0.78 , 0.782, 0.502, 0.78 , 0.778, 0.476, 0.776,\n",
       "        0.777, 0.498, 0.776, 0.782, 0.51 , 0.776, 0.776, 0.502, 0.776,\n",
       "        0.778, 0.5  , 0.775, 0.778, 0.498, 0.78 , 0.782, 0.498, 0.781,\n",
       "        0.767, 0.562, 0.774, 0.781, 0.612, 0.782, 0.777, 0.706, 0.773,\n",
       "        0.776, 0.635, 0.77 , 0.774, 0.654, 0.771, 0.775, 0.651, 0.774,\n",
       "        0.772, 0.62 , 0.773, 0.773, 0.733, 0.777, 0.777, 0.697, 0.783,\n",
       "        0.775, 0.615, 0.784, 0.779, 0.693, 0.779, 0.779, 0.648, 0.775,\n",
       "        0.773, 0.599, 0.782, 0.777, 0.535, 0.773, 0.776, 0.703, 0.776,\n",
       "        0.78 , 0.769, 0.779, 0.776, 0.78 , 0.779, 0.779, 0.78 , 0.78 ,\n",
       "        0.779, 0.779, 0.778, 0.774, 0.776, 0.776, 0.78 , 0.783, 0.778,\n",
       "        0.775, 0.775, 0.778, 0.775, 0.776, 0.776, 0.777, 0.776, 0.779,\n",
       "        0.777, 0.775, 0.774, 0.778, 0.779, 0.78 , 0.777, 0.778, 0.778,\n",
       "        0.777, 0.777, 0.778, 0.777, 0.778, 0.779, 0.777, 0.779, 0.779,\n",
       "        0.78 , 0.776, 0.776, 0.778, 0.778, 0.775, 0.779, 0.786, 0.779,\n",
       "        0.774, 0.771, 0.777, 0.775, 0.766, 0.765, 0.774, 0.766, 0.766,\n",
       "        0.781, 0.776, 0.778, 0.774, 0.775, 0.779, 0.779, 0.788, 0.767,\n",
       "        0.777, 0.776, 0.773, 0.781, 0.776, 0.777, 0.78 , 0.773, 0.773,\n",
       "        0.772, 0.77 , 0.782, 0.77 , 0.774, 0.77 , 0.774, 0.78 , 0.774,\n",
       "        0.78 , 0.775, 0.775, 0.777, 0.766, 0.778, 0.769, 0.777, 0.777]),\n",
       " 'split1_test_precision_micro': array([0.795, 0.795, 0.781, 0.786, 0.777, 0.761, 0.764, 0.757, 0.775,\n",
       "        0.748, 0.775, 0.76 , 0.792, 0.788, 0.787, 0.755, 0.759, 0.768,\n",
       "        0.767, 0.76 , 0.736, 0.748, 0.75 , 0.742, 0.779, 0.794, 0.778,\n",
       "        0.778, 0.77 , 0.762, 0.77 , 0.769, 0.772, 0.758, 0.748, 0.761,\n",
       "        0.782, 0.788, 0.789, 0.781, 0.775, 0.775, 0.765, 0.775, 0.781,\n",
       "        0.765, 0.75 , 0.754, 0.776, 0.78 , 0.784, 0.766, 0.785, 0.754,\n",
       "        0.755, 0.768, 0.77 , 0.755, 0.749, 0.739, 0.787, 0.785, 0.793,\n",
       "        0.775, 0.78 , 0.757, 0.761, 0.753, 0.761, 0.741, 0.735, 0.746,\n",
       "        0.785, 0.785, 0.782, 0.786, 0.764, 0.761, 0.768, 0.762, 0.746,\n",
       "        0.748, 0.759, 0.763, 0.783, 0.78 , 0.79 , 0.773, 0.772, 0.766,\n",
       "        0.772, 0.761, 0.761, 0.733, 0.751, 0.754, 0.795, 0.284, 0.797,\n",
       "        0.791, 0.502, 0.792, 0.792, 0.504, 0.796, 0.791, 0.498, 0.793,\n",
       "        0.791, 0.522, 0.792, 0.793, 0.695, 0.793, 0.792, 0.502, 0.791,\n",
       "        0.795, 0.522, 0.793, 0.791, 0.496, 0.793, 0.79 , 0.496, 0.799,\n",
       "        0.79 , 0.371, 0.791, 0.793, 0.504, 0.793, 0.792, 0.576, 0.793,\n",
       "        0.793, 0.498, 0.793, 0.794, 0.501, 0.791, 0.791, 0.496, 0.793,\n",
       "        0.791, 0.651, 0.791, 0.784, 0.596, 0.792, 0.801, 0.662, 0.789,\n",
       "        0.797, 0.688, 0.797, 0.793, 0.483, 0.79 , 0.792, 0.66 , 0.8  ,\n",
       "        0.784, 0.55 , 0.796, 0.787, 0.563, 0.793, 0.79 , 0.558, 0.795,\n",
       "        0.794, 0.637, 0.794, 0.793, 0.507, 0.788, 0.795, 0.692, 0.794,\n",
       "        0.794, 0.697, 0.791, 0.795, 0.664, 0.79 , 0.798, 0.472, 0.796,\n",
       "        0.794, 0.595, 0.79 , 0.793, 0.792, 0.791, 0.791, 0.792, 0.792,\n",
       "        0.791, 0.791, 0.793, 0.793, 0.793, 0.791, 0.799, 0.791, 0.791,\n",
       "        0.791, 0.793, 0.793, 0.793, 0.792, 0.79 , 0.791, 0.792, 0.791,\n",
       "        0.793, 0.792, 0.794, 0.791, 0.793, 0.793, 0.79 , 0.79 , 0.789,\n",
       "        0.789, 0.79 , 0.793, 0.792, 0.791, 0.796, 0.794, 0.792, 0.793,\n",
       "        0.791, 0.793, 0.792, 0.792, 0.794, 0.792, 0.787, 0.791, 0.794,\n",
       "        0.794, 0.796, 0.792, 0.785, 0.792, 0.793, 0.784, 0.791, 0.795,\n",
       "        0.792, 0.795, 0.789, 0.79 , 0.788, 0.787, 0.792, 0.794, 0.792,\n",
       "        0.794, 0.789, 0.787, 0.794, 0.801, 0.788, 0.795, 0.786, 0.788,\n",
       "        0.787, 0.792, 0.791, 0.793, 0.793, 0.788, 0.796, 0.794, 0.786,\n",
       "        0.791, 0.795, 0.792, 0.794, 0.789, 0.793, 0.792, 0.787, 0.794]),\n",
       " 'split2_test_precision_micro': array([0.796, 0.792, 0.796, 0.793, 0.777, 0.775, 0.759, 0.785, 0.788,\n",
       "        0.743, 0.762, 0.767, 0.804, 0.791, 0.796, 0.788, 0.773, 0.791,\n",
       "        0.784, 0.774, 0.777, 0.766, 0.78 , 0.777, 0.796, 0.792, 0.808,\n",
       "        0.784, 0.778, 0.782, 0.777, 0.771, 0.781, 0.762, 0.751, 0.76 ,\n",
       "        0.789, 0.791, 0.797, 0.782, 0.788, 0.784, 0.767, 0.778, 0.785,\n",
       "        0.78 , 0.772, 0.775, 0.802, 0.792, 0.797, 0.772, 0.781, 0.784,\n",
       "        0.774, 0.759, 0.783, 0.767, 0.758, 0.769, 0.783, 0.793, 0.785,\n",
       "        0.789, 0.791, 0.786, 0.785, 0.768, 0.756, 0.756, 0.743, 0.769,\n",
       "        0.779, 0.804, 0.796, 0.786, 0.783, 0.786, 0.753, 0.77 , 0.77 ,\n",
       "        0.776, 0.76 , 0.763, 0.783, 0.791, 0.799, 0.793, 0.804, 0.769,\n",
       "        0.77 , 0.782, 0.772, 0.76 , 0.775, 0.782, 0.802, 0.491, 0.806,\n",
       "        0.797, 0.5  , 0.798, 0.795, 0.5  , 0.794, 0.793, 0.711, 0.793,\n",
       "        0.797, 0.62 , 0.793, 0.802, 0.544, 0.799, 0.798, 0.498, 0.796,\n",
       "        0.8  , 0.502, 0.801, 0.8  , 0.709, 0.803, 0.798, 0.493, 0.799,\n",
       "        0.8  , 0.502, 0.797, 0.794, 0.502, 0.799, 0.792, 0.547, 0.804,\n",
       "        0.797, 0.516, 0.796, 0.798, 0.462, 0.795, 0.801, 0.502, 0.797,\n",
       "        0.807, 0.619, 0.8  , 0.8  , 0.482, 0.796, 0.811, 0.635, 0.799,\n",
       "        0.799, 0.703, 0.806, 0.791, 0.661, 0.805, 0.798, 0.703, 0.805,\n",
       "        0.799, 0.522, 0.802, 0.796, 0.722, 0.797, 0.802, 0.639, 0.81 ,\n",
       "        0.803, 0.738, 0.793, 0.796, 0.685, 0.803, 0.804, 0.328, 0.798,\n",
       "        0.803, 0.672, 0.806, 0.797, 0.514, 0.792, 0.805, 0.608, 0.8  ,\n",
       "        0.803, 0.768, 0.802, 0.798, 0.799, 0.801, 0.798, 0.801, 0.8  ,\n",
       "        0.8  , 0.8  , 0.8  , 0.8  , 0.804, 0.8  , 0.802, 0.806, 0.801,\n",
       "        0.801, 0.8  , 0.8  , 0.797, 0.802, 0.803, 0.801, 0.802, 0.8  ,\n",
       "        0.8  , 0.8  , 0.798, 0.8  , 0.801, 0.799, 0.802, 0.802, 0.8  ,\n",
       "        0.801, 0.803, 0.802, 0.796, 0.799, 0.799, 0.8  , 0.799, 0.8  ,\n",
       "        0.801, 0.801, 0.802, 0.801, 0.8  , 0.801, 0.8  , 0.803, 0.807,\n",
       "        0.806, 0.803, 0.807, 0.803, 0.808, 0.8  , 0.805, 0.817, 0.811,\n",
       "        0.799, 0.802, 0.796, 0.808, 0.803, 0.808, 0.799, 0.801, 0.8  ,\n",
       "        0.8  , 0.81 , 0.811, 0.804, 0.795, 0.801, 0.804, 0.807, 0.813,\n",
       "        0.809, 0.805, 0.812, 0.794, 0.807, 0.805, 0.805, 0.799, 0.805,\n",
       "        0.796, 0.801, 0.806, 0.8  , 0.806, 0.806, 0.808, 0.803, 0.8  ]),\n",
       " 'split3_test_precision_micro': array([0.786, 0.79 , 0.79 , 0.774, 0.787, 0.781, 0.768, 0.761, 0.77 ,\n",
       "        0.741, 0.758, 0.76 , 0.797, 0.787, 0.79 , 0.785, 0.765, 0.777,\n",
       "        0.756, 0.771, 0.773, 0.751, 0.756, 0.772, 0.792, 0.79 , 0.785,\n",
       "        0.779, 0.771, 0.78 , 0.771, 0.754, 0.775, 0.759, 0.76 , 0.763,\n",
       "        0.771, 0.789, 0.791, 0.787, 0.783, 0.791, 0.78 , 0.767, 0.763,\n",
       "        0.766, 0.774, 0.754, 0.795, 0.789, 0.79 , 0.74 , 0.776, 0.776,\n",
       "        0.761, 0.776, 0.758, 0.76 , 0.765, 0.724, 0.777, 0.777, 0.796,\n",
       "        0.779, 0.758, 0.785, 0.763, 0.764, 0.774, 0.754, 0.771, 0.775,\n",
       "        0.79 , 0.779, 0.795, 0.782, 0.785, 0.783, 0.788, 0.767, 0.77 ,\n",
       "        0.758, 0.756, 0.764, 0.786, 0.791, 0.789, 0.775, 0.774, 0.772,\n",
       "        0.752, 0.774, 0.758, 0.747, 0.769, 0.743, 0.796, 0.498, 0.799,\n",
       "        0.794, 0.314, 0.804, 0.799, 0.498, 0.802, 0.801, 0.503, 0.799,\n",
       "        0.802, 0.502, 0.798, 0.802, 0.496, 0.8  , 0.8  , 0.498, 0.797,\n",
       "        0.799, 0.555, 0.801, 0.801, 0.44 , 0.796, 0.8  , 0.284, 0.794,\n",
       "        0.8  , 0.499, 0.802, 0.802, 0.498, 0.795, 0.805, 0.498, 0.799,\n",
       "        0.801, 0.605, 0.802, 0.802, 0.55 , 0.795, 0.803, 0.482, 0.8  ,\n",
       "        0.796, 0.663, 0.788, 0.801, 0.532, 0.788, 0.8  , 0.354, 0.793,\n",
       "        0.791, 0.732, 0.789, 0.798, 0.321, 0.787, 0.789, 0.658, 0.799,\n",
       "        0.793, 0.686, 0.789, 0.789, 0.606, 0.796, 0.798, 0.728, 0.789,\n",
       "        0.793, 0.575, 0.796, 0.798, 0.589, 0.791, 0.798, 0.712, 0.789,\n",
       "        0.79 , 0.472, 0.795, 0.787, 0.718, 0.793, 0.801, 0.661, 0.792,\n",
       "        0.794, 0.603, 0.793, 0.793, 0.794, 0.795, 0.795, 0.792, 0.791,\n",
       "        0.793, 0.797, 0.796, 0.79 , 0.796, 0.793, 0.794, 0.795, 0.793,\n",
       "        0.792, 0.793, 0.791, 0.796, 0.793, 0.794, 0.794, 0.79 , 0.792,\n",
       "        0.791, 0.793, 0.796, 0.79 , 0.795, 0.796, 0.794, 0.793, 0.793,\n",
       "        0.796, 0.798, 0.797, 0.794, 0.795, 0.796, 0.794, 0.794, 0.796,\n",
       "        0.796, 0.797, 0.794, 0.796, 0.793, 0.79 , 0.787, 0.796, 0.789,\n",
       "        0.794, 0.792, 0.8  , 0.791, 0.791, 0.799, 0.792, 0.797, 0.792,\n",
       "        0.795, 0.796, 0.794, 0.793, 0.793, 0.797, 0.796, 0.785, 0.794,\n",
       "        0.786, 0.797, 0.796, 0.792, 0.785, 0.797, 0.789, 0.783, 0.794,\n",
       "        0.794, 0.793, 0.792, 0.802, 0.799, 0.792, 0.788, 0.786, 0.795,\n",
       "        0.789, 0.796, 0.786, 0.794, 0.792, 0.79 , 0.792, 0.797, 0.797]),\n",
       " 'split4_test_precision_micro': array([0.79 , 0.796, 0.797, 0.764, 0.777, 0.774, 0.758, 0.763, 0.752,\n",
       "        0.763, 0.76 , 0.764, 0.793, 0.79 , 0.792, 0.794, 0.77 , 0.787,\n",
       "        0.777, 0.763, 0.762, 0.744, 0.751, 0.758, 0.799, 0.791, 0.798,\n",
       "        0.767, 0.778, 0.775, 0.77 , 0.769, 0.759, 0.757, 0.759, 0.745,\n",
       "        0.796, 0.787, 0.789, 0.78 , 0.77 , 0.796, 0.771, 0.779, 0.772,\n",
       "        0.765, 0.755, 0.765, 0.784, 0.794, 0.783, 0.765, 0.781, 0.796,\n",
       "        0.773, 0.749, 0.767, 0.748, 0.732, 0.771, 0.793, 0.794, 0.793,\n",
       "        0.754, 0.771, 0.78 , 0.76 , 0.75 , 0.75 , 0.75 , 0.744, 0.736,\n",
       "        0.802, 0.786, 0.798, 0.785, 0.766, 0.777, 0.752, 0.773, 0.764,\n",
       "        0.766, 0.751, 0.746, 0.778, 0.806, 0.803, 0.779, 0.773, 0.802,\n",
       "        0.756, 0.761, 0.77 , 0.767, 0.762, 0.759, 0.795, 0.502, 0.798,\n",
       "        0.799, 0.566, 0.796, 0.8  , 0.622, 0.802, 0.801, 0.511, 0.8  ,\n",
       "        0.8  , 0.498, 0.798, 0.797, 0.386, 0.797, 0.8  , 0.536, 0.802,\n",
       "        0.797, 0.503, 0.801, 0.797, 0.498, 0.801, 0.799, 0.497, 0.798,\n",
       "        0.799, 0.513, 0.8  , 0.802, 0.502, 0.794, 0.795, 0.502, 0.795,\n",
       "        0.798, 0.47 , 0.792, 0.8  , 0.502, 0.802, 0.799, 0.498, 0.801,\n",
       "        0.796, 0.556, 0.799, 0.8  , 0.666, 0.794, 0.788, 0.561, 0.789,\n",
       "        0.797, 0.669, 0.796, 0.794, 0.483, 0.799, 0.792, 0.55 , 0.796,\n",
       "        0.796, 0.717, 0.796, 0.8  , 0.484, 0.798, 0.799, 0.533, 0.8  ,\n",
       "        0.795, 0.745, 0.798, 0.794, 0.689, 0.797, 0.794, 0.571, 0.797,\n",
       "        0.799, 0.622, 0.796, 0.789, 0.629, 0.797, 0.805, 0.718, 0.796,\n",
       "        0.804, 0.652, 0.796, 0.799, 0.8  , 0.798, 0.797, 0.801, 0.802,\n",
       "        0.799, 0.797, 0.8  , 0.801, 0.797, 0.799, 0.798, 0.795, 0.799,\n",
       "        0.799, 0.801, 0.801, 0.8  , 0.802, 0.799, 0.801, 0.8  , 0.801,\n",
       "        0.801, 0.798, 0.798, 0.8  , 0.8  , 0.799, 0.799, 0.799, 0.8  ,\n",
       "        0.801, 0.798, 0.798, 0.799, 0.801, 0.799, 0.801, 0.801, 0.798,\n",
       "        0.8  , 0.799, 0.801, 0.8  , 0.798, 0.798, 0.801, 0.797, 0.803,\n",
       "        0.796, 0.798, 0.796, 0.797, 0.795, 0.8  , 0.795, 0.795, 0.799,\n",
       "        0.803, 0.798, 0.796, 0.786, 0.799, 0.793, 0.791, 0.798, 0.792,\n",
       "        0.786, 0.793, 0.797, 0.796, 0.793, 0.798, 0.801, 0.799, 0.793,\n",
       "        0.794, 0.796, 0.803, 0.787, 0.801, 0.793, 0.804, 0.799, 0.793,\n",
       "        0.797, 0.802, 0.796, 0.799, 0.798, 0.801, 0.794, 0.798, 0.787]),\n",
       " 'mean_test_precision_micro': array([0.786 , 0.7886, 0.7878, 0.775 , 0.7744, 0.7646, 0.7626, 0.7658,\n",
       "        0.7652, 0.7444, 0.759 , 0.7576, 0.7898, 0.7854, 0.789 , 0.777 ,\n",
       "        0.7654, 0.7754, 0.763 , 0.764 , 0.7608, 0.756 , 0.7572, 0.759 ,\n",
       "        0.7862, 0.7868, 0.787 , 0.7744, 0.7718, 0.7684, 0.7684, 0.7612,\n",
       "        0.7678, 0.7544, 0.7532, 0.757 , 0.7802, 0.786 , 0.7844, 0.7786,\n",
       "        0.7752, 0.779 , 0.7688, 0.7716, 0.7718, 0.7666, 0.7626, 0.7594,\n",
       "        0.7868, 0.7862, 0.7844, 0.7584, 0.7754, 0.773 , 0.7636, 0.7604,\n",
       "        0.7658, 0.7508, 0.7484, 0.7464, 0.78  , 0.7838, 0.7874, 0.7672,\n",
       "        0.7736, 0.7732, 0.7622, 0.7552, 0.7576, 0.7498, 0.7452, 0.7546,\n",
       "        0.784 , 0.7874, 0.7888, 0.78  , 0.7702, 0.7744, 0.7616, 0.7648,\n",
       "        0.7596, 0.76  , 0.7534, 0.756 , 0.7808, 0.7878, 0.7882, 0.7772,\n",
       "        0.7784, 0.773 , 0.7588, 0.7692, 0.7666, 0.7502, 0.76  , 0.7602,\n",
       "        0.7928, 0.4554, 0.7956, 0.7914, 0.4768, 0.7934, 0.7928, 0.5262,\n",
       "        0.7944, 0.7924, 0.5442, 0.7926, 0.7926, 0.5288, 0.7914, 0.7942,\n",
       "        0.5244, 0.7922, 0.7936, 0.5064, 0.7934, 0.793 , 0.5222, 0.7952,\n",
       "        0.7942, 0.529 , 0.7946, 0.793 , 0.4492, 0.7932, 0.7932, 0.4766,\n",
       "        0.7932, 0.7946, 0.5032, 0.7914, 0.792 , 0.525 , 0.7934, 0.7934,\n",
       "        0.5178, 0.7916, 0.7944, 0.5026, 0.7926, 0.7952, 0.4952, 0.7944,\n",
       "        0.7914, 0.6102, 0.7904, 0.7932, 0.5776, 0.7904, 0.7954, 0.5836,\n",
       "        0.7886, 0.792 , 0.6854, 0.7916, 0.79  , 0.5204, 0.7904, 0.7892,\n",
       "        0.6444, 0.7948, 0.7888, 0.619 , 0.7912, 0.789 , 0.6216, 0.7922,\n",
       "        0.7932, 0.631 , 0.7954, 0.792 , 0.662 , 0.793 , 0.792 , 0.6326,\n",
       "        0.7916, 0.794 , 0.5902, 0.7906, 0.7918, 0.6124, 0.794 , 0.789 ,\n",
       "        0.612 , 0.789 , 0.797 , 0.6324, 0.792 , 0.795 , 0.6774, 0.792 ,\n",
       "        0.7918, 0.793 , 0.7928, 0.792 , 0.7932, 0.793 , 0.7924, 0.7928,\n",
       "        0.7934, 0.7916, 0.7932, 0.7918, 0.7946, 0.794 , 0.7924, 0.7916,\n",
       "        0.7924, 0.7926, 0.7922, 0.793 , 0.7924, 0.7928, 0.792 , 0.7926,\n",
       "        0.7924, 0.7916, 0.792 , 0.7918, 0.7936, 0.7934, 0.7924, 0.7924,\n",
       "        0.792 , 0.7928, 0.7932, 0.7936, 0.7916, 0.7928, 0.7938, 0.7932,\n",
       "        0.793 , 0.7932, 0.7936, 0.7932, 0.793 , 0.7934, 0.7926, 0.7912,\n",
       "        0.7908, 0.7946, 0.7944, 0.7928, 0.792 , 0.7944, 0.7902, 0.7904,\n",
       "        0.7914, 0.79  , 0.7932, 0.7926, 0.794 , 0.7934, 0.7906, 0.7902,\n",
       "        0.7916, 0.7928, 0.7914, 0.7932, 0.789 , 0.7886, 0.793 , 0.7928,\n",
       "        0.7934, 0.79  , 0.7922, 0.7938, 0.7896, 0.7922, 0.7912, 0.7912,\n",
       "        0.796 , 0.7892, 0.7948, 0.7896, 0.7934, 0.7916, 0.7906, 0.7906,\n",
       "        0.7938, 0.791 , 0.7928, 0.7902, 0.7936, 0.791 , 0.7924, 0.791 ]),\n",
       " 'std_test_precision_micro': array([0.0120499 , 0.00954149, 0.00856505, 0.01308434, 0.01091055,\n",
       "        0.01755677, 0.00366606, 0.00984683, 0.01672603, 0.01162067,\n",
       "        0.01120714, 0.01063203, 0.01404849, 0.00733757, 0.00536656,\n",
       "        0.0151921 , 0.0054626 , 0.01336563, 0.01857956, 0.00787401,\n",
       "        0.0144969 , 0.01056409, 0.01165161, 0.01379855, 0.01260793,\n",
       "        0.00998799, 0.01475127, 0.00760526, 0.00594643, 0.01448586,\n",
       "        0.00765768, 0.01096175, 0.01068457, 0.00935094, 0.00526878,\n",
       "        0.00641872, 0.01192309, 0.00565685, 0.01449966, 0.00816333,\n",
       "        0.00982649, 0.01657709, 0.0064622 , 0.00757892, 0.0102645 ,\n",
       "        0.0074458 , 0.00932952, 0.00939361, 0.01018627, 0.00699714,\n",
       "        0.00960417, 0.0119432 , 0.01107429, 0.01639512, 0.00838093,\n",
       "        0.01040385, 0.01090688, 0.01477024, 0.01220819, 0.01987561,\n",
       "        0.0112783 , 0.00923905, 0.0094361 , 0.01813725, 0.01118213,\n",
       "        0.01298307, 0.01367333, 0.00974474, 0.0095205 , 0.00523068,\n",
       "        0.0136    , 0.01484049, 0.01253794, 0.00863944, 0.00970361,\n",
       "        0.00961249, 0.01212271, 0.00983056, 0.01494791, 0.00735935,\n",
       "        0.01053755, 0.01003992, 0.0069455 , 0.0090111 , 0.00426146,\n",
       "        0.01178813, 0.01506519, 0.00895321, 0.01290891, 0.01546609,\n",
       "        0.01070327, 0.00803492, 0.00591946, 0.01202331, 0.0116619 ,\n",
       "        0.01279687, 0.00879545, 0.08579417, 0.00935094, 0.00816333,\n",
       "        0.08516901, 0.00906863, 0.00793473, 0.04800167, 0.0088    ,\n",
       "        0.00915642, 0.08353538, 0.00786384, 0.01048046, 0.0463655 ,\n",
       "        0.0080895 , 0.00923905, 0.0999972 , 0.01038075, 0.00833307,\n",
       "        0.01488086, 0.00711618, 0.00965401, 0.01948743, 0.00820731,\n",
       "        0.00702567, 0.09284396, 0.00811419, 0.00829458, 0.08294914,\n",
       "        0.00879545, 0.00893085, 0.05306826, 0.00936803, 0.00736478,\n",
       "        0.00391918, 0.00796492, 0.00931665, 0.03121538, 0.0094784 ,\n",
       "        0.00811419, 0.0460452 , 0.00900222, 0.00861626, 0.02799714,\n",
       "        0.00722772, 0.00775629, 0.00688186, 0.00725534, 0.01327554,\n",
       "        0.0442511 , 0.00939361, 0.00879545, 0.06411739, 0.00496387,\n",
       "        0.01174053, 0.12409448, 0.00861626, 0.00843801, 0.03254904,\n",
       "        0.01207642, 0.00831865, 0.12662954, 0.01162067, 0.00767854,\n",
       "        0.05063043, 0.0107963 , 0.0097857 , 0.0751851 , 0.00998799,\n",
       "        0.00927362, 0.09497494, 0.00778203, 0.00901998, 0.07584458,\n",
       "        0.00926499, 0.00920869, 0.06792349, 0.00481664, 0.00672309,\n",
       "        0.07382574, 0.0081388 , 0.00827043, 0.13975464, 0.00840476,\n",
       "        0.01038075, 0.07834947, 0.00777174, 0.00704273, 0.0771518 ,\n",
       "        0.00831865, 0.01082589, 0.08882927, 0.00839047, 0.00862554,\n",
       "        0.0769015 , 0.00761577, 0.0082801 , 0.00715542, 0.00765245,\n",
       "        0.0069282 , 0.00773046, 0.00779744, 0.00752596, 0.007494  ,\n",
       "        0.0081388 , 0.00972831, 0.00932523, 0.00861162, 0.00773563,\n",
       "        0.00742967, 0.0080895 , 0.00915642, 0.00932952, 0.00826075,\n",
       "        0.00888594, 0.00950789, 0.00930806, 0.00881816, 0.00920869,\n",
       "        0.00791454, 0.00861626, 0.0088227 , 0.0091214 , 0.00810925,\n",
       "        0.00788923, 0.00705975, 0.00873155, 0.00835703, 0.00817313,\n",
       "        0.00904212, 0.00910824, 0.00830903, 0.00765768, 0.00815843,\n",
       "        0.00752064, 0.00861162, 0.0077201 , 0.00746726, 0.00765768,\n",
       "        0.00899778, 0.00933809, 0.00833307, 0.00773563, 0.00901998,\n",
       "        0.00844748, 0.00574804, 0.009992  , 0.0104    , 0.01108152,\n",
       "        0.01001199, 0.00968297, 0.01363231, 0.01345511, 0.01044988,\n",
       "        0.01630215, 0.01478648, 0.00748331, 0.00902441, 0.0068    ,\n",
       "        0.01099818, 0.00974885, 0.0097242 , 0.00682935, 0.00597997,\n",
       "        0.0113842 , 0.00783837, 0.01104536, 0.01252837, 0.00741889,\n",
       "        0.00867179, 0.00874986, 0.00861162, 0.01202664, 0.01282809,\n",
       "        0.01199   , 0.01154816, 0.01041153, 0.01072194, 0.01132078,\n",
       "        0.01132431, 0.01148216, 0.00749933, 0.01028786, 0.00608605,\n",
       "        0.0097857 , 0.01031504, 0.0082801 , 0.01342237, 0.00964572,\n",
       "        0.01252198, 0.00928655, 0.00822192]),\n",
       " 'rank_test_precision_micro': array([175, 162, 166, 194, 195, 219, 223, 214, 217, 256, 235, 239, 150,\n",
       "        177, 156, 190, 216, 191, 222, 220, 228, 243, 241, 235, 174, 172,\n",
       "        170, 197, 202, 208, 208, 227, 210, 247, 249, 242, 183, 175, 178,\n",
       "        187, 193, 186, 207, 204, 202, 212, 223, 234, 171, 173, 178, 238,\n",
       "        191, 200, 221, 229, 215, 250, 253, 254, 184, 181, 168, 211, 198,\n",
       "        199, 225, 245, 239, 252, 255, 246, 180, 169, 160, 184, 205, 195,\n",
       "        226, 218, 233, 231, 248, 243, 182, 166, 165, 189, 188, 201, 237,\n",
       "        206, 213, 251, 231, 230,  66, 287,   3, 122, 285,  34,  66, 275,\n",
       "         15,  84, 272,  78,  78, 274, 122,  21, 277,  93,  31, 281,  34,\n",
       "         62, 278,   6,  20, 273,  14,  57, 288,  46,  55, 286,  46,  11,\n",
       "        282, 126,  98, 276,  41,  34, 280, 113,  15, 283,  78,   7, 284,\n",
       "         15, 122, 268, 140,  46, 271, 140,   4, 270, 162,  98, 257, 113,\n",
       "        147, 279, 142, 153, 260,   9, 160, 265, 128, 156, 264,  93,  46,\n",
       "        263,   4,  98, 259,  62,  98, 261, 113,  22, 269, 136, 109, 266,\n",
       "         23, 156, 267, 155,   1, 262,  98,   8, 258,  98, 109,  62,  74,\n",
       "         98,  46,  62,  86,  66,  34, 113,  44, 109,  11,  23,  86, 121,\n",
       "         84,  77,  93,  57,  86,  66,  98,  78,  86, 113,  98, 112,  31,\n",
       "         41,  86,  86,  98,  66,  46,  29, 113,  74,  27,  44,  57,  46,\n",
       "         31,  46,  57,  43,  78, 128, 135,  11,  19,  66,  98,  15, 145,\n",
       "        142, 126, 147,  55,  78,  23,  34, 136, 145, 113,  66, 122,  46,\n",
       "        156, 162,  57,  74,  34, 147,  93,  26, 152,  93, 128, 128,   2,\n",
       "        154,   9, 151,  34, 113, 136, 136,  27, 133,  66, 144,  29, 134,\n",
       "         86, 132]),\n",
       " 'split0_test_roc_auc_ovo': array([0.86151778, 0.86186979, 0.86632586, 0.84238148, 0.84150546,\n",
       "        0.82244116, 0.84476552, 0.84226548, 0.82752524, 0.8186211 ,\n",
       "        0.81470504, 0.81518104, 0.8623858 , 0.86586185, 0.86968591,\n",
       "        0.84820157, 0.85511368, 0.85235764, 0.83328933, 0.84009344,\n",
       "        0.8436815 , 0.84664155, 0.83069729, 0.83192531, 0.86180979,\n",
       "        0.8623498 , 0.86139778, 0.85230164, 0.84452151, 0.84269348,\n",
       "        0.83634938, 0.84108546, 0.84084145, 0.82863726, 0.83705339,\n",
       "        0.83257732, 0.86134578, 0.86743388, 0.86036977, 0.86070577,\n",
       "        0.85394566, 0.84730956, 0.84784557, 0.84770156, 0.84168947,\n",
       "        0.84524552, 0.84189347, 0.83001728, 0.86661787, 0.86470184,\n",
       "        0.85828973, 0.84438551, 0.84738156, 0.84446751, 0.82886526,\n",
       "        0.83197931, 0.83476136, 0.80578489, 0.82644922, 0.80696091,\n",
       "        0.85720972, 0.8685859 , 0.86325381, 0.84684555, 0.83971744,\n",
       "        0.85468567, 0.83384534, 0.83610938, 0.83415735, 0.83259332,\n",
       "        0.81496504, 0.82993728, 0.8622098 , 0.86489784, 0.86432583,\n",
       "        0.85657371, 0.84720556, 0.84622954, 0.82940527, 0.84469352,\n",
       "        0.82721724, 0.83960943, 0.82583321, 0.82761724, 0.86106578,\n",
       "        0.8627738 , 0.86500984, 0.8502976 , 0.85533769, 0.85046161,\n",
       "        0.84522552, 0.83526936, 0.85208563, 0.82444519, 0.83616938,\n",
       "        0.84060545, 0.87216995, 0.31407303, 0.87409799, 0.87224996,\n",
       "        0.51382422, 0.87254196, 0.87356598, 0.76459623, 0.87409399,\n",
       "        0.87335397, 0.71742348, 0.87357798, 0.87269796, 0.79828477,\n",
       "        0.874894  , 0.87251796, 0.80829293, 0.87314997, 0.87212995,\n",
       "        0.49089985, 0.87384998, 0.874798  , 0.53004048, 0.87282597,\n",
       "        0.87558201, 0.77909247, 0.87555401, 0.87142994, 0.41762268,\n",
       "        0.87404998, 0.87430199, 0.76821629, 0.87226596, 0.87406999,\n",
       "        0.66631466, 0.87332197, 0.87221796, 0.52150834, 0.87581001,\n",
       "        0.874762  , 0.38660619, 0.87450599, 0.87345398, 0.21379542,\n",
       "        0.87182595, 0.87577001, 0.60672171, 0.87418999, 0.86636586,\n",
       "        0.61378582, 0.86948991, 0.86986592, 0.68248292, 0.87181795,\n",
       "        0.87293797, 0.79242868, 0.87266596, 0.87597002, 0.67766284,\n",
       "        0.8684739 , 0.87006992, 0.73829581, 0.87007792, 0.87188195,\n",
       "        0.74077985, 0.87319797, 0.87204195, 0.66121458, 0.87033393,\n",
       "        0.87391398, 0.78961263, 0.87392198, 0.87149794, 0.7564801 ,\n",
       "        0.87174595, 0.87290597, 0.65953855, 0.87050193, 0.87422199,\n",
       "        0.76978832, 0.87311797, 0.87416199, 0.72650362, 0.87197395,\n",
       "        0.87082593, 0.64191827, 0.87035793, 0.87142994, 0.53407655,\n",
       "        0.87020992, 0.87268596, 0.7750084 , 0.87280596, 0.87585001,\n",
       "        0.85740572, 0.87371398, 0.87671003, 0.875154  , 0.87578201,\n",
       "        0.87617402, 0.87635802, 0.87612202, 0.87584201, 0.87592601,\n",
       "        0.87618202, 0.87565401, 0.87615002, 0.87599802, 0.87570601,\n",
       "        0.87664603, 0.87612602, 0.87614602, 0.87579801, 0.87598602,\n",
       "        0.87590601, 0.87583801, 0.87538601, 0.87587401, 0.87545801,\n",
       "        0.87593401, 0.87585401, 0.87533401, 0.875086  , 0.87615802,\n",
       "        0.87585001, 0.87637802, 0.87604602, 0.87586201, 0.87588601,\n",
       "        0.87598202, 0.87625802, 0.87564601, 0.87593401, 0.87588601,\n",
       "        0.87586601, 0.87575801, 0.87597002, 0.87616202, 0.87621802,\n",
       "        0.87595802, 0.87595002, 0.87607802, 0.87613002, 0.87607002,\n",
       "        0.87463399, 0.874906  , 0.87300597, 0.8689619 , 0.8686939 ,\n",
       "        0.87006992, 0.86918191, 0.86527784, 0.86730588, 0.86765788,\n",
       "        0.86530184, 0.86978992, 0.87493   , 0.87458199, 0.87328997,\n",
       "        0.86910991, 0.87347398, 0.87062993, 0.8687379 , 0.87034193,\n",
       "        0.8688459 , 0.86676187, 0.86477384, 0.86984992, 0.87089393,\n",
       "        0.87145394, 0.87419399, 0.87152194, 0.87305397, 0.87118994,\n",
       "        0.86599386, 0.86821389, 0.86939791, 0.86612186, 0.86940591,\n",
       "        0.87026992, 0.87534601, 0.87368198, 0.8689659 , 0.87407799,\n",
       "        0.86991392, 0.87462199, 0.87182995, 0.86940191, 0.87028192,\n",
       "        0.86912191, 0.87162995, 0.86989792]),\n",
       " 'split1_test_roc_auc_ovo': array([0.87861006, 0.87738604, 0.87540201, 0.86407383, 0.84728956,\n",
       "        0.84450751, 0.83428935, 0.84834957, 0.85468167, 0.83831341,\n",
       "        0.84947759, 0.84000144, 0.87445399, 0.87312997, 0.86946991,\n",
       "        0.85916175, 0.84239348, 0.85751372, 0.84854158, 0.83353734,\n",
       "        0.83702539, 0.8377534 , 0.82720124, 0.83269732, 0.86955391,\n",
       "        0.87142994, 0.86342181, 0.86351782, 0.85512168, 0.84516152,\n",
       "        0.85709771, 0.84189747, 0.84874158, 0.84134946, 0.83570537,\n",
       "        0.84297349, 0.8685219 , 0.87219796, 0.86777388, 0.87140194,\n",
       "        0.8560057 , 0.86384982, 0.85462167, 0.85748972, 0.85497768,\n",
       "        0.84414551, 0.84498152, 0.83808141, 0.86498584, 0.86746188,\n",
       "        0.86938991, 0.85335365, 0.8627978 , 0.85135162, 0.8434495 ,\n",
       "        0.85039161, 0.84867158, 0.82893526, 0.82679723, 0.82931727,\n",
       "        0.87292197, 0.86448583, 0.87830205, 0.85514968, 0.85124562,\n",
       "        0.84043345, 0.84303349, 0.8250252 , 0.84081345, 0.81480904,\n",
       "        0.82190915, 0.82994928, 0.87551001, 0.87112594, 0.8686619 ,\n",
       "        0.8690379 , 0.84042545, 0.85436967, 0.8497376 , 0.85272164,\n",
       "        0.82354118, 0.83421735, 0.83630938, 0.83618938, 0.86983792,\n",
       "        0.87086993, 0.86910591, 0.86508184, 0.85746572, 0.86390982,\n",
       "        0.84815357, 0.84747756, 0.85093361, 0.83033329, 0.83337333,\n",
       "        0.83971344, 0.87763004, 0.21412743, 0.87565001, 0.87789005,\n",
       "        0.64339829, 0.87636202, 0.87789005, 0.71507544, 0.87707003,\n",
       "        0.87699003, 0.79744476, 0.87897406, 0.87785805, 0.65366646,\n",
       "        0.87687803, 0.87731004, 0.82944127, 0.87717003, 0.87650202,\n",
       "        0.50807213, 0.87795405, 0.87870606, 0.79844478, 0.87574201,\n",
       "        0.87726604, 0.37851806, 0.87721804, 0.87751804, 0.23085969,\n",
       "        0.87876606, 0.87702203, 0.28665259, 0.87668203, 0.87636202,\n",
       "        0.50202803, 0.87839005, 0.87801405, 0.6562985 , 0.87823805,\n",
       "        0.87584201, 0.77351238, 0.87818205, 0.87793005, 0.30444087,\n",
       "        0.875246  , 0.87579001, 0.34141746, 0.87818205, 0.87269796,\n",
       "        0.69163907, 0.87376998, 0.87725004, 0.62872206, 0.87454599,\n",
       "        0.87589401, 0.72135154, 0.87877006, 0.87857006, 0.74406391,\n",
       "        0.87810205, 0.87706203, 0.48247172, 0.875086  , 0.87631802,\n",
       "        0.74514392, 0.87679403, 0.87623402, 0.64662635, 0.87565401,\n",
       "        0.87659803, 0.65107042, 0.87722604, 0.87791805, 0.62918607,\n",
       "        0.87591001, 0.87600202, 0.68667899, 0.87755404, 0.87791805,\n",
       "        0.4625074 , 0.87753404, 0.87794205, 0.7627282 , 0.87951007,\n",
       "        0.875278  , 0.80767292, 0.87875406, 0.87744604, 0.73448375,\n",
       "        0.87611402, 0.87757404, 0.43841901, 0.87768604, 0.87698603,\n",
       "        0.6311981 , 0.87725004, 0.87891006, 0.87997008, 0.87946207,\n",
       "        0.88012208, 0.87994208, 0.87997808, 0.87946607, 0.87980208,\n",
       "        0.87977808, 0.88015808, 0.88033409, 0.88010208, 0.87876606,\n",
       "        0.87919407, 0.88002608, 0.88015008, 0.88051009, 0.88068609,\n",
       "        0.88071009, 0.88011408, 0.87987008, 0.88046209, 0.87956207,\n",
       "        0.87978608, 0.87957007, 0.87886206, 0.87987408, 0.88029808,\n",
       "        0.88006208, 0.87999408, 0.87982208, 0.88008608, 0.87987808,\n",
       "        0.87981808, 0.88026208, 0.88080609, 0.87989408, 0.88014208,\n",
       "        0.87995008, 0.87995008, 0.88016208, 0.88072209, 0.88025408,\n",
       "        0.88019808, 0.88036209, 0.88047409, 0.88067409, 0.88024608,\n",
       "        0.87633802, 0.87665403, 0.87762204, 0.87887406, 0.87537401,\n",
       "        0.87637402, 0.87396998, 0.87275796, 0.87837005, 0.87575001,\n",
       "        0.87305797, 0.87377398, 0.87885806, 0.87677003, 0.87750204,\n",
       "        0.87830605, 0.87575401, 0.87588601, 0.87288597, 0.87593401,\n",
       "        0.87581801, 0.87626602, 0.87114594, 0.875022  , 0.87817405,\n",
       "        0.87670203, 0.87699003, 0.87822605, 0.87773404, 0.87663403,\n",
       "        0.87690603, 0.87752604, 0.87624602, 0.87667403, 0.87630202,\n",
       "        0.875302  , 0.87897806, 0.87620202, 0.874986  , 0.87682603,\n",
       "        0.87879406, 0.87765004, 0.87921007, 0.87529   , 0.87799805,\n",
       "        0.87834205, 0.87837005, 0.87886606]),\n",
       " 'split2_test_roc_auc_ovo': array([0.87236596, 0.87949007, 0.87598602, 0.87269796, 0.87072593,\n",
       "        0.86595386, 0.85846574, 0.86328581, 0.86557785, 0.83489336,\n",
       "        0.84963759, 0.85464167, 0.8813181 , 0.88339413, 0.8812461 ,\n",
       "        0.87116994, 0.85464167, 0.86611386, 0.85657371, 0.85726172,\n",
       "        0.85966975, 0.84317749, 0.84594954, 0.86030976, 0.87570201,\n",
       "        0.88183811, 0.88007808, 0.86796589, 0.86538185, 0.86913391,\n",
       "        0.85760972, 0.85838573, 0.86348982, 0.84070145, 0.84600554,\n",
       "        0.84747356, 0.87781405, 0.87720604, 0.87835405, 0.87001392,\n",
       "        0.86472584, 0.86642186, 0.8563097 , 0.86925391, 0.86629786,\n",
       "        0.85941775, 0.85953375, 0.85974176, 0.88181011, 0.87839005,\n",
       "        0.87391398, 0.86609786, 0.86487784, 0.85847174, 0.85916175,\n",
       "        0.8497656 , 0.86436183, 0.83446935, 0.84808157, 0.84160147,\n",
       "        0.88058209, 0.88047009, 0.88039809, 0.86692587, 0.86927391,\n",
       "        0.87337797, 0.8624258 , 0.84622954, 0.85132562, 0.84156547,\n",
       "        0.83856142, 0.85256564, 0.87589801, 0.88279812, 0.87914207,\n",
       "        0.87074593, 0.87919007, 0.86761388, 0.83664539, 0.84914959,\n",
       "        0.86166179, 0.85312165, 0.84862158, 0.84099346, 0.874702  ,\n",
       "        0.88249012, 0.88461415, 0.87637002, 0.86993392, 0.86324581,\n",
       "        0.85846574, 0.86756188, 0.85162163, 0.84888158, 0.85705771,\n",
       "        0.85338165, 0.88579017, 0.21966351, 0.88670219, 0.88425415,\n",
       "        0.54232068, 0.88485816, 0.88365414, 0.42081873, 0.88539017,\n",
       "        0.88382214, 0.85415367, 0.88447815, 0.88412615, 0.7434599 ,\n",
       "        0.88407415, 0.88543017, 0.60719772, 0.88493416, 0.88483016,\n",
       "        0.37155394, 0.88638618, 0.88593417, 0.5248164 , 0.88439015,\n",
       "        0.88710219, 0.81112098, 0.88341013, 0.88419015, 0.26425623,\n",
       "        0.88593417, 0.88440615, 0.64134626, 0.88356614, 0.88371014,\n",
       "        0.56672107, 0.88453415, 0.88505816, 0.5689331 , 0.88903022,\n",
       "        0.88371014, 0.80798093, 0.88284213, 0.88458615, 0.43823501,\n",
       "        0.88457815, 0.88502216, 0.58964143, 0.88554617, 0.88499416,\n",
       "        0.65658251, 0.88379014, 0.88530216, 0.4874318 , 0.88533017,\n",
       "        0.88630218, 0.75815213, 0.88231012, 0.88401014, 0.82068913,\n",
       "        0.88656218, 0.88465815, 0.78231252, 0.88037409, 0.88377814,\n",
       "        0.7811485 , 0.88712219, 0.88342613, 0.60430967, 0.88257812,\n",
       "        0.88507416, 0.79638874, 0.88711819, 0.88657819, 0.68694699,\n",
       "        0.88672219, 0.88270612, 0.81664907, 0.88589017, 0.88495016,\n",
       "        0.7186395 , 0.88927823, 0.88825021, 0.2748484 , 0.88261812,\n",
       "        0.88499816, 0.7438159 , 0.88414615, 0.88841821, 0.55434887,\n",
       "        0.87758204, 0.88373014, 0.66989872, 0.8809661 , 0.88981424,\n",
       "        0.85944175, 0.88632618, 0.88679419, 0.8875862 , 0.88649418,\n",
       "        0.88718219, 0.8874022 , 0.88689819, 0.88692219, 0.8875302 ,\n",
       "        0.88705419, 0.8874262 , 0.8874502 , 0.88717019, 0.8873102 ,\n",
       "        0.8877262 , 0.88622618, 0.8876142 , 0.88662619, 0.8873622 ,\n",
       "        0.8872142 , 0.8875502 , 0.8873862 , 0.8872662 , 0.88716619,\n",
       "        0.8873062 , 0.88794621, 0.88701019, 0.88713419, 0.8874062 ,\n",
       "        0.8873022 , 0.8876502 , 0.8875982 , 0.88717819, 0.88701019,\n",
       "        0.8872982 , 0.8874062 , 0.8873982 , 0.8873742 , 0.88711819,\n",
       "        0.88695019, 0.8873902 , 0.8873342 , 0.8872702 , 0.88703419,\n",
       "        0.8872462 , 0.8873102 , 0.8873022 , 0.88717819, 0.8873742 ,\n",
       "        0.88527816, 0.88546217, 0.88716619, 0.88592217, 0.88678219,\n",
       "        0.8876822 , 0.88465815, 0.88590617, 0.88349814, 0.88409815,\n",
       "        0.88837021, 0.88641418, 0.88623418, 0.88633018, 0.88672619,\n",
       "        0.88657019, 0.88551417, 0.88913823, 0.88415015, 0.88857422,\n",
       "        0.88455415, 0.88573417, 0.88964623, 0.8875582 , 0.88661819,\n",
       "        0.88366614, 0.88366214, 0.8872342 , 0.8875942 , 0.88698619,\n",
       "        0.88876222, 0.88559817, 0.88860622, 0.88374614, 0.88827021,\n",
       "        0.8877942 , 0.88618618, 0.88651418, 0.88670619, 0.8874822 ,\n",
       "        0.88708219, 0.8873582 , 0.88666619, 0.88904222, 0.88543017,\n",
       "        0.88612618, 0.88603018, 0.88633018]),\n",
       " 'split3_test_roc_auc_ovo': array([0.87419799, 0.87873806, 0.87768204, 0.85686171, 0.85862174,\n",
       "        0.8685339 , 0.85125362, 0.84533753, 0.85769772, 0.82932527,\n",
       "        0.83794541, 0.84256548, 0.87069793, 0.87657403, 0.87690203,\n",
       "        0.86376982, 0.84214947, 0.85078561, 0.84481352, 0.85814173,\n",
       "        0.85234164, 0.83510936, 0.83921743, 0.84814557, 0.874998  ,\n",
       "        0.87374998, 0.87267396, 0.86407783, 0.86296181, 0.86002176,\n",
       "        0.85072961, 0.84165347, 0.85723772, 0.84459751, 0.84442951,\n",
       "        0.83594138, 0.87022592, 0.87565401, 0.8811341 , 0.8685899 ,\n",
       "        0.86215379, 0.8687619 , 0.86497384, 0.85967375, 0.85125362,\n",
       "        0.84754956, 0.84794557, 0.84886558, 0.87892606, 0.87321797,\n",
       "        0.87651002, 0.83831941, 0.85979776, 0.86095778, 0.85132562,\n",
       "        0.85253364, 0.8435975 , 0.84115746, 0.82310517, 0.81015696,\n",
       "        0.874734  , 0.87058993, 0.88291013, 0.84919359, 0.84706155,\n",
       "        0.85137762, 0.83289333, 0.8371974 , 0.85732972, 0.83835741,\n",
       "        0.83684539, 0.84490152, 0.87438199, 0.87348198, 0.87609402,\n",
       "        0.8627538 , 0.86029776, 0.85560169, 0.86458583, 0.84876558,\n",
       "        0.85234564, 0.83259332, 0.83406935, 0.84860958, 0.87448999,\n",
       "        0.88053809, 0.875186  , 0.86020576, 0.86089377, 0.85717771,\n",
       "        0.84854158, 0.84113346, 0.84782557, 0.82760524, 0.84266548,\n",
       "        0.83686539, 0.88298613, 0.80668091, 0.8814701 , 0.88249412,\n",
       "        0.16997072, 0.88321013, 0.88221012, 0.20084721, 0.88386214,\n",
       "        0.8810501 , 0.82849726, 0.88165411, 0.88302213, 0.76200019,\n",
       "        0.88229012, 0.88207811, 0.40257444, 0.88437015, 0.88241412,\n",
       "        0.44939119, 0.88279812, 0.88319813, 0.69754316, 0.8812341 ,\n",
       "        0.88157011, 0.15893454, 0.88452215, 0.8812741 , 0.21046337,\n",
       "        0.88171411, 0.8815301 , 0.52644442, 0.88380614, 0.88205411,\n",
       "        0.37314997, 0.88060609, 0.88350214, 0.71640746, 0.88324613,\n",
       "        0.88236612, 0.81720508, 0.88203811, 0.8813421 , 0.68466695,\n",
       "        0.8810861 , 0.88382214, 0.43309493, 0.8815381 , 0.88210611,\n",
       "        0.78986064, 0.87617402, 0.88195411, 0.55488888, 0.88032609,\n",
       "        0.88091809, 0.26740028, 0.88087409, 0.88371414, 0.81165299,\n",
       "        0.8814901 , 0.88338613, 0.13922623, 0.87842605, 0.8814501 ,\n",
       "        0.73904782, 0.8810901 , 0.88238612, 0.76826429, 0.88267012,\n",
       "        0.88166611, 0.63882622, 0.88443815, 0.88397414, 0.78592457,\n",
       "        0.87974208, 0.87889806, 0.61202979, 0.88182211, 0.8813821 ,\n",
       "        0.59887358, 0.8813781 , 0.88409415, 0.79343269, 0.88454215,\n",
       "        0.88221412, 0.34914559, 0.88236212, 0.8811541 , 0.79094866,\n",
       "        0.88022608, 0.88530216, 0.72920367, 0.88345414, 0.88355014,\n",
       "        0.64323829, 0.88301813, 0.88435415, 0.88439815, 0.88467415,\n",
       "        0.88502216, 0.88463015, 0.88453415, 0.88449415, 0.88486216,\n",
       "        0.88507816, 0.88487816, 0.88513816, 0.88471416, 0.88367814,\n",
       "        0.88421015, 0.88478616, 0.88465815, 0.88500216, 0.88444615,\n",
       "        0.88491416, 0.88481416, 0.88499016, 0.88489416, 0.88492616,\n",
       "        0.88455015, 0.88473816, 0.88388214, 0.88491816, 0.88429015,\n",
       "        0.88448215, 0.88506216, 0.88485016, 0.88489016, 0.88486216,\n",
       "        0.88489016, 0.88520216, 0.88489816, 0.88427415, 0.88472616,\n",
       "        0.88476216, 0.88500216, 0.88478616, 0.88500616, 0.88482216,\n",
       "        0.88515416, 0.88469416, 0.88505016, 0.88453015, 0.88485016,\n",
       "        0.88165011, 0.8812421 , 0.88350214, 0.87909407, 0.88218611,\n",
       "        0.87839005, 0.87886606, 0.88089809, 0.87691003, 0.87921407,\n",
       "        0.88275412, 0.87858606, 0.88273012, 0.88247412, 0.88528216,\n",
       "        0.88209411, 0.88280612, 0.88044609, 0.8809741 , 0.87722604,\n",
       "        0.87698603, 0.87645802, 0.88009408, 0.87951007, 0.88335013,\n",
       "        0.88008208, 0.88262612, 0.87907407, 0.88007808, 0.87923407,\n",
       "        0.87905806, 0.88034609, 0.87799405, 0.87911807, 0.87833405,\n",
       "        0.8810021 , 0.88197011, 0.88448215, 0.88263012, 0.88324613,\n",
       "        0.88244612, 0.8813421 , 0.88205011, 0.88026608, 0.88250212,\n",
       "        0.8812541 , 0.88043809, 0.87887806]),\n",
       " 'split4_test_roc_auc_ovo': array([0.87348198, 0.87299797, 0.875238  , 0.85312565, 0.85360366,\n",
       "        0.86663787, 0.83984944, 0.84238948, 0.82560921, 0.84260148,\n",
       "        0.84205747, 0.83629338, 0.87327397, 0.87565401, 0.87278596,\n",
       "        0.86743388, 0.84693355, 0.86062577, 0.85563369, 0.84494952,\n",
       "        0.84432151, 0.82914927, 0.83430535, 0.85128162, 0.874698  ,\n",
       "        0.87626602, 0.87134194, 0.85143762, 0.85860974, 0.86100978,\n",
       "        0.8497696 , 0.85814173, 0.84759756, 0.82983728, 0.84570153,\n",
       "        0.83639338, 0.87334997, 0.86759388, 0.874702  , 0.85734572,\n",
       "        0.85274964, 0.86574585, 0.85717371, 0.85706971, 0.84779756,\n",
       "        0.84139346, 0.84670155, 0.85377766, 0.87354998, 0.87754204,\n",
       "        0.86926591, 0.8497176 , 0.85032961, 0.86195579, 0.8499176 ,\n",
       "        0.83015728, 0.8498496 , 0.82242116, 0.80927495, 0.84342949,\n",
       "        0.87160995, 0.87056193, 0.87110594, 0.84187747, 0.8563017 ,\n",
       "        0.85391366, 0.84558953, 0.82787325, 0.83182931, 0.83350134,\n",
       "        0.83498936, 0.82797725, 0.87320597, 0.87189395, 0.87283797,\n",
       "        0.8560377 , 0.85382966, 0.85495368, 0.84561353, 0.85101762,\n",
       "        0.84313349, 0.84015744, 0.83679339, 0.83068929, 0.87122994,\n",
       "        0.87546201, 0.87597802, 0.86011776, 0.8562337 , 0.86093778,\n",
       "        0.84771756, 0.84727356, 0.84481752, 0.8497696 , 0.84190547,\n",
       "        0.83949743, 0.87892206, 0.47567161, 0.8815341 , 0.87923807,\n",
       "        0.62170995, 0.87969808, 0.87974608, 0.71149138, 0.87997808,\n",
       "        0.87879006, 0.53992064, 0.87917007, 0.87912207, 0.67063473,\n",
       "        0.87875406, 0.87942607, 0.34143346, 0.88030608, 0.87984208,\n",
       "        0.71548345, 0.87898206, 0.88059009, 0.77931647, 0.87883806,\n",
       "        0.88001008, 0.7749484 , 0.87744204, 0.87985808, 0.45038721,\n",
       "        0.88041409, 0.87965807, 0.61108178, 0.88081009, 0.88074209,\n",
       "        0.84146946, 0.88013408, 0.87926607, 0.84949359, 0.87920607,\n",
       "        0.87885006, 0.187279  , 0.88046609, 0.87945407, 0.82964927,\n",
       "        0.88045009, 0.88070609, 0.68237492, 0.87854206, 0.87825405,\n",
       "        0.57367718, 0.86833389, 0.87735404, 0.84160547, 0.87586201,\n",
       "        0.88027808, 0.58596138, 0.87911807, 0.88061009, 0.80768092,\n",
       "        0.87790605, 0.87321397, 0.49254388, 0.87680203, 0.87974208,\n",
       "        0.60064961, 0.87621402, 0.87745004, 0.7936167 , 0.87733004,\n",
       "        0.87863406, 0.40918255, 0.87707003, 0.87659803, 0.53264452,\n",
       "        0.87931807, 0.88006208, 0.82543321, 0.87962207, 0.87570201,\n",
       "        0.76108018, 0.87940607, 0.87703803, 0.59281349, 0.88273012,\n",
       "        0.87866606, 0.76424423, 0.87229796, 0.87913807, 0.76979632,\n",
       "        0.87895406, 0.87866206, 0.79490872, 0.87567801, 0.87761004,\n",
       "        0.72234756, 0.87787405, 0.88017408, 0.87968607, 0.87959007,\n",
       "        0.87958207, 0.87989008, 0.88025808, 0.87973008, 0.88019408,\n",
       "        0.87989408, 0.87995808, 0.88025408, 0.88019408, 0.88010608,\n",
       "        0.87936607, 0.87971008, 0.87948607, 0.87960207, 0.87871406,\n",
       "        0.87985408, 0.87983808, 0.88037809, 0.87992608, 0.87983808,\n",
       "        0.87989808, 0.87892606, 0.87937807, 0.87989008, 0.87981408,\n",
       "        0.88020608, 0.87921807, 0.87972608, 0.88031809, 0.88053009,\n",
       "        0.88011408, 0.88005008, 0.87969408, 0.87992608, 0.88025408,\n",
       "        0.87980208, 0.88042609, 0.88001008, 0.88055809, 0.88025008,\n",
       "        0.88039409, 0.87990608, 0.88010208, 0.88032209, 0.88044609,\n",
       "        0.87910607, 0.8810941 , 0.87958207, 0.87773404, 0.87856206,\n",
       "        0.87697803, 0.87864606, 0.87862206, 0.87410999, 0.87605402,\n",
       "        0.87401798, 0.87708203, 0.88032209, 0.87825405, 0.87896606,\n",
       "        0.88018208, 0.87741004, 0.87901006, 0.87722604, 0.87626602,\n",
       "        0.87624202, 0.87544601, 0.87936607, 0.8812421 , 0.87688203,\n",
       "        0.87601002, 0.87633802, 0.87723004, 0.87831805, 0.87461799,\n",
       "        0.875054  , 0.87563401, 0.87829005, 0.87473   , 0.87958607,\n",
       "        0.87784205, 0.87884206, 0.87856606, 0.87661403, 0.87906607,\n",
       "        0.87753004, 0.87908207, 0.87893006, 0.87936207, 0.8809461 ,\n",
       "        0.87930607, 0.87949807, 0.87939807]),\n",
       " 'mean_test_roc_auc_ovo': array([0.87203475, 0.87409639, 0.87412679, 0.85782813, 0.85434927,\n",
       "        0.85361486, 0.84572473, 0.84832557, 0.84621834, 0.83275092,\n",
       "        0.83876462, 0.8377366 , 0.87242596, 0.8749228 , 0.87401798,\n",
       "        0.86194739, 0.84824637, 0.85747932, 0.84777036, 0.84679675,\n",
       "        0.84740796, 0.83836621, 0.83547417, 0.84487192, 0.87135234,\n",
       "        0.87312677, 0.86978272, 0.85986016, 0.85731932, 0.85560409,\n",
       "        0.8503112 , 0.84823277, 0.85158163, 0.83702459, 0.84177907,\n",
       "        0.83907183, 0.87025152, 0.87201715, 0.87246676, 0.86561145,\n",
       "        0.85791613, 0.8624178 , 0.8561849 , 0.85823773, 0.85240324,\n",
       "        0.84755036, 0.84821117, 0.84609674, 0.87317797, 0.87226276,\n",
       "        0.86947391, 0.85037481, 0.85703691, 0.85544089, 0.84654394,\n",
       "        0.84296549, 0.84824837, 0.82655362, 0.82674163, 0.82629322,\n",
       "        0.87141154, 0.87093874, 0.875194  , 0.85199843, 0.85272004,\n",
       "        0.85475768, 0.8435575 , 0.83448695, 0.84309109, 0.83216531,\n",
       "        0.82945407, 0.83706619, 0.87224116, 0.87283957, 0.87221236,\n",
       "        0.86302981, 0.8561897 , 0.85575369, 0.84519752, 0.84926959,\n",
       "        0.84157987, 0.83993984, 0.83632538, 0.83681979, 0.87026512,\n",
       "        0.87442679, 0.87397878, 0.8624146 , 0.85997296, 0.85914655,\n",
       "        0.84962079, 0.84774316, 0.84945679, 0.83620698, 0.84223428,\n",
       "        0.84201267, 0.87949967, 0.4060433 , 0.87989088, 0.87922527,\n",
       "        0.49824477, 0.87933407, 0.87941327, 0.5625658 , 0.88007888,\n",
       "        0.87880126, 0.74748796, 0.87957087, 0.87936527, 0.72560921,\n",
       "        0.87937807, 0.87935247, 0.59778796, 0.87998608, 0.87914367,\n",
       "        0.50708011, 0.87999408, 0.88064529, 0.66603226, 0.87860606,\n",
       "        0.88030608, 0.58052289, 0.87962927, 0.87885406, 0.31471784,\n",
       "        0.88017568, 0.87938367, 0.56674827, 0.87942607, 0.87938767,\n",
       "        0.58993664, 0.87939727, 0.87961167, 0.6625282 , 0.8811061 ,\n",
       "        0.87910607, 0.59451671, 0.87960687, 0.87935327, 0.49415751,\n",
       "        0.87863726, 0.88022208, 0.53065009, 0.87959967, 0.87688363,\n",
       "        0.66510904, 0.87431159, 0.87834525, 0.63902622, 0.87757644,\n",
       "        0.87926607, 0.6250588 , 0.87874766, 0.88057489, 0.77234996,\n",
       "        0.87850686, 0.87767804, 0.52697003, 0.87615322, 0.87863406,\n",
       "        0.72135394, 0.87888366, 0.87830765, 0.69480632, 0.87771324,\n",
       "        0.87917727, 0.65701611, 0.87995488, 0.87931327, 0.67823645,\n",
       "        0.87868766, 0.87811485, 0.72006592, 0.87907807, 0.87883486,\n",
       "        0.66217779, 0.88014288, 0.88029728, 0.63006528, 0.88027488,\n",
       "        0.87839645, 0.66135938, 0.87758364, 0.87951727, 0.67673083,\n",
       "        0.87661723, 0.87959087, 0.6814877 , 0.87811805, 0.88076209,\n",
       "        0.74272628, 0.87963647, 0.8813885 , 0.8813589 , 0.8812005 ,\n",
       "        0.88161651, 0.88164451, 0.8815581 , 0.8812909 , 0.88166291,\n",
       "        0.88159731, 0.88161491, 0.88186531, 0.88163571, 0.8811133 ,\n",
       "        0.8814285 , 0.8813749 , 0.88161091, 0.8815077 , 0.8814389 ,\n",
       "        0.88171971, 0.88163091, 0.88160211, 0.88168451, 0.8813901 ,\n",
       "        0.8814949 , 0.8814069 , 0.88089329, 0.8813805 , 0.88159331,\n",
       "        0.88158051, 0.88166051, 0.88160851, 0.88166691, 0.88163331,\n",
       "        0.88162051, 0.88183571, 0.88168851, 0.8814805 , 0.88162531,\n",
       "        0.8814661 , 0.88170531, 0.88165251, 0.88194371, 0.88171571,\n",
       "        0.88179011, 0.88164451, 0.88180131, 0.88176691, 0.88179731,\n",
       "        0.87940127, 0.87987168, 0.88017568, 0.87811725, 0.87831965,\n",
       "        0.87789885, 0.87706443, 0.87669243, 0.87603882, 0.87655482,\n",
       "        0.87670043, 0.87712923, 0.88061489, 0.87968207, 0.88035329,\n",
       "        0.87925247, 0.87899166, 0.87902206, 0.87679483, 0.87766844,\n",
       "        0.87648922, 0.87613322, 0.87700523, 0.87863646, 0.87918367,\n",
       "        0.87758284, 0.87876206, 0.87865726, 0.87935567, 0.87773244,\n",
       "        0.87715483, 0.87746364, 0.87810685, 0.87607802, 0.87837965,\n",
       "        0.87844206, 0.88026448, 0.87988928, 0.87798045, 0.88013968,\n",
       "        0.87915327, 0.88001088, 0.87973728, 0.87867246, 0.87943167,\n",
       "        0.87883006, 0.87919327, 0.87867406]),\n",
       " 'std_test_roc_auc_ovo': array([0.00566966, 0.00651391, 0.0039953 , 0.01021535, 0.01001806,\n",
       "        0.01788391, 0.00847477, 0.0078064 , 0.01644605, 0.008294  ,\n",
       "        0.01283251, 0.01284574, 0.00612688, 0.00566286, 0.00450629,\n",
       "        0.00793959, 0.00567824, 0.00557663, 0.00846603, 0.00961647,\n",
       "        0.00782181, 0.00612477, 0.00657832, 0.01100876, 0.00524783,\n",
       "        0.00640551, 0.00674616, 0.00670731, 0.00730918, 0.01007496,\n",
       "        0.00767758, 0.00819485, 0.00791311, 0.00650504, 0.00446084,\n",
       "        0.0053845 , 0.00546071, 0.00401885, 0.00752515, 0.00555285,\n",
       "        0.00469972, 0.00771577, 0.00548137, 0.00687536, 0.00820984,\n",
       "        0.00625536, 0.00601489, 0.01072981, 0.00660059, 0.00541374,\n",
       "        0.0062347 , 0.00935654, 0.00693548, 0.00662123, 0.01015447,\n",
       "        0.0097743 , 0.00965315, 0.01208321, 0.01244018, 0.01530527,\n",
       "        0.00773499, 0.00525901, 0.00714929, 0.00859781, 0.0099007 ,\n",
       "        0.01062112, 0.01066271, 0.00749844, 0.00982215, 0.00927269,\n",
       "        0.00932816, 0.00985602, 0.00510295, 0.0057704 , 0.00525895,\n",
       "        0.00610382, 0.01327169, 0.00684035, 0.01199355, 0.00269002,\n",
       "        0.01451398, 0.0072199 , 0.00730193, 0.00747109, 0.00496541,\n",
       "        0.00709761, 0.00667461, 0.00847165, 0.00532584, 0.00493975,\n",
       "        0.00457146, 0.01088445, 0.00275543, 0.01087584, 0.00818935,\n",
       "        0.00581965, 0.00467503, 0.22153386, 0.0045416 , 0.00415723,\n",
       "        0.17103359, 0.00448637, 0.00353353, 0.21780123, 0.00418103,\n",
       "        0.00355584, 0.11351108, 0.00360147, 0.00406995, 0.05499571,\n",
       "        0.00338466, 0.00436475, 0.20094282, 0.00443776, 0.0044652 ,\n",
       "        0.11435916, 0.00428153, 0.00380641, 0.11814822, 0.00405039,\n",
       "        0.00398576, 0.26417771, 0.00361776, 0.00429514, 0.09944158,\n",
       "        0.00387544, 0.00350167, 0.1601235 , 0.0044054 , 0.00360723,\n",
       "        0.15771221, 0.00364185, 0.00451971, 0.11536419, 0.00463154,\n",
       "        0.00350474, 0.25933217, 0.0030055 , 0.00369259, 0.23089877,\n",
       "        0.00452707, 0.00389101, 0.12458996, 0.00378183, 0.00667947,\n",
       "        0.07393695, 0.00552424, 0.00520584, 0.12090738, 0.00475319,\n",
       "        0.0045755 , 0.19206615, 0.00329659, 0.00306207, 0.05458101,\n",
       "        0.00591563, 0.00564792, 0.22950769, 0.00350573, 0.00416111,\n",
       "        0.06229178, 0.0048283 , 0.00417438, 0.07321055, 0.00462773,\n",
       "        0.00388889, 0.14056619, 0.00497147, 0.00538333, 0.09099189,\n",
       "        0.00493894, 0.0033788 , 0.08588437, 0.00510105, 0.00389408,\n",
       "        0.11698318, 0.00532313, 0.0051279 , 0.19034466, 0.00445464,\n",
       "        0.00500534, 0.16531632, 0.00542991, 0.00550899, 0.10988165,\n",
       "        0.00348489, 0.00452461, 0.12892164, 0.0037668 , 0.00525428,\n",
       "        0.09952443, 0.00447359, 0.00367484, 0.00427187, 0.00387466,\n",
       "        0.00396404, 0.00389842, 0.00377123, 0.00393519, 0.00407903,\n",
       "        0.00393522, 0.00411873, 0.00398768, 0.00390695, 0.00401853,\n",
       "        0.00398791, 0.00366915, 0.00404572, 0.0038894 , 0.0040461 ,\n",
       "        0.00396719, 0.00410472, 0.00419697, 0.00399513, 0.00416671,\n",
       "        0.00398746, 0.00434103, 0.0040909 , 0.00423631, 0.00388454,\n",
       "        0.0039545 , 0.00410109, 0.00410027, 0.00396935, 0.00391575,\n",
       "        0.00400593, 0.00397841, 0.00410294, 0.00395567, 0.00391936,\n",
       "        0.00393364, 0.00408105, 0.00398274, 0.00386239, 0.00380645,\n",
       "        0.00398955, 0.00396218, 0.00395524, 0.00379404, 0.00393587,\n",
       "        0.00378927, 0.00373195, 0.00486232, 0.0054113 , 0.00613115,\n",
       "        0.00566428, 0.0051985 , 0.00710038, 0.00532578, 0.00536707,\n",
       "        0.00804017, 0.00554   , 0.00378261, 0.00420768, 0.00499704,\n",
       "        0.00576637, 0.00448379, 0.00607663, 0.00551264, 0.00596296,\n",
       "        0.00498441, 0.00600992, 0.00847169, 0.00595681, 0.00543448,\n",
       "        0.00410038, 0.00371012, 0.00503732, 0.00472891, 0.00532161,\n",
       "        0.00731295, 0.00571644, 0.00615778, 0.00581961, 0.00606837,\n",
       "        0.00584842, 0.00362892, 0.00487568, 0.00616204, 0.00474215,\n",
       "        0.00568929, 0.00427157, 0.00483352, 0.00644874, 0.00516674,\n",
       "        0.00554657, 0.00461015, 0.00521995]),\n",
       " 'rank_test_roc_auc_ovo': array([176, 165, 164, 195, 205, 206, 229, 216, 227, 251, 241, 243, 172,\n",
       "        161, 166, 189, 218, 196, 221, 225, 224, 242, 249, 231, 179, 169,\n",
       "        183, 191, 197, 202, 212, 219, 210, 245, 237, 240, 182, 177, 171,\n",
       "        185, 194, 187, 200, 193, 208, 223, 220, 228, 168, 173, 184, 211,\n",
       "        198, 203, 226, 234, 217, 255, 254, 256, 178, 180, 160, 209, 207,\n",
       "        204, 232, 250, 233, 252, 253, 244, 174, 170, 175, 186, 199, 201,\n",
       "        230, 215, 238, 239, 247, 246, 181, 162, 167, 188, 190, 192, 213,\n",
       "        222, 214, 248, 235, 236,  82, 287,  69,  99, 285,  95,  85, 281,\n",
       "         64, 113, 258,  80,  91, 260,  90,  94, 276,  67, 104, 284,  66,\n",
       "         51, 267, 123,  55, 279,  75, 110, 288,  61,  89, 280,  84,  88,\n",
       "        278,  87,  76, 269,  48, 105, 277,  77,  93, 286, 120,  59, 282,\n",
       "         78, 149, 268, 163, 128, 273, 143,  97, 275, 115,  53, 257, 124,\n",
       "        139, 283, 156, 122, 261, 109, 130, 263, 138, 102, 272,  68,  96,\n",
       "        265, 116, 133, 262, 106, 111, 270,  62,  56, 274,  57, 126, 271,\n",
       "        141,  81, 266, 153,  79, 264, 131,  50, 259,  74,  41,  44,  46,\n",
       "         24,  17,  32,  45,  14,  29,  25,   2,  19,  47,  38,  43,  26,\n",
       "         33,  37,   8,  21,  28,  12,  40,  34,  39,  49,  42,  30,  31,\n",
       "         15,  27,  13,  20,  23,   3,  11,  35,  22,  36,  10,  16,   1,\n",
       "          9,   6,  18,   4,   7,   5,  86,  71,  60, 132, 129, 136, 147,\n",
       "        152, 159, 154, 151, 146,  52,  73,  54,  98, 108, 107, 150, 140,\n",
       "        155, 157, 148, 121, 101, 142, 114, 119,  92, 137, 145, 144, 134,\n",
       "        158, 127, 125,  58,  70, 135,  63, 103,  65,  72, 118,  83, 112,\n",
       "        100, 117]),\n",
       " 'split0_test_jaccard': array([0.61211129, 0.62356792, 0.625     , 0.60777958, 0.59803922,\n",
       "        0.56634304, 0.60666667, 0.61083744, 0.58692185, 0.57075472,\n",
       "        0.58132045, 0.58121019, 0.60761589, 0.62335526, 0.6327212 ,\n",
       "        0.61400651, 0.60461285, 0.59271523, 0.5696    , 0.59805511,\n",
       "        0.59468439, 0.61705686, 0.58986928, 0.58833063, 0.61221122,\n",
       "        0.61551155, 0.61513158, 0.60862355, 0.60790774, 0.58681672,\n",
       "        0.59803922, 0.58945687, 0.59805511, 0.57487923, 0.59550562,\n",
       "        0.60325203, 0.61211129, 0.62121212, 0.6       , 0.60565724,\n",
       "        0.60526316, 0.58986928, 0.60430464, 0.60361842, 0.60197368,\n",
       "        0.60163934, 0.60134003, 0.59450727, 0.62457912, 0.62790698,\n",
       "        0.61589404, 0.59775641, 0.59672131, 0.61049285, 0.6009772 ,\n",
       "        0.59349593, 0.59380098, 0.5562701 , 0.57398374, 0.5664    ,\n",
       "        0.60330579, 0.62295082, 0.6192053 , 0.57835218, 0.62091503,\n",
       "        0.60777958, 0.58116883, 0.58360129, 0.59061489, 0.5902439 ,\n",
       "        0.56726094, 0.59324759, 0.61500816, 0.64309211, 0.62354892,\n",
       "        0.60561056, 0.59837398, 0.61788618, 0.59127625, 0.59805511,\n",
       "        0.58552632, 0.5974026 , 0.58757962, 0.58642973, 0.62644628,\n",
       "        0.62335526, 0.60912052, 0.61386139, 0.61627907, 0.59669421,\n",
       "        0.57894737, 0.62214984, 0.62251656, 0.58238173, 0.58814103,\n",
       "        0.61337684, 0.62352941, 0.502     , 0.6281407 , 0.62604341,\n",
       "        0.502     , 0.62771285, 0.62689076, 0.01792829, 0.6293823 ,\n",
       "        0.62666667, 0.        , 0.62876254, 0.61976549, 0.502     ,\n",
       "        0.62913907, 0.62771285, 0.0059761 , 0.6187291 , 0.62876254,\n",
       "        0.        , 0.63377926, 0.6252073 , 0.32905983, 0.63149079,\n",
       "        0.63484087, 0.502     , 0.63149079, 0.6293823 , 0.4680203 ,\n",
       "        0.62541806, 0.62771285, 0.        , 0.62352941, 0.63545151,\n",
       "        0.02970297, 0.62604341, 0.62289562, 0.502     , 0.62416107,\n",
       "        0.63      , 0.5       , 0.62562396, 0.6293823 , 0.        ,\n",
       "        0.6327212 , 0.63545151, 0.        , 0.63255034, 0.61295681,\n",
       "        0.52339499, 0.62644628, 0.63193277, 0.51439299, 0.63361345,\n",
       "        0.62895175, 0.58823529, 0.62292359, 0.62666667, 0.46005917,\n",
       "        0.6185738 , 0.6252073 , 0.40854701, 0.61512605, 0.62562396,\n",
       "        0.37455197, 0.62396007, 0.625     , 0.48158254, 0.61848739,\n",
       "        0.62103506, 0.56229508, 0.63018242, 0.62521008, 0.56086957,\n",
       "        0.63712375, 0.62686567, 0.49408673, 0.64119601, 0.62731872,\n",
       "        0.47068966, 0.63043478, 0.62857143, 0.47540984, 0.62809917,\n",
       "        0.61976549, 0.41029412, 0.63484087, 0.62895175, 0.30493274,\n",
       "        0.62229617, 0.62604341, 0.505     , 0.62541806, 0.63333333,\n",
       "        0.62681745, 0.63043478, 0.63036304, 0.63636364, 0.63410596,\n",
       "        0.63289037, 0.6345515 , 0.63636364, 0.63349917, 0.63349917,\n",
       "        0.6293823 , 0.62582781, 0.62728785, 0.62728785, 0.63576159,\n",
       "        0.64191419, 0.63245033, 0.62374582, 0.62562396, 0.63      ,\n",
       "        0.62562396, 0.62852405, 0.62852405, 0.62956811, 0.62790698,\n",
       "        0.63349917, 0.62956811, 0.62624585, 0.62396007, 0.6318408 ,\n",
       "        0.63531353, 0.63333333, 0.62956811, 0.63122924, 0.63245033,\n",
       "        0.62895175, 0.63018242, 0.63061564, 0.62833333, 0.6281407 ,\n",
       "        0.63531353, 0.62771285, 0.63471074, 0.63227953, 0.63756178,\n",
       "        0.62913907, 0.62604341, 0.63      , 0.63061564, 0.62624585,\n",
       "        0.63227953, 0.64154104, 0.63166667, 0.62396007, 0.62023217,\n",
       "        0.62895175, 0.62437396, 0.61386139, 0.61412151, 0.6252073 ,\n",
       "        0.61513158, 0.61513158, 0.63377926, 0.62975207, 0.6318408 ,\n",
       "        0.62333333, 0.62562396, 0.62981575, 0.63166667, 0.64607679,\n",
       "        0.61740558, 0.62646566, 0.62541806, 0.62417219, 0.63560732,\n",
       "        0.62666667, 0.62956811, 0.6327212 , 0.62292359, 0.62354892,\n",
       "        0.6187291 , 0.62046205, 0.63484087, 0.61602671, 0.62396007,\n",
       "        0.61538462, 0.62396007, 0.6327212 , 0.62582781, 0.63210702,\n",
       "        0.62562396, 0.62562396, 0.6270903 , 0.61449753, 0.63      ,\n",
       "        0.61627907, 0.62956811, 0.6270903 ]),\n",
       " 'split1_test_jaccard': array([0.66171617, 0.66227348, 0.64157119, 0.65089723, 0.63442623,\n",
       "        0.61264182, 0.61374795, 0.60294118, 0.63474026, 0.60252366,\n",
       "        0.63295269, 0.61352657, 0.65619835, 0.65074135, 0.6490939 ,\n",
       "        0.608     , 0.61066236, 0.62276423, 0.61928105, 0.61904762,\n",
       "        0.58227848, 0.59872611, 0.6031746 , 0.59561129, 0.64065041,\n",
       "        0.66174056, 0.63486842, 0.64251208, 0.62601626, 0.6192    ,\n",
       "        0.62662338, 0.62801932, 0.63344051, 0.6128    , 0.59872611,\n",
       "        0.61451613, 0.6483871 , 0.65359477, 0.65466448, 0.64157119,\n",
       "        0.63826367, 0.63114754, 0.62096774, 0.63295269, 0.64790997,\n",
       "        0.62460064, 0.59871589, 0.6064    , 0.636953  , 0.63875205,\n",
       "        0.64820847, 0.6256    , 0.64638158, 0.61320755, 0.61295419,\n",
       "        0.6288    , 0.63607595, 0.608     , 0.6078125 , 0.58505564,\n",
       "        0.65081967, 0.64811784, 0.65614618, 0.63235294, 0.64052288,\n",
       "        0.61305732, 0.61821086, 0.61102362, 0.61074919, 0.58757962,\n",
       "        0.58398744, 0.6       , 0.6446281 , 0.6446281 , 0.64203612,\n",
       "        0.6525974 , 0.61996779, 0.61513688, 0.62214984, 0.61488673,\n",
       "        0.59810127, 0.5968    , 0.61191626, 0.61774194, 0.64542484,\n",
       "        0.63934426, 0.65346535, 0.63209076, 0.63106796, 0.62318841,\n",
       "        0.62805873, 0.62123613, 0.61637239, 0.58084772, 0.60663507,\n",
       "        0.6076555 , 0.66115702, 0.17321016, 0.66390728, 0.65166667,\n",
       "        0.502     , 0.65505804, 0.65562914, 0.01195219, 0.66056572,\n",
       "        0.65568369, 0.        , 0.65614618, 0.65568369, 0.06640625,\n",
       "        0.65505804, 0.65671642, 0.4330855 , 0.65785124, 0.65676568,\n",
       "        0.502     , 0.65511551, 0.65946844, 0.5122449 , 0.65841584,\n",
       "        0.65397351, 0.00787402, 0.65728477, 0.65346535, 0.496     ,\n",
       "        0.66611296, 0.65460526, 0.12027972, 0.65397351, 0.65671642,\n",
       "        0.49949546, 0.65841584, 0.65562914, 0.25088339, 0.65728477,\n",
       "        0.65841584, 0.        , 0.65557404, 0.65780731, 0.501     ,\n",
       "        0.65397351, 0.65397351, 0.0019802 , 0.65841584, 0.65737705,\n",
       "        0.51928375, 0.65224626, 0.640599  , 0.49563046, 0.65789474,\n",
       "        0.66777963, 0.55053191, 0.65008292, 0.66334992, 0.51020408,\n",
       "        0.66222962, 0.66009852, 0.1754386 , 0.65346535, 0.65733114,\n",
       "        0.55263158, 0.66666667, 0.640599  , 0.1697417 , 0.66225166,\n",
       "        0.6490939 , 0.47727273, 0.655     , 0.65174129, 0.49658314,\n",
       "        0.65719064, 0.66062603, 0.44664634, 0.65666667, 0.65671642,\n",
       "        0.49949239, 0.65016502, 0.65946844, 0.52395672, 0.65723794,\n",
       "        0.65666667, 0.5877551 , 0.65339967, 0.65890183, 0.41666667,\n",
       "        0.65      , 0.66333333, 0.10356537, 0.65886288, 0.65950413,\n",
       "        0.36915888, 0.65403624, 0.65614618, 0.65448505, 0.65282392,\n",
       "        0.65339967, 0.65505804, 0.65505804, 0.65282392, 0.65339967,\n",
       "        0.65671642, 0.65557404, 0.65557404, 0.65282392, 0.66831683,\n",
       "        0.65511551, 0.65339967, 0.65339967, 0.655     , 0.65671642,\n",
       "        0.65614618, 0.65505804, 0.65174129, 0.65339967, 0.65448505,\n",
       "        0.65282392, 0.65614618, 0.65789474, 0.65837479, 0.65339967,\n",
       "        0.65671642, 0.65671642, 0.65174129, 0.65174129, 0.65008292,\n",
       "        0.65066225, 0.65174129, 0.655     , 0.65505804, 0.65339967,\n",
       "        0.66280992, 0.65837479, 0.65505804, 0.655     , 0.65282392,\n",
       "        0.65671642, 0.65391015, 0.65448505, 0.65666667, 0.65391015,\n",
       "        0.64851485, 0.65166667, 0.65780731, 0.6589404 , 0.66280992,\n",
       "        0.65733114, 0.64926591, 0.65789474, 0.65614618, 0.64297521,\n",
       "        0.65737705, 0.66227348, 0.65562914, 0.65946844, 0.65008292,\n",
       "        0.65231788, 0.64725458, 0.65024631, 0.65789474, 0.65950413,\n",
       "        0.65789474, 0.65837479, 0.65522876, 0.64967105, 0.65837479,\n",
       "        0.67107438, 0.65016502, 0.66059603, 0.64744646, 0.65016502,\n",
       "        0.65196078, 0.65733114, 0.65681445, 0.65614618, 0.65897858,\n",
       "        0.65188834, 0.66056572, 0.6589404 , 0.64744646, 0.65625   ,\n",
       "        0.66003317, 0.65676568, 0.66174056, 0.65181518, 0.65841584,\n",
       "        0.65845649, 0.65196078, 0.65950413]),\n",
       " 'split2_test_jaccard': array([0.66775244, 0.66123779, 0.66666667, 0.66231648, 0.64205457,\n",
       "        0.63826367, 0.61501597, 0.64869281, 0.65472313, 0.60031104,\n",
       "        0.62870515, 0.63191153, 0.67868852, 0.65793781, 0.66721044,\n",
       "        0.65971108, 0.64082278, 0.65737705, 0.65217391, 0.63548387,\n",
       "        0.63974152, 0.63149606, 0.64343598, 0.63798701, 0.66502463,\n",
       "        0.66233766, 0.6867863 , 0.65329053, 0.64019449, 0.6512    ,\n",
       "        0.645469  , 0.63064516, 0.64847512, 0.62460568, 0.60849057,\n",
       "        0.62382445, 0.65579119, 0.6601626 , 0.66775777, 0.64724919,\n",
       "        0.65584416, 0.65048544, 0.63249211, 0.64423077, 0.65378422,\n",
       "        0.64686998, 0.63402889, 0.64228935, 0.67434211, 0.66178862,\n",
       "        0.6720517 , 0.63402889, 0.64847512, 0.65217391, 0.64353312,\n",
       "        0.62106918, 0.65335463, 0.62540193, 0.61889764, 0.6304    ,\n",
       "        0.64886731, 0.66065574, 0.65764331, 0.65857605, 0.6650641 ,\n",
       "        0.65316045, 0.65489567, 0.63464567, 0.61146497, 0.6163522 ,\n",
       "        0.60400616, 0.63216561, 0.64583333, 0.67816092, 0.66666667,\n",
       "        0.6542811 , 0.64886731, 0.65372168, 0.61102362, 0.62843296,\n",
       "        0.632     , 0.63218391, 0.62441315, 0.62140575, 0.6511254 ,\n",
       "        0.66071429, 0.66995074, 0.66450567, 0.67763158, 0.63449367,\n",
       "        0.632     , 0.64495114, 0.63285024, 0.625     , 0.64171975,\n",
       "        0.64781906, 0.67647059, 0.        , 0.68300654, 0.66666667,\n",
       "        0.15824916, 0.66939444, 0.66828479, 0.01768173, 0.66284779,\n",
       "        0.66121113, 0.62805663, 0.66231648, 0.66884176, 0.54653938,\n",
       "        0.66121113, 0.67540984, 0.51694915, 0.6710311 , 0.66939444,\n",
       "        0.        , 0.66612111, 0.67213115, 0.502     , 0.67323481,\n",
       "        0.67266776, 0.50510204, 0.67491749, 0.66939444, 0.        ,\n",
       "        0.66995074, 0.67159278, 0.00796813, 0.66884176, 0.66284779,\n",
       "        0.502     , 0.67156863, 0.65845649, 0.33382353, 0.67816092,\n",
       "        0.66830065, 0.03585657, 0.66447368, 0.66939444, 0.36331361,\n",
       "        0.66503268, 0.6748366 , 0.502     , 0.66775777, 0.68099174,\n",
       "        0.43387816, 0.66942149, 0.67159278, 0.32110092, 0.66883117,\n",
       "        0.68656716, 0.35512367, 0.6737013 , 0.67156863, 0.60134228,\n",
       "        0.68196721, 0.66235864, 0.39679715, 0.67554077, 0.66939444,\n",
       "        0.52173913, 0.67715232, 0.67156863, 0.48712446, 0.67804878,\n",
       "        0.66775244, 0.57164869, 0.66721311, 0.67540984, 0.41013072,\n",
       "        0.68174204, 0.67651888, 0.59190031, 0.66341463, 0.66883117,\n",
       "        0.53194651, 0.67384106, 0.67656766, 0.15894869, 0.66666667,\n",
       "        0.68071313, 0.48019017, 0.68196721, 0.67098865, 0.4942768 ,\n",
       "        0.66233766, 0.67874794, 0.47523427, 0.67266776, 0.67704918,\n",
       "        0.61008403, 0.67434211, 0.66993464, 0.67528271, 0.67536705,\n",
       "        0.67100977, 0.67377049, 0.67159278, 0.67159278, 0.67213115,\n",
       "        0.67213115, 0.67159278, 0.67710049, 0.67266776, 0.67326733,\n",
       "        0.68506494, 0.67161716, 0.67269737, 0.67159278, 0.67320261,\n",
       "        0.6699187 , 0.67434211, 0.67598684, 0.67269737, 0.67487685,\n",
       "        0.67266776, 0.67213115, 0.67213115, 0.66939444, 0.67105263,\n",
       "        0.67377049, 0.67423015, 0.6738056 , 0.67594108, 0.67051071,\n",
       "        0.67269737, 0.67545305, 0.67487685, 0.66990291, 0.6677686 ,\n",
       "        0.67156863, 0.67213115, 0.6721044 , 0.67159278, 0.67377049,\n",
       "        0.67215815, 0.67487685, 0.67377049, 0.67159278, 0.67161716,\n",
       "        0.67373573, 0.6796748 , 0.68308703, 0.68039539, 0.67862969,\n",
       "        0.68412439, 0.67810458, 0.6852459 , 0.67532468, 0.67821782,\n",
       "        0.69752066, 0.68965517, 0.6710311 , 0.67594108, 0.66502463,\n",
       "        0.68472906, 0.67704918, 0.6852459 , 0.67263844, 0.6748366 ,\n",
       "        0.67213115, 0.6742671 , 0.68438538, 0.68965517, 0.67816092,\n",
       "        0.67094703, 0.67215815, 0.67816092, 0.68256579, 0.69039735,\n",
       "        0.68585526, 0.67715232, 0.68874172, 0.6639478 , 0.68360656,\n",
       "        0.68085106, 0.67927632, 0.67475728, 0.68085106, 0.66883117,\n",
       "        0.67430442, 0.68092105, 0.67479675, 0.68092105, 0.68144499,\n",
       "        0.68421053, 0.67862969, 0.67585089]),\n",
       " 'split3_test_jaccard': array([0.6497545 , 0.6557377 , 0.65460526, 0.63192182, 0.65309446,\n",
       "        0.64505673, 0.63232964, 0.6200318 , 0.62662338, 0.5953125 ,\n",
       "        0.6128    , 0.61538462, 0.66884176, 0.65365854, 0.65742251,\n",
       "        0.64983713, 0.62933754, 0.6414791 , 0.60897436, 0.6336    ,\n",
       "        0.63621795, 0.60413355, 0.61392405, 0.63106796, 0.65957447,\n",
       "        0.65517241, 0.6504065 , 0.64239482, 0.63064516, 0.64401294,\n",
       "        0.63004847, 0.61198738, 0.63474026, 0.61987382, 0.61904762,\n",
       "        0.61712439, 0.63535032, 0.65522876, 0.66071429, 0.64851485,\n",
       "        0.64715447, 0.65568369, 0.648     , 0.62236629, 0.62735849,\n",
       "        0.6256    , 0.63955343, 0.60952381, 0.66338259, 0.65353038,\n",
       "        0.65742251, 0.584     , 0.63458401, 0.63754045, 0.6200318 ,\n",
       "        0.63097199, 0.60197368, 0.616     , 0.6263911 , 0.57275542,\n",
       "        0.64205457, 0.63974152, 0.66721044, 0.63829787, 0.61217949,\n",
       "        0.64344942, 0.61958266, 0.62360447, 0.63782051, 0.61014263,\n",
       "        0.63535032, 0.63474026, 0.65403624, 0.64181524, 0.66227348,\n",
       "        0.64495114, 0.65097403, 0.64250412, 0.65528455, 0.62358643,\n",
       "        0.62962963, 0.61341853, 0.6096    , 0.6224    , 0.65203252,\n",
       "        0.65905383, 0.64950166, 0.63474026, 0.63311688, 0.63285024,\n",
       "        0.60446571, 0.63489499, 0.6128    , 0.59584665, 0.625     ,\n",
       "        0.59270998, 0.66557377, 0.        , 0.66666667, 0.65950413,\n",
       "        0.30142566, 0.67387687, 0.66831683, 0.        , 0.67272727,\n",
       "        0.67107438, 0.5025025 , 0.6655574 , 0.67326733, 0.502     ,\n",
       "        0.66776316, 0.67054908, 0.00982318, 0.66832504, 0.66942149,\n",
       "        0.49241658, 0.66611842, 0.66831683, 0.1375969 , 0.67161716,\n",
       "        0.66833333, 0.43943944, 0.66336634, 0.66887417, 0.21491228,\n",
       "        0.66006601, 0.669967  , 0.49799599, 0.67109635, 0.67      ,\n",
       "        0.        , 0.66227348, 0.67391304, 0.        , 0.66940789,\n",
       "        0.66998342, 0.55467869, 0.67272727, 0.67164179, 0.51560818,\n",
       "        0.66059603, 0.67275748, 0.14942529, 0.66832504, 0.66336634,\n",
       "        0.39497307, 0.65861514, 0.67215815, 0.390625  , 0.65415987,\n",
       "        0.6672213 , 0.32071504, 0.65671642, 0.65793781, 0.59332322,\n",
       "        0.65409836, 0.67154472, 0.03      , 0.64851485, 0.65466448,\n",
       "        0.5210084 , 0.66940789, 0.65841584, 0.45768566, 0.65409836,\n",
       "        0.65353038, 0.42228739, 0.66280992, 0.67154472, 0.56893819,\n",
       "        0.6523888 , 0.65785124, 0.36755952, 0.66336634, 0.6661157 ,\n",
       "        0.37442922, 0.65793781, 0.66389351, 0.56030534, 0.65579119,\n",
       "        0.65346535, 0.45957011, 0.66612378, 0.65139116, 0.53465347,\n",
       "        0.66121113, 0.67269737, 0.51293103, 0.65845649, 0.66174056,\n",
       "        0.36783439, 0.66009852, 0.66121113, 0.66174056, 0.66171617,\n",
       "        0.66393443, 0.66123779, 0.65960912, 0.65953947, 0.6650165 ,\n",
       "        0.66392092, 0.65686275, 0.66447368, 0.66065574, 0.66558442,\n",
       "        0.66503268, 0.66231648, 0.66013072, 0.65953947, 0.65681445,\n",
       "        0.66612111, 0.66176471, 0.66229508, 0.66229508, 0.65798046,\n",
       "        0.65901639, 0.65849673, 0.66231648, 0.66883117, 0.65742251,\n",
       "        0.66448445, 0.66721044, 0.66339869, 0.66065574, 0.65953947,\n",
       "        0.66280992, 0.66666667, 0.66611842, 0.66118421, 0.66282895,\n",
       "        0.66721044, 0.66284779, 0.66229508, 0.66392092, 0.66392092,\n",
       "        0.66556837, 0.66229508, 0.66502463, 0.65953947, 0.65630115,\n",
       "        0.65533981, 0.66502463, 0.65522876, 0.6639478 , 0.65901639,\n",
       "        0.67159278, 0.65905383, 0.65960912, 0.66995074, 0.66178862,\n",
       "        0.66830065, 0.66068515, 0.66393443, 0.66280992, 0.66118421,\n",
       "        0.66231648, 0.65953947, 0.6650165 , 0.66557377, 0.6504065 ,\n",
       "        0.66339869, 0.6503268 , 0.66611842, 0.66336634, 0.65957447,\n",
       "        0.64926591, 0.6650165 , 0.65296053, 0.64829822, 0.66284779,\n",
       "        0.66118421, 0.66286645, 0.65957447, 0.67164179, 0.66886326,\n",
       "        0.65901639, 0.65302782, 0.65089723, 0.66171617, 0.65635179,\n",
       "        0.66447368, 0.6525974 , 0.6639478 , 0.66013072, 0.65630115,\n",
       "        0.66013072, 0.66556837, 0.66884176]),\n",
       " 'split4_test_jaccard': array([0.65403624, 0.66169154, 0.66390728, 0.61311475, 0.63680782,\n",
       "        0.62889984, 0.61341853, 0.61712439, 0.60697306, 0.61897106,\n",
       "        0.61538462, 0.62776025, 0.66009852, 0.65289256, 0.65619835,\n",
       "        0.65780731, 0.62479608, 0.64676617, 0.6307947 , 0.62140575,\n",
       "        0.6091954 , 0.58709677, 0.59708738, 0.61030596, 0.66721854,\n",
       "        0.65737705, 0.66556291, 0.6272    , 0.63784666, 0.63414634,\n",
       "        0.62843296, 0.62316476, 0.6144    , 0.61244019, 0.61191626,\n",
       "        0.60093897, 0.66112957, 0.65139116, 0.64891847, 0.63815789,\n",
       "        0.63022508, 0.66169154, 0.62703583, 0.64239482, 0.62622951,\n",
       "        0.62279294, 0.60987261, 0.62279294, 0.63819095, 0.6589404 ,\n",
       "        0.64072848, 0.61663948, 0.64215686, 0.66557377, 0.62908497,\n",
       "        0.59120521, 0.61803279, 0.59744409, 0.57256778, 0.63004847,\n",
       "        0.65841584, 0.65551839, 0.65897858, 0.60702875, 0.62884927,\n",
       "        0.6369637 , 0.616     , 0.59742351, 0.60063898, 0.60127592,\n",
       "        0.59235669, 0.58359621, 0.67      , 0.64860427, 0.66500829,\n",
       "        0.6446281 , 0.62074554, 0.63680782, 0.60383387, 0.62847791,\n",
       "        0.60666667, 0.61513158, 0.60096154, 0.59746434, 0.63486842,\n",
       "        0.67504188, 0.67111853, 0.64065041, 0.63268608, 0.67594108,\n",
       "        0.60771704, 0.61138211, 0.62843296, 0.63015873, 0.61426256,\n",
       "        0.61806656, 0.65890183, 0.502     , 0.66556291, 0.6638796 ,\n",
       "        0.29084967, 0.66169154, 0.66611018, 0.54070474, 0.66944908,\n",
       "        0.66722408, 0.49846154, 0.66666667, 0.6672213 , 0.        ,\n",
       "        0.66445183, 0.6627907 , 0.18024032, 0.66222962, 0.66611018,\n",
       "        0.08840864, 0.67054908, 0.66222962, 0.5025025 , 0.66833333,\n",
       "        0.66110184, 0.        , 0.66777963, 0.66611296, 0.00592885,\n",
       "        0.66333333, 0.66444073, 0.06883365, 0.66611018, 0.66889632,\n",
       "        0.502     , 0.65780731, 0.65890183, 0.502     , 0.65833333,\n",
       "        0.6661157 , 0.        , 0.65676568, 0.66555184, 0.502     ,\n",
       "        0.66944908, 0.66721854, 0.        , 0.66777963, 0.66447368,\n",
       "        0.3772791 , 0.66666667, 0.66777409, 0.59466019, 0.65950413,\n",
       "        0.65131579, 0.40190736, 0.65296053, 0.66334992, 0.58259773,\n",
       "        0.66      , 0.66062603, 0.28294036, 0.66104553, 0.65789474,\n",
       "        0.47491249, 0.66225166, 0.66336634, 0.55079365, 0.66056572,\n",
       "        0.66777409, 0.09314587, 0.66333333, 0.66331658, 0.35674931,\n",
       "        0.66777409, 0.66171617, 0.61363636, 0.66776316, 0.66284779,\n",
       "        0.52080123, 0.66390728, 0.66006601, 0.37372263, 0.6627907 ,\n",
       "        0.665     , 0.29608939, 0.66721044, 0.65066225, 0.5588585 ,\n",
       "        0.6650165 , 0.67554077, 0.54952077, 0.65886288, 0.6722408 ,\n",
       "        0.44674086, 0.66169154, 0.66666667, 0.66386555, 0.66445183,\n",
       "        0.6650165 , 0.66777963, 0.66889632, 0.66831683, 0.66556837,\n",
       "        0.6672213 , 0.66998342, 0.66611842, 0.66666667, 0.66666667,\n",
       "        0.66282895, 0.66666667, 0.66666667, 0.66888519, 0.66888519,\n",
       "        0.6672213 , 0.67109635, 0.66666667, 0.66888519, 0.6672213 ,\n",
       "        0.66943522, 0.66888519, 0.6661157 , 0.66445183, 0.66887417,\n",
       "        0.6672213 , 0.66721854, 0.66666667, 0.66666667, 0.6672213 ,\n",
       "        0.66888519, 0.66500829, 0.6661157 , 0.66666667, 0.66271186,\n",
       "        0.66666667, 0.66888519, 0.66888519, 0.66500829, 0.66777409,\n",
       "        0.66611296, 0.66998342, 0.66887417, 0.66500829, 0.66556291,\n",
       "        0.66943522, 0.66222962, 0.67221298, 0.66336634, 0.66500829,\n",
       "        0.66280992, 0.6650165 , 0.66115702, 0.66942149, 0.65890183,\n",
       "        0.66171617, 0.66721854, 0.67056856, 0.66556291, 0.66336634,\n",
       "        0.64860427, 0.6677686 , 0.65841584, 0.65568369, 0.66389351,\n",
       "        0.65619835, 0.64918033, 0.65728477, 0.6627907 , 0.66336634,\n",
       "        0.65557404, 0.66666667, 0.66998342, 0.6655574 , 0.66009852,\n",
       "        0.6589404 , 0.66112957, 0.67001675, 0.64851485, 0.66943522,\n",
       "        0.65785124, 0.6722408 , 0.66721854, 0.65953947, 0.6627907 ,\n",
       "        0.66944908, 0.66225166, 0.66721854, 0.66333333, 0.66888519,\n",
       "        0.65723794, 0.66445183, 0.65196078]),\n",
       " 'mean_test_jaccard': array([0.64907413, 0.65290169, 0.65035008, 0.63320597, 0.63288446,\n",
       "        0.61824102, 0.61623575, 0.61992552, 0.62199633, 0.5975746 ,\n",
       "        0.61423258, 0.61395863, 0.65428861, 0.6477171 , 0.65252928,\n",
       "        0.63787241, 0.62204632, 0.63222036, 0.6161648 , 0.62151847,\n",
       "        0.61242355, 0.60770187, 0.60949826, 0.61266057, 0.64893585,\n",
       "        0.65042785, 0.65055114, 0.6348042 , 0.62852206, 0.6270752 ,\n",
       "        0.6257226 , 0.6166547 , 0.6258222 , 0.60891978, 0.60673724,\n",
       "        0.61193119, 0.64255389, 0.64831788, 0.646411  , 0.63623007,\n",
       "        0.63535011, 0.6377755 , 0.62656006, 0.6291126 , 0.63145117,\n",
       "        0.62430058, 0.61670217, 0.61510267, 0.64748956, 0.64818368,\n",
       "        0.64686104, 0.61160496, 0.63366378, 0.63579771, 0.62131625,\n",
       "        0.61310846, 0.62064761, 0.60062322, 0.59993055, 0.59693191,\n",
       "        0.64069264, 0.64539686, 0.65183676, 0.62292156, 0.63350615,\n",
       "        0.63088209, 0.6179716 , 0.61005971, 0.61025771, 0.60111885,\n",
       "        0.59659231, 0.60874993, 0.64590117, 0.65126013, 0.6519067 ,\n",
       "        0.64041366, 0.62778573, 0.63321133, 0.61671363, 0.61868783,\n",
       "        0.61038478, 0.61098732, 0.60689411, 0.60908835, 0.64197949,\n",
       "        0.6515019 , 0.65063136, 0.6371697 , 0.63815632, 0.63263352,\n",
       "        0.61023777, 0.62692284, 0.62259443, 0.60284697, 0.61515168,\n",
       "        0.61592559, 0.65712653, 0.23544203, 0.66145682, 0.65355209,\n",
       "        0.3509049 , 0.65754675, 0.65704634, 0.11765339, 0.65899443,\n",
       "        0.65637199, 0.32580413, 0.65588985, 0.65695591, 0.32338913,\n",
       "        0.65552465, 0.65863578, 0.22921485, 0.65563322, 0.65809086,\n",
       "        0.21656505, 0.65833668, 0.65747067, 0.39668083, 0.66061839,\n",
       "        0.65818346, 0.2908831 , 0.6589678 , 0.65744584, 0.23697229,\n",
       "        0.65697622, 0.65766372, 0.1390155 , 0.65671024, 0.65878241,\n",
       "        0.30663969, 0.65522173, 0.65395922, 0.31774138, 0.6574696 ,\n",
       "        0.65856312, 0.21810705, 0.65503293, 0.65875554, 0.37638436,\n",
       "        0.6563545 , 0.66084753, 0.1306811 , 0.65896573, 0.65583312,\n",
       "        0.44976181, 0.65467917, 0.65681136, 0.46328191, 0.65480067,\n",
       "        0.66036713, 0.44330266, 0.65127695, 0.65657459, 0.5495053 ,\n",
       "        0.6553738 , 0.65596704, 0.25874462, 0.65073851, 0.65298175,\n",
       "        0.48896871, 0.65988772, 0.65178996, 0.4293856 , 0.65469038,\n",
       "        0.65183717, 0.42532995, 0.65570776, 0.6574445 , 0.47865419,\n",
       "        0.65924386, 0.6567156 , 0.50276585, 0.65848136, 0.65636596,\n",
       "        0.4794718 , 0.65525719, 0.65771341, 0.41846864, 0.65411713,\n",
       "        0.65512213, 0.44677978, 0.66070839, 0.65217913, 0.46187763,\n",
       "        0.65217229, 0.66327256, 0.42925029, 0.65485361, 0.6607736 ,\n",
       "        0.48412712, 0.65612064, 0.65686433, 0.6583475 , 0.65769299,\n",
       "        0.65725015, 0.65847949, 0.65830398, 0.65715443, 0.65792297,\n",
       "        0.65787442, 0.65596816, 0.6581109 , 0.65602039, 0.66191937,\n",
       "        0.66199125, 0.65729006, 0.65532805, 0.65612828, 0.65712373,\n",
       "        0.65700625, 0.65815705, 0.65704279, 0.65736908, 0.65649413,\n",
       "        0.65748849, 0.65704547, 0.65694078, 0.65700246, 0.65651796,\n",
       "        0.65950124, 0.65974178, 0.65703607, 0.6572468 , 0.65596095,\n",
       "        0.6568013 , 0.65781034, 0.65854532, 0.65622903, 0.65496996,\n",
       "        0.66071384, 0.65799036, 0.65861069, 0.6575603 , 0.65917024,\n",
       "        0.65793899, 0.65742178, 0.65843087, 0.65668457, 0.65472744,\n",
       "        0.65586103, 0.66002735, 0.66000055, 0.658122  , 0.65713929,\n",
       "        0.66096199, 0.65516295, 0.65555363, 0.65699292, 0.65341815,\n",
       "        0.66000922, 0.65899279, 0.6589885 , 0.65870688, 0.65429978,\n",
       "        0.6542602 , 0.65544716, 0.65774806, 0.65669146, 0.65894351,\n",
       "        0.6534057 , 0.65172294, 0.65768708, 0.65793109, 0.65901677,\n",
       "        0.65470561, 0.65671489, 0.65888442, 0.65335829, 0.65741152,\n",
       "        0.65533395, 0.6557883 , 0.66199765, 0.65125547, 0.66096874,\n",
       "        0.65299833, 0.65781415, 0.65690693, 0.6550762 , 0.65526614,\n",
       "        0.65877686, 0.65563195, 0.65895879, 0.65413956, 0.65900943,\n",
       "        0.65526295, 0.65803576, 0.65664957]),\n",
       " 'std_test_jaccard': array([0.01949378, 0.01485325, 0.01541318, 0.0210343 , 0.01857146,\n",
       "        0.02813663, 0.00855797, 0.01553368, 0.02328424, 0.01558341,\n",
       "        0.01814655, 0.01781837, 0.02458819, 0.01240318, 0.01146389,\n",
       "        0.02226847, 0.01301383, 0.02271668, 0.02735099, 0.01339883,\n",
       "        0.02256665, 0.01530716, 0.01871188, 0.01933095, 0.0206089 ,\n",
       "        0.01766223, 0.02465301, 0.01550499, 0.01147981, 0.02281078,\n",
       "        0.01538285, 0.01502514, 0.01761933, 0.01761957, 0.00862287,\n",
       "        0.00861693, 0.01750972, 0.01385692, 0.02403604, 0.01574344,\n",
       "        0.01731663, 0.02605056, 0.01429451, 0.01493445, 0.01834445,\n",
       "        0.01433151, 0.0169029 , 0.01630513, 0.01841749, 0.01288013,\n",
       "        0.01867883, 0.0183186 , 0.01907272, 0.02148712, 0.01441661,\n",
       "        0.01728066, 0.0218361 , 0.02400897, 0.02255657, 0.02783726,\n",
       "        0.01940683, 0.01325511, 0.0167598 , 0.02769706, 0.01833133,\n",
       "        0.01756529, 0.02334254, 0.01815809, 0.01574654, 0.01109129,\n",
       "        0.0227785 , 0.02084891, 0.01790495, 0.01364267, 0.01672465,\n",
       "        0.01783455, 0.01978821, 0.01470722, 0.02173749, 0.01144523,\n",
       "        0.0180013 , 0.01310119, 0.0122287 , 0.01450747, 0.00988362,\n",
       "        0.01808907, 0.02247407, 0.01633613, 0.02070012, 0.02553388,\n",
       "        0.01902599, 0.01170649, 0.00740163, 0.02092122, 0.01792066,\n",
       "        0.01808664, 0.01785335, 0.22664731, 0.01802421, 0.01465851,\n",
       "        0.13328955, 0.016251  , 0.01579144, 0.21162558, 0.01544076,\n",
       "        0.01574945, 0.27006893, 0.01404891, 0.01948055, 0.23841999,\n",
       "        0.01384361, 0.01673601, 0.21200048, 0.01901853, 0.01537771,\n",
       "        0.23142691, 0.01329465, 0.01673399, 0.14652132, 0.01544627,\n",
       "        0.0132961 , 0.23547238, 0.01489434, 0.01517828, 0.21471009,\n",
       "        0.01611022, 0.0161104 , 0.18473886, 0.01761221, 0.01259565,\n",
       "        0.23843091, 0.01539617, 0.0167961 , 0.18632804, 0.01833517,\n",
       "        0.01481997, 0.25341693, 0.01593989, 0.01542153, 0.19622826,\n",
       "        0.01287611, 0.01463107, 0.19439552, 0.0137161 , 0.02282598,\n",
       "        0.06125844, 0.01535932, 0.01706447, 0.09634676, 0.01164354,\n",
       "        0.01927288, 0.1067899 , 0.01637445, 0.01557545, 0.05522207,\n",
       "        0.02064694, 0.01592767, 0.14237183, 0.02001007, 0.01458569,\n",
       "        0.06235731, 0.01860666, 0.01680506, 0.13342718, 0.01973988,\n",
       "        0.01712568, 0.17505553, 0.01336418, 0.01804237, 0.08341257,\n",
       "        0.01496289, 0.01627744, 0.09137035, 0.00934268, 0.01507527,\n",
       "        0.05652187, 0.01462499, 0.01582116, 0.14411141, 0.01358085,\n",
       "        0.02003843, 0.09507122, 0.01578662, 0.01372171, 0.09211628,\n",
       "        0.01579437, 0.01931236, 0.16455435, 0.01567753, 0.01517831,\n",
       "        0.11345933, 0.01444291, 0.01406163, 0.01286057, 0.01381518,\n",
       "        0.01343539, 0.01350682, 0.01250926, 0.0135431 , 0.01362076,\n",
       "        0.01510179, 0.01643138, 0.01686242, 0.01579709, 0.01334093,\n",
       "        0.0140905 , 0.01378839, 0.01705567, 0.01640312, 0.01505205,\n",
       "        0.01636916, 0.01631021, 0.0162512 , 0.0153655 , 0.01597791,\n",
       "        0.01394786, 0.01500451, 0.01604315, 0.0169862 , 0.01402453,\n",
       "        0.01327286, 0.01433984, 0.01547238, 0.0152122 , 0.01370107,\n",
       "        0.01579933, 0.01575846, 0.0153233 , 0.01483249, 0.01419705,\n",
       "        0.01300029, 0.01586902, 0.0133045 , 0.01370046, 0.01278252,\n",
       "        0.0152193 , 0.01722299, 0.01556745, 0.01399485, 0.01560138,\n",
       "        0.01492708, 0.0128646 , 0.01739263, 0.01857554, 0.01960395,\n",
       "        0.01838893, 0.01800715, 0.02312228, 0.02234772, 0.01800037,\n",
       "        0.02645638, 0.02427047, 0.01378677, 0.01549076, 0.01238393,\n",
       "        0.01992731, 0.01784239, 0.01814331, 0.01387015, 0.01015716,\n",
       "        0.01883858, 0.01548438, 0.01913835, 0.02129538, 0.01366852,\n",
       "        0.01642101, 0.01540323, 0.01560325, 0.01994353, 0.02155934,\n",
       "        0.02158739, 0.01889643, 0.01760556, 0.01923119, 0.02010014,\n",
       "        0.02122059, 0.0192111 , 0.01449252, 0.018122  , 0.01248295,\n",
       "        0.01725195, 0.01785683, 0.01653659, 0.02197526, 0.01703009,\n",
       "        0.02188835, 0.01654889, 0.01685956]),\n",
       " 'rank_test_jaccard': array([172, 154, 171, 197, 198, 221, 226, 219, 215, 254, 231, 232, 143,\n",
       "        176, 155, 187, 214, 200, 227, 216, 235, 247, 243, 234, 173, 170,\n",
       "        169, 193, 204, 206, 210, 225, 209, 245, 249, 236, 182, 174, 179,\n",
       "        190, 192, 188, 208, 203, 201, 211, 224, 230, 177, 175, 178, 237,\n",
       "        194, 191, 217, 233, 218, 252, 253, 255, 184, 181, 160, 212, 195,\n",
       "        202, 222, 242, 240, 251, 256, 246, 180, 165, 158, 185, 205, 196,\n",
       "        223, 220, 239, 238, 248, 244, 183, 163, 168, 189, 186, 199, 241,\n",
       "        207, 213, 250, 229, 228,  79, 282,   5, 148, 274,  65,  81, 288,\n",
       "         24, 104, 275, 114,  89, 276, 122,  36, 283, 119,  50, 285,  44,\n",
       "         67, 272,  12,  46, 279,  27,  69, 281,  88,  63, 286,  97,  32,\n",
       "        278, 130, 147, 277,  68,  38, 284, 134,  34, 273, 106,   8, 287,\n",
       "         28, 116, 265, 141,  93, 263, 137,  13, 267, 164, 101, 257, 124,\n",
       "        112, 280, 167, 153, 259,  17, 161, 268, 140, 159, 270, 118,  70,\n",
       "        262,  20,  95, 258,  40, 105, 261, 129,  60, 271, 146, 132, 266,\n",
       "         11, 156, 264, 157,   1, 269, 136,   9, 260, 109,  92,  43,  61,\n",
       "         75,  41,  45,  77,  55,  56, 111,  49, 110,   4,   3,  74, 126,\n",
       "        108,  80,  85,  47,  83,  73, 103,  66,  82,  90,  86, 102,  19,\n",
       "         18,  84,  76, 113,  94,  58,  39, 107, 135,  10,  52,  37,  64,\n",
       "         21,  53,  71,  42,  99, 138, 115,  14,  16,  48,  78,   7, 131,\n",
       "        121,  87, 149,  15,  25,  26,  35, 142, 144, 123,  59,  98,  30,\n",
       "        150, 162,  62,  54,  22, 139,  96,  31, 151,  72, 125, 117,   2,\n",
       "        166,   6, 152,  57,  91, 133, 127,  33, 120,  29, 145,  23, 128,\n",
       "         51, 100]),\n",
       " 'split0_test_neg_log_loss': array([-0.4652977 , -0.49144546, -0.45780493, -0.52670682, -0.51931383,\n",
       "        -0.5859821 , -0.54008071, -0.55385124, -0.5774296 , -0.64530549,\n",
       "        -0.65942769, -0.67377966, -0.46842845, -0.46182249, -0.45305339,\n",
       "        -0.51721753, -0.49469598, -0.49239156, -0.58962924, -0.55166814,\n",
       "        -0.5420831 , -0.61542864, -0.60333372, -0.6011942 , -0.4656941 ,\n",
       "        -0.46675357, -0.46660239, -0.50899047, -0.52214289, -0.52086649,\n",
       "        -0.56897909, -0.54210819, -0.53579982, -0.62868802, -0.59282202,\n",
       "        -0.6143454 , -0.46554187, -0.45395499, -0.46741553, -0.48090977,\n",
       "        -0.49029431, -0.50349112, -0.5199434 , -0.51827957, -0.53263811,\n",
       "        -0.54500461, -0.55499152, -0.58815731, -0.45374149, -0.46312687,\n",
       "        -0.499389  , -0.52147181, -0.50389492, -0.52321098, -0.60958566,\n",
       "        -0.77763938, -0.56610152, -0.70193825, -0.69332729, -0.71769797,\n",
       "        -0.47502449, -0.46004752, -0.46505402, -0.50550061, -0.54091717,\n",
       "        -0.61047562, -0.5945394 , -0.57504474, -0.58131588, -0.61827338,\n",
       "        -0.69688716, -0.65078461, -0.46981166, -0.46068489, -0.46366308,\n",
       "        -0.48482796, -0.50940182, -0.51078719, -0.60382698, -0.54363963,\n",
       "        -0.58947559, -0.57364688, -0.62808411, -0.6642421 , -0.46730424,\n",
       "        -0.46388913, -0.46082398, -0.5011027 , -0.49001531, -0.49844587,\n",
       "        -0.53278278, -0.56970909, -0.52298428, -0.63095912, -0.58580108,\n",
       "        -0.57230912, -0.45553781, -0.71585313, -0.45096974, -0.44935442,\n",
       "        -0.7169161 , -0.44852967, -0.44647572, -0.67792495, -0.44492125,\n",
       "        -0.44451781, -0.73585314, -0.4441981 , -0.45374512, -0.68902833,\n",
       "        -0.45084267, -0.45061016, -0.67692852, -0.44865488, -0.44765939,\n",
       "        -0.72658681, -0.44594545, -0.44273584, -0.6918145 , -0.44561673,\n",
       "        -0.44952432, -0.71786815, -0.44935198, -0.44872182, -0.70219232,\n",
       "        -0.44548801, -0.44472307, -0.72828908, -0.44770534, -0.44433729,\n",
       "        -0.6858812 , -0.44694359, -0.45433828, -0.74813201, -0.44960358,\n",
       "        -0.44801005, -0.70605789, -0.44596603, -0.44511826, -0.71585823,\n",
       "        -0.44814843, -0.44179519, -0.69715705, -0.4435302 , -0.45963826,\n",
       "        -0.70351884, -0.45630651, -0.452328  , -0.65179424, -0.44804125,\n",
       "        -0.44589079, -0.56796208, -0.44612911, -0.44095625, -0.64553566,\n",
       "        -0.45046915, -0.45545857, -0.61849706, -0.45621971, -0.44849874,\n",
       "        -0.64994885, -0.4449716 , -0.44870114, -0.65794836, -0.4505042 ,\n",
       "        -0.44420499, -0.55852734, -0.44340543, -0.45148556, -0.59842163,\n",
       "        -0.45079443, -0.44874818, -0.66486502, -0.45034876, -0.44501939,\n",
       "        -0.59953318, -0.44574172, -0.44326039, -0.60984901, -0.4466505 ,\n",
       "        -0.45276879, -0.66252132, -0.45061145, -0.44853574, -0.72043581,\n",
       "        -0.44993253, -0.44725456, -0.57569439, -0.44538262, -0.44054355,\n",
       "        -0.47190256, -0.44396219, -0.44126127, -0.44426051, -0.44245144,\n",
       "        -0.44051034, -0.44062136, -0.44110878, -0.44066799, -0.44022967,\n",
       "        -0.44119458, -0.44032766, -0.44065229, -0.44008031, -0.44263416,\n",
       "        -0.44303543, -0.44289823, -0.44216163, -0.44085798, -0.44094227,\n",
       "        -0.44060732, -0.44133623, -0.44200376, -0.44006809, -0.44103596,\n",
       "        -0.43957242, -0.44299327, -0.44394147, -0.44248855, -0.44178096,\n",
       "        -0.44135526, -0.44056989, -0.44086951, -0.44046692, -0.44031643,\n",
       "        -0.43976932, -0.43989241, -0.44059223, -0.44292554, -0.4455025 ,\n",
       "        -0.44340812, -0.44171565, -0.44158909, -0.44055951, -0.44107903,\n",
       "        -0.43998988, -0.44034211, -0.43932861, -0.43974959, -0.43978338,\n",
       "        -0.44147956, -0.44250489, -0.44459218, -0.45211146, -0.45133075,\n",
       "        -0.44970998, -0.45137589, -0.45951579, -0.45643731, -0.45746087,\n",
       "        -0.46047837, -0.44914664, -0.44311498, -0.44365994, -0.44426612,\n",
       "        -0.45281727, -0.44500289, -0.44901309, -0.4522901 , -0.45002886,\n",
       "        -0.45183786, -0.45810892, -0.46015393, -0.45036457, -0.44723115,\n",
       "        -0.44703835, -0.4425897 , -0.44777384, -0.44500334, -0.44738395,\n",
       "        -0.45439168, -0.45232181, -0.45115849, -0.45852143, -0.4522886 ,\n",
       "        -0.44981936, -0.4408829 , -0.44378206, -0.45077379, -0.44271104,\n",
       "        -0.44977396, -0.44031793, -0.44687353, -0.44968447, -0.45075847,\n",
       "        -0.44992367, -0.44707276, -0.45087338]),\n",
       " 'split1_test_neg_log_loss': array([-0.43650713, -0.43934575, -0.43976766, -0.48477266, -0.51882734,\n",
       "        -0.53179422, -0.59320737, -0.54393946, -0.5342485 , -0.58326696,\n",
       "        -0.56715529, -0.62700653, -0.44474012, -0.44532816, -0.4542659 ,\n",
       "        -0.48592205, -0.53419564, -0.48649456, -0.53745724, -0.58652609,\n",
       "        -0.56602458, -0.60300822, -0.61618268, -0.59002309, -0.45493061,\n",
       "        -0.45117327, -0.47194165, -0.479615  , -0.49608957, -0.51312556,\n",
       "        -0.50189117, -0.54659447, -0.51803104, -0.58431574, -0.58554787,\n",
       "        -0.5775299 , -0.45438593, -0.44746448, -0.45589466, -0.45503558,\n",
       "        -0.48780435, -0.47486123, -0.50092696, -0.497359  , -0.50388406,\n",
       "        -0.55302753, -0.53990312, -0.56813809, -0.46689614, -0.45726335,\n",
       "        -0.45565973, -0.50956416, -0.48533595, -0.51319206, -0.56028957,\n",
       "        -0.55066413, -0.57463153, -0.69449353, -0.68386524, -0.62896838,\n",
       "        -0.44776246, -0.46409513, -0.43704065, -0.50415605, -0.51044081,\n",
       "        -0.53775837, -0.56065307, -0.61766854, -0.55575215, -0.72438435,\n",
       "        -0.69507947, -0.65968035, -0.44228904, -0.45210807, -0.49734166,\n",
       "        -0.46860549, -0.5319684 , -0.49938955, -0.53036674, -0.53946651,\n",
       "        -0.63612207, -0.62074303, -0.62710563, -0.62604317, -0.45303095,\n",
       "        -0.45279944, -0.45731845, -0.46845251, -0.49187784, -0.4693555 ,\n",
       "        -0.53586873, -0.53022714, -0.52505401, -0.6156327 , -0.59282048,\n",
       "        -0.57967285, -0.44541805, -0.73040937, -0.44690128, -0.44005541,\n",
       "        -0.74016897, -0.44241179, -0.43884609, -0.68620097, -0.43904907,\n",
       "        -0.43817219, -0.68966499, -0.4360941 , -0.44516389, -0.68749195,\n",
       "        -0.44596615, -0.44072949, -0.65947245, -0.4411307 , -0.44089544,\n",
       "        -0.74052145, -0.43904778, -0.43523099, -0.67633539, -0.44119212,\n",
       "        -0.44609043, -0.71333737, -0.44677909, -0.44030913, -0.72203631,\n",
       "        -0.43976767, -0.44025523, -0.71204504, -0.44119062, -0.44021667,\n",
       "        -0.69687228, -0.4373275 , -0.44482891, -0.68227379, -0.45337275,\n",
       "        -0.44335562, -0.70327901, -0.4405669 , -0.43790829, -0.72330617,\n",
       "        -0.4426158 , -0.44069449, -0.70748838, -0.4379309 , -0.45016889,\n",
       "        -0.65113342, -0.44853779, -0.43864345, -0.73457481, -0.44514087,\n",
       "        -0.44082819, -0.65312052, -0.43585924, -0.435429  , -0.59718106,\n",
       "        -0.43750503, -0.45206503, -0.74480678, -0.44804539, -0.44001993,\n",
       "        -0.61820135, -0.44083174, -0.44032471, -0.70339464, -0.44030072,\n",
       "        -0.43933196, -0.66700518, -0.43972904, -0.43865212, -0.70737492,\n",
       "        -0.44363824, -0.44061723, -0.63899132, -0.43892558, -0.43931749,\n",
       "        -0.75913341, -0.43973107, -0.43677989, -0.60848474, -0.43595771,\n",
       "        -0.44649056, -0.57755382, -0.4414826 , -0.44068115, -0.63803561,\n",
       "        -0.44100027, -0.43599876, -0.74809644, -0.43771095, -0.43878169,\n",
       "        -0.69036701, -0.43864344, -0.43865094, -0.43689733, -0.43840687,\n",
       "        -0.43398803, -0.43403808, -0.43599125, -0.43404786, -0.4335601 ,\n",
       "        -0.43361269, -0.43208775, -0.4326128 , -0.43192177, -0.44053362,\n",
       "        -0.43770364, -0.43542052, -0.4346527 , -0.43388248, -0.43537553,\n",
       "        -0.43394185, -0.43336558, -0.43328949, -0.432118  , -0.43258349,\n",
       "        -0.43234237, -0.43685066, -0.44016175, -0.438512  , -0.43406758,\n",
       "        -0.43437836, -0.43418952, -0.43289927, -0.43271718, -0.43286024,\n",
       "        -0.43231043, -0.43214597, -0.43197907, -0.43654778, -0.43662684,\n",
       "        -0.43880108, -0.43470217, -0.43433571, -0.43370595, -0.43236841,\n",
       "        -0.43213511, -0.43256169, -0.43127272, -0.43168691, -0.43189044,\n",
       "        -0.43998023, -0.43914   , -0.43799038, -0.43389112, -0.44086623,\n",
       "        -0.4394286 , -0.44283318, -0.44391576, -0.43490157, -0.44051031,\n",
       "        -0.44433887, -0.44381463, -0.43451502, -0.43755941, -0.43666358,\n",
       "        -0.43545855, -0.43934101, -0.43898598, -0.44509434, -0.4401135 ,\n",
       "        -0.43929468, -0.43909364, -0.44779878, -0.44081021, -0.43516192,\n",
       "        -0.43929892, -0.43863472, -0.43501703, -0.4371206 , -0.43879978,\n",
       "        -0.43792656, -0.43640834, -0.43843412, -0.43755125, -0.44079194,\n",
       "        -0.44100472, -0.43463069, -0.43965728, -0.44157585, -0.43670803,\n",
       "        -0.43385931, -0.43561739, -0.433423  , -0.43910937, -0.43523964,\n",
       "        -0.43444393, -0.43480702, -0.433584  ]),\n",
       " 'split2_test_neg_log_loss': array([-0.45169462, -0.43718386, -0.44587969, -0.46085572, -0.46380703,\n",
       "        -0.47116515, -0.49868531, -0.49614856, -0.50767361, -0.58162975,\n",
       "        -0.5682792 , -0.53524764, -0.43524435, -0.4316355 , -0.43275024,\n",
       "        -0.46224366, -0.50525907, -0.4799578 , -0.52663416, -0.52010833,\n",
       "        -0.51107109, -0.5913811 , -0.5971688 , -0.52732081, -0.44494137,\n",
       "        -0.4344007 , -0.44118873, -0.46889812, -0.47293955, -0.46527654,\n",
       "        -0.51123652, -0.5113336 , -0.48548379, -0.59429267, -0.54637533,\n",
       "        -0.57363721, -0.44122514, -0.44049165, -0.44147097, -0.4680922 ,\n",
       "        -0.47640778, -0.47226603, -0.5153201 , -0.47473066, -0.47616554,\n",
       "        -0.51922388, -0.51242708, -0.51537704, -0.43479688, -0.4419031 ,\n",
       "        -0.44808829, -0.47071723, -0.49191716, -0.49516917, -0.50701337,\n",
       "        -0.53496319, -0.49820056, -0.63814609, -0.571149  , -0.61860092,\n",
       "        -0.43626404, -0.43720715, -0.43602296, -0.47400543, -0.467921  ,\n",
       "        -0.45768476, -0.50606526, -0.56870753, -0.54846254, -0.62107797,\n",
       "        -0.62431426, -0.56239787, -0.44125979, -0.4409456 , -0.43947877,\n",
       "        -0.46209224, -0.44845659, -0.47101301, -0.57664552, -0.55404502,\n",
       "        -0.50842776, -0.57088039, -0.58603935, -0.62542283, -0.44528769,\n",
       "        -0.43348362, -0.42931045, -0.45037687, -0.46599944, -0.48025146,\n",
       "        -0.50597128, -0.47990449, -0.53525105, -0.55148016, -0.53313587,\n",
       "        -0.55452626, -0.43503691, -0.74290001, -0.4306306 , -0.43090453,\n",
       "        -0.69252297, -0.43000426, -0.43090556, -0.7095318 , -0.4289568 ,\n",
       "        -0.43015481, -0.64533541, -0.42983902, -0.4358316 , -0.6687526 ,\n",
       "        -0.43468862, -0.43043263, -0.68902565, -0.43176386, -0.43060607,\n",
       "        -0.71878674, -0.42747691, -0.42768199, -0.71018187, -0.4291621 ,\n",
       "        -0.43102819, -0.66892382, -0.43549194, -0.43121522, -0.72135687,\n",
       "        -0.43027277, -0.43063619, -0.69052991, -0.43185257, -0.43049519,\n",
       "        -0.69696731, -0.42965298, -0.43467937, -0.68854534, -0.43144236,\n",
       "        -0.43289963, -0.67188225, -0.43507308, -0.43032361, -0.69901587,\n",
       "        -0.42998172, -0.42911719, -0.71660105, -0.42718354, -0.42852611,\n",
       "        -0.6605625 , -0.43262763, -0.42817396, -0.72379576, -0.42770431,\n",
       "        -0.42534766, -0.62651731, -0.43100989, -0.42967703, -0.58001459,\n",
       "        -0.42477495, -0.42926984, -0.601308  , -0.43896793, -0.4296815 ,\n",
       "        -0.58219619, -0.42398814, -0.4295661 , -0.74095093, -0.43058972,\n",
       "        -0.42862395, -0.56638084, -0.42415288, -0.42780941, -0.66338899,\n",
       "        -0.42683366, -0.4323697 , -0.55889319, -0.42631145, -0.42889768,\n",
       "        -0.62297046, -0.42292979, -0.42276094, -0.81937525, -0.4306462 ,\n",
       "        -0.42811413, -0.60868881, -0.4327506 , -0.42278599, -0.76624529,\n",
       "        -0.44200832, -0.43085673, -0.65338387, -0.43562821, -0.41914275,\n",
       "        -0.49968481, -0.42536656, -0.4256025 , -0.42441623, -0.42612411,\n",
       "        -0.42451442, -0.42426256, -0.42632652, -0.42464741, -0.42373996,\n",
       "        -0.42468738, -0.42366948, -0.42263565, -0.42329037, -0.42744188,\n",
       "        -0.42408222, -0.42934204, -0.42412183, -0.42647552, -0.42407458,\n",
       "        -0.42394076, -0.4236036 , -0.42457116, -0.42398073, -0.42377887,\n",
       "        -0.42298444, -0.42459309, -0.4264518 , -0.42629176, -0.4249978 ,\n",
       "        -0.4246366 , -0.42273227, -0.42460813, -0.42408932, -0.4258455 ,\n",
       "        -0.42464906, -0.42345574, -0.42442468, -0.42510962, -0.4287288 ,\n",
       "        -0.42653404, -0.42385088, -0.42365927, -0.42450641, -0.42431975,\n",
       "        -0.42457588, -0.42409163, -0.42245286, -0.42353189, -0.42371312,\n",
       "        -0.42625394, -0.42596157, -0.42427249, -0.42638345, -0.42431772,\n",
       "        -0.42289813, -0.42912965, -0.42673436, -0.43132314, -0.43015031,\n",
       "        -0.42189052, -0.42717151, -0.4245895 , -0.42460615, -0.42468612,\n",
       "        -0.4254083 , -0.42647958, -0.42015783, -0.42847896, -0.42223724,\n",
       "        -0.4284346 , -0.42546072, -0.41816932, -0.42199985, -0.42453984,\n",
       "        -0.43007044, -0.43024689, -0.42293229, -0.42384035, -0.42336228,\n",
       "        -0.42209937, -0.42539643, -0.42211763, -0.42946599, -0.4227313 ,\n",
       "        -0.42361425, -0.42700062, -0.42422565, -0.42444825, -0.42257041,\n",
       "        -0.4236501 , -0.42266079, -0.42445669, -0.41968897, -0.42673458,\n",
       "        -0.42567674, -0.42433859, -0.42646276]),\n",
       " 'split3_test_neg_log_loss': array([-0.44493439, -0.43596487, -0.43712557, -0.48887068, -0.48177205,\n",
       "        -0.47004001, -0.52337865, -0.53755377, -0.50305159, -0.58983551,\n",
       "        -0.62171559, -0.57806075, -0.45597627, -0.44019486, -0.4365968 ,\n",
       "        -0.47590164, -0.52975471, -0.50917953, -0.55245097, -0.50954631,\n",
       "        -0.52804579, -0.5912452 , -0.60136082, -0.56884565, -0.44098189,\n",
       "        -0.445583  , -0.44466789, -0.47713476, -0.47376281, -0.48083383,\n",
       "        -0.51486751, -0.54208955, -0.51484477, -0.55978212, -0.58327291,\n",
       "        -0.58962138, -0.45273488, -0.44009705, -0.4298442 , -0.46085552,\n",
       "        -0.47283491, -0.46638579, -0.47336993, -0.49025011, -0.50536212,\n",
       "        -0.52186062, -0.54145197, -0.53179092, -0.43399604, -0.44673353,\n",
       "        -0.43805448, -0.54597546, -0.48008458, -0.47926816, -0.51565756,\n",
       "        -0.54445137, -0.56410974, -0.61848636, -0.66881773, -0.74473321,\n",
       "        -0.44094121, -0.45427388, -0.43101484, -0.52010109, -0.5183619 ,\n",
       "        -0.51249245, -0.59964914, -0.58029878, -0.50117755, -0.62566709,\n",
       "        -0.63541126, -0.60098082, -0.44940558, -0.44419799, -0.44035796,\n",
       "        -0.47029386, -0.48326834, -0.50170724, -0.48840586, -0.53284659,\n",
       "        -0.52167404, -0.63865095, -0.6361167 , -0.57265825, -0.44377006,\n",
       "        -0.43073222, -0.44342108, -0.48548802, -0.4817902 , -0.48622771,\n",
       "        -0.52205791, -0.54285931, -0.52572392, -0.63201192, -0.5699197 ,\n",
       "        -0.58745953, -0.44046567, -0.72863372, -0.44330493, -0.43504113,\n",
       "        -0.71906238, -0.43216978, -0.43199058, -0.7599536 , -0.4294239 ,\n",
       "        -0.43372037, -0.67476014, -0.43284853, -0.43791486, -0.71497461,\n",
       "        -0.441526  , -0.43503484, -0.70453761, -0.43122361, -0.43159924,\n",
       "        -0.6999569 , -0.4318112 , -0.43025981, -0.68222295, -0.43392991,\n",
       "        -0.43978887, -0.74983379, -0.43673529, -0.43853747, -0.71728812,\n",
       "        -0.43691484, -0.4335808 , -0.70210859, -0.42954234, -0.43119293,\n",
       "        -0.72256663, -0.43280634, -0.43981407, -0.70480908, -0.43903917,\n",
       "        -0.43555707, -0.67170403, -0.43567148, -0.43566898, -0.67746806,\n",
       "        -0.43376401, -0.42810468, -0.70582631, -0.4328709 , -0.43251907,\n",
       "        -0.60716053, -0.44821313, -0.43178616, -0.72648513, -0.43395734,\n",
       "        -0.43338731, -0.85450862, -0.4339739 , -0.42618719, -0.53469028,\n",
       "        -0.43159695, -0.4357726 , -0.99072081, -0.4372233 , -0.43322946,\n",
       "        -0.60057828, -0.43424593, -0.42992154, -0.60094125, -0.43036365,\n",
       "        -0.43124831, -0.67120944, -0.42617017, -0.43545197, -0.56186433,\n",
       "        -0.43949515, -0.43789974, -0.69527576, -0.43266325, -0.4301198 ,\n",
       "        -0.68066159, -0.43406224, -0.42695471, -0.57200948, -0.42565234,\n",
       "        -0.43556872, -0.8353899 , -0.43269562, -0.43255413, -0.57362254,\n",
       "        -0.43700497, -0.42613355, -0.62277098, -0.42928698, -0.42789286,\n",
       "        -0.66230932, -0.42914016, -0.43060594, -0.43054609, -0.43048381,\n",
       "        -0.42897162, -0.42892346, -0.43006046, -0.42694767, -0.42573503,\n",
       "        -0.42653716, -0.4250696 , -0.42556211, -0.42603886, -0.43221106,\n",
       "        -0.43070751, -0.4304995 , -0.42824907, -0.4277764 , -0.42763004,\n",
       "        -0.42692748, -0.42683375, -0.4263302 , -0.42519205, -0.42544326,\n",
       "        -0.42560907, -0.42948921, -0.43208427, -0.43251948, -0.42762721,\n",
       "        -0.42980709, -0.42911607, -0.42658096, -0.42628901, -0.42647427,\n",
       "        -0.42522966, -0.4250747 , -0.42544295, -0.43135137, -0.43162582,\n",
       "        -0.43379596, -0.4273037 , -0.42722988, -0.42796001, -0.42645337,\n",
       "        -0.42624056, -0.42605159, -0.42466399, -0.42493453, -0.42456537,\n",
       "        -0.43013871, -0.43143371, -0.42649588, -0.43467386, -0.42800642,\n",
       "        -0.43462842, -0.43470438, -0.43014518, -0.43840068, -0.4327049 ,\n",
       "        -0.42754007, -0.43278943, -0.42755323, -0.42930211, -0.42380664,\n",
       "        -0.42759174, -0.42658457, -0.43104206, -0.43074306, -0.43662778,\n",
       "        -0.43604996, -0.43811507, -0.4314167 , -0.43338605, -0.42737976,\n",
       "        -0.43271758, -0.42925608, -0.4324942 , -0.43107481, -0.4340821 ,\n",
       "        -0.43423507, -0.42982505, -0.43472512, -0.43312369, -0.43469609,\n",
       "        -0.4304012 , -0.42889695, -0.42591052, -0.42891054, -0.42585669,\n",
       "        -0.4279134 , -0.42883616, -0.42698909, -0.43083995, -0.42694534,\n",
       "        -0.42814253, -0.43016512, -0.4339663 ]),\n",
       " 'split4_test_neg_log_loss': array([-0.44434347, -0.44384756, -0.4419711 , -0.49015861, -0.50303979,\n",
       "        -0.46394688, -0.56189444, -0.53169016, -0.60701689, -0.56773302,\n",
       "        -0.56002864, -0.57749401, -0.44569543, -0.44093984, -0.44574602,\n",
       "        -0.4709305 , -0.50710241, -0.48221901, -0.50939127, -0.52806821,\n",
       "        -0.53090357, -0.61444031, -0.58426732, -0.52201662, -0.44142435,\n",
       "        -0.44030904, -0.44913143, -0.49038308, -0.48022505, -0.47653573,\n",
       "        -0.52317522, -0.4946512 , -0.53298656, -0.61696045, -0.53324647,\n",
       "        -0.56748457, -0.44373248, -0.45450229, -0.44616818, -0.47724981,\n",
       "        -0.48679926, -0.47070324, -0.48831215, -0.49051085, -0.51182437,\n",
       "        -0.55119113, -0.52339967, -0.51153293, -0.44535087, -0.43859506,\n",
       "        -0.4570792 , -0.52692927, -0.50894581, -0.47922524, -0.52875342,\n",
       "        -0.61115232, -0.5627262 , -0.70293214, -0.75540725, -0.58370542,\n",
       "        -0.44863957, -0.45147745, -0.44838317, -0.52501983, -0.48469399,\n",
       "        -0.4980244 , -0.56875123, -0.69161043, -0.59417966, -0.62925929,\n",
       "        -0.60870331, -0.62460517, -0.44657019, -0.44492545, -0.44604593,\n",
       "        -0.48742584, -0.49053947, -0.49799835, -0.53193075, -0.53048324,\n",
       "        -0.55386655, -0.57870403, -0.5868837 , -0.58379978, -0.4515051 ,\n",
       "        -0.44651121, -0.44356929, -0.47592259, -0.48953822, -0.4808765 ,\n",
       "        -0.51776668, -0.53488878, -0.52534568, -0.51637069, -0.56123914,\n",
       "        -0.58630824, -0.44331942, -0.76792264, -0.43932516, -0.43754507,\n",
       "        -0.68681342, -0.43727161, -0.43619377, -0.66908351, -0.43586211,\n",
       "        -0.43644771, -0.69392357, -0.43654169, -0.44202568, -0.9324659 ,\n",
       "        -0.4440075 , -0.43859615, -0.70250042, -0.43738842, -0.43628338,\n",
       "        -0.67746028, -0.436629  , -0.43618073, -0.68625925, -0.43627177,\n",
       "        -0.4418376 , -0.68114649, -0.44599429, -0.43866222, -0.69737914,\n",
       "        -0.4364717 , -0.43661123, -0.68958197, -0.43434414, -0.43386071,\n",
       "        -0.67788328, -0.43446096, -0.44221916, -0.78364086, -0.44529045,\n",
       "        -0.44103848, -0.72956005, -0.43776021, -0.43811375, -0.7125454 ,\n",
       "        -0.4361224 , -0.43415543, -0.69137766, -0.4363865 , -0.43988293,\n",
       "        -0.723551  , -0.4555427 , -0.44194587, -0.61325723, -0.44138908,\n",
       "        -0.43290564, -0.69334764, -0.43631711, -0.43253575, -0.60335878,\n",
       "        -0.4379801 , -0.44693776, -0.80266996, -0.4416184 , -0.43410951,\n",
       "        -0.7130393 , -0.44094856, -0.43818732, -0.55811065, -0.44015343,\n",
       "        -0.43556564, -0.79965002, -0.43840321, -0.44327219, -0.71811634,\n",
       "        -0.43721432, -0.43548117, -0.51742351, -0.43650226, -0.44184206,\n",
       "        -0.59275354, -0.43523192, -0.43835062, -0.70580849, -0.4312081 ,\n",
       "        -0.44305819, -0.64565302, -0.44972561, -0.43897587, -0.63574482,\n",
       "        -0.43670576, -0.43535777, -0.59323595, -0.44249283, -0.43592887,\n",
       "        -0.62124976, -0.43729126, -0.43589661, -0.44063271, -0.43717473,\n",
       "        -0.43543246, -0.43771412, -0.434498  , -0.43391317, -0.43329649,\n",
       "        -0.43486889, -0.43374843, -0.43258677, -0.43325824, -0.43539059,\n",
       "        -0.43677469, -0.43715778, -0.43543535, -0.43632256, -0.43763345,\n",
       "        -0.43455603, -0.43398383, -0.43378706, -0.43446903, -0.4348613 ,\n",
       "        -0.43430994, -0.43855315, -0.4370086 , -0.43698519, -0.43548552,\n",
       "        -0.43523361, -0.43693065, -0.43513956, -0.43273806, -0.43324969,\n",
       "        -0.43425964, -0.43319984, -0.4338842 , -0.43620283, -0.44333391,\n",
       "        -0.43743581, -0.4345607 , -0.4349186 , -0.43465138, -0.4337251 ,\n",
       "        -0.43345107, -0.43406469, -0.43294717, -0.43263676, -0.43282981,\n",
       "        -0.43438267, -0.43199417, -0.43348964, -0.43590105, -0.43473601,\n",
       "        -0.43645114, -0.43571055, -0.43467168, -0.44023313, -0.43995965,\n",
       "        -0.44319723, -0.43685895, -0.4337436 , -0.4353246 , -0.43367743,\n",
       "        -0.43244421, -0.43800343, -0.43421389, -0.43724985, -0.43805572,\n",
       "        -0.43769709, -0.4395689 , -0.43389951, -0.43046481, -0.43791305,\n",
       "        -0.4397471 , -0.43872651, -0.43764499, -0.43595937, -0.44041033,\n",
       "        -0.44076108, -0.43900026, -0.43580688, -0.44036011, -0.43422566,\n",
       "        -0.43648365, -0.43487276, -0.4354303 , -0.43976106, -0.43330092,\n",
       "        -0.43690888, -0.4336891 , -0.43386652, -0.43256564, -0.43059137,\n",
       "        -0.4323879 , -0.43251093, -0.43147487]),\n",
       " 'mean_test_neg_log_loss': array([-0.44855546, -0.4495575 , -0.44450979, -0.4902729 , -0.49735201,\n",
       "        -0.50458567, -0.54344929, -0.53263664, -0.54588404, -0.59355415,\n",
       "        -0.59532128, -0.59831772, -0.45001692, -0.44398417, -0.44448247,\n",
       "        -0.48244308, -0.51420156, -0.49004849, -0.54311258, -0.53918342,\n",
       "        -0.53562563, -0.60310069, -0.60046267, -0.56188007, -0.44959446,\n",
       "        -0.44764392, -0.45470642, -0.48500429, -0.48903197, -0.49132763,\n",
       "        -0.5240299 , -0.5273554 , -0.5174292 , -0.5968078 , -0.56825292,\n",
       "        -0.58452369, -0.45152406, -0.44730209, -0.44815871, -0.46842858,\n",
       "        -0.48282812, -0.47754148, -0.49957451, -0.49422604, -0.50597484,\n",
       "        -0.53806155, -0.53443467, -0.54299926, -0.44695628, -0.44952438,\n",
       "        -0.45965414, -0.51493159, -0.49403568, -0.49801312, -0.54425992,\n",
       "        -0.60377408, -0.55315391, -0.67119927, -0.6745133 , -0.65874118,\n",
       "        -0.44972635, -0.45342022, -0.44350313, -0.5057566 , -0.50446698,\n",
       "        -0.52328712, -0.56593162, -0.60666601, -0.55617756, -0.64373241,\n",
       "        -0.65207909, -0.61968976, -0.44986725, -0.4485724 , -0.45737748,\n",
       "        -0.47464908, -0.49272692, -0.49617907, -0.54623517, -0.5400962 ,\n",
       "        -0.5619132 , -0.59652506, -0.6128459 , -0.61443323, -0.45217961,\n",
       "        -0.44548312, -0.44688865, -0.47626854, -0.4838442 , -0.48303141,\n",
       "        -0.52288948, -0.53151776, -0.52687179, -0.58929092, -0.56858325,\n",
       "        -0.5760552 , -0.44395557, -0.73714377, -0.44222634, -0.43858011,\n",
       "        -0.71109677, -0.43807742, -0.43688234, -0.70053897, -0.43564263,\n",
       "        -0.43660258, -0.68790745, -0.43590429, -0.44293623, -0.73854268,\n",
       "        -0.44340619, -0.43908065, -0.68649293, -0.4380323 , -0.4374087 ,\n",
       "        -0.71266244, -0.43618207, -0.43441787, -0.68936279, -0.43723453,\n",
       "        -0.44165388, -0.70622192, -0.44287052, -0.43948917, -0.71205055,\n",
       "        -0.437783  , -0.4371613 , -0.70451092, -0.436927  , -0.43602056,\n",
       "        -0.69603414, -0.43623827, -0.44317596, -0.72148022, -0.44374966,\n",
       "        -0.44017217, -0.69649664, -0.43900754, -0.43742658, -0.70563874,\n",
       "        -0.43812647, -0.4347734 , -0.70369009, -0.43558041, -0.44214705,\n",
       "        -0.66918526, -0.44824555, -0.43857549, -0.68998143, -0.43924657,\n",
       "        -0.43567192, -0.67909123, -0.43665785, -0.43295704, -0.59215607,\n",
       "        -0.43646524, -0.44390076, -0.75160052, -0.44441494, -0.43710783,\n",
       "        -0.63279279, -0.43699719, -0.43734016, -0.65226917, -0.43838234,\n",
       "        -0.43579497, -0.65255457, -0.43437215, -0.43933425, -0.64983324,\n",
       "        -0.43959516, -0.4390232 , -0.61508976, -0.43695026, -0.43703928,\n",
       "        -0.65101044, -0.43553935, -0.43362131, -0.66310539, -0.43402297,\n",
       "        -0.44120008, -0.66596138, -0.44145317, -0.43670658, -0.66681681,\n",
       "        -0.44133037, -0.43512027, -0.63863632, -0.43810032, -0.43245794,\n",
       "        -0.58910269, -0.43488072, -0.43440345, -0.43535057, -0.43492819,\n",
       "        -0.43268338, -0.43311191, -0.433597  , -0.43204482, -0.43131225,\n",
       "        -0.43218014, -0.43098058, -0.43080993, -0.43091791, -0.43564226,\n",
       "        -0.4344607 , -0.43506362, -0.43292412, -0.43306299, -0.43313117,\n",
       "        -0.43199469, -0.4318246 , -0.43199633, -0.43116558, -0.43154058,\n",
       "        -0.43096365, -0.43449587, -0.43592958, -0.4353594 , -0.43279181,\n",
       "        -0.43308218, -0.43270768, -0.43201948, -0.4312601 , -0.43174923,\n",
       "        -0.43124362, -0.43075373, -0.43126463, -0.43442743, -0.43716358,\n",
       "        -0.435995  , -0.43242662, -0.43234651, -0.43227665, -0.43158913,\n",
       "        -0.4312785 , -0.43142234, -0.43013307, -0.43050793, -0.43055642,\n",
       "        -0.43444702, -0.43420687, -0.43336811, -0.43659219, -0.43585143,\n",
       "        -0.43662325, -0.43875073, -0.43899655, -0.44025916, -0.44015721,\n",
       "        -0.43948901, -0.43795623, -0.43270326, -0.43409044, -0.43261998,\n",
       "        -0.43474401, -0.4350823 , -0.43468257, -0.43877126, -0.43741262,\n",
       "        -0.43866284, -0.44006945, -0.43828765, -0.4354051 , -0.43444514,\n",
       "        -0.43777448, -0.43589078, -0.43517247, -0.43459969, -0.43680769,\n",
       "        -0.43788275, -0.43659038, -0.43644845, -0.43980449, -0.43694672,\n",
       "        -0.43626464, -0.43325678, -0.43380116, -0.4370939 , -0.43222942,\n",
       "        -0.43442113, -0.43222427, -0.43312177, -0.43437768, -0.43405388,\n",
       "        -0.43411495, -0.43377888, -0.43527226]),\n",
       " 'std_test_neg_log_loss': array([0.0096558 , 0.02111564, 0.00724012, 0.02109518, 0.02165116,\n",
       "        0.04759733, 0.03235454, 0.01966859, 0.04040393, 0.02685806,\n",
       "        0.03892675, 0.04762218, 0.01130705, 0.00996121, 0.00860932,\n",
       "        0.01899848, 0.01518221, 0.01046174, 0.02717224, 0.02743342,\n",
       "        0.01815749, 0.01056859, 0.0102889 , 0.03215458, 0.00948948,\n",
       "        0.01105501, 0.01227312, 0.01381775, 0.01852468, 0.02170379,\n",
       "        0.02349001, 0.02064531, 0.01792798, 0.0243136 , 0.02380099,\n",
       "        0.01657155, 0.00863634, 0.00623578, 0.01276542, 0.00969981,\n",
       "        0.00689002, 0.01326355, 0.01719398, 0.01412847, 0.01810242,\n",
       "        0.01457326, 0.01488513, 0.03016606, 0.01234531, 0.00927623,\n",
       "        0.02098456, 0.02503661, 0.01090251, 0.01776389, 0.03733699,\n",
       "        0.09094874, 0.02778654, 0.03568093, 0.05950105, 0.0616418 ,\n",
       "        0.01344207, 0.00922217, 0.01218138, 0.01781909, 0.02562792,\n",
       "        0.05073392, 0.03339183, 0.04576591, 0.03212724, 0.04050172,\n",
       "        0.03684279, 0.03527799, 0.01039597, 0.00706848, 0.02179485,\n",
       "        0.00979819, 0.02782825, 0.01334845, 0.04010551, 0.00839628,\n",
       "        0.04650005, 0.02778399, 0.02177016, 0.03292429, 0.0083445 ,\n",
       "        0.01228775, 0.01126951, 0.01693117, 0.00956764, 0.00945753,\n",
       "        0.01076567, 0.02920331, 0.00429595, 0.0469089 , 0.02095775,\n",
       "        0.01204823, 0.0067589 , 0.01761555, 0.00696195, 0.00617694,\n",
       "        0.01937615, 0.00676405, 0.00558596, 0.032611  , 0.0060186 ,\n",
       "        0.00479624, 0.02940511, 0.00480322, 0.00629783, 0.09807158,\n",
       "        0.00532442, 0.00673855, 0.01679196, 0.00645764, 0.00630121,\n",
       "        0.02194603, 0.0063027 , 0.00520805, 0.01157383, 0.00570851,\n",
       "        0.00629356, 0.02866421, 0.00564143, 0.00558501, 0.01025856,\n",
       "        0.00494334, 0.00494878, 0.01446566, 0.0066529 , 0.00539029,\n",
       "        0.01508697, 0.00590073, 0.00650753, 0.03866275, 0.00778337,\n",
       "        0.00541306, 0.02214113, 0.00397536, 0.00476251, 0.01613523,\n",
       "        0.0064804 , 0.00567822, 0.00871734, 0.00542628, 0.01143369,\n",
       "        0.04094317, 0.00851156, 0.00842357, 0.0485991 , 0.00745492,\n",
       "        0.00707778, 0.09670852, 0.00509041, 0.00503716, 0.03591619,\n",
       "        0.0084835 , 0.00989538, 0.14151829, 0.00695332, 0.00659417,\n",
       "        0.04592284, 0.00735817, 0.00712978, 0.06634893, 0.00746701,\n",
       "        0.00557471, 0.0877051 , 0.00772336, 0.0078933 , 0.06089182,\n",
       "        0.00787788, 0.00557124, 0.06659641, 0.00793959, 0.00642034,\n",
       "        0.0622893 , 0.00752245, 0.0075848 , 0.08984648, 0.00710762,\n",
       "        0.00858174, 0.08969541, 0.00780689, 0.0086244 , 0.06817627,\n",
       "        0.00478877, 0.00702863, 0.06078421, 0.00559042, 0.00794416,\n",
       "        0.08761111, 0.00672183, 0.00564231, 0.00710385, 0.00584828,\n",
       "        0.005496  , 0.00590397, 0.00506632, 0.00570201, 0.00594904,\n",
       "        0.00597539, 0.00607695, 0.0062871 , 0.00587229, 0.00551171,\n",
       "        0.00649918, 0.00488798, 0.00622836, 0.00535539, 0.00630247,\n",
       "        0.00591729, 0.00616444, 0.00620106, 0.00597294, 0.00631979,\n",
       "        0.00599247, 0.00659309, 0.00612993, 0.00554426, 0.00595161,\n",
       "        0.00560012, 0.00623301, 0.00588791, 0.00574813, 0.00528322,\n",
       "        0.00570224, 0.00594894, 0.00591698, 0.00593548, 0.00647628,\n",
       "        0.00564358, 0.00625789, 0.00628722, 0.00557311, 0.00590674,\n",
       "        0.00550723, 0.00583826, 0.00604438, 0.00584907, 0.00591284,\n",
       "        0.00575787, 0.00589515, 0.00744739, 0.00844416, 0.00959763,\n",
       "        0.00861998, 0.00767127, 0.01176988, 0.00864573, 0.00954207,\n",
       "        0.01364045, 0.00779431, 0.00640507, 0.00660321, 0.0076645 ,\n",
       "        0.00970312, 0.00736622, 0.00946915, 0.00889313, 0.00891949,\n",
       "        0.00756779, 0.01042938, 0.01442088, 0.00960401, 0.00805055,\n",
       "        0.0059491 , 0.00522147, 0.00802405, 0.00699468, 0.00796257,\n",
       "        0.01042355, 0.00921296, 0.00926371, 0.01007201, 0.00964061,\n",
       "        0.00895164, 0.00491537, 0.00762259, 0.00939384, 0.00727982,\n",
       "        0.00895075, 0.00603499, 0.00777753, 0.00988174, 0.00890552,\n",
       "        0.00848386, 0.00750427, 0.00824595]),\n",
       " 'rank_test_neg_log_loss': array([171, 174, 163, 196, 202, 206, 226, 218, 228, 242, 243, 246, 178,\n",
       "        160, 162, 189, 209, 195, 225, 222, 220, 248, 247, 232, 175, 168,\n",
       "        182, 193, 194, 197, 214, 216, 211, 245, 235, 238, 179, 167, 169,\n",
       "        185, 190, 188, 204, 200, 208, 221, 219, 224, 166, 173, 184, 210,\n",
       "        199, 203, 227, 249, 230, 268, 269, 263, 176, 181, 156, 207, 205,\n",
       "        213, 234, 250, 231, 257, 260, 254, 177, 172, 183, 186, 198, 201,\n",
       "        229, 223, 233, 244, 251, 252, 180, 164, 165, 187, 192, 191, 212,\n",
       "        217, 215, 240, 236, 237, 159, 286, 151, 128, 282, 122, 102, 277,\n",
       "         81,  97, 272,  86, 153, 287, 155, 135, 271, 121, 114, 284,  90,\n",
       "         57, 273, 112, 149, 281, 152, 139, 283, 118, 110, 279, 103,  89,\n",
       "        275,  91, 154, 285, 157, 144, 276, 133, 116, 280, 124,  67, 278,\n",
       "         79, 150, 267, 170, 127, 274, 136,  82, 270,  99,  37, 241,  94,\n",
       "        158, 288, 161, 109, 255, 106, 113, 261, 126,  83, 262,  54, 137,\n",
       "        258, 140, 134, 253, 105, 107, 259,  78,  46, 264,  49, 146, 265,\n",
       "        148, 100, 266, 147,  72, 256, 123,  30, 239,  68,  56,  75,  69,\n",
       "         32,  40,  45,  23,  14,  24,   8,   5,   6,  80,  62,  70,  36,\n",
       "         38,  42,  20,  19,  21,   9,  16,   7,  63,  87,  76,  35,  39,\n",
       "         34,  22,  11,  18,  10,   4,  12,  59, 111,  88,  29,  28,  27,\n",
       "         17,  13,  15,   1,   2,   3,  61,  53,  44,  96,  84,  98, 130,\n",
       "        132, 145, 143, 138, 120,  33,  51,  31,  66,  71,  65, 131, 115,\n",
       "        129, 142, 125,  77,  60, 117,  85,  73,  64, 101, 119,  95,  93,\n",
       "        141, 104,  92,  43,  48, 108,  26,  58,  25,  41,  55,  50,  52,\n",
       "         47,  74])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRIAL_4_MLP.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FOUR NEG LOG LOSS RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.1,\n",
       " 'classifier__hidden_layer_sizes': (20,),\n",
       " 'classifier__learning_rate': 'constant',\n",
       " 'classifier__solver': 'adam'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL FOUR NEG LOG LOSS RESULTS ---------------------------------------------\")\n",
    "TRIAL_4_MLP.cv_results_['rank_test_neg_log_loss']\n",
    "TRIAL_4_MLP.cv_results_['params'][ np.argmin(TRIAL_4_MLP.cv_results_['rank_test_neg_log_loss']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FOUR F1 RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'tanh',\n",
       " 'classifier__alpha': 0.1,\n",
       " 'classifier__hidden_layer_sizes': (15,),\n",
       " 'classifier__learning_rate': 'constant',\n",
       " 'classifier__solver': 'sgd'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL FOUR F1 RESULTS ---------------------------------------------\")\n",
    "TRIAL_4_MLP.cv_results_['rank_test_f1_micro']\n",
    "TRIAL_4_MLP.cv_results_['params'][ np.argmin(TRIAL_4_MLP.cv_results_['rank_test_f1_micro']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FOUR ROC AUC RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.1,\n",
       " 'classifier__hidden_layer_sizes': (10,),\n",
       " 'classifier__learning_rate': 'adaptive',\n",
       " 'classifier__solver': 'adam'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL FOUR ROC AUC RESULTS ---------------------------------------------\")\n",
    "TRIAL_4_MLP.cv_results_['rank_test_roc_auc_ovo']\n",
    "TRIAL_4_MLP.cv_results_['params'][ np.argmin(TRIAL_4_MLP.cv_results_['rank_test_roc_auc_ovo']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FOUR JACCARD SCORE RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'tanh',\n",
       " 'classifier__alpha': 0.1,\n",
       " 'classifier__hidden_layer_sizes': (15,),\n",
       " 'classifier__learning_rate': 'constant',\n",
       " 'classifier__solver': 'sgd'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL FOUR JACCARD SCORE RESULTS ---------------------------------------------\")\n",
    "TRIAL_4_MLP.cv_results_['rank_test_jaccard']\n",
    "TRIAL_4_MLP.cv_results_['params'][ np.argmin(TRIAL_4_MLP.cv_results_['rank_test_jaccard']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 4 Multilayered Perceptrons using best NEG LOG LOSS hyperperameters :0.7468597268533321\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 4 Multilayered Perceptrons using best F1 hyperperameters :0.5068058283469602\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 4 Multilayered Perceptrons using best ROC_AUC hyperperameters :0.7756360480518887\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 4 Multilayered Perceptrons using best JACCARD hyperperameters :0.5058237792901841\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best NEG LOG LOSS results to train on test set, \n",
    "bestMPLT4_1 = MLPClassifier(activation = 'logistic', alpha = .1, hidden_layer_sizes = (20,), \n",
    "                          learning_rate = 'constant',solver = 'adam')\n",
    "bestMPLT4_1.fit(X_train,y_train)\n",
    "y_pred4_1 = bestMPLT4_1.predict(X_test)\n",
    "print('Accuracy of Trial 4 Multilayered Perceptrons using best NEG LOG LOSS hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred4_1)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best F1 results to train on test set, \n",
    "bestMPLT4_2 = MLPClassifier(activation = 'tanh', alpha = 0.1, hidden_layer_sizes = (15,), \n",
    "                          learning_rate = 'constant',solver = 'sgd')\n",
    "bestMPLT4_2.fit(X_train,y_train)\n",
    "y_pred4_2 = bestMPLT4_2.predict(X_test)\n",
    "print('Accuracy of Trial 4 Multilayered Perceptrons using best F1 hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred4_2)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best ROC_AUC results to train on test set,\n",
    "bestMPLT4_3 = MLPClassifier(activation = 'logistic', alpha = 0.1, hidden_layer_sizes = (10,), \n",
    "                          learning_rate = 'adaptive',solver = 'adam')\n",
    "bestMPLT4_3.fit(X_train,y_train)\n",
    "y_pred4_3 = bestMPLT4_3.predict(X_test)\n",
    "print('Accuracy of Trial 4 Multilayered Perceptrons using best ROC_AUC hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred4_3)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best JACCARD results to train on test set, \n",
    "bestMPLT4_4 = MLPClassifier(activation = 'tanh', alpha = 0.1, hidden_layer_sizes = (15,), \n",
    "                          learning_rate = 'constant',solver = 'sgd')\n",
    "bestMPLT4_4.fit(X_train,y_train)\n",
    "y_pred4_4 = bestMPLT4_4.predict(X_test)\n",
    "print('Accuracy of Trial 4 Multilayered Perceptrons using best JACCARD hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred4_4)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTI LAYERED PERCEPTRON TRIAL FIVE ON LEAGUE OF LEGENDS DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:    8.9s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    9.7s\n",
      "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed:   10.8s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   13.1s\n",
      "[Parallel(n_jobs=-1)]: Done 189 tasks      | elapsed:   14.1s\n",
      "[Parallel(n_jobs=-1)]: Done 210 tasks      | elapsed:   15.4s\n",
      "[Parallel(n_jobs=-1)]: Done 233 tasks      | elapsed:   17.7s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:   19.1s\n",
      "[Parallel(n_jobs=-1)]: Done 281 tasks      | elapsed:   21.0s\n",
      "[Parallel(n_jobs=-1)]: Done 306 tasks      | elapsed:   23.4s\n",
      "[Parallel(n_jobs=-1)]: Done 333 tasks      | elapsed:   25.9s\n",
      "[Parallel(n_jobs=-1)]: Done 360 tasks      | elapsed:   28.8s\n",
      "[Parallel(n_jobs=-1)]: Done 389 tasks      | elapsed:   30.7s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:   34.3s\n",
      "[Parallel(n_jobs=-1)]: Done 449 tasks      | elapsed:   36.1s\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:   39.4s\n",
      "[Parallel(n_jobs=-1)]: Done 513 tasks      | elapsed:   42.9s\n",
      "[Parallel(n_jobs=-1)]: Done 546 tasks      | elapsed:   45.9s\n",
      "[Parallel(n_jobs=-1)]: Done 581 tasks      | elapsed:   48.7s\n",
      "[Parallel(n_jobs=-1)]: Done 616 tasks      | elapsed:   52.4s\n",
      "[Parallel(n_jobs=-1)]: Done 653 tasks      | elapsed:   55.9s\n",
      "[Parallel(n_jobs=-1)]: Done 690 tasks      | elapsed:   59.1s\n",
      "[Parallel(n_jobs=-1)]: Done 729 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 809 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 850 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 893 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 936 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 981 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1026 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1073 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1120 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1169 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1269 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1320 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1373 tasks      | elapsed:  2.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FIVE RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1440 out of 1440 | elapsed:  2.3min finished\n"
     ]
    }
   ],
   "source": [
    "ITERATIONS = 1\n",
    "for i in range(1):\n",
    "    #For reach \"trial\", randomly select 5000 data points\n",
    "    random5000DataPoints = leagueData.sample(n = 5000, replace = True)\n",
    "    #From that 5000, create x set and y set\n",
    "    xSet = random5000DataPoints.iloc[:,1:17].values\n",
    "    ySet = random5000DataPoints.iloc[:,0].values\n",
    "    # Run GridSearch to find best hyperparameters per 5000 data points \n",
    "    \n",
    "    pipe = Pipeline([('std', StandardScaler()),('classifier', MLPClassifier(max_iter = 250))])\n",
    "\n",
    "    search_space = [\n",
    "                {\n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                {\n",
    "                 \n",
    "                  \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['sgd'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                \n",
    "                {\n",
    "                \n",
    "                 \n",
    "                 'classifier': [MLPClassifier(max_iter = 250)],\n",
    "                 'classifier__hidden_layer_sizes': [(5,),(10,),(15,),(20,)],\n",
    "                 'classifier__activation': ['logistic','tanh'],\n",
    "                 'classifier__solver': ['adam'],\n",
    "                 'classifier__alpha': [0.0001,0.001,0.01,.1],\n",
    "                 'classifier__learning_rate': ['constant','invscaling','adaptive']},\n",
    "                \n",
    "                ]\n",
    "    \n",
    "    runHyperParamSearch = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=5), \n",
    "                          scoring=['recall_micro', 'f1_micro','precision_micro','roc_auc_ovo','jaccard','neg_log_loss'], \n",
    "                          refit=False, verbose=10, n_jobs = -1)\n",
    "    TRIAL_5_MLP = runHyperParamSearch.fit(xSet, ySet)\n",
    "    print(\"---------------------------------TRIAL FIVE RESULTS ---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.59721456, 0.63234506, 0.68519006, 0.90417891, 0.91448693,\n",
       "        0.95291839, 1.39529977, 1.46976461, 1.48327513, 1.70496883,\n",
       "        1.68855252, 1.63910985, 0.79038043, 0.63744707, 0.62503662,\n",
       "        0.83191557, 0.87084894, 0.91818886, 1.34335561, 1.43002863,\n",
       "        1.46616025, 1.74930687, 1.68344731, 1.58686399, 0.73473115,\n",
       "        0.65646305, 0.62934103, 0.83491726, 0.84783001, 0.90758204,\n",
       "        1.31272764, 1.36127067, 1.35476618, 1.7093698 , 1.7448009 ,\n",
       "        1.6427115 , 0.69529538, 0.61272764, 0.67217822, 0.81800385,\n",
       "        0.85503573, 0.90938325, 1.31293015, 1.32754269, 1.35466499,\n",
       "        1.74449768, 1.69825883, 1.63570638, 0.77466569, 0.7187151 ,\n",
       "        0.68108597, 0.99375434, 0.97964315, 1.03068609, 1.49578667,\n",
       "        1.52971506, 1.61128559, 1.95808458, 1.94527173, 1.86199927,\n",
       "        0.77606483, 0.70750885, 0.69919982, 0.99665737, 1.09574285,\n",
       "        1.15579433, 1.67914333, 1.98020339, 1.74730258, 2.29046998,\n",
       "        2.04495821, 2.23171926, 0.8784544 , 0.94711328, 0.9230927 ,\n",
       "        1.12646899, 1.19042459, 1.18441901, 1.59277091, 1.64030833,\n",
       "        1.70826907, 2.1299315 , 2.0821897 , 2.02954659, 0.89627051,\n",
       "        0.73873663, 0.71551595, 1.01637306, 1.02718382, 1.06381407,\n",
       "        1.54763193, 1.57745595, 1.59827237, 2.10150719, 1.97820115,\n",
       "        1.90113473, 2.11952152, 0.15723462, 2.46872363, 1.76912112,\n",
       "        0.18545933, 2.40056443, 1.82036605, 0.24611115, 2.33460803,\n",
       "        1.68805161, 0.3370893 , 2.26384635, 1.86470385, 0.15953693,\n",
       "        2.35972977, 1.65172048, 0.19306617, 2.22000918, 1.69245605,\n",
       "        0.27754812, 2.53347902, 1.93106031, 0.26622915, 2.7744863 ,\n",
       "        2.2459311 , 0.18435912, 2.30728426, 1.81946516, 0.18616052,\n",
       "        2.22201099, 1.74029689, 0.19256597, 2.33660965, 1.75891209,\n",
       "        0.20827942, 2.35762777, 2.1212244 , 0.15873652, 2.34621792,\n",
       "        1.67063675, 0.16033812, 2.34701905, 1.72788615, 0.2555203 ,\n",
       "        2.33921156, 1.68314724, 0.20888   , 2.28856831, 1.02468166,\n",
       "        0.57679572, 1.63010259, 0.86414304, 1.06161313, 1.45955563,\n",
       "        0.85383468, 1.29601445, 1.48107405, 0.81710343, 2.14334369,\n",
       "        1.52090802, 1.03158674, 0.63324499, 1.62689857, 0.84782925,\n",
       "        1.20693822, 1.51730485, 1.00376353, 1.20753756, 1.4171185 ,\n",
       "        0.92809792, 1.4969872 , 1.57565546, 0.92819786, 0.67838321,\n",
       "        1.51750493, 0.87845564, 1.33394723, 1.49328427, 0.99255409,\n",
       "        1.1281702 , 1.64191184, 0.77476625, 1.67824321, 1.58516335,\n",
       "        0.94711466, 1.23426113, 1.62860107, 0.94391193, 0.63974986,\n",
       "        1.49048171, 0.92919922, 1.37968626, 1.48557787, 0.81700258,\n",
       "        1.7274857 , 1.50299301, 1.71507497, 1.9878087 , 1.51300135,\n",
       "        1.16350074, 1.24036684, 1.28770752, 1.24837384, 1.04429865,\n",
       "        1.07052021, 1.12096376, 1.00526514, 1.01056881, 1.5262125 ,\n",
       "        1.76051364, 1.69485736, 1.19963155, 1.27439618, 1.30071869,\n",
       "        1.15028973, 1.13127279, 1.03909421, 1.16860504, 1.11786127,\n",
       "        1.21134176, 1.64331279, 1.60137734, 1.73989582, 1.08393273,\n",
       "        1.24216833, 1.42442489, 1.17911386, 1.28080115, 1.00296249,\n",
       "        1.07292247, 1.1314733 , 1.34175348, 1.36427293, 1.46806183,\n",
       "        1.44634395, 1.19773006, 1.25357795, 1.21384387, 1.15799537,\n",
       "        1.11686096, 1.31533098, 1.09704332, 1.12636843, 1.20343494,\n",
       "        1.68765159, 1.88432102, 1.81115742, 2.59062796, 2.67139778,\n",
       "        3.0254015 , 3.3816082 , 3.21426373, 3.19835072, 3.35928907,\n",
       "        3.35468655, 3.32435856, 1.919451  , 1.96368885, 1.69655943,\n",
       "        2.19668918, 2.72564406, 2.34351568, 3.14830737, 3.17883401,\n",
       "        2.7868968 , 3.38961463, 3.34807963, 3.34597721, 1.70076289,\n",
       "        1.79424286, 1.81255836, 2.38124776, 2.41877975, 2.59403043,\n",
       "        3.14940805, 3.18533955, 3.2229722 , 3.34117346, 3.34877982,\n",
       "        3.3378705 , 1.3939991 , 1.50079088, 1.45795407, 1.9700942 ,\n",
       "        1.85769801, 1.95538168, 2.33090467, 2.7626761 , 2.77588716,\n",
       "        3.26100459, 2.40957189, 2.24563065]),\n",
       " 'std_fit_time': array([0.00872858, 0.05170001, 0.06029001, 0.05737546, 0.06601468,\n",
       "        0.07845496, 0.07136817, 0.08388584, 0.06777183, 0.06648536,\n",
       "        0.05397511, 0.04174559, 0.04702966, 0.01987178, 0.0141308 ,\n",
       "        0.01619622, 0.0553228 , 0.02957098, 0.02390188, 0.06816643,\n",
       "        0.06907255, 0.04216813, 0.05586641, 0.02392404, 0.06554105,\n",
       "        0.01435542, 0.00641404, 0.02137911, 0.02183531, 0.02184515,\n",
       "        0.05387371, 0.03350295, 0.01833351, 0.02733472, 0.04980815,\n",
       "        0.10198913, 0.09811186, 0.02714677, 0.04978727, 0.03055909,\n",
       "        0.03432502, 0.02848254, 0.0238002 , 0.03166032, 0.01529991,\n",
       "        0.03903119, 0.06122939, 0.0383166 , 0.06827784, 0.02531959,\n",
       "        0.01826659, 0.02754537, 0.02293416, 0.0142479 , 0.05280057,\n",
       "        0.03892237, 0.04700231, 0.09432702, 0.09028532, 0.08104284,\n",
       "        0.0425548 , 0.03380046, 0.02643124, 0.05533267, 0.08519784,\n",
       "        0.05491645, 0.10289401, 0.26455592, 0.23734236, 0.20192643,\n",
       "        0.1649533 , 0.11753409, 0.09046003, 0.05497606, 0.10639246,\n",
       "        0.12020378, 0.12324254, 0.07940964, 0.10044298, 0.13383746,\n",
       "        0.07713333, 0.06295409, 0.10824238, 0.07859487, 0.0978464 ,\n",
       "        0.032133  , 0.01541221, 0.01709402, 0.01849281, 0.03201952,\n",
       "        0.06140036, 0.05482549, 0.04849777, 0.1204879 , 0.04716376,\n",
       "        0.01488514, 0.22448113, 0.02133154, 0.16687013, 0.10173794,\n",
       "        0.0197722 , 0.18238485, 0.09144711, 0.02406981, 0.08582943,\n",
       "        0.05339304, 0.13298726, 0.03207323, 0.16988017, 0.03238706,\n",
       "        0.12394545, 0.18484045, 0.01243431, 0.09188129, 0.06677869,\n",
       "        0.14556926, 0.14489845, 0.13440908, 0.09617165, 0.23625265,\n",
       "        0.24018669, 0.02558125, 0.13768052, 0.09255998, 0.03124374,\n",
       "        0.1152309 , 0.06327077, 0.04918835, 0.07655912, 0.12178333,\n",
       "        0.02627449, 0.06386367, 0.16113345, 0.02032354, 0.08270907,\n",
       "        0.14342991, 0.00922012, 0.1585297 , 0.03240432, 0.17442326,\n",
       "        0.11171028, 0.11082051, 0.03611622, 0.05370505, 0.23561563,\n",
       "        0.17224858, 0.1898158 , 0.16392316, 0.68928548, 0.14026374,\n",
       "        0.11121331, 0.83029358, 0.17703927, 0.03730311, 0.75961728,\n",
       "        0.13452036, 0.06623601, 0.35241386, 0.2994113 , 0.21322612,\n",
       "        0.57834593, 0.1566474 , 0.18179392, 0.57138125, 0.05378933,\n",
       "        0.15637713, 0.83061427, 0.10176763, 0.15577374, 0.41857313,\n",
       "        0.12911152, 0.08867928, 0.75292029, 0.13233189, 0.18538577,\n",
       "        0.59363607, 0.2510052 , 0.15973868, 0.69647956, 0.16829615,\n",
       "        0.19345524, 0.69282217, 0.10867896, 0.13467809, 0.3802404 ,\n",
       "        0.05748643, 0.14931178, 1.00573316, 0.12196894, 0.18284663,\n",
       "        1.21175904, 0.12198317, 0.1810583 , 0.17327504, 0.30159918,\n",
       "        0.08233124, 0.17732807, 0.10542878, 0.10123564, 0.17822192,\n",
       "        0.2478467 , 0.28223275, 0.13704458, 0.15856633, 0.18814859,\n",
       "        0.25668937, 0.23713048, 0.27513094, 0.09625194, 0.20858732,\n",
       "        0.32505049, 0.21554513, 0.14233566, 0.20926534, 0.34156412,\n",
       "        0.2691271 , 0.16954759, 0.06576987, 0.30544489, 0.0526304 ,\n",
       "        0.18230927, 0.18589833, 0.24163518, 0.36495016, 0.16057693,\n",
       "        0.18820145, 0.23898095, 0.25177522, 0.08542052, 0.0778952 ,\n",
       "        0.18386046, 0.19536642, 0.18826825, 0.18828159, 0.09729565,\n",
       "        0.26599984, 0.26033794, 0.0921831 , 0.12695354, 0.18124787,\n",
       "        0.35288391, 0.34636415, 0.3197349 , 0.51088537, 0.58469663,\n",
       "        0.19341587, 0.15796501, 0.03836183, 0.05169077, 0.05605001,\n",
       "        0.01877981, 0.03605166, 0.50345436, 0.42199504, 0.3333888 ,\n",
       "        0.56738756, 0.14176273, 0.65704259, 0.07734283, 0.09858122,\n",
       "        0.81771777, 0.05498127, 0.08216729, 0.03860592, 0.22567828,\n",
       "        0.53088966, 0.1928396 , 0.46797402, 0.61016355, 0.33431852,\n",
       "        0.05921983, 0.07498242, 0.07103476, 0.08508791, 0.05813065,\n",
       "        0.05481739, 0.17606289, 0.41532119, 0.27027404, 0.37508994,\n",
       "        0.43582146, 0.32435161, 0.67896189, 0.80379006, 0.49290655,\n",
       "        0.06981277, 0.43354664, 0.29893764]),\n",
       " 'mean_score_time': array([0.01130977, 0.01371169, 0.01251087, 0.01501393, 0.01240983,\n",
       "        0.01801605, 0.0157186 , 0.01631455, 0.01541543, 0.01661205,\n",
       "        0.01291261, 0.01210985, 0.0134119 , 0.0114109 , 0.01151171,\n",
       "        0.01151056, 0.01341219, 0.0162127 , 0.0163166 , 0.01511455,\n",
       "        0.01501226, 0.01501031, 0.0170146 , 0.01170945, 0.0123105 ,\n",
       "        0.01120973, 0.0115098 , 0.01361184, 0.01901593, 0.01471457,\n",
       "        0.01511488, 0.01361079, 0.01571412, 0.01320996, 0.0120101 ,\n",
       "        0.01120925, 0.01190958, 0.01080842, 0.01131015, 0.01251073,\n",
       "        0.01341147, 0.01421118, 0.01581192, 0.01771517, 0.01631408,\n",
       "        0.01551352, 0.01251187, 0.01150918, 0.01110973, 0.01151037,\n",
       "        0.0113091 , 0.01221013, 0.01271162, 0.01411424, 0.02141876,\n",
       "        0.01481566, 0.0169147 , 0.02051659, 0.01291051, 0.0132113 ,\n",
       "        0.01241059, 0.01351066, 0.01421256, 0.01801529, 0.01961656,\n",
       "        0.0142148 , 0.01871643, 0.02101822, 0.01741576, 0.02171879,\n",
       "        0.02091722, 0.01671424, 0.0202178 , 0.0136117 , 0.01201181,\n",
       "        0.01261082, 0.01260986, 0.01181054, 0.01521168, 0.02231989,\n",
       "        0.01811604, 0.02261982, 0.01571383, 0.01291113, 0.01841574,\n",
       "        0.01150904, 0.01150942, 0.01451316, 0.01261034, 0.01651411,\n",
       "        0.01351013, 0.01531487, 0.02122025, 0.01831474, 0.01341023,\n",
       "        0.01471314, 0.00940828, 0.0104095 , 0.01421208, 0.01121006,\n",
       "        0.00950837, 0.00940814, 0.0105092 , 0.01121039, 0.00980816,\n",
       "        0.01020899, 0.00990887, 0.00970879, 0.01130929, 0.009408  ,\n",
       "        0.00950861, 0.01030922, 0.01251073, 0.01110973, 0.00990782,\n",
       "        0.01079998, 0.01191034, 0.01141019, 0.01301146, 0.01000853,\n",
       "        0.00940814, 0.00990815, 0.00910821, 0.0097085 , 0.00940795,\n",
       "        0.00950861, 0.01020842, 0.01040864, 0.00970807, 0.01130996,\n",
       "        0.00990791, 0.01000848, 0.00930824, 0.00930805, 0.00990915,\n",
       "        0.00940814, 0.01100898, 0.00950794, 0.01020865, 0.00980802,\n",
       "        0.00950809, 0.0105094 , 0.00980835, 0.01020856, 0.00910754,\n",
       "        0.00950799, 0.00940723, 0.00960865, 0.00940852, 0.0098084 ,\n",
       "        0.00990839, 0.01010904, 0.01030898, 0.0103085 , 0.01040902,\n",
       "        0.01090965, 0.00940824, 0.009308  , 0.00930815, 0.01010876,\n",
       "        0.00990844, 0.00980835, 0.0106091 , 0.01191039, 0.01000919,\n",
       "        0.0107091 , 0.01120958, 0.01040912, 0.01161022, 0.0109097 ,\n",
       "        0.009408  , 0.00960836, 0.0098083 , 0.01080914, 0.00990896,\n",
       "        0.00990887, 0.01020865, 0.01010909, 0.01030898, 0.01040888,\n",
       "        0.00940814, 0.00910783, 0.00920749, 0.01120939, 0.00960846,\n",
       "        0.00980854, 0.01020865, 0.01361203, 0.01000862, 0.01040888,\n",
       "        0.01020927, 0.01020913, 0.00930815, 0.01120987, 0.00950813,\n",
       "        0.00990891, 0.00970826, 0.00950794, 0.01000915, 0.01190987,\n",
       "        0.01080971, 0.01080966, 0.0101089 , 0.010109  , 0.01080928,\n",
       "        0.00890779, 0.00960841, 0.01020856, 0.00980868, 0.01000886,\n",
       "        0.01090956, 0.00980859, 0.01010861, 0.01020861, 0.0106092 ,\n",
       "        0.01010866, 0.00950832, 0.00900741, 0.00940843, 0.00930781,\n",
       "        0.00980883, 0.00980859, 0.01040921, 0.00980859, 0.00990853,\n",
       "        0.01030898, 0.01271081, 0.01060915, 0.00900798, 0.00950837,\n",
       "        0.00970836, 0.01060967, 0.01090913, 0.00970874, 0.00950885,\n",
       "        0.00970836, 0.00950842, 0.00980859, 0.00960803, 0.00990877,\n",
       "        0.00900812, 0.00970855, 0.01060953, 0.01110916, 0.00970812,\n",
       "        0.01040926, 0.01090932, 0.01080909, 0.0105094 , 0.01040912,\n",
       "        0.01100783, 0.0101088 , 0.009408  , 0.01030869, 0.01040897,\n",
       "        0.00970826, 0.01100922, 0.00980849, 0.01040897, 0.01010842,\n",
       "        0.01080961, 0.012711  , 0.01201005, 0.01100969, 0.0098083 ,\n",
       "        0.00960851, 0.00940833, 0.00980897, 0.01010895, 0.01060915,\n",
       "        0.01090984, 0.01040869, 0.01030912, 0.0111094 , 0.01030865,\n",
       "        0.01020904, 0.00930824, 0.00910816, 0.00980849, 0.00960855,\n",
       "        0.00990849, 0.00970798, 0.01000919, 0.01070933, 0.00990868,\n",
       "        0.00950847, 0.00740681, 0.00600486]),\n",
       " 'std_score_time': array([2.44717299e-04, 4.46065695e-03, 2.66673426e-03, 3.14803990e-03,\n",
       "        1.31977238e-03, 5.39051698e-03, 3.44565141e-03, 3.71338889e-03,\n",
       "        1.59439413e-03, 2.26737734e-03, 9.71208488e-04, 1.28243175e-03,\n",
       "        1.52947129e-03, 6.64292136e-04, 1.55112865e-03, 8.37312872e-04,\n",
       "        1.82919811e-03, 3.53317388e-03, 1.66769760e-03, 1.28161991e-03,\n",
       "        2.07367819e-03, 2.82958784e-03, 6.83936051e-03, 1.16626164e-03,\n",
       "        1.29091926e-03, 1.16699760e-03, 9.50072994e-04, 3.76356071e-03,\n",
       "        6.50945905e-03, 1.20954054e-03, 1.46490473e-03, 1.32045327e-03,\n",
       "        1.20780557e-03, 5.09372255e-04, 8.36175038e-04, 5.10081648e-04,\n",
       "        1.85753050e-03, 6.00606704e-04, 6.78010485e-04, 1.26642221e-03,\n",
       "        1.74614396e-03, 1.12491946e-03, 3.69971228e-03, 3.49062022e-03,\n",
       "        2.62170163e-03, 2.02580618e-03, 6.32711641e-04, 3.16356498e-04,\n",
       "        1.02031088e-03, 1.51731440e-03, 1.36428239e-03, 8.14095298e-04,\n",
       "        1.25056632e-03, 1.02354241e-03, 1.09161287e-02, 1.36502187e-03,\n",
       "        2.99409541e-03, 5.68730605e-03, 3.74777584e-04, 1.69154149e-03,\n",
       "        1.02034274e-03, 2.96948021e-03, 2.40098876e-03, 3.14865378e-03,\n",
       "        5.11731865e-03, 2.78641047e-03, 4.13360760e-03, 2.25964394e-03,\n",
       "        5.18496658e-03, 7.54035278e-03, 1.11465309e-02, 5.06009781e-03,\n",
       "        1.34521682e-02, 1.82968859e-03, 1.00171592e-03, 1.06854211e-03,\n",
       "        3.75212570e-04, 1.07787643e-03, 3.26702796e-03, 7.16602789e-03,\n",
       "        2.88840003e-03, 5.20485005e-03, 1.80734934e-03, 1.11457712e-03,\n",
       "        5.64875295e-03, 1.05109806e-03, 8.36970179e-04, 1.79053773e-03,\n",
       "        8.61524229e-04, 2.91838051e-03, 1.05012140e-03, 1.32677091e-03,\n",
       "        3.29779217e-03, 6.13318725e-03, 1.11488868e-03, 5.92605378e-03,\n",
       "        3.74878638e-04, 1.11445708e-03, 4.34660739e-03, 1.53807023e-03,\n",
       "        5.47987623e-04, 2.00343437e-04, 4.47980971e-04, 1.24997812e-03,\n",
       "        3.99995372e-04, 6.78619888e-04, 6.64110925e-04, 2.45067515e-04,\n",
       "        2.82425927e-03, 3.74546497e-04, 7.07392946e-04, 8.72280026e-04,\n",
       "        4.04029889e-03, 1.39355630e-03, 3.74317187e-04, 1.86955821e-03,\n",
       "        2.61811263e-03, 2.01204999e-03, 4.15051172e-03, 7.07763844e-04,\n",
       "        2.00105132e-04, 1.20093037e-03, 2.00319773e-04, 2.44815441e-04,\n",
       "        2.00130004e-04, 5.00111031e-07, 6.78999528e-04, 3.74355709e-04,\n",
       "        2.45262693e-04, 2.87630531e-03, 3.74725277e-04, 1.16800773e-07,\n",
       "        4.00614806e-04, 5.10388759e-04, 7.35033846e-04, 2.00224519e-04,\n",
       "        2.39018029e-03, 6.64157308e-07, 5.10388826e-04, 4.00281446e-04,\n",
       "        6.28991411e-07, 4.47661129e-04, 4.00209498e-04, 5.10239142e-04,\n",
       "        1.99937906e-04, 8.37112377e-04, 2.00009538e-04, 1.99723322e-04,\n",
       "        5.83649994e-04, 6.00410159e-04, 2.00010163e-04, 3.74330551e-04,\n",
       "        6.00735493e-04, 2.44698037e-04, 3.74330247e-04, 1.32036504e-03,\n",
       "        3.74482780e-04, 4.00316867e-04, 2.44931406e-04, 8.60813235e-04,\n",
       "        7.35553010e-04, 2.44835585e-04, 1.02046877e-03, 2.26869843e-03,\n",
       "        3.16431402e-04, 6.78683207e-04, 1.16702188e-03, 3.74737734e-04,\n",
       "        3.23325251e-03, 2.59893832e-03, 3.74228017e-04, 2.00343551e-04,\n",
       "        4.00126019e-04, 3.11085270e-03, 3.74686654e-04, 2.00367485e-04,\n",
       "        2.44756170e-04, 1.99725200e-04, 2.45281947e-04, 4.90660616e-04,\n",
       "        1.99986016e-04, 2.00510053e-04, 2.45184501e-04, 2.67781916e-03,\n",
       "        3.74878093e-04, 3.99947945e-04, 5.10295354e-04, 6.46653808e-03,\n",
       "        3.16657407e-04, 5.83723553e-04, 2.44834471e-04, 2.45145753e-04,\n",
       "        5.99837443e-04, 3.67191623e-03, 5.48161905e-04, 4.90466111e-04,\n",
       "        6.00266483e-04, 4.86280395e-07, 3.16054510e-04, 3.12382624e-03,\n",
       "        2.36003906e-03, 1.40123712e-03, 3.74686897e-04, 4.90261612e-04,\n",
       "        2.13729738e-03, 3.74164520e-04, 3.74571981e-04, 1.91490569e-03,\n",
       "        4.00138452e-04, 1.04968778e-03, 1.71609542e-03, 2.44639556e-04,\n",
       "        3.75272922e-04, 2.45184501e-04, 8.60785500e-04, 3.74941621e-04,\n",
       "        6.33239089e-04, 3.16506798e-04, 2.00367258e-04, 2.45242751e-04,\n",
       "        6.00711560e-04, 8.72482506e-04, 1.56364323e-03, 4.00030807e-04,\n",
       "        3.74380963e-04, 2.45087573e-04, 5.40423394e-03, 1.24161526e-03,\n",
       "        3.16959021e-04, 6.32862131e-04, 7.48914517e-04, 1.49756131e-03,\n",
       "        2.39804052e-03, 2.45106619e-04, 3.01578299e-07, 2.45321360e-04,\n",
       "        4.47501077e-04, 2.45126321e-04, 1.99914000e-04, 6.63665129e-04,\n",
       "        6.10649513e-07, 1.40075687e-03, 2.26909367e-03, 1.11454701e-03,\n",
       "        4.00090569e-04, 8.00627631e-04, 1.06821998e-03, 9.28035070e-04,\n",
       "        4.47874352e-04, 2.00271663e-04, 9.50981001e-04, 5.83216594e-04,\n",
       "        4.90271458e-04, 1.43664775e-03, 8.00115013e-04, 4.00495714e-04,\n",
       "        2.51159344e-03, 6.78648280e-04, 5.83699043e-04, 1.99938076e-04,\n",
       "        2.45145475e-04, 4.66896716e-03, 2.51199240e-03, 6.32937647e-04,\n",
       "        1.12336320e-03, 3.74929086e-04, 3.74508299e-04, 7.49590066e-04,\n",
       "        5.83355591e-04, 1.28187995e-03, 1.11472685e-03, 1.99939270e-04,\n",
       "        2.45009335e-04, 9.70355924e-04, 4.00376586e-04, 2.45418242e-04,\n",
       "        2.45010078e-04, 2.00343551e-04, 7.48952755e-04, 1.99771461e-04,\n",
       "        3.75005381e-04, 6.78542567e-04, 5.47770172e-04, 6.78859221e-04,\n",
       "        2.00391234e-04, 6.33465460e-04, 1.53155591e-03, 3.16506511e-04]),\n",
       " 'param_classifier': masked_array(data=[MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250),\n",
       "                    MLPClassifier(max_iter=250)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__activation': masked_array(data=['logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__alpha': masked_array(data=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__hidden_layer_sizes': masked_array(data=[(5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,),\n",
       "                    (5,), (5,), (5,), (10,), (10,), (10,), (15,), (15,),\n",
       "                    (15,), (20,), (20,), (20,), (5,), (5,), (5,), (10,),\n",
       "                    (10,), (10,), (15,), (15,), (15,), (20,), (20,), (20,)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__learning_rate': masked_array(data=['constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive',\n",
       "                    'constant', 'invscaling', 'adaptive', 'constant',\n",
       "                    'invscaling', 'adaptive', 'constant', 'invscaling',\n",
       "                    'adaptive', 'constant', 'invscaling', 'adaptive'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd', 'sgd',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'sgd'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'logistic',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.0001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.001,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.01,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (5,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (10,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (15,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'constant',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'invscaling',\n",
       "   'classifier__solver': 'adam'},\n",
       "  {'classifier': MLPClassifier(max_iter=250),\n",
       "   'classifier__activation': 'tanh',\n",
       "   'classifier__alpha': 0.1,\n",
       "   'classifier__hidden_layer_sizes': (20,),\n",
       "   'classifier__learning_rate': 'adaptive',\n",
       "   'classifier__solver': 'adam'}],\n",
       " 'split0_test_recall_micro': array([0.758, 0.762, 0.775, 0.744, 0.747, 0.745, 0.745, 0.74 , 0.74 ,\n",
       "        0.739, 0.732, 0.725, 0.775, 0.782, 0.779, 0.746, 0.748, 0.743,\n",
       "        0.743, 0.735, 0.741, 0.737, 0.74 , 0.727, 0.763, 0.755, 0.767,\n",
       "        0.756, 0.748, 0.751, 0.745, 0.732, 0.73 , 0.757, 0.733, 0.739,\n",
       "        0.767, 0.76 , 0.775, 0.748, 0.746, 0.761, 0.741, 0.748, 0.75 ,\n",
       "        0.745, 0.748, 0.721, 0.759, 0.762, 0.766, 0.738, 0.736, 0.73 ,\n",
       "        0.753, 0.739, 0.727, 0.749, 0.742, 0.741, 0.776, 0.755, 0.765,\n",
       "        0.74 , 0.754, 0.75 , 0.745, 0.732, 0.727, 0.722, 0.722, 0.724,\n",
       "        0.763, 0.761, 0.759, 0.741, 0.741, 0.764, 0.743, 0.726, 0.759,\n",
       "        0.726, 0.733, 0.733, 0.761, 0.765, 0.763, 0.754, 0.763, 0.75 ,\n",
       "        0.743, 0.749, 0.735, 0.749, 0.745, 0.752, 0.77 , 0.512, 0.763,\n",
       "        0.771, 0.489, 0.77 , 0.77 , 0.62 , 0.766, 0.766, 0.511, 0.768,\n",
       "        0.772, 0.511, 0.765, 0.769, 0.513, 0.768, 0.764, 0.513, 0.766,\n",
       "        0.767, 0.533, 0.768, 0.772, 0.499, 0.766, 0.765, 0.489, 0.768,\n",
       "        0.764, 0.489, 0.769, 0.766, 0.511, 0.763, 0.767, 0.488, 0.762,\n",
       "        0.767, 0.477, 0.768, 0.767, 0.511, 0.768, 0.765, 0.556, 0.763,\n",
       "        0.765, 0.508, 0.768, 0.77 , 0.568, 0.772, 0.772, 0.611, 0.765,\n",
       "        0.767, 0.537, 0.774, 0.759, 0.685, 0.772, 0.764, 0.494, 0.773,\n",
       "        0.767, 0.559, 0.77 , 0.767, 0.547, 0.775, 0.765, 0.478, 0.765,\n",
       "        0.765, 0.53 , 0.768, 0.77 , 0.621, 0.761, 0.766, 0.602, 0.767,\n",
       "        0.771, 0.386, 0.771, 0.766, 0.686, 0.772, 0.77 , 0.72 , 0.764,\n",
       "        0.762, 0.726, 0.765, 0.762, 0.761, 0.767, 0.764, 0.759, 0.767,\n",
       "        0.765, 0.763, 0.767, 0.765, 0.76 , 0.765, 0.769, 0.761, 0.763,\n",
       "        0.763, 0.762, 0.763, 0.766, 0.761, 0.762, 0.761, 0.762, 0.759,\n",
       "        0.763, 0.765, 0.764, 0.762, 0.763, 0.764, 0.761, 0.764, 0.764,\n",
       "        0.761, 0.762, 0.763, 0.762, 0.766, 0.768, 0.761, 0.765, 0.766,\n",
       "        0.762, 0.761, 0.765, 0.764, 0.764, 0.762, 0.767, 0.771, 0.766,\n",
       "        0.778, 0.766, 0.774, 0.763, 0.771, 0.77 , 0.771, 0.772, 0.774,\n",
       "        0.772, 0.768, 0.768, 0.767, 0.771, 0.777, 0.766, 0.774, 0.775,\n",
       "        0.765, 0.769, 0.761, 0.763, 0.763, 0.77 , 0.779, 0.769, 0.768,\n",
       "        0.767, 0.779, 0.772, 0.774, 0.774, 0.77 , 0.761, 0.775, 0.772,\n",
       "        0.772, 0.772, 0.765, 0.774, 0.768, 0.774, 0.768, 0.768, 0.778]),\n",
       " 'split1_test_recall_micro': array([0.793, 0.787, 0.776, 0.762, 0.783, 0.768, 0.782, 0.785, 0.772,\n",
       "        0.753, 0.766, 0.764, 0.782, 0.776, 0.776, 0.777, 0.78 , 0.786,\n",
       "        0.759, 0.772, 0.772, 0.769, 0.764, 0.749, 0.783, 0.803, 0.789,\n",
       "        0.773, 0.777, 0.786, 0.767, 0.778, 0.758, 0.744, 0.753, 0.759,\n",
       "        0.788, 0.791, 0.781, 0.786, 0.773, 0.78 , 0.762, 0.777, 0.773,\n",
       "        0.776, 0.776, 0.766, 0.791, 0.789, 0.79 , 0.768, 0.782, 0.76 ,\n",
       "        0.747, 0.757, 0.755, 0.743, 0.75 , 0.76 , 0.79 , 0.795, 0.785,\n",
       "        0.788, 0.754, 0.757, 0.761, 0.756, 0.771, 0.759, 0.767, 0.757,\n",
       "        0.783, 0.782, 0.777, 0.774, 0.782, 0.778, 0.75 , 0.773, 0.771,\n",
       "        0.75 , 0.761, 0.747, 0.799, 0.794, 0.79 , 0.769, 0.769, 0.767,\n",
       "        0.775, 0.768, 0.771, 0.745, 0.745, 0.757, 0.782, 0.465, 0.786,\n",
       "        0.785, 0.511, 0.789, 0.785, 0.491, 0.785, 0.783, 0.492, 0.791,\n",
       "        0.791, 0.489, 0.783, 0.791, 0.579, 0.791, 0.787, 0.476, 0.786,\n",
       "        0.787, 0.514, 0.796, 0.793, 0.498, 0.79 , 0.781, 0.511, 0.792,\n",
       "        0.788, 0.675, 0.79 , 0.793, 0.369, 0.786, 0.787, 0.489, 0.795,\n",
       "        0.79 , 0.649, 0.789, 0.8  , 0.452, 0.79 , 0.792, 0.308, 0.793,\n",
       "        0.793, 0.693, 0.79 , 0.787, 0.552, 0.793, 0.792, 0.685, 0.787,\n",
       "        0.787, 0.576, 0.787, 0.782, 0.52 , 0.787, 0.786, 0.619, 0.791,\n",
       "        0.793, 0.67 , 0.789, 0.792, 0.718, 0.793, 0.787, 0.488, 0.785,\n",
       "        0.788, 0.605, 0.779, 0.796, 0.731, 0.783, 0.793, 0.604, 0.788,\n",
       "        0.79 , 0.574, 0.789, 0.791, 0.542, 0.787, 0.794, 0.749, 0.785,\n",
       "        0.788, 0.732, 0.786, 0.785, 0.789, 0.787, 0.786, 0.786, 0.787,\n",
       "        0.788, 0.787, 0.789, 0.787, 0.786, 0.786, 0.787, 0.791, 0.786,\n",
       "        0.788, 0.788, 0.787, 0.789, 0.788, 0.787, 0.787, 0.786, 0.789,\n",
       "        0.788, 0.792, 0.8  , 0.79 , 0.787, 0.786, 0.786, 0.79 , 0.789,\n",
       "        0.787, 0.786, 0.786, 0.788, 0.789, 0.797, 0.792, 0.791, 0.787,\n",
       "        0.787, 0.788, 0.784, 0.787, 0.786, 0.787, 0.787, 0.79 , 0.784,\n",
       "        0.785, 0.783, 0.781, 0.789, 0.786, 0.789, 0.792, 0.787, 0.794,\n",
       "        0.788, 0.78 , 0.792, 0.786, 0.782, 0.788, 0.771, 0.786, 0.795,\n",
       "        0.787, 0.787, 0.785, 0.801, 0.786, 0.786, 0.792, 0.78 , 0.786,\n",
       "        0.787, 0.78 , 0.782, 0.787, 0.773, 0.795, 0.788, 0.795, 0.784,\n",
       "        0.79 , 0.779, 0.784, 0.786, 0.789, 0.782, 0.78 , 0.795, 0.786]),\n",
       " 'split2_test_recall_micro': array([0.783, 0.764, 0.769, 0.769, 0.758, 0.758, 0.763, 0.751, 0.76 ,\n",
       "        0.738, 0.737, 0.76 , 0.775, 0.775, 0.762, 0.767, 0.767, 0.753,\n",
       "        0.756, 0.772, 0.765, 0.739, 0.74 , 0.737, 0.758, 0.766, 0.767,\n",
       "        0.769, 0.757, 0.761, 0.759, 0.756, 0.74 , 0.737, 0.749, 0.735,\n",
       "        0.762, 0.768, 0.769, 0.775, 0.769, 0.774, 0.754, 0.766, 0.763,\n",
       "        0.752, 0.757, 0.747, 0.775, 0.762, 0.766, 0.757, 0.762, 0.771,\n",
       "        0.749, 0.74 , 0.744, 0.746, 0.739, 0.755, 0.772, 0.78 , 0.77 ,\n",
       "        0.758, 0.767, 0.74 , 0.764, 0.746, 0.754, 0.762, 0.739, 0.747,\n",
       "        0.781, 0.763, 0.78 , 0.735, 0.758, 0.772, 0.749, 0.756, 0.748,\n",
       "        0.757, 0.741, 0.734, 0.755, 0.769, 0.769, 0.76 , 0.751, 0.769,\n",
       "        0.747, 0.762, 0.745, 0.747, 0.749, 0.747, 0.773, 0.511, 0.771,\n",
       "        0.77 , 0.531, 0.772, 0.773, 0.506, 0.776, 0.771, 0.489, 0.773,\n",
       "        0.776, 0.535, 0.768, 0.776, 0.566, 0.771, 0.771, 0.511, 0.776,\n",
       "        0.77 , 0.565, 0.768, 0.771, 0.511, 0.776, 0.769, 0.594, 0.773,\n",
       "        0.768, 0.41 , 0.775, 0.768, 0.633, 0.769, 0.777, 0.491, 0.774,\n",
       "        0.774, 0.506, 0.769, 0.767, 0.511, 0.773, 0.775, 0.489, 0.78 ,\n",
       "        0.774, 0.561, 0.78 , 0.773, 0.725, 0.775, 0.772, 0.533, 0.777,\n",
       "        0.777, 0.628, 0.765, 0.772, 0.629, 0.767, 0.771, 0.596, 0.778,\n",
       "        0.773, 0.608, 0.775, 0.774, 0.739, 0.772, 0.778, 0.586, 0.775,\n",
       "        0.777, 0.532, 0.763, 0.769, 0.617, 0.772, 0.767, 0.661, 0.772,\n",
       "        0.774, 0.639, 0.772, 0.776, 0.649, 0.773, 0.781, 0.51 , 0.768,\n",
       "        0.773, 0.538, 0.769, 0.771, 0.768, 0.77 , 0.772, 0.772, 0.772,\n",
       "        0.77 , 0.774, 0.776, 0.772, 0.773, 0.774, 0.773, 0.768, 0.77 ,\n",
       "        0.77 , 0.769, 0.77 , 0.771, 0.774, 0.769, 0.769, 0.769, 0.771,\n",
       "        0.772, 0.772, 0.774, 0.772, 0.772, 0.771, 0.769, 0.769, 0.775,\n",
       "        0.769, 0.773, 0.769, 0.773, 0.774, 0.775, 0.773, 0.772, 0.774,\n",
       "        0.771, 0.771, 0.771, 0.772, 0.77 , 0.772, 0.77 , 0.771, 0.769,\n",
       "        0.766, 0.777, 0.773, 0.771, 0.769, 0.771, 0.77 , 0.775, 0.768,\n",
       "        0.766, 0.763, 0.767, 0.778, 0.765, 0.774, 0.764, 0.772, 0.765,\n",
       "        0.77 , 0.769, 0.775, 0.779, 0.766, 0.764, 0.765, 0.765, 0.768,\n",
       "        0.768, 0.768, 0.768, 0.763, 0.766, 0.759, 0.771, 0.774, 0.775,\n",
       "        0.765, 0.776, 0.769, 0.765, 0.774, 0.768, 0.774, 0.765, 0.778]),\n",
       " 'split3_test_recall_micro': array([0.779, 0.772, 0.779, 0.766, 0.764, 0.753, 0.743, 0.766, 0.747,\n",
       "        0.731, 0.747, 0.726, 0.783, 0.775, 0.781, 0.764, 0.767, 0.761,\n",
       "        0.733, 0.75 , 0.76 , 0.739, 0.741, 0.767, 0.781, 0.777, 0.776,\n",
       "        0.735, 0.768, 0.776, 0.757, 0.75 , 0.729, 0.723, 0.757, 0.747,\n",
       "        0.777, 0.78 , 0.773, 0.751, 0.774, 0.764, 0.756, 0.747, 0.761,\n",
       "        0.738, 0.75 , 0.759, 0.783, 0.764, 0.787, 0.752, 0.745, 0.741,\n",
       "        0.727, 0.752, 0.756, 0.715, 0.747, 0.749, 0.782, 0.758, 0.775,\n",
       "        0.754, 0.786, 0.772, 0.754, 0.744, 0.757, 0.718, 0.741, 0.744,\n",
       "        0.761, 0.77 , 0.782, 0.764, 0.78 , 0.763, 0.748, 0.764, 0.737,\n",
       "        0.73 , 0.72 , 0.724, 0.786, 0.768, 0.769, 0.768, 0.762, 0.771,\n",
       "        0.736, 0.762, 0.734, 0.735, 0.74 , 0.726, 0.789, 0.572, 0.784,\n",
       "        0.784, 0.489, 0.786, 0.79 , 0.593, 0.786, 0.788, 0.547, 0.783,\n",
       "        0.778, 0.489, 0.782, 0.785, 0.533, 0.781, 0.786, 0.511, 0.786,\n",
       "        0.785, 0.511, 0.788, 0.782, 0.489, 0.79 , 0.783, 0.503, 0.787,\n",
       "        0.784, 0.511, 0.779, 0.785, 0.514, 0.784, 0.786, 0.489, 0.773,\n",
       "        0.791, 0.491, 0.785, 0.789, 0.63 , 0.788, 0.78 , 0.511, 0.78 ,\n",
       "        0.784, 0.622, 0.781, 0.77 , 0.552, 0.776, 0.777, 0.637, 0.779,\n",
       "        0.78 , 0.63 , 0.789, 0.778, 0.5  , 0.783, 0.783, 0.636, 0.779,\n",
       "        0.782, 0.696, 0.779, 0.784, 0.648, 0.777, 0.782, 0.638, 0.782,\n",
       "        0.784, 0.552, 0.783, 0.785, 0.714, 0.78 , 0.784, 0.61 , 0.786,\n",
       "        0.782, 0.547, 0.783, 0.78 , 0.586, 0.782, 0.79 , 0.622, 0.782,\n",
       "        0.779, 0.569, 0.79 , 0.779, 0.781, 0.779, 0.78 , 0.781, 0.781,\n",
       "        0.781, 0.781, 0.781, 0.782, 0.779, 0.78 , 0.779, 0.782, 0.781,\n",
       "        0.781, 0.778, 0.781, 0.781, 0.781, 0.779, 0.782, 0.781, 0.778,\n",
       "        0.782, 0.781, 0.783, 0.781, 0.782, 0.781, 0.777, 0.781, 0.78 ,\n",
       "        0.781, 0.781, 0.781, 0.779, 0.781, 0.781, 0.78 , 0.779, 0.779,\n",
       "        0.779, 0.78 , 0.779, 0.78 , 0.78 , 0.782, 0.782, 0.776, 0.789,\n",
       "        0.781, 0.787, 0.781, 0.786, 0.778, 0.785, 0.784, 0.778, 0.779,\n",
       "        0.782, 0.784, 0.783, 0.78 , 0.782, 0.778, 0.78 , 0.786, 0.777,\n",
       "        0.786, 0.778, 0.769, 0.784, 0.783, 0.783, 0.785, 0.787, 0.78 ,\n",
       "        0.778, 0.781, 0.776, 0.781, 0.78 , 0.768, 0.781, 0.783, 0.784,\n",
       "        0.782, 0.784, 0.784, 0.778, 0.782, 0.782, 0.78 , 0.778, 0.787]),\n",
       " 'split4_test_recall_micro': array([0.77 , 0.774, 0.773, 0.763, 0.754, 0.746, 0.757, 0.731, 0.75 ,\n",
       "        0.756, 0.754, 0.736, 0.782, 0.777, 0.768, 0.766, 0.767, 0.76 ,\n",
       "        0.754, 0.751, 0.748, 0.748, 0.74 , 0.741, 0.772, 0.774, 0.782,\n",
       "        0.771, 0.775, 0.771, 0.744, 0.754, 0.76 , 0.747, 0.753, 0.743,\n",
       "        0.774, 0.768, 0.776, 0.75 , 0.761, 0.762, 0.773, 0.756, 0.754,\n",
       "        0.749, 0.749, 0.739, 0.775, 0.776, 0.776, 0.745, 0.757, 0.766,\n",
       "        0.751, 0.761, 0.757, 0.744, 0.749, 0.73 , 0.78 , 0.778, 0.777,\n",
       "        0.771, 0.772, 0.748, 0.745, 0.737, 0.749, 0.752, 0.746, 0.75 ,\n",
       "        0.771, 0.773, 0.778, 0.769, 0.761, 0.763, 0.764, 0.756, 0.757,\n",
       "        0.733, 0.746, 0.738, 0.772, 0.78 , 0.773, 0.763, 0.776, 0.771,\n",
       "        0.746, 0.748, 0.758, 0.728, 0.765, 0.755, 0.79 , 0.511, 0.792,\n",
       "        0.783, 0.511, 0.786, 0.792, 0.511, 0.785, 0.789, 0.489, 0.788,\n",
       "        0.782, 0.56 , 0.787, 0.788, 0.27 , 0.79 , 0.788, 0.72 , 0.786,\n",
       "        0.79 , 0.519, 0.79 , 0.783, 0.488, 0.788, 0.789, 0.489, 0.798,\n",
       "        0.789, 0.579, 0.787, 0.789, 0.511, 0.789, 0.783, 0.489, 0.791,\n",
       "        0.79 , 0.515, 0.789, 0.792, 0.342, 0.79 , 0.787, 0.36 , 0.791,\n",
       "        0.783, 0.543, 0.781, 0.786, 0.502, 0.79 , 0.78 , 0.68 , 0.781,\n",
       "        0.794, 0.589, 0.778, 0.785, 0.558, 0.784, 0.785, 0.497, 0.78 ,\n",
       "        0.785, 0.491, 0.788, 0.789, 0.651, 0.784, 0.784, 0.618, 0.791,\n",
       "        0.79 , 0.533, 0.789, 0.792, 0.606, 0.79 , 0.785, 0.738, 0.791,\n",
       "        0.788, 0.427, 0.784, 0.79 , 0.69 , 0.788, 0.789, 0.619, 0.786,\n",
       "        0.784, 0.639, 0.793, 0.784, 0.787, 0.785, 0.786, 0.79 , 0.786,\n",
       "        0.788, 0.787, 0.788, 0.786, 0.786, 0.786, 0.79 , 0.785, 0.786,\n",
       "        0.785, 0.787, 0.787, 0.785, 0.786, 0.785, 0.787, 0.788, 0.785,\n",
       "        0.789, 0.783, 0.786, 0.789, 0.783, 0.786, 0.787, 0.784, 0.787,\n",
       "        0.785, 0.79 , 0.787, 0.784, 0.784, 0.783, 0.791, 0.787, 0.789,\n",
       "        0.786, 0.789, 0.786, 0.789, 0.787, 0.786, 0.779, 0.779, 0.785,\n",
       "        0.777, 0.775, 0.779, 0.79 , 0.779, 0.783, 0.767, 0.776, 0.784,\n",
       "        0.785, 0.786, 0.782, 0.781, 0.787, 0.791, 0.784, 0.776, 0.786,\n",
       "        0.784, 0.771, 0.775, 0.787, 0.785, 0.79 , 0.781, 0.784, 0.79 ,\n",
       "        0.786, 0.784, 0.79 , 0.779, 0.78 , 0.784, 0.789, 0.781, 0.778,\n",
       "        0.783, 0.783, 0.795, 0.788, 0.785, 0.781, 0.783, 0.783, 0.788]),\n",
       " 'mean_test_recall_micro': array([0.7766, 0.7718, 0.7744, 0.7608, 0.7612, 0.754 , 0.758 , 0.7546,\n",
       "        0.7538, 0.7434, 0.7472, 0.7422, 0.7794, 0.777 , 0.7732, 0.764 ,\n",
       "        0.7658, 0.7606, 0.749 , 0.756 , 0.7572, 0.7464, 0.745 , 0.7442,\n",
       "        0.7714, 0.775 , 0.7762, 0.7608, 0.765 , 0.769 , 0.7544, 0.754 ,\n",
       "        0.7434, 0.7416, 0.749 , 0.7446, 0.7736, 0.7734, 0.7748, 0.762 ,\n",
       "        0.7646, 0.7682, 0.7572, 0.7588, 0.7602, 0.752 , 0.756 , 0.7464,\n",
       "        0.7766, 0.7706, 0.777 , 0.752 , 0.7564, 0.7536, 0.7454, 0.7498,\n",
       "        0.7478, 0.7394, 0.7454, 0.747 , 0.78  , 0.7732, 0.7744, 0.7622,\n",
       "        0.7666, 0.7534, 0.7538, 0.743 , 0.7516, 0.7426, 0.743 , 0.7444,\n",
       "        0.7718, 0.7698, 0.7752, 0.7566, 0.7644, 0.768 , 0.7508, 0.755 ,\n",
       "        0.7544, 0.7392, 0.7402, 0.7352, 0.7746, 0.7752, 0.7728, 0.7628,\n",
       "        0.7642, 0.7656, 0.7494, 0.7578, 0.7486, 0.7408, 0.7488, 0.7474,\n",
       "        0.7808, 0.5142, 0.7792, 0.7786, 0.5062, 0.7806, 0.782 , 0.5442,\n",
       "        0.7796, 0.7794, 0.5056, 0.7806, 0.7798, 0.5168, 0.777 , 0.7818,\n",
       "        0.4922, 0.7802, 0.7792, 0.5462, 0.78  , 0.7798, 0.5284, 0.782 ,\n",
       "        0.7802, 0.497 , 0.782 , 0.7774, 0.5172, 0.7836, 0.7786, 0.5328,\n",
       "        0.78  , 0.7802, 0.5076, 0.7782, 0.78  , 0.4892, 0.779 , 0.7824,\n",
       "        0.5276, 0.78  , 0.783 , 0.4892, 0.7818, 0.7798, 0.4448, 0.7814,\n",
       "        0.7798, 0.5854, 0.78  , 0.7772, 0.5798, 0.7812, 0.7786, 0.6292,\n",
       "        0.7778, 0.781 , 0.592 , 0.7786, 0.7752, 0.5784, 0.7786, 0.7778,\n",
       "        0.5684, 0.7802, 0.78  , 0.6048, 0.7802, 0.7812, 0.6606, 0.7802,\n",
       "        0.7792, 0.5616, 0.7796, 0.7808, 0.5504, 0.7764, 0.7824, 0.6578,\n",
       "        0.7772, 0.779 , 0.643 , 0.7808, 0.781 , 0.5146, 0.7798, 0.7806,\n",
       "        0.6306, 0.7804, 0.7848, 0.644 , 0.777 , 0.7772, 0.6408, 0.7806,\n",
       "        0.7762, 0.7772, 0.7776, 0.7776, 0.7776, 0.7786, 0.7784, 0.7784,\n",
       "        0.7802, 0.7784, 0.7768, 0.7782, 0.7796, 0.7774, 0.7772, 0.7774,\n",
       "        0.7768, 0.7776, 0.7784, 0.778 , 0.7764, 0.7772, 0.7772, 0.7764,\n",
       "        0.7788, 0.7786, 0.7814, 0.7788, 0.7774, 0.7776, 0.776 , 0.7776,\n",
       "        0.779 , 0.7766, 0.7784, 0.7772, 0.7772, 0.7788, 0.7808, 0.7794,\n",
       "        0.7788, 0.779 , 0.777 , 0.7778, 0.777 , 0.7784, 0.7774, 0.7778,\n",
       "        0.777 , 0.7774, 0.7786, 0.7774, 0.7776, 0.7776, 0.7798, 0.7766,\n",
       "        0.7796, 0.7768, 0.7776, 0.7798, 0.7786, 0.7762, 0.7784, 0.7784,\n",
       "        0.7774, 0.7816, 0.773 , 0.7788, 0.7796, 0.7784, 0.7748, 0.773 ,\n",
       "        0.7828, 0.7766, 0.7786, 0.7804, 0.777 , 0.7784, 0.7772, 0.7784,\n",
       "        0.7776, 0.7768, 0.7746, 0.7752, 0.778 , 0.7816, 0.7786, 0.7784,\n",
       "        0.7788, 0.7794, 0.7782, 0.7796, 0.7774, 0.777 , 0.7778, 0.7834]),\n",
       " 'std_test_recall_micro': array([0.01187603, 0.00886341, 0.00332265, 0.00874986, 0.01222129,\n",
       "        0.00846168, 0.01411382, 0.01916872, 0.01114271, 0.0095205 ,\n",
       "        0.01212271, 0.01666613, 0.00361109, 0.00260768, 0.00713863,\n",
       "        0.01005982, 0.01022546, 0.01423517, 0.00965401, 0.01424079,\n",
       "        0.01126765, 0.01192644, 0.00950789, 0.01342237, 0.00976934,\n",
       "        0.01593738, 0.00856505, 0.01420422, 0.01100909, 0.01208305,\n",
       "        0.00875443, 0.01469694, 0.01332066, 0.01130664, 0.00839047,\n",
       "        0.0082365 , 0.00891291, 0.01087382, 0.00391918, 0.01553061,\n",
       "        0.01036533, 0.007494  , 0.01045753, 0.01137365, 0.00793473,\n",
       "        0.0128841 , 0.01048809, 0.01576832, 0.0106132 , 0.01057544,\n",
       "        0.01011929, 0.01025671, 0.01570478, 0.01557691, 0.00941488,\n",
       "        0.00888594, 0.01140877, 0.01237093, 0.00422374, 0.01060189,\n",
       "        0.0060663 , 0.01487817, 0.00674092, 0.016253  , 0.01202664,\n",
       "        0.0107629 , 0.00788416, 0.00819756, 0.01430524, 0.01877871,\n",
       "        0.01446375, 0.01107429, 0.00899778, 0.00752064, 0.0082801 ,\n",
       "        0.01562818, 0.01518684, 0.00603324, 0.00702567, 0.01579873,\n",
       "        0.01137717, 0.01208967, 0.0136147 , 0.00746726, 0.01613196,\n",
       "        0.01068457, 0.00917388, 0.00549181, 0.0082801 , 0.00793977,\n",
       "        0.01336563, 0.00790949, 0.01415062, 0.00800999, 0.00858836,\n",
       "        0.01121784, 0.00813388, 0.03401999, 0.01060943, 0.00665132,\n",
       "        0.01582909, 0.00793977, 0.00892188, 0.05199769, 0.00770973,\n",
       "        0.00926499, 0.02226747, 0.00877724, 0.0064622 , 0.02748381,\n",
       "        0.00878635, 0.00813388, 0.11353484, 0.00945304, 0.00982649,\n",
       "        0.08799409, 0.008     , 0.00941063, 0.01979495, 0.0117303 ,\n",
       "        0.00808455, 0.00831865, 0.00954987, 0.00897998, 0.03931615,\n",
       "        0.01135958, 0.01049952, 0.08925111, 0.00769415, 0.01108873,\n",
       "        0.08366505, 0.0102645 , 0.00737564, 0.0009798 , 0.01224745,\n",
       "        0.00997196, 0.06206964, 0.00950789, 0.01354991, 0.0936235 ,\n",
       "        0.00938936, 0.00941063, 0.09445295, 0.01066958, 0.0095373 ,\n",
       "        0.06527051, 0.00701427, 0.00767854, 0.07592207, 0.00856505,\n",
       "        0.00736478, 0.05539097, 0.00722219, 0.0091433 , 0.03472751,\n",
       "        0.00877724, 0.00919565, 0.06913349, 0.00770973, 0.00874986,\n",
       "        0.06086904, 0.0059127 , 0.0091214 , 0.07433277, 0.00735935,\n",
       "        0.00936803, 0.06722678, 0.00752064, 0.00767854, 0.06636143,\n",
       "        0.00893532, 0.00906422, 0.02842956, 0.00958332, 0.01111036,\n",
       "        0.05332692, 0.00994786, 0.01067708, 0.05223026, 0.00949526,\n",
       "        0.00748331, 0.09409059, 0.00708237, 0.00928655, 0.05792271,\n",
       "        0.00677052, 0.00851822, 0.08469475, 0.00916515, 0.00910824,\n",
       "        0.0791237 , 0.01139474, 0.00865794, 0.0109252 , 0.00793977,\n",
       "        0.00852291, 0.01107429, 0.00786384, 0.00939361, 0.00906863,\n",
       "        0.00813388, 0.00854634, 0.00970361, 0.0079599 , 0.00798999,\n",
       "        0.0111463 , 0.00919565, 0.0094361 , 0.01010742, 0.00958332,\n",
       "        0.00861626, 0.00977753, 0.00954149, 0.01043839, 0.01006777,\n",
       "        0.01065082, 0.00994786, 0.00930806, 0.01205985, 0.01060943,\n",
       "        0.00873155, 0.00873155, 0.00995992, 0.00964572, 0.0090111 ,\n",
       "        0.009992  , 0.00997196, 0.00955824, 0.00910824, 0.00803492,\n",
       "        0.00964158, 0.01160345, 0.0095163 , 0.00846168, 0.00944458,\n",
       "        0.01060943, 0.00792465, 0.00935094, 0.00902441, 0.0095163 ,\n",
       "        0.00745654, 0.00700286, 0.00926499, 0.0063435 , 0.0072    ,\n",
       "        0.00344093, 0.01083328, 0.00608605, 0.00768375, 0.00957914,\n",
       "        0.00508331, 0.00886341, 0.00828493, 0.00908625, 0.00956243,\n",
       "        0.00628013, 0.00811419, 0.00665132, 0.00779744, 0.00601332,\n",
       "        0.01019019, 0.00909065, 0.00693974, 0.00789937, 0.01230285,\n",
       "        0.00997196, 0.00991161, 0.00889044, 0.0085557 , 0.00906863,\n",
       "        0.00851822, 0.0054626 , 0.00773563, 0.00805978, 0.0052    ,\n",
       "        0.01273421, 0.01065833, 0.00752596, 0.0048    , 0.0088227 ,\n",
       "        0.00444522, 0.0109654 , 0.00835225, 0.00760526, 0.00557136,\n",
       "        0.00536656, 0.0107963 , 0.00445421]),\n",
       " 'rank_test_recall_micro': array([151, 180, 171, 200, 199, 217, 205, 214, 219, 245, 235, 250,  59,\n",
       "        137, 175, 195, 189, 202, 229, 211, 207, 237, 241, 244, 182, 166,\n",
       "        158, 200, 191, 185, 215, 217, 245, 251, 229, 242, 173, 174, 167,\n",
       "        198, 192, 186, 208, 204, 203, 223, 211, 238, 151, 183, 140, 223,\n",
       "        210, 221, 239, 227, 233, 254, 239, 236,  38, 176, 171, 197, 188,\n",
       "        222, 219, 247, 225, 249, 247, 243, 180, 184, 163, 209, 193, 187,\n",
       "        226, 213, 215, 255, 253, 256, 169, 162, 179, 196, 194, 190, 228,\n",
       "        206, 232, 252, 231, 234,  21, 280,  62,  76, 282,  25,   8, 273,\n",
       "         52,  59, 283,  25,  45, 278, 140,  11, 285,  31,  62, 272,  38,\n",
       "         45, 275,   8,  31, 284,   8, 122, 277,   2,  76, 274,  38,  31,\n",
       "        281,  98,  38, 287,  66,   6, 276,  38,   4, 286,  12,  45, 288,\n",
       "         15,  51, 266,  38, 127, 267,  18,  75, 263, 103,  20, 265,  76,\n",
       "        163, 268,  84, 103, 269,  31,  38, 264,  31,  17, 257,  31,  62,\n",
       "        270,  52,  22, 271, 155,   6, 258, 127,  65, 260,  22,  19, 279,\n",
       "         45,  25, 262,  30,   1, 259, 137, 127, 261,  25, 158, 127, 113,\n",
       "        108, 108,  76,  87,  93,  31,  93, 146,  98,  52, 118, 127, 118,\n",
       "        146, 108,  87, 101, 155, 127, 127, 155,  69,  84,  16,  70, 122,\n",
       "        108, 161, 113,  68, 150,  87, 127, 127,  70,  22,  59,  70,  66,\n",
       "        140, 106, 140,  86, 118, 106, 140, 126,  76, 118, 113, 108,  45,\n",
       "        151,  52, 146, 113,  45,  76, 158,  93,  93, 122,  14, 177,  70,\n",
       "         52,  87, 168, 177,   5, 151,  76,  29, 140,  87, 127,  87, 113,\n",
       "        146, 169, 165, 101,  13,  76,  93,  74,  58, 100,  52, 122, 137,\n",
       "        103,   3]),\n",
       " 'split0_test_f1_micro': array([0.758, 0.762, 0.775, 0.744, 0.747, 0.745, 0.745, 0.74 , 0.74 ,\n",
       "        0.739, 0.732, 0.725, 0.775, 0.782, 0.779, 0.746, 0.748, 0.743,\n",
       "        0.743, 0.735, 0.741, 0.737, 0.74 , 0.727, 0.763, 0.755, 0.767,\n",
       "        0.756, 0.748, 0.751, 0.745, 0.732, 0.73 , 0.757, 0.733, 0.739,\n",
       "        0.767, 0.76 , 0.775, 0.748, 0.746, 0.761, 0.741, 0.748, 0.75 ,\n",
       "        0.745, 0.748, 0.721, 0.759, 0.762, 0.766, 0.738, 0.736, 0.73 ,\n",
       "        0.753, 0.739, 0.727, 0.749, 0.742, 0.741, 0.776, 0.755, 0.765,\n",
       "        0.74 , 0.754, 0.75 , 0.745, 0.732, 0.727, 0.722, 0.722, 0.724,\n",
       "        0.763, 0.761, 0.759, 0.741, 0.741, 0.764, 0.743, 0.726, 0.759,\n",
       "        0.726, 0.733, 0.733, 0.761, 0.765, 0.763, 0.754, 0.763, 0.75 ,\n",
       "        0.743, 0.749, 0.735, 0.749, 0.745, 0.752, 0.77 , 0.512, 0.763,\n",
       "        0.771, 0.489, 0.77 , 0.77 , 0.62 , 0.766, 0.766, 0.511, 0.768,\n",
       "        0.772, 0.511, 0.765, 0.769, 0.513, 0.768, 0.764, 0.513, 0.766,\n",
       "        0.767, 0.533, 0.768, 0.772, 0.499, 0.766, 0.765, 0.489, 0.768,\n",
       "        0.764, 0.489, 0.769, 0.766, 0.511, 0.763, 0.767, 0.488, 0.762,\n",
       "        0.767, 0.477, 0.768, 0.767, 0.511, 0.768, 0.765, 0.556, 0.763,\n",
       "        0.765, 0.508, 0.768, 0.77 , 0.568, 0.772, 0.772, 0.611, 0.765,\n",
       "        0.767, 0.537, 0.774, 0.759, 0.685, 0.772, 0.764, 0.494, 0.773,\n",
       "        0.767, 0.559, 0.77 , 0.767, 0.547, 0.775, 0.765, 0.478, 0.765,\n",
       "        0.765, 0.53 , 0.768, 0.77 , 0.621, 0.761, 0.766, 0.602, 0.767,\n",
       "        0.771, 0.386, 0.771, 0.766, 0.686, 0.772, 0.77 , 0.72 , 0.764,\n",
       "        0.762, 0.726, 0.765, 0.762, 0.761, 0.767, 0.764, 0.759, 0.767,\n",
       "        0.765, 0.763, 0.767, 0.765, 0.76 , 0.765, 0.769, 0.761, 0.763,\n",
       "        0.763, 0.762, 0.763, 0.766, 0.761, 0.762, 0.761, 0.762, 0.759,\n",
       "        0.763, 0.765, 0.764, 0.762, 0.763, 0.764, 0.761, 0.764, 0.764,\n",
       "        0.761, 0.762, 0.763, 0.762, 0.766, 0.768, 0.761, 0.765, 0.766,\n",
       "        0.762, 0.761, 0.765, 0.764, 0.764, 0.762, 0.767, 0.771, 0.766,\n",
       "        0.778, 0.766, 0.774, 0.763, 0.771, 0.77 , 0.771, 0.772, 0.774,\n",
       "        0.772, 0.768, 0.768, 0.767, 0.771, 0.777, 0.766, 0.774, 0.775,\n",
       "        0.765, 0.769, 0.761, 0.763, 0.763, 0.77 , 0.779, 0.769, 0.768,\n",
       "        0.767, 0.779, 0.772, 0.774, 0.774, 0.77 , 0.761, 0.775, 0.772,\n",
       "        0.772, 0.772, 0.765, 0.774, 0.768, 0.774, 0.768, 0.768, 0.778]),\n",
       " 'split1_test_f1_micro': array([0.793, 0.787, 0.776, 0.762, 0.783, 0.768, 0.782, 0.785, 0.772,\n",
       "        0.753, 0.766, 0.764, 0.782, 0.776, 0.776, 0.777, 0.78 , 0.786,\n",
       "        0.759, 0.772, 0.772, 0.769, 0.764, 0.749, 0.783, 0.803, 0.789,\n",
       "        0.773, 0.777, 0.786, 0.767, 0.778, 0.758, 0.744, 0.753, 0.759,\n",
       "        0.788, 0.791, 0.781, 0.786, 0.773, 0.78 , 0.762, 0.777, 0.773,\n",
       "        0.776, 0.776, 0.766, 0.791, 0.789, 0.79 , 0.768, 0.782, 0.76 ,\n",
       "        0.747, 0.757, 0.755, 0.743, 0.75 , 0.76 , 0.79 , 0.795, 0.785,\n",
       "        0.788, 0.754, 0.757, 0.761, 0.756, 0.771, 0.759, 0.767, 0.757,\n",
       "        0.783, 0.782, 0.777, 0.774, 0.782, 0.778, 0.75 , 0.773, 0.771,\n",
       "        0.75 , 0.761, 0.747, 0.799, 0.794, 0.79 , 0.769, 0.769, 0.767,\n",
       "        0.775, 0.768, 0.771, 0.745, 0.745, 0.757, 0.782, 0.465, 0.786,\n",
       "        0.785, 0.511, 0.789, 0.785, 0.491, 0.785, 0.783, 0.492, 0.791,\n",
       "        0.791, 0.489, 0.783, 0.791, 0.579, 0.791, 0.787, 0.476, 0.786,\n",
       "        0.787, 0.514, 0.796, 0.793, 0.498, 0.79 , 0.781, 0.511, 0.792,\n",
       "        0.788, 0.675, 0.79 , 0.793, 0.369, 0.786, 0.787, 0.489, 0.795,\n",
       "        0.79 , 0.649, 0.789, 0.8  , 0.452, 0.79 , 0.792, 0.308, 0.793,\n",
       "        0.793, 0.693, 0.79 , 0.787, 0.552, 0.793, 0.792, 0.685, 0.787,\n",
       "        0.787, 0.576, 0.787, 0.782, 0.52 , 0.787, 0.786, 0.619, 0.791,\n",
       "        0.793, 0.67 , 0.789, 0.792, 0.718, 0.793, 0.787, 0.488, 0.785,\n",
       "        0.788, 0.605, 0.779, 0.796, 0.731, 0.783, 0.793, 0.604, 0.788,\n",
       "        0.79 , 0.574, 0.789, 0.791, 0.542, 0.787, 0.794, 0.749, 0.785,\n",
       "        0.788, 0.732, 0.786, 0.785, 0.789, 0.787, 0.786, 0.786, 0.787,\n",
       "        0.788, 0.787, 0.789, 0.787, 0.786, 0.786, 0.787, 0.791, 0.786,\n",
       "        0.788, 0.788, 0.787, 0.789, 0.788, 0.787, 0.787, 0.786, 0.789,\n",
       "        0.788, 0.792, 0.8  , 0.79 , 0.787, 0.786, 0.786, 0.79 , 0.789,\n",
       "        0.787, 0.786, 0.786, 0.788, 0.789, 0.797, 0.792, 0.791, 0.787,\n",
       "        0.787, 0.788, 0.784, 0.787, 0.786, 0.787, 0.787, 0.79 , 0.784,\n",
       "        0.785, 0.783, 0.781, 0.789, 0.786, 0.789, 0.792, 0.787, 0.794,\n",
       "        0.788, 0.78 , 0.792, 0.786, 0.782, 0.788, 0.771, 0.786, 0.795,\n",
       "        0.787, 0.787, 0.785, 0.801, 0.786, 0.786, 0.792, 0.78 , 0.786,\n",
       "        0.787, 0.78 , 0.782, 0.787, 0.773, 0.795, 0.788, 0.795, 0.784,\n",
       "        0.79 , 0.779, 0.784, 0.786, 0.789, 0.782, 0.78 , 0.795, 0.786]),\n",
       " 'split2_test_f1_micro': array([0.783, 0.764, 0.769, 0.769, 0.758, 0.758, 0.763, 0.751, 0.76 ,\n",
       "        0.738, 0.737, 0.76 , 0.775, 0.775, 0.762, 0.767, 0.767, 0.753,\n",
       "        0.756, 0.772, 0.765, 0.739, 0.74 , 0.737, 0.758, 0.766, 0.767,\n",
       "        0.769, 0.757, 0.761, 0.759, 0.756, 0.74 , 0.737, 0.749, 0.735,\n",
       "        0.762, 0.768, 0.769, 0.775, 0.769, 0.774, 0.754, 0.766, 0.763,\n",
       "        0.752, 0.757, 0.747, 0.775, 0.762, 0.766, 0.757, 0.762, 0.771,\n",
       "        0.749, 0.74 , 0.744, 0.746, 0.739, 0.755, 0.772, 0.78 , 0.77 ,\n",
       "        0.758, 0.767, 0.74 , 0.764, 0.746, 0.754, 0.762, 0.739, 0.747,\n",
       "        0.781, 0.763, 0.78 , 0.735, 0.758, 0.772, 0.749, 0.756, 0.748,\n",
       "        0.757, 0.741, 0.734, 0.755, 0.769, 0.769, 0.76 , 0.751, 0.769,\n",
       "        0.747, 0.762, 0.745, 0.747, 0.749, 0.747, 0.773, 0.511, 0.771,\n",
       "        0.77 , 0.531, 0.772, 0.773, 0.506, 0.776, 0.771, 0.489, 0.773,\n",
       "        0.776, 0.535, 0.768, 0.776, 0.566, 0.771, 0.771, 0.511, 0.776,\n",
       "        0.77 , 0.565, 0.768, 0.771, 0.511, 0.776, 0.769, 0.594, 0.773,\n",
       "        0.768, 0.41 , 0.775, 0.768, 0.633, 0.769, 0.777, 0.491, 0.774,\n",
       "        0.774, 0.506, 0.769, 0.767, 0.511, 0.773, 0.775, 0.489, 0.78 ,\n",
       "        0.774, 0.561, 0.78 , 0.773, 0.725, 0.775, 0.772, 0.533, 0.777,\n",
       "        0.777, 0.628, 0.765, 0.772, 0.629, 0.767, 0.771, 0.596, 0.778,\n",
       "        0.773, 0.608, 0.775, 0.774, 0.739, 0.772, 0.778, 0.586, 0.775,\n",
       "        0.777, 0.532, 0.763, 0.769, 0.617, 0.772, 0.767, 0.661, 0.772,\n",
       "        0.774, 0.639, 0.772, 0.776, 0.649, 0.773, 0.781, 0.51 , 0.768,\n",
       "        0.773, 0.538, 0.769, 0.771, 0.768, 0.77 , 0.772, 0.772, 0.772,\n",
       "        0.77 , 0.774, 0.776, 0.772, 0.773, 0.774, 0.773, 0.768, 0.77 ,\n",
       "        0.77 , 0.769, 0.77 , 0.771, 0.774, 0.769, 0.769, 0.769, 0.771,\n",
       "        0.772, 0.772, 0.774, 0.772, 0.772, 0.771, 0.769, 0.769, 0.775,\n",
       "        0.769, 0.773, 0.769, 0.773, 0.774, 0.775, 0.773, 0.772, 0.774,\n",
       "        0.771, 0.771, 0.771, 0.772, 0.77 , 0.772, 0.77 , 0.771, 0.769,\n",
       "        0.766, 0.777, 0.773, 0.771, 0.769, 0.771, 0.77 , 0.775, 0.768,\n",
       "        0.766, 0.763, 0.767, 0.778, 0.765, 0.774, 0.764, 0.772, 0.765,\n",
       "        0.77 , 0.769, 0.775, 0.779, 0.766, 0.764, 0.765, 0.765, 0.768,\n",
       "        0.768, 0.768, 0.768, 0.763, 0.766, 0.759, 0.771, 0.774, 0.775,\n",
       "        0.765, 0.776, 0.769, 0.765, 0.774, 0.768, 0.774, 0.765, 0.778]),\n",
       " 'split3_test_f1_micro': array([0.779, 0.772, 0.779, 0.766, 0.764, 0.753, 0.743, 0.766, 0.747,\n",
       "        0.731, 0.747, 0.726, 0.783, 0.775, 0.781, 0.764, 0.767, 0.761,\n",
       "        0.733, 0.75 , 0.76 , 0.739, 0.741, 0.767, 0.781, 0.777, 0.776,\n",
       "        0.735, 0.768, 0.776, 0.757, 0.75 , 0.729, 0.723, 0.757, 0.747,\n",
       "        0.777, 0.78 , 0.773, 0.751, 0.774, 0.764, 0.756, 0.747, 0.761,\n",
       "        0.738, 0.75 , 0.759, 0.783, 0.764, 0.787, 0.752, 0.745, 0.741,\n",
       "        0.727, 0.752, 0.756, 0.715, 0.747, 0.749, 0.782, 0.758, 0.775,\n",
       "        0.754, 0.786, 0.772, 0.754, 0.744, 0.757, 0.718, 0.741, 0.744,\n",
       "        0.761, 0.77 , 0.782, 0.764, 0.78 , 0.763, 0.748, 0.764, 0.737,\n",
       "        0.73 , 0.72 , 0.724, 0.786, 0.768, 0.769, 0.768, 0.762, 0.771,\n",
       "        0.736, 0.762, 0.734, 0.735, 0.74 , 0.726, 0.789, 0.572, 0.784,\n",
       "        0.784, 0.489, 0.786, 0.79 , 0.593, 0.786, 0.788, 0.547, 0.783,\n",
       "        0.778, 0.489, 0.782, 0.785, 0.533, 0.781, 0.786, 0.511, 0.786,\n",
       "        0.785, 0.511, 0.788, 0.782, 0.489, 0.79 , 0.783, 0.503, 0.787,\n",
       "        0.784, 0.511, 0.779, 0.785, 0.514, 0.784, 0.786, 0.489, 0.773,\n",
       "        0.791, 0.491, 0.785, 0.789, 0.63 , 0.788, 0.78 , 0.511, 0.78 ,\n",
       "        0.784, 0.622, 0.781, 0.77 , 0.552, 0.776, 0.777, 0.637, 0.779,\n",
       "        0.78 , 0.63 , 0.789, 0.778, 0.5  , 0.783, 0.783, 0.636, 0.779,\n",
       "        0.782, 0.696, 0.779, 0.784, 0.648, 0.777, 0.782, 0.638, 0.782,\n",
       "        0.784, 0.552, 0.783, 0.785, 0.714, 0.78 , 0.784, 0.61 , 0.786,\n",
       "        0.782, 0.547, 0.783, 0.78 , 0.586, 0.782, 0.79 , 0.622, 0.782,\n",
       "        0.779, 0.569, 0.79 , 0.779, 0.781, 0.779, 0.78 , 0.781, 0.781,\n",
       "        0.781, 0.781, 0.781, 0.782, 0.779, 0.78 , 0.779, 0.782, 0.781,\n",
       "        0.781, 0.778, 0.781, 0.781, 0.781, 0.779, 0.782, 0.781, 0.778,\n",
       "        0.782, 0.781, 0.783, 0.781, 0.782, 0.781, 0.777, 0.781, 0.78 ,\n",
       "        0.781, 0.781, 0.781, 0.779, 0.781, 0.781, 0.78 , 0.779, 0.779,\n",
       "        0.779, 0.78 , 0.779, 0.78 , 0.78 , 0.782, 0.782, 0.776, 0.789,\n",
       "        0.781, 0.787, 0.781, 0.786, 0.778, 0.785, 0.784, 0.778, 0.779,\n",
       "        0.782, 0.784, 0.783, 0.78 , 0.782, 0.778, 0.78 , 0.786, 0.777,\n",
       "        0.786, 0.778, 0.769, 0.784, 0.783, 0.783, 0.785, 0.787, 0.78 ,\n",
       "        0.778, 0.781, 0.776, 0.781, 0.78 , 0.768, 0.781, 0.783, 0.784,\n",
       "        0.782, 0.784, 0.784, 0.778, 0.782, 0.782, 0.78 , 0.778, 0.787]),\n",
       " 'split4_test_f1_micro': array([0.77 , 0.774, 0.773, 0.763, 0.754, 0.746, 0.757, 0.731, 0.75 ,\n",
       "        0.756, 0.754, 0.736, 0.782, 0.777, 0.768, 0.766, 0.767, 0.76 ,\n",
       "        0.754, 0.751, 0.748, 0.748, 0.74 , 0.741, 0.772, 0.774, 0.782,\n",
       "        0.771, 0.775, 0.771, 0.744, 0.754, 0.76 , 0.747, 0.753, 0.743,\n",
       "        0.774, 0.768, 0.776, 0.75 , 0.761, 0.762, 0.773, 0.756, 0.754,\n",
       "        0.749, 0.749, 0.739, 0.775, 0.776, 0.776, 0.745, 0.757, 0.766,\n",
       "        0.751, 0.761, 0.757, 0.744, 0.749, 0.73 , 0.78 , 0.778, 0.777,\n",
       "        0.771, 0.772, 0.748, 0.745, 0.737, 0.749, 0.752, 0.746, 0.75 ,\n",
       "        0.771, 0.773, 0.778, 0.769, 0.761, 0.763, 0.764, 0.756, 0.757,\n",
       "        0.733, 0.746, 0.738, 0.772, 0.78 , 0.773, 0.763, 0.776, 0.771,\n",
       "        0.746, 0.748, 0.758, 0.728, 0.765, 0.755, 0.79 , 0.511, 0.792,\n",
       "        0.783, 0.511, 0.786, 0.792, 0.511, 0.785, 0.789, 0.489, 0.788,\n",
       "        0.782, 0.56 , 0.787, 0.788, 0.27 , 0.79 , 0.788, 0.72 , 0.786,\n",
       "        0.79 , 0.519, 0.79 , 0.783, 0.488, 0.788, 0.789, 0.489, 0.798,\n",
       "        0.789, 0.579, 0.787, 0.789, 0.511, 0.789, 0.783, 0.489, 0.791,\n",
       "        0.79 , 0.515, 0.789, 0.792, 0.342, 0.79 , 0.787, 0.36 , 0.791,\n",
       "        0.783, 0.543, 0.781, 0.786, 0.502, 0.79 , 0.78 , 0.68 , 0.781,\n",
       "        0.794, 0.589, 0.778, 0.785, 0.558, 0.784, 0.785, 0.497, 0.78 ,\n",
       "        0.785, 0.491, 0.788, 0.789, 0.651, 0.784, 0.784, 0.618, 0.791,\n",
       "        0.79 , 0.533, 0.789, 0.792, 0.606, 0.79 , 0.785, 0.738, 0.791,\n",
       "        0.788, 0.427, 0.784, 0.79 , 0.69 , 0.788, 0.789, 0.619, 0.786,\n",
       "        0.784, 0.639, 0.793, 0.784, 0.787, 0.785, 0.786, 0.79 , 0.786,\n",
       "        0.788, 0.787, 0.788, 0.786, 0.786, 0.786, 0.79 , 0.785, 0.786,\n",
       "        0.785, 0.787, 0.787, 0.785, 0.786, 0.785, 0.787, 0.788, 0.785,\n",
       "        0.789, 0.783, 0.786, 0.789, 0.783, 0.786, 0.787, 0.784, 0.787,\n",
       "        0.785, 0.79 , 0.787, 0.784, 0.784, 0.783, 0.791, 0.787, 0.789,\n",
       "        0.786, 0.789, 0.786, 0.789, 0.787, 0.786, 0.779, 0.779, 0.785,\n",
       "        0.777, 0.775, 0.779, 0.79 , 0.779, 0.783, 0.767, 0.776, 0.784,\n",
       "        0.785, 0.786, 0.782, 0.781, 0.787, 0.791, 0.784, 0.776, 0.786,\n",
       "        0.784, 0.771, 0.775, 0.787, 0.785, 0.79 , 0.781, 0.784, 0.79 ,\n",
       "        0.786, 0.784, 0.79 , 0.779, 0.78 , 0.784, 0.789, 0.781, 0.778,\n",
       "        0.783, 0.783, 0.795, 0.788, 0.785, 0.781, 0.783, 0.783, 0.788]),\n",
       " 'mean_test_f1_micro': array([0.7766, 0.7718, 0.7744, 0.7608, 0.7612, 0.754 , 0.758 , 0.7546,\n",
       "        0.7538, 0.7434, 0.7472, 0.7422, 0.7794, 0.777 , 0.7732, 0.764 ,\n",
       "        0.7658, 0.7606, 0.749 , 0.756 , 0.7572, 0.7464, 0.745 , 0.7442,\n",
       "        0.7714, 0.775 , 0.7762, 0.7608, 0.765 , 0.769 , 0.7544, 0.754 ,\n",
       "        0.7434, 0.7416, 0.749 , 0.7446, 0.7736, 0.7734, 0.7748, 0.762 ,\n",
       "        0.7646, 0.7682, 0.7572, 0.7588, 0.7602, 0.752 , 0.756 , 0.7464,\n",
       "        0.7766, 0.7706, 0.777 , 0.752 , 0.7564, 0.7536, 0.7454, 0.7498,\n",
       "        0.7478, 0.7394, 0.7454, 0.747 , 0.78  , 0.7732, 0.7744, 0.7622,\n",
       "        0.7666, 0.7534, 0.7538, 0.743 , 0.7516, 0.7426, 0.743 , 0.7444,\n",
       "        0.7718, 0.7698, 0.7752, 0.7566, 0.7644, 0.768 , 0.7508, 0.755 ,\n",
       "        0.7544, 0.7392, 0.7402, 0.7352, 0.7746, 0.7752, 0.7728, 0.7628,\n",
       "        0.7642, 0.7656, 0.7494, 0.7578, 0.7486, 0.7408, 0.7488, 0.7474,\n",
       "        0.7808, 0.5142, 0.7792, 0.7786, 0.5062, 0.7806, 0.782 , 0.5442,\n",
       "        0.7796, 0.7794, 0.5056, 0.7806, 0.7798, 0.5168, 0.777 , 0.7818,\n",
       "        0.4922, 0.7802, 0.7792, 0.5462, 0.78  , 0.7798, 0.5284, 0.782 ,\n",
       "        0.7802, 0.497 , 0.782 , 0.7774, 0.5172, 0.7836, 0.7786, 0.5328,\n",
       "        0.78  , 0.7802, 0.5076, 0.7782, 0.78  , 0.4892, 0.779 , 0.7824,\n",
       "        0.5276, 0.78  , 0.783 , 0.4892, 0.7818, 0.7798, 0.4448, 0.7814,\n",
       "        0.7798, 0.5854, 0.78  , 0.7772, 0.5798, 0.7812, 0.7786, 0.6292,\n",
       "        0.7778, 0.781 , 0.592 , 0.7786, 0.7752, 0.5784, 0.7786, 0.7778,\n",
       "        0.5684, 0.7802, 0.78  , 0.6048, 0.7802, 0.7812, 0.6606, 0.7802,\n",
       "        0.7792, 0.5616, 0.7796, 0.7808, 0.5504, 0.7764, 0.7824, 0.6578,\n",
       "        0.7772, 0.779 , 0.643 , 0.7808, 0.781 , 0.5146, 0.7798, 0.7806,\n",
       "        0.6306, 0.7804, 0.7848, 0.644 , 0.777 , 0.7772, 0.6408, 0.7806,\n",
       "        0.7762, 0.7772, 0.7776, 0.7776, 0.7776, 0.7786, 0.7784, 0.7784,\n",
       "        0.7802, 0.7784, 0.7768, 0.7782, 0.7796, 0.7774, 0.7772, 0.7774,\n",
       "        0.7768, 0.7776, 0.7784, 0.778 , 0.7764, 0.7772, 0.7772, 0.7764,\n",
       "        0.7788, 0.7786, 0.7814, 0.7788, 0.7774, 0.7776, 0.776 , 0.7776,\n",
       "        0.779 , 0.7766, 0.7784, 0.7772, 0.7772, 0.7788, 0.7808, 0.7794,\n",
       "        0.7788, 0.779 , 0.777 , 0.7778, 0.777 , 0.7784, 0.7774, 0.7778,\n",
       "        0.777 , 0.7774, 0.7786, 0.7774, 0.7776, 0.7776, 0.7798, 0.7766,\n",
       "        0.7796, 0.7768, 0.7776, 0.7798, 0.7786, 0.7762, 0.7784, 0.7784,\n",
       "        0.7774, 0.7816, 0.773 , 0.7788, 0.7796, 0.7784, 0.7748, 0.773 ,\n",
       "        0.7828, 0.7766, 0.7786, 0.7804, 0.777 , 0.7784, 0.7772, 0.7784,\n",
       "        0.7776, 0.7768, 0.7746, 0.7752, 0.778 , 0.7816, 0.7786, 0.7784,\n",
       "        0.7788, 0.7794, 0.7782, 0.7796, 0.7774, 0.777 , 0.7778, 0.7834]),\n",
       " 'std_test_f1_micro': array([0.01187603, 0.00886341, 0.00332265, 0.00874986, 0.01222129,\n",
       "        0.00846168, 0.01411382, 0.01916872, 0.01114271, 0.0095205 ,\n",
       "        0.01212271, 0.01666613, 0.00361109, 0.00260768, 0.00713863,\n",
       "        0.01005982, 0.01022546, 0.01423517, 0.00965401, 0.01424079,\n",
       "        0.01126765, 0.01192644, 0.00950789, 0.01342237, 0.00976934,\n",
       "        0.01593738, 0.00856505, 0.01420422, 0.01100909, 0.01208305,\n",
       "        0.00875443, 0.01469694, 0.01332066, 0.01130664, 0.00839047,\n",
       "        0.0082365 , 0.00891291, 0.01087382, 0.00391918, 0.01553061,\n",
       "        0.01036533, 0.007494  , 0.01045753, 0.01137365, 0.00793473,\n",
       "        0.0128841 , 0.01048809, 0.01576832, 0.0106132 , 0.01057544,\n",
       "        0.01011929, 0.01025671, 0.01570478, 0.01557691, 0.00941488,\n",
       "        0.00888594, 0.01140877, 0.01237093, 0.00422374, 0.01060189,\n",
       "        0.0060663 , 0.01487817, 0.00674092, 0.016253  , 0.01202664,\n",
       "        0.0107629 , 0.00788416, 0.00819756, 0.01430524, 0.01877871,\n",
       "        0.01446375, 0.01107429, 0.00899778, 0.00752064, 0.0082801 ,\n",
       "        0.01562818, 0.01518684, 0.00603324, 0.00702567, 0.01579873,\n",
       "        0.01137717, 0.01208967, 0.0136147 , 0.00746726, 0.01613196,\n",
       "        0.01068457, 0.00917388, 0.00549181, 0.0082801 , 0.00793977,\n",
       "        0.01336563, 0.00790949, 0.01415062, 0.00800999, 0.00858836,\n",
       "        0.01121784, 0.00813388, 0.03401999, 0.01060943, 0.00665132,\n",
       "        0.01582909, 0.00793977, 0.00892188, 0.05199769, 0.00770973,\n",
       "        0.00926499, 0.02226747, 0.00877724, 0.0064622 , 0.02748381,\n",
       "        0.00878635, 0.00813388, 0.11353484, 0.00945304, 0.00982649,\n",
       "        0.08799409, 0.008     , 0.00941063, 0.01979495, 0.0117303 ,\n",
       "        0.00808455, 0.00831865, 0.00954987, 0.00897998, 0.03931615,\n",
       "        0.01135958, 0.01049952, 0.08925111, 0.00769415, 0.01108873,\n",
       "        0.08366505, 0.0102645 , 0.00737564, 0.0009798 , 0.01224745,\n",
       "        0.00997196, 0.06206964, 0.00950789, 0.01354991, 0.0936235 ,\n",
       "        0.00938936, 0.00941063, 0.09445295, 0.01066958, 0.0095373 ,\n",
       "        0.06527051, 0.00701427, 0.00767854, 0.07592207, 0.00856505,\n",
       "        0.00736478, 0.05539097, 0.00722219, 0.0091433 , 0.03472751,\n",
       "        0.00877724, 0.00919565, 0.06913349, 0.00770973, 0.00874986,\n",
       "        0.06086904, 0.0059127 , 0.0091214 , 0.07433277, 0.00735935,\n",
       "        0.00936803, 0.06722678, 0.00752064, 0.00767854, 0.06636143,\n",
       "        0.00893532, 0.00906422, 0.02842956, 0.00958332, 0.01111036,\n",
       "        0.05332692, 0.00994786, 0.01067708, 0.05223026, 0.00949526,\n",
       "        0.00748331, 0.09409059, 0.00708237, 0.00928655, 0.05792271,\n",
       "        0.00677052, 0.00851822, 0.08469475, 0.00916515, 0.00910824,\n",
       "        0.0791237 , 0.01139474, 0.00865794, 0.0109252 , 0.00793977,\n",
       "        0.00852291, 0.01107429, 0.00786384, 0.00939361, 0.00906863,\n",
       "        0.00813388, 0.00854634, 0.00970361, 0.0079599 , 0.00798999,\n",
       "        0.0111463 , 0.00919565, 0.0094361 , 0.01010742, 0.00958332,\n",
       "        0.00861626, 0.00977753, 0.00954149, 0.01043839, 0.01006777,\n",
       "        0.01065082, 0.00994786, 0.00930806, 0.01205985, 0.01060943,\n",
       "        0.00873155, 0.00873155, 0.00995992, 0.00964572, 0.0090111 ,\n",
       "        0.009992  , 0.00997196, 0.00955824, 0.00910824, 0.00803492,\n",
       "        0.00964158, 0.01160345, 0.0095163 , 0.00846168, 0.00944458,\n",
       "        0.01060943, 0.00792465, 0.00935094, 0.00902441, 0.0095163 ,\n",
       "        0.00745654, 0.00700286, 0.00926499, 0.0063435 , 0.0072    ,\n",
       "        0.00344093, 0.01083328, 0.00608605, 0.00768375, 0.00957914,\n",
       "        0.00508331, 0.00886341, 0.00828493, 0.00908625, 0.00956243,\n",
       "        0.00628013, 0.00811419, 0.00665132, 0.00779744, 0.00601332,\n",
       "        0.01019019, 0.00909065, 0.00693974, 0.00789937, 0.01230285,\n",
       "        0.00997196, 0.00991161, 0.00889044, 0.0085557 , 0.00906863,\n",
       "        0.00851822, 0.0054626 , 0.00773563, 0.00805978, 0.0052    ,\n",
       "        0.01273421, 0.01065833, 0.00752596, 0.0048    , 0.0088227 ,\n",
       "        0.00444522, 0.0109654 , 0.00835225, 0.00760526, 0.00557136,\n",
       "        0.00536656, 0.0107963 , 0.00445421]),\n",
       " 'rank_test_f1_micro': array([151, 180, 171, 200, 199, 217, 205, 214, 219, 245, 235, 250,  59,\n",
       "        137, 175, 195, 189, 202, 229, 211, 207, 237, 241, 244, 182, 166,\n",
       "        158, 200, 191, 185, 216, 217, 245, 251, 229, 242, 173, 174, 168,\n",
       "        198, 192, 186, 208, 204, 203, 223, 211, 237, 151, 183, 142, 223,\n",
       "        210, 221, 239, 227, 233, 254, 239, 236,  38, 176, 171, 197, 188,\n",
       "        222, 219, 247, 225, 249, 247, 243, 180, 184, 164, 209, 193, 187,\n",
       "        226, 213, 215, 255, 253, 256, 169, 162, 179, 196, 194, 190, 228,\n",
       "        206, 232, 252, 231, 234,  21, 280,  63,  78, 282,  25,   8, 273,\n",
       "         52,  59, 283,  28,  45, 278, 142,  11, 285,  31,  63, 272,  38,\n",
       "         45, 275,   8,  31, 284,   8, 118, 277,   2,  75, 274,  38,  31,\n",
       "        281,  98,  38, 287,  67,   6, 276,  38,   4, 286,  12,  45, 288,\n",
       "         15,  45, 266,  38, 128, 267,  18,  75, 263, 103,  20, 265,  78,\n",
       "        162, 268,  78, 103, 269,  31,  38, 264,  31,  17, 257,  31,  62,\n",
       "        270,  52,  21, 271, 156,   6, 258, 128,  65, 260,  23,  19, 279,\n",
       "         45,  25, 262,  30,   1, 259, 137, 128, 261,  25, 158, 127, 113,\n",
       "        108, 108,  78,  87,  92,  31,  92, 147,  98,  57, 118, 128, 118,\n",
       "        147, 113,  87, 101, 156, 128, 128, 155,  69,  85,  16,  71, 122,\n",
       "        108, 161, 108,  65, 150,  92, 128, 128,  69,  23,  59,  71,  67,\n",
       "        142, 106, 137,  86, 125, 106, 142, 125,  75, 118, 113, 108,  45,\n",
       "        151,  52, 146, 113,  45,  78, 158,  92,  92, 122,  14, 177,  71,\n",
       "         52,  87, 167, 178,   5, 151,  78,  29, 137,  87, 128,  87, 113,\n",
       "        147, 169, 164, 101,  13,  78,  92,  71,  58, 100,  52, 122, 137,\n",
       "        103,   3]),\n",
       " 'split0_test_precision_micro': array([0.758, 0.762, 0.775, 0.744, 0.747, 0.745, 0.745, 0.74 , 0.74 ,\n",
       "        0.739, 0.732, 0.725, 0.775, 0.782, 0.779, 0.746, 0.748, 0.743,\n",
       "        0.743, 0.735, 0.741, 0.737, 0.74 , 0.727, 0.763, 0.755, 0.767,\n",
       "        0.756, 0.748, 0.751, 0.745, 0.732, 0.73 , 0.757, 0.733, 0.739,\n",
       "        0.767, 0.76 , 0.775, 0.748, 0.746, 0.761, 0.741, 0.748, 0.75 ,\n",
       "        0.745, 0.748, 0.721, 0.759, 0.762, 0.766, 0.738, 0.736, 0.73 ,\n",
       "        0.753, 0.739, 0.727, 0.749, 0.742, 0.741, 0.776, 0.755, 0.765,\n",
       "        0.74 , 0.754, 0.75 , 0.745, 0.732, 0.727, 0.722, 0.722, 0.724,\n",
       "        0.763, 0.761, 0.759, 0.741, 0.741, 0.764, 0.743, 0.726, 0.759,\n",
       "        0.726, 0.733, 0.733, 0.761, 0.765, 0.763, 0.754, 0.763, 0.75 ,\n",
       "        0.743, 0.749, 0.735, 0.749, 0.745, 0.752, 0.77 , 0.512, 0.763,\n",
       "        0.771, 0.489, 0.77 , 0.77 , 0.62 , 0.766, 0.766, 0.511, 0.768,\n",
       "        0.772, 0.511, 0.765, 0.769, 0.513, 0.768, 0.764, 0.513, 0.766,\n",
       "        0.767, 0.533, 0.768, 0.772, 0.499, 0.766, 0.765, 0.489, 0.768,\n",
       "        0.764, 0.489, 0.769, 0.766, 0.511, 0.763, 0.767, 0.488, 0.762,\n",
       "        0.767, 0.477, 0.768, 0.767, 0.511, 0.768, 0.765, 0.556, 0.763,\n",
       "        0.765, 0.508, 0.768, 0.77 , 0.568, 0.772, 0.772, 0.611, 0.765,\n",
       "        0.767, 0.537, 0.774, 0.759, 0.685, 0.772, 0.764, 0.494, 0.773,\n",
       "        0.767, 0.559, 0.77 , 0.767, 0.547, 0.775, 0.765, 0.478, 0.765,\n",
       "        0.765, 0.53 , 0.768, 0.77 , 0.621, 0.761, 0.766, 0.602, 0.767,\n",
       "        0.771, 0.386, 0.771, 0.766, 0.686, 0.772, 0.77 , 0.72 , 0.764,\n",
       "        0.762, 0.726, 0.765, 0.762, 0.761, 0.767, 0.764, 0.759, 0.767,\n",
       "        0.765, 0.763, 0.767, 0.765, 0.76 , 0.765, 0.769, 0.761, 0.763,\n",
       "        0.763, 0.762, 0.763, 0.766, 0.761, 0.762, 0.761, 0.762, 0.759,\n",
       "        0.763, 0.765, 0.764, 0.762, 0.763, 0.764, 0.761, 0.764, 0.764,\n",
       "        0.761, 0.762, 0.763, 0.762, 0.766, 0.768, 0.761, 0.765, 0.766,\n",
       "        0.762, 0.761, 0.765, 0.764, 0.764, 0.762, 0.767, 0.771, 0.766,\n",
       "        0.778, 0.766, 0.774, 0.763, 0.771, 0.77 , 0.771, 0.772, 0.774,\n",
       "        0.772, 0.768, 0.768, 0.767, 0.771, 0.777, 0.766, 0.774, 0.775,\n",
       "        0.765, 0.769, 0.761, 0.763, 0.763, 0.77 , 0.779, 0.769, 0.768,\n",
       "        0.767, 0.779, 0.772, 0.774, 0.774, 0.77 , 0.761, 0.775, 0.772,\n",
       "        0.772, 0.772, 0.765, 0.774, 0.768, 0.774, 0.768, 0.768, 0.778]),\n",
       " 'split1_test_precision_micro': array([0.793, 0.787, 0.776, 0.762, 0.783, 0.768, 0.782, 0.785, 0.772,\n",
       "        0.753, 0.766, 0.764, 0.782, 0.776, 0.776, 0.777, 0.78 , 0.786,\n",
       "        0.759, 0.772, 0.772, 0.769, 0.764, 0.749, 0.783, 0.803, 0.789,\n",
       "        0.773, 0.777, 0.786, 0.767, 0.778, 0.758, 0.744, 0.753, 0.759,\n",
       "        0.788, 0.791, 0.781, 0.786, 0.773, 0.78 , 0.762, 0.777, 0.773,\n",
       "        0.776, 0.776, 0.766, 0.791, 0.789, 0.79 , 0.768, 0.782, 0.76 ,\n",
       "        0.747, 0.757, 0.755, 0.743, 0.75 , 0.76 , 0.79 , 0.795, 0.785,\n",
       "        0.788, 0.754, 0.757, 0.761, 0.756, 0.771, 0.759, 0.767, 0.757,\n",
       "        0.783, 0.782, 0.777, 0.774, 0.782, 0.778, 0.75 , 0.773, 0.771,\n",
       "        0.75 , 0.761, 0.747, 0.799, 0.794, 0.79 , 0.769, 0.769, 0.767,\n",
       "        0.775, 0.768, 0.771, 0.745, 0.745, 0.757, 0.782, 0.465, 0.786,\n",
       "        0.785, 0.511, 0.789, 0.785, 0.491, 0.785, 0.783, 0.492, 0.791,\n",
       "        0.791, 0.489, 0.783, 0.791, 0.579, 0.791, 0.787, 0.476, 0.786,\n",
       "        0.787, 0.514, 0.796, 0.793, 0.498, 0.79 , 0.781, 0.511, 0.792,\n",
       "        0.788, 0.675, 0.79 , 0.793, 0.369, 0.786, 0.787, 0.489, 0.795,\n",
       "        0.79 , 0.649, 0.789, 0.8  , 0.452, 0.79 , 0.792, 0.308, 0.793,\n",
       "        0.793, 0.693, 0.79 , 0.787, 0.552, 0.793, 0.792, 0.685, 0.787,\n",
       "        0.787, 0.576, 0.787, 0.782, 0.52 , 0.787, 0.786, 0.619, 0.791,\n",
       "        0.793, 0.67 , 0.789, 0.792, 0.718, 0.793, 0.787, 0.488, 0.785,\n",
       "        0.788, 0.605, 0.779, 0.796, 0.731, 0.783, 0.793, 0.604, 0.788,\n",
       "        0.79 , 0.574, 0.789, 0.791, 0.542, 0.787, 0.794, 0.749, 0.785,\n",
       "        0.788, 0.732, 0.786, 0.785, 0.789, 0.787, 0.786, 0.786, 0.787,\n",
       "        0.788, 0.787, 0.789, 0.787, 0.786, 0.786, 0.787, 0.791, 0.786,\n",
       "        0.788, 0.788, 0.787, 0.789, 0.788, 0.787, 0.787, 0.786, 0.789,\n",
       "        0.788, 0.792, 0.8  , 0.79 , 0.787, 0.786, 0.786, 0.79 , 0.789,\n",
       "        0.787, 0.786, 0.786, 0.788, 0.789, 0.797, 0.792, 0.791, 0.787,\n",
       "        0.787, 0.788, 0.784, 0.787, 0.786, 0.787, 0.787, 0.79 , 0.784,\n",
       "        0.785, 0.783, 0.781, 0.789, 0.786, 0.789, 0.792, 0.787, 0.794,\n",
       "        0.788, 0.78 , 0.792, 0.786, 0.782, 0.788, 0.771, 0.786, 0.795,\n",
       "        0.787, 0.787, 0.785, 0.801, 0.786, 0.786, 0.792, 0.78 , 0.786,\n",
       "        0.787, 0.78 , 0.782, 0.787, 0.773, 0.795, 0.788, 0.795, 0.784,\n",
       "        0.79 , 0.779, 0.784, 0.786, 0.789, 0.782, 0.78 , 0.795, 0.786]),\n",
       " 'split2_test_precision_micro': array([0.783, 0.764, 0.769, 0.769, 0.758, 0.758, 0.763, 0.751, 0.76 ,\n",
       "        0.738, 0.737, 0.76 , 0.775, 0.775, 0.762, 0.767, 0.767, 0.753,\n",
       "        0.756, 0.772, 0.765, 0.739, 0.74 , 0.737, 0.758, 0.766, 0.767,\n",
       "        0.769, 0.757, 0.761, 0.759, 0.756, 0.74 , 0.737, 0.749, 0.735,\n",
       "        0.762, 0.768, 0.769, 0.775, 0.769, 0.774, 0.754, 0.766, 0.763,\n",
       "        0.752, 0.757, 0.747, 0.775, 0.762, 0.766, 0.757, 0.762, 0.771,\n",
       "        0.749, 0.74 , 0.744, 0.746, 0.739, 0.755, 0.772, 0.78 , 0.77 ,\n",
       "        0.758, 0.767, 0.74 , 0.764, 0.746, 0.754, 0.762, 0.739, 0.747,\n",
       "        0.781, 0.763, 0.78 , 0.735, 0.758, 0.772, 0.749, 0.756, 0.748,\n",
       "        0.757, 0.741, 0.734, 0.755, 0.769, 0.769, 0.76 , 0.751, 0.769,\n",
       "        0.747, 0.762, 0.745, 0.747, 0.749, 0.747, 0.773, 0.511, 0.771,\n",
       "        0.77 , 0.531, 0.772, 0.773, 0.506, 0.776, 0.771, 0.489, 0.773,\n",
       "        0.776, 0.535, 0.768, 0.776, 0.566, 0.771, 0.771, 0.511, 0.776,\n",
       "        0.77 , 0.565, 0.768, 0.771, 0.511, 0.776, 0.769, 0.594, 0.773,\n",
       "        0.768, 0.41 , 0.775, 0.768, 0.633, 0.769, 0.777, 0.491, 0.774,\n",
       "        0.774, 0.506, 0.769, 0.767, 0.511, 0.773, 0.775, 0.489, 0.78 ,\n",
       "        0.774, 0.561, 0.78 , 0.773, 0.725, 0.775, 0.772, 0.533, 0.777,\n",
       "        0.777, 0.628, 0.765, 0.772, 0.629, 0.767, 0.771, 0.596, 0.778,\n",
       "        0.773, 0.608, 0.775, 0.774, 0.739, 0.772, 0.778, 0.586, 0.775,\n",
       "        0.777, 0.532, 0.763, 0.769, 0.617, 0.772, 0.767, 0.661, 0.772,\n",
       "        0.774, 0.639, 0.772, 0.776, 0.649, 0.773, 0.781, 0.51 , 0.768,\n",
       "        0.773, 0.538, 0.769, 0.771, 0.768, 0.77 , 0.772, 0.772, 0.772,\n",
       "        0.77 , 0.774, 0.776, 0.772, 0.773, 0.774, 0.773, 0.768, 0.77 ,\n",
       "        0.77 , 0.769, 0.77 , 0.771, 0.774, 0.769, 0.769, 0.769, 0.771,\n",
       "        0.772, 0.772, 0.774, 0.772, 0.772, 0.771, 0.769, 0.769, 0.775,\n",
       "        0.769, 0.773, 0.769, 0.773, 0.774, 0.775, 0.773, 0.772, 0.774,\n",
       "        0.771, 0.771, 0.771, 0.772, 0.77 , 0.772, 0.77 , 0.771, 0.769,\n",
       "        0.766, 0.777, 0.773, 0.771, 0.769, 0.771, 0.77 , 0.775, 0.768,\n",
       "        0.766, 0.763, 0.767, 0.778, 0.765, 0.774, 0.764, 0.772, 0.765,\n",
       "        0.77 , 0.769, 0.775, 0.779, 0.766, 0.764, 0.765, 0.765, 0.768,\n",
       "        0.768, 0.768, 0.768, 0.763, 0.766, 0.759, 0.771, 0.774, 0.775,\n",
       "        0.765, 0.776, 0.769, 0.765, 0.774, 0.768, 0.774, 0.765, 0.778]),\n",
       " 'split3_test_precision_micro': array([0.779, 0.772, 0.779, 0.766, 0.764, 0.753, 0.743, 0.766, 0.747,\n",
       "        0.731, 0.747, 0.726, 0.783, 0.775, 0.781, 0.764, 0.767, 0.761,\n",
       "        0.733, 0.75 , 0.76 , 0.739, 0.741, 0.767, 0.781, 0.777, 0.776,\n",
       "        0.735, 0.768, 0.776, 0.757, 0.75 , 0.729, 0.723, 0.757, 0.747,\n",
       "        0.777, 0.78 , 0.773, 0.751, 0.774, 0.764, 0.756, 0.747, 0.761,\n",
       "        0.738, 0.75 , 0.759, 0.783, 0.764, 0.787, 0.752, 0.745, 0.741,\n",
       "        0.727, 0.752, 0.756, 0.715, 0.747, 0.749, 0.782, 0.758, 0.775,\n",
       "        0.754, 0.786, 0.772, 0.754, 0.744, 0.757, 0.718, 0.741, 0.744,\n",
       "        0.761, 0.77 , 0.782, 0.764, 0.78 , 0.763, 0.748, 0.764, 0.737,\n",
       "        0.73 , 0.72 , 0.724, 0.786, 0.768, 0.769, 0.768, 0.762, 0.771,\n",
       "        0.736, 0.762, 0.734, 0.735, 0.74 , 0.726, 0.789, 0.572, 0.784,\n",
       "        0.784, 0.489, 0.786, 0.79 , 0.593, 0.786, 0.788, 0.547, 0.783,\n",
       "        0.778, 0.489, 0.782, 0.785, 0.533, 0.781, 0.786, 0.511, 0.786,\n",
       "        0.785, 0.511, 0.788, 0.782, 0.489, 0.79 , 0.783, 0.503, 0.787,\n",
       "        0.784, 0.511, 0.779, 0.785, 0.514, 0.784, 0.786, 0.489, 0.773,\n",
       "        0.791, 0.491, 0.785, 0.789, 0.63 , 0.788, 0.78 , 0.511, 0.78 ,\n",
       "        0.784, 0.622, 0.781, 0.77 , 0.552, 0.776, 0.777, 0.637, 0.779,\n",
       "        0.78 , 0.63 , 0.789, 0.778, 0.5  , 0.783, 0.783, 0.636, 0.779,\n",
       "        0.782, 0.696, 0.779, 0.784, 0.648, 0.777, 0.782, 0.638, 0.782,\n",
       "        0.784, 0.552, 0.783, 0.785, 0.714, 0.78 , 0.784, 0.61 , 0.786,\n",
       "        0.782, 0.547, 0.783, 0.78 , 0.586, 0.782, 0.79 , 0.622, 0.782,\n",
       "        0.779, 0.569, 0.79 , 0.779, 0.781, 0.779, 0.78 , 0.781, 0.781,\n",
       "        0.781, 0.781, 0.781, 0.782, 0.779, 0.78 , 0.779, 0.782, 0.781,\n",
       "        0.781, 0.778, 0.781, 0.781, 0.781, 0.779, 0.782, 0.781, 0.778,\n",
       "        0.782, 0.781, 0.783, 0.781, 0.782, 0.781, 0.777, 0.781, 0.78 ,\n",
       "        0.781, 0.781, 0.781, 0.779, 0.781, 0.781, 0.78 , 0.779, 0.779,\n",
       "        0.779, 0.78 , 0.779, 0.78 , 0.78 , 0.782, 0.782, 0.776, 0.789,\n",
       "        0.781, 0.787, 0.781, 0.786, 0.778, 0.785, 0.784, 0.778, 0.779,\n",
       "        0.782, 0.784, 0.783, 0.78 , 0.782, 0.778, 0.78 , 0.786, 0.777,\n",
       "        0.786, 0.778, 0.769, 0.784, 0.783, 0.783, 0.785, 0.787, 0.78 ,\n",
       "        0.778, 0.781, 0.776, 0.781, 0.78 , 0.768, 0.781, 0.783, 0.784,\n",
       "        0.782, 0.784, 0.784, 0.778, 0.782, 0.782, 0.78 , 0.778, 0.787]),\n",
       " 'split4_test_precision_micro': array([0.77 , 0.774, 0.773, 0.763, 0.754, 0.746, 0.757, 0.731, 0.75 ,\n",
       "        0.756, 0.754, 0.736, 0.782, 0.777, 0.768, 0.766, 0.767, 0.76 ,\n",
       "        0.754, 0.751, 0.748, 0.748, 0.74 , 0.741, 0.772, 0.774, 0.782,\n",
       "        0.771, 0.775, 0.771, 0.744, 0.754, 0.76 , 0.747, 0.753, 0.743,\n",
       "        0.774, 0.768, 0.776, 0.75 , 0.761, 0.762, 0.773, 0.756, 0.754,\n",
       "        0.749, 0.749, 0.739, 0.775, 0.776, 0.776, 0.745, 0.757, 0.766,\n",
       "        0.751, 0.761, 0.757, 0.744, 0.749, 0.73 , 0.78 , 0.778, 0.777,\n",
       "        0.771, 0.772, 0.748, 0.745, 0.737, 0.749, 0.752, 0.746, 0.75 ,\n",
       "        0.771, 0.773, 0.778, 0.769, 0.761, 0.763, 0.764, 0.756, 0.757,\n",
       "        0.733, 0.746, 0.738, 0.772, 0.78 , 0.773, 0.763, 0.776, 0.771,\n",
       "        0.746, 0.748, 0.758, 0.728, 0.765, 0.755, 0.79 , 0.511, 0.792,\n",
       "        0.783, 0.511, 0.786, 0.792, 0.511, 0.785, 0.789, 0.489, 0.788,\n",
       "        0.782, 0.56 , 0.787, 0.788, 0.27 , 0.79 , 0.788, 0.72 , 0.786,\n",
       "        0.79 , 0.519, 0.79 , 0.783, 0.488, 0.788, 0.789, 0.489, 0.798,\n",
       "        0.789, 0.579, 0.787, 0.789, 0.511, 0.789, 0.783, 0.489, 0.791,\n",
       "        0.79 , 0.515, 0.789, 0.792, 0.342, 0.79 , 0.787, 0.36 , 0.791,\n",
       "        0.783, 0.543, 0.781, 0.786, 0.502, 0.79 , 0.78 , 0.68 , 0.781,\n",
       "        0.794, 0.589, 0.778, 0.785, 0.558, 0.784, 0.785, 0.497, 0.78 ,\n",
       "        0.785, 0.491, 0.788, 0.789, 0.651, 0.784, 0.784, 0.618, 0.791,\n",
       "        0.79 , 0.533, 0.789, 0.792, 0.606, 0.79 , 0.785, 0.738, 0.791,\n",
       "        0.788, 0.427, 0.784, 0.79 , 0.69 , 0.788, 0.789, 0.619, 0.786,\n",
       "        0.784, 0.639, 0.793, 0.784, 0.787, 0.785, 0.786, 0.79 , 0.786,\n",
       "        0.788, 0.787, 0.788, 0.786, 0.786, 0.786, 0.79 , 0.785, 0.786,\n",
       "        0.785, 0.787, 0.787, 0.785, 0.786, 0.785, 0.787, 0.788, 0.785,\n",
       "        0.789, 0.783, 0.786, 0.789, 0.783, 0.786, 0.787, 0.784, 0.787,\n",
       "        0.785, 0.79 , 0.787, 0.784, 0.784, 0.783, 0.791, 0.787, 0.789,\n",
       "        0.786, 0.789, 0.786, 0.789, 0.787, 0.786, 0.779, 0.779, 0.785,\n",
       "        0.777, 0.775, 0.779, 0.79 , 0.779, 0.783, 0.767, 0.776, 0.784,\n",
       "        0.785, 0.786, 0.782, 0.781, 0.787, 0.791, 0.784, 0.776, 0.786,\n",
       "        0.784, 0.771, 0.775, 0.787, 0.785, 0.79 , 0.781, 0.784, 0.79 ,\n",
       "        0.786, 0.784, 0.79 , 0.779, 0.78 , 0.784, 0.789, 0.781, 0.778,\n",
       "        0.783, 0.783, 0.795, 0.788, 0.785, 0.781, 0.783, 0.783, 0.788]),\n",
       " 'mean_test_precision_micro': array([0.7766, 0.7718, 0.7744, 0.7608, 0.7612, 0.754 , 0.758 , 0.7546,\n",
       "        0.7538, 0.7434, 0.7472, 0.7422, 0.7794, 0.777 , 0.7732, 0.764 ,\n",
       "        0.7658, 0.7606, 0.749 , 0.756 , 0.7572, 0.7464, 0.745 , 0.7442,\n",
       "        0.7714, 0.775 , 0.7762, 0.7608, 0.765 , 0.769 , 0.7544, 0.754 ,\n",
       "        0.7434, 0.7416, 0.749 , 0.7446, 0.7736, 0.7734, 0.7748, 0.762 ,\n",
       "        0.7646, 0.7682, 0.7572, 0.7588, 0.7602, 0.752 , 0.756 , 0.7464,\n",
       "        0.7766, 0.7706, 0.777 , 0.752 , 0.7564, 0.7536, 0.7454, 0.7498,\n",
       "        0.7478, 0.7394, 0.7454, 0.747 , 0.78  , 0.7732, 0.7744, 0.7622,\n",
       "        0.7666, 0.7534, 0.7538, 0.743 , 0.7516, 0.7426, 0.743 , 0.7444,\n",
       "        0.7718, 0.7698, 0.7752, 0.7566, 0.7644, 0.768 , 0.7508, 0.755 ,\n",
       "        0.7544, 0.7392, 0.7402, 0.7352, 0.7746, 0.7752, 0.7728, 0.7628,\n",
       "        0.7642, 0.7656, 0.7494, 0.7578, 0.7486, 0.7408, 0.7488, 0.7474,\n",
       "        0.7808, 0.5142, 0.7792, 0.7786, 0.5062, 0.7806, 0.782 , 0.5442,\n",
       "        0.7796, 0.7794, 0.5056, 0.7806, 0.7798, 0.5168, 0.777 , 0.7818,\n",
       "        0.4922, 0.7802, 0.7792, 0.5462, 0.78  , 0.7798, 0.5284, 0.782 ,\n",
       "        0.7802, 0.497 , 0.782 , 0.7774, 0.5172, 0.7836, 0.7786, 0.5328,\n",
       "        0.78  , 0.7802, 0.5076, 0.7782, 0.78  , 0.4892, 0.779 , 0.7824,\n",
       "        0.5276, 0.78  , 0.783 , 0.4892, 0.7818, 0.7798, 0.4448, 0.7814,\n",
       "        0.7798, 0.5854, 0.78  , 0.7772, 0.5798, 0.7812, 0.7786, 0.6292,\n",
       "        0.7778, 0.781 , 0.592 , 0.7786, 0.7752, 0.5784, 0.7786, 0.7778,\n",
       "        0.5684, 0.7802, 0.78  , 0.6048, 0.7802, 0.7812, 0.6606, 0.7802,\n",
       "        0.7792, 0.5616, 0.7796, 0.7808, 0.5504, 0.7764, 0.7824, 0.6578,\n",
       "        0.7772, 0.779 , 0.643 , 0.7808, 0.781 , 0.5146, 0.7798, 0.7806,\n",
       "        0.6306, 0.7804, 0.7848, 0.644 , 0.777 , 0.7772, 0.6408, 0.7806,\n",
       "        0.7762, 0.7772, 0.7776, 0.7776, 0.7776, 0.7786, 0.7784, 0.7784,\n",
       "        0.7802, 0.7784, 0.7768, 0.7782, 0.7796, 0.7774, 0.7772, 0.7774,\n",
       "        0.7768, 0.7776, 0.7784, 0.778 , 0.7764, 0.7772, 0.7772, 0.7764,\n",
       "        0.7788, 0.7786, 0.7814, 0.7788, 0.7774, 0.7776, 0.776 , 0.7776,\n",
       "        0.779 , 0.7766, 0.7784, 0.7772, 0.7772, 0.7788, 0.7808, 0.7794,\n",
       "        0.7788, 0.779 , 0.777 , 0.7778, 0.777 , 0.7784, 0.7774, 0.7778,\n",
       "        0.777 , 0.7774, 0.7786, 0.7774, 0.7776, 0.7776, 0.7798, 0.7766,\n",
       "        0.7796, 0.7768, 0.7776, 0.7798, 0.7786, 0.7762, 0.7784, 0.7784,\n",
       "        0.7774, 0.7816, 0.773 , 0.7788, 0.7796, 0.7784, 0.7748, 0.773 ,\n",
       "        0.7828, 0.7766, 0.7786, 0.7804, 0.777 , 0.7784, 0.7772, 0.7784,\n",
       "        0.7776, 0.7768, 0.7746, 0.7752, 0.778 , 0.7816, 0.7786, 0.7784,\n",
       "        0.7788, 0.7794, 0.7782, 0.7796, 0.7774, 0.777 , 0.7778, 0.7834]),\n",
       " 'std_test_precision_micro': array([0.01187603, 0.00886341, 0.00332265, 0.00874986, 0.01222129,\n",
       "        0.00846168, 0.01411382, 0.01916872, 0.01114271, 0.0095205 ,\n",
       "        0.01212271, 0.01666613, 0.00361109, 0.00260768, 0.00713863,\n",
       "        0.01005982, 0.01022546, 0.01423517, 0.00965401, 0.01424079,\n",
       "        0.01126765, 0.01192644, 0.00950789, 0.01342237, 0.00976934,\n",
       "        0.01593738, 0.00856505, 0.01420422, 0.01100909, 0.01208305,\n",
       "        0.00875443, 0.01469694, 0.01332066, 0.01130664, 0.00839047,\n",
       "        0.0082365 , 0.00891291, 0.01087382, 0.00391918, 0.01553061,\n",
       "        0.01036533, 0.007494  , 0.01045753, 0.01137365, 0.00793473,\n",
       "        0.0128841 , 0.01048809, 0.01576832, 0.0106132 , 0.01057544,\n",
       "        0.01011929, 0.01025671, 0.01570478, 0.01557691, 0.00941488,\n",
       "        0.00888594, 0.01140877, 0.01237093, 0.00422374, 0.01060189,\n",
       "        0.0060663 , 0.01487817, 0.00674092, 0.016253  , 0.01202664,\n",
       "        0.0107629 , 0.00788416, 0.00819756, 0.01430524, 0.01877871,\n",
       "        0.01446375, 0.01107429, 0.00899778, 0.00752064, 0.0082801 ,\n",
       "        0.01562818, 0.01518684, 0.00603324, 0.00702567, 0.01579873,\n",
       "        0.01137717, 0.01208967, 0.0136147 , 0.00746726, 0.01613196,\n",
       "        0.01068457, 0.00917388, 0.00549181, 0.0082801 , 0.00793977,\n",
       "        0.01336563, 0.00790949, 0.01415062, 0.00800999, 0.00858836,\n",
       "        0.01121784, 0.00813388, 0.03401999, 0.01060943, 0.00665132,\n",
       "        0.01582909, 0.00793977, 0.00892188, 0.05199769, 0.00770973,\n",
       "        0.00926499, 0.02226747, 0.00877724, 0.0064622 , 0.02748381,\n",
       "        0.00878635, 0.00813388, 0.11353484, 0.00945304, 0.00982649,\n",
       "        0.08799409, 0.008     , 0.00941063, 0.01979495, 0.0117303 ,\n",
       "        0.00808455, 0.00831865, 0.00954987, 0.00897998, 0.03931615,\n",
       "        0.01135958, 0.01049952, 0.08925111, 0.00769415, 0.01108873,\n",
       "        0.08366505, 0.0102645 , 0.00737564, 0.0009798 , 0.01224745,\n",
       "        0.00997196, 0.06206964, 0.00950789, 0.01354991, 0.0936235 ,\n",
       "        0.00938936, 0.00941063, 0.09445295, 0.01066958, 0.0095373 ,\n",
       "        0.06527051, 0.00701427, 0.00767854, 0.07592207, 0.00856505,\n",
       "        0.00736478, 0.05539097, 0.00722219, 0.0091433 , 0.03472751,\n",
       "        0.00877724, 0.00919565, 0.06913349, 0.00770973, 0.00874986,\n",
       "        0.06086904, 0.0059127 , 0.0091214 , 0.07433277, 0.00735935,\n",
       "        0.00936803, 0.06722678, 0.00752064, 0.00767854, 0.06636143,\n",
       "        0.00893532, 0.00906422, 0.02842956, 0.00958332, 0.01111036,\n",
       "        0.05332692, 0.00994786, 0.01067708, 0.05223026, 0.00949526,\n",
       "        0.00748331, 0.09409059, 0.00708237, 0.00928655, 0.05792271,\n",
       "        0.00677052, 0.00851822, 0.08469475, 0.00916515, 0.00910824,\n",
       "        0.0791237 , 0.01139474, 0.00865794, 0.0109252 , 0.00793977,\n",
       "        0.00852291, 0.01107429, 0.00786384, 0.00939361, 0.00906863,\n",
       "        0.00813388, 0.00854634, 0.00970361, 0.0079599 , 0.00798999,\n",
       "        0.0111463 , 0.00919565, 0.0094361 , 0.01010742, 0.00958332,\n",
       "        0.00861626, 0.00977753, 0.00954149, 0.01043839, 0.01006777,\n",
       "        0.01065082, 0.00994786, 0.00930806, 0.01205985, 0.01060943,\n",
       "        0.00873155, 0.00873155, 0.00995992, 0.00964572, 0.0090111 ,\n",
       "        0.009992  , 0.00997196, 0.00955824, 0.00910824, 0.00803492,\n",
       "        0.00964158, 0.01160345, 0.0095163 , 0.00846168, 0.00944458,\n",
       "        0.01060943, 0.00792465, 0.00935094, 0.00902441, 0.0095163 ,\n",
       "        0.00745654, 0.00700286, 0.00926499, 0.0063435 , 0.0072    ,\n",
       "        0.00344093, 0.01083328, 0.00608605, 0.00768375, 0.00957914,\n",
       "        0.00508331, 0.00886341, 0.00828493, 0.00908625, 0.00956243,\n",
       "        0.00628013, 0.00811419, 0.00665132, 0.00779744, 0.00601332,\n",
       "        0.01019019, 0.00909065, 0.00693974, 0.00789937, 0.01230285,\n",
       "        0.00997196, 0.00991161, 0.00889044, 0.0085557 , 0.00906863,\n",
       "        0.00851822, 0.0054626 , 0.00773563, 0.00805978, 0.0052    ,\n",
       "        0.01273421, 0.01065833, 0.00752596, 0.0048    , 0.0088227 ,\n",
       "        0.00444522, 0.0109654 , 0.00835225, 0.00760526, 0.00557136,\n",
       "        0.00536656, 0.0107963 , 0.00445421]),\n",
       " 'rank_test_precision_micro': array([151, 180, 171, 200, 199, 217, 205, 214, 219, 245, 235, 250,  59,\n",
       "        137, 175, 195, 189, 202, 229, 211, 207, 237, 241, 244, 182, 166,\n",
       "        158, 200, 191, 185, 215, 217, 245, 251, 229, 242, 173, 174, 167,\n",
       "        198, 192, 186, 208, 204, 203, 223, 211, 238, 151, 183, 140, 223,\n",
       "        210, 221, 239, 227, 233, 254, 239, 236,  38, 176, 171, 197, 188,\n",
       "        222, 219, 247, 225, 249, 247, 243, 180, 184, 163, 209, 193, 187,\n",
       "        226, 213, 215, 255, 253, 256, 169, 162, 179, 196, 194, 190, 228,\n",
       "        206, 232, 252, 231, 234,  21, 280,  62,  76, 282,  25,   8, 273,\n",
       "         52,  59, 283,  25,  45, 278, 140,  11, 285,  31,  62, 272,  38,\n",
       "         45, 275,   8,  31, 284,   8, 122, 277,   2,  76, 274,  38,  31,\n",
       "        281,  98,  38, 287,  66,   6, 276,  38,   4, 286,  12,  45, 288,\n",
       "         15,  51, 266,  38, 127, 267,  18,  75, 263, 103,  20, 265,  76,\n",
       "        163, 268,  84, 103, 269,  31,  38, 264,  31,  17, 257,  31,  62,\n",
       "        270,  52,  22, 271, 155,   6, 258, 127,  65, 260,  22,  19, 279,\n",
       "         45,  25, 262,  30,   1, 259, 137, 127, 261,  25, 158, 127, 113,\n",
       "        108, 108,  76,  87,  93,  31,  93, 146,  98,  52, 118, 127, 118,\n",
       "        146, 108,  87, 101, 155, 127, 127, 155,  69,  84,  16,  70, 122,\n",
       "        108, 161, 113,  68, 150,  87, 127, 127,  70,  22,  59,  70,  66,\n",
       "        140, 106, 140,  86, 118, 106, 140, 126,  76, 118, 113, 108,  45,\n",
       "        151,  52, 146, 113,  45,  76, 158,  93,  93, 122,  14, 177,  70,\n",
       "         52,  87, 168, 177,   5, 151,  76,  29, 140,  87, 127,  87, 113,\n",
       "        146, 169, 165, 101,  13,  76,  93,  74,  58, 100,  52, 122, 137,\n",
       "        103,   3]),\n",
       " 'split0_test_roc_auc_ovo': array([0.83795757, 0.84500098, 0.8411191 , 0.80804309, 0.82859304,\n",
       "        0.81954866, 0.81768776, 0.80884748, 0.79694972, 0.82000888,\n",
       "        0.80151193, 0.80797106, 0.84773831, 0.8506117 , 0.84506101,\n",
       "        0.8278887 , 0.82702828, 0.82941744, 0.81226514, 0.81602696,\n",
       "        0.8138499 , 0.81172087, 0.81890835, 0.80607014, 0.84548121,\n",
       "        0.84103906, 0.85508586, 0.82865707, 0.82604381, 0.83631678,\n",
       "        0.80662641, 0.80999604, 0.81538665, 0.81505849, 0.8074468 ,\n",
       "        0.81566678, 0.84770229, 0.84151529, 0.84928305, 0.83399966,\n",
       "        0.81691939, 0.83631678, 0.81583486, 0.82084529, 0.81862822,\n",
       "        0.8080631 , 0.82578368, 0.81284142, 0.83500814, 0.84325213,\n",
       "        0.83917416, 0.82194582, 0.81786785, 0.80876944, 0.82266217,\n",
       "        0.82451907, 0.80900156, 0.81553872, 0.814046  , 0.81153278,\n",
       "        0.84860272, 0.84364032, 0.84821454, 0.81758371, 0.83710916,\n",
       "        0.81681534, 0.81761573, 0.80976793, 0.81166885, 0.80581001,\n",
       "        0.80634227, 0.79874259, 0.83881799, 0.84704197, 0.84009861,\n",
       "        0.82695625, 0.83596861, 0.83026985, 0.82310638, 0.80032736,\n",
       "        0.82886917, 0.80491358, 0.79402831, 0.79716983, 0.84487692,\n",
       "        0.84264384, 0.84172339, 0.83065404, 0.83674098, 0.82615186,\n",
       "        0.81420207, 0.82305436, 0.8130095 , 0.81920449, 0.81195699,\n",
       "        0.82381473, 0.8508198 , 0.47605041, 0.85254863, 0.85139608,\n",
       "        0.58265801, 0.85026353, 0.85147611, 0.67366605, 0.85235654,\n",
       "        0.85158817, 0.2981403 , 0.85063171, 0.85225249, 0.37258833,\n",
       "        0.85250061, 0.85197235, 0.52787149, 0.85099988, 0.85039559,\n",
       "        0.78630457, 0.85096787, 0.85178026, 0.70022691, 0.85174424,\n",
       "        0.85048363, 0.58749635, 0.85093585, 0.85114796, 0.60783819,\n",
       "        0.85211642, 0.85159217, 0.20541142, 0.85004342, 0.8518603 ,\n",
       "        0.50134665, 0.85235654, 0.84910297, 0.43964479, 0.85166421,\n",
       "        0.85119998, 0.34020466, 0.8502115 , 0.85099588, 0.26014191,\n",
       "        0.85006343, 0.85121599, 0.78647665, 0.85202438, 0.84971926,\n",
       "        0.44674422, 0.84800243, 0.84757423, 0.74520068, 0.84947114,\n",
       "        0.85114796, 0.67604721, 0.85149612, 0.84699395, 0.59341921,\n",
       "        0.84993137, 0.84694192, 0.77241785, 0.85250861, 0.84777032,\n",
       "        0.49407513, 0.85296083, 0.84929106, 0.64617275, 0.84846666,\n",
       "        0.84907095, 0.57663909, 0.85151213, 0.84558126, 0.50438812,\n",
       "        0.84542118, 0.84563329, 0.5572257 , 0.847098  , 0.8510319 ,\n",
       "        0.66613841, 0.84914299, 0.85179627, 0.6551571 , 0.84810248,\n",
       "        0.84165536, 0.30575599, 0.85054767, 0.85002741, 0.75519351,\n",
       "        0.84494495, 0.85221647, 0.79616534, 0.84951116, 0.84934308,\n",
       "        0.80023531, 0.84991936, 0.85246859, 0.85257265, 0.85262867,\n",
       "        0.85201638, 0.85217645, 0.85215644, 0.85217245, 0.85242057,\n",
       "        0.85212843, 0.85183629, 0.85222848, 0.85210842, 0.85275273,\n",
       "        0.85192033, 0.8520604 , 0.85277274, 0.85205239, 0.85195234,\n",
       "        0.85218846, 0.85234854, 0.85188831, 0.85208441, 0.85190432,\n",
       "        0.8520684 , 0.85148412, 0.85230051, 0.85114395, 0.85222848,\n",
       "        0.8526807 , 0.85175225, 0.85221247, 0.85189632, 0.8520724 ,\n",
       "        0.85208441, 0.85214044, 0.85193634, 0.85228451, 0.85230452,\n",
       "        0.85232853, 0.85283677, 0.85279275, 0.85240857, 0.85212443,\n",
       "        0.85222047, 0.85239656, 0.85218046, 0.85241257, 0.85225649,\n",
       "        0.84901893, 0.84865475, 0.8504036 , 0.85038759, 0.85188431,\n",
       "        0.84930306, 0.84722206, 0.84625359, 0.84989135, 0.84881483,\n",
       "        0.84638165, 0.85016348, 0.8504196 , 0.85092785, 0.85021951,\n",
       "        0.84950716, 0.85118797, 0.84812649, 0.84670581, 0.85174024,\n",
       "        0.85096387, 0.84728208, 0.84547721, 0.8524886 , 0.85154815,\n",
       "        0.84841063, 0.84940311, 0.85226049, 0.84836261, 0.84762225,\n",
       "        0.84550522, 0.84702996, 0.84824655, 0.84846666, 0.85105191,\n",
       "        0.84446472, 0.85130803, 0.85051165, 0.85117997, 0.84967924,\n",
       "        0.85089183, 0.84905094, 0.85221647, 0.85116396, 0.85154815,\n",
       "        0.85173224, 0.85037158, 0.8508238 ]),\n",
       " 'split1_test_roc_auc_ovo': array([0.88012198, 0.8758199 , 0.87291049, 0.85555009, 0.87213011,\n",
       "        0.86001625, 0.85979214, 0.86328583, 0.85524594, 0.836769  ,\n",
       "        0.83846182, 0.84646169, 0.87216613, 0.87028522, 0.87731662,\n",
       "        0.8663233 , 0.86904862, 0.85610636, 0.84817051, 0.85793524,\n",
       "        0.84148328, 0.84422861, 0.85357713, 0.84216361, 0.87575987,\n",
       "        0.87999392, 0.87320663, 0.8566146 , 0.86229735, 0.86164904,\n",
       "        0.86365001, 0.86216929, 0.84558526, 0.832643  , 0.83936225,\n",
       "        0.84598546, 0.87713253, 0.8787133 , 0.87122968, 0.87318662,\n",
       "        0.86192117, 0.869845  , 0.85520592, 0.85613837, 0.85300886,\n",
       "        0.85563813, 0.85490978, 0.85312891, 0.87530765, 0.8756198 ,\n",
       "        0.87609603, 0.85907179, 0.85914383, 0.85056367, 0.84023067,\n",
       "        0.84151329, 0.84198752, 0.82503932, 0.83018181, 0.84424061,\n",
       "        0.87663229, 0.88066624, 0.87471536, 0.87341873, 0.8489709 ,\n",
       "        0.84182344, 0.84233569, 0.85213243, 0.84993937, 0.83530029,\n",
       "        0.84159933, 0.83704913, 0.8766523 , 0.87110562, 0.86840431,\n",
       "        0.86646337, 0.86235738, 0.86268154, 0.83948631, 0.84926705,\n",
       "        0.85689474, 0.84613753, 0.83087815, 0.82390277, 0.87795293,\n",
       "        0.87688441, 0.87505553, 0.86759992, 0.86685956, 0.85563813,\n",
       "        0.85587024, 0.84510903, 0.84971126, 0.82492726, 0.83072607,\n",
       "        0.84698994, 0.88340757, 0.40341925, 0.88286331, 0.88387179,\n",
       "        0.46533322, 0.88485627, 0.88235506, 0.46222772, 0.88424798,\n",
       "        0.88386779, 0.46233577, 0.88271523, 0.88123452, 0.61026337,\n",
       "        0.88288332, 0.88313144, 0.61947983, 0.8830594 , 0.88425598,\n",
       "        0.37928758, 0.88287931, 0.88415993, 0.69169078, 0.8838838 ,\n",
       "        0.88324749, 0.35368318, 0.8855286 , 0.88133457, 0.47506993,\n",
       "        0.8824351 , 0.88192285, 0.76447801, 0.88318346, 0.88399585,\n",
       "        0.2948267 , 0.88363568, 0.88206692, 0.48704373, 0.88258317,\n",
       "        0.88266321, 0.82072923, 0.88452011, 0.884292  , 0.42097575,\n",
       "        0.88365969, 0.88395583, 0.22907087, 0.88239908, 0.87735264,\n",
       "        0.7454528 , 0.88206692, 0.87862125, 0.71056391, 0.88361967,\n",
       "        0.88122251, 0.7462812 , 0.88228703, 0.88075829, 0.60731794,\n",
       "        0.88393582, 0.88193886, 0.48387019, 0.88177078, 0.88132656,\n",
       "        0.65767031, 0.87729661, 0.88233905, 0.77853281, 0.8787253 ,\n",
       "        0.8832675 , 0.80342486, 0.8826392 , 0.87786088, 0.28777929,\n",
       "        0.88071026, 0.87943365, 0.7592955 , 0.87859724, 0.88096639,\n",
       "        0.82260614, 0.88110245, 0.88247112, 0.66464569, 0.88157068,\n",
       "        0.87622409, 0.60688573, 0.88178678, 0.8828553 , 0.5650775 ,\n",
       "        0.88075429, 0.88546456, 0.83179459, 0.88108244, 0.88149865,\n",
       "        0.80219226, 0.88105443, 0.88596881, 0.8853285 , 0.88586876,\n",
       "        0.88556461, 0.88566866, 0.88581674, 0.88582874, 0.88561264,\n",
       "        0.88580473, 0.88564465, 0.88596881, 0.8859448 , 0.88558462,\n",
       "        0.88613289, 0.88518443, 0.88557662, 0.88568067, 0.88549258,\n",
       "        0.88549658, 0.88572469, 0.88551659, 0.88566866, 0.88550458,\n",
       "        0.88576871, 0.8853325 , 0.88583274, 0.88489229, 0.88544856,\n",
       "        0.88601283, 0.88605685, 0.88568067, 0.88552459, 0.88539253,\n",
       "        0.88583674, 0.88585275, 0.88589277, 0.8857367 , 0.88578472,\n",
       "        0.88554861, 0.88606085, 0.88584475, 0.88573269, 0.88596481,\n",
       "        0.88567267, 0.88596481, 0.88600883, 0.88579673, 0.88606085,\n",
       "        0.88311543, 0.88479624, 0.88530049, 0.88303539, 0.88299937,\n",
       "        0.8803621 , 0.88343158, 0.87917752, 0.87878533, 0.88043013,\n",
       "        0.87759676, 0.87925356, 0.88490029, 0.88232705, 0.88421996,\n",
       "        0.88282329, 0.88094238, 0.88087434, 0.87457129, 0.88171875,\n",
       "        0.87675235, 0.87992588, 0.87525162, 0.87900944, 0.88510039,\n",
       "        0.88425598, 0.88304339, 0.88364769, 0.88038611, 0.88206692,\n",
       "        0.88220299, 0.87880534, 0.8776808 , 0.8785212 , 0.87671633,\n",
       "        0.88449209, 0.88431601, 0.88359566, 0.88446808, 0.88478824,\n",
       "        0.88134257, 0.88226302, 0.88095438, 0.88209093, 0.88486427,\n",
       "        0.88248312, 0.88397985, 0.88192285]),\n",
       " 'split2_test_roc_auc_ovo': array([0.86334986, 0.86005227, 0.85875164, 0.85705882, 0.84023067,\n",
       "        0.84268386, 0.85029154, 0.83988651, 0.83598462, 0.81825604,\n",
       "        0.82365465, 0.83050997, 0.86377807, 0.86072059, 0.85968009,\n",
       "        0.8504196 , 0.84863474, 0.83618071, 0.83171455, 0.83853785,\n",
       "        0.83439185, 0.82057316, 0.82061318, 0.81379388, 0.8545496 ,\n",
       "        0.86438236, 0.85777917, 0.84146327, 0.84065888, 0.84777032,\n",
       "        0.84762625, 0.83392762, 0.82799275, 0.8154947 , 0.81506649,\n",
       "        0.81142073, 0.86335787, 0.86108877, 0.86035641, 0.85739098,\n",
       "        0.85436551, 0.85278475, 0.84139524, 0.84638165, 0.84245575,\n",
       "        0.83501615, 0.84112711, 0.82841295, 0.86381809, 0.85522193,\n",
       "        0.86126885, 0.83815767, 0.84585739, 0.85486576, 0.83746533,\n",
       "        0.80603612, 0.80758487, 0.82361263, 0.81603496, 0.81503848,\n",
       "        0.85997623, 0.86420628, 0.85343306, 0.84245575, 0.84919901,\n",
       "        0.84397648, 0.8384298 , 0.83592859, 0.8276926 , 0.83095418,\n",
       "        0.82083328, 0.81848815, 0.86391413, 0.85467366, 0.86639133,\n",
       "        0.82988166, 0.85762709, 0.84456477, 0.81441418, 0.82828889,\n",
       "        0.81851616, 0.82601979, 0.81563877, 0.81698342, 0.86366201,\n",
       "        0.85977613, 0.86002025, 0.85323697, 0.84547721, 0.8469099 ,\n",
       "        0.82527944, 0.8402987 , 0.83561644, 0.83363148, 0.8216977 ,\n",
       "        0.83005775, 0.86877249, 0.59080595, 0.86349393, 0.86771998,\n",
       "        0.66196439, 0.86462648, 0.86413024, 0.49479148, 0.86598714,\n",
       "        0.86617923, 0.5551647 , 0.86556693, 0.86710768, 0.58745633,\n",
       "        0.8661232 , 0.86803613, 0.71491802, 0.86615522, 0.86539485,\n",
       "        0.7126009 , 0.86698762, 0.86544688, 0.58639582, 0.86538285,\n",
       "        0.86459046, 0.53329411, 0.86660344, 0.86664346, 0.64798162,\n",
       "        0.86786805, 0.86419427, 0.22366826, 0.86385811, 0.86462248,\n",
       "        0.67817224, 0.86606317, 0.86643135, 0.49249837, 0.86421828,\n",
       "        0.86493063, 0.6700363 , 0.86616322, 0.86433834, 0.67230139,\n",
       "        0.86548289, 0.86657942, 0.65835865, 0.86579905, 0.86585507,\n",
       "        0.72483082, 0.86458246, 0.86370603, 0.83101421, 0.86560295,\n",
       "        0.86082064, 0.54512784, 0.86501067, 0.86535083, 0.73944589,\n",
       "        0.86205724, 0.86657542, 0.69105047, 0.86602316, 0.86221731,\n",
       "        0.6621965 , 0.86587108, 0.86664346, 0.64985853, 0.8617771 ,\n",
       "        0.86708767, 0.82853301, 0.86545888, 0.86251746, 0.62880834,\n",
       "        0.8679801 , 0.86290565, 0.54233849, 0.8632178 , 0.8628176 ,\n",
       "        0.68480344, 0.86802813, 0.86429832, 0.69696133, 0.86634331,\n",
       "        0.86133689, 0.72445864, 0.86065256, 0.86788005, 0.74019425,\n",
       "        0.8673518 , 0.86721173, 0.50740158, 0.86686756, 0.86683955,\n",
       "        0.61482958, 0.86545088, 0.87096955, 0.87022519, 0.87029722,\n",
       "        0.87048932, 0.87051733, 0.87085349, 0.87072943, 0.87096155,\n",
       "        0.87099356, 0.87080547, 0.87099356, 0.87089751, 0.87050933,\n",
       "        0.86995306, 0.87033724, 0.87095754, 0.8704573 , 0.8706654 ,\n",
       "        0.87064539, 0.87054935, 0.87058937, 0.87068141, 0.87049732,\n",
       "        0.87091752, 0.87059337, 0.8708775 , 0.87099756, 0.87054134,\n",
       "        0.87095354, 0.8706694 , 0.87082148, 0.8704453 , 0.87063739,\n",
       "        0.87078946, 0.87092553, 0.87088551, 0.8704453 , 0.87068541,\n",
       "        0.87021719, 0.87060537, 0.87043729, 0.87108561, 0.8706694 ,\n",
       "        0.87068941, 0.8704453 , 0.87103358, 0.87061738, 0.87072543,\n",
       "        0.8673598 , 0.86767996, 0.86954486, 0.86427031, 0.86495864,\n",
       "        0.86658743, 0.86159701, 0.86167705, 0.85975612, 0.86028438,\n",
       "        0.85760308, 0.85820337, 0.86476655, 0.86621125, 0.86623126,\n",
       "        0.86377407, 0.86206924, 0.86980098, 0.85986017, 0.8611528 ,\n",
       "        0.86367002, 0.86521476, 0.86188115, 0.86531481, 0.86796409,\n",
       "        0.86347792, 0.86340589, 0.86462248, 0.86406221, 0.86493463,\n",
       "        0.86229335, 0.86078062, 0.86110077, 0.86243342, 0.86102874,\n",
       "        0.85877965, 0.86799611, 0.86837229, 0.86667147, 0.86619124,\n",
       "        0.86956487, 0.8659111 , 0.86392614, 0.86759191, 0.86257749,\n",
       "        0.86215728, 0.86479856, 0.86436235]),\n",
       " 'split3_test_roc_auc_ovo': array([0.85945598, 0.86293366, 0.85984416, 0.83980246, 0.84253979,\n",
       "        0.84187147, 0.82947347, 0.83755338, 0.83488809, 0.81410203,\n",
       "        0.82223396, 0.8136338 , 0.86174108, 0.85507786, 0.86491062,\n",
       "        0.8495832 , 0.84479288, 0.8497913 , 0.81909644, 0.83490009,\n",
       "        0.84281992, 0.81431013, 0.81834008, 0.83733327, 0.85796726,\n",
       "        0.86200921, 0.85839146, 0.83477203, 0.84560527, 0.8549498 ,\n",
       "        0.82917332, 0.82697626, 0.82940543, 0.81233717, 0.83275105,\n",
       "        0.83030187, 0.85987618, 0.86163303, 0.85798727, 0.83833776,\n",
       "        0.84541718, 0.84974728, 0.83499614, 0.82154563, 0.8359486 ,\n",
       "        0.82846098, 0.82855702, 0.84139123, 0.85225249, 0.85553008,\n",
       "        0.86055651, 0.83895405, 0.8289252 , 0.82751652, 0.80467546,\n",
       "        0.82378271, 0.8301738 , 0.8030927 , 0.8239668 , 0.8239708 ,\n",
       "        0.8588917 , 0.84911497, 0.85547805, 0.84067088, 0.85513388,\n",
       "        0.83950232, 0.83662893, 0.81825203, 0.82493127, 0.79839442,\n",
       "        0.81422609, 0.81728357, 0.85134005, 0.85561412, 0.86103674,\n",
       "        0.83205872, 0.85508986, 0.81843212, 0.82673614, 0.82650403,\n",
       "        0.81648318, 0.82006091, 0.79575314, 0.80155996, 0.86595512,\n",
       "        0.85502983, 0.85644652, 0.84293198, 0.84139524, 0.85378523,\n",
       "        0.82119746, 0.83215476, 0.82249009, 0.8074468 , 0.82688021,\n",
       "        0.8115768 , 0.87094554, 0.63334254, 0.86914066, 0.87099356,\n",
       "        0.23190824, 0.86976497, 0.8688005 , 0.74309166, 0.86863642,\n",
       "        0.86926472, 0.70883508, 0.86898059, 0.8673638 , 0.80964387,\n",
       "        0.8683963 , 0.87023319, 0.57890019, 0.8679801 , 0.86846834,\n",
       "        0.46423669, 0.87093753, 0.86888454, 0.33448189, 0.87048531,\n",
       "        0.87064939, 0.66595032, 0.86934876, 0.86969293, 0.50788181,\n",
       "        0.87028922, 0.86954086, 0.7431717 , 0.86719572, 0.8704453 ,\n",
       "        0.56412504, 0.87006911, 0.87146179, 0.31691339, 0.86738381,\n",
       "        0.87036526, 0.39735232, 0.86828025, 0.86992504, 0.67635536,\n",
       "        0.86882851, 0.86866043, 0.83753337, 0.87054935, 0.86465049,\n",
       "        0.66174428, 0.86473853, 0.86778801, 0.77625571, 0.8646665 ,\n",
       "        0.86545088, 0.7537248 , 0.86679153, 0.86509471, 0.70907519,\n",
       "        0.86819621, 0.86473453, 0.50635307, 0.86609119, 0.86543887,\n",
       "        0.70745441, 0.86671549, 0.86304171, 0.74943873, 0.86543487,\n",
       "        0.86393815, 0.70180367, 0.86247744, 0.86593511, 0.75499742,\n",
       "        0.86675551, 0.86998507, 0.56334866, 0.86401418, 0.86701564,\n",
       "        0.79736993, 0.86659143, 0.86361799, 0.79060665, 0.86731978,\n",
       "        0.86856439, 0.60087883, 0.86575903, 0.86671149, 0.70406877,\n",
       "        0.86699162, 0.86862441, 0.71944821, 0.86642335, 0.87056936,\n",
       "        0.57808379, 0.86468651, 0.86932075, 0.86904862, 0.86819621,\n",
       "        0.86896058, 0.86861241, 0.86930474, 0.86905262, 0.8690166 ,\n",
       "        0.86834428, 0.86903261, 0.86854838, 0.86896858, 0.86926472,\n",
       "        0.86868845, 0.86891255, 0.8688045 , 0.86876048, 0.86906463,\n",
       "        0.86892456, 0.86842832, 0.86808415, 0.86867644, 0.86894457,\n",
       "        0.86823222, 0.86852036, 0.86910865, 0.86899659, 0.86918469,\n",
       "        0.86896458, 0.86897658, 0.86853637, 0.86869645, 0.86882851,\n",
       "        0.86850436, 0.8690126 , 0.86936877, 0.86937278, 0.86924872,\n",
       "        0.86920069, 0.86865643, 0.86854838, 0.86908064, 0.86895257,\n",
       "        0.86928473, 0.86893657, 0.86911665, 0.86896458, 0.86906463,\n",
       "        0.86719172, 0.86584707, 0.86762793, 0.86634731, 0.86700363,\n",
       "        0.86445039, 0.86346192, 0.86148496, 0.8694288 , 0.86367802,\n",
       "        0.86307373, 0.86194118, 0.86866443, 0.8685884 , 0.86628328,\n",
       "        0.86670348, 0.86747986, 0.86257749, 0.86711568, 0.8679801 ,\n",
       "        0.86675151, 0.86587508, 0.86223732, 0.86265753, 0.8679721 ,\n",
       "        0.86583907, 0.86953686, 0.86950884, 0.86679153, 0.86533482,\n",
       "        0.86471452, 0.86455444, 0.86587108, 0.86407021, 0.86424229,\n",
       "        0.85808731, 0.86924071, 0.86699162, 0.86547489, 0.86764394,\n",
       "        0.86583907, 0.8692087 , 0.86779201, 0.86883652, 0.86597113,\n",
       "        0.86341789, 0.86679553, 0.86848034]),\n",
       " 'split4_test_roc_auc_ovo': array([0.85956003, 0.85834344, 0.86358598, 0.83495212, 0.84671781,\n",
       "        0.84638565, 0.83303519, 0.81855218, 0.83536031, 0.82755654,\n",
       "        0.82528344, 0.82533546, 0.85952801, 0.85825139, 0.86027237,\n",
       "        0.84145126, 0.84680986, 0.84277991, 0.84404052, 0.82965355,\n",
       "        0.82496728, 0.8252154 , 0.82750051, 0.82020898, 0.85710284,\n",
       "        0.8586836 , 0.86443839, 0.85468967, 0.85006343, 0.84221163,\n",
       "        0.82952149, 0.82938942, 0.84126317, 0.84043077, 0.83005775,\n",
       "        0.82601979, 0.86497065, 0.85972811, 0.85915583, 0.83395163,\n",
       "        0.84149929, 0.84253979, 0.85215644, 0.84072291, 0.83111426,\n",
       "        0.83423977, 0.82698826, 0.83211074, 0.85799127, 0.85425746,\n",
       "        0.85570216, 0.84560127, 0.83149444, 0.85246459, 0.82886317,\n",
       "        0.83432781, 0.82554757, 0.82380672, 0.81176089, 0.80697858,\n",
       "        0.8648786 , 0.86501867, 0.8580513 , 0.84945113, 0.84113111,\n",
       "        0.84761425, 0.82265817, 0.8188003 , 0.8291293 , 0.82391077,\n",
       "        0.82554356, 0.83410771, 0.86314976, 0.86307773, 0.85873163,\n",
       "        0.85214844, 0.83774547, 0.84017865, 0.83071006, 0.83083412,\n",
       "        0.83465998, 0.81531461, 0.81943661, 0.82296231, 0.85829141,\n",
       "        0.85589425, 0.85848351, 0.84137523, 0.85473369, 0.84724607,\n",
       "        0.82991768, 0.82212191, 0.83459995, 0.80851532, 0.83979046,\n",
       "        0.83287511, 0.86760392, 0.40068593, 0.86944481, 0.86697562,\n",
       "        0.1743884 , 0.86866443, 0.86935677, 0.54862153, 0.87008112,\n",
       "        0.86960489, 0.82573566, 0.86947683, 0.86870445, 0.58797258,\n",
       "        0.86886053, 0.86997707, 0.20586764, 0.86910064, 0.87114964,\n",
       "        0.79574914, 0.86902461, 0.86849635, 0.72881275, 0.86954086,\n",
       "        0.87021318, 0.2485523 , 0.86847634, 0.86937278, 0.28434963,\n",
       "        0.86874447, 0.86939279, 0.64677704, 0.86946482, 0.87094554,\n",
       "        0.43534671, 0.87016116, 0.8687965 , 0.50919045, 0.86814418,\n",
       "        0.87034125, 0.72297392, 0.86861241, 0.86939679, 0.23660652,\n",
       "        0.86798811, 0.86929274, 0.27554536, 0.87152582, 0.86794809,\n",
       "        0.55905458, 0.86495464, 0.86571501, 0.539249  , 0.86599114,\n",
       "        0.86414224, 0.78297496, 0.8675599 , 0.87084549, 0.62759576,\n",
       "        0.86719572, 0.8652908 , 0.61141192, 0.86485059, 0.86535083,\n",
       "        0.47369327, 0.86481457, 0.86782403, 0.53292594, 0.86706366,\n",
       "        0.86875248, 0.69861413, 0.86640734, 0.86577504, 0.67693964,\n",
       "        0.86680753, 0.869845  , 0.5919345 , 0.87074144, 0.8692127 ,\n",
       "        0.74216721, 0.86832027, 0.86907663, 0.80972391, 0.86916868,\n",
       "        0.86559495, 0.37555777, 0.86678753, 0.86954486, 0.80643431,\n",
       "        0.86860841, 0.86742784, 0.66001145, 0.86846033, 0.86676752,\n",
       "        0.6948443 , 0.86958888, 0.87060137, 0.8708575 , 0.87012914,\n",
       "        0.87064939, 0.87063339, 0.87057736, 0.87074544, 0.87073744,\n",
       "        0.87022519, 0.87038527, 0.87052133, 0.8704533 , 0.87062138,\n",
       "        0.87080547, 0.86998907, 0.87092153, 0.87056936, 0.87076145,\n",
       "        0.87047731, 0.87080147, 0.87059337, 0.87075344, 0.87072143,\n",
       "        0.87103758, 0.87096555, 0.87050132, 0.87052934, 0.87081748,\n",
       "        0.87048531, 0.8706694 , 0.87062138, 0.8708655 , 0.87064139,\n",
       "        0.87051733, 0.87118966, 0.87059337, 0.87088551, 0.87064539,\n",
       "        0.87043729, 0.87071343, 0.87063339, 0.87056936, 0.87082548,\n",
       "        0.87091752, 0.87017316, 0.87097755, 0.87082948, 0.8706534 ,\n",
       "        0.86853637, 0.87032924, 0.8696369 , 0.86792808, 0.86854438,\n",
       "        0.86928473, 0.86966892, 0.86738782, 0.86711168, 0.86255748,\n",
       "        0.86455044, 0.86557494, 0.86952885, 0.86979698, 0.86959288,\n",
       "        0.86814418, 0.86815219, 0.87067741, 0.86786805, 0.86472653,\n",
       "        0.87001309, 0.86352194, 0.86597113, 0.86202122, 0.86931275,\n",
       "        0.8690086 , 0.86979298, 0.86794408, 0.87014115, 0.86940079,\n",
       "        0.86615122, 0.86998107, 0.86862441, 0.86559495, 0.86645937,\n",
       "        0.86553492, 0.87139375, 0.8663273 , 0.86815219, 0.8706534 ,\n",
       "        0.87133773, 0.87077746, 0.86977697, 0.86888854, 0.86952485,\n",
       "        0.86581105, 0.86936477, 0.86954486]),\n",
       " 'mean_test_roc_auc_ovo': array([0.86008908, 0.86043005, 0.85924227, 0.83908132, 0.84604228,\n",
       "        0.84210118, 0.83805602, 0.83362507, 0.83168574, 0.8233385 ,\n",
       "        0.82222916, 0.82478239, 0.86099032, 0.85898935, 0.86144814,\n",
       "        0.84713321, 0.84726288, 0.84285514, 0.83105743, 0.83541074,\n",
       "        0.83150245, 0.82320963, 0.82778785, 0.82391397, 0.85817216,\n",
       "        0.86122163, 0.8617803 , 0.84323933, 0.84493375, 0.84857951,\n",
       "        0.83531949, 0.83249173, 0.83192665, 0.82319283, 0.82493687,\n",
       "        0.82587893, 0.8626079 , 0.8605357 , 0.85960245, 0.84737333,\n",
       "        0.84402451, 0.85024672, 0.83991772, 0.83712677, 0.83623114,\n",
       "        0.83228363, 0.83547317, 0.83357705, 0.85687553, 0.85677628,\n",
       "        0.85855954, 0.84074612, 0.83665774, 0.838836  , 0.82677936,\n",
       "        0.8260358 , 0.82285906, 0.81821802, 0.81919809, 0.82035225,\n",
       "        0.86179631, 0.8605293 , 0.85797846, 0.84471604, 0.84630881,\n",
       "        0.83794637, 0.83153366, 0.82697626, 0.82867228, 0.81887393,\n",
       "        0.82170891, 0.82113423, 0.85877485, 0.85830262, 0.85893252,\n",
       "        0.84150169, 0.84975768, 0.83922539, 0.82689062, 0.82704429,\n",
       "        0.83108464, 0.82248928, 0.811147  , 0.81251566, 0.86214768,\n",
       "        0.85804569, 0.85834584, 0.84715963, 0.84904134, 0.84594624,\n",
       "        0.82929338, 0.83254775, 0.83108545, 0.81874507, 0.82621029,\n",
       "        0.82906287, 0.86830986, 0.50086082, 0.86749827, 0.8681914 ,\n",
       "        0.42325045, 0.86763514, 0.86722374, 0.58447969, 0.86826184,\n",
       "        0.86810096, 0.5700423 , 0.86747426, 0.86733259, 0.5935849 ,\n",
       "        0.86775279, 0.86867004, 0.52940743, 0.86745905, 0.86793288,\n",
       "        0.62763578, 0.86815939, 0.86775359, 0.60832163, 0.86820741,\n",
       "        0.86783683, 0.47779525, 0.8681786 , 0.86763834, 0.50462424,\n",
       "        0.86829065, 0.86732859, 0.51670128, 0.86674911, 0.86837389,\n",
       "        0.49476347, 0.86845713, 0.8675719 , 0.44905814, 0.86679873,\n",
       "        0.86790006, 0.59025929, 0.8675575 , 0.86778961, 0.45327619,\n",
       "        0.86720453, 0.86794088, 0.55739698, 0.86845953, 0.86510511,\n",
       "        0.62756534, 0.864869  , 0.86468091, 0.7204567 , 0.86587028,\n",
       "        0.86455685, 0.7008312 , 0.86662905, 0.86580865, 0.6553708 ,\n",
       "        0.86626327, 0.86509631, 0.6130207 , 0.86624886, 0.86442078,\n",
       "        0.59901792, 0.86553172, 0.86582786, 0.67138575, 0.86429352,\n",
       "        0.86642335, 0.72180295, 0.865699  , 0.86353395, 0.57058256,\n",
       "        0.86553492, 0.86556053, 0.60282857, 0.86473373, 0.86620885,\n",
       "        0.74261703, 0.86663705, 0.86625207, 0.72341893, 0.86650099,\n",
       "        0.86267513, 0.52270739, 0.86510671, 0.86740382, 0.71419367,\n",
       "        0.86573021, 0.868189  , 0.70296423, 0.86646897, 0.86700363,\n",
       "        0.69803705, 0.86614001, 0.86986582, 0.86960649, 0.869424  ,\n",
       "        0.86953606, 0.86952165, 0.86974176, 0.86970574, 0.86974976,\n",
       "        0.86949924, 0.86954086, 0.86965211, 0.86967452, 0.86974656,\n",
       "        0.86950004, 0.86929674, 0.86980659, 0.86950404, 0.86958728,\n",
       "        0.86954646, 0.86957047, 0.86933436, 0.86957287, 0.86951444,\n",
       "        0.86960489, 0.86937918, 0.86972415, 0.86931195, 0.86964411,\n",
       "        0.86981939, 0.8696249 , 0.86957447, 0.86948563, 0.86951444,\n",
       "        0.86954646, 0.86982419, 0.86973535, 0.86974496, 0.86973375,\n",
       "        0.86954646, 0.86977457, 0.86965131, 0.86977537, 0.86970734,\n",
       "        0.86975696, 0.86958328, 0.86986341, 0.86972415, 0.86975216,\n",
       "        0.86704445, 0.86746145, 0.86850276, 0.86639373, 0.86707807,\n",
       "        0.86599754, 0.8650763 , 0.86319619, 0.86499466, 0.86315297,\n",
       "        0.86184113, 0.86302731, 0.86765595, 0.8675703 , 0.86730938,\n",
       "        0.86619044, 0.86596633, 0.86641134, 0.8632242 , 0.86546368,\n",
       "        0.86563016, 0.86436395, 0.86216369, 0.86429832, 0.8683795 ,\n",
       "        0.86619844, 0.86703645, 0.86759672, 0.86594872, 0.86587188,\n",
       "        0.86417346, 0.86423029, 0.86430472, 0.86381729, 0.86389973,\n",
       "        0.86227174, 0.86885092, 0.86715971, 0.86718932, 0.86779121,\n",
       "        0.86779521, 0.86744224, 0.8669332 , 0.86771437, 0.86689718,\n",
       "        0.86512032, 0.86706206, 0.86702684]),\n",
       " 'std_test_roc_auc_ovo': array([0.01343454, 0.00985564, 0.01034157, 0.01775023, 0.01436349,\n",
       "        0.01302547, 0.0150725 , 0.0188442 , 0.01899427, 0.008005  ,\n",
       "        0.01186645, 0.01349085, 0.00788655, 0.00658215, 0.01036189,\n",
       "        0.01255148, 0.01335848, 0.0094745 , 0.01384244, 0.0136108 ,\n",
       "        0.0108662 , 0.01152358, 0.01330227, 0.01376715, 0.00984577,\n",
       "        0.01246424, 0.00647974, 0.01093161, 0.01186055, 0.00897744,\n",
       "        0.01922894, 0.01690515, 0.01066895, 0.01122253, 0.01182119,\n",
       "        0.01214234, 0.00945251, 0.01178442, 0.00700084, 0.0155391 ,\n",
       "        0.01529857, 0.01135098, 0.01407057, 0.01391319, 0.01145742,\n",
       "        0.01521823, 0.01116657, 0.01343221, 0.01333223, 0.01047181,\n",
       "        0.01185592, 0.01202302, 0.01435109, 0.0179575 , 0.01268861,\n",
       "        0.0119639 , 0.01305345, 0.00828033, 0.00685921, 0.01318046,\n",
       "        0.00911512, 0.01307437, 0.0089708 , 0.01791034, 0.00639944,\n",
       "        0.01089774, 0.00961962, 0.01517934, 0.01230097, 0.01436108,\n",
       "        0.0118594 , 0.01374635, 0.01279616, 0.00817212, 0.01004523,\n",
       "        0.01531146, 0.01080318, 0.01477951, 0.00828548, 0.01564512,\n",
       "        0.01452599, 0.01369659, 0.01420023, 0.01108406, 0.01076797,\n",
       "        0.01103479, 0.01060141, 0.01247803, 0.01069913, 0.01048636,\n",
       "        0.01425568, 0.00913061, 0.01249338, 0.00992305, 0.00926048,\n",
       "        0.01157764, 0.01040675, 0.09569426, 0.00981934, 0.01036832,\n",
       "        0.19115974, 0.01107238, 0.00993529, 0.1071329 , 0.01017015,\n",
       "        0.01028177, 0.18460513, 0.0102555 , 0.00919355, 0.13846891,\n",
       "        0.00965255, 0.0099247 , 0.173007  , 0.01018765, 0.01087268,\n",
       "        0.17264591, 0.01021826, 0.01030987, 0.14517854, 0.01031242,\n",
       "        0.01060866, 0.15393045, 0.01097519, 0.00967201, 0.12697199,\n",
       "        0.00965453, 0.00979394, 0.24994863, 0.01063109, 0.01040886,\n",
       "        0.12806108, 0.0100085 , 0.01066901, 0.06999529, 0.00986928,\n",
       "        0.01017543, 0.18806082, 0.01088295, 0.01070899, 0.191325  ,\n",
       "        0.01068461, 0.01039615, 0.25627544, 0.00985163, 0.00889469,\n",
       "        0.11128003, 0.01077363, 0.00998645, 0.09887655, 0.01082135,\n",
       "        0.00972084, 0.08540353, 0.00977435, 0.01098703, 0.0580884 ,\n",
       "        0.01096674, 0.01109603, 0.10913892, 0.00929253, 0.01067521,\n",
       "        0.09582209, 0.0077262 , 0.0105688 , 0.0869977 , 0.0097373 ,\n",
       "        0.01092587, 0.08951389, 0.00999175, 0.01038635, 0.16313817,\n",
       "        0.0113485 , 0.01126535, 0.07987229, 0.01041273, 0.00968813,\n",
       "        0.06097168, 0.01019863, 0.00990718, 0.06446195, 0.01070899,\n",
       "        0.01157933, 0.15660221, 0.01013194, 0.01044967, 0.08149049,\n",
       "        0.01157803, 0.01053841, 0.11458195, 0.01005234, 0.01034212,\n",
       "        0.09232037, 0.01000032, 0.01062098, 0.01038838, 0.01053831,\n",
       "        0.01064268, 0.01062672, 0.01067496, 0.01067636, 0.01053483,\n",
       "        0.01069306, 0.01072711, 0.01071066, 0.0107321 , 0.01040398,\n",
       "        0.01084681, 0.01049971, 0.01041554, 0.01066621, 0.01064691,\n",
       "        0.01056567, 0.01059558, 0.01068599, 0.01066159, 0.01066348,\n",
       "        0.01071874, 0.01076541, 0.01063375, 0.01074473, 0.01054019,\n",
       "        0.01056809, 0.01088157, 0.01062617, 0.01067783, 0.01057677,\n",
       "        0.01071327, 0.01070856, 0.01077089, 0.01060691, 0.01061493,\n",
       "        0.0105251 , 0.01053522, 0.01048081, 0.01057423, 0.01073508,\n",
       "        0.01061393, 0.01063247, 0.01073806, 0.01058882, 0.01071731,\n",
       "        0.01082167, 0.01153313, 0.01107141, 0.01039237, 0.00990945,\n",
       "        0.00998592, 0.01176005, 0.0106326 , 0.00969411, 0.01012859,\n",
       "        0.01013959, 0.00958864, 0.01102118, 0.01002571, 0.01082092,\n",
       "        0.01062923, 0.00964301, 0.01084345, 0.00948303, 0.00977773,\n",
       "        0.00852189, 0.01036996, 0.00963654, 0.00854163, 0.0106216 ,\n",
       "        0.0114709 , 0.01090127, 0.01005442, 0.01038802, 0.01103584,\n",
       "        0.0116731 , 0.01052008, 0.00967639, 0.00956019, 0.00829751,\n",
       "        0.0130475 , 0.01052873, 0.01048322, 0.01057345, 0.01120461,\n",
       "        0.00988699, 0.01071899, 0.00928485, 0.00983112, 0.01081509,\n",
       "        0.00992837, 0.01072642, 0.00999982]),\n",
       " 'rank_test_roc_auc_ovo': array([171, 170, 173, 206, 194, 201, 208, 216, 222, 242, 247, 240, 167,\n",
       "        174, 165, 192, 190, 200, 227, 214, 224, 243, 231, 241, 180, 166,\n",
       "        164, 199, 196, 188, 215, 219, 221, 244, 239, 238, 158, 168, 172,\n",
       "        189, 198, 185, 204, 210, 212, 220, 213, 217, 183, 184, 177, 203,\n",
       "        211, 207, 235, 237, 245, 254, 251, 250, 163, 169, 182, 197, 193,\n",
       "        209, 223, 233, 230, 252, 248, 249, 176, 179, 175, 202, 186, 205,\n",
       "        234, 232, 226, 246, 256, 255, 161, 181, 178, 191, 187, 195, 228,\n",
       "        218, 225, 253, 236, 229,  56, 283,  82,  60, 288,  77,  91, 275,\n",
       "         58,  64, 277,  83,  88, 273,  73,  50, 279,  85,  66, 267,  63,\n",
       "         72, 270,  59,  68, 285,  62,  76, 282,  57,  89, 281, 104,  55,\n",
       "        284,  53,  79, 287, 103,  67, 274,  81,  71, 286,  92,  65, 278,\n",
       "         52, 135, 268, 139, 141, 260, 123, 142, 263, 106, 125, 266, 112,\n",
       "        136, 269, 114, 143, 272, 131, 124, 265, 147, 109, 259, 127, 152,\n",
       "        276, 130, 129, 271, 140, 115, 257, 105, 113, 258, 107, 157, 280,\n",
       "        134,  87, 261, 126,  61, 262, 108, 100, 264, 118,   1,  25,  44,\n",
       "         36,  37,  13,  19,  10,  42,  35,  21,  20,  11,  41,  48,   5,\n",
       "         40,  27,  32,  31,  46,  30,  38,  26,  45,  16,  47,  23,   4,\n",
       "         24,  29,  43,  38,  32,   3,  14,  12,  15,  32,   7,  22,   6,\n",
       "         18,   8,  28,   2,  16,   9,  97,  84,  51, 111,  95, 119, 137,\n",
       "        154, 138, 155, 162, 156,  75,  80,  90, 117, 120, 110, 153, 132,\n",
       "        128, 144, 160, 146,  54, 116,  98,  78, 121, 122, 149, 148, 145,\n",
       "        151, 150, 159,  49,  94,  93,  70,  69,  86, 101,  74, 102, 133,\n",
       "         96,  99]),\n",
       " 'split0_test_jaccard': array([0.61708861, 0.62041467, 0.63826367, 0.60186625, 0.60031596,\n",
       "        0.59587956, 0.59394904, 0.58990536, 0.59119497, 0.58832808,\n",
       "        0.58059468, 0.57298137, 0.63355049, 0.64320786, 0.64696486,\n",
       "        0.5968254 , 0.59615385, 0.59141494, 0.59206349, 0.58528951,\n",
       "        0.59148265, 0.58712716, 0.58795563, 0.57007874, 0.62261146,\n",
       "        0.61417323, 0.61865794, 0.6096    , 0.60063391, 0.60350318,\n",
       "        0.58870968, 0.57324841, 0.57680251, 0.61305732, 0.58346334,\n",
       "        0.58505564, 0.62236629, 0.61783439, 0.64      , 0.60126582,\n",
       "        0.59873618, 0.61389338, 0.59340659, 0.59550562, 0.60443038,\n",
       "        0.5971564 , 0.60126582, 0.5640625 , 0.61685215, 0.61736334,\n",
       "        0.62258065, 0.58346582, 0.59633028, 0.58009331, 0.6073132 ,\n",
       "        0.58832808, 0.57476636, 0.60658307, 0.59047619, 0.58757962,\n",
       "        0.63870968, 0.61477987, 0.61850649, 0.58860759, 0.61075949,\n",
       "        0.6031746 , 0.5952381 , 0.58832565, 0.57007874, 0.56965944,\n",
       "        0.56220472, 0.57538462, 0.62200957, 0.62888199, 0.61987382,\n",
       "        0.59148265, 0.59467919, 0.62480127, 0.59906396, 0.56850394,\n",
       "        0.61867089, 0.57053292, 0.58018868, 0.58411215, 0.62243286,\n",
       "        0.6263911 , 0.62380952, 0.60828025, 0.62735849, 0.60443038,\n",
       "        0.59206349, 0.6028481 , 0.58722741, 0.60472441, 0.592     ,\n",
       "        0.60446571, 0.62783172, 0.51151151, 0.61774194, 0.62824675,\n",
       "        0.        , 0.62903226, 0.62903226, 0.43113772, 0.62439807,\n",
       "        0.62379421, 0.511     , 0.62700965, 0.63166397, 0.511     ,\n",
       "        0.62096774, 0.62681745, 0.51202405, 0.62760835, 0.6224    ,\n",
       "        0.51202405, 0.62258065, 0.62419355, 0.51905252, 0.62640902,\n",
       "        0.63225806, 0.06179775, 0.62439807, 0.6221865 , 0.        ,\n",
       "        0.62640902, 0.6187399 , 0.        , 0.62681745, 0.62197092,\n",
       "        0.511     , 0.61835749, 0.62297735, 0.07747748, 0.61736334,\n",
       "        0.62540193, 0.00380952, 0.62760835, 0.62358643, 0.511     ,\n",
       "        0.62520194, 0.6221865 , 0.13953488, 0.62019231, 0.62035541,\n",
       "        0.50701403, 0.62820513, 0.63141026, 0.19553073, 0.63402889,\n",
       "        0.63166397, 0.40881459, 0.62339744, 0.62113821, 0.31610044,\n",
       "        0.63252033, 0.60876623, 0.48105437, 0.63461538, 0.62480127,\n",
       "        0.26666667, 0.63029316, 0.62113821, 0.5       , 0.62903226,\n",
       "        0.62358643, 0.34061135, 0.63533225, 0.6191248 , 0.14705882,\n",
       "        0.62875197, 0.6215781 , 0.33427762, 0.62760835, 0.632     ,\n",
       "        0.42313546, 0.61637239, 0.62679426, 0.49747475, 0.62236629,\n",
       "        0.63004847, 0.323043  , 0.63593005, 0.62318841, 0.54949785,\n",
       "        0.63225806, 0.62722853, 0.56181534, 0.61996779, 0.61612903,\n",
       "        0.56918239, 0.62096774, 0.62101911, 0.61698718, 0.6272    ,\n",
       "        0.62179487, 0.61378205, 0.62779553, 0.624     , 0.6208    ,\n",
       "        0.62660256, 0.624     , 0.61476726, 0.62279294, 0.63449367,\n",
       "        0.61698718, 0.62019231, 0.6208    , 0.6192    , 0.62019231,\n",
       "        0.6256    , 0.6176    , 0.6192    , 0.61637239, 0.62041467,\n",
       "        0.6144    , 0.6208    , 0.62460064, 0.6211878 , 0.61980831,\n",
       "        0.62140575, 0.62179487, 0.61881978, 0.62179487, 0.62300319,\n",
       "        0.61698718, 0.61858974, 0.62019231, 0.61797753, 0.625     ,\n",
       "        0.63174603, 0.6176    , 0.62460064, 0.62679426, 0.6192    ,\n",
       "        0.61698718, 0.624     , 0.6224    , 0.62300319, 0.61858974,\n",
       "        0.62660256, 0.63301282, 0.6256    , 0.6407767 , 0.62619808,\n",
       "        0.63430421, 0.62440571, 0.62944984, 0.63434022, 0.63476874,\n",
       "        0.63166397, 0.63548387, 0.63344051, 0.62760835, 0.6288    ,\n",
       "        0.62779553, 0.6318328 , 0.63798701, 0.625     , 0.63897764,\n",
       "        0.63533225, 0.62339744, 0.62801932, 0.61821086, 0.62019231,\n",
       "        0.62321145, 0.63258786, 0.64354839, 0.62741935, 0.62998405,\n",
       "        0.62479871, 0.64469453, 0.63166397, 0.63489499, 0.63782051,\n",
       "        0.62962963, 0.61637239, 0.6388443 , 0.63166397, 0.63402889,\n",
       "        0.62987013, 0.6221865 , 0.6384    , 0.62760835, 0.63311688,\n",
       "        0.62939297, 0.62939297, 0.64193548]),\n",
       " 'split1_test_jaccard': array([0.66341463, 0.65700483, 0.63458401, 0.62460568, 0.64943457,\n",
       "        0.63057325, 0.65008026, 0.65434084, 0.63578275, 0.61102362,\n",
       "        0.62379421, 0.6271722 , 0.6483871 , 0.63929147, 0.63338789,\n",
       "        0.64090177, 0.64630225, 0.6565008 , 0.61562998, 0.63866878,\n",
       "        0.63636364, 0.63099042, 0.62834646, 0.60719875, 0.64943457,\n",
       "        0.67651888, 0.65466448, 0.63387097, 0.64090177, 0.65539452,\n",
       "        0.62838915, 0.63961039, 0.61769352, 0.59874608, 0.61041009,\n",
       "        0.61501597, 0.65528455, 0.65960912, 0.64620355, 0.65203252,\n",
       "        0.63968254, 0.64573269, 0.62101911, 0.6432    , 0.63853503,\n",
       "        0.6416    , 0.6466877 , 0.6256    , 0.66613419, 0.6607717 ,\n",
       "        0.66129032, 0.6288    , 0.64379085, 0.61904762, 0.60031596,\n",
       "        0.61057692, 0.62132921, 0.59463722, 0.60127592, 0.61965135,\n",
       "        0.65686275, 0.66666667, 0.65097403, 0.65751212, 0.61137441,\n",
       "        0.60806452, 0.61451613, 0.6163522 , 0.63650794, 0.62047244,\n",
       "        0.6336478 , 0.61912226, 0.64943457, 0.64724919, 0.63798701,\n",
       "        0.63665595, 0.64951768, 0.63843648, 0.6099844 , 0.63149351,\n",
       "        0.6318328 , 0.60753532, 0.61637239, 0.59390048, 0.6704918 ,\n",
       "        0.6639478 , 0.65630115, 0.63391442, 0.62921348, 0.63074485,\n",
       "        0.63768116, 0.62337662, 0.63937008, 0.60280374, 0.59459459,\n",
       "        0.61732283, 0.64667747, 0.42902882, 0.65316045, 0.64811784,\n",
       "        0.511     , 0.65522876, 0.64869281, 0.48533873, 0.65097403,\n",
       "        0.64772727, 0.46750524, 0.65960912, 0.65905383, 0.        ,\n",
       "        0.64542484, 0.65849673, 0.40536723, 0.65793781, 0.65309446,\n",
       "        0.00380228, 0.65089723, 0.65422078, 0.51253761, 0.66612111,\n",
       "        0.66121113, 0.49598394, 0.65686275, 0.64563107, 0.511     ,\n",
       "        0.66123779, 0.65472313, 0.58333333, 0.65686275, 0.66286645,\n",
       "        0.2845805 , 0.6525974 , 0.65139116, 0.        , 0.66557912,\n",
       "        0.65909091, 0.58608491, 0.65409836, 0.67320261, 0.39846323,\n",
       "        0.65686275, 0.66068515, 0.19628339, 0.66231648, 0.66341463,\n",
       "        0.57945205, 0.65460526, 0.65533981, 0.52941176, 0.65728477,\n",
       "        0.66178862, 0.50315457, 0.6547812 , 0.65139116, 0.48355664,\n",
       "        0.65196078, 0.63787375, 0.45945946, 0.65024631, 0.6525974 ,\n",
       "        0.44862518, 0.65960912, 0.66231648, 0.43103448, 0.65635179,\n",
       "        0.65901639, 0.52364865, 0.65953947, 0.65533981, 0.        ,\n",
       "        0.64811784, 0.65359477, 0.27389706, 0.64181524, 0.66883117,\n",
       "        0.53298611, 0.64600326, 0.66009852, 0.52631579, 0.65302782,\n",
       "        0.65798046, 0.3641791 , 0.65353038, 0.65960912, 0.32547865,\n",
       "        0.65252855, 0.66449511, 0.64548023, 0.65097403, 0.65528455,\n",
       "        0.56422764, 0.6514658 , 0.64811784, 0.65409836, 0.65252855,\n",
       "        0.65089723, 0.65089723, 0.65196078, 0.65359477, 0.65309446,\n",
       "        0.65353038, 0.65196078, 0.6503268 , 0.6503268 , 0.65196078,\n",
       "        0.65905383, 0.6497545 , 0.65528455, 0.65359477, 0.65196078,\n",
       "        0.65409836, 0.65302782, 0.65309446, 0.65024631, 0.65203252,\n",
       "        0.65466448, 0.65472313, 0.66233766, 0.67741935, 0.65517241,\n",
       "        0.65196078, 0.6503268 , 0.65316045, 0.65517241, 0.65522876,\n",
       "        0.65139116, 0.6514658 , 0.65089723, 0.65245902, 0.65353038,\n",
       "        0.6699187 , 0.66288493, 0.6601626 , 0.65196078, 0.65196078,\n",
       "        0.65359477, 0.64935065, 0.65196078, 0.65089723, 0.65196078,\n",
       "        0.65309446, 0.65964344, 0.64590164, 0.6504065 , 0.6465798 ,\n",
       "        0.64847512, 0.65691057, 0.6525974 , 0.65466448, 0.66343042,\n",
       "        0.65365854, 0.66339869, 0.65584416, 0.64516129, 0.65845649,\n",
       "        0.65203252, 0.64495114, 0.65472313, 0.63476874, 0.65089723,\n",
       "        0.66448445, 0.65365854, 0.6547812 , 0.65322581, 0.67694805,\n",
       "        0.65089723, 0.6525974 , 0.66068515, 0.64516129, 0.65203252,\n",
       "        0.65365854, 0.64458805, 0.64667747, 0.65365854, 0.63795853,\n",
       "        0.66393443, 0.65359477, 0.66666667, 0.64991896, 0.65853659,\n",
       "        0.64354839, 0.65273312, 0.6525974 , 0.65746753, 0.64781906,\n",
       "        0.64401294, 0.66612378, 0.65483871]),\n",
       " 'split2_test_jaccard': array([0.64772727, 0.62834646, 0.63449367, 0.63216561, 0.62068966,\n",
       "        0.61829653, 0.63255814, 0.61335404, 0.62264151, 0.59754224,\n",
       "        0.58320127, 0.61844197, 0.63768116, 0.63768116, 0.62578616,\n",
       "        0.62957075, 0.63191153, 0.61705426, 0.62229102, 0.63694268,\n",
       "        0.62875197, 0.5940902 , 0.59876543, 0.59097978, 0.62068966,\n",
       "        0.63091483, 0.63191153, 0.62980769, 0.61852433, 0.62302839,\n",
       "        0.61316212, 0.61695447, 0.60122699, 0.59034268, 0.6096423 ,\n",
       "        0.59293395, 0.62519685, 0.6288    , 0.62681745, 0.64114833,\n",
       "        0.63507109, 0.63723917, 0.61137441, 0.62439807, 0.625     ,\n",
       "        0.60759494, 0.61912226, 0.6046875 , 0.64      , 0.61858974,\n",
       "        0.62679426, 0.61792453, 0.62401264, 0.63993711, 0.61085271,\n",
       "        0.59375   , 0.60675883, 0.60062893, 0.59597523, 0.62015504,\n",
       "        0.63636364, 0.648     , 0.63492063, 0.61829653, 0.62419355,\n",
       "        0.59501558, 0.62300319, 0.60620155, 0.61860465, 0.62101911,\n",
       "        0.59969325, 0.60775194, 0.64448052, 0.62321145, 0.64401294,\n",
       "        0.58722741, 0.61829653, 0.63694268, 0.6078125 , 0.61695447,\n",
       "        0.60747664, 0.62325581, 0.59467919, 0.5945122 , 0.61356467,\n",
       "        0.63736264, 0.63216561, 0.62848297, 0.60663507, 0.63679245,\n",
       "        0.5984127 , 0.62222222, 0.60587326, 0.60406886, 0.60658307,\n",
       "        0.59968354, 0.63795853, 0.511     , 0.6318328 , 0.63022508,\n",
       "        0.51897436, 0.6352    , 0.63621795, 0.05544933, 0.63987138,\n",
       "        0.63242376, 0.        , 0.63504823, 0.63754045, 0.47576099,\n",
       "        0.6288    , 0.63987138, 0.19480519, 0.63123994, 0.63004847,\n",
       "        0.511     , 0.63987138, 0.63081862, 0.50791855, 0.62760835,\n",
       "        0.62884927, 0.511     , 0.63870968, 0.62980769, 0.52900232,\n",
       "        0.63563403, 0.62820513, 0.39979654, 0.6388443 , 0.62760835,\n",
       "        0.42923795, 0.6304    , 0.64205457, 0.00391389, 0.63371151,\n",
       "        0.64012739, 0.04816956, 0.6327504 , 0.62479871, 0.511     ,\n",
       "        0.63446055, 0.63942308, 0.        , 0.64401294, 0.63430421,\n",
       "        0.15738964, 0.64052288, 0.63738019, 0.51838879, 0.64057508,\n",
       "        0.63578275, 0.36548913, 0.63915858, 0.64090177, 0.54909091,\n",
       "        0.62579618, 0.63751987, 0.42301711, 0.62898089, 0.6341853 ,\n",
       "        0.515006  , 0.64135703, 0.63795853, 0.42352941, 0.63768116,\n",
       "        0.63665595, 0.63445378, 0.63344051, 0.63902439, 0.46718147,\n",
       "        0.63942308, 0.63974152, 0.48964013, 0.62261146, 0.6327504 ,\n",
       "        0.42835821, 0.63751987, 0.62898089, 0.5058309 , 0.63694268,\n",
       "        0.63132137, 0.36219081, 0.63285024, 0.64044944, 0.55400254,\n",
       "        0.64082278, 0.64847512, 0.38053097, 0.62939297, 0.63738019,\n",
       "        0.4989154 , 0.62980769, 0.63476874, 0.63057325, 0.63434022,\n",
       "        0.63751987, 0.63809524, 0.63809524, 0.63258786, 0.63955343,\n",
       "        0.64217252, 0.63694268, 0.63738019, 0.6384    , 0.6391097 ,\n",
       "        0.63116057, 0.63317384, 0.63317384, 0.63216561, 0.63317384,\n",
       "        0.63476874, 0.63897764, 0.63216561, 0.63157895, 0.63216561,\n",
       "        0.63593005, 0.63694268, 0.6352    , 0.64240506, 0.63578275,\n",
       "        0.63461538, 0.63476874, 0.63157895, 0.63099042, 0.64      ,\n",
       "        0.63157895, 0.63795853, 0.63157895, 0.63853503, 0.63723917,\n",
       "        0.64285714, 0.63621795, 0.63578275, 0.63665595, 0.63535032,\n",
       "        0.63535032, 0.63593005, 0.63578275, 0.63258786, 0.63636364,\n",
       "        0.63549921, 0.63242376, 0.6327504 , 0.63207547, 0.64205457,\n",
       "        0.63621795, 0.6341853 , 0.63099042, 0.63535032, 0.63375796,\n",
       "        0.64228935, 0.62998405, 0.62857143, 0.62559242, 0.63074485,\n",
       "        0.64251208, 0.62816456, 0.64296998, 0.62599049, 0.63578275,\n",
       "        0.62757528, 0.63665087, 0.62980769, 0.6388443 , 0.64354839,\n",
       "        0.62857143, 0.62599049, 0.63050314, 0.6263911 , 0.63232964,\n",
       "        0.63291139, 0.62700965, 0.63232964, 0.62380952, 0.62857143,\n",
       "        0.61685215, 0.6341853 , 0.63955343, 0.64398734, 0.62757528,\n",
       "        0.6433121 , 0.6327504 , 0.62992126, 0.64069952, 0.63174603,\n",
       "        0.63897764, 0.62875197, 0.64251208]),\n",
       " 'split3_test_jaccard': array([0.6568323 , 0.64375   , 0.6546875 , 0.6394453 , 0.63523957,\n",
       "        0.61941448, 0.60278207, 0.63265306, 0.60714286, 0.59056317,\n",
       "        0.60835913, 0.58796992, 0.65987461, 0.64788732, 0.66046512,\n",
       "        0.63410853, 0.63536776, 0.6265625 , 0.59667674, 0.61419753,\n",
       "        0.63076923, 0.59969325, 0.60030864, 0.63249211, 0.65457413,\n",
       "        0.65101721, 0.64779874, 0.59480122, 0.6375    , 0.64387917,\n",
       "        0.62844037, 0.61656442, 0.58499234, 0.57838661, 0.61912226,\n",
       "        0.61076923, 0.64937107, 0.65517241, 0.64641745, 0.61926606,\n",
       "        0.64632238, 0.63297045, 0.62170543, 0.60530421, 0.63343558,\n",
       "        0.59567901, 0.61360124, 0.62166405, 0.65610143, 0.6374808 ,\n",
       "        0.66666667, 0.61609907, 0.60218409, 0.60397554, 0.58064516,\n",
       "        0.6125    , 0.62111801, 0.57142857, 0.61196319, 0.60903427,\n",
       "        0.65884194, 0.62654321, 0.64228935, 0.615625  , 0.65594855,\n",
       "        0.63751987, 0.61801242, 0.6097561 , 0.61852433, 0.58035714,\n",
       "        0.6009245 , 0.60615385, 0.63117284, 0.6389325 , 0.65506329,\n",
       "        0.63692308, 0.65299685, 0.63026521, 0.60686427, 0.63239875,\n",
       "        0.60151515, 0.58397535, 0.57251908, 0.57341577, 0.66299213,\n",
       "        0.63806552, 0.63391442, 0.64086687, 0.63214838, 0.63765823,\n",
       "        0.59756098, 0.628125  , 0.58823529, 0.59665145, 0.60061444,\n",
       "        0.58358663, 0.65802269, 0.34556575, 0.65495208, 0.6544    ,\n",
       "        0.        , 0.65869219, 0.66019417, 0.5532382 , 0.6565008 ,\n",
       "        0.65861514, 0.15009381, 0.6544586 , 0.64761905, 0.        ,\n",
       "        0.65064103, 0.65378422, 0.28374233, 0.65238095, 0.65594855,\n",
       "        0.511     , 0.65594855, 0.6581876 , 0.511     , 0.65971108,\n",
       "        0.65008026, 0.        , 0.66183575, 0.65500795, 0.45504386,\n",
       "        0.65865385, 0.65273312, 0.511     , 0.64808917, 0.65654952,\n",
       "        0.51253761, 0.65495208, 0.65594855, 0.        , 0.64139021,\n",
       "        0.66452648, 0.4863774 , 0.65489567, 0.65912763, 0.49728261,\n",
       "        0.65751212, 0.64912281, 0.511     , 0.64912281, 0.65550239,\n",
       "        0.44165436, 0.65015974, 0.63836478, 0.1641791 , 0.64217252,\n",
       "        0.6488189 , 0.32022472, 0.64920635, 0.64968153, 0.4012945 ,\n",
       "        0.66131621, 0.64251208, 0.25149701, 0.65718799, 0.6511254 ,\n",
       "        0.50205198, 0.64864865, 0.65451664, 0.48211244, 0.64808917,\n",
       "        0.65550239, 0.53745072, 0.64659271, 0.6489533 , 0.56594724,\n",
       "        0.65451664, 0.65384615, 0.38964578, 0.65335463, 0.65434084,\n",
       "        0.58848921, 0.64573269, 0.65550239, 0.55428571, 0.6565008 ,\n",
       "        0.65560821, 0.50653595, 0.64943457, 0.65134707, 0.30769231,\n",
       "        0.6523126 , 0.66292135, 0.33916084, 0.65286624, 0.6464    ,\n",
       "        0.37626628, 0.66019417, 0.64808917, 0.65015974, 0.64864865,\n",
       "        0.65023847, 0.65238095, 0.6507177 , 0.6507177 , 0.6507177 ,\n",
       "        0.65238095, 0.6523126 , 0.64808917, 0.64912281, 0.64920635,\n",
       "        0.6512    , 0.65238095, 0.6507177 , 0.64649682, 0.6507177 ,\n",
       "        0.65015974, 0.65015974, 0.64920635, 0.65286624, 0.6507177 ,\n",
       "        0.64649682, 0.65669291, 0.65127389, 0.65555556, 0.6507177 ,\n",
       "        0.6523126 , 0.6507177 , 0.64603175, 0.65127389, 0.64912281,\n",
       "        0.65238095, 0.65015974, 0.6507177 , 0.64864865, 0.6496    ,\n",
       "        0.6518283 , 0.65079365, 0.64864865, 0.64864865, 0.64864865,\n",
       "        0.65023847, 0.64808917, 0.64912281, 0.64912281, 0.65175719,\n",
       "        0.65669291, 0.64835165, 0.6624    , 0.65238095, 0.66136725,\n",
       "        0.657277  , 0.66139241, 0.6492891 , 0.65981013, 0.65876777,\n",
       "        0.64984227, 0.65141956, 0.65560821, 0.65605096, 0.6539075 ,\n",
       "        0.65189873, 0.65615142, 0.6481775 , 0.65023847, 0.65977742,\n",
       "        0.6471519 , 0.65869219, 0.64873418, 0.63622047, 0.65495208,\n",
       "        0.65555556, 0.65555556, 0.65764331, 0.66190476, 0.65079365,\n",
       "        0.64984227, 0.6518283 , 0.64779874, 0.65834633, 0.65134707,\n",
       "        0.63806552, 0.65238095, 0.65335463, 0.65605096, 0.65396825,\n",
       "        0.65495208, 0.65550239, 0.64873418, 0.65560821, 0.65451664,\n",
       "        0.65134707, 0.6481775 , 0.66028708]),\n",
       " 'split4_test_jaccard': array([0.63722397, 0.64183835, 0.64308176, 0.62380952, 0.61198738,\n",
       "        0.60125589, 0.61611374, 0.58551618, 0.6124031 , 0.62052877,\n",
       "        0.615625  , 0.59196291, 0.6523126 , 0.64205457, 0.63693271,\n",
       "        0.62798092, 0.6336478 , 0.6214511 , 0.615625  , 0.60725552,\n",
       "        0.60747664, 0.60808709, 0.59627329, 0.59467919, 0.64207221,\n",
       "        0.64409449, 0.64951768, 0.63937008, 0.64057508, 0.63823065,\n",
       "        0.6       , 0.615625  , 0.62323391, 0.60896445, 0.60979463,\n",
       "        0.59906396, 0.64183835, 0.63693271, 0.64444444, 0.61715161,\n",
       "        0.61942675, 0.62870515, 0.6368    , 0.62287481, 0.61259843,\n",
       "        0.59775641, 0.60903427, 0.59597523, 0.64511041, 0.64444444,\n",
       "        0.64444444, 0.60342146, 0.61611374, 0.63091483, 0.61984733,\n",
       "        0.62063492, 0.6203125 , 0.60371517, 0.60472441, 0.58778626,\n",
       "        0.65299685, 0.64423077, 0.64659271, 0.64660494, 0.63636364,\n",
       "        0.61230769, 0.60526316, 0.59538462, 0.60410095, 0.61189358,\n",
       "        0.60374415, 0.60691824, 0.63993711, 0.64420063, 0.64135703,\n",
       "        0.63849765, 0.62063492, 0.6296875 , 0.62658228, 0.61514196,\n",
       "        0.6136725 , 0.58668731, 0.60681115, 0.59253499, 0.63866878,\n",
       "        0.65023847, 0.6368    , 0.62618297, 0.64890282, 0.6341853 ,\n",
       "        0.60620155, 0.60990712, 0.62068966, 0.58409786, 0.62279294,\n",
       "        0.61598746, 0.66237942, 0.511     , 0.66343042, 0.65555556,\n",
       "        0.511     , 0.65483871, 0.66613162, 0.511     , 0.656     ,\n",
       "        0.66022544, 0.        , 0.65806452, 0.65175719, 0.38803894,\n",
       "        0.6592    , 0.65806452, 0.18708241, 0.66183575, 0.65916399,\n",
       "        0.53719008, 0.65594855, 0.66237942, 0.51512097, 0.66346154,\n",
       "        0.65168539, 0.        , 0.66025641, 0.65912763, 0.        ,\n",
       "        0.6768    , 0.66022544, 0.30756579, 0.65700483, 0.65912763,\n",
       "        0.511     , 0.66185897, 0.65224359, 0.        , 0.6656    ,\n",
       "        0.66292135, 0.51256281, 0.6624    , 0.66451613, 0.10354223,\n",
       "        0.66346154, 0.65810594, 0.28888889, 0.66290323, 0.65500795,\n",
       "        0.39066667, 0.65348101, 0.65977742, 0.48016701, 0.66346154,\n",
       "        0.64912281, 0.43262411, 0.64903846, 0.66774194, 0.45562914,\n",
       "        0.64649682, 0.65266559, 0.2339688 , 0.65217391, 0.6581876 ,\n",
       "        0.27417027, 0.64686998, 0.65764331, 0.12842466, 0.66349206,\n",
       "        0.6624    , 0.54675325, 0.65384615, 0.65714286, 0.52190238,\n",
       "        0.6656    , 0.66507177, 0.50686378, 0.66022544, 0.66343042,\n",
       "        0.29136691, 0.66129032, 0.65434084, 0.6       , 0.66398714,\n",
       "        0.65916399, 0.35327314, 0.66091052, 0.66183575, 0.45993031,\n",
       "        0.6608    , 0.66347687, 0.42360061, 0.65869219, 0.65550239,\n",
       "        0.51478495, 0.66720257, 0.65659777, 0.65974441, 0.66141732,\n",
       "        0.65814696, 0.66237942, 0.65814696, 0.65971108, 0.6592    ,\n",
       "        0.66025641, 0.6576    , 0.6565008 , 0.6565008 , 0.66183575,\n",
       "        0.65709729, 0.65869219, 0.65709729, 0.6592    , 0.6592    ,\n",
       "        0.65654952, 0.65814696, 0.656     , 0.65810594, 0.66025641,\n",
       "        0.65709729, 0.6607717 , 0.65500795, 0.65814696, 0.66131621,\n",
       "        0.65555556, 0.65814696, 0.6592    , 0.65605096, 0.6592    ,\n",
       "        0.65544872, 0.66237942, 0.6592    , 0.65659777, 0.65605096,\n",
       "        0.65555556, 0.66235864, 0.65974441, 0.66131621, 0.65814696,\n",
       "        0.66131621, 0.65594855, 0.66131621, 0.6592    , 0.65814696,\n",
       "        0.64864865, 0.65141956, 0.65489567, 0.64659271, 0.64454976,\n",
       "        0.64696486, 0.66666667, 0.64920635, 0.6528    , 0.63479624,\n",
       "        0.64102564, 0.65550239, 0.6581876 , 0.6565008 , 0.65341812,\n",
       "        0.65402844, 0.65865385, 0.6656    , 0.65550239, 0.64556962,\n",
       "        0.6576    , 0.65822785, 0.63823065, 0.64398734, 0.65700483,\n",
       "        0.65654952, 0.664     , 0.6518283 , 0.65495208, 0.664     ,\n",
       "        0.6565008 , 0.65384615, 0.6656051 , 0.65141956, 0.65079365,\n",
       "        0.65384615, 0.6645469 , 0.65348101, 0.64649682, 0.65555556,\n",
       "        0.65555556, 0.67041801, 0.6608    , 0.65654952, 0.6507177 ,\n",
       "        0.65335463, 0.6539075 , 0.66349206]),\n",
       " 'mean_test_jaccard': array([0.64445736, 0.63827086, 0.64102212, 0.62437847, 0.62353343,\n",
       "        0.61308394, 0.61909665, 0.6151539 , 0.61383304, 0.60159718,\n",
       "        0.60231486, 0.59970567, 0.64636119, 0.64202448, 0.64070735,\n",
       "        0.62587747, 0.62867664, 0.62259672, 0.60845725, 0.6164708 ,\n",
       "        0.61896883, 0.60399762, 0.60232989, 0.59908571, 0.63787641,\n",
       "        0.64334373, 0.64051008, 0.62148999, 0.62762702, 0.63280718,\n",
       "        0.61174026, 0.61240054, 0.60078986, 0.59789943, 0.60648652,\n",
       "        0.60056775, 0.63881142, 0.63966973, 0.64077658, 0.62617287,\n",
       "        0.62784779, 0.63170817, 0.61686111, 0.61825654, 0.62279988,\n",
       "        0.60795735, 0.61794226, 0.60239786, 0.64483963, 0.63573001,\n",
       "        0.64435527, 0.60994218, 0.61648632, 0.61479368, 0.60379487,\n",
       "        0.60515798, 0.60885698, 0.59539859, 0.60088299, 0.60484131,\n",
       "        0.64875497, 0.6400441 , 0.63865664, 0.62532924, 0.62772793,\n",
       "        0.61121645, 0.6112066 , 0.60320402, 0.60956332, 0.60068034,\n",
       "        0.60004288, 0.60306618, 0.63740692, 0.63649515, 0.63965882,\n",
       "        0.61815735, 0.62722503, 0.63202663, 0.61006148, 0.61289853,\n",
       "        0.61463359, 0.59439734, 0.5941141 , 0.58769512, 0.64163005,\n",
       "        0.64320111, 0.63659814, 0.6275455 , 0.62885165, 0.62876224,\n",
       "        0.60638398, 0.61729581, 0.60827914, 0.59846926, 0.60331701,\n",
       "        0.60420924, 0.64657397, 0.46162122, 0.64422354, 0.64330905,\n",
       "        0.30819487, 0.64659838, 0.64805376, 0.4072328 , 0.64554886,\n",
       "        0.64455716, 0.22571981, 0.64683802, 0.6455269 , 0.27495999,\n",
       "        0.64100672, 0.64740686, 0.31660424, 0.64620056, 0.64413109,\n",
       "        0.41500328, 0.64504927, 0.64595999, 0.51312593, 0.64866222,\n",
       "        0.64481682, 0.21375634, 0.64841253, 0.64235217, 0.29900924,\n",
       "        0.65174694, 0.64292534, 0.36033913, 0.6455237 , 0.64562457,\n",
       "        0.44967121, 0.64363319, 0.64492305, 0.01627827, 0.64472884,\n",
       "        0.65041361, 0.32740084, 0.64635055, 0.6490463 , 0.40425761,\n",
       "        0.64749978, 0.64590469, 0.22714143, 0.64770955, 0.64571692,\n",
       "        0.41523535, 0.6453948 , 0.64445449, 0.37753548, 0.64750456,\n",
       "        0.64543541, 0.40606143, 0.6431164 , 0.64617092, 0.44113433,\n",
       "        0.64361806, 0.63586751, 0.36979935, 0.6446409 , 0.6441794 ,\n",
       "        0.40130402, 0.64535559, 0.64671463, 0.3930202 , 0.64692929,\n",
       "        0.64743223, 0.51658355, 0.64575022, 0.64391703, 0.34041798,\n",
       "        0.64728191, 0.64676646, 0.39886487, 0.64112302, 0.65027056,\n",
       "        0.45286718, 0.64138371, 0.64514338, 0.53678143, 0.64656495,\n",
       "        0.6468245 , 0.3818444 , 0.64653115, 0.64728596, 0.43932033,\n",
       "        0.6477444 , 0.6533194 , 0.4701176 , 0.64237864, 0.64213923,\n",
       "        0.50467533, 0.6459276 , 0.64171853, 0.64231259, 0.64482695,\n",
       "        0.64371948, 0.64350698, 0.64534324, 0.64412228, 0.64467312,\n",
       "        0.64698857, 0.64456321, 0.64141284, 0.64342867, 0.64732125,\n",
       "        0.64309977, 0.64283876, 0.64341468, 0.64213144, 0.64304893,\n",
       "        0.64423527, 0.64358243, 0.64193328, 0.64183397, 0.64311738,\n",
       "        0.64171773, 0.64598608, 0.64568403, 0.65094295, 0.64455948,\n",
       "        0.64317001, 0.64315102, 0.64175818, 0.64305651, 0.64531095,\n",
       "        0.64155739, 0.64411065, 0.64251724, 0.6428436 , 0.6442841 ,\n",
       "        0.65038115, 0.64597103, 0.64578781, 0.64507517, 0.64266134,\n",
       "        0.64349739, 0.64266368, 0.64411651, 0.64296222, 0.64336366,\n",
       "        0.64410756, 0.64497024, 0.64430954, 0.64444647, 0.64414989,\n",
       "        0.64464783, 0.64871213, 0.64230662, 0.64739303, 0.64510423,\n",
       "        0.64369595, 0.64715771, 0.64633038, 0.64218276, 0.64506539,\n",
       "        0.64565346, 0.64395075, 0.64989152, 0.63830002, 0.64620093,\n",
       "        0.64642878, 0.64612537, 0.63991461, 0.63809776, 0.65052913,\n",
       "        0.64295704, 0.64614626, 0.64884166, 0.64316572, 0.64582797,\n",
       "        0.64354234, 0.64439334, 0.64481498, 0.64442579, 0.64129824,\n",
       "        0.64046558, 0.64421606, 0.65038001, 0.64562361, 0.64593291,\n",
       "        0.64544765, 0.64671808, 0.64609057, 0.64758663, 0.64358326,\n",
       "        0.64341705, 0.64527074, 0.65261308]),\n",
       " 'std_test_jaccard': array([0.01627624, 0.01273614, 0.00751883, 0.01260813, 0.01725313,\n",
       "        0.01272019, 0.02024184, 0.02592666, 0.0149577 , 0.01234118,\n",
       "        0.01739052, 0.02008644, 0.00960826, 0.00352443, 0.01199992,\n",
       "        0.01520242, 0.01701965, 0.0208305 , 0.0118468 , 0.01987063,\n",
       "        0.01688253, 0.01514353, 0.01368908, 0.02053454, 0.01384495,\n",
       "        0.02081073, 0.0133121 , 0.01670828, 0.0158293 , 0.01798077,\n",
       "        0.01566105, 0.02154873, 0.01797006, 0.01257591, 0.01205011,\n",
       "        0.01108136, 0.01302176, 0.01575015, 0.00735099, 0.01812421,\n",
       "        0.01704082, 0.0105381 , 0.01427259, 0.01653233, 0.01270482,\n",
       "        0.01734034, 0.01551769, 0.02202552, 0.01665581, 0.01635369,\n",
       "        0.01770382, 0.01549684, 0.01681155, 0.02112499, 0.01317486,\n",
       "        0.0121342 , 0.01790917, 0.01262344, 0.00734941, 0.01456207,\n",
       "        0.00938008, 0.01794514, 0.01138316, 0.02441517, 0.0169637 ,\n",
       "        0.01435364, 0.00986661, 0.01007272, 0.02225726, 0.0214777 ,\n",
       "        0.02269632, 0.01463144, 0.00976775, 0.00911462, 0.01143137,\n",
       "        0.02356387, 0.02164961, 0.00502063, 0.00904619, 0.02331693,\n",
       "        0.01035714, 0.01867079, 0.01623101, 0.00806401, 0.02215539,\n",
       "        0.01282956, 0.01075777, 0.01088032, 0.01348225, 0.01240163,\n",
       "        0.01628502, 0.00940049, 0.01985642, 0.00773449, 0.01096757,\n",
       "        0.01230067, 0.01269956, 0.06617659, 0.01684447, 0.01178281,\n",
       "        0.25165691, 0.01205934, 0.01397506, 0.18028244, 0.01215067,\n",
       "        0.01436308, 0.22246939, 0.01326176, 0.00981973, 0.2280477 ,\n",
       "        0.01409893, 0.01231218, 0.12550585, 0.01407061, 0.01494351,\n",
       "        0.20584443, 0.01267625, 0.01542979, 0.0037706 , 0.01780101,\n",
       "        0.0122991 , 0.23768936, 0.01457879, 0.01425731, 0.24535556,\n",
       "        0.01825965, 0.01634805, 0.20327878, 0.01150226, 0.01722236,\n",
       "        0.08848399, 0.01645984, 0.01188975, 0.03063712, 0.01871757,\n",
       "        0.01524666, 0.24865936, 0.01361519, 0.02078763, 0.15614406,\n",
       "        0.01489903, 0.01401608, 0.17006398, 0.01560174, 0.01592949,\n",
       "        0.14364989, 0.00992114, 0.01105047, 0.16253367, 0.01103014,\n",
       "        0.01072659, 0.06189392, 0.01107003, 0.01522685, 0.07859397,\n",
       "        0.01289878, 0.01461205, 0.10554063, 0.01087514, 0.01257564,\n",
       "        0.10918589, 0.00958296, 0.01519571, 0.13547605, 0.01241198,\n",
       "        0.01488694, 0.0961823 , 0.01016327, 0.01392379, 0.22500719,\n",
       "        0.0125993 , 0.01493744, 0.08919346, 0.01442441, 0.01532981,\n",
       "        0.10235173, 0.01467666, 0.0142371 , 0.03718997, 0.01498123,\n",
       "        0.0132337 , 0.06406283, 0.01061825, 0.01419492, 0.10583169,\n",
       "        0.01001939, 0.01431411, 0.11532545, 0.01497248, 0.01461989,\n",
       "        0.06975945, 0.01771822, 0.01248754, 0.0160221 , 0.01240839,\n",
       "        0.01281634, 0.01674748, 0.01092765, 0.01352174, 0.01352496,\n",
       "        0.01171915, 0.0123746 , 0.01468283, 0.01184913, 0.00967188,\n",
       "        0.01637369, 0.01411725, 0.01412615, 0.014608  , 0.01427607,\n",
       "        0.01200757, 0.0144284 , 0.01405384, 0.01556093, 0.01460373,\n",
       "        0.01553541, 0.01500472, 0.01378368, 0.01861418, 0.01497329,\n",
       "        0.0131235 , 0.01311467, 0.01471244, 0.01400588, 0.01289683,\n",
       "        0.01490024, 0.01492383, 0.0143794 , 0.01380013, 0.01160792,\n",
       "        0.01276294, 0.01719261, 0.01384632, 0.012075  , 0.01389977,\n",
       "        0.01571035, 0.01135178, 0.01359032, 0.01319525, 0.0143224 ,\n",
       "        0.01131641, 0.01066512, 0.01360534, 0.00734293, 0.01121748,\n",
       "        0.00845655, 0.01644506, 0.00995605, 0.01050425, 0.01314805,\n",
       "        0.00762769, 0.01251223, 0.01263893, 0.01337055, 0.01262468,\n",
       "        0.00978376, 0.01234574, 0.00961939, 0.01248466, 0.00857309,\n",
       "        0.01363755, 0.0139102 , 0.01044302, 0.01151621, 0.01859306,\n",
       "        0.01416587, 0.01441522, 0.01087177, 0.01430656, 0.01285778,\n",
       "        0.01244419, 0.00945469, 0.01243731, 0.0129908 , 0.00867596,\n",
       "        0.01679642, 0.01699326, 0.01033448, 0.00806834, 0.01260732,\n",
       "        0.00941627, 0.01715766, 0.01082854, 0.01173776, 0.00935998,\n",
       "        0.00869918, 0.01444206, 0.00892454]),\n",
       " 'rank_test_jaccard': array([ 93, 177, 164, 199, 200, 217, 204, 213, 216, 243, 242, 249,  42,\n",
       "        152, 167, 197, 190, 202, 227, 212, 205, 235, 241, 250, 179, 126,\n",
       "        168, 203, 193, 185, 220, 219, 245, 252, 230, 247, 174, 172, 166,\n",
       "        196, 191, 187, 210, 206, 201, 229, 208, 240,  82, 184,  98, 224,\n",
       "        211, 214, 236, 232, 226, 253, 244, 233,  13, 170, 175, 198, 192,\n",
       "        221, 222, 238, 225, 246, 248, 239, 180, 182, 173, 207, 195, 186,\n",
       "        223, 218, 215, 254, 255, 256, 158, 128, 181, 194, 188, 189, 231,\n",
       "        209, 228, 251, 237, 234,  38, 262, 102, 127, 282,  37,  17, 269,\n",
       "         65,  92, 286,  32,  66, 284, 165,  24, 281,  46, 106, 268,  79,\n",
       "         53, 259,  15,  84, 287,  16, 146, 283,   3, 139, 278,  67,  63,\n",
       "        264, 115,  81, 288,  86,   6, 280,  43,  11, 271,  22,  56, 285,\n",
       "         19,  60, 267,  70,  94, 276,  21,  69, 270, 133,  47, 265, 116,\n",
       "        183, 277,  89, 104, 272,  71,  36, 274,  31,  23, 258,  59, 112,\n",
       "        279,  28,  34, 273, 163,   9, 263, 161,  75, 257,  39,  33, 275,\n",
       "         40,  27, 266,  18,   1, 261, 145, 150, 260,  55, 156, 147,  83,\n",
       "        113, 120,  72, 107,  87,  30,  90, 160, 122,  26, 134, 141, 124,\n",
       "        151, 136, 101, 118, 153, 154, 132, 157,  51,  61,   4,  91, 129,\n",
       "        131, 155, 135,  73, 159, 109, 144, 140, 100,   7,  52,  58,  77,\n",
       "        143, 121, 142, 108, 137, 125, 110,  80,  99,  95, 105,  88,  14,\n",
       "        148,  25,  76, 114,  29,  44, 149,  78,  62, 111,  10, 176,  45,\n",
       "         41,  49, 171, 178,   5, 138,  48,  12, 130,  57, 119,  97,  85,\n",
       "         96, 162, 169, 103,   8,  64,  54,  68,  35,  50,  20, 117, 123,\n",
       "         74,   2]),\n",
       " 'split0_test_neg_log_loss': array([-0.50659688, -0.49541939, -0.50369931, -0.58453049, -0.54113708,\n",
       "        -0.55776452, -0.58168495, -0.61457669, -0.65248227, -0.62832673,\n",
       "        -0.65688377, -0.62732658, -0.48950931, -0.48591209, -0.49487122,\n",
       "        -0.54447878, -0.5395993 , -0.52739484, -0.5970258 , -0.60186536,\n",
       "        -0.60098977, -0.6122256 , -0.61307825, -0.64737309, -0.49525748,\n",
       "        -0.50009783, -0.4775592 , -0.53928761, -0.54322008, -0.52551723,\n",
       "        -0.61921737, -0.61672893, -0.57940095, -0.61834354, -0.64426733,\n",
       "        -0.60302192, -0.48991907, -0.49845653, -0.48837094, -0.52929638,\n",
       "        -0.55860342, -0.51562189, -0.57812523, -0.5638575 , -0.56694717,\n",
       "        -0.60969757, -0.56964933, -0.59750196, -0.51873335, -0.49618874,\n",
       "        -0.50532626, -0.54963248, -0.58006683, -0.59505552, -0.57077233,\n",
       "        -0.57316491, -0.67712792, -0.61934077, -0.63150009, -0.71974193,\n",
       "        -0.48890109, -0.49793158, -0.48823635, -0.57148052, -0.54314474,\n",
       "        -0.57127303, -0.60759226, -0.64457079, -0.61916691, -0.65362379,\n",
       "        -0.6470586 , -0.65252034, -0.50452722, -0.49833752, -0.50566945,\n",
       "        -0.54082828, -0.53019411, -0.53197977, -0.57408829, -0.64877886,\n",
       "        -0.56614243, -0.68817612, -0.69037746, -0.69711542, -0.49830176,\n",
       "        -0.5035821 , -0.49990754, -0.53203262, -0.5136423 , -0.53833805,\n",
       "        -0.57907889, -0.5830977 , -0.59838415, -0.60556163, -0.65343582,\n",
       "        -0.61072685, -0.48294299, -0.71161806, -0.48082314, -0.47935998,\n",
       "        -0.71771179, -0.48159948, -0.47916514, -0.67798792, -0.47728941,\n",
       "        -0.47791951, -0.76128367, -0.47948089, -0.4819653 , -0.77989937,\n",
       "        -0.48187048, -0.47901776, -0.70549542, -0.47970372, -0.48043049,\n",
       "        -0.68904063, -0.4795434 , -0.47781875, -0.67728267, -0.47793647,\n",
       "        -0.4835719 , -0.69157316, -0.48303174, -0.47969081, -0.71938162,\n",
       "        -0.47785661, -0.47874697, -0.75963965, -0.48074965, -0.47780527,\n",
       "        -0.71689396, -0.47686381, -0.4851206 , -0.70318497, -0.48109807,\n",
       "        -0.48032449, -0.70915048, -0.48140382, -0.47951143, -0.72062331,\n",
       "        -0.48033156, -0.4784692 , -0.66946903, -0.47729497, -0.48443319,\n",
       "        -0.80226684, -0.48724065, -0.48699354, -0.65597742, -0.48426653,\n",
       "        -0.48158388, -0.66133158, -0.47953997, -0.48651518, -0.68248269,\n",
       "        -0.48274971, -0.48983001, -0.58890235, -0.48028974, -0.48686485,\n",
       "        -0.76665201, -0.4788179 , -0.48378521, -0.70875514, -0.48427386,\n",
       "        -0.48417162, -0.72852377, -0.48074124, -0.49156126, -0.79618507,\n",
       "        -0.4941553 , -0.48998197, -0.69406056, -0.48715221, -0.48219559,\n",
       "        -0.65504315, -0.48522143, -0.4803391 , -0.66213825, -0.48600354,\n",
       "        -0.49865613, -0.83526064, -0.48296677, -0.48253025, -0.59539431,\n",
       "        -0.49013708, -0.47845829, -0.54972647, -0.48319493, -0.48295919,\n",
       "        -0.54210418, -0.48212953, -0.48084392, -0.47947235, -0.48043651,\n",
       "        -0.48003138, -0.47968881, -0.4807809 , -0.47972769, -0.47915258,\n",
       "        -0.47931522, -0.47980419, -0.47908744, -0.47964131, -0.48226233,\n",
       "        -0.48103005, -0.48024302, -0.47911842, -0.47961145, -0.47967215,\n",
       "        -0.47926111, -0.479075  , -0.4795009 , -0.47929706, -0.47956852,\n",
       "        -0.47919371, -0.48120755, -0.48093543, -0.48174978, -0.47980569,\n",
       "        -0.47978849, -0.480383  , -0.47975998, -0.47938287, -0.47925545,\n",
       "        -0.47919277, -0.47923682, -0.47889815, -0.48018737, -0.48049255,\n",
       "        -0.48083022, -0.47918736, -0.47913726, -0.48036193, -0.47904089,\n",
       "        -0.47846084, -0.47857444, -0.47883516, -0.47837242, -0.47862172,\n",
       "        -0.48401378, -0.48563861, -0.48227125, -0.48200627, -0.47958806,\n",
       "        -0.48504472, -0.48887043, -0.4904788 , -0.4861754 , -0.48609513,\n",
       "        -0.49284079, -0.48475358, -0.48238585, -0.48127003, -0.48261543,\n",
       "        -0.48391169, -0.48220487, -0.48832547, -0.49110243, -0.48339119,\n",
       "        -0.48280054, -0.488933  , -0.48954961, -0.48115866, -0.48078917,\n",
       "        -0.48607191, -0.48326459, -0.47943381, -0.48641623, -0.48748176,\n",
       "        -0.49282356, -0.48907918, -0.48714685, -0.48648395, -0.48625445,\n",
       "        -0.49470866, -0.48045949, -0.48214433, -0.48136946, -0.4819272 ,\n",
       "        -0.48078759, -0.48347699, -0.47819223, -0.48004053, -0.48054264,\n",
       "        -0.47962435, -0.48224023, -0.48069393]),\n",
       " 'split1_test_neg_log_loss': array([-0.43094343, -0.43890238, -0.44232277, -0.47642543, -0.44986458,\n",
       "        -0.4704309 , -0.4817008 , -0.48026722, -0.48524633, -0.57387414,\n",
       "        -0.5649671 , -0.52791192, -0.44350755, -0.44834342, -0.4372585 ,\n",
       "        -0.46167404, -0.454538  , -0.50125272, -0.50606783, -0.48721459,\n",
       "        -0.53576108, -0.53399019, -0.49324905, -0.54908906, -0.43736542,\n",
       "        -0.43307516, -0.44320759, -0.47761762, -0.47265704, -0.47196325,\n",
       "        -0.47119033, -0.47447954, -0.51879178, -0.55116865, -0.55373651,\n",
       "        -0.52527487, -0.43734079, -0.43339223, -0.44671293, -0.44798964,\n",
       "        -0.46859294, -0.45151038, -0.48219367, -0.48882861, -0.49143774,\n",
       "        -0.49663377, -0.49139003, -0.4980015 , -0.43750186, -0.45242053,\n",
       "        -0.43707938, -0.48835637, -0.48139608, -0.49754348, -0.52597652,\n",
       "        -0.55726821, -0.5490651 , -0.65768705, -0.63509434, -0.53888582,\n",
       "        -0.43767177, -0.43063196, -0.43971572, -0.45047956, -0.50525228,\n",
       "        -0.51710734, -0.53800101, -0.50833587, -0.52123077, -0.56748136,\n",
       "        -0.55731471, -0.57022089, -0.43668577, -0.44548779, -0.46026695,\n",
       "        -0.4604252 , -0.46567659, -0.46775051, -0.53861737, -0.50732505,\n",
       "        -0.48922173, -0.53986601, -0.60054539, -0.61397139, -0.43629847,\n",
       "        -0.43746622, -0.44066416, -0.45446214, -0.45934242, -0.48044274,\n",
       "        -0.48930145, -0.51826707, -0.50869653, -0.58092193, -0.57768879,\n",
       "        -0.52958598, -0.44066288, -0.7065502 , -0.44322723, -0.43815335,\n",
       "        -0.70545482, -0.43279482, -0.43395455, -0.6960331 , -0.43171674,\n",
       "        -0.43052408, -0.70234104, -0.43323265, -0.44204801, -0.70565071,\n",
       "        -0.44000133, -0.43580043, -0.68147948, -0.43476619, -0.43169608,\n",
       "        -0.71544511, -0.43573833, -0.43124568, -0.68214666, -0.43141132,\n",
       "        -0.44096672, -0.70529292, -0.43543851, -0.43974818, -0.69516246,\n",
       "        -0.43601344, -0.43547042, -0.67234358, -0.43403418, -0.43038771,\n",
       "        -0.71433335, -0.43134648, -0.44721129, -0.72840329, -0.44458734,\n",
       "        -0.43739153, -0.65982997, -0.43487049, -0.43249752, -0.69693371,\n",
       "        -0.43259023, -0.43138433, -0.718168  , -0.43357954, -0.44398181,\n",
       "        -0.60883555, -0.44016043, -0.43786889, -0.70823226, -0.4310218 ,\n",
       "        -0.43240697, -0.5998796 , -0.43093683, -0.43314312, -0.69185987,\n",
       "        -0.42851714, -0.44447789, -0.72491073, -0.4377883 , -0.43438716,\n",
       "        -0.65417937, -0.43872752, -0.43087005, -0.61018674, -0.43579789,\n",
       "        -0.4313727 , -0.57427184, -0.430186  , -0.43763942, -0.83839767,\n",
       "        -0.43741656, -0.4366888 , -0.6720735 , -0.43913545, -0.43454626,\n",
       "        -0.55895457, -0.43349116, -0.43144503, -0.67613054, -0.43313711,\n",
       "        -0.44568385, -0.71274854, -0.43929955, -0.43361543, -0.72492342,\n",
       "        -0.43429251, -0.42533068, -0.55581006, -0.43404368, -0.43327332,\n",
       "        -0.56726332, -0.43311519, -0.43031408, -0.43226954, -0.43055249,\n",
       "        -0.42785671, -0.42793376, -0.42810016, -0.42559446, -0.42626314,\n",
       "        -0.42784669, -0.4255784 , -0.42536888, -0.42552429, -0.43025427,\n",
       "        -0.42922909, -0.4318402 , -0.42854164, -0.42788063, -0.42904079,\n",
       "        -0.42733501, -0.42752321, -0.42687178, -0.42737728, -0.42608981,\n",
       "        -0.4260186 , -0.42956374, -0.42949294, -0.43167719, -0.43048321,\n",
       "        -0.42713431, -0.4266023 , -0.42542462, -0.42750163, -0.4271949 ,\n",
       "        -0.4264137 , -0.42578884, -0.42489362, -0.43304614, -0.43244224,\n",
       "        -0.43070611, -0.42669648, -0.4264361 , -0.42786336, -0.4261767 ,\n",
       "        -0.42716652, -0.42555926, -0.42481408, -0.42533217, -0.4251574 ,\n",
       "        -0.42920942, -0.42463257, -0.42548736, -0.42744883, -0.42786552,\n",
       "        -0.43151656, -0.42659554, -0.43271919, -0.43371644, -0.4297472 ,\n",
       "        -0.43487167, -0.43315136, -0.42665835, -0.42822353, -0.42819443,\n",
       "        -0.42801011, -0.43165351, -0.43136107, -0.44145673, -0.42929673,\n",
       "        -0.43711549, -0.43288193, -0.43892093, -0.4334533 , -0.42510805,\n",
       "        -0.42688751, -0.42818496, -0.42617073, -0.43143519, -0.42895627,\n",
       "        -0.42904717, -0.4337033 , -0.43589664, -0.43474774, -0.43738952,\n",
       "        -0.42518956, -0.42754407, -0.4277381 , -0.42792606, -0.42462993,\n",
       "        -0.42991378, -0.42840561, -0.43080118, -0.42822484, -0.42444675,\n",
       "        -0.42779704, -0.42627256, -0.42901239]),\n",
       " 'split2_test_neg_log_loss': array([-0.45918918, -0.46331985, -0.46668297, -0.47322517, -0.50877023,\n",
       "        -0.50655526, -0.52056135, -0.54528257, -0.52881248, -0.5994449 ,\n",
       "        -0.59143913, -0.6043015 , -0.45689847, -0.46279091, -0.46394696,\n",
       "        -0.49543175, -0.49942922, -0.51377231, -0.54463264, -0.54205349,\n",
       "        -0.53867472, -0.61650147, -0.60311625, -0.61986603, -0.47159663,\n",
       "        -0.45671417, -0.46692936, -0.5108672 , -0.50536392, -0.49492573,\n",
       "        -0.51127446, -0.54520019, -0.55581559, -0.59261379, -0.59546382,\n",
       "        -0.61690195, -0.45847788, -0.462476  , -0.46342717, -0.47452403,\n",
       "        -0.48136517, -0.48121783, -0.51398626, -0.50496166, -0.51699059,\n",
       "        -0.5393265 , -0.52762261, -0.55354383, -0.45848198, -0.48162876,\n",
       "        -0.46098852, -0.51406768, -0.49502252, -0.47828163, -0.53369679,\n",
       "        -0.62862768, -0.62952392, -0.64115919, -0.60783665, -0.64753555,\n",
       "        -0.46546983, -0.45796736, -0.47416829, -0.50585198, -0.49260143,\n",
       "        -0.49635275, -0.53289386, -0.53804969, -0.57358209, -0.57007279,\n",
       "        -0.58399427, -0.6005    , -0.4584649 , -0.47415097, -0.45507612,\n",
       "        -0.52587626, -0.47259713, -0.49947209, -0.59054373, -0.56170445,\n",
       "        -0.57284939, -0.61902313, -0.64150701, -0.63181458, -0.45632327,\n",
       "        -0.45926374, -0.46582071, -0.48248204, -0.49946806, -0.49563003,\n",
       "        -0.56025456, -0.51445575, -0.52833866, -0.56102638, -0.59966689,\n",
       "        -0.5843841 , -0.46232728, -0.69658548, -0.472042  , -0.45877947,\n",
       "        -0.68325057, -0.46150746, -0.45983698, -0.69615474, -0.45835234,\n",
       "        -0.45634443, -0.74240745, -0.45777342, -0.46282578, -0.68793467,\n",
       "        -0.46647143, -0.45609857, -0.68207229, -0.46051006, -0.4591249 ,\n",
       "        -0.74625792, -0.45618524, -0.45976811, -0.68880776, -0.45847103,\n",
       "        -0.46768411, -0.71285061, -0.46480861, -0.46047462, -0.68268765,\n",
       "        -0.4593357 , -0.46095555, -0.70897715, -0.45976994, -0.45978979,\n",
       "        -0.67555155, -0.45601563, -0.46476336, -0.71427981, -0.47032618,\n",
       "        -0.46253792, -0.6868735 , -0.46194832, -0.46051366, -0.76750367,\n",
       "        -0.45892335, -0.45602383, -0.70555764, -0.45743157, -0.46501198,\n",
       "        -0.74637116, -0.46467225, -0.46190047, -0.55141004, -0.45923329,\n",
       "        -0.46652128, -0.70250733, -0.45773142, -0.45776168, -0.63094423,\n",
       "        -0.46334562, -0.46006227, -0.63773899, -0.4632285 , -0.46186062,\n",
       "        -0.66391171, -0.46224608, -0.45728292, -0.6689793 , -0.46474852,\n",
       "        -0.45417038, -0.53712432, -0.455975  , -0.46874094, -0.68622423,\n",
       "        -0.45934802, -0.46168468, -0.7039581 , -0.46003603, -0.46174095,\n",
       "        -0.64201311, -0.45398884, -0.4584999 , -0.63551743, -0.45682522,\n",
       "        -0.47197909, -0.65297371, -0.47077613, -0.45485204, -0.6211597 ,\n",
       "        -0.46003197, -0.45662129, -0.7423958 , -0.45435343, -0.45507603,\n",
       "        -0.69612191, -0.4570414 , -0.45277699, -0.45456081, -0.45310901,\n",
       "        -0.45164778, -0.45242049, -0.45147211, -0.44984532, -0.45021105,\n",
       "        -0.44961429, -0.44892107, -0.44864061, -0.44894346, -0.45439398,\n",
       "        -0.45256996, -0.45477768, -0.45093041, -0.45078215, -0.45035982,\n",
       "        -0.45140597, -0.45053065, -0.44962277, -0.44863168, -0.44839543,\n",
       "        -0.44964091, -0.45334287, -0.4547907 , -0.45435954, -0.45071138,\n",
       "        -0.45263957, -0.45054875, -0.44905329, -0.45180003, -0.45131087,\n",
       "        -0.44847208, -0.44996713, -0.44883422, -0.45469298, -0.45541673,\n",
       "        -0.45607162, -0.4524668 , -0.45136058, -0.45172851, -0.44940908,\n",
       "        -0.44908974, -0.4495319 , -0.4484643 , -0.44930053, -0.44932998,\n",
       "        -0.45255254, -0.45337762, -0.45061493, -0.45901977, -0.45655168,\n",
       "        -0.45422463, -0.46159077, -0.46209353, -0.46592889, -0.46383155,\n",
       "        -0.46937476, -0.46887173, -0.45618752, -0.454329  , -0.4559487 ,\n",
       "        -0.45858324, -0.4603609 , -0.44778249, -0.46373948, -0.46191568,\n",
       "        -0.45792508, -0.45621733, -0.46083632, -0.45545213, -0.45355953,\n",
       "        -0.45900832, -0.45857817, -0.4568295 , -0.4564729 , -0.45646239,\n",
       "        -0.46055896, -0.46376542, -0.46216385, -0.46007613, -0.46369972,\n",
       "        -0.4660775 , -0.45292185, -0.45180739, -0.45362921, -0.45397374,\n",
       "        -0.44836824, -0.45439932, -0.45726775, -0.45143165, -0.45872472,\n",
       "        -0.46016033, -0.45499543, -0.45632339]),\n",
       " 'split3_test_neg_log_loss': array([-0.47566908, -0.46104093, -0.46701999, -0.51992343, -0.51299847,\n",
       "        -0.5104836 , -0.53568064, -0.54742228, -0.55703873, -0.61552035,\n",
       "        -0.58517818, -0.63571158, -0.47211637, -0.48530469, -0.46613444,\n",
       "        -0.49677718, -0.50236821, -0.49143151, -0.5862106 , -0.53231109,\n",
       "        -0.52225655, -0.6172818 , -0.60868065, -0.58158833, -0.47139784,\n",
       "        -0.46639095, -0.47286147, -0.51829656, -0.50784446, -0.48725074,\n",
       "        -0.55497681, -0.54268003, -0.54094781, -0.60049679, -0.57572681,\n",
       "        -0.56878022, -0.46913967, -0.46712186, -0.471769  , -0.51526941,\n",
       "        -0.501615  , -0.48961002, -0.53294104, -0.56381748, -0.53053148,\n",
       "        -0.56292374, -0.5713838 , -0.53894401, -0.48384508, -0.48492123,\n",
       "        -0.4722304 , -0.51777512, -0.5413662 , -0.54121114, -0.64917374,\n",
       "        -0.61042536, -0.5565871 , -0.71469645, -0.60262979, -0.6227574 ,\n",
       "        -0.47121483, -0.49022294, -0.4890492 , -0.51001262, -0.4896748 ,\n",
       "        -0.51498848, -0.53915682, -0.60831277, -0.59990875, -0.67268814,\n",
       "        -0.61533193, -0.62211128, -0.4798995 , -0.47789663, -0.47311257,\n",
       "        -0.55043234, -0.49000333, -0.56719863, -0.57578559, -0.56040773,\n",
       "        -0.59470876, -0.63515302, -0.67444021, -0.66890759, -0.45910783,\n",
       "        -0.48259407, -0.47249394, -0.50360776, -0.50614945, -0.49041355,\n",
       "        -0.57829294, -0.56397781, -0.56880091, -0.62811042, -0.5821585 ,\n",
       "        -0.61050822, -0.45946121, -0.68187163, -0.45848283, -0.45312792,\n",
       "        -0.7515595 , -0.45496891, -0.45556388, -0.67440454, -0.45518339,\n",
       "        -0.45316824, -0.6831233 , -0.45438954, -0.46263912, -0.69942523,\n",
       "        -0.45865935, -0.45421334, -0.68947037, -0.4575606 , -0.45555443,\n",
       "        -0.72694907, -0.45230478, -0.45493938, -0.74133419, -0.45258047,\n",
       "        -0.45797571, -0.73811957, -0.45924252, -0.45669305, -0.69351313,\n",
       "        -0.45456361, -0.45439478, -0.68910782, -0.45735021, -0.4523601 ,\n",
       "        -0.69603879, -0.45290253, -0.45707195, -0.7057361 , -0.46377273,\n",
       "        -0.4545381 , -0.70473138, -0.45904941, -0.45470921, -0.67550497,\n",
       "        -0.45456633, -0.45441417, -0.6684063 , -0.45198679, -0.4628476 ,\n",
       "        -0.66864201, -0.46351593, -0.45560967, -0.69065981, -0.46098791,\n",
       "        -0.45888539, -0.65224814, -0.45627475, -0.4590142 , -0.63384961,\n",
       "        -0.45356974, -0.46252329, -0.77735209, -0.4629696 , -0.45810818,\n",
       "        -0.6287311 , -0.45683078, -0.46311955, -0.61336802, -0.45840275,\n",
       "        -0.46114531, -0.63889241, -0.46417369, -0.45939116, -0.66796239,\n",
       "        -0.45798071, -0.45430168, -0.72442538, -0.46331636, -0.45570498,\n",
       "        -0.55511732, -0.45675758, -0.46240068, -0.63794166, -0.45546866,\n",
       "        -0.45854986, -0.77837727, -0.45967602, -0.45755052, -0.65439809,\n",
       "        -0.45632095, -0.45339467, -0.65000136, -0.45676509, -0.45073745,\n",
       "        -0.69036882, -0.45983491, -0.45523185, -0.45506727, -0.45631826,\n",
       "        -0.45488051, -0.45536664, -0.45361723, -0.4537158 , -0.45366884,\n",
       "        -0.45533767, -0.45293505, -0.45410349, -0.45292859, -0.45607348,\n",
       "        -0.45539162, -0.45684294, -0.45483569, -0.45499452, -0.45415346,\n",
       "        -0.45399207, -0.45474212, -0.45533934, -0.45392528, -0.45333369,\n",
       "        -0.45369128, -0.45887851, -0.45601714, -0.45667002, -0.45407402,\n",
       "        -0.45447354, -0.45437192, -0.45417408, -0.45445749, -0.45340524,\n",
       "        -0.45470303, -0.45312101, -0.45284853, -0.45600518, -0.45604321,\n",
       "        -0.4561218 , -0.45542519, -0.45489195, -0.45438196, -0.45379536,\n",
       "        -0.45420976, -0.45313894, -0.45269237, -0.45290838, -0.45242574,\n",
       "        -0.45788546, -0.45843208, -0.45650377, -0.45571653, -0.45719427,\n",
       "        -0.46081796, -0.46330041, -0.46626701, -0.45375036, -0.46161404,\n",
       "        -0.46555961, -0.46416558, -0.45562543, -0.45551084, -0.45704744,\n",
       "        -0.45699812, -0.45629287, -0.46327255, -0.45517604, -0.45636815,\n",
       "        -0.45674819, -0.45787974, -0.46111538, -0.46254328, -0.45570787,\n",
       "        -0.45808143, -0.45325704, -0.4510837 , -0.45806618, -0.46044866,\n",
       "        -0.46037533, -0.46025552, -0.45744524, -0.46068711, -0.46049333,\n",
       "        -0.47151784, -0.45265785, -0.45706455, -0.45964745, -0.45635494,\n",
       "        -0.45731801, -0.45282661, -0.45481925, -0.45372481, -0.45728344,\n",
       "        -0.46215834, -0.45650616, -0.4541184 ]),\n",
       " 'split4_test_neg_log_loss': array([-0.47167135, -0.47002424, -0.46224918, -0.53413572, -0.50545985,\n",
       "        -0.51581254, -0.53995381, -0.58984171, -0.53099765, -0.58635118,\n",
       "        -0.57559423, -0.58652365, -0.46744435, -0.47420775, -0.4666958 ,\n",
       "        -0.50979283, -0.50376367, -0.49630838, -0.52698442, -0.55888923,\n",
       "        -0.55764721, -0.60451949, -0.58154335, -0.60097764, -0.47077172,\n",
       "        -0.46797338, -0.46017289, -0.48189432, -0.49192601, -0.50574365,\n",
       "        -0.54711358, -0.5595003 , -0.52781668, -0.56157259, -0.5722253 ,\n",
       "        -0.58791197, -0.4583694 , -0.46572572, -0.46752947, -0.51873846,\n",
       "        -0.50445757, -0.50049175, -0.49596874, -0.51590393, -0.53839706,\n",
       "        -0.54827703, -0.57171955, -0.54990021, -0.47988674, -0.50617472,\n",
       "        -0.48071644, -0.49698635, -0.55635422, -0.49375637, -0.57539314,\n",
       "        -0.56567333, -0.57406241, -0.5832367 , -0.67351691, -0.71243829,\n",
       "        -0.48635609, -0.45763238, -0.46794051, -0.49476641, -0.51324357,\n",
       "        -0.49805208, -0.58190251, -0.61095391, -0.56120195, -0.58639728,\n",
       "        -0.59841127, -0.58232681, -0.46280116, -0.48507442, -0.4702175 ,\n",
       "        -0.50544969, -0.5147516 , -0.52161018, -0.5427506 , -0.56944236,\n",
       "        -0.54549246, -0.63506977, -0.62011973, -0.58437838, -0.47228595,\n",
       "        -0.47520062, -0.47075695, -0.50728219, -0.48708988, -0.49822642,\n",
       "        -0.55799554, -0.56551212, -0.53977266, -0.641903  , -0.55693893,\n",
       "        -0.57381888, -0.46088998, -0.74121361, -0.45808897, -0.45756108,\n",
       "        -0.74294648, -0.45701629, -0.45315886, -0.69556245, -0.45265785,\n",
       "        -0.45347298, -0.69677231, -0.45227663, -0.46085524, -0.68639491,\n",
       "        -0.46043165, -0.45513655, -0.72938314, -0.45504599, -0.45244896,\n",
       "        -0.66162421, -0.45494919, -0.45404816, -0.6780519 , -0.45330841,\n",
       "        -0.46059516, -0.73326475, -0.45892926, -0.45502567, -0.74080936,\n",
       "        -0.45493323, -0.45486797, -0.68532523, -0.45440499, -0.4512371 ,\n",
       "        -0.70423569, -0.45184319, -0.46269594, -0.71212397, -0.46305124,\n",
       "        -0.45442326, -0.67581332, -0.45544526, -0.45414247, -0.71264562,\n",
       "        -0.45555329, -0.45327976, -0.70865125, -0.45120546, -0.45724019,\n",
       "        -0.71081495, -0.46001687, -0.45729821, -0.72974814, -0.45795508,\n",
       "        -0.45993977, -0.59904838, -0.45416501, -0.45149555, -0.68297868,\n",
       "        -0.45540058, -0.46219373, -0.74517607, -0.46210049, -0.45788615,\n",
       "        -0.81360018, -0.46125023, -0.4548171 , -0.75456925, -0.45525829,\n",
       "        -0.45279808, -0.65188987, -0.45712808, -0.45921746, -0.65707873,\n",
       "        -0.45469345, -0.45237301, -0.73869114, -0.4516407 , -0.45434152,\n",
       "        -0.64970871, -0.45531105, -0.45433244, -0.55236993, -0.45282392,\n",
       "        -0.46134343, -0.8552693 , -0.45936295, -0.45163652, -0.58142576,\n",
       "        -0.45449028, -0.45402517, -0.65976174, -0.45381721, -0.45674532,\n",
       "        -0.63821112, -0.45208195, -0.45052272, -0.45074279, -0.4538361 ,\n",
       "        -0.44985464, -0.45121437, -0.45089142, -0.45066845, -0.44934685,\n",
       "        -0.45093643, -0.45071292, -0.45097678, -0.45069896, -0.45337515,\n",
       "        -0.45164543, -0.45240874, -0.44980655, -0.45112701, -0.45053976,\n",
       "        -0.44998744, -0.44950938, -0.44965234, -0.44961521, -0.44929534,\n",
       "        -0.44907834, -0.45255285, -0.4514696 , -0.45145957, -0.45086604,\n",
       "        -0.44974843, -0.44958439, -0.44963064, -0.44891745, -0.44963357,\n",
       "        -0.45023047, -0.44873437, -0.44947885, -0.45141849, -0.45185153,\n",
       "        -0.451981  , -0.4524781 , -0.45013254, -0.45135979, -0.44975774,\n",
       "        -0.44969594, -0.45248922, -0.44859978, -0.44881949, -0.44923083,\n",
       "        -0.45131292, -0.45036627, -0.45201292, -0.45222999, -0.45206507,\n",
       "        -0.45184478, -0.45029703, -0.45375184, -0.45425547, -0.46118226,\n",
       "        -0.45698799, -0.4569478 , -0.45257226, -0.45014259, -0.45090662,\n",
       "        -0.45133224, -0.45328791, -0.44809278, -0.45276759, -0.45817658,\n",
       "        -0.45014582, -0.45944736, -0.45511029, -0.46191656, -0.45208641,\n",
       "        -0.45128263, -0.44994601, -0.45368229, -0.4491822 , -0.45072829,\n",
       "        -0.45583625, -0.44850405, -0.45117858, -0.45568685, -0.45554003,\n",
       "        -0.45785574, -0.44707388, -0.45563606, -0.45313615, -0.44957316,\n",
       "        -0.44700936, -0.44826334, -0.44850997, -0.45083089, -0.45016728,\n",
       "        -0.45364075, -0.44911983, -0.45042498]),\n",
       " 'mean_test_neg_log_loss': array([-0.46881398, -0.46574136, -0.46839484, -0.51764805, -0.50364604,\n",
       "        -0.51220936, -0.53191631, -0.55547809, -0.55091549, -0.60070346,\n",
       "        -0.59481248, -0.59635505, -0.46589521, -0.47131177, -0.46578138,\n",
       "        -0.50163091, -0.49993968, -0.50603195, -0.55218426, -0.54446675,\n",
       "        -0.55106587, -0.59690371, -0.57993351, -0.59977883, -0.46927782,\n",
       "        -0.4648503 , -0.4641461 , -0.50559266, -0.5042023 , -0.49708012,\n",
       "        -0.54075451, -0.5477178 , -0.54455456, -0.58483907, -0.58828395,\n",
       "        -0.58037819, -0.46264936, -0.46543447, -0.4675619 , -0.49716358,\n",
       "        -0.50292682, -0.48769037, -0.52064299, -0.52747384, -0.52886081,\n",
       "        -0.55137172, -0.54635306, -0.5475783 , -0.4756898 , -0.4842668 ,\n",
       "        -0.4712682 , -0.5133636 , -0.53084117, -0.52116963, -0.5710025 ,\n",
       "        -0.5870319 , -0.59727329, -0.64322403, -0.63011556, -0.6482718 ,\n",
       "        -0.46992272, -0.46687724, -0.47182201, -0.50651822, -0.50878336,\n",
       "        -0.51955473, -0.55990929, -0.58204461, -0.57501809, -0.61005267,\n",
       "        -0.60042216, -0.60553587, -0.46847571, -0.47618946, -0.47286852,\n",
       "        -0.51660235, -0.49464455, -0.51760224, -0.56435712, -0.56953169,\n",
       "        -0.55368295, -0.62345761, -0.64539796, -0.63923747, -0.46446346,\n",
       "        -0.47162135, -0.46992866, -0.49597335, -0.49313842, -0.50061016,\n",
       "        -0.55298467, -0.54906209, -0.54879858, -0.60350467, -0.59397779,\n",
       "        -0.58180481, -0.46125687, -0.70756779, -0.46253284, -0.45739636,\n",
       "        -0.72018463, -0.45757739, -0.45633588, -0.68802855, -0.45503995,\n",
       "        -0.45428585, -0.71718555, -0.45543063, -0.46206669, -0.71186098,\n",
       "        -0.46148685, -0.45605333, -0.69758014, -0.45751731, -0.45585097,\n",
       "        -0.70786339, -0.45574419, -0.45556401, -0.69352464, -0.45474154,\n",
       "        -0.46215872, -0.7162202 , -0.46029013, -0.45832647, -0.70631084,\n",
       "        -0.45654052, -0.45688713, -0.70307869, -0.45726179, -0.45431599,\n",
       "        -0.70141067, -0.45379433, -0.46337263, -0.71274563, -0.46456711,\n",
       "        -0.45784306, -0.68727973, -0.45854346, -0.45627486, -0.71464226,\n",
       "        -0.45639295, -0.45471426, -0.69405044, -0.45429967, -0.46270296,\n",
       "        -0.7073861 , -0.46312123, -0.45993416, -0.66720554, -0.45869292,\n",
       "        -0.45986746, -0.64300301, -0.4557296 , -0.45758595, -0.66442302,\n",
       "        -0.45671656, -0.46381744, -0.69481605, -0.46127533, -0.45982139,\n",
       "        -0.70541487, -0.4595745 , -0.45797497, -0.67117169, -0.45969626,\n",
       "        -0.45673162, -0.62614044, -0.4576408 , -0.46331005, -0.72916962,\n",
       "        -0.46071881, -0.45900603, -0.70664174, -0.46025615, -0.45770586,\n",
       "        -0.61216737, -0.45695401, -0.45740343, -0.63281956, -0.45685169,\n",
       "        -0.46724247, -0.76692589, -0.46241628, -0.45603695, -0.63546026,\n",
       "        -0.45905456, -0.45356602, -0.63153909, -0.45643487, -0.45575826,\n",
       "        -0.62681387, -0.4568406 , -0.45393791, -0.45442255, -0.45485047,\n",
       "        -0.45285421, -0.45332481, -0.45297236, -0.45191035, -0.45172849,\n",
       "        -0.45261006, -0.45159033, -0.45163544, -0.45154732, -0.45527184,\n",
       "        -0.45397323, -0.45522252, -0.45264654, -0.45287915, -0.4527532 ,\n",
       "        -0.45239632, -0.45227607, -0.45219742, -0.45176931, -0.45133656,\n",
       "        -0.45152457, -0.4551091 , -0.45454116, -0.45518322, -0.45318807,\n",
       "        -0.45275687, -0.45229807, -0.45160852, -0.45241189, -0.45216001,\n",
       "        -0.45180241, -0.45136963, -0.45099068, -0.45507003, -0.45524925,\n",
       "        -0.45514215, -0.45325078, -0.45239169, -0.45313911, -0.45163595,\n",
       "        -0.45172456, -0.45185875, -0.45068114, -0.4509466 , -0.45095314,\n",
       "        -0.45499482, -0.45448943, -0.45337804, -0.45528428, -0.45465292,\n",
       "        -0.45668973, -0.45813083, -0.46106208, -0.45876531, -0.46049404,\n",
       "        -0.46392697, -0.46157801, -0.45468588, -0.4538952 , -0.45494252,\n",
       "        -0.45576708, -0.45676001, -0.45576687, -0.46084845, -0.45782967,\n",
       "        -0.45694702, -0.45907187, -0.4611065 , -0.45890479, -0.45345021,\n",
       "        -0.45626636, -0.45464615, -0.45344   , -0.45631454, -0.45681547,\n",
       "        -0.45972826, -0.45906149, -0.45876623, -0.45953636, -0.46067541,\n",
       "        -0.46306986, -0.45213143, -0.45487809, -0.45514167, -0.45329179,\n",
       "        -0.4526794 , -0.45347437, -0.45391807, -0.45285055, -0.45423297,\n",
       "        -0.45667616, -0.45382684, -0.45411462]),\n",
       " 'std_test_neg_log_loss': array([0.02453373, 0.01815168, 0.01984313, 0.04104382, 0.02971545,\n",
       "        0.02781597, 0.03226832, 0.04582362, 0.05576918, 0.01954466,\n",
       "        0.03230211, 0.03834693, 0.015368  , 0.0142654 , 0.01824335,\n",
       "        0.02668744, 0.02703693, 0.01301421, 0.03460445, 0.03723977,\n",
       "        0.02739877, 0.03178116, 0.04467482, 0.03337812, 0.01846861,\n",
       "        0.02158875, 0.01198184, 0.02310313, 0.02316382, 0.01797688,\n",
       "        0.0492085 , 0.04538045, 0.02143813, 0.02491259, 0.03096701,\n",
       "        0.03185709, 0.01709973, 0.02063418, 0.01343497, 0.03084295,\n",
       "        0.0308145 , 0.02144115, 0.03342733, 0.03091519, 0.02484226,\n",
       "        0.03656846, 0.03220086, 0.03183005, 0.02717093, 0.01813098,\n",
       "        0.02247084, 0.02111381, 0.03718239, 0.04244961, 0.04370155,\n",
       "        0.02761094, 0.04885695, 0.04356751, 0.02514836, 0.06609408,\n",
       "        0.01839487, 0.02444573, 0.01798662, 0.03875872, 0.01918363,\n",
       "        0.02721146, 0.02965751, 0.05060399, 0.03362941, 0.04425366,\n",
       "        0.03009794, 0.02930342, 0.02268582, 0.01743067, 0.01765422,\n",
       "        0.03193   , 0.02455539, 0.03315562, 0.02020109, 0.04537466,\n",
       "        0.03585538, 0.04789015, 0.03326406, 0.03983257, 0.02046672,\n",
       "        0.02224403, 0.01886418, 0.02604474, 0.01901849, 0.01982207,\n",
       "        0.03303008, 0.0275588 , 0.03151848, 0.02965971, 0.03269412,\n",
       "        0.02985835, 0.01340824, 0.01964884, 0.01291369, 0.01321252,\n",
       "        0.02485836, 0.01558076, 0.01445715, 0.00972929, 0.0145305 ,\n",
       "        0.01502877, 0.02959083, 0.01474914, 0.01264179, 0.03476615,\n",
       "        0.0134987 , 0.01372423, 0.01810843, 0.01431724, 0.0155563 ,\n",
       "        0.02962514, 0.01399495, 0.01487963, 0.02425146, 0.01485219,\n",
       "        0.01384267, 0.01736856, 0.0152194 , 0.01280442, 0.02101008,\n",
       "        0.01334197, 0.0138807 , 0.03062336, 0.01487052, 0.01528121,\n",
       "        0.01491482, 0.01446128, 0.01245997, 0.00881411, 0.01190428,\n",
       "        0.01391646, 0.01826484, 0.01486239, 0.01503506, 0.03059218,\n",
       "        0.01516622, 0.01491607, 0.02092426, 0.01402027, 0.01309869,\n",
       "        0.06595038, 0.01497427, 0.01580254, 0.06273681, 0.0168867 ,\n",
       "        0.01594318, 0.03937972, 0.01541666, 0.01716692, 0.02637729,\n",
       "        0.01748434, 0.01462248, 0.07032712, 0.01356875, 0.01666786,\n",
       "        0.07166874, 0.01282481, 0.01695737, 0.055557  , 0.01563384,\n",
       "        0.01695607, 0.06621513, 0.01632858, 0.01742822, 0.07376932,\n",
       "        0.01847525, 0.01749783, 0.02325456, 0.01583112, 0.01528214,\n",
       "        0.04522115, 0.01649899, 0.01571934, 0.04299089, 0.01691359,\n",
       "        0.01780105, 0.07553226, 0.01444593, 0.01566958, 0.05116994,\n",
       "        0.01793624, 0.01689044, 0.07190935, 0.01567648, 0.01595062,\n",
       "        0.06276103, 0.01571035, 0.01610406, 0.01504808, 0.01581931,\n",
       "        0.0166014 , 0.01642685, 0.01672685, 0.01717787, 0.01680657,\n",
       "        0.01640491, 0.01721828, 0.01708238, 0.01718067, 0.01648864,\n",
       "        0.01645444, 0.01538505, 0.01610969, 0.01644071, 0.01612139,\n",
       "        0.01648706, 0.01641749, 0.01679194, 0.01656431, 0.01703671,\n",
       "        0.01691368, 0.0164776 , 0.0163447 , 0.01596916, 0.01572462,\n",
       "        0.01672863, 0.01711001, 0.01729234, 0.01652105, 0.01652921,\n",
       "        0.01683327, 0.01698786, 0.01714801, 0.01503518, 0.01529436,\n",
       "        0.01592993, 0.01663597, 0.01674178, 0.0166532 , 0.01680687,\n",
       "        0.01634111, 0.01680977, 0.01717583, 0.01684849, 0.01696314,\n",
       "        0.01752064, 0.01947041, 0.01806641, 0.01739165, 0.01646868,\n",
       "        0.01722955, 0.02019634, 0.01870966, 0.01717885, 0.01796469,\n",
       "        0.01876546, 0.01689091, 0.01766599, 0.0168837 , 0.01733772,\n",
       "        0.01784199, 0.01614322, 0.01915559, 0.01671632, 0.01724223,\n",
       "        0.01489259, 0.01781747, 0.01635382, 0.01534434, 0.0176503 ,\n",
       "        0.01890576, 0.01765611, 0.01694928, 0.01777109, 0.01879859,\n",
       "        0.0202627 , 0.01832546, 0.01672925, 0.01647472, 0.01569759,\n",
       "        0.02255452, 0.01693221, 0.01729052, 0.01705988, 0.01824919,\n",
       "        0.01661683, 0.0176467 , 0.01526372, 0.01644736, 0.01803013,\n",
       "        0.01680494, 0.01787322, 0.01646253]),\n",
       " 'rank_test_neg_log_loss': array([173, 165, 171, 205, 195, 201, 212, 227, 221, 247, 241, 242, 167,\n",
       "        178, 166, 193, 191, 198, 224, 214, 222, 243, 233, 245, 174, 163,\n",
       "        160, 197, 196, 189, 213, 218, 215, 237, 239, 234, 152, 164, 170,\n",
       "        190, 194, 185, 207, 209, 210, 223, 216, 217, 182, 184, 177, 202,\n",
       "        211, 208, 231, 238, 244, 261, 255, 263, 175, 168, 180, 199, 200,\n",
       "        206, 228, 236, 232, 250, 246, 249, 172, 183, 181, 203, 187, 204,\n",
       "        229, 230, 226, 252, 262, 259, 161, 179, 176, 188, 186, 192, 225,\n",
       "        220, 219, 248, 240, 235, 144, 279, 151, 108, 286, 111,  92, 268,\n",
       "         69,  54, 285,  79, 148, 281, 146,  88, 272, 110,  86, 280,  82,\n",
       "         80, 269,  64, 149, 284, 137, 119, 276,  95, 104, 274, 107,  56,\n",
       "        273,  46, 157, 282, 162, 116, 267, 120,  90, 283,  93,  63, 270,\n",
       "         55, 153, 278, 155, 135, 265, 121, 134, 260,  81, 112, 264,  98,\n",
       "        158, 271, 145, 133, 275, 130, 117, 266, 131,  99, 253, 113, 156,\n",
       "        287, 140, 125, 277, 136, 114, 251, 106, 109, 257, 103, 169, 288,\n",
       "        150,  87, 258, 126,  45, 256,  94,  83, 254, 102,  50,  57,  65,\n",
       "         33,  40,  35,  18,  14,  27,   9,  11,   8,  77,  51,  75,  28,\n",
       "         34,  30,  25,  22,  21,  15,   5,   7,  71,  59,  74,  37,  31,\n",
       "         23,  10,  26,  20,  16,   6,   4,  70,  76,  73,  38,  24,  36,\n",
       "         12,  13,  17,   1,   2,   3,  68,  58,  41,  78,  61,  97, 118,\n",
       "        142, 122, 138, 159, 147,  62,  48,  67,  85, 100,  84, 141, 115,\n",
       "        105, 128, 143, 124,  43,  89,  60,  42,  91, 101, 132, 127, 123,\n",
       "        129, 139, 154,  19,  66,  72,  39,  29,  44,  49,  32,  53,  96,\n",
       "         47,  52])}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRIAL_5_MLP.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FIVE NEG LOG LOSS RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.1,\n",
       " 'classifier__hidden_layer_sizes': (20,),\n",
       " 'classifier__learning_rate': 'constant',\n",
       " 'classifier__solver': 'adam'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL FIVE NEG LOG LOSS RESULTS ---------------------------------------------\")\n",
    "TRIAL_5_MLP.cv_results_['rank_test_neg_log_loss']\n",
    "TRIAL_5_MLP.cv_results_['params'][ np.argmin(TRIAL_5_MLP.cv_results_['rank_test_neg_log_loss']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FIVE F1 RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'tanh',\n",
       " 'classifier__alpha': 0.1,\n",
       " 'classifier__hidden_layer_sizes': (15,),\n",
       " 'classifier__learning_rate': 'constant',\n",
       " 'classifier__solver': 'sgd'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL FIVE F1 RESULTS ---------------------------------------------\")\n",
    "TRIAL_5_MLP.cv_results_['rank_test_f1_micro']\n",
    "TRIAL_5_MLP.cv_results_['params'][ np.argmin(TRIAL_5_MLP.cv_results_['rank_test_f1_micro']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FIVE ROC AUC RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'logistic',\n",
       " 'classifier__alpha': 0.0001,\n",
       " 'classifier__hidden_layer_sizes': (5,),\n",
       " 'classifier__learning_rate': 'constant',\n",
       " 'classifier__solver': 'adam'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL FIVE ROC AUC RESULTS ---------------------------------------------\")\n",
    "TRIAL_5_MLP.cv_results_['rank_test_roc_auc_ovo']\n",
    "TRIAL_5_MLP.cv_results_['params'][ np.argmin(TRIAL_5_MLP.cv_results_['rank_test_roc_auc_ovo']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------TRIAL FIVE JACCARD SCORE RESULTS ---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': MLPClassifier(max_iter=250),\n",
       " 'classifier__activation': 'tanh',\n",
       " 'classifier__alpha': 0.1,\n",
       " 'classifier__hidden_layer_sizes': (15,),\n",
       " 'classifier__learning_rate': 'constant',\n",
       " 'classifier__solver': 'sgd'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---------------------------------TRIAL FIVE JACCARD SCORE RESULTS ---------------------------------------------\")\n",
    "TRIAL_5_MLP.cv_results_['rank_test_jaccard']\n",
    "TRIAL_5_MLP.cv_results_['params'][ np.argmin(TRIAL_5_MLP.cv_results_['rank_test_jaccard']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 5 Multilayered Perceptrons using best NEG LOG LOSS hyperperameters :0.7756588864020463\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 5 Multilayered Perceptrons using best F1 hyperperameters :0.5058237792901841\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 5 Multilayered Perceptrons using best ROC_AUC hyperperameters :0.5058237792901841\n",
      "------------------------------------------------------------------------------------------------\n",
      "Accuracy of Trial 5 Multilayered Perceptrons using best JACCARD hyperperameters :0.5058237792901841\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best NEG LOG LOSS results to train on test set, \n",
    "bestMPLT5_1 = MLPClassifier(activation = 'logistic', alpha = .1, hidden_layer_sizes = (20,), \n",
    "                          learning_rate = 'constant',solver = 'adam')\n",
    "bestMPLT5_1.fit(X_train,y_train)\n",
    "y_pred5_1 = bestMPLT5_1.predict(X_test)\n",
    "print('Accuracy of Trial 5 Multilayered Perceptrons using best NEG LOG LOSS hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred5_1)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best F1 results to train on test set, \n",
    "bestMPLT5_2 = MLPClassifier(activation = 'tanh', alpha = 0.1, hidden_layer_sizes = (15,), \n",
    "                          learning_rate = 'constant',solver = 'sgd')\n",
    "bestMPLT5_2.fit(X_train,y_train)\n",
    "y_pred5_2 = bestMPLT5_2.predict(X_test)\n",
    "print('Accuracy of Trial 5 Multilayered Perceptrons using best F1 hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred5_2)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best ROC_AUC results to train on test set,\n",
    "bestMPLT5_3 = MLPClassifier(activation = 'logistic', alpha = 0.0001, hidden_layer_sizes = (5,), \n",
    "                          learning_rate = 'constant',solver = 'adam')\n",
    "bestMPLT5_3.fit(X_train,y_train)\n",
    "y_pred5_3 = bestMPLT5_3.predict(X_test)\n",
    "print('Accuracy of Trial 5 Multilayered Perceptrons using best ROC_AUC hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred5_3)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "# Using Best hyperparameters of best JACCARD results to train on test set, \n",
    "bestMPLT5_4 = MLPClassifier(activation = 'tanh', alpha = 0.1, hidden_layer_sizes = (15,), \n",
    "                          learning_rate = 'constant',solver = 'sgd')\n",
    "bestMPLT5_4.fit(X_train,y_train)\n",
    "y_pred5_4 = bestMPLT5_4.predict(X_test)\n",
    "print('Accuracy of Trial 5 Multilayered Perceptrons using best JACCARD hyperparameters :'\n",
    "      + str(accuracy_score(y_test,y_pred5_4)))\n",
    "print(\"------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
